FUNDAMENTALS COMPUTERORGANIZATION ANDARCHITECTURE Mostafa Abd-El-Barr King Fahd University Petroleum & Minerals (KFUPM) Hesham El-Rewini Southern Methodist University JOHN WILEY & SONS, INC PUBLICATIONFUNDAMENTALS COMPUTER ORGANIZATION ANDARCHITECTUREWILEY SERIES PARALLEL DISTRIBUTED COMPUTING SERIES EDITOR: Albert Y. Zomaya Parallel & Distributed Simulation Systems /Richard Fujimoto Surviving Design Microprocessor Multimicroprocessor Systems: Lessons Learned /Veljko Milutinovic Mobile Processing Distributed Open Environments /Peter Sapaty Introduction Parallel Algorithms /C. Xavier S.S. Iyengar Solutions Parallel Distributed Computing Problems: Lessons fromBiological Sciences /Albert Y. Zomaya, Fikret Ercal, Stephan Olariu (Editors) New Parallel Algorithms Direct Solution Linear Equations / C. Siva Ram Murthy, K.N. Balasubramanya Murthy, Srinivas Aluru Practical PRAM Programming /Joerg Keller, Christoph Kessler, Jesper Larsson Traeff Computational Collective Intelligence /Tadeusz M. Szuba Parallel & Distributed Computing: Survey Models, Paradigms, Approaches /Claudia Leopold Fundamentals Distributed Object Systems: CORBA Perspective / Zahir Tari Omran Bukhres Pipelined Processor Farms: Structured Design Embedded Parallel Systems /Martin Fleury Andrew Downton Handbook Wireless Networks Mobile Computing /Ivan Stojmenoviic (Editor) Internet-Based Workﬂow Management: Toward Semantic Web / Dan C. Marinescu Parallel Computing Heterogeneous Networks /Alexey L. Lastovetsky Tools Environments Parallel Distributed Computing Tools / Salim Hariri Manish ParasharDistributed Computing: Fundamentals, Simulations Advanced Topics, Second Edition /Hagit Attiya Jennifer Welch Smart Environments: Technology, Protocols Applications / Diane J. Cook Sajal K. Das (Editors) Fundamentals Computer Organization Architecture /M. Abd-El-Barr H. El-RewiniFUNDAMENTALS COMPUTERORGANIZATION ANDARCHITECTURE Mostafa Abd-El-Barr King Fahd University Petroleum & Minerals (KFUPM) Hesham El-Rewini Southern Methodist University JOHN WILEY & SONS, INC PUBLICATIONThis book printed acid-free paper. /C131 Copyright #2005 John Wiley & Sons, Inc. rights reserved. Published John Wiley & Sons, Inc., Hoboken, New Jersey. Published simultaneously Canada. part publication may reproduced, stored retrieval system, transmitted form means, electronic, mechanical, photocopying, recording, scanning, otherwise, except aspermitted Section 107 108 1976 United States Copyright Act, without either prior written permission Publisher, authorization payment appropriate per-copy fee Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, 01923, permission addressed Permissions Department, John Wiley & Sons, Inc., 111 River Street, Hoboken, NJ 07030; (201) 748-6011, fax (201) 748-6008. Limit Liability /Disclaimer Warranty: publisher author used best efforts preparing book, make representations warranties respect accuracy completeness contents book speciﬁcally disclaim implied warranties merchantability ﬁtness particular purpose. warranty may created extended salesrepresentatives written sales materials. advice strategies contained herein may besuitable situation. consult professional appropriate. Neither publisher author shall liable loss proﬁt commercial damages, including limited special, incidental, consequential, damages. general information products services please contact Customer Care Department within U.S. 877-762-2974, outside U.S. 317-572-3993 fax 317-572-4002. Wiley also publishes books variety electronic formats. content appears print, however, may available electronic format. Library Congress Cataloging-in-Publication Data: Abd-El-Barr, Mostafa. Fundamentals computer organization architecture /Mostafa Abd-El-Barr, Hesham El-Rewini p. cm. — (Wiley series parallel distributed computing) Includes bibliographical references index. ISBN 0-471-46741-3 (cloth volume 1) — ISBN 0-471-46740-5 (cloth volume 2) 1. Computer architecture. 2. Parallel processing (Electronic computers) I. Abd-El-Barr, Mostafa, 1950– II. Title. III. Series. QA76.9.A73E47 2004 004.2 02—dc22 2004014372 Printed United States America 1 0987654321978-750-84 00,fax978-646-86 00,oronthewebatwww.copyrigh t.com. Requests tothePublisherTo family members (Ebtesam, Muhammad, Abd-El-Rahman, Ibrahim, Mai) support love —Mostafa Abd-El-Barr students, better tomorrow —Hesham El-Rewini& CONTENTS Preface xi 1. Introduction Computer Systems 1 1.1. Historical Background 2 1.2. Architectural Development Styles 41.3. Technological Development 51.4. Performance Measures 61.5. Summary 11Exercises 12 References Reading 14 2. Instruction Set Architecture Design 15 2.1. Memory Locations Operations 15 2.2. Addressing Modes 182.3. Instruction Types 262.4. Programming Examples 312.5. Summary 33Exercises 34References Reading 35 3. Assembly Language Programming 37 3.1. Simple Machine 383.2. Instructions Mnemonics Syntax 40 3.3. Assembler Directives Commands 43 3.4. Assembly Execution Programs 44 3.5. Example: X86Family 47 3.6. Summary 55Exercises 56References Reading 57 4. Computer Arithmetic 59 4.1. Number Systems 594.2. Integer Arithmetic 63 vii4.3 Floating-Point Arithmetic 74 4.4 Summary 79Exercises 79References Reading 81 5. Processing Unit Design 83 5.1. CPU Basics 835.2. Register Set 855.3. Datapath 895.4. CPU Instruction Cycle 915.5. Control Unit 955.6. Summary 104 Exercises 104 References 106 6. Memory System Design 107 6.1. Basic Concepts 1076.2. Cache Memory 1096.3. Summary 130Exercises 131References Reading 133 7. Memory System Design II 135 7.1. Main Memory 135 7.2. Virtual Memory 1427.3. Read-Only Memory 156 7.4. Summary 158 Exercises 158References Reading 160 8. Input–Output Design Organization 161 8.1. Basic Concepts 1628.2. Programmed /O 164 8.3. Interrupt-Driven /O 167 8.4. Direct Memory Access (DMA) 1758.5. Buses 1778.6. Input–Output Interfaces 1818.7. Summary 182 Exercises 183 References Reading 183viii CONTENTS9 Pipelining Design Techniques 185 9.1. General Concepts 185 9.2. Instruction Pipeline 187 9.3. Example Pipeline Processors 201 9.4. Instruction-Level Parallelism 2079.5. Arithmetic Pipeline 2099.6. Summary 213Exercises 213References Reading 215 10 Reduced Instruction Set Computers (RISCs) 215 10.1. RISC /CISC Evolution Cycle 217 10.2. RISCs Design Principles 21810.3. Overlapped Register Windows 220 10.4. RISCs Versus CISCs 221 10.5. Pioneer (University) RISC Machines 22310.6. Example Advanced RISC Machines 22710.7. Summary 232Exercises 233References Reading 233 11 Introduction Multiprocessors 235 11.1. Introduction 23511.2. Classiﬁcation Computer Architectures 23611.3. SIMD Schemes 244 11.4. MIMD Schemes 246 11.5. Interconnection Networks 252 11.6. Analysis Performance Metrics 25411.7. Summary 254Exercises 255References Reading 256 Index 259CONTENTS ix& PREFACE book intended students computer engineering, computer science, electrical engineering. material covered book suitable one- semester course “Computer Organization & Assembly Language” one-semester course “Computer Architecture.” book assumes studentsstudying computer organization /or computer architecture must exposure basic course digital logic design introductory course high-level computer language. book reﬂects authors’ experience teaching courses computer organ- ization computer architecture ﬁfteen years. material used book used undergraduate classes. coverage book takes basically two viewpoints computers. ﬁrst programmer’s viewpoint second overall structure function computer. Theﬁrst viewpoint covers normally taught junior level course ComputerOrganization Assembly Language second viewpoint covers isnormally taught senior level course Computer Architecture. follows,we provide chapter-by-chapter review material covered book. doingso, aim providing course instructors, students, practicing engineers /scien- tists enough information help select appropriate chapter orsequences chapters cover /review. Chapter 1 sets stage material presented remaining chapters. coverage chapter starts brief historical review development ofcomputer systems. objective understand factors affecting computingas know today hopefully forecast future computation. alsointroduce general issues related general-purpose special-purpose machines. Computer systems deﬁned interfaces number levels abstraction, providing functional support predecessor. interface application programs high-level language referred toasLanguage Architecture. Instruction Set Architecture deﬁnes interface basic machine instruction set Runtime I/O Control .A different deﬁnition computer architecture built four basic viewpoints.These structure, organization, implementation, performance.The structure deﬁnes interconnection various hardware components, theorganization deﬁnes dynamic interplay management various com- ponents, implementation deﬁnes detailed design hardware components, performance speciﬁes behavior computer system. Architectural xidevelopment styles covered Chapter 1. devote last part cov- erage chapter discussion different CPU performance measuresused. sequence consisting Chapters 2 3 introduces basic issues related instruction set architecture assembly language programming. Chapter 2 coversthe basic principles involved instruction set architecture design. start byaddressing issue storing retrieving information memory, followed discussion number different addressing modes. also explain instruction execution sequencing detail. show appli-cation presented addressing modes instruction characteristics writingsample segment codes performing number simple programming tasks.Building material presented Chapter 2, Chapter 3 considers issuesrelated assembly language programming. introduce programmer’s viewof hypothetical machine. mnemonics syntax used representing thedifferent instructions machine model introduced. follow discussion execution assembly programs assembly language example X86 Intel CISC family. sequence chapters 4 5 covers design analysis arithmetic cir- cuits design Central Processing Unit (CPU). Chapter 4 introduces thereader fundamental issues related arithmetic operations circuitsused support computation computers. ﬁrst introduce issues numberrepresentations, base conversion, integer arithmetic. particular, introduce number algorithms together hardware schemes used performing integer addition, subtraction, multiplication, division. far ﬂoating-point arith- metic, introduce issues ﬂoating-point representation, ﬂoating-point oper-ations, ﬂoating-point hardware schemes. Chapter 5 covers main issuesrelated organization design CPU. primary function CPUis execute set instructions stored computer’s memory. simple CPU con-sists set registers, Arithmetic Logic Unit (ALU), Control Unit (CU). Thebasic principles needed understanding instruction fetch-execution cycle, CPU register set design ﬁrst introduced. use basic principles design real machines 80 /C286 MIPS shown. detailed discussion typical CPU data path control unit design also provided. Chapters 6 7 combined dedicated Memory System Design. typical memory hierarchy starts small, expensive, relatively fast unit, called cache . cache followed hierarchy larger, less expensive, rela- tively slow main memory unit. Cache main memory built using solid-state semiconductor material. followed hierarchy far larger, less expensive, much slower magnetic memories consist typically (hard) disk tape. start discussion Chapter 6 analyzing fac- tors inﬂuencing success memory hierarchy computer. remainingpart Chapter 6 devoted design analysis cache memories. Theissues related design analysis main virtual memory arecovered Chapter 7. brief coverage different read-only memory (ROM)implementations also provided Chapter 7.xii PREFACEI/O plays crucial role modern computer system. clear understanding appreciation fundamentals /O operations, devices, interfaces great importance. focus Chapter 8 study input–output (I /O) design organization. cover basic issues related programmed Interrupt- driven /O. interrupt architecture real machines 80 /C286 MC9328MX1 /MXL AITC explained. followed detailed discussion Direct Memory Access (DMA), busses (synchronous asynchronous), arbitration schemes. coverage Chapter 8 concludes discussion I/O interfaces. exists two basic techniques increase instruction execution rate processor. are: increase clock rate, thus decreasing instructionexecution time, alternatively increase number instructions beexecuted simultaneously. Pipelining instruction-level parallelism examplesof latter technique. Pipelining focus discussion provided Chapter9. idea one instruction processed processor time. achieved dividing execution instruction among number sub-units (stages), performing part required oper-ations, i.e., instruction fetch, instruction decode, operand fetch, instructionexecution, store results. Performance measures pipeline processor areintroduced. main issues contributing instruction pipeline hazards dis-cussed possible solutions introduced. addition, present con-cept arithmetic pipelining together problems involved designing pipeline. coverage concludes review two pipeline processors, i.e., ARM 1026EJ-S UltraSPARC-III. Chapter 10 dedicated study Reduced Instruction Set Computers (RISCs). machines represent noticeable shift computer architecture paradigm. RISC paradigm emphasizes enhancement computer architectures theresources needed make execution frequent time-consuming operations efﬁcient. RISC-based machines characterized bya number common features, as, simple reduced instruction set, ﬁxed instruction format, one instruction per machine cycle, pipeline instruction fetch /exe- cute units, ample number general purpose registers (or alternatively optimized compiler code generation), Load /Store memory operations, hardwired control unit design. coverage chapter starts discussion evolutionof RISC architectures studies led introduction. Overlapped Reg-ister Windows, essential concept RISC development, also discussed. Weshow application basic RISC principles machines BerkeleyRISC, Stanford MIPS, Compaq Alpha, SUN UltraSparc. covered essential issues design analysis uniprocessors pointing main limitations single stream machine, provide anintroduction basic concepts related multiprocessors Chapter 11. Herea number processors (two more) connected manner allows themto share simultaneous execution single task. main advantage forusing multiprocessors creation powerful computers connecting manyexisting smaller ones. addition, multiprocessor consisting number ofPREFACE xiiisingle uniprocessors expected cost effective building high- performance single processor. present number different topologies usedfor interconnecting multiple processors, different classiﬁcation schemes, atopology-based taxonomy interconnection networks. Two memory-organizationschemes MIMD (multiple instruction multiple data) multiprocessors, i.e., SharedMemory Message Passing, also introduced. coverage chapterends touch analysis performance metrics multiprocessors. Interested readers referred elaborate discussions multiprocessors book entitled Advanced Computer Architectures Parallel Processing , John Wiley Sons, Inc., 2005. chapter-by-chapter review topics covered book, clear chapters book are, great extent, self-containedand inclusive. believe approach help course instructors toselectively choose set chapters suitable targeted curriculum. However,our experience indicates group chapters consisting Chapters 1 5 8 typically suitable junior level course Computer Organization Assembly Language Computer Science, Computer Engineering, ElectricalEngineering students. group chapters consisting Chapters 1, 6, 7, 9–11is typically suitable senior level course Computer Architecture. Practicingengineers scientists ﬁnd feasible selectively consult material cov-ered individual chapters /or groups chapters indicated chapter-by- chapter review. example, ﬁnd memory system design, interested readers may consult sequence consisting Chapters 6 7. ACKNOWLEDGMENTS would like express thanks appreciation number people helped preparation book. Students Computer Organization Computer Architecture courses University Saskatchewan (UofS), SMU, KFUPM, Kuwait University used drafts different chapters andprovided us useful feedback comments led improvement ofthe presentation material book; thankful. colleaguesDonald Evan, Fatih Kocan, Peter Seidel, Mitch Thornton, A. Naseer, HabibAmmari, Hakki Cankaya offered constructive comments excellent sugges- tions led noticeable improvement style presentation book material. indebted anonymous reviewers arranged John Wiley suggestions corrections. Special thanks Albert Y. Zomaya, series editor Val Moliere, Kirsten Rohstedt, Christine Punzo JohnWiley help making book reality. course, responsibility forerrors inconsistencies rests us. Finally, all, want thankour families patience support writing book. OSTAFA ABD-EL-BARR HESHAM EL-REWINIxiv PREFACE& CHAPTER 1 Introduction Computer Systems technological advances witnessed computer industry result long chain immense successful efforts made two major forces. Theseare academia, represented university research centers, industry,represented computer companies. is, however, fair say current tech-nological advances computer industry owe inception universityresearch centers. order appreciate current technological advances thecomputer industry, one trace back history computers andtheir development. objective historical review understand factors affecting computing know today hopefully forecast future computation. great majority computers daily use areknown general purpose machines. machines built speciﬁc application mind, rather capable performing computationneeded diversity applications. machines distinguishedfrom built serve (tailored to) speciﬁc applications. latter knownasspecial purpose machines. brief historical background given Section 1.1. Computer systems conventionally deﬁned interfaces number layered abstraction levels, providing functional support pre-decessor. Included among levels application programs, high-levellanguages, set machine instructions. Based interface betweendifferent levels system, number computer architectures deﬁned.The interface application programs high-level language isreferred language architecture. Theinstruction set architecture deﬁnes interface basic machine instruction set runtime andI/O control. different deﬁnition computer architecture built four basic viewpoints. structure, organization, implementation, performance. deﬁnition, structure deﬁnes interconnection various hardware com-ponents, organization deﬁnes dynamic interplay management thevarious components, implementation deﬁnes detailed design hardwarecomponents, performance speciﬁes behavior computer system.Architectural development styles covered Section 1.2. 1Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.A number technological developments presented Section 1.3. discus- sion chapter concludes detailed coverage CPU performance measures. 1.1. HISTORICAL BACKGROUND section, would like provide historical background evolution cornerstone ideas computing industry. emphasize outset thatthe effort build computers originated one single place. everyreason us believe attempts build ﬁrst computer existed differentgeographically distributed places. also ﬁrmly believe building computer requires teamwork. Therefore, people attribute machine name single researcher, actually mean researcher may haveled team introduced machine. We, therefore, see appropriateto mention machine place ﬁrst introduced without linking thatto speciﬁc name. believe approach fair eliminateany controversy researchers names. probably fair say ﬁrst program-controlled (mechanical) computer ever build Z1 (1938). followed 1939 Z2 ﬁrst oper- ational program-controlled computer ﬁxed-point arithmetic. However, ﬁrst recorded university-based attempt build computer originated Iowa StateUniversity campus early 1940s. Researchers campus able tobuild small-scale special-purpose electronic computer. However, computerwas never completely operational. time complete design ofa fully functional programmable special-purpose machine, Z3, reported inGermany 1941. appears lack funding prevented design frombeing implemented. History recorded two attempts progress, researchers different parts world opportunities gain ﬁrst-hand experience visits laboratories institutes carrying work. assumed ﬁrst-hand visits interchange ideas enabled visitors embark similar projects laboratories back home. far general-purpose machines concerned, University Pennsylvania recorded hosted building Electronic Numerical Integrator andCalculator (ENIAC) machine 1944. ﬁrst operational general-purposemachine built using vacuum tubes. machine primarily built help compute artillery ﬁring tables World War II. programmable manual set- ting switches plugging cables. machine slow today’s standard, limited amount storage primitive programmability. improved version ENIAC proposed campus. improved version theENIAC, called Electronic Discrete Variable Automatic Computer (EDVAC),was attempt improve way programs entered explore conceptof stored programs. 1952 EDVAC project completed.Inspired ideas implemented ENIAC, researchers Institute Advanced Study (IAS) Princeton built (in 1946) IAS machine, 10 times faster ENIAC.2 INTRODUCTION COMPUTER SYSTEMSIn 1946 EDVAC project progress, similar project initiated Cambridge University. project build stored-program com- puter, known Electronic Delay Storage Automatic Calculator (EDSAC). Itwas 1949 EDSAC became world’s ﬁrst full-scale, stored-program,fully operational computer. spin-off EDSAC resulted series machinesintroduced Harvard. series consisted MARK I, II, III, IV. lattertwo machines introduced concept separate memories instructions data. term Harvard Architecture given machines indicate use separate memories. noted term Harvard Architecture used today describe machines separate cache instructions data. ﬁrst general-purpose commercial computer, UNIVersal Automatic Computer (UNIVAC I), market middle 1951. represented animprovement BINAC, built 1949. IBM announced ﬁrst com-puter, IBM701, 1952. early 1950s witnessed slowdown computerindustry. 1964 IBM announced line products name IBM 360 series. series included number models varied price performance. led Digital Equipment Corporation (DEC) introduce ﬁrst minicomputer , PDP-8. considered remarkably low-cost machine. Intel introduced ﬁrst micropro- cessor , Intel 4004, 1971. world witnessed birth ﬁrst personal computer (PC) 1977 Apple computer series ﬁrst introduced. 1977 world also witnessed introduction VAX-11 /780 DEC. Intel followed suit introducing ﬁrst popular microprocessor, 80 /C286 series. Personal computers, introduced 1977 Altair, Processor Technology, North Star, Tandy, Commodore, Apple, many others, enhancedthe productivity end-users numerous departments. Personal computers fromCompaq, Apple, IBM, Dell, many others, soon became pervasive, changedthe face computing. parallel small-scale machines, supercomputers coming play. ﬁrst supercomputer, CDC 6600, introduced 1961 ControlData Corporation. Cray Research Corporation introduced best cost /performance supercomputer, Cray-1, 1976. 1980s 1990s witnessed introduction many commercial parallel computers multiple processors. generally classiﬁed twomain categories: (1) shared memory (2) distributed memory systems. Thenumber processors single machine ranged several sharedmemory computer hundreds thousands massively parallel system.Examples parallel computers era include Sequent Symmetry, InteliPSC, nCUBE, Intel Paragon, Thinking Machines (CM-2, CM-5), MsPar (MP),Fujitsu (VPP500), others. One clear trends computing substitution centralized servers networks computers. networks connect inexpensive, powerful desktopmachines form unequaled computing power. Local area networks (LAN) ofpowerful personal computers workstations began replace mainframes andminis 1990. individual desktop computers soon connectedinto larger complexes computing wide area networks (WAN).1.1. HISTORICAL BACKGROUND 3The pervasiveness Internet created interest network computing recently grid computing. Grids geographically distributed platforms com- putation. provide dependable, consistent, pervasive, inexpensive access high-end computational facilities. Table 1.1 modiﬁed table proposed Lawrence Tesler (1995). table, major characteristics different computing paradigms associated decade computing, starting 1960. 1.2. ARCHITECTURAL DEVELOPMENT STYLES Computer architects always striving increase performance architectures. taken number forms. Among philosophy thatby single instruction, one use smaller number instructions toperform job. immediate consequence need fewermemory read /write operations eventual speedup operations. also argued increasing complexity instructions number addressingmodes theoretical advantage reducing “semantic gap” theinstructions high-level language low-level (machine) language. single (machine) instruction convert several binary coded decimal (BCD) numbers binary example complex instructions intendedto be. huge number addressing modes considered (more 20 theVAX machine) adds complexity instructions. Machines followingthis philosophy referred complex instructions set computers (CISCs). Examples CISC machines include Intel Pentium TM, Motorola MC68000TM, IBM & Macintosh PowerPCTM. noted capabilities added processors, manufacturers realized increasingly difﬁcult support higher clockrates would possible otherwise. increasedTABLE 1.1 Four Decades Computing Feature Batch Time-sharing Desktop Network Decade 1960s 1970s 1980s 1990s Location Computer room Terminal room Desktop Mobile Users Experts Specialists Individuals Groups Data Alphanumeric Text, numbers Fonts, graphs Multimedia Objective Calculate Access Present Communicate Interface Punched card Keyboard & CRT See & point Ask & tell Operation Process Edit Layout Orchestrate Connectivity None Peripheral cable LAN Internet Owners Corporate computer centersDivisional shops Departmental end-usersEveryone CRT, cathode ray tube; LAN, local area network.4 INTRODUCTION COMPUTER SYSTEMScomplexity computations within single clock period. number studies mid-1970s early-1980s also identiﬁed typical programs than80% instructions executed using assignment statements, conditionalbranching procedure calls. also surprising ﬁnd simple assign-ment statements constitute almost 50% operations. ﬁndings caused adifferent philosophy emerge. philosophy promotes optimization ofarchitectures speeding operations frequently used reducing instruction complexities number addressing modes. Machines following philosophy referred reduced instructions set computers (RISCs). Examples RISCs include Sun SPARC TMand MIPSTMmachines. two philosophies architecture design led unresolved controversy architecture style “best.” should, however, men-tioned studies indicated RISC architectures would indeed lead tofaster execution programs. majority contemporary microprocessor chips seems follow RISC paradigm. book present salient features examples CISC RISC machines. 1.3. TECHNOLOGICAL DEVELOPMENT Computer technology shown unprecedented rate improvement. includes development processors memories. Indeed, advancesin technology fueled computer industry. integration numbersof transistors (a transistor controlled /off switch) single chip increased hundred millions. impressive increase beenmade possible advances fabrication technology transistors. scale integration grown small-scale (SSI) medium-scale (MSI) large-scale (LSI) large-scale integration (VLSI), currently wafer-scale integration (WSI). Table 1.2 shows typical numbers devices per chipin technologies. mentioned continuous decrease minimum devices feature size led continuous increase number devices per chip, TABLE 1.2 Numbers Devices per Chip Integration Technology Typical number devices Typical functions SSI Bipolar 10–20 Gates ﬂip-ﬂops MSI Bipolar & MOS 50–100 Adders & counters LSI Bipolar & MOS 100–10,000 ROM & RAM VLSI CMOS (mostly) 10,000–5,000,000 ProcessorsWSI CMOS .5,000,000 DSP & special purposes SSI, small-scale integration; MSI, medium-scale integration; LSI, large-scale integration; VLSI, large-scale integration; WSI, wafer-scale integration.1.3. TECHNOLOGICAL DEVELOPMENT 5which turn led number developments. Among increase number devices RAM memories, turn helps designers trade offmemory size speed. improvement feature size provides golden oppor-tunities introducing improved design styles. 1.4. PERFORMANCE MEASURES section, consider important issue assessing performance computer. particular, focus discussion number performance measures used assess computers. Let us admit outset various facets performance computer. example, user acomputer measures performance based time taken execute givenjob (program). hand, laboratory engineer measures performanceof system total amount work done given time. userconsiders program execution time measure performance, laboratoryengineer considers throughput important measure performance. Ametric assessing performance computer helps comparing alternative designs. Performance analysis help answering questions fast program executed using given computer? order answer question, need determine time taken computer execute given job. Wedeﬁne clock cycle time time two consecutive rising (trailing)edges periodic clock signal (Fig. 1.1). Clock cycles allow counting unit compu-tations, storage computation results synchronized rising (trail-ing) clock edges. time required execute job computer often expressed terms clock cycles. denote number CPU clock cycles executing job cycle count (CC), cycle time CT, clock frequency f¼1/CT.T h e time taken CPU execute job expressed CPU time ¼CC/C2CT¼CC=f may easier count number instructions executed given program ascompared counting number CPU clock cycles needed executing Figure 1.1 Clock signal6 INTRODUCTION COMPUTER SYSTEMSprogram. Therefore, average number clock cycles per instruction (CPI) used alternate performance measure. following equation showshow compute CPI. CPI¼CPU clock cycles program Instruction count CPU time ¼Instruction count /C2CPI/C2Clock cycle time ¼Instruction count /C2CPI Clock rate known instruction set given machine consists number ofinstruction categories: ALU (simple assignment arithmetic logic instruc- tions), load,store ,branch , on. case CPI instruction category known, overall CPI computed CPI¼Pn i¼1CPI i/C2Ii Instruction count Iiis number times instruction type iis executed program CPI iis average number clock cycles needed execute instruction. Example Consider computing overall CPI machine following performance measures recorded executing set benchmark programs. Assume clock rate CPU 200 MHz. Instruction categoryPercentage occurrenceNo. cycles per instruction ALU 38 1 Load & store 15 3 Branch 42 4 Others 5 5 Assuming execution 100 instructions, overall CPI computed CPI a¼Pn i¼1CPI i/C2Ii Instruction count¼38/C21þ15/C23þ42/C24þ5/C25 100¼2:76 noted CPI reﬂects organization instruction set archi- tecture processor instruction count reﬂects instruction set archi-tecture compiler technology used. shows degree interdependence two performance parameters. Therefore, imperative the1.4. PERFORMANCE MEASURES 7CPI instruction count considered assessing merits given computer equivalently comparing performance two machines. different performance measure given lot attention recent years MIPS (million instructions-per-second (the rate instruction execution per unit time)), deﬁned MIPS ¼Instruction count Execution time /C2106¼Clock rate CPI/C2106 Example Suppose set benchmark programs considered executed another machine, call machine B, followingmeasures recorded. Instruction categoryPercentage occurrenceNo. cycles per instruction ALU 35 1 Load & store 30 2 Branch 15 3 Others 20 5 MIPS rating machine considered previous example (machine A) machine B assuming clock rate 200 MHz? CPI a¼Pn i¼1CPI i/C2Ii Instruction count¼38/C21þ15/C23þ42/C24þ5/C25 100¼2:76 MIPS a¼Clock rate CPI a/C2106¼200/C2106 2:76/C2106¼70:24 CPI b¼Pn i¼1CPI i/C2Ii Instruction count¼35/C21þ30/C22þ20/C25þ15/C23 100¼2:4 MIPS b¼Clock rate CPI a/C2106¼200/C2106 2:4/C2106¼83:67 Thus MIPS b.MIPS a. interesting note although MIPS used performance measure machines, one careful using compare machines different instruction sets. MIPS track execution time. Consider, example, following measurement made two differentmachines running given set benchmark programs.8 INTRODUCTION COMPUTER SYSTEMSInstruction categoryNo. instructions (in millions)No. cycles per instruction Machine (A) ALU 8 1 Load & store 4 3Branch 2 4 Others 4 3 Machine (B) ALU 10 1 Load & store 8 2 Branch 2 4 Others 4 3 CPI a¼Pn i¼1CPI i/C2Ii Instruction count¼(8/C21þ4/C23þ4/C23þ2/C24)/C2106 (8þ4þ4þ2)/C2106ﬃ2:2 MIPS a¼Clock rate CPI a/C2106¼200/C2106 2:2/C2106ﬃ90:9 CPU a¼Instruction count /C2CPI Clock rate¼18/C2106/C22:2 200/C2106¼0:198 CPI b¼Pn i¼1CPI i/C2Ii Instruction count¼(10/C21þ8/C22þ4/C24þ2/C24)/C2106 (10þ8þ4þ2)/C2106¼2:1 MIPS b¼Clock rate CPI a/C2106¼200/C2106 2:1/C2106¼95:2 CPU b¼Instruction count /C2CPI Clock rate¼20/C2106/C22:1 200/C2106¼0:21 MIPS b.MIPS CPU b.CPU example shows although machine B higher MIPS compared machine A, requires longer CPU time execute set benchmark programs. Million ﬂoating-point instructions per second, MFLOP (rate ﬂoating-point instruction execution per unit time) also used measure machines’performance. deﬁned MFLOPS ¼Number floating -point operations program Execution time /C21061.4. PERFORMANCE MEASURES 9While MIPS measures rate average instructions, MFLOPS deﬁned subset ﬂoating-point instructions. argument MFLOPS factthat set ﬂoating-point operations may consistent across machinesand therefore actual ﬂoating-point operations vary machine tomachine. Yet another argument fact performance machine fora given program measured MFLOPS cannot generalized provide asingle performance metric machine. performance machine regarding one particular program might interesting broad audience. use arithmetic geometric means arethe popular ways summarize performance regarding larger sets programs(e.g., benchmark suites). deﬁned below. Arithmetic mean ¼ 1 nXn i¼1Execution time Geometric mean ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1Execution time ins execution time iis execution time ith program nis total number programs set benchmarks. following table shows example computing metrics. ItemCPU time computer (s)CPU time computer B (s) Program 1 50 10 Program 2 500 100 Program 3 5000 1000 Arithmetic mean 1835 370 Geometric mean 500 100 conclude coverage section discussion known Amdahl’s law speedup ( SUo) due enhancement. case, consider speedup measure machine performs enhancement relative original performance. following relationship formulates Amdahl’s law. SUo¼Performance enhancement Performance enhancement Speedup ¼Execution time enhancement Execution time enhancement Consider, example, possible enhancement machine reduce theexecution time benchmarks 25 15 s. say speedup resulting reduction SU o¼25=15¼1:67.10 INTRODUCTION COMPUTER SYSTEMSIn given form, Amdahl’s law accounts cases whereby improvement applied instruction execution time. However, sometimes may possible achieve performance enhancement fraction time, D. case new formula developed order relate speedup, SUDdue enhance- ment fraction time Dto speedup due overall enhancement, SUo. relationship expressed SUo¼1 (1/C0D)þ(D=SUD) noted D¼1, is, enhancement possible times, SUo¼SUD, expected. Consider, example, machine speedup 30 possible applying enhancement. certain conditions enhancement onlypossible 30% time, speedup due partial applicationof enhancement? SU o¼1 (1/C0D)þ(D=SUD)¼1 (1/C00:3)þ0:3 30¼1 0:7þ0:01¼1:4 interesting note formula generalized shown toaccount case whereby number different independent enhancements canbe applied separately different fractions time, 1,D2,...,Dn, thus leading respectively speedup enhancements SUD1,SUD2,...,SUDn. SUo¼1 ½1/C0(D1þD2þ/C1/C1/C1þ Dn)/C138þ(D1þD2þ/C1/C1/C1þ Dn) (SUD1þSUD2þ/C1/C1/C1þ SUDn) 1.5. SUMMARY chapter, provided brief historical background development computer systems, starting ﬁrst recorded attempt build computer, Z1, 1938, passing CDC 6600 Cray supercomputers,and ending today’s modern high-performance machines. provideda discussion RISC versus CISC architectural styles impact onmachine performance. followed brief discussion technological development impact computing performance. coverage chapter concluded detailed treatment issues involved assessing per-formance computers. particular, introduced number performance measures CPI, MIPS, MFLOPS, Arithmetic /Geometric performance means, none deﬁning performance machine consistently. Possible1.5. SUMMARY 11ways evaluating speedup given partial general improvement measure- ments machine discussed end Chapter. EXERCISES 1. trend computing following points view? (a) Cost hardware (b) Size memory (c) Speed hardware (d) Number processing elements (e) Geographical locations system components 2. Given trend computing last 20 years, predictions future computing? 3. Find meaning following: (a) Cluster computing (b) Grid computing (c) Quantum computing (d) Nanotechnology 4. Assume switching component transistor switch zero time. propose construct disk-shaped computer chip com- ponent. limitation time takes send electronic signals one edge chip other. Make simplifying assumption elec- tronic signals travel 300,000 kilometers per second. limit-ation diameter round chip computation result byused anywhere chip clock rate 1 GHz? diameterrestrictions whole chip operate 1 THz ¼10 12Hz? chip feasible? 5. Compare uniprocessor systems multiprocessor systems following aspects: (a) Ease programming (b) need synchronization (c) Performance evaluation (d) Run time system 6. Consider program runs 50 computer A, 500 MHz clock. would like run program another machine, B, 20 s. machine B requires 2.5 times many clock cycles machine program, clock rate must machine B MHz? 7. Suppose two implementations instruction set archi- tecture. Machine clock cycle time 50 ns CPI 4.0 program, machine B clock cycle 65 ns CPI 2.5 program. machine faster much?12 INTRODUCTION COMPUTER SYSTEMS8. compiler designer trying decide two code sequences particular machine. hardware designers supplied following facts: Instruction classCPI instruction class A1 B3C4 particular high-level language, compiler writer considering two sequences require following instruction counts: Code sequenceInstruction counts (in millions) ABC 12 1 2 24 3 1 CPI sequence? code sequence faster? much? 9. Consider machine three instruction classes CPI measurements follows: Instruction classCPI instruction class A2 B5 C7 Suppose measured code given program two different compilers obtained following data: Code sequenceInstruction counts (in millions) AB C Compiler 1 15 5 3Compiler 2 25 2 2EXERCISES 13Assume machine’s clock rate 500 MHz. code sequence execute faster according MIPS? according execution time? 10. Three enhancements following speedups proposed new machine: Speedup(a) ¼30, Speedup(b) ¼20, Speedup(c) ¼15. Assume set programs, fraction use 25% enhancement (a), 30% enhancement (b), 45% enhancement (c). one enhancement implemented, chosen maximize speedup? two enhancements implemented, chosen, maximize speedup? REFERENCES READING J.-L. Baer, Computer architecture, IEEE Comput. , 17(10), 77–87, (1984). S. Dasgupta, Computer Architecture: Modern Synthesis , John Wiley, New York, 1989. M. Flynn, computer organization effectiveness ,IEEE Trans Comput. , C-21, 948–960 (1972). D. Gajski, V. Milutinovic, H. Siegel, B. Furht, Computer Architecture: Tutorial , Computer Society Press, Los Alamitos, Calif, 1987. W. Giloi, Towards taxonomy computer architecture based machine data type view, Proceedings 10th International Symposium Computer Architecture, 6–15, (1983). W. Handler, impact classiﬁcation schemes computer architecture, Proceedings 1977 International Conference Parallel Processing, 7–15, (1977). J. Hennessy D. Patterson, Computer Architecture: Quantitative Approach , 2nd ed., Morgan Kaufmann, San Francisco, 1996. K. Hwang F. Briggs, Computer Architecture Parallel Processing , 2nd ed., McGraw-Hill, New York, 1996. D. J. Kuck, Structure Computers Computations , John Wiley, New York, 1978. G. J. Myers, Advances Computer Architecture , John Wiley, New York, 1982. L. Tesler, Networked computing 1990s, reprinted Sept. 1991 Scientiﬁc American, Computer 21st Century, 10–21, (1995). P. Treleaven, Control-driven data-driven demand-driven computer architecture (abstract), Parallel Comput. , 2, (1985). P. Treleaven, D. Brownbridge, R. Hopkins, Data drive demand driven computer architecture, ACM Comput. Surv. , 14(1), 95–143, (1982). Websites http://www.gigaﬂop.demon.co.uk //C24wasel14 INTRODUCTION COMPUTER SYSTEMS& CHAPTER 2 Instruction Set Architecture Design chapter, consider basic principles involved instruction set architecture design. discussion starts consideration memory locations addresses. present abstract model main memory consideredas sequence cells capable storing nbits. address issue stor- ing retrieving information memory. information storedand/or retrieved memory needs addressed. discussion number different ways address memory locations (addressing modes) thenext topic discussed chapter. program consists number instruc-tions accessed certain order. motivates us explain issueof instruction execution sequencing detail. show applicationof presented addressing modes instruction characteristics writing samplesegment codes performing number simple programming tasks. unique characteristic computer memory organized hier- archy. hierarchy, larger slower memories used supplement smallerand faster ones. typical memory hierarchy starts small, expensive, rela-tively fast module, called cache . cache followed hierarchy larger, less expensive, relatively slow main memory part. Cache main memory built using semiconductor material. followed hierarchy larger,less expensive, far slower magnetic memories consist (hard) diskand tape. characteristics factors inﬂuencing success memory hierarchy computer discussed detail Chapters 6 7. concentration chapter (main) memory programmer’s point view. par-ticular, focus way information stored retrieved memory. 2.1. MEMORY LOCATIONS OPERATIONS (main) memory modeled array millions adjacent cells, capable storing binary digit (bit), value 1 0. cells 15Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.organized form groups ﬁxed number, say n, cells dealt atomic entity. entity consisting 8 bits called byte. many systems, entity consisting nbits stored retrieved memory using one basic memory operation called word (the smallest addressable entity). Typical size word ranges 16 64 bits. is, however, customary express size memory terms bytes. example,the size typical memory personal computer 256 Mbytes, is, 256/C22 20¼228bytes. order able move word memory, distinct address assigned word. address used determine location memory given word stored. called memory write operation. Similarly, address used determine memory locationfrom word retrieved memory. called memoryread operation. number bits, l, needed distinctly address Mwords memory given byl¼log 2M. example, size memory 64 M(read 64 mega- words), number bits address log2(64/C2220)¼log2(226)¼ 26 bits. Alternatively, number bits address l, maximum memory size (in terms number words addressed using theselbits) M¼2 l. Figure 2.1 illustrates concept memory words word address explained above. mentioned above, two basic memory operations. memory write memory read operations. memory write operation word stored memory location whose address speciﬁed. amemory read operation word read memory location whose address isspeciﬁed. Typically, memory read memory write operations performed bythecentral processing unit (CPU). Figure 2.1 Illustration main memory addressing16 INSTRUCTION SET ARCHITECTURE DESIGNThree basic steps needed order CPU perform write operation speciﬁed memory location: 1. word stored memory location ﬁrst loaded CPU speciﬁed register, called memory data register (MDR ). 2. address location word stored loaded CPU speciﬁed register, called memory address register (MAR ). 3. signal, called write , issued CPU indicating word stored MDR stored memory location whose address loaded MAR. Figure 2.2 illustrates operation writing word given 7E (in hex) memory location whose address 2005. Part aof ﬁgure shows status reg- isters memory locations involved write operation execution theoperation. Part bof ﬁgure shows status execution operation. worth mentioning MDR MAR registers used exclusively CPU accessible programmer. Similar write operation, three basic steps needed order perform memory read operation: 1. address location word read loaded MAR. 2. signal, called read, issued CPU indicating word whose address MAR read MDR. 3. time, corresponding memory delay reading speciﬁed word, required word loaded memory MDR readyfor use CPU. execution execution Figure 2.2 Illustration memory write operation2.1. MEMORY LOCATIONS OPERATIONS 17Figure 2.3 illustrates operation reading word stored memory location whose address 2010. Part aof ﬁgure shows status registers memory locations involved read operation execution operation. Part bof ﬁgure shows status read operation. 2.2. ADDRESSING MODES Information involved operation performed CPU needs addressed. computer terminology, information called operand . Therefore, instruction issued processor must carry least two types information. operation performed, encoded called op-code ﬁeld, address information operand operation beperformed, encoded called address ﬁeld. Instructions classiﬁed based number operands as: three-address , two-address ,one-and-half-address ,one-address , zero-address . explain classes together simple examples following paragraphs. noted presenting examples, would use convention operation , source ,destination express instruction. convention, operation rep- resents operation performed, example, add,subtract ,write ,o rread. source ﬁeld represents source operand(s). source operand con- stant, value stored register, value stored memory. destinationﬁeld represents place result operation stored, forexample, register memory location. three-address instruction takes form operation add-1 ,add-2 ,add-3 . form, add-1 ,add-2 , add-3 refers register memory location. Consider, example, instruction ADD R 1,R2,R3. instruction indicates thatFigure 2.3 Illustration memory read operation18 INSTRUCTION SET ARCHITECTURE DESIGNthe operation performed addition . also indicates values added stored registers R1andR2that results stored register R3. example three-address instruction refers memory locations may take form ADD ,B,C. instruction adds contents memory location Ato contents memory location Band stores result memory location C. two-address instruction takes form operation add-1 ,add-2 . form, add-1 andadd-2 refers register memory location. Consider, example, instruction ADD R 1,R2. instruction adds contents regis- terR1to contents register R2and stores results register R2. original contents register R2are lost due operation original contents register R1remain intact. instruction equivalent three-address instruction form ADD R 1,R2,R2. similar instruction uses memory locations instead registers take form ADD ,B. case, contents memory location Aare added contents memory location Band result used override original contents memory location B. operation performed three-address instruction ADD ,B,Ccan per- formed two two-address instructions MOVE B ,CandADD ,C. ﬁrst instruction moves contents location Binto location Cand second instruction adds contents location Ato location C(the con- tents location B) stores result location C. one-address instruction takes form ADD R 1. case instruction implicitly refers register, called Accumulator R acc, contents accumulator added contents register R1and results stored back accumulator Racc. memory location used instead reg- ister instruction form ADD B used. case, instruction adds content accumulator Raccto content memory location Band stores result back accumulator Racc. instruction ADD R 1is equival- ent three-address instruction ADD R 1,Racc,Raccor two-address instruc- tionADD R 1,Racc. two- one-address instruction, one-and-half address instruction. Consider, example, instruction ADD B ,R1. case, instruction adds contents register R1to contents memory location Band stores result register R1. Owing fact instruction uses two types addressing, is, register memory location, called one-and-half-address instruction. register addressing needs smaller number bits needed memory addressing. interesting indicate exist zero-address instructions. instructions use stack operation . stack data organization mechanism last data item stored ﬁrst data item retrieved. Two speciﬁc oper- ations performed stack. push popoperations. Figure 2.4 illustrates two operations. seen, speciﬁc register, called stack pointer (SP), used indicate stack location addressed. stack push operation, SP value used indicate location (called top stack) value (5A) tobe stored (in case location 1023). storing (pushing) value SP is2.2. ADDRESSING MODES 19incremented indicate location 1024. stack pop operation, SP ﬁrst decremented become 1021. value stored location (DD case) isretrieved (popped out) stored shown register. Different operations performed using stack structure. Consider, example, instruction ADD (SP)þ,(SP). instruction adds contents stack location pointed SP pointed SP þ1 stores result stack location pointed current value SP.Figure 2.5 illustrates addition operation. Table 2.1 summarizes instruc-tion classiﬁcation discussed above. different ways operands addressed called addressing modes . Addressing modes differ way address information operands speciﬁed. simplest addressing mode include operand theinstruction, is, address information needed. called immediate addressing . involved addressing mode compute address operand adding constant value content register. called indexed addressing . two addressing modes exist number addressing modes including absolute addressing, direct addressing, indirectaddressing. number different addressing modes explained below. Figure 2.4 stack push pop operations -52 39 1050-13 39 1050SP 1000 1001 10021000 1001 1002SP Figure 2.5 Addition using stack20 INSTRUCTION SET ARCHITECTURE DESIGN2.2.1. Immediate Mode According addressing mode, value operand (immediately) avail- able instruction itself. Consider, example, case loading decimal value 1000 register Ri. operation performed using instruction following: LOAD #1000, Ri. instruction, operation per- formed load value register. source operand (immediately) givenas 1000, destination register R i. noted order indi- cate value 1000 mentioned instruction operand notits address (immediate mode), customary preﬁx operand specialcharacter (#). seen use immediate addressing mode simple. use immediate addressing leads poor programming practice. change value operand requires change every instructionthat uses immediate value operand. ﬂexible addressing modeis explained below. 2.2.2. Direct (Absolute) Mode According addressing mode, address memory location holds operand included instruction. Consider, example, case loading value operand stored memory location 1000 register R i. oper- ation performed using instruction LOAD 1000, Ri. instruc- tion, source operand value stored memory location whose address 1000, destination register Ri. Note value 1000 preﬁxed special characters, indicating (direct absolute) address thesource operand. Figure 2.6 shows illustration direct addressing mode. ForTABLE 2.1 Instruction Classiﬁcation Instruction class Example Three-address ADD R 1,R2,R3 ADD ,B,C Two-address ADD R 1,R2 ADD ,B One-and-half-address ADD B ,R1 One-address ADD R 1 Zero-address ADD (SP)þ,(SP) Memory OperandOperation Address Figure 2.6 Illustration direct addressing mode2.2. ADDRESSING MODES 21example, content memory location whose address 1000 ( 2345) time instruction LOAD 1000, Riis executed, result execut- ing instruction load value ( 2345) register Ri. Direct (absolute) addressing mode provides ﬂexibility compared immediate mode. However, requires explicit inclusion operand address instruction. ﬂexible addressing mechanism provided use indirect addressing mode. explained below. 2.2.3. Indirect Mode indirect mode, included instruction address operand, rather name register memory location holds (effec-tive) address operand. order indicate use indirection instruc-tion, customary include name register memory location parentheses. Consider, example, instruction LOAD (1000), R i. instruc- tion memory location 1000 enclosed parentheses, thus indicating indirec- tion. meaning instruction load register Riwith contents memory location whose address stored memory address 1000. indirec- tion made either register memory location, therefore, identify two types indirect addressing. register indirect addressing ,i fa register used hold address operand, memory indirect addressing , memory location used hold address operand. two types illustrated Figure 2.7. Figure 2.7 Illustration indirect addressing mode22 INSTRUCTION SET ARCHITECTURE DESIGN2.2.4. Indexed Mode addressing mode, address operand obtained adding con- stant content register, called index register . Consider, example, instruction LOAD X (Rind),Ri. instruction loads register Riwith contents memory location whose address sum contents registerR indand value X. Index addressing indicated instruction including name index register parentheses using symbol Xto indicate constant added. Figure 2.8 illustrates indexed addressing. beseen, indexing requires additional level complexity register indirectaddressing. 2.2.5. Modes addressing modes presented represent commonly used modes processors. provide programmer sufﬁcient means handle general programming tasks. However, number addressing modes used number processors facilitate execution speciﬁc programmingtasks. additional addressing modes involved compared thosepresented above. Among addressing modes relative ,autoincrement , theautodecrement modes represent well-known ones. explained below. Relative Mode Recall indexed addressing, index register, R ind, used. Relative addressing indexed addressing except program counter (PC) replaces index register. example, instruction LOAD X (PC), Riloads register Riwith contents memory location whose address sum contents program counter (PC) value X. Figure 2.9 illustrates relative addressing mode. Autoincrement Mode addressing mode similar register indirect addressing mode sense effective address operand content register, call autoincrement register , included instruction. Memory +Operation Value X operand Index Register ( Rind) Figure 2.8 Illustration indexed addressing mode2.2. ADDRESSING MODES 23However, autoincrement, content autoincrement register automati- cally incremented accessing operand. before, indirection indicated byincluding autoincrement register parentheses. automatic increment theregister’s content accessing operand indicated including ( þ) parentheses. Consider, example, instruction LOAD (R auto)þ,Ri. instruction loads register Riwith operand whose address content register Rauto. loading operand register Ri, content register Rautois incremented, pointing example next item list items. Figure 2.10illustrates autoincrement addressing mode.Memory +Operation Value X operand Program Counter (PC) Figure 2.9 Illustration relative addressing mode Figure 2.10 Illustration autoincrement addressing mode24 INSTRUCTION SET ARCHITECTURE DESIGNAutodecrement Mode Similar autoincrement, autodecrement mode uses register hold address operand. However, case content autodecrement register ﬁrst decremented new contentis used effective address operand. order reﬂect fact thecontent autodecrement register decremented accessing operand,a(2) included indirection parentheses. Consider, example, instruction LOAD/C0(R auto),Ri. instruction decrements content register Rautoand uses new content effective address operand loaded register Ri. Figure 2.11 illustrates autodecrement addres- sing mode. seven addressing modes presented summarized Table 2.2. case, table shows name addressing mode, deﬁnition, gen-eric example illustrating use mode. presenting different addressing modes used load instruction illustration. However, understood types instructions given machine. following section elaborate differ- ent types instructions typically constitute instruction set givenmachine. Figure 2.11 Illustration autodecrement addressing mode2.2. ADDRESSING MODES 252.3. INSTRUCTION TYPES type instructions forming instruction set machine indication power underlying architecture machine. Instructions generalbe classiﬁed following Subsections 2.3.1 2.3.4. 2.3.1. Data Movement Instructions Data movement instructions used move data among different units machine. notably among instructions used move data among different registers CPU. simple register register movementof data made instruction MOVE R i,RjTABLE 2.2 Summary Addressing Modes Addressing mode Deﬁnition Example Operation Immediate Value operand included instructionload #1000, Ri Ri 1000 Direct (Absolute)Address operand included instructionload 1000, Ri Ri M[1000] Register indirectOperand memory location whose address register speciﬁed instructionload (Rj),Ri Ri M[Rj] Memory indirectOperand memory location whose address inthe memory location speciﬁed instructionload (1000), R Ri M[1000] Indexed Address operand sum index value thecontents index registerload X (R ind),Ri Ri M[RindþX] Relative Address operand sum index value contents program counterload X (PC),Ri Ri M[PCþX] Autoincrement Address operand register whose value isincremented fetching operandload (R auto)þ,RiRi M[Rauto] Rauto Rautoþ1 Autodecrement Address operand register whose value isdecremented fetchingthe operandload2(R auto),RiRauto Rauto21 Ri M[Rauto]26 INSTRUCTION SET ARCHITECTURE DESIGNThis instruction moves content register Rito register Rj. effect instruc- tion override contents (destination) register Rjwithout changing con- tents (source) register Ri. Data movement instructions include used move data (from) registers (to) memory. instructions usually referred load andstore instructions, respectively. Examples two instructions LOAD 25838 ,Rj STORE R i,1024 ﬁrst instruction loads content memory location whose address 25838into destination register R j. content memory location unchanged executing LOAD instruction. STORE instruction stores content source register Riinto memory location 1024. content source register unchanged executing STORE instruction. Table 2.3 shows common data transfer operations meanings. 2.3.2. Arithmetic Logical Instructions Arithmetic logical instructions used perform arithmetic logical manipulation registers memory contents. Examples arithmetic instructionsinclude ADD andSUBTRACT instructions. ADD R 1,R2,R0 SUBTRACT R 1,R2,R0 ﬁrst instruction adds contents source registers R1andR2and stores result destination register R0. second instruction subtracts contents source registers R1andR2and stores result destination register R0. contents source registers unchanged ADD SUBTRACT instructions. addition ADD andSUBTRACT instructions, machines MULTIPLY andDIVIDE instructions. two instructions expensive implement could substituted use repeated addition repeatedsubtraction. Therefore, modern architectures MULTIPLY orTABLE 2.3 Common Data Movement Operations Data movement operation Meaning MOVE Move data (a word block) given source (a register memory) given destination LOAD Load data memory register STORE Store data memory register PUSH Store data register stack POP Retrieve data stack register2.3. INSTRUCTION TYPES 27DIVIDE instructions instruction set. Table 2.4 shows common arith- metic operations meanings. Logical instructions used perform logical operations ,OR, SHIFT ,COMPARE , ROTATE . names indicate, instructions per- form, respectively, and, or, shift, compare, rotate operations register memory contents. Table 2.5 presents number logical operations. 2.3.3. Sequencing Instructions Control (sequencing) instructions used change sequence instructions executed. take form CONDITIONAL BRANCHING (CONDITIONAL JUMP ),UNCONDITIONAL BRANCHING (JUMP ), CALL instructions. common characteristic among instructions execution changes program counter ( PC) value. change made PC value unconditional, example, unconditional branching jump instructions. case, earlier value PCis lost execution program starts new value speciﬁed instruction. Consider, example,the instruction JUMP NEW -ADDRESS . Execution instruction cause PCto loaded memory location represented NEW-ADDRESS whereby instruction stored new address executed. hand,TABLE 2.4 Common Arithmetic Operations Arithmetic operations Meaning ADD Perform arithmetic sum two operands SUBTRACT Perform arithmetic difference two operands MULTIPLY Perform product two operands DIVIDE Perform division two operands INCREMENT Add one contents register DECREMENT Subtract one contents register TABLE 2.5 Common Logical Operations Logical operation Meaning Perform logical ANDing two operands Perform logical ORing two operands EXOR Perform XORing two operands Perform complement operand COMPARE Perform logical comparison two operands set ﬂag accordingly SHIFT Perform logical shift (right left) content register ROTATE Perform logical shift (right left) wraparound content register28 INSTRUCTION SET ARCHITECTURE DESIGNthe change made PCby branching instruction conditional based value speciﬁc ﬂag. Examples ﬂags include Negative (N),Zero (Z),Overﬂow (V), Carry (C). ﬂags represent individual bits speciﬁc register, called CONDITION CODE (CC)REGISTER . values ﬂags set based results executing different instructions. meaning ﬂags shown Table 2.6. Consider, example, following group instructions. LOAD #100, R1 Loop :ADD (R2)þ,R0 DECREMENT R 1 BRANCH -IF-GREATER -THAN Loop fourth instruction conditional branch instruction, indicates result decrementing contents register R1is greater zero, is, Zﬂag set, next instruction executed labeled Loop. noted conditional branch instructions could used exe- cute program loops (as shown above). TheCALL instructions used cause execution program transfer subroutine. CALL instruction effect JUMP terms loading PCwith new value next instruction executed. However, CALL instruction incremented value PC(to point next instruction sequence) pushed onto stack. Execution RETURN instruction subroutine load PCwith popped value stack. effect resuming program execution point wherebranching subroutine occurred. Figure 2.12 shows program segment uses CALL instruction. pro- gram segment sums number values, N, stores result memory location SUM . values added stored Nconsecutive memory locations starting NUM . subroutine, called ADDITION , used perform actual addition values main program stores results SUM . Table 2.7 presents common transfer control operations.TABLE 2.6 Examples Condition Flags Flag name Meaning Negative (N) Set 1 result recent operation negative, 0 otherwise Zero (Z) Set 1 result recent operation 0, 0 otherwise Overﬂow (V) Set 1 result recent operation causes overﬂow, 0 otherwise Carry (C) Set 1 recent operation results carry, 0 otherwise2.3. INSTRUCTION TYPES 292.3.4. Input /Output Instructions Input output instructions (I /O instructions) used transfer data computer peripheral devices. two basic /O instructions used INPUT andOUTPUT instructions. INPUT instruction used transfer data input device processor. Examples input devices include keyboard mouse . Input devices interfaced computer dedicated input ports . Computers use dedicated addresses address ports. Suppose input port keyboard connected computer carries unique address 1000. Therefore, execution instruction INPUT 1000 cause data stored speciﬁc register interface keyboardand computer, call input data register , moved speciﬁc register (called accumulator) computer. Similarly, execution instructionOUTPUT 2000 causes data stored accumulator moved data output register output device whose address 2000. Alternatively, com- puter address ports usual way addressing memory locations. case, computer input data input device executing instruc-tion MOVE R in,R0. instruction moves content register Rin register R0. Similarly, instruction MOVE R 0,Rinmoves contents register R0into register Rin, is, performs output operation. ThisCLEAR R0 MOVE N,R1 MOVE # NUM ,R2 CALL SUBROUTINE ADDITION MOVE R0,SUM SUBROUTINE ADDITION Loop:ADD (R2)+,R0 DEC R1 BRANCH-IF-GREATER Loop RETURN ADDITION Figure 2.12 program segment using subroutine TABLE 2.7 Transfer Control Operations Transfer control operation Meaning BRANCH-IF-CONDITION Transfer control new address condition true JUMP Unconditional transfer controlCALL Transfer control subroutineRETURN Transfer control caller routine30 INSTRUCTION SET ARCHITECTURE DESIGNlatter scheme called memory-mapped Input /Output . Among advantages memory-mapped /O ability execute number memory-dedicated instructions registers /O devices addition elimination need dedicated /O instructions. main disadvantage need dedicate part memory address space /O devices. 2.4. PROGRAMMING EXAMPLES introduced addressing modes instruction types, move illustrate use concepts number programming examples. presenting examples, generic mnemonics used. done order emphasize understanding use different addressing modes inperforming different operations independent machine used. Applications ofsimilar principles using real-life machine examples presented Chapter 3. Example 1 example, would like show program segment used perform task adding 100 numbers stored consecutive memory loca- tions starting location 1000. results stored memory location 2000. CLEAR R 0; R0 0 MOVE #100,R1; R1 100 CLEAR R 2; R2 0 LOOP :ADD 1000 (R2),R0; R0 R0þM(1000þR2) INCREMENT R 2; R2 R2þ1 DECREMENT R 1; R1 R1/C01 BRANCH -IF.0L P ;GO LOOP contents R 1.0 STORE R 0,2000 ; M(2000 ) R0 example, use made immediate ( MOVE #100,R1)a n di n e x e d( ADD 1000(R 2),R0) addressing. Example 2 example autoincrement addressing used perform task performed Example 1. CLEAR R 0; R0 0 MOVE #100,R1; R1 100 CLEAR R 2; R2 0 LOOP :ADD 1000( R2)þ,R0; R0 R0þM(1000þR2)&R2 R2þ1 DECREMENT R 1; R1 R1/C01 BRANCH -IF.0 LOOP ;GO LOOP contents R 1.0 STORE R 0,2000 ; M(2000 ) R0 seen, given task performed using one programmingmethodology. method used programmer depends /her experience2.4. PROGRAMMING EXAMPLES 31as well richness instruction set machine used. Note also use autoincrement addressing Example 2 led decrease thenumber instructions used perform task. Example 3 example illustrates use subroutine, SORT , sort Nvalues ascending order (Fig. 2.13). numbers originally stored list starting location 1000. sorted values also stored list starting location 1000. subroutine sorts data using well-known “Bubble Sort” technique. content register R 3is checked end every loop ﬁnd whether list sorted not. Example 4 example illustrates use subroutine, SEARCH , search value VAL list Nvalues (Fig. 2.14). assume list orig- inally sorted therefore brute force search used. search, value VAL compared every element list top bottom. content register R3is used indicate whether VAL found. ﬁrst element list located address 1000. Example 5 example illustrates use subroutine, SEARCH , search value VAL list Nvalues (as Example 4) (Fig. 2.15). Here, make use stack send parameters VAL andN. Figure 2.13 SORT subroutine32 INSTRUCTION SET ARCHITECTURE DESIGN2.5. SUMMARY chapter considered main issues relating instruction set design characteristics. presented model main memory memory isabstracted sequence cells, capable storing nbits. number addressing modes presented. Thes e include immediate, direct, indirect, indexed, autoincre- ment, autodecrement. Examples showing use addressing modeswere presented. also presente discussion instruction types, include data movement, arithmetic /logical, instruction sequencing, Input /Output. dis- cussion concluded presentation number examples showing use principles concepts discussed chapter programming solution anumber sample problems. next chapter, introduce concepts involvedin programming solution real-life problems using assembly language. Figure 2.14 SEARCH subroutine Figure 2.15 Subroutine SEARCH using stack send parameters VAL andN2.5. SUMMARY 33EXERCISES 1. Write program using addressing modes instruction types pre- sented Sections 2.2 2.3 reverse bits stored 16-bit register R0. 2. Consider computer number registers three reg- isters R0¼1500, R1¼4500, R2¼1000. Show effective address memory registers’ contents following instructions (assume numbers decimal). (a)ADD (R0)þ,R2 (b)SUBTRACT 2(R1),R2 (c)MOVE 500(R0),R2 (d)LOAD #5000, R2 (e)STORE R 0, 100( R2) 3. Assume top stack program pointed register SP. required write program segments perform followingtasks (assume following addressing modes available:indexed, autoincrement, autodecrement). (a) Pop top three elements stack, add them, push result back onto stack. (b) Pop top two elements stack, subtract them, push results back onto stack. (c) Push ﬁve elements (one time) onto stack. (d) Remove top ﬁve elements top stack. (e) Copy third element top stack register R 0. 4. required write program segment perform operation C AþBwhere AandBrepresents set 100 memory locations storing value set values represented Aare stored starting memory location 1000 represented Bare stored start- ing memory location 2000. results stored starting memorylocation 3000. operation performed using thefollowing instruction classes. (a) machine one-address instructions (b) machine one-and-half instructions (c) machine two-address instructions (d) machine three-address instructions (e) machine zero-address instructions 5. Write program segments perform operation C CþA/C2Busing instruction classes indicated Exercise 4 above. Assume A,B, Care memory addresses. 6. Assume series ﬁve tests offered class consisting 50 students. score obtained students ﬁve tests stored sequentially memory locations starting respectively memory locations1000, 2000, 3000, 4000, 5000. required write program34 INSTRUCTION SET ARCHITECTURE DESIGNthat calculates average score obtained student ﬁve tests store memory locations starting memory location 6000. Eachstudent identiﬁed /her student ID. may assume students’ IDs sequential. 7. Repeat Exercise 6 assuming memory used byte addressable score occupies 32-bit. 8. Rewrite program Exercise 6 assuming students’ IDs sequential, is, student ID used pointer tohis/her test scores. 9. Repeat Exercise 6 assuming students scores stored array S(50,5), is, row holds scores obtained student (each score column row) ﬁrst element thearray, is, (0,0) stored memory location 4000. scores arestored rowwise, is, one row other. average score obtainedby student stored memory location pointed /her ID. 10. Repeat Exercise 9 assuming job write subroutine perform task Exercise 9. Assume number students, number tests, location ﬁrst element thearray passed subroutine parameters registers R 1,R2, andR3, respectively. REFERENCES READING C. M. Gilmore, Microprocessors :Principles Applications , 2nd ed., McGraw-Hill, New York, 1996. V. C. Hamacher, Z. G. Vranesic, S. G. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002 A. D. Patterson, J. L. Hennessy, Computer Organization & Design ;The Hardware /Software Interface , Morgan Kaufmann, San Mateo, CA, 1994 B. Wilkinson, Computer Architecture :Design Performance , 2nd ed., Prentice-Hall, Hertfordshire, UK, 1996.REFERENCES READING 35& CHAPTER 3 Assembly Language Programming Chapter 2 introduced basic concepts principles involved design instruction set machine. chapter considers issues related assem-bly language programming. Although high-level languages compiler technol-ogy witnessed great advances years, assembly language remainsnecessary cases. Programming assembly result machine codethat much smaller much faster generated compiler high-level language. Small fast code could critical embedded portableapplications, resources may limited. cases, small portions program may heavily used written assembly language. reader book, learning assembly languages writing assembly code beextremely helpful understanding computer organization architecture. computer program represented different levels abstraction. pro- gram could written machine-independent, high-level language Javaor Cþþ. computer execute programs represented machine language speciﬁc architecture. machine language program agiven architecture collection machine instructions represented binary form. Programs written level higher machine language must trans- lated binary representation computer execute them. assemblylanguage program symbolic representation machine language program.Machine language pure binary code, whereas assembly language direct map-ping binary code onto symbolic form easier humans understandand manage. Converting symbolic representation machine language per-formed special program called assembler. assembler program thataccepts symbolic language program (source) produces machine language equivalent (target). translating program binary code, assembler replace symbolic addresses numeric addresses, replace symbolic operationcodes machine operation codes, reserve storage instructions data, andtranslate constants machine representation. purpose chapter give reader general overview assembly language programming. meant manual assemblylanguage speciﬁc architecture. start chapter discussion 37Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.simple hypothetical machine referred throughout chapter. machine ﬁve registers instruction set 10 instructions.We use simple machine deﬁne rather simple assembly language thatwill easy understand help explain main issues assembly pro-gramming. introduce instruction mnemonics syntax assemblerdirectives commands. discussion execution assembly programs isthen presented. conclude chapter showing real-world example assembly language X86 Intel CISC family. 3.1. SIMPLE MACHINE Machine language native language given processor. Since assembly language symbolic form machine language, different type processorhas unique assembly language. study assembly language agiven processor, need ﬁrst understand details processor. need toknow memory size organization, processor registers, instruction format, entire instruction set. section, present simple hypothetical processor, used explaining different topics inassembly language throughout chapter. simple machine accumulator-based processor, ﬁve 16-bit registers: Program Counter (PC), Instruction Register (IR), Address Register(AR), Accumulator (AC), Data Register (DR). PC contains addressof next instruction executed. IR contains operation code portionof instruction executed. AR contains address portion (if any) instruction executed. AC serves implicit source destination data. DR used hold data. memory unit made 4096 words ofstorage. word size 16 bits. processor shown Figure 3.1. CPUALU Figure 3.1 simple machine38 ASSEMBLY LANGUAGE PROGRAMMINGWe assume simple processor supports three types instructions: data transfer, data processing, program control. data transfer operations load, store, move data registers AC DR. data processinginstructions add, subtract, and, not. program control instructions arejump conditional jump. instruction set processor summarized inTable 3.1. instruction size 16 bits, 4 bits operation code 12 bitsfor address (when appropriate). Example 1 Let us write machine language program adds contents memory location 12 (00C-hex), initialized 350 memory location 14 (00E-hex), initialized 96, store result location 16 (010-hex), initialized 0. program given binary instructions Table 3.2. ﬁrst column gives memory location binary instruction operand. second columnTABLE 3.1 Instruction Set Simple Processor Operation code Operand Meaning instruction 0000 Stop execution 0001 adr Load operand memory (location adr) AC 0010 adr Store contents AC memory (location adr) 0011 Copy contents AC DR 0100 Copy contents DR AC 0101 Add DR AC 0110 Subtract DR AC 0111 bitwise DR AC 1000 Complement contents AC 1001 adr Jump instruction address adr 1010 adr Jump instruction adr AC ¼0 TABLE 3.2 Simple Machine Language Program Binary (Example 1) Memory location (bytes) Binary instruction Description 0000 0000 0000 0001 0000 0000 1100 Load contents location 12 AC 0000 0000 0010 0011 0000 0000 0000 Move contents AC DR 0000 0000 0100 0001 0000 0000 1110 Load contents location 14 AC 0000 0000 0110 0101 0000 0000 0000 Add DR AC 0000 0000 1000 0010 0000 0001 0000 Store contents AC location 16 0000 0000 1010 0000 0000 0000 0000 Stop0000 0000 1100 0000 0001 0101 1110 Data value 350 0000 0000 1110 0000 0000 0110 0000 Data value 96 0000 0001 0000 0000 0000 0000 0000 Data value 03.1. SIMPLE MACHINE 39lists contents memory locations. example, contents location 0 instruction opcode: 0001, operand address: 0000 0000 1100. Please note case operations require operand, operand portion instruction shown zeros. program expected stored indicatedmemory locations starting location 0 execution. program bestored different memory locations, addresses instructionsneed updated reﬂect new locations. clear programs written binary code difﬁcult understand and, course, debug. Representing instructions hexadecimal reduce number digits four per instruction. Table 3.3 shows program hexadecimal. 3.2. INSTRUCTION MNEMONICS SYNTAX Assembly language symbolic form machine language. Assembly programs written short abbreviations called mnemonics. mnemonic abbrevi-ation represents actual machine instruction. Assembly language program-ming writing machine instructions mnemonic form, machine instruction (binary hex value) replaced mnemonic. Clearly use mnemonics meaningful hex binary values, whichwould make programming low level easier manageable. assembly program consists sequence assembly statements, statements written one per line. line assembly program split intothe following four ﬁelds: label, operation code (opcode), operand, comments.Figure 3.2 shows four-column format assembly instruction. Labels used provide symbolic names memory addresses. label identiﬁer used program line order branch labeled line. Itcan also used access data using symbolic names. maximum length aTABLE 3.3 Simple Machine Language Program Hexadecimal (Example 1) Memory location (bytes) Hex instruction 000 100C 002 3000 004 100E 006 5000 008 2010 00A 0000 00C 015E 00E 0060010 000040 ASSEMBLY LANGUAGE PROGRAMMINGlabel differs one assembly language another. allow 32 characters length, others may restricted six characters. Assembly languages someprocessors require colon label others not. example, SPARCassembly requires colon every label, Motorola assembly not. TheIntel assembly requires colons code labels data labels. operation code (opcode) ﬁeld contains symbolic abbreviation given operation. operand ﬁeld consists additional information data opcode requires. operand ﬁeld may used specify constant, label, immedi- ate data, register, address. comments ﬁeld provides space documen-tation explain done purpose debugging maintenance. simple processor described previous section, assume label ﬁeld, may empty, six characters. colonrequirement label. Comments preceded “ /”. simple mnemonics ten binary instructions Table 3.1 summarized Table 3.4. Let us consider following assembly instruction: START LD X \ copy contents location X AC label instruction LD X isSTART , means memory address instruction. label used program reference shown following instruction: BRA START \ go statement label STARTLabel (Optional)Operation Code (Required)Operand (Required instructions)Comment (Optional) Figure 3.2 Assembly instruction format TABLE 3.4 Assembly Language Simple Processor Mnemonic Operand Meaning instruction STOP Stop execution LD x Load operand memory (location x) AC ST x Store contents AC memory (location x) MOVAC Copy contents AC DRMOV Copy contents DR AC ADD Add DR AC SUB Subtract DR ACAND bitwise DR AC Complement contents AC BRA adr Jump instruction address adr BZ adr Jump instruction adrif AC¼03.2. INSTRUCTION MNEMONICS SYNTAX 41The jump instruction make processor jump memory address associ- ated label START, thus executing instruction LD X immediatelyafter BRA instruction. addition program instructions, assembly program may also include pseudo instructions assembler directives. Assembler directives commandsthat understood assembler correspond actual machineinstructions. example, assembler asked allocate memory storage. assembly language simple processor, assume use pseudo instruction W reserve word (16 bits) memory. example,the following pseudo instruction reserves word label X initializingit decimal value 350: X W 350 \ reserve word initialized 350Again, label pseudo instruction W 350 X, means memory address value. following assembly code machine language program Example 1 previous section. LD X \ AC X MOVAC \ DR AC LD \ AC ADD \ AC ACþDR ST Z \ Z AC STOP X W 350 \ reserve word initialized 350Y W 96 \ reserve word initialized 96Z W 0 \ result stored Example 2 example, write assembly program perform multiplication operation: Z X /C3Y, X, Y, Z memory locations. know, assembly simple CPU multiplication operation. compute product applying add operation multiple times. order add X times, use N counter initialized X decremented one addition step. BZ instruction beused test case N reaches 0. use memory location tostore N need loaded AC BZ instruction executed. also use memory location ONE store constant 1. Memory location Z partial products eventually ﬁnal result. following assembly program using assembly language simple processor. assume values X small enough allow product stored single word. thesake example, let us assume X initialized 5 15,respectively.42 ASSEMBLY LANGUAGE PROGRAMMINGLD X \ Load X AC ST N \ Store AC (X original value) N LOOP LD N \ AC N BZ EXIT \ Go EXIT AC ¼0 (N reached 0) LD ONE \ AC 1 MOVAC \ DR AC LD N \ AC N SUB \ subtract 1 N ST N \ store decrements N LD \ AC MOVAC \ DR AC LD Z \ AC Z (partial product) ADD \ Add Z ST Z \ store new value ZBRA LOOP EXIT STOPX W 5 \ reserve word initialized 5 W 15 \ reserve word initialized 15 Z W 0 \ reserve word initialized 0ONE W 1 \ reserve word initialized 1 N W 0 \ reserve word initialized 0 3.3. ASSEMBLER DIRECTIVES COMMANDS previous section, introduced reader assembly machine languages. provided several assembly code segments written using simple machine model. writing assembly language programs speciﬁc architecture, numberof practical issues need considered. Among issues following: .Assembler directives .Use symbols .Use synthetic operations .Assembler syntax .Interaction operating system use assembler directives , also called pseudo-operations , important issue writing assembly language programs. Assembler directives commands understood assembler correspond actual machine instruc-tions. Assembler directives affect way assembler performs conversionof assembly code machine code. example, special assembler directives canbe used instruct assembler place data items proper align- ment. Alignment data memory required efﬁcient implementation archi- tectures. proper alignment data, data n-bytes width must stored an3.3. ASSEMBLER DIRECTIVES COMMANDS 43address divisible n, example, word two-byte width stored locations addresses divisible two. assembly language programs symbols used represent numbers, example, immediate data. done make code easier read, understand, debug. Symbols translated corresponding numerical values theassembler. use synthetic operations helps assembly programmers use instructions directly supported architecture. translated theassembler set instructions deﬁned architecture. example, assem-blers allow use (a synthetic) increment instruction architectures forwhich increment instruction deﬁned use instruc-tions add instruction. Assemblers usually impose conventions referring hardware com- ponents registers memory locations. One convention preﬁx-ing immediate values special characters (#) register name character (%). underlying hardware machines cannot accessed directly pro- gram. operating system (OS) plays role mediating access resources memory /O facilities. Interactions operating systems (OS) take place form code causes execution function ispart OS. functions called system calls. 3.4. ASSEMBLY EXECUTION PROGRAMS know now, program written assembly language needs trans- lated binary machine language executed. section, wewill learn get point writing assembly program theexecution phase. Figure 3.3 shows three steps assembly execution pro-cess. assembler reads source program assembly language generatesthe object program binary form. object program passed linker. linker check object ﬁle calls procedures link library. linker combine required procedures link library object programand produce executable program. loader loads executable program intomemory branches CPU starting address. program beginsexecution. Figure 3.3 Assembly execution process44 ASSEMBLY LANGUAGE PROGRAMMING3.4.1. Assemblers Assemblers programs generate machine code instructions source code program written assembly language. assembler replace symbolic addresses numeric addresses, replace symbolic operation codes machine oper- ation codes, reserve storage instructions data, translate constants intomachine representation. functions assembler performed scanning assembly pro- gram mapping instructions machine code equivalent. Since symbolscan used instructions deﬁned later ones, single scanning ofthe program might enough perform mapping. simple assembler scansthe entire assembly program twice, scan called pass. ﬁrst pass, generates table includes symbols binary values. table called symbol table. second pass, assembler use symboltable tables generate object program, output informationthat needed linker. 3.4.2. Data Structures assembler uses least three tables perform functions: symbol table, opcode table, pseudo instruction table. symbol table, generatedin pass one, entry every symbol program. Associated symbol binary value information. Table 3.5 shows symbol table multiplication program segment Example 2. assume instruction LD X starting location 0 memory. Since instructiontakes two bytes, value symbol LOOP 4 (004 hexadecimal).Symbol N, example, stored decimal location 40 (028 hexadecimal).The values symbols obtained similar way. opcode table provides information operation codes. Associated symbolic opcode table numerical value information type, instruction length, operands. Table 3.6 shows opcode TABLE 3.5 Symbol Table Multiplication Segment (Example 2) SymbolValue (hexadecimal)Other information Loop 004 EXIT 01E X 020 022Z 024 ONE 026 N 0283.4. ASSEMBLY EXECUTION PROGRAMS 45table simple processor described Section 3.1. example, explain information associated opcode LD. one operand, memory address binary value 0001. instruction length LD 2 bytes type memory-reference. entries pseudo instruction table pseudo instructions symbols. entry refers assembler procedure processes pseudo instructionwhen encountered program. example, END encountered, trans-lation process terminated. order keep track instruction locations, assembler maintains vari- able called instruction location counter (ILC). ILC contains value memory location assigned instruction operand processed. ILC initialized 0 incremented processing instruction. ILC isincremented length instruction processed, number ofbytes allocated result data allocation pseudo instruction. Figures 3.4 3.5 show simpliﬁed ﬂowcharts pass one pass two two- pass assembler. Remember main function pass one build symboltable pass two’s main function generate object code. 3.4.3. Linker Loader linker entity combine object modules may resulted assembling multiple assembly modules separately. loader operatingsystem utility reads executable memory start execution. summary, assembly modules translated object modules, func- tions linker loader prepare program execution. functions include combining object modules together, resolving addresses unknown assem- bly time, allocating storage, ﬁnally executing program.TABLE 3.6 Opcode Table Assembly Simple Processor Opcode OperandOpcode value (binary)Instruction length (bytes) Instruction type STOP — 0000 2 Control LD Mem-adr 0001 2 Memory-reference ST Mem-adr 0010 2 Memory-reference MOVAC — 0011 2 Register-reference MOV — 0100 2 Register-reference ADD — 0101 2 Register-reference SUB — 0110 2 Register-reference — 0111 2 Register-reference — 1000 2 Register-reference BRA Mem-adr 1001 2 Control BZ Mem-adr 1010 2 Control46 ASSEMBLY LANGUAGE PROGRAMMING3.5. EXAMPLE: X86 FAMILY section, discuss assembly language features use X86 family. present basic organizational features system, basic programming model, addressing modes, sample different instruction types used, andStart ENDProcess next instruction Stop NoYes Opcode Lookup Symbol Table Lookup Generate machine code Figure 3.5 Simpliﬁed pass two two-pass assemblerStart END LabelILC← 0 Process next instruction Increment ILCAdd Symbol Table value = ILC Pass 2 NoYes Yes Figure 3.4 Simpliﬁed pass one two-pass assembler3.5. EXAMPLE: X86FAMILY 47ﬁnally examples showing use assembly language system programming sample real-life problems. late 1970s, Intel introduced 8086 ﬁrst 16-bit microprocessor. processor 16-bit external bus. 8086 evolved series fasterand powerful processors starting 80286 ending Pentium. latter introduced 1993. Intel family processors usually calledtheX86family. Table 3.7 summarizes main features main members family. Intel Pentium processor three million transistors compu- tational power ranges two ﬁve times predecessor processor,the 80486. number new features introduced Pentium processor, among incorporation dual-pipelined superscalar architecturecapable processing one instruction per clock cycle. basic programming model 386, 486, Pentium shown Figure 3.6. consists three register groups. general purpose regis- ters, segment registers, instruction pointer (program counter) ﬂag register. ﬁrst set consists general purpose registers A, B, C, D, SI (sourceindex), DI (destination index), SP (stack pointer), BP (base pointer). shouldbe noted naming registers, used Xto indicate eXtended. second set registers consists CS (code segment), SS (stack segment), andfour data segment registers DS, ES, FS, GS. third set registers consistsof instruction pointer (program counter) ﬂags (status) register. latter shown Figure 3.7. Among status bits shown Figure 3.7, ﬁrst ﬁve identical bits introduced early 8085 8-bit microproces- sor. next 6–11 bits identical introduced 8086. ﬂags thebits 12–14 introduced 80286 16–17 bits introduced inthe 80386. ﬂag bit 18 introduced 80486. Table 3.8 shows mean-ing ﬂags. X86family instruction perform operation one two oper- ands. two-operand instructions, second operand immediate data TABLE 3.7 Main Features Intel X86Microprocessor Family Feature 8086 286 386 486 Pentium Date introduced 1978 1982 1985 1991 1993 Data bus 8 bits 16 bits 32 bits 32 bits 64 bitsAddress bus 20 bits 24 bits 32 bits 32 bits 32 bitsOperating speed 5,8,10 MHz 6,8,10, 12.5, 16, 20 MHz16, 20,25, 33, 40, 50 MHz25, 33, 50 MHz50, 60, 66, 100 MHz Instruction cache sizeNA NA 16 bytes 32 bytes 8 Kbytes Data cache size NA NA 256 bytes 8 Kbytes 8 KbytesPhysical memory 1 Mbytes 16 Mbytes 4 Gbytes 4 Gbytes 4 GbytesData word size 16 bits 16 bits 16 bits 32 bits 32 bits48 ASSEMBLY LANGUAGE PROGRAMMINGFigure 3.6 base register sets X86programming model 31 18 17 16 15 14 13 12 11 10 9 8 1 Reserved AG VM RF 0 NT IOPL6543 2 0 ODI Z P C7 Figure 3.7 TheX86ﬂag register TABLE 3.8 X86Status Flags Flag Meaning Processor Flag Meaning Processor C Carry P Parity Auxiliary Z Zero Sign Trap Interrupt Direction Overﬂow IOPL /O privilege level 286 NT Nested task 286 RF Resume 386 VM Virtual mode 386 AC Alignment check 4863.5. EXAMPLE: X86FAMILY 492’s complement format. Data transfer, arithmetic logical instructions act immediate data, registers, memory locations. X86family, direct indirect memory addressing used. direct addressing, displacement address consisting 8-, 16-, 32-bit word used asthe logical address. logical address added shifted contents seg-ment register (segment base address) give physical memory address. Figure 3.8illustrates direct addressing process. Address indirection X86 family obtained using content base pointer register (BPR), content index register, sum baseregister index register. Figure 3.9 illustrates indirect addressing using BPR. TheX86family processors deﬁnes number instruction types. Using naming convention introduced before, instruction types data movement, arithmetic logic, sequencing (control transfer). addition, X86 family deﬁnes instruction types string manipulation, bit manipulation,and high-level language support. Data movement instructions X86 family include mainly four subtypes. general-purpose, accumulator-speciﬁc, address-object, ﬂag instructions. sample instructions shown Table 3.9. Arithmetic logic instructions X86 family include mainly ﬁve subtypes. addition, subtraction, multiplication, division, logic instructions. sample arithmetic instructions shown Table 3.10. Logic instructions include typical ,OR,NOT,XOR, TEST. latter performs logic compare source destination sets ﬂags accord- ingly. addition, X86family set shift rotate instructions. sample shown Table 3.11.Original 16 bits 7/15/31 15 0 Op Code Address Segment base Address Σ Physical AddressDisplacement from2 nd word instructionShifted 16 bits Logical Address0 Figure 3.8 Direct addressing X86family50 ASSEMBLY LANGUAGE PROGRAMMINGControl transfer instructions X86 family include mainly four subtypes. conditional, iteration, interrupt, unconditional. sample instructions shown Table 3.12. Processor control instructions X86 family include mainly three subtypes. external synchronization, ﬂag manipulation, general control instruc- tions. sample instructions shown Table 3.13. introduced basic features instruction set X86processor family, move present number programming examples showShifted 16 bits 2ndword instruction Original 16 bits 7/15/31 0 15 0 Op Code Address Logical AddressSegment base Address Physical Address BPRå åDisplacement Figure 3.9 Indirect addressing using BPR X86family TABLE 3.9 Sample X86Data Movement Instructions Mnemonic Operation Subtype MOV Move source destination General purpose POP Pop source stack General purpose POPA Pop General purpose PUSH Push source onto stack General purpose PUSHA Push General purpose XCHG Exchange source destination General purpose Input accumulator Accumulator Output accumulator Accumulator XLAT Table lookup translate byte Accumulator LEA Load effective address register Address-object LMSW Load machine status word Address-object SMSW Store machine status word Address-object POPF Pop ﬂags stack Flag PUSHF Push ﬂags onto stack Flag3.5. EXAMPLE: X86FAMILY 51how instruction set used. examples presented presented end Chapter 2. Example 3 Adding 100 numbers stored consecutive memory locations starting location 1000, results stored memory location 2000. LIST isTABLE 3.10 Sample X86Arithmetic Instructions Mnemonic Operation Subtype ADD Add source destination Addition ADC Add source destination carry Addition INC Increment operand 1 Addition SUB Subtract source destination Subtraction SBB Subtract source destination borrow Subtraction DEC Decrement operand 1 Subtraction MUL Unsigned multiply source destination Multiply IMUL Signed multiply source destination Multiply DIV Unsigned division accumulator source Division IDIV Signed division accumulator source Division TABLE 3.11 Sample X86Shift Rotate Instructions Mnemonic Operation ROR Rotate right ROL Rotate left RCL Rotate left carry RCR Rotate right carry SAR Arithmetic shift right SAL Arithmetic shift left SHR Logic shift right SHL Logic shift left TABLE 3.12 Sample X86Control Transfer Instructions Mnemonic Operation Subtype SET Set byte true false based condition Conditional JS Jump sign Conditional LOOP Loop CX equal zero Iteration LOOPE Loop CX equal zero & ZF ¼1 Iteration INT Interrupt Interrupt IRET Interrupt return Interrupt JMP Jump unconditional Unconditional RET Return procedure Unconditional52 ASSEMBLY LANGUAGE PROGRAMMINGdeﬁned array Nelements size byte. FLAG memory variable used indicate whether list sorted not. register CXis used coun- ter Loop instruction. Loop instruction decrements CXregister branch result zero. addressing mode used access array List [BXþ1] called based addressing mode. noted since using BX BXþ1 CXcounter loaded value 999 order exceed list. MOV CX, 1000 21 ; Counter ¼CX (1000 21) MOV BX, Offset LIST ; BX pointer LIST CALL SORT ...... SORT PROC NEAR Again: MOV FLAG, 0 ; FLAG 0 Next: MOV AL, [BX] CMP AL, [BXþ1] ;Compare current next values JLE Skip ;Branch current ,next values XCHG AL, [BXþ1] ;If not, Swap contents MOV [BXþ1], AL ;current location next one MOV FLAG, 1 ;Indicate swap Skip: INC BX ; BX BXþ1 LOOP Next ;Go next value CMP FLAG, 1 ;Was swapJE ;If yes Repeat processRET SORT ENDP Example 4 implement SEARCH algorithm 8086 instruction set.LIST deﬁned array Nelements size word. FLAG memory variable used indicate whether list sorted not. register CXis used counter loop instruction. Loop instruction decrementsTABLE 3.13 Sample X86Processor Control Instructions Mnemonic Operation Subtype HLT Halt External sync LOCK Lock bus External sync CLC Clear carry ﬂag Flag CLI Clear interrupt ﬂag Flag STI Set interrupt ﬂag Flag INVD Invalidate data cache General control3.5. EXAMPLE: X86FAMILY 53theCXregister branch result zero. addressing mode used access array List [BX þ1] called based addressing mode. MOV CX, 1000 ; Counter ¼CX 1000 MOV BX, Offset LIST;B X pointer LIST MOV SI, 0 ; SI used index MOV AX, VAL ; AX VAL CALL SEARCH ; Test FLAG check whether value found ...... SEARCH PROC NEAR MOV FLAG, 0 ; FLAG 0 Next: CMP AX, [BX þSI] ;Compare current value VAL JE Found ;Branch equal ADD SI, 2 ; SI SIþ2, next value LOOP Next ;Go next valueJMP Not_Found Found: MOV FLAG, 1 ;Indicate value found MOV POSITION, SI ;Return index value List Not_Found: RET SEARCH ENDP Example 5 Example 4 using stack features X86. PUSH DS ;See Table 3.9 MOV CX, 1000 ;Counter ¼CX 1000 MOV BX, OFFSET LIST ;Point beginning LISTPUSH BXPUSH VAL ;VAL word variableCALL SEARCH ;Test FLAG check whether value found ;If found get index SI register using POP SI ...... SEARCH PROC NEAR POP TEMP ;Save IPPOP AX ;AX VAL. Value search POP SI ;SI OFFSET LIST let BX ¼SI54 ASSEMBLY LANGUAGE PROGRAMMINGPOP ES ;Make ES ¼DS (See Table) CLD ;Set auto-increment mode REPNE SCASW ;Scan LIST value AX found; increment SI 2, decrement CX if; zeroscan next location LIST. ;If occurrence found Zero ﬂag set JNZ Not_Found ;If value branch Not_Found? MOV FLAG, 1 ;Yes SUB SI, BX PUSH SI ;Save position Not_Found: PUSH TEMP ;Restore IP RET SEARCH ENDP noted example call procedure initiated, IPregister last value pushed top stack. Therefore, care made avoid altering value IPregister. top stack thus saved temporary variable TEMP procedure entry restored exit. 3.6. SUMMARY machine language collection machine instructions represented 0s 1s. Assembly language provides easier use symbolic representation, analphanumeric equivalent machine language used. one-to-one corre-spondence assembly language statements machine instructions. Anassembler program accepts symbolic language program (source program)and produces machine language equivalent (target program). Although assemblylanguage programming difﬁcult compared programming high-level languages, still important learn assembly. applications, small por- tions program heavily used may need written assemblylanguage. Programming assembly result machine code smallerand faster generated compiler high-level language. Assembly pro-grammers access hardware features target machine mightnot accessible high-level language programmers. addition, learning assem-bly languages great help understanding low level details computerorganization architecture. chapter provided general overview assembly language programming. programmer view X86 Intel microprocessor family processors also introduced real-world example. Examples presented showing use X86instruction set writing sample programs similar presented Chapter 2.3.6. SUMMARY 55EXERCISES 1. difference following pairs? .Compilers assemblers .Source code target code .Mnemonics hexadecimal representation .Pseudo instructions instructions .Labels addresses .Symbol table opcode table .Program counter (PC) instruction location counter (ILC) 2. Using assembly language simple processor Section 3.1, write assembly code segments following operation: .Swap two numbers .Logical .Negation 3. Add input /output instructions instruction set simple processor Section 3.1 write assembly program ﬁnd Fibonacci sequence. 4. Obtain machine language code multiplication assembly program given Section 3.2. 5. great advances high-level languages compilers, people argue assembly language important anymore. Give argu- ments view. 6. Write program segment using instruction X86 family computeP200 i¼1XiYi, XiandYiare signed 8-bit numbers. Assume over- ﬂow occur. 7. Write subroutine using X86instructions called main program different code segment. subroutine multiply signed 16-bit number CX signed 8-bit number AL. main programwill call subroutine, store result two consecutive memory words, stop. Assume SI DI contain signed 8-bit 16-bit numbers, respectively. 8. Write program using X86instructions compare source string 100 words pointed offset 2000H DS destination stringpointed offset 4000H DS. 9. Write program using X86instructions generate ﬁrst 10 numbers Fibonacci series, is, generate series 1, 1, 2, 3, 5, 8, 13, 21, 34. 10. Write program using X86instructions convert word text upper case lower case. Assume word consists ASCII charactersstored successive memory locations starting location START ending location FINISH.56 ASSEMBLY LANGUAGE PROGRAMMINGREFERENCES READING C. M. Gilmore, Microprocessors: Principles Applications , 2nd ed., McGraw-Hill, New York, 1996. V. C. Hamacher, Z. G. Vranesic S. G. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002. J. P. Hayes, Computer Architecture Organization , McGraw-Hill, New York, 1998. V. Heuring, H. Jordan, Computer Systems Design Architecture , Addison Wesley, NJ, 1997. K. R. Irvine, Assembly Language Intel-Based Computers , 4th ed., Prentice Hall, NJ, 2003. A. D. Patterson J. L. Hennessy, Computer Organization & Design; Hardware / Software Interface , Morgan Kaufmann, San Mafeo, 1994. W. Stallings, Computer Organization Architectures: Designing Performance , 4th ed., Prentice-Hall, NJ, U.S.A, 1996. A. Tanenbaum, Structured Computer Organization , 4th ed., Prentice Hall, Paramus, NJ, U.S.A, 1999. J. Uffenbeck, 80/C286 Family, Design, Programming, Interfacing , 3rd ed., Prentice Hall, Essex, UK, 2002. B. Wilkinson, Computer Architecture: Design Performance , 2nd ed., Prentice-Hall, Hertfordshire, UK, 1996.REFERENCES READING 57& CHAPTER 4 Computer Arithmetic chapter dedicated discussion computer arithmetic. goal introduce reader fundamental issues related arithmetic operationsand circuits used support computation computers. coverage starts withan introduction number systems. particular, introduce issues asnumber representations base conversion. followed discussion oninteger arithmetic. regard, introduce number algorithms togetherwith hardware schemes used performing integer addition, subtraction,multiplication, division. end chapter discussion ﬂoating- point arithmetic. particular, introduce issues ﬂoating-point represen- tation, ﬂoating-point operations, ﬂoating-point hardware schemes. IEEEﬂoating-point standard last topic discussed chapter. 4.1. NUMBER SYSTEMS number system uses speciﬁc radix (base). Radices power 2 widely used digital systems. radices include binary (base 2), quaternary (base 4), octagonal (base 8), hexagonal (base 16). base 2 binary system dominant computer systems. unsigned integer number Acan represented using ndigits base b: A¼(a n/C01an/C02...a2a1a0)b. representation (called positional representation ) digit aiis given 0/C20ai/C20(b/C01). Using positional representation, dec- imal value unsigned integer number Ais given A¼Pn/C01 i¼0ai/C2bi. Con- sider, example, positional representation decimal number A¼106. Using 8 digits base 2, Ais represented A¼0/C227þ1/C226þ1/C225þ 0/C224þ1/C223þ0/C222þ1/C221þ0/C220. Using ndigits, largest value unsigned number Ais given Amax¼bn/C01. example, largest unsigned number obtained using 4 digits base 2 24/C01¼15. case, decimal numbers ranging 0 15 (corresponding binary 0000 1111) represented. Similarly,the largest unsigned number obtained using 4 digits base 4 59Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.44/C01¼255. case, decimal numbers ranging 0 255 (corresponding 0000 3333) represented. Consider use ndigits represent real number Xin radix bsuch signiﬁcant kdigits represent integral part least signiﬁcant mdigits represents fraction part. value Xis given X¼Xk/C01 i¼/C0mxi/C2bi¼xk/C01bk/C01þxk/C02bk/C02þ/C1/C1/C1þ x1b1 þx0b0þx/C01b/C01þ/C1/C1/C1þ x/C0mb/C0m Consider, example, representation real number X¼25.375. number represented binary using k¼5 m¼3 follows. X¼X4 i¼/C03xi/C2bi¼x4/C224þx3/C223þx2/C222þx1/C221 þx0/C220þx/C01/C22/C01þx/C02/C22/C02þx/C032/C03 x4¼1,x3¼1,x2¼0,x1¼0,x0¼1,x/C01¼0,x/C02¼1, x/C03¼1. often necessary convert representation number given base another, example, base 2 base 10. achieved using number methods (algorithms). important tool algorithms div-ision algorithm. basis division algorithm representing integerain terms another integer cusing base b. basic relation used a¼c/C2qþr, qis quotient ris remainder, 0/C20r/C20b/C01 q¼ba=cc. Radix conversion discussed below. 4.1.1. Radix Conversion Algorithm radix conversion algorithm used convert number representation given radix, r 1, another representation different radix, r2. Consider conversion integral part number X,Xint. integral part Xintcan expressed Xint¼½ /C1 /C1 /C1 (xk/C01r2þxk/C02)r2þ/C1/C1/C1þ x2)r2þx1/C138/C8/C9 r2þx0: Dividing Xintbyr2will result quotient Xq¼{½/C1/C1/C1(xk/C01r2þxk/C02)r2þ/C1/C1/C1þ x2)r2þx1/C138} remainder Xrem¼x0. Repeating division process quo- tient retaining remainders required digits zero quotient obtained result required representation Xintin new radix r2. Using similar argument, possible show repeated multiplication fractional part X(Xf)b r2retaining obtained integers required digits, result required representation fractional part new radix, r2. should, however, noted unlike integral part conversion, the60 COMPUTER ARITHMETICfractional part conversion may terminate ﬁnite number repeated mul- tiplications. Therefore, process may terminated number ofsteps, thus leading acceptable approximation. Example Consider conversion decimal number 67.575 binary. r 1¼10,r2¼2,Xint¼67, Xf¼0.575. integral part Xint, repeated division 2 result following quotients remainders: Quotient 33 16 8 4 2 1 0 Remainder 1 1 0 0 0 0 1 Therefore integral part radix r2¼2i sXint¼(1000011). similar method used obtain fractional part (through repeated multiplication): Fractional part 0.150 0.300 0.600 0.200 0.400 0.800 0.600 0.200 ... Carry bit 1 0 0 1 0 0 1 1 ... fractional part Xf¼(.10010011 ...). Therefore, resultant representation number 67.575 binary given (1000011.10010011 ...). 4.1.2. Negative Integer Representation exist number methods representation negative integers. include sign-magnitude ,radix complement , diminished radix complement. brieﬂy explained below. 4.1.3. Sign-Magnitude According representation, signiﬁcant bit (of nbits used represent number) used represent sign number “1” signiﬁcant bit position indicates negative number “0” themost signiﬁcant bit position indicates positive number. remaining ( n/C01) bits used represent magnitude number. example, negativenumber ( 218) represented using 6 bits, base 2 sign-magnitude format, follows (110010), ( þ18) represented (010010). Although simple, sign-magnitude representation complicated performing arithmetic opera-tions. particular, sign bit dealt separately magnitudebits. Consider, example, addition two numbers þ18 (010010) 219 (110011) using sign-magnitude representation. Since two numbers carrydifferent signs, result carry sign larger number magnitude, case ( 219). remaining 5-bit numbers subtracted (10011210010) produce (00001), is, ( 21).4.1. NUMBER SYSTEMS 614.1.4. Radix Complement According system, positive number represented way sign-magnitude. However, negative number represented using b’s comp- lement (for base b numbers). Consider, example, representation number ( 219) using 2’s complement. case, number 19 ﬁrst represented (010011). digit complemented, hence name radix complement toproduce (101100). Finally “1” added least signiﬁcant bit position resultin (101101). Now, consider 2’s complement representation number ( þ18). Since number positive, represented (010010), inthe sign-magnitude case. Now, consider addition two numbers. thiscase, add corresponding bits without giving special treatment sign bit. results adding two numbers produces (111111). 2’s comp- lement representation ( 21), expected. main advantage 2’s comp- lement representation special treatment needed sign thenumbers. Another characteristic 2’s complement fact carrycoming signiﬁcant bit performing arithmetic operations isignored without affecting correctness result. Consider, example,adding 219 (101101) andþ26 (011010). result (1)(000111), correct (þ7) carry bit ignored. 4.1.5. Diminished Radix Complement representation similar radix complement except fact “1” added least signiﬁcant bit complementing digits number, asis done radix complement. According number system representation, (219) represented (101100), ( þ18) represented (010010). add two numbers obtain (111110), 1’s complement ( 21). main disadvantage diminished radix representation need correction factor whenever carry obtained signiﬁcant bit performingarithmetic operations. Consider, example, adding 23 (111100) toþ18 (010010) TABLE 4.1 2’s 1’s Complement Representation 8-Bit Number Number Representation Example 2’s Complement x¼0 0 0 (00000000) 0,x,256 x 77 (01001101) 2128/C20x,0 256 2jxj 256 (11001000) 1’s Complement x¼0 0 255 (11111111) 0,x,256 x 77 (01001101) 2127/C20x,0 255 2jxj 256 (11000111)62 COMPUTER ARITHMETICto obtain (1)(001110). carry bit added least signiﬁcant bit result, obtain (001111), is, ( þ15), correct result. Table 4.1 shows comparison 2’s complement 1’s comp- lement representation 8-bit number, x. 4.2. INTEGER ARITHMETIC section, introduce number techniques used perform integer arith- metic using radix complement representation numbers. discussion willfocus base “2” binary representation. 4.2.1. Two’s Complement (2’s) Representation order represent number 2’s complement, perform following two steps. 1. Perform Boolean complement bit (including sign bit); 2. Add 1 least signiﬁcant bit (treating number unsigned binary integer), is,/C0A¼ Aþ1 Example Consider representation ( 222) using 2’s complement. 22¼00010110 + 11101001 (1’s complement) þ 1 11101010 (/C022) 4.2.2. Two’s Complement Arithmetic Addition Addition two n-bit numbers 2’s complement performed using n-bit adder. carry-out bit ignored without affecting correct- ness results, long results addition range /C02n/C01toþ2n/C01/C01. Example Consider addition two 2’s complement numbers ( 27) (þ4). addition carried ( 27)þ(þ4)¼23, is, 1001þ (0100) ¼1101, ( 23) 2’s complement. condition result range /C02n/C01toþ2n/C01/C01i important result outside range lead overﬂow hence awrong result. simple terms, overﬂow occur result produced a4.2. INTEGER ARITHMETIC 63given operation outside range representable numbers. Consider fol- lowing two examples. Example Consider adding two 2’s complement numbers ( þ7) (þ6). addition done ( þ7)þ(þ6)¼þ13, is, 0111þ(0110) ¼1101, wrong result. result exceeds largest value ( þ7). Example Consider adding two 2’s complement numbers ( 27) ( 24). addition done ( 27)þ(24)¼211, is, 1001þ(1100)¼0101, wrong result. result less smallest value ( 28). Notice original numbers negative result positive. two examples, make following observation: two numbers (both positive negative) added, overﬂow detected result opposite sign added numbers. Subtraction 2’s complement, subtraction performed way addition performed. example, perform B/C0A¼Bþ /C22A Aþ1, is, sub- tracting Afrom Bis addition complement AtoB. Example Consider subtraction 2 27¼25. performed 2þ7¯þ1¼0010þ1000þ0001¼1011 (25). noted earlier observation occurrence overﬂow context addition applies case subtraction well. sub- traction addition complement. Consider following illustrative example. Example Consider subtraction 7 2(27)¼14. performed 7þ7¯þ1¼0111þ1000þ0001¼(1) 0000, wrong answer (result .7). Hardware Structures Addition Subtraction Signed Numbers addition two n-bit numbers AandBrequires basic hardware circuit accepts three inputs, is, ai,bi, ci/C01. three bits represent respectively two current bits numbers AandB(at position i) carry bit previous bit position (at position i21). circuit pro- duce two outputs, is, siandcirepresenting respectively sum carry, according following truth-table. ai 00001111 bi 00110011 ci21 01010101 si 01101001 ci 0001011164 COMPUTER ARITHMETICThe output logic functions given si¼ai/C8bi/C8ci/C01and ci¼aibiþ aici/C01þbici/C01. circuit used implement two functions called full-adder (FA) shown Figure 4.1. Addition two n-bit numbers AandBcan carried using nconsecutive FAs arrangement known carry-ripple adder (CRT) , see Figure 4.2. n-bit CRT adder shown Figure 4.2 used add 2’s complement numbers AandBin bn/C01andan/C01represent sign bits. cir- cuit used perform subtraction using relation B/C0A¼Bþ /C22A Aþ1. Figure 4.3 shows structure binary addition /subtraction logic network. ﬁgure, two inputs B represent arguments added /sub- tracted. control input determines whether add subtract operation performed control input 0 add operation performed whileif control input 1 subtract operation performed. simple circuit thatcan implement Add /Sub block Figure 4.3 shown Figure 4.4 case 4-bit inputs. One main drawbacks CRT circuit expected long delay time inputs presented circuit ﬁnal output obtained. Thisis dependence stage carry output produced pre-vious stage. chain dependence makes CRT adder’s delay O(n), nis number stages adder. order speed addition process, isnecessary introduce addition circuits chain dependence amongthe adder stages must broken. number fast addition circuits exist lit-erature. Among carry-look-ahead (CLA) adder well known. CLA adder introduced below. Consider CRT adder circuit. two logic functions realized i¼ ai/C8bi/C8ci/C01andci¼aibiþaici/C01þbici/C01. two functions rewritten terms two new subfunctions, carry generate ,Gi¼aibiand carry pro- pagate ,Pi¼ai/C8bi. Using two new subfunctions, rewrite logic equation carry output stage ci¼GiþPici/C01. Now, write Figure 4.1 full-adder (FA) circuit4.2. INTEGER ARITHMETIC 65the series carry outputs different stages follows. c0¼G0þP0c/C01 c1¼G1þP1c0¼G1þP1(G0þP0c/C01)¼G1þG0P1þP0P1c/C01 c2¼G2þP2c1¼G2þP2(G1þG0P1þP0P1c/C01) ¼G2þG0P1P2þG1P2þP0P1P2c/C01 c3¼G3þP3c2¼G3þP3(G2þG1P2þG0P1P2þP0P1P2c/C01) ¼G3þG2P3þG1P2P3þG0P1P2P3þP0P1P2P3c/C01 ... sequence carry outputs shows total independence among different car- ries (broken carry chain). Figure 4.5 shows overall architecture 4-bit CLA adder. basically three blocks CLA. ﬁrst one used generate theG Pis, second used create carry output. third block used generate sum outputs. Regardless number bits CLA, delay ﬁrst block equivalent one gate delay, delay second block equivalent two gate delay delaythrough third block equivalent one gate delay. Figure 4.5, showthe generation carry sum outputs. reader encouraged completethe design (see Chapter Exercises). Figure 4.2 n-Bit carry-ripple (CRT) adder CA B Control = 0/1 Add/Sub Figure 4.3 Addition /subtraction logic network66 COMPUTER ARITHMETICControl = 0/1 Input Argument Figure 4.4 Add /Sub circuit b3 P3 G3P2 G2 P1 P0 G1 G0 c-1a3 b2a2 b1a1 b0a0 AND-OR circuit realizing c3AND-OR circuit realizing c2 c3 P3 s3 s2 s1 s0P2 P1 P0c2 c1 c0 Figure 4.5 4-bit CLA adder structure4.2. INTEGER ARITHMETIC 67Multiplication discussing multiplication, shall assume two input arguments multiplier Qgiven Q¼qn/C01qn/C02/C1/C1/C1q1q0and multiplicand Mgiven M¼mn/C01mm/C02/C1/C1/C1m1m0. number methods exist performing multiplication. methods discussed below. Paper Pencil Method (for Unsigned Numbers) simplest method performing multiplication two unsigned numbers. method illus- trated example shown below. Example Consider multiplication two unsigned numbers 14 10. process shown using binary representation two numbers. 1110 (14) Multiplicand( M) 1010 (10) Multiplier(Q) 0000 (Partial Product) 1110 (Partial Product) 0000 (Partial Product) 1110 (Partial Product) ¼¼¼¼¼¼ 10001100 (140) Final Product(P) multiplication performed using array cells consisting FA AND. cell computes given partial product. Figure 4.6 shows basic cell example array 4 /C24 multiplier array. characterizes method need adding npartial products regard- less values multiplier bits. noted given bit themultiplier 0, need computing corresponding partial product. following method makes use observation. Add-Shift Method case, multiplication performed series ( n) conditional addition shift operations given bit multiplier 0 shift operation performed, given bit multiplier 1 addition partial products shift operation performed. follow-ing example illustrates method. Example Consider multiplication two unsigned numbers 11 13. process shown tabular form. process, Ais 4-bit register initialized 0s C carry bit signiﬁcant bit position. process repeated n¼4 times (the number bits multiplier Q). bit multiplier “1”, AþMand concatenation AQis shifted one bit position right. If, hand, bit “0”, onlya shift operation performed AQ. structure required perform an68 COMPUTER ARITHMETICFigure 4.6 Array multiplier unsigned binary numbers Figure 4.7 Hardware structure add-shift multiplicationMC Q 1011 0 0000 1101 Initial values 1011 0 1011 1101 Add First cycle 1011 0 0101 1110 Shift 1011 0 0010 1111 Shift Second cycle 1011 0 1101 1111 Add Third cycle 1011 0 0110 1111 Shift 1011 1 0001 1111 Add Fourth cycle 1011 0 1000 1111 Shift4.2. INTEGER ARITHMETIC 69operation shown Figure 4.7. ﬁgure, control logic used determine operation performed depending least signiﬁcant bit Q.A n n-bit adder used add contents registers AandM. order speed multiplication operation, number techniques used. techniques based observation larger thenumber consecutive zeros ones, fewer partial products gen-erated. group consecutive zeros multiplier requires generation new partial product. group kconsecutive ones multiplier requires gener- ation fewer knew partial products. One technique makes use observation Booth’s algorithm. discuss 2-bit Booth’s algorithm. Booth’s Algorithm technique, two bits multiplier, Q(i)Q(i21), (0/C20i/C20n/C01), inspected time. action taken depends binary values two bits, two values respectively 01, AþM; two values 10, A/C0M. action needed values 00 11. four cases, arithmetic shift right oper-ation concatenation AQis performed. whole process repeated n times ( nis number bits multiplier). Booth’s algorithm requires inclusion bit Q(21)¼0 least signiﬁcant bit multiplier Qat beginning multiplication process. Booth’s algorithm illustratedin Figure 4.8. following examples show apply steps Booth’s algorithm. =10=01A= 0, Q( −1)= 0 M= Multiplicand Q= Multiplier n= Count Q(0), Q( −1)? 11 00 A= A− A= A+ ASR(A,Q) n= n − 1 n= 0?No Yes Done Figure 4.8 Booth’s algorithm70 COMPUTER ARITHMETICExample Consider multiplication two positive numbers M¼0111 (7) andQ¼0011 (3) assuming n¼4. steps needed tabulated below. Q Q (21) 0111 0000 0011 0 Initial value 0111 1001 0011 0 A¼A2M 0111 1100 1001 1 ASR End cycle #1 --------------------------------------------------------------------- 0111 1110 0100 1 ASR End cycle #2 ---------------------------------------------------------------------0111 0101 0100 1 A¼AþM 0111 0010 1010 0 ASR End cycle #3 --------------------------------------------------------------------- 0111 0001 0101 1 ASR End cycle #4 |ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ} þ21 (correct result) Example Consider multiplication two numbers M¼0111 (7) Q¼1101 (23) assuming n¼4. steps needed tabulated below. Q Q (21) 0111 0000 1101 0 Initial value 0111 1001 1101 0 A¼A2M 0111 1100 1110 1 ASR End cycle #1 --------------------------------------------------------------------- 0111 0011 1110 1 A¼AþM 0111 0001 1111 0 ASR End cycle #2 --------------------------------------------------------------------- 0111 1010 1111 0 A¼A2M 0111 1101 0111 1 ASR End cycle #3 ---------------------------------------------------------------------0111 1110 1011 1 ASR End cycle #4 |ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ} 221 (correct result) hardware structure shown Figure 4.9 used perform operations required Booth’s algorithm. consists ALU perform add / sub operation depending two bits Q(0)Q(21). control circuitry also required perform ASR (AQ) issue appropriate signals needed control number cycles. main drawbacks Booth’s algorithm variability number add/sub operations inefﬁciency algorithm bit pattern Q4.2. INTEGER ARITHMETIC 71becomes repeated pair 0 (1) followed 1(0). last situation improved three rather two bits inspected time. Division Among four basic arithmetic operations, division considered complex time consuming. simplest form, integer division oper- ation takes two arguments, dividend Xand divisor D.It produces two outputs, quotient Qand remainder R.The four quantities satisfy relation X¼ Q/C2DþRwhere R,D. number complications arise dealing division. obvious among case D¼0. Another subtle difﬁculty requirement resulting quotient exceed capacity reg-ister holding it. satisﬁed Q,2 n/C01,w h e r e nis number bits register holding quotient. implies relation X,2n/C01Dmust also satisﬁed. Failure satisfy conditions lead overﬂow condition. start showing division algorithm assuming values involved, is, divided, divisor, quotient, remainder interpreted frac- tions. process also valid integer values shown later. order obtain positive fractional quotient Q¼0q1q2/C1/C1/C1qn/C01, division operation performed sequence repeated subtractions shifts. step, remainder compared divisor D. remainder larger, quotient bit set “1”; otherwise quotient bit set to“0”. represented following equation r i¼2ri/C01/C0qi/C2D riandri/C01are current previous remainder, respectively, r0¼ Xandi¼1, 2, ...,(n21). Example Consider division dividend X¼0.5¼(0.100000) divisor D¼0.75¼(0.1100). process illustrated following table. r0¼X 0† 100000 Initial values 2r0 01 † 00000 e q1¼1 2Dþ 11 † 010 r1¼2r02D 00 † 01000 2r1 00 † 1000 e q2¼0 Figure 4.9 Hardware structure implementing Booth’s algorithm72 COMPUTER ARITHMETICr2¼2r1 00 † 1000 2r2 01 † 0 0 0 Set q3¼1 2Dþ 11 † 010 r3¼2r22D 00 † 010 resultant quotient Q¼(0.101)¼(5/8) remainder R¼(1/32). values correct since X¼QDþR¼(5/8)(3/4)þ1/32¼1/2. show validity process case integer values. case, equation X¼QDþRcan rewritten 22n/C02Xf¼2n/C01Qf/C2 2n/C01Dfþ2n/C01Rf, leading Xf¼Qf/C2Dfþ2/C0(n/C01)Rfwhere Xf,Df,Qf, Rf fractions. offer following illustrative example. Example Consider division dividend X¼32¼(0100000) divisor D¼6¼(0110). process illustrated following table. r0¼X 0100000 Initial values 2r0 0100000 e q1¼1 2Dþ 11010 r1¼2r02D 0001000 2r1 001000 e q2¼0 r2¼2r1 001000 2r2 01000 e q3¼1 2Dþ 11010 r3¼2r22D 00010 2r3 0010 e q4¼0 resultant quotient Q¼0101¼(5) remainder R¼0010 (2). correct values. hardware structure binary division shown Figure 4.10. ﬁgure, divisor ( D) contents register added using ( nþ1)-bit adder. control logic used perform required shift left operation (see Exercises). comparison remainder divisor considered difﬁcult step division process. way used perform com- parison subtract Dfrom 2 ri/C01and result negative, set qi¼0. required restoring previous value adding back subtracted value(restoring division ). alternative use non-restoring division algorithm: Step #1: following ntimes 1. sign 0, shift left AQ subtract A; otherwise shift left AQ add A. 2. sign 0, set q 0¼1; otherwise set q0¼0. Step #2: sign 1, add A.4.2. INTEGER ARITHMETIC 73Example Consider division dividend X¼8¼(1000) divisor D¼3¼(0011) using non-restoring algorithm. process illustrated following table. Initially 0 0 0 0 0 1 0 0 09 >>>= >>>;First cycle00011 Shift 0 0 0 0 1 0 0 0 Subtract 1 1 1 0 1 Setx 0 1110 000 0 Shift 1 1 1 0 0 0 0 0 00011) Second cycle Setx0 11111 000 0 Shift 1 1 1 1 0 0 0 0 00011) Third cycle Setx0 00001 000 1 Shift 0 0 0 1 0 0 0 1Subtract 1 1 1 0 1) Fourth cycle Setx 0 11111 001 0 |ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ} Quotient 11111) Restore remainder 0001100010 Remainder |ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ} 4.3. FLOATING-POINT ARITHMETIC considered integer representation arithmetic, consider section ﬂoating-point representation arithmetic. 4.3.1. Floating-Point Representation (Scientiﬁc Notation) ﬂoating-point (FP) number represented following form: +m/C3be, m, called mantissa , represents fraction part number isDivisor (D) Shift Left(n+1)-bit AdderAdd Control Logic Register Dividend (X)C Figure 4.10 Binary division structure74 COMPUTER ARITHMETICnormally represented signed binary fraction, erepresents exponent, b represents base (radix) exponent. Example Figure 4.11 representation ﬂoating-point number m¼23 bits, e¼8 bits, S(sign bit) ¼1 bit. value stored Sis 0, number positive value stored Sis 1, number negative. exponent example, represent positive numbers 0 255. represent positive negative exponents, ﬁxed value, called bias, subtracted exponent ﬁeld obtain true exponent. Assume example bias ¼128 used, true exponents range 2128 (stored 0 exponent ﬁeld) þ127 (stored 255 exponent ﬁeld) represented. Based representation, exponent þ4 represented storing 132 exponent ﬁeld, exponent 212 represented storing 116 exponent ﬁeld. Assuming b¼2, FP number 1.75 represented forms shown Figure 4.12. simplify performing operations FP numbers increase precision, always represented called normalized forms. FP number said normalized leftmost bit mantissa 1. Therefore, among three possible representations 1.75, ﬁrst representation normal-ized used. Since signiﬁcant bit (MSB) normalized FP number always 1, bit often stored assumed hidden bit left radixpoint, is, stored mantissa 1. m.Therefore, nonzero normalized number represents value ( /C01) s*(1:m)*2e/C0128. Floating-Point Arithmetic Addition /Subtraction difﬁculty adding two FP numbers stems fact may different exponents.Therefore, adding two FP numbers, exponents must equalized, thatis, mantissa number smaller magnitude exponent must bealigned. Figure 4.11 Representation ﬂoating-point number +0.111*21 +0.0111*22 +0.00000000000000000000111*2210 10000001 11100000000000000000000 0 10000010 01110000000000000000000 0 10010101 00000000000000000000111 Figure 4.12 Different representation FP number4.3. FLOATING-POINT ARITHMETIC 75Steps Required Add /Subtract Two Floating-Point Numbers 1. Compare magnitude two exponents make suitable alignment number smaller magnitude exponent. 2. Perform addition /subtraction. 3. Perform normalization shifting resulting mantissa adjusting resulting exponent. Example Consider adding two FP numbers 1.1100 *24and 1.1000 *22. 1. Alignment: 1.1000 *22has aligned 0.0110 *24 2. Addition: Add two numbers get 10.0010 *24. 3. Normalization: ﬁnal normalized result 0.1000 *26(assuming 4 bits allowed radix point). Addition /subtraction two FP numbers illustrated using schematic shown Figure 4.13. Multiplication Multiplication pair FP numbers X¼mx*2aandY¼ my*2bis represented X*Y¼(mx/C3my)*2aþb. general algorithm multiplication FP numbers consists three basic steps. are: 1. Compute exponent product adding exponents together. 2. Multiply two mantissas. 3. Normalize round ﬁnal product. Example Consider multiplying two FP numbers X¼1.000 *222and Y¼21.010 *221. 1. Add exponents: 22þ(21)¼23. 2. Multiply mantissas: 1.000 *21.010¼21.010000. product 21.0100 *223. Figure 4.13 Addition /subtraction FP numbers76 COMPUTER ARITHMETICMultiplication two FP numbers illustrated using schematic shown Figure 4.14. Division Division pair FP numbers X¼mx*2aandY¼my*2bis represented X=Y¼(mx=my)*2a/C0b. general algorithm division FP numbers consists three basic steps: 1. Compute exponent result subtracting exponents. 2. Divide mantissa determine sign result.3. Normalize round resulting value, necessary. Example Consider division two FP numbers X¼1.0000 *2 22and Y¼21.0100 *221. 1. Subtract exponents: 222(21)¼21. 2. Divide mantissas: 1.0000 42 1.0100 ¼20.1101. 3. result 20.1101 *221. Division two FP numbers illustrated using schematic shown Figure 4.15. 4.3.3. IEEE Floating-Point Standard essentially two IEEE standard ﬂoating-point formats. basic extended formats. these, IEEE deﬁnes two formats, is, single-precision double-precision formats. single-precision format 32-bit double-precision 64-bit. single extended format least 44 bits double extended format least 80 bits.Exponent E2 Exponent E1 AddMantissa M2 Mantissa M1 Multiply Result normalization round logic Result Exponent Result Mantissa Figure 4.14 FP multiplication4.3. FLOATING-POINT ARITHMETIC 77In single-precision format, base 2 used, thus allowing use hidden bit. exponent ﬁeld 8 bits. IEEE single-precision representation shown Figure 4.16. 8-bit exponent allows 256 combinations. Among these, two com- binations reserved special values: 1.e¼0 reserved zero (with fraction m¼0) denormalized numbers (with fraction m=0). 2.e¼255 reserved +1(with fraction m¼0) number (NaN) (with fraction m=0). m¼0 m=0 e¼0 0 Denormalized e¼255 +1 NaN single extended IEEE format extends exponent ﬁeld 8 11 bits mantissa ﬁeld 23 þ1 32 bits (without hidden bit). results total length least 44 bits. single extended format used calculatingintermediate results. 4.3.4. Double-Precision IEEE Format exponent ﬁeld 11 bits signiﬁcant ﬁeld 52 bits. format shown Figure 4.17. Similar single-precision format, extreme values e(0 2047) reserved purpose.Exponent E2 Exponent E1 SubtractMantissa M2 Mantissa M1 Divide Result normalization round logic Result Exponent Result Mantissa Figure 4.15 FP division Figure 4.16 IEEE single-precision representation78 COMPUTER ARITHMETICA number attributes characterizing IEEE single- double-precision formats summarized Table 4.2. 4.4. SUMMARY chapter, discussed number issues related computer arithmetic. discussion started introduction number representation radix con-version techniques. discussed integer arithmetic and, particular, dis-cussed four main operations, is, addition, subtraction, multiplication, anddivision. case, shown basic architectures organization. Thelast topic discussed chapter ﬂoating-point representation arith-metic. also shown basic architectures needed perform basic ﬂoat- ing-point operations addition, subtraction, multiplication, division. ended discussion chapter IEEE ﬂoating-point numberrepresentation. EXERCISES 1. Represent decimal values 26, 2123 signed, 10-bit numbers using following binary formats: (a) Sign-and-magnitude; (b) 2’s complement. 2. Compute decimal value binary number 1011 1101 0101 0110 given number represents unsigned integer. Repeat number represents 2’s complement. Repeat number represents sign-magnitude integer. Figure 4.17 Double-precision representation TABLE 4.2 Characteristics IEEE Single Double Floating-Point Formats Characteristic Single-precision Double-precision Length bits 32 64 Fraction part bits 23 52 Hidden bits 1 1 Exponent length bits 8 11 Bias 127 1023 Approximate range 2128/C253:8/C2103821024/C259:0/C210307 Smallest normalized number 2/C0126/C2510/C0382/C01022/C2510/C0308EXERCISES 793. Consider binary numbers following addition subtraction pro- blems signed 6-bit values 2’s complement representation. Per- form following operations, specifying whether overﬂow occurs. 010110 011001 110111 100001 111111 011010 þ001001þ010000þ101011 2011101 2000111 2100010 4. Multiply following pairs signed 2’s complement numbers using 2-bit Booth algorithm. M¼010111 M¼110011 M¼110101 M¼1111 Q¼110110 Q¼101100 Q¼011011 Q¼1111 5. Divide following pairs signed 20s complement numbers using restoring nonrestoring algorithms. X¼010111 X¼110011 X¼110101 X¼1111 D¼110110 D¼101100 D¼011011 D¼1111 6. Show perform addition, subtraction, multiplication, division following ﬂoating numbers. A¼ 0 10001 011011 B¼ 1 01111 101010 numbers represented 12-bit format using base b¼2, 5-bit exponent ewith bias ¼16, 6-bit normalized mantissa m. 7. Show complete design (in terms logic equations) 4-bit adder / subtractor using carry-look-ahead technique carries c1,c2,c3,c4. Assume two 4-bit input numbers A¼a4a3a2a1and B¼b4b3b2b1. 8. Design BCD adder using 4-bit binary adder least number logic gates. adder receive two 4-bit numbers B pro- duce 4-bit sum carry output. 9. Show design 16-bit CLA uses 4-bit CLA block shown Figure 4.5. Compute delay area (in terms number oflogic gates required). 10. Compare longest path delay input output 32-bit adder using 4-bit CLA adder blocks multilevel architecture 32-bit CRTadder. Assume gate delay given g. 11. Convert following decimal numbers IEEE single- precision ﬂoating-point counterparts. (a)276 (b) 0.92 (c) 5.312580 COMPUTER ARITHMETIC(d)20.000072 (e) 8.04/C21021 12. Convert following IEEE single-precision ﬂoating-point numbers decimal counterparts. (a) 6589 00000 (b) 807B 00000H (c) CDEF 0000H 13. Complete logic design array multiplier shown Figure 4.6. 14. Design control logic shown Figure 4.7. 15. Provide complete logic design Control Logic indicated Figure 4.10. REFERENCES READING C. Hamacher, Z. Vranesic S. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002. V. Heuring H. Jordan, Computer Systems Design Archiecture , Addison Wesley Longman, NJ, USA, 1997. K. Israel, Computer Arithmetic Algorithms , 2nd ed., A. K. Peters, Ltd., Massachusetts, 2002. W. Stallings, Computer Organization Architectures: Designing Performance , 4th ed., Prentice-Hall, NJ, USA, 1996. B. Wilkinson, Computer Architecture: Design Performance , 2nd ed., Prentice-Hall, Hertfordshire, UK, 1996.REFERENCES READING 81& CHAPTER 5 Processing Unit Design previous chapters, studied history computer systems fundamen- tal issues related memory locations, addressing modes, assembly language, andcomputer arithmetic. chapter, focus attention main componentof computer system, central processing unit (CPU). primary function ofthe CPU execute set instructions stored computer’s memory. Asimple CPU consists set registers, arithmetic logic unit (ALU), con-trol unit (CU). follows, reader introduced organization andmain operations CPU. 5.1. CPU BASICS typical CPU three major components: (1) register set, (2) arithmetic logic unit (ALU), (3) control unit (CU). register set differs one computerarchitecture another. usually combination general-purpose special-purpose registers. General-purpose registers used purpose, hence name general purpose. Special-purpose registers speciﬁc functions within CPU. example, program counter (PC) special-purpose registerthat used hold address instruction executed next. Anotherexample special-purpose registers instruction register (IR), isused hold instruction currently executed. ALU provides cir-cuitry needed perform arithmetic, logical shift operations demanded ofthe instruction set. Chapter 4, covered number arithmetic oper-ations circuits used support computation ALU. control unit entity responsible fetching instruction executed main memory decoding executing it. Figure 5.1 shows main com-ponents CPU interactions memory system input / output devices. CPU fetches instructions memory, reads writes data memory, transfers data input /output devices. typical 83Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.simple execution cycle summarized follows: 1. next instruction executed, whose address obtained PC, fetched memory stored IR. 2. instruction decoded. 3. Operands fetched memory stored CPU registers, needed. 4. instruction executed. 5. Results transferred CPU registers memory, needed. execution cycle repeated long instructions execute. check pending interrupts usually included cycle. Examples inter- rupts include /O device request, arithmetic overﬂow, page fault (see Chapter 7). interrupt request encountered, transfer interrupt handling routinetakes place. Interrupt handling routines programs invoked collect thestate currently executing program, correct cause interrupt, restore state program. actions CPU execution cycle deﬁned micro-orders issued control unit. micro-orders individual control signals sent dedicated control lines. example, let us assume want execute aninstruction moves contents register Xto register Y.Let us also assume registers connected data bus, D.The control unit issue con- trol signal tell register Xto place contents data bus D.After delay, another control signal sent tell register Yto read data bus D.The acti- vation control signals determined using either hardwired control micropro-gramming. concepts explained later chapter.RegistersControl UnitCPUMemory System Input / OutputData Instructions ALU Figure 5.1 Central processing unit main components interactions memory /O84 PROCESSING UNIT DESIGNThe remainder chapter organized follows. Section 5.2 presents register set explains different types registers functions. Sec- tion 5.3, understand meant datapath control. CPU instructioncycle control unit covered Sections 5.4 5.5, respectively. 5.2. REGISTER SET Registers essentially extremely fast memory locations within CPU used create store results CPU operations calculations. Differ-ent computers different register sets. differ number registers, reg-ister types, length register. also differ usage register. General-purpose registers used multiple purposes assigned variety functions programmer. Special-purpose registers restrictedto speciﬁc functions. cases, registers used hold dataand cannot used calculations operand addresses. length dataregister must long enough hold values data types. machinesallow two contiguous registers hold double-length values. Address registersmay dedicated particular addressing mode may used address generalpurpose. Address registers must long enough hold largest address. number registers particular architecture affects instruction set design. small number registers may result increase memory references.Another type registers used hold processor status bits, ﬂags. bitsare set CPU result execution operation. status bitscan tested later time part another operation. 5.2.1. Memory Access Registers Two registers essential memory write read operations: memory data register (MDR) memory address register (MAR). MDR MAR used exclusively CPU directly accessible programmers. order perform write operation speciﬁed memory location, MDR MAR used follows: 1. word stored memory location ﬁrst loaded CPU MDR. 2. address location word stored loaded CPU MAR. 3. write signal issued CPU. Similarly, perform memory read operation, MDR MAR used follows: 1. address location word read loaded MAR.5.2. REGISTER SET 852. read signal issued CPU. 3. required word loaded memory MDR ready use CPU. 5.2.2. Instruction Fetching Registers Two main registers involved fetching instruction execution: pro- gram counter (PC) instruction register (IR). PC register con- tains address next instruction fetched. fetched instruction loaded IR execution. successful instruction fetch, PC updatedto point next instruction executed. case branch operation, thePC updated point branch target instruction branch resolved, is, target address known. 5.2.3. Condition Registers Condition registers, ﬂags, used maintain status information. architec- tures contain special program status word (PSW) register. PSW contains bitsthat set CPU indicate current status executing program. Theseindicators typically arithmetic operations, interrupts, memory protectioninformation, processor status. 5.2.4. Special-Purpose Address Registers Index Register covered Chapter 2, index addressing, address operand obtained adding constant content register, called index register. index register holds address displacement. Index addressing indi- cated instruction including name index register parentheses using symbol Xto indicate constant added. Segment Pointers discuss Chapter 6, order support segmen- tation, address issued processor consist segment number (base)and displacement (or offset) within segment. segment register holds theaddress base segment. Stack Pointer shown Chapter 2, stack data organization mechanism last data item stored ﬁrst data item retrieved. Two speciﬁc oper- ations performed stack. Push Popoperations. speciﬁc register, called stack pointer (SP), used indicate stack location addressed. stack push operation, SP value used indicate thelocation (called top stack). storing (pushing) value, SP incremented (in architectures, e.g. X86, SP decremented stack grows low memory).86 PROCESSING UNIT DESIGN5.2.5. 80 386 Registers discussed Chapter 3, Intel basic programming model 386, 486, Pentium consists three register groups. general-purpose registers, segment registers, instruction pointer (program counter) ﬂag register. Figure 5.2 (which repeats Fig. 3.6) shows three sets registers. ﬁrst set consists general purpose registers A, B, C, D, SI (source index), DI (destinationindex), SP (stack pointer), BP (base pointer). second set registers consistsof CS (code segment), SS (stack segment), four data segment registers DS, ES,FS, GS. third set registers consists instruction pointer (programcounter) ﬂags (status) register. Among status bits, ﬁrst ﬁve iden-tical bits introduced early 8085 8-bit microprocessor. next 6–11 bits identical introduced 8086. ﬂags bits 12–14 introduced 80286 16–17 bits introduced 80386.The ﬂag bit 18 introduced 80486. 5.2.6. MIPS Registers MIPS CPU contains 32 general-purpose registers numbered 0–31. Register xis designated $x. Register $zero always contains hardwired value 0. Table 5.1 lists registers describes intended use. Registers$at (1), $k0 (26), $k1 (27) reserved use assembler operatingsystem. Registers $a0–$a3 (4–7) used pass ﬁrst four arguments routines Figure 5.2 main register sets 80 /C286 (80386 extended 16 bit registers except segment registers)5.2. REGISTER SET 87(remaining arguments passed stack). Registers $v0 $v1 (2, 3) used return values functions. Registers $t0–$t9 (8–15, 24, 25) caller-saved registers used temporary quantities need preserved acrosscalls. Registers $s0–$s7 (16–23) calle-saved registers hold long-livedvalues preserved across calls. Register $sp(29) stack pointer, points last location use stack. Register $fp(30) frame pointer. Register $ra(31) written thereturn address function call. Register $gp(28) global pointer pointsinto middle 64 K block memory heap holds constants global variables. objects heap quickly accessed single load store instruction.TABLE 5.1 MIPS General-Purpose Registers Name Number Usage Name Number Usage zero 0 Constant 0 s0 16 Saved temporary (preserved across call) 1 Reserved assembler s1 17 Saved temporary (preserved across call) v0 2 Expression evaluation ands2 18 Saved temporary (preserved across call) v1 3 results function s3 19 Saved temporary (preserved across call) a0 4 Argument 1 s4 20 Saved temporary (preserved across call) a1 5 Argument 2 s5 21 Saved temporary (preserved across call) a2 6 Argument 3 s6 22 Saved temporary (preserved across call) a3 7 Argument 4 s7 23 Saved temporary (preserved across call) t0 8 Temporary (not preserved across call)t8 24 Temporary (not preserved across call) t1 9 Temporary (not preserved across call)t9 25 Temporary (not preserved across call) t2 10 Temporary (not preserved across call)k0 26 Reserved OS kernel t3 11 Temporary (not preserved across call)k1 27 Reserved OS kernel t4 12 Temporary (not preserved across call)gp 28 Pointer global area t5 13 Temporary (not preserved across call)sp 29 Stack pointer t6 14 Temporary (not preserved across call)fp 30 Frame pointer t7 15 Temporary (not preserved across call)ra 31 Return address (used function call)88 PROCESSING UNIT DESIGN5.3. DATAPATH CPU divided data section control section. data section, also called datapath, contains registers ALU. datapath iscapable performing certain operations data items. control section basi-cally control unit, issues control signals datapath. Internal theCPU, data move one register another ALU registers. Internal data movements performed via local buses, may carry data, instructions, addresses. Externally, data move registers memory andI/O devices, often means system bus. Internal data movement among registers ALU registers may carried using differentorganizations including one-bus, two-bus, three-bus organizations. Dedicateddatapaths may also used components transfer data them-selves frequently. example, contents PC transferred theMAR fetch new instruction beginning instruction cycle. Hence, dedicated datapath PC MAR could useful speeding part instruction execution. 5.3.1. One-Bus Organization Using one bus, CPU registers ALU use single bus move outgoing incoming data. Since bus handle single data movement withinone clock cycle, two-operand operations need two cycles fetch operandsfor ALU. Additional registers may also needed buffer data ALU. bus organization simplest least expensive, limits amount data transfer done clock cycle, slowdown overall performance. Figure 5.3 shows one-bus datapath consisting ofa set general-purpose registers, memory address register (MAR), memorydata register (MDR), instruction register (IR), program counter (PC), andan ALU. General Purpose RegistersPC IR MAR MDRALUA B Memory BusProgram Counter (PC), ALU Figure 5.3 One-bus datapath5.3. DATAPATH 895.3.2. Two-Bus Organization Using two buses faster solution one-bus organization. case, gen- eral-purpose registers connected buses. Data transferred two different registers input point ALU time. Therefore, two- operand operation fetch operands clock cycle. additionalbuffer register may needed hold output ALU two busesare busy carrying two operands. Figure 5.4 ashows two-bus organization. cases, one buses may dedicated moving data registers(in-bus ), dedicated transferring data registers (out-bus ). case, additional buffer register may used, one ALU inputs, hold one operands. ALU output connected directly in-bus, transfer result one registers. Figure 5.4 b shows two-bus organization in-bus out-bus. 5.3.3. Three-Bus Organization three-bus organization, two buses may used source buses third used destination. source buses move data registers ( out-bus ), (a) (b)PC IR MAR MDRALUAIn-bus Out-busGeneral Purpose Registers General Purpose RegistersPC IR MAR MDRALU Memory Memory BusBus 1 Bus 2 Figure 5.4 Two-bus organizations. ( a) Example Two-Bus Datapath. ( b) Another Example Two-Bus Datapath in-bus out-bus90 PROCESSING UNIT DESIGNthe destination bus may move data register ( in-bus ). two out-buses connected ALU input point. output ALU connected directly in-bus. expected, buses have, data movewithin single clock cycle. However, increasing number buses also increase complexity hardware. Figure 5.5 shows example three-bus datapath. 5.4. CPU INSTRUCTION CYCLE sequence operations performed CPU execution instruc- tions presented Fig. 5.6. long instructions execute, nextinstruction fetched main memory. instruction executed based onthe operation speciﬁed opcode ﬁeld instruction. completion instruction execution, test made determine whether interrupt occurred. interrupt handling routine needs invoked case interrupt. Figure 5.5 Three-bus datapath Figure 5.6 CPU functions5.4. CPU INSTRUCTION CYCLE 91The basic actions fetching instruction, executing instruction, hand- ling interrupt deﬁned sequence micro-operations. group controlsignals must enabled prescribed sequence trigger execution micro-operation. section, show micro-operations implement instructionfetch, execution simple arithmetic instructions, interrupt handling. 5.4.1. Fetch Instructions sequence events fetching instruction summarized follows: 1. contents PC loaded MAR. 2. value PC incremented. (This operation done parallel memory access.) 3. result memory read operation, instruction loaded MDR.4. contents MDR loaded IR. Let us consider one-bus datapath organization shown Fig. 5.3. see fetch operation accomplished three steps shown table below, 0,t1,t2. Note multiple operations separated “;” imply accomplished parallel. Step Micro-operation t0 MAR /C0(PC); /C0(PC) t1 MDR /C0Mem[MAR]; PC /C0(A)þ4 t2 IR /C0(MDR) Using three-bus datapath shown Figure 5.5, following table shows steps needed. Step Micro-operationt 0 MAR /C0(PC); PC /C0(PC)þ4 t1 MDR /C0Mem[MAR] t2 IR /C0(MDR) 5.4.2. Execute Simple Arithmetic Operation Add R 1,R2,R0This instruction adds contents source registers R1andR2,a n stores results destination register R0. addition executed follows: 1. registers R0,R1,R2, extracted IR. 2. contents R1andR2are passed ALU addition. 3. output ALU transferred R0.92 PROCESSING UNIT DESIGNUsing one-bus datapath shown Figure 5.3, addition take three steps shown following table, t0,t1,t2. Step Micro-operation t0 /C0(R1) t1 B /C0(R2) t2 R0 /C0(A)þ(B) Using two-bus datapath shown Figure 5.4 a, addition take two steps shown following table, t0,t1. Step Micro-operation t0 /C0(R1)þ(R2) t1 R0 /C0(A) Using two-bus datapath in-bus out-bus shown Figure 5.4 b, addition take two steps shown below, t0,t1. Step Micro-operationt 0 /C0(R1) t1 R0 /C0(A)þ(R2) Using three-bus datapath shown Figure 5.5, addition take one step shown following table. Step Micro-operationt 0 R0 /C0(R1)þ(R2) Add X, R 0This instruction adds contents memory location Xto register R0 stores result R0. addition executed follows: 1. memory location Xis extracted IR loaded MAR. 2. result memory read operation, contents Xare loaded MDR. 3. contents MDR added contents R0. Using one-bus datapath shown Figure 5.3, addition take ﬁve steps shown below, t0,t1,t2,t3,t4.5.4. CPU INSTRUCTION CYCLE 93Step Micro-operation t0 MAR /C0X t1 MDR /C0Mem[MAR] t2 /C0(R0) t3 B /C0(MDR ) t4 R0 /C0(A)þ(B) Using two-bus datapath shown Figure 5.4 a, addition take four steps shown below, t0,t1,t2,t3. Step Micro-operation t0 MAR /C0X t1 MDR /C0Mem[MAR] t2 /C0(R0)þ(MDR ) t3 R0 /C0(A) Using two-bus datapath in-bus out-bus shown Figure 5.4 b, addition take four steps shown below, t0,t1,t2,t3. Step Micro-operation t0 MAR /C0X t1 MDR /C0Mem[MAR] t2 /C0(R0) t3 R0 /C0(A)þ(MDR ) Using three-bus datapath shown Figure 5.5, addition take three steps shown below, t0,t1,t2. Step Micro-operationt 0 MAR /C0X t1 MDR /C0Mem[MAR] t2 R0 /C0R0þ(MDR ) 5.4.3. Interrupt Handling execution instruction, test performed check pending inter- rupts. interrupt request waiting, following steps take place: 1. contents PC loaded MDR (to saved). 2. MAR loaded address PC contents saved. 3. PC loaded address ﬁrst instruction interrupt hand- ling routine.94 PROCESSING UNIT DESIGN4. contents MDR (old value PC) stored memory. following table shows sequence events, t1,t2,t3. Step Micro-operation t1 MDR /C0(PC) t2 MAR /C0address1 (where save old PC); PC /C0address2 (interrupt handling routine) t3 Mem[MAR] /C0(MDR) 5.5. CONTROL UNIT control unit main component directs system operations sending control signals datapath. signals control ﬂow data within CPUand CPU external units memory /O. Control buses generally carry signals control unit computer components clock-driven manner. system clock produces continuous sequence pulses speciﬁed duration frequency. sequence steps 0,t1,t2,..., Figure 5.7 Timing control signals5.5. CONTROL UNIT 95(t0,t1,t2,...) used execute certain instruction. op-code ﬁeld fetched instruction decoded provide control signal generator infor- mation instruction executed. Step information generated alogic circuit module used inputs generate control signals. Thesignal generator speciﬁed simply set Boolean equations itsoutput terms inputs. Figure 5.7 shows block diagram describes howtiming used generating control signals. mainly two different types control units: microprogrammed hardwired. microprogrammed control, control signals associated oper- ations stored special memory units inaccessible programmer controlwords. control word microinstruction speciﬁes one micro-operations. sequence microinstructions called microprogram, isstored ROM RAM called control memory CM. hardwired control, ﬁxed logic circuits correspond directly Boolean expressions used generate control signals. Clearly hardwired control faster microprogrammed control. However, hardwired control could expensive complicated complex systems. Hardwired control econ-omical small control units. also noted microprogrammed controlcould adapt easily changes system design. easily add new instruc-tions without changing hardware. Hardwired control require redesign theentire systems case change. Example 1 Let us revisit add operation add contents source registers R 1,R2, store results destination register R0. shown earlier operation done one step using three-bus datapath shown Figure 5.5. Let us try examine control sequence needed accomplish addition step t0. Suppose op-code ﬁeld current instruction decoded Inst-x type. First need select source registers destination register,then select Add ALU function performed. following tableshows needed step control sequence. Step Instruction type Micro-operation Control t0 Inst-x R0 /C0(R1)þ(R2) Select R1as source 1 out-bus1 (R 1out-bus1) Select R2as source 2 out-bus2 (R 2out-bus2) Select R0as destination in-bus (R 0in-bus) Select ALU function Add (Add) Figure 5.8 shows signals generated execute Inst-x time period t0. gate ensures signals issued op-code decoded Inst-x time period t0. signals (R 1out-bus 1), (R 2out-bus2),96 PROCESSING UNIT DESIGN(R0in-bus), (Add) select R1as source out-bus1, R2as source out- bus2, R0as destination in-bus, select ALUs add function, respectively. Example 2 Let us repeat operation previous example using one-bus datapath shown Fig. 5.3. shown earlier operation carried three steps using one-bus datapath. Suppose op-code ﬁeld current instruction decoded Inst-x type. following table shows neededsteps control sequence. Step Instruction type Micro-operation t0 Inst-x /C0(R1) Select R1as source (R 1out) Select Aas destination (A in) t1 Inst-x B /C0(R2) Select R2as source (R 2out) Select Bas destination (B in) t2 Inst-x R0 /C0(A)þ(B) Select ALU function Add (Add) Select R0as destination (R 0in) Figure 5.9 shows signals generated execute Inst-x time periods t0, t1, t2. gates ensure appropriate signals issued op-code decoded Inst-x appropriate time period. t0, signals (R 1out) (A in) issued move contents R 1into A. Similarly t1, signals (R 2out) (B in) issued move contents R 2 B. Finally, signals (R 0in) (Add) issued t2to add con- tents B move results R 0. 5.5.1. Hardwired Implementation hardwired control, direct implementation accomplished using logic cir- cuits. control line, one must ﬁnd Boolean expression terms theinput control signal generator shown Figure 5.7. Let us explain theIn-bus Out-bus 1ALU Out-bus 2t0 Inst-xR0 R1 R2R0in-bus R2out-bus2R2out-bus1Add Figure 5.8 Signals generated execute Inst-x three-bus datapath time period t05.5. CONTROL UNIT 97implementation using simple example. Assume instruction set machine three instructions: Inst-x, Inst-y, Inst-z; A, B, C, D, E, F, G, H control lines. following table shows control lines thatshould activated three instructions three steps 0,t1, t2. Step Inst-x Inst-y Inst-z t0 D, B, E F, H, G E, H t1 C, A, H G D, A, C t2 G, C B, C Boolean expressions control lines A, B, C obtained follows: A¼Inst-x/C1t1þInst-z/C1t1¼(Inst-xþInst-z)/C1t1 B¼Inst-x/C1t0þInst-y/C1t2 C¼Inst-x/C1t1þInst-x/C1t2þInst-y/C1t2þInst-z/C1t1 ¼(Inst-xþInst-z)/C1t1þ(Inst-xþInst-y)/C1t2 Figure 5.10 shows logic circuits control lines. Boolean expressions rest control lines obtained similar way. Figure 5.11 shows state diagram execution cycle instructions. 5.5.2. Microprogrammed Control Unit idea microprogrammed control units introduced M. V. Wilkes early 1950s. Microprogramming motivated desire reduce complex- ities involved hardwired control. studied earlier, instruction Figure 5.9 Signals generated execute Inst-x one-bus datapath time period t0,t1,t298 PROCESSING UNIT DESIGNInst-x Inst-z Inst-x Inst-y Inst-y t2t2 Inst-x t0t1A C B Figure 5.10 Logic circuits control lines A, B, C Fetch next instruction Decode D, B, E C, A, H G, CF, H, G G B, CE, H D, A, Ct0 t1 t2t0 t1 t2t0 t1Inst-x Inst-z Inst-y Figure 5.11 Instruction execution state diagram5.5. CONTROL UNIT 99implemented using set micro-operations. Associated micro-operation set control lines must activated carry corresponding micro-operation. idea microprogrammed control store control signalsassociated implementation certain instruction microprogram ina special memory called control memory (CM). microprogram consists asequence microinstructions. microinstruction vector bits, eachbit control signal, condition code, address next microinstruction. Microinstructions fetched CM way program instructions fetched main memory (Fig. 5.12). instruction fetched memory, op-code ﬁeld instruc- tion determine microprogram executed. words, theop-code mapped microinstruction address control memory. Themicroinstruction processor uses address fetch ﬁrst microinstruction inthe microprogram. fetching microinstruction, appropriate controllines enabled. Every control line corresponds “1” bit turned on.Every control line corresponds “0” bit left off. completing execution one microinstruction, new microinstruction fetched executed. condition code bits indicate branchmust taken, next microinstruction speciﬁed address bits cur-rent microinstruction. Otherwise, next microinstruction sequence befetched executed. length microinstruction determined based number micro- operations speciﬁed microinstructions, way control bits interpreted, way address next microinstruction obtained. microinstruction may specify one micro-operations activatedsimultaneously. length microinstruction increase number ofparallel micro-operations per microinstruction increases. Furthermore, eachcontrol bit microinstruction corresponds exactly one control line, thelength microinstruction could get bigger. length microinstructioncould reduced control lines coded speciﬁc ﬁelds microinstruction. Decoders needed map ﬁeld individual control lines. Clearly, using decoders reduce number control lines activated sim- ultaneously. tradeoff length microinstructions theamount parallelism. important reduce length microinstructionsto reduce cost access time control memory. may also desirablethat micro-operations performed parallel control lines beactivated simultaneously. External inputControl Control AddressControl data Register SequencerControl Memory Figure 5.12 Fetching microinstructions (control words)100 PROCESSING UNIT DESIGNHorizontal Versus Vertical Microinstructions Microinstructions classiﬁed horizontal orvertical. Individual bits horizontal microinstructions correspond individual control lines. Horizontal microinstructions long allow maximum parallelism since bit controls single control line. Invertical microinstructions, control lines coded speciﬁc ﬁelds within amicroinstruction. Decoders needed map ﬁeld kbits 2 kpossible com- binations control lines. example, 3-bit ﬁeld microinstruction could used specify one eight possible lines. encoding, vertical microinstructions much shorter horizontal ones. Control lines encodedin ﬁeld cannot activated simultaneously. Therefore, vertical micro- instructions allow limited parallelism. noted decoding needed horizontal microinstructions decoding necessary thevertical case. Example 3 Consider three-bus datapath shown Figure 5.5. addition PC, IR, MAR, MDR, assume 16 general-purpose registers numbered R 0–R15. Also, assume ALU supports eight functions (add, sub- tract, multiply, divide, AND, OR, shift left, shift right). Consider add oper- ation Add R1,R2,R0, adds contents source registers R1,R2, store results destination register R0. example, study format microinstruction horizontal organization. use horizontal microinstructions, control bit control line. format microinstruction control bits forthe following: .ALU operations .Registers output out-bus1 (source 1) .Registers output out-bus2 (source 2) .Registers input in-bus (destination) .Other operations shown following table shows number bits needed ALU, Source 1, Source 2, destination: Purpose Number bits Explanations ALU 8 bits 8 functions Source 1 20 bits 16 general-purpose registers þ4 special- purpose registers Source 2 16 bits 16 general-purpose registersDestination 20 bits 16 general-purpose registers þ4 special- purpose registers Figure 5.13 microinstruction Add R1,R2,R0on three-bus datapath.5.5. CONTROL UNIT 101Example 4 example, use vertical microinstructions, decoders needed. use three-bus datapath shown Figure 5.5. Assume 16 general-purpose registers ALU supports eight functions. following tables show encoding ALU functions, registersconnected out-bus 1 (Source 1), registers connected out-bus 2 (Source 2),and registers connected in-bus (Destination). Purpose Number bits Explanations ALU 4 bits 8 functions þnone Source 1 5 bits 16 general-purpose registers þ4 special-purpose registersþnone Source 2 5 bits 16 general-purpose registers þnone Destination 5 bits 16 general-purpose registers þ4 special-purpose registersþnone Encoding ALU function 0000 None (ALU connect out-bus1 in-bus) 0001 Add 0010 Subtract 0011 Multiple 0100 Divide 0101 0110 0111 Shift left 1000 Shift right Encoding Source 1 Destination Encoding Source 2 00000 R 0 R0 00000 R 0 00001 R 1 R1 00001 R 1 00010 R 2 R2 00010 R 2 00011 R 3 R3 00011 R 3 00100 R 4 R4 00100 R 4 00101 R 5 R5 00101 R 5 00110 R 6 R6 00110 R 6 00111 R 7 R7 00111 R 7 01000 R 8 R8 01000 R 8 01001 R 9 R9 01001 R 9 01010 R 10 R10 01010 R 10 Figure 5.13 Microinstruction Add R1,R2,R0102 PROCESSING UNIT DESIGNEncoding Source 1 Destination Encoding Source 2 01011 R 11 R11 01011 R 11 01100 R 12 R12 01100 R 12 01101 R 13 R13 01101 R 13 01110 R 14 R14 01110 R 14 01111 R 15 R15 01111 R 15 10000 PC PC 10000 None 10001 IR IR 10010 MAR MAR 10011 MDR MDR 10100 NONE NONE Figure 5.14 microinstruction Add R1,R2,R0using three-bus data- path vertical organization: Example 5 Using encoding Example 4, let us ﬁnd vertical microin- structions used fetching instruction. MAR PC First, need select PC source 1 using “10000” source 1 ﬁeld. Similarly, select MAR destination using “10010” destina- tion ﬁeld. also need use “0000” ALU ﬁeld, decoded “NONE”. shown ALU encoding table (Example 4), “NONE” means out-bus1 connected in-bus. ﬁeld source 2 set “10000”,which means none registers selected. microinstruction shownin Figure 5.15. Memory Read Write Memory operations easily accommodated adding 1 bit read another write. two microinstructions Figure 5.16 perform memory read write, respectively. Fetch Fetching instruction done using three microinstructions Figure 5.17. ﬁrst second microinstructions shown above. third microin- struction moves contents MDR IR (IR MDR). MDR selected source 1 using “10011” source 1 ﬁeld. Similarly, IR selected destina- tion using “10001” destination ﬁeld. also need use “0000” (“NONE”) Figure 5.14 Microinstruction Add R1,R2,R0 Figure 5.15 Microinstruction MAR /C0PC5.5. CONTROL UNIT 103in ALU ﬁeld, means out-bus1 connected in-bus. ﬁeld source 2 set “10000”, means none registers selected. 5.6. SUMMARY CPU part computer interprets carries instructions con- tained programs write. CPU’s main components register ﬁle, ALU, control unit. register ﬁle contains general-purpose special reg- isters. General-purpose registers may used hold operands intermediate results.The special registers may used memory access, sequencing, status information,or hold fetched instruction decoding execution. Arithmetic logi-cal operations performed ALU. Internal CPU, data may move oneregister another registers ALU. Data may also move theCPU external components memory /O. control unit com- ponent controls state instruction cycle. long instructions execute, next instruction fetched main memory. instruction executed based operation speciﬁed op-code ﬁeld instruction. control unitgenerates signals control ﬂow data within CPU CPU andexternal units memory /O. control unit implemented using hardwired microprogramming techniques. EXERCISES 1. many instruction bits required specify following: (a) Two operand registers one result register machine 64 general-purpose registers? Figure 5.16 Microinstructions memory read write Figure 5.17 Microinstructions fetching instruction104 PROCESSING UNIT DESIGN(b) Three memory addresses machine 64 KB main memory? 2. Show micro-operations load,store , jump instructions using: (a) One-bus system (b) Two-bus system (c) Three-bus system 3. Add control signals tables Section 5.4. 4. Data movement within CPU performed several different ways. Contrast following methods terms advantages disadvantages: (a) Dedicated connections (b) One-bus datapath (c) Two-bus datapath (d) Three-bus datapath 5. Find method encoding microinstructions described following table minimum number control bits used inherent parallelism among microoperations preserved. Microinstruction Control signals activated I1 a,b,c,d,e I2 a,d,f,g I3 b,h I4 c I5 c,e,g,i I6 a,h,j 6. Suppose instruction set machine three instructions: Inst-1, Inst-2, Inst-3; A, B, C, D, E, F, G, H control lines. following table shows control lines activated forthe three instructions three steps T0, T1, T2. Step Inst-1 Inst-2 Inst-3 T0 D, B, E F, H, G E, H T1 C, A, H G D, A, CT2 G, C B, C (a) Hardwired approach: (i) Write Boolean expressions control lines A–G. (ii) Draw logic circuit control line. (b) Microprogramming approach: (i) Assuming horizontal representation, write micropro- gram instructions Inst-1. Indicate microinstruction size.EXERCISES 105(ii) allow horizontal vertical representation, would best grouping? microinstruction size? Write microprogram Inst-1. 7. certain processor microinstruction format containing 10 separate control ﬁelds C0:C9. Cican activate one nidistinct control lines, niis speciﬁed follows: i: 012 3 4 5 6789 ni:4431 191 67182 2 (a) minimum number control bits needed represent 10 control ﬁelds? (b) maximum number control bits needed purely horizon- tal format used control information? 8. main differences following pairs? (a) Vertical horizontal microinstructions (b) Microprogramming hardwired control 9. Using single-bus architecture, generate necessary control signals, proper order (with minimum number micro-instructions), forconditional branch instruction. 10. Write micro-program fetch instruction using one-bus datapath two-bus datapath. REFERENCES READING R. J. Baron L. Higbie, Computer Architecture , Addison Wesley, Canada, 1992. M. J. Flynn, Computer Architecture , Jones Barlett, MA, USA, 1995. J. P. Hayes, Computer Architecture Organization , McGraw-Hill, New York, 1998. J. Hennessy D. Patterson, Computer Architecture: Quantitative Approach , Morgan Kaufmann, San Francisco, CA, 2003. V. P. Heuring H. F. Jordan, Computer Systems Design Architecture , Addison Wesley, NJ, 1997. M. Murdocca V. Heuring, Principles Computer Architecture , Prentice Hall, NJ, USA, 2000. D. Patterson J. Hennessy, Computer Organization Design , Morgan Kaufmann, San Mateo, CA, 1998. W. Stallings, Computer Organization Architecture: Designing Performance , NJ, 1996. A. S. Tanenbaum, Structured Computer Organization , Prentice Hall, NJ, USA, 1999.106 PROCESSING UNIT DESIGN& CHAPTER 6 Memory System Design chapter, study computer memory system. stated Chapter 3 without memory information stored retrieved computer. Itis interesting observe early 1946 recognized Burks, Goldstine,and Von Neumann computer memory organized hierarchy. Insuch hierarchy, larger slower memories used supplement smaller andfaster ones. observation since proven essential constructing com-puter memory. put aside set CPU registers (as ﬁrst level storingand retrieving information inside CPU, see Chapter 5), typical memory hierarchy starts small, expensive, relatively fast unit, called cache . cache followed hierarchy larger, less expensive, relatively slow main memory unit. Cache main memory built using solid-state semi- conductor material. followed hierarchy far larger, less expensive,and much slower magnetic memories consist typically (hard) disk thetape. deliberation chapter starts discussing characteristics fac-tors inﬂuencing success memory hierarchy computer. direct ourattention design analysis cache memory. Discussion (main) memory unit conducted Chapter 7. Also discussed Chapter 7 issues related virtual memory design. brief coverage different read- memory (ROM) implementations also provided Chapter 7. 6.1. BASIC CONCEPTS section, introduce number fundamental concepts relate memory hierarchy computer. 6.1.1. Memory Hierarchy mentioned above, typical memory hierarchy starts small, expensive, relatively fast unit, called cache , followed larger, less expensive, rela- tively slow main memory unit. Cache main memory built using solid-state 107Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.semiconductor material (typically CMOS transistors). customary call fast memory level primary memory . solid-state memory followed larger, less expensive, far slower magnetic memories consist typically (hard) diskand tape. customary call disk secondary memory , tape con- ventionally called tertiary memory . objective behind designing memory hier- archy memory system performs consists entirely fastest unitand whose cost dominated cost slowest unit. memory hierarchy characterized number parameters. Among parameters access type ,capacity ,cycle time ,latency ,bandwidth ,a n cost.T h e term access refers action physically takes place read orwrite oper- ation. capacity memory level usually measured bytes. cycle time isdeﬁned time elapsed start read operation start subsequentread. latency deﬁned time interval request information andthe access ﬁrst bit information. bandwidth provides measure thenumber bits per second acces sed. cost memory level usually speciﬁed dollars per megabytes. Figure 6.1 depicts typical memory hierarchy.Table 6.1 provides typical values memory hierarchy parameters. term random access refers fact access memory location takes ﬁxed amount time regardless actual memory location /or sequence accesses takes place. example, write operation memory location 100 takes 15 ns operation followed read oper- ation memory location 3000, latter operation also take 15 ns. compared sequential access access location 100 takes 500 ns, consecutive access location 101 takes 505 ns, expected access location 300 may take 1500 ns. memory tocycle locations 100 300, location requiring 5 ns. effectiveness memory hierarchy depends principle moving information fast memory infrequently accessing many times beforereplacing new information. principle possible due phenomenoncalled locality reference ; is, within given period time, programs tend reference relatively conﬁned area memory repeatedly. exist two forms locality: spatial temporal locality. Spatial locality refers CPU Registers Cache Latency Main MemoryBandwidth Secondary Storage (Disk) Speed Cost per bit TertiaryStorage (Tape) Capacity (megabytes) Figure 6.1 Typical memory hierarchy108 MEMORY SYSTEM DESIGN Iphenomenon given address referenced, likely addresses near referenced within short period time, example, consecu-tive instructions straightline program. Temporal locality , hand, refers phenomenon particular memory item referenced, mostlikely referenced next, example, instruction program loop. sequence events takes place processor makes request item follows. First, item sought ﬁrst memory level memoryhierarchy. probability ﬁnding requested item ﬁrst level calledthehit ratio ,h 1. probability ﬁnding (missing) requested item ﬁrst level memory hierarchy called miss ratio ,( 12h1). requested item causes “ miss,” sought next subsequent memory level. probability ﬁnding requested item second memory level, hitratio second level, h 2. miss ratio second memory level (1/C0h2). process repeated item found. Upon ﬁnding requested item, brought sent processor. memory hierarchy consists ofthree levels, average memory access time expressed follows: av¼h1/C2t1þ(1/C0h1)½t1þh2/C2t2þ(1/C0h2)(t2þt3)/C138 ¼t1þ(1/C0h1)½t2þ(1/C0h2)t3/C138 average access time memory level deﬁned time required accesso n ew r di nt h tl e v e l .I nt h se q u n , 1,t2,t3represent, respectively, access times three levels. 6.2. CACHE MEMORY Cache memory owes introduction Wilkes back 1965. time, Wilkes distinguished two types main memory: conventional slave memory . Wilkes terminology, slave memory second level unconventional high-speed memory, nowadays corresponds called cache memory (the term cache means safe place hiding storing things). idea behind using cache ﬁrst level memory hierarchy keep information expected used frequently CPU cacheTABLE 6.1 Memory Hierarchy Parameters Access type Capacity Latency Bandwidth Cost /MB CPU registers Random 64–1024 bytes 1–10 ns System clock rateHigh Cache memory Random 8–512 KB 15–20 ns 10–20 MB /s $500 Main memory Random 16–512 MB 30–50 ns 1–2 MB /s $20–50 Disk memory Direct 1–20 GB 10–30 ms 1–2 MB /s $0.25 Tape memory Sequential 1–20 TB 30–10,000 ms 1–2 MB /s $0.0256.2. CACHE MEMORY 109(a small high-speed memory near CPU). end result given time active portion main memory duplicated cache. Therefore,when processor makes request memory reference, request ﬁrstsought cache. request corresponds element currently resid-ing cache, call cache hit. hand, request corre-sponds element currently cache, call cache miss.Acache hit ratio ,h c, deﬁned probability ﬁnding requested element cache. cache miss ratio (1/C0hc) deﬁned probability ﬁnding requested element cache. case requested element found cache, brought subsequent memory level memory hierarchy. Assuming thatthe element exists next memory level, is, main memory, hasto brought placed cache. expectation next requested elementwill residing neighboring locality current requested element (spatiallocality), upon cache miss actually brought main memory block elements contains requested element. advantage transferring block main memory cache visible could poss- ible transfer block using one main memory access time. possibilitycould achieved increasing rate information transferredbetween main memory cache. One possible technique used toincrease bandwidth memory interleaving . achieve best results, assume block brought main memory cache, upon cachemiss, consists elements stored different memory modules, is, whereby consecutive memory addresses stored successive memory modules. Figure 6.2 illustrates simple case main memory consisting eight memorymodules. assumed case block consists 8 bytes. introduced basic idea leading use cache memory, would like assess impact temporal spatial locality performance ofthe memory hierarchy. order make assessment, limit M7M6 M5M4 M3M2M1M0 Byte Main memory Block Cache Figure 6.2 Memory interleaving using eight modules110 MEMORY SYSTEM DESIGN Ideliberation simple case hierarchy consisting two levels, is, cache main memory. assume main memory access time tm cache access time tc. measure impact locality terms average access time, deﬁned average time required access element (aword) requested processor two-level hierarchy. 6.2.1. Impact Temporal Locality case, assume instructions program loops, executed many times, example, ntimes, loaded cache, used replaced new instructions. average access time, av,i sg v e nb tav¼ntcþtm n¼tcþtm n deriving expression, assumed requested memory elementhas created cache miss, thus leading transfer main memory block timet m. Following that, naccesses made requested element, taking tc. expression reveals number repeated accesses, n,i n c r e e , average access time decreases, desirable feature memory hierarchy. 6.2.2. Impact Spatial Locality case, assumed size block transferred main memory cache, upon cache miss, melements. also assume due spatial locality, melements requested, one time, processor. Based assumptions, average access time, tav, given tav¼mtcþtm m¼tcþtm deriving expression, assumed requested memory elementhas created cache miss, thus leading transfer main memory block, con-sisting melements, time m. Following that, maccesses, one elements constituting block, made. expression reveals asthe number elements block, m, increases, average access time decreases, desirable feature memory hierarchy. 6.2.3. Impact Combined Temporal Spatial Locality case, assume element requested processor created cache miss leading transfer block, consisting elements , cache (that take m). Now, due spatial locality, melements constituting block requested, one time, processor (requiring mtc). Following that, orig- inally requested element accessed ( n21) times (temporal locality), is, a6.2. CACHE MEMORY 111total ntimes access element. Based assumptions, average access time, tav, given tav¼mtcþtm m/C16/C17 þ(n/C01)tc n¼tcþtm m/C16/C17 þ(n/C01)tc n¼tm nmþtc simplifying assumption expression assume tm¼mtc. case expression simplify tav¼mtc nmþtc¼tcþtc n¼nþ1 ntc expression reveals number repeated accesses nincreases, average access time approach tc. signiﬁcant performance improvement. clear discussion requests items exist cache (cache miss) occur, blocks would brought cache. raise two basic questions: place incomingmain memory block cache? case cache totallyﬁlled, cache block incoming main memory block replace? Place-ment incoming blocks replacement existing blocks performed accord-ing speciﬁc protocols (algorithms). protocols strongly related theinternal organization cache. Cache internal organization discussed following subsections. However, discussing cache organization, ﬁrst introduce cache-mapping function. 6.2.4. Cache-Mapping Function Without loss generality, present cache-mapping function taking consider- ation interface two successive levels memory hierarchy: primarylevel secondary level. focus interface cache mainmemory, cache represents primary level, main memory rep-resents secondary level. principles apply interface anytwo memory levels hierarchy. following discussion, focus atten-tion interface cache main memory. noted request accessing memory element made processor issuing address requested element. address issued processor may correspond element exists currently thecache (cache hit); otherwise, may correspond element currently resid-ing main memory. Therefore, address translation made order todetermine whereabouts requested element. one functions performed memory management unit (MMU). schematic addressmapping function shown Figure 6.3. ﬁgure, system address represents address issued processor requested element. address used address translation functioninside MMU. address translation reveals issued address correspondsto element currently residing cache, element made112 MEMORY SYSTEM DESIGN Iavailable processor. If, hand, element currently cache, brought (as part block) main memory placed cache element requested made available processor. 6.2.5. Cache Memory Organization three main different organization techniques used cache memory. three techniques discussed below. techniques differ two main aspects: 1. criterion used place, cache, incoming block main memory. 2. criterion used replace cache block incoming block (on cache full). Direct Mapping simplest among three techniques. simplicity stems fact places incoming main memory block speciﬁcﬁxed cache block location. placement done based ﬁxed relation incoming block number, i, cache block number, j, number cache blocks, N: j¼imod N Example 1 Consider, example, case main memory consisting 4K blocks, cache memory consisting 128 blocks, block size 16 words. Figure 6.4 shows division main memory cache according thedirect-mapped cache technique. ﬁgure shows, total 32 main memory blocks map given cache block. example, main memory blocks 0, 128, 256, 384, ..., 3968 map cache block 0. therefore call direct-mapping technique aThe requested elementMemory Management Unit (MMU) SecondaryLevel Block Primary LevelAddress inPrimary MemoryMiss SystemAddressTranslationFunction Hit Figure 6.3 Address mapping operation6.2. CACHE MEMORY 113many-to-one mapping technique. main advantage direct-mapping tech- nique simplicity determining place incoming main memoryblock cache. main disadvantage inefﬁcient use cache. Thisis according technique, number main memory blocks may com-pete given cache block even exist empty cache blocks. dis-advantage lead achieving low cache hit ratio. According direct-mapping technique MMU interprets address issued processor dividing address three ﬁelds shown Figure 6.5. Thelengths, bits, ﬁelds Figure 6.5 are: 1. Word ﬁeld ¼log 2B, Bis size block words. 2. Block ﬁeld ¼log2N, Nis size cache blocks. 3. Tag ﬁeld ¼log2(M/N), Mis size main memory blocks. 4. number bits main memory address ¼log2(B/C2M) noted total number bits computed ﬁrst three equations add length main memory address. used check correctness computation. Example 2 Compute four parameters Example 1. Word ﬁeld ¼log2B¼log216¼log224¼4 bits Block ﬁeld ¼log2N¼log2128¼log227¼7 bits Tag ﬁeld ¼log2(M/N)¼log2(22/C2210/27)¼5 bits number bits main memory address ¼log2(B/C2M)¼log2 (24/C2212)¼16 bits.3Tag 1 0 31384Cache 1290 1 2 126 127 4095Main Memory 0 1 2128 129 130256 257 258384 385 3863968 127 255 383 4095 0123 3 1 Figure 6.4 Mapping main memory blocks cache blocks Figure 6.5 Direct-mapped address ﬁelds114 MEMORY SYSTEM DESIGN IHaving shown division main memory address, proceed explain protocol used MMU satisfy request made processor accessing given element. illustrate protocol using parameters given inthe example presented (Fig. 6.6). steps protocol are: 1. Use Block ﬁeld determine cache block contain element requested processor. Block ﬁeld used directly determine cache block sought, hence name technique: direct-mapping. 2. Check corresponding Tag memory see whether match content Tag ﬁeld . match two indicates targeted cache block determined step 1 currently holding main memory element requested processor, is, cache hit . 3. Among elements contained cache block, targeted element selected using Word ﬁeld . 4. step 2, match found, indicates cache miss . Therefore, required block brought main memory, deposited cache, targeted element made available processor. cache Tag memory cache block memory updated accordingly. Figure 6.6 Direct-mapped address translation6.2. CACHE MEMORY 115The direct-mapping technique answers placement incoming main memory block cache question, also answers replacement question.Upon encountering totally ﬁlled cache new main memory block bebrought, replacement trivial determined equation j¼imod N. main advantages direct-mapping technique simplicity measured terms direct determination cache block; search needed. alsosimple terms replacement mechanism. main disadvantage tech- nique expected poor utilization cache memory. represented terms possibility targeting given cache block, requires frequentreplacement blocks rest cache used. Consider, example,the sequence requests made processor elements held mainmemory blocks 1, 33, 65, 97, 129, 161. Consider also cache size is32 blocks. clear blocks map cache block number 1. There-fore, blocks compete cache block despite fact theremaining 31 cache blocks used. expected poor utilization cache direct mapping technique mainly due restriction placement incoming main memoryblocks cache (the many-to-one property). restriction relaxed,that is, make possible incoming main memory block placedin empty (available) cache block, resulting technique would ﬂex-ible efﬁcient utilization cache would possible. ﬂexible tech-nique, called Associative Mapping technique, explained next. Fully Associative Mapping According technique, incoming main memory block placed available cache block. Therefore, addressissued processor need two ﬁelds. TagandWord ﬁelds. ﬁrst uniquely identiﬁes block residing cache. Thesecond ﬁeld identiﬁes element within block requested pro-cessor. MMU interprets address issued processor dividing itinto two ﬁelds shown Figure 6.7. length, bits, ﬁelds inFigure 6.7 given by: 1. Word ﬁeld ¼log 2B, Bis size block words 2. Tag ﬁeld ¼log2M, Mis size main memory blocks 3. number bits main memory address ¼log2(B/C2M) noted total number bits computed ﬁrst two equations add length main memory address. used check correctness computation. Figure 6.7 Associative-mapped address ﬁelds116 MEMORY SYSTEM DESIGN IExample 3 Compute three parameters memory system following speciﬁcation: size main memory 4K blocks, size cache 128 blocks, block size 16 words. Assume system uses associativemapping. Word ﬁeld ¼log 2B¼log216¼log224¼4 bits Tag ﬁeld ¼log2M¼log227/C2210¼12 bits number bits main memory address ¼log2(B/C2M)¼log2 (24/C2212)¼16 bits. shown division main memory address, proceed explain protocol used MMU satisfy request made processor accessing given element. illustrate protocol using parameters given example presented (see Fig. 6.8). steps protocol are: 1. Use Tag ﬁeld search Tag memory match tags stored. 2. match tag memory indicates corresponding targeted cache block determined step 1 currently holding main memory elementrequested processor, is, cache hit . Figure 6.8 Associative-mapped address translation6.2. CACHE MEMORY 1173. Among elements contained cache block, targeted element selected using Word ﬁeld . 4. step 2, match found, indicates cache miss . Therefore, required block brought main memory, deposited ﬁrst available cache block, targeted element (word) made available tothe processor. cache Tag memory cache block memory updated accordingly. noted search made step 1 requires matching tag ﬁeld address every entry tag memory . search, done sequentially, could lead long delay. Therefore, tags stored associative (content addressable) memory . allows entire contents tag memory searched parallel (associatively), hence name, associativemapping. noted that, regardless cache organization used, mechanism needed ensure accessed cache block contains valid information. val- idity information cache block checked via use single bit cache block, called valid bit . valid bit cache block updated way valid bit ¼1, corresponding cache block car- ries valid information; otherwise, information cache block invalid. computer system powered up, valid bits made equal 0, indicatingthat carry invalid information. blocks brought cache, statusesare changed accordingly indicate validity information contained. main advantage associative-mapping technique efﬁcient use cache. stems fact exists restriction placeincoming main memory blocks. unoccupied cache block potentially beused receive incoming main memory blocks. main disadvantage ofthe technique, however, hardware overhead required perform associat-ive search conducted order ﬁnd match tag ﬁeld tagmemory discussed above. compromise simple inefﬁcient direct cache organization involved efﬁcient associative cache organization achieved con- ducting search limited set cache blocks knowing ahead time cache incoming main memory block placed. isthe basis set-associative mapping technique explained next. Set-Associative Mapping set-associative mapping technique, cache divided number sets. set consists number blocks. given main memory block maps speciﬁc cache set based equations¼imod S, Sis number sets cache, iis main memory block number, sis specﬁc cache set block imaps. However, incoming block maps block assigned cache set. Therefore, address issued processor divided three distinct ﬁelds. Tag,Set, andWord ﬁelds. Setﬁeld used uniquely identify speciﬁc cache set118 MEMORY SYSTEM DESIGN Ithat ideally hold targeted block. Tagﬁeld uniquely identiﬁes tar- geted block within determined set. Word ﬁeld identiﬁes element (word) within block requested processor. According set-associative mapping technique, MMU interprets address issued processor bydividing three ﬁelds shown Figure 6.9. length, bits, ofthe ﬁelds Figure 6.9 given 1. Word ﬁeld ¼log 2B, Bis size block words 2. Set ﬁeld ¼log2S, Sis number sets cache 3. Tag ﬁeld ¼log2(M/S), Mis size main memory blocks. S¼N=Bs, Nis number cache blocks Bsis number blocks per set 4. number bits main memory address ¼log2(B/C2M) noted total number bits computed ﬁrst three equations add length main memory address. used check correctness computation. Example 4 Compute three parameters (Word, Set, Tag) memory system following speciﬁcation: size main memory 4K blocks, size cache 128 blocks, block size 16 words. Assume system uses set-associative mapping four blocks per set. S¼128 4¼32 sets : 1. Word ﬁeld ¼log2B¼log216¼log224¼4 bits 2. Set ﬁeld ¼log232¼5 bits 3. Tag ﬁeld ¼log2(4/C2210/32)¼7 bits number bits main memory address ¼log2(B/C2M)¼log2 (24/C2212)¼16 bits. shown division main memory address, proceed explain protocol used MMU satisfy request made processor accessing given element. illustrate protocol using parameters given example presented (see Fig. 6.10). steps protocol are: 1. Use Set ﬁeld (5 bits) determine (directly) speciﬁed set (1 32 sets). Figure 6.9 Set-associative-mapped address ﬁelds6.2. CACHE MEMORY 1192. Use Tag ﬁeld ﬁnd match (four) blocks deter- mined set. match tag memory indicates speciﬁed set deter- mined step 1 currently holding targeted block, is, cache hit . 3. Among 16 words (elements) contained hit cache block, requested word selected using selector help Word ﬁeld .Ta gCache Block # 0Main Memory Address Tag Field 75 4Set Field Word Field 03 1 Step #1 Step #2 Associative Search setBlock # 1 Block # 2 Block # 3 7 bitsSet 0 Set 1 Selector Requested WordSet Set 311271 1290 384 Figure 6.10 Set-associative-mapped address translation120 MEMORY SYSTEM DESIGN I4. step 2, match found, indicates cache miss . Therefore, required block brought main memory, deposited speciﬁed set ﬁrst, targeted element (word) made available tothe processor. cache Tag memory cache block memory updated accordingly. noted search made step 2 requires matching tag ﬁeldof address every entry tag memory speciﬁed set. search performed parallel (associatively) set, hence name,set-associative mapping. hardware overhead required performing associ-ative search within set order ﬁnd match tag ﬁeld tagmemory complex used case fully associative technique. set-associative-mapping technique expected produce moderate cache utilization efﬁciency, is, efﬁcient fully associative technique andnot poor direct technique. However, technique inherits simplicity direct mapping technique terms determining target set. overall qualitative comparison among three mapping techniques shown Table 6.2. Owing moderate complexity moderate cache utilization, set-associative technique used Intel Pentium line processors. discussion shows associative-mapping set-associative techniques answer question placement incoming main memoryblock cache. important question posed beginning discussion cache memory replacement. Speciﬁcally, upon encoun- tering totally ﬁlled cache new main memory block brought, cache blocks selected replacement? discussed below. 6.2.6. Replacement Techniques number replacement techniques used. include randomly selected block ( random selection ), block cache longest ( ﬁrst-in- ﬁrst-out, FIFO ), block used least residing cache ( least recently used, LRU ). Let us assume computer system powered up, random number generator starts generating numbers 0 ( N21). name indicates, TABLE 6.2 Qualitative Comparison Among Cache Mapping Techniques Mapping technique SimplicityAssociative tag searchExpected cache utilizationReplacement technique Direct Yes None Low neededAssociative Involved High Yes Set-associative Moderate Moderate Moderate Yes6.2. CACHE MEMORY 121random selection cache block replacement done based output random number generator time replacement. technique simple anddoes require much additional overhead. However, main shortcoming itdoes take locality consideration. Random techniques found effec-tive enough ﬁrst used Intel iAPX microprocessor series. FIFO technique takes time spent block cache measure replacement. block cache longest selected replace-ment regardless recent pattern access block. technique requireskeeping track lifetime cache block. Therefore, simple therandom selection technique. Intuitively, FIFO technique reasonable usefor straightline programs locality reference concern. According LRU replacement technique, cache block recently used least selected replacement. Among three replacementtechniques, LRU technique effective. history block usage (as criterion replacement) taken consideration. LRU algorithm requires use cache controller circuit keeps track refer-ences blocks residing cache. achieved anumber possible implementations. Among implementations use ofcounters. case cache block assigned counter. Upon cache hit,the counter corresponding block set 0, counters smallervalue original value counter hit block incremented 1, counters larger value kept unchanged. Upon cache miss, block whose counter showing maximum value chosen replacement, counter set 0, counters incremented 1. introduced three cache mapping technique, offer follow- ing example, illustrates main observations made three techniques. Example 5 Consider case 4 /C28 two-dimensional array numbers, A. Assume number array occupies one word array elements stored column-major order main memory location 1000 location 1031. cache consists eight blocks consisting two words. Assumealso whenever needed, LRU replacement policy used. would like exam- ine changes cache three mapping techniques used asthe following sequence requests array elements made processor: 0,0,a0,1,a0,2,a0,3,a0,4,a0,5,a0,6,a0,7 a1,0,a1,1,a1,2,a1,3,a1,4,a1,5,a1,6,a1,7 Solution distribution array elements main memory shown Figure 6.11. Shown also status cache requests made. Direct Mapping Table 6.3 shows 16 cache misses (not single cache hit) number replacements made 12 (these shown122 MEMORY SYSTEM DESIGN Itinted). also shows available eight cache blocks, four (0, 2, 4, 6) used, remaining four inactive time. represents a50% cache utilization. Fully Associative Mapping Table 6.4 shows eight cache hits (50% total number requests) replacements made. also shows 100% cache utilization. Set-Associative Mapping (With Two Blocks per Set) Table 6.5 shows 16 cache misses (not single cache hit) number replace- ments made 12 (these shown tinted). also shows available four cache sets, two sets used, remaining two inactive time. represents 50% cache utilization. Figure 6.11 Array elements main memory6.2. CACHE MEMORY 1236.2.7. Cache Write Policies discussed main issues related cache mapping techniques repla- cement policies, would like address important related issue, is,cache coherence. Coherence cache word copy mainmemory maintained times, possible. number policies (techniques) used performing write operations main memory blocks residing cache. policies determine degree coherence thatTABLE 6.3 Direct Mapping RequestCache hit/missMM block number (i)Cache block number (j)Cache status BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7 A(0,0) Miss 0 0 0 1 00 A(0,1) Miss 2 2 0 1 0 1 00 11 A(0,2) Miss 4 4 0 1 0 1 0 1 00 11 22 A(0,3) Miss 6 6 0 1 0 1 0 1 0 1 00 11 22 33 A(0,4) Miss 8 0 0 1 0 1 0 1 0 1 44 11 22 33 A(0,5) Miss 10 2 0 1 0 1 0 1 0 1 44 55 22 33 A(0,6) Miss 12 4 0 1 0 1 0 1 0 1 44 55 66 33 A(0,7) Miss 14 6 0 1 0 1 0 1 0 1 44 55 66 77 A(1,0) Miss 0 0 0 1 0 1 0 1 0 1 00 55 66 77 A(1,1) Miss 2 2 0 1 0 1 0 1 0 1 00 11 66 66 A(1,2) Miss 4 4 0 1 0 1 0 1 0 1 00 11 22 66 A(1,3) Miss 6 6 0 1 0 1 0 1 0 1 00 11 22 33 A(1,4) Miss 8 0 0 1 0 1 0 1 0 1 44 11 22 33 A(1,5) Miss 10 2 0 1 0 1 0 1 0 1 44 55 22 33 A(1,6) Miss 12 4 0 1 0 1 0 1 0 1 44 55 66 33 A(1,7) Miss 14 6 0 1 0 1 0 1 0 1 44 55 66 77124 MEMORY SYSTEM DESIGN Ican maintained cache words counterparts main memory. following paragraphs, discuss write policies. particular, dis-cuss two main cases: cache write policies upon cache hit cache write pol-icies upon cache miss. also discuss cache read policy upon cache miss.Cache read upon cache hit straightforward. Cache Write Policies Upon Cache Hit essentially two possible write policies upon cache hit. write-through write-back .TABLE 6.4 Fully Associative Mapping RequestCache hit/missMM block number (i)Cache block numberCache status BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7 A(0,0) Miss 0 0 0 1 00 A(0,1) Miss 2 1 0101 0011 A(0,2) Miss 4 2 010101 001122 A(0,3) Miss 6 3 01010101 00112233 A(0,4) Miss 8 4 0101010101 0011223344 A(0,5) Miss 10 5 010101010101 001122334455 A(0,6) Miss 12 6 01010101010101 00112233445566 A(0,7) Miss 14 7 0101010101010101 0011223344556677 A(1,0) Hit 0 0 0101010101010101 0011223344556677 A(1,1) Hit 2 1 0101010101010101 0011223344556677 A(1,2) Hit 4 2 0101010101010101 0011223344556677 A(1,3) Hit 6 3 0101010101010101 0011223344556677 A(1,4) Hit 8 4 0101010101010101 0011223344556677 A(1,5) Hit 10 5 0101010101010101 0011223344556677 A(1,6) Hit 12 6 0101010101010101 0011223344556677 A(1,7) Hit 14 7 0101010101010101 00112233445566776.2. CACHE MEMORY 125In write-through policy, every write operation cache repeated main memory time. write-back policy, writes made tothe cache. write main memory postponed replacement isneeded. Every cache block assigned bit, called dirty bit , indicate least one write operation made block residing cache. replacement time, dirty bit checked; set, block written back main memory, otherwise, simply overwritten incomingTABLE 6.5 Set-Associative Mapping RequestCache hit/missMM block number (i)Cache block numberCache status Set # 0 Set # 1 Set # 2 Set # 3 BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7 A(0,0) Miss 0 0 0 1 00 A(0,1) Miss 2 2 0 1 0 1 00 11 A(0,2) Miss 4 0 0101 01 0022 11 A(0,3) Miss 6 2 0101 0101 0022 1133 A(0,4) Miss 8 0 0 1 0 1 0101 4422 1133 A(0,5) Miss 10 2 0101 01014422 5533 A(0,6) Miss 12 0 0 1 0 1 0101 4466 5533 A(0,7) Miss 14 2 0101 0101 4466 5577 A(1,0) Miss 0 0 0 1 0 1 0101 0066 5577 A(1,1) Miss 2 2 0101 0101 0066 1177 A(1,2) Miss 4 0 0 1 0 1 0101 0022 1177 A(1,3) Miss 6 2 0101 0101 0022 1133 A(1,4) Miss 8 0 0 1 0 1 0101 4422 1133 A(1,5) Miss 10 2 0101 0101 4422 5533 A(1,6) Miss 12 0 0 1 0 1 0101 4466 5533 A(1,7) Miss 14 2 0101 01014466 4422126 MEMORY SYSTEM DESIGN Iblock. write-through policy maintains coherence cache blocks counterparts main memory expense extra time needed towrite main memory. leads increase average access time.On hand, write-back policy eliminates increase averageaccess time. However, coherence guaranteed time replacement. Cache Write Policy Upon Cache Miss Two main schemes used. write-allocate whereby main memory block brought cache updated. scheme called write-no-allocate whereby missed main memory block updated main memory brought cache. general, write-through caches use write-no-allocate policy write-back caches use write-allocate policy . Cache Read Policy Upon Cache Miss Two possible strategies used. ﬁrst, main memory missed block brought cache requiredword forwarded immediately CPU soon available. second strategy, missed main memory block entirely stored cache required word forwarded CPU. discussed issues related design analysis cache memory, brieﬂy present formulae average access time memory hierarchy underdifferent cache write policies. Case No. 1: Cache Write-Through Policy Write-allocate case, average access time memory system given a¼tcþ(1/C0h)tbþw(tm/C0tc) tbis time required transfer block cache, ( tm/C0tc) additional time incurred due write operations, wis fraction write oper- ations. noted data path organization allow, tb¼tm; otherwise, tb¼Btm, Bis block size words. Write-no-allocate case, average access time expressed ta¼tcþ(1/C0w)(1/C0h)tbþw(tm/C0tc) Case No. 2: Cache Write-Back Policy average access time system uses write-back policy given ta¼tcþ(1/C0h)tbþwb(1/C0h)tb, wb probability block altered cache.6.2. CACHE MEMORY 1276.2.8. Real-Life Cache Organization Analysis Intel’s Pentium IV Processor Cache Intel’s Pentium 4 processor uses two- level cache organization shown schematically Figure 6.12. ﬁgure, L1 represents 8 KB data cache. four-way set-associative. block size 64 bytes. Consider following example (tailored L1 Pentium cache). Example 6 Cache organization Set-associative Main Memory size 16 MB Cache L1 size 8 KB Number blocks per set Four CPU addressing Byte addressable main memory address divided three ﬁelds: Word, Set, Tag (Fig. 6.13). length ﬁeld computed follows. Number main memory blocks M¼224=26¼218blocks Number cache blocks N¼213=26¼128 blocks S¼128 =4¼32 sets Set ﬁeld ¼log232¼5 bits Word ﬁeld ¼log2B¼log264¼log226¼6 bits Tag ﬁeld ¼log2(218/25)¼13 bits Main memory address ¼log2(B/C2M)¼log2(26/C2218)¼24 bits second cache level Figure 6.11 L2. called advanced transfer cache . organized eight-way set-associative cache 256 KB total size 128-byte block size. Following similar set steps shown forthe L1 level, obtain following: Number main memory blocks M¼2 24=27¼217blocks Processor Cache Level 1 L 1Cache Level 2 L 2Main Memory Figure 6.12 Pentium IV two-level cache128 MEMORY SYSTEM DESIGN INumber cache blocks N¼218=27¼211blocks S¼211=23¼28sets Set ﬁeld ¼log228¼8 bits Word ﬁeld ¼log2B¼log2128¼log227¼7 bits Tag ﬁeld ¼log2(217/28)¼9 bits following tables summarize L1 L2 Pentium 4 cache performance terms cache hit ratio cache latency. CPUL1 Hit ratioL2 Hit ratioL1 LatencyL2 LatencyAverage latency Pentium 4 1.5 GHz 90% 99% 1.33 ns 6.0 ns 1.8 ns PowerPC 604 Processor Cache PowerPC cache divided data instruction caches, called Harvard Organization . instruction data caches organized 16 KB four-way set-associative. following table sum-marizes PowerPC 604 cache basic characteristics. Cache organization Set-associative Block size 32 bytes Main memory size 4 GB ( M¼128 Mega blocks) Cache size 16 KB ( N¼512 blocks) Number blocks per set Four Number cache sets (S) 128 sets main memory address divided three ﬁelds: Word, Set, Tag (Fig. 6.13). length ﬁeld computed follows: Number main memory blocks M¼232=25¼227blocks Number cache blocks N¼214=25¼512 blocks S¼512 =4¼128 sets Set ﬁeld ¼log2128¼7 bits Word ﬁeld ¼log2B¼log232¼log225¼5 bits Tag ﬁeld ¼log2(227/27)¼20 bits Main memory address ¼log2(B/C2M)¼log2(25/C2227)¼32 bits Figure 6.13 Division main memory address6.2. CACHE MEMORY 129PMC-Sierra RM7000A 64-bit MIPS RISC Processor RM7000 uses different cache organization compared Intel PowerPC. case, three separate caches included. are: 1. Primary instruction cache: 16 KB, four-way set-associative cache 32-byte block size (eight instructions) 2. Primary data cache: 16 KB, four-way set-associative cache 32 bytes block size (eight words) 3. Secondary cache: 256 KB, four-way set-associative cache instruc- tions data addition three on-chip caches, RM7000 provides dedicated tertiarycache interface, supports tertiary cache sizes 512 KB, 2 MB, 8 MB.This tertiary cache accessed secondary cache miss. primary caches require one cycle access. caches 64-bit read data path 128-bit write data path. caches accessed sim-ultaneously, giving aggregate bandwidth 4 GB per second. secondarycache 64-bit data path accessed primary cache miss. three-cycle miss penalty. Owing unusual cache organization RM7000, uses two cache access schemes described below. Non-Blocking Caches scheme, caches stall miss, rather processor continues operate primary caches one following events takes place: 1. Two cache misses outstanding third load /store instruction appears instruction bus. 2. subsequent instruction requires data either instructions caused cache miss. use nonblocking caches improves overall performance allowing cache continue operating even though cache miss occurred. Cache Locking scheme, critical code data segments locked pri- mary secondary caches. locked contents updated write hit, cannot selected replacement miss. RM7000 allows three caches locked separately. However, two available four sets eachcache locked. particular, RM7000 allows maximum 128 KB data orcode locked secondary cache, maximum 8 KB code locked instruction cache, maximum 8 KB data locked data cache. 6.3. SUMMARY chapter, consider design analysis ﬁrst level memory hierarchy, is, cache memory. context, locality issues were130 MEMORY SYSTEM DESIGN Idiscussed effect average access time explained. Three cache mapping techniques, namely direct, associative, set-associative mappingswere analyzed performance measures compared. also introducedthree replacement techniques: Random, FIFO, LRU replacement. impact ofthe three techniques cache hit ratio analyzed. Cache writing policies werealso introduced analyzed. discussion cache ended presentation ofthe cache memory organization characteristics three real-life examples: Pentium IV, PowerPC, PMC-Sierra RM7000, processors. Chapter 7, discuss issues related design aspects internal externalorganization main memory. also discuss issues related virtualmemory. EXERCISES 1. Determine memory interleaving factor required obtain average access time less 60 ns given main memory access time 100 ns cache access time 20 ns. averageaccess time resulting system? 2. average access time system three levels memory, cache memory, semiconductor main memory, magnetic disk second-ary memory, access times memories 20 ns, 100 ns, 1 ms,respectively. cache hit ratio 90% main memory hit ratio 95%. 3. computer system MM consisting 16 MB 32-bit words. also 8 KB cache. Assume computer uses byte-addressable mechan-ism. Determine number bits ﬁeld address ofthe following organizations: (a) Direct mapping block size one word (b) Direct mapping block size eight words (c) Associative mapping block size eight words (d) Set-associative mapping set size four block block size one word. 4. Consider execution following program segment 8 /C28 array A. i: ¼0t o7d SUM:¼0 j: ¼0t o7d SUM:¼SUMþA(i,j) End AVE(i): ¼SUM/8 End Assume main memory divided eight interleaved memory blocks cache memory block consists eight elements. Assume also cache memory access time 10 ns memory access time ten times cache memory access time. Computethe average access time per element array A.EXERCISES 1315. Consider execution following program segment 4 /C210 array A. two-dimensional array stored main memory column- major order. Assume eight blocks cache, justone word, LRU used replacement. Show trace con-tents cache memory blocks different values indices jandk assuming three different cache memory organizations, is, direct ,associ- ative , set-associative mapping . Provide observations results obtained. SUM:¼0 j: ¼0t o9d SUM:¼SUMþA(0,j) End AVE:¼SUM/10 k: ¼0t o9d A(0,k): ¼A(0, k) /AVE End 6. Consider case 4 /C28 two-dimensional array numbers, A. Assume number array occupies one word array elements stored row-major main memory location 1000 location 1031.The cache consists eight blocks consisting four words. Assume alsothat whenever needed, LRU replacement policy used. would like toexamine changes cache three mapping tech- niques used following sequence requests array elements made: 0,0,a0,1,a0,2,a0,3,a0,4,,a0,5,a0,6,a0,7 a1,0,a1,1,a1,2,a1,3,a1,4,a1,5,a1,6,a1,7 a2,0,a2,1,a2,2,a2,3,a2,4,a2,5,a2,6,a2,7 a3,0,a3,1,a3,2,a3,3,a3,4,a3,5,a3,6,a3,7 Show status cache given requests made, number replacements made, estimate cache utilization. 7. computer system MM consisting 1 16-bit words. also 4 K word cache organized block-set-associative manner, four blocks per set 64 words per block. Assume cache initially empty. Suppose CPU fetches 4352 words locations 0, 1,2,..., 4351 (in order). repeats fetch sequence nine times. cache 10 times faster MM, estimate improvementfactor resulting use cache. Assume whenever block tobe brought MM correspondence set cache full, new block replaces least recently used block set. Repeat case using recently used replacement technique; is, cache full, new block replace recently usedblock cache. Note: example quoted Reference #3.132 MEMORY SYSTEM DESIGN IREFERENCES READING S. D. Burd, Systems Architecture , 3rd ed., Thomson Learning Ltd, Boston, USA, 2001. H. Cragon, Memory Systems Pipelined Processors , Jones Bartlett: Sudbury, MA, 1996. V. C. Hamacher, Z. G. Vranesic, S. G. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002. J. L. Hennessy, D. A. Patterson, Computer Architecture: Quantitative Approach , Morgan Kaufmann, San Francisco, CA, 1996. V. P. Heuring, H. F. Jordan, Computer Systems Design Architecture , Addison- Wesley, NJ, USA, 1997. D. A. Patterson, J. L. Hennessy, Computer Organization & Design: Hardware / Software Interface , Morgan Kaufmann, San Mateo, CA, 1994. H. S. Stone, High-Performance Computer Architecture , Addison-Wesley, Amsterdam, Netherlands, 1987. M. Wilkes, Slave memories dynamic storage allocation, IEEE Trans. Electron. Comput., EC-14(2), 270–271, (1965). B. Wilkinson, Computer Architecture: Design Performance , Prentice-Hall, Hertfordshire, UK, 1996. Websites http://www.sysopt.com http://www.intel.com http://www.AcerHardware.com http://www.pmc-sierra.com /products /details /rm7000a http://physinfo.ulb.ac.be /divers_html /PowerPC_Programming_Info /into_to_ppc / ppc2_hardware.htmlREFERENCES READING 133& CHAPTER 7 Memory System Design II Chapter 6 introduced concept memory hierarchy. also character- ized memory hierarchy terms locality reference impact aver-age access time. moved cover different issues related ﬁrst levelof hierarchy, is, cache memory (the reader advised carefully reviewChapter 6 proceeding chapter). chapter, continue cover-age different levels memory hierarchy. particular, start discus-sion issues related design analysis (main) memory unit. Issuesrelated virtual memory design discussed. brief coverage different read-only memory (ROM) implementations provided end chapter. 7.1. MAIN MEMORY name implies, main memory provides main storage computer. Figure 7.1 shows typical interface main memory CPU.Two CPU registers used interface CPU main memory. memory address register (MAR) memory data register (MDR). MDR used hold data stored /or retrieved /from memory location whose address held MAR. possible visualize typical internal main memory structure consisting rows columns basic cells. cell capable storing one bit infor-mation. Figure 7.2 provides conceptual internal organization memory chip. Inthis ﬁgure, cells belonging given row assumed form bits givenmemory word. Address lines n/C01An/C02...A1A0are used inputs address decoder order generate word select lines W2n/C01...W1W0. given word select line common memory cells row. given time,the address decoder activates one word select line deactivating remaining lines. word select line used enable cells row read write. Data (bit) lines used input output contents cells. Eachmemory cell connected two data lines. given data line common allcells given column. 135Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.In static CMOS technology, main memory cell consists six transistors shown Figure 7.3. six transistor static CMOS memory cell consists two inverters back back. noted cell could exist one thetwo stable states. example, Figure 7.3 A¼1, transistor N 2will point B¼0, turn cause transistor P1to on, thus causing pointA¼1. represents cell stable state, call state 1. similar way A0 A1 A2 Data LinesAn-1 W2n-1· · ·· ··W 2W1W0Cell Cell Cell Figure 7.2 conceptual internal organization memory chipCPU Main Memory MARb n b MDRb linesn lines0 1 2 2n− 1D0 − Db−1A0 − An−1 R / W Figure 7.1 typical CPU main memory interface136 MEMORY SYSTEM DESIGN IIone show A¼0, B¼1, represents cell stable state, call state 0. two transistors N3andN4are used connect cell two data (bit) lines. Normally (if word select activated) two transistors turned off, thus protecting cell signal values carried data lines.The two transistors turned word select line activated. takesplace two transistors turned depend intended memory operation shown below. Read operation: 1. lines band bare precharged high. 2. word select line activated, thus turning transistors N3andN4. 3. Depending internal value stored cell, point A(B) lead discharge line b(/C22b b). Write operation: 1. bit lines precharged b(/C22b b)¼1(0). 2. word select line activated, thus turning transistors N3andN4. 3. bit line precharged 0 force point A(B), 1, 0. internal organization memory array satisfy important memory design factor, is, efﬁcient utilization memory chip. Consider, example, 1K /C24 memory chip. Using organization shown Figure 7.2, memory array organized 1K rows cells, consisting four cells. chip 10 pins address four pins data. However, may lead best utilization chip area.Data (bit) line bData (bit) line b N3P1 P2 N1AB Word select lineN4 N2 Figure 7.3 Static CMOS memory cell7.1. MAIN MEMORY 137Another possible organization memory cell array 64 /C264, is, organize array form 64 rows, consisting 64 cells. case, six address lines (forming called row address) needed inorder select one 64 rows. remaining four address lines (called thecolumn address) used select appropriate 4 bits among available64 bits constituting row. Figure 7.4 illustrates organization. Another important factor related design main memory subsystem number chip pins required integrated circuit. Consider, example, thedesign memory subsystem whose capacity 4K bits. Different organization ofthe memory capacity lead different number chip pins requirement.Table 7.1 illustrates observation. clear table increasing thenumber bits per addressable location results increase number pinsneeded integrated circuit. Another factor pertinent design main memory subsystem required number memory chips. important realize available per chip memory capacity limiting factor designing memory subsystems. W0 A0 A1 A2 A5W1 W2 W63 B63 A6 A7 A8 A9B62 B2 D3D2 D1 D0B1B064× 64 memory cell array 16 1 multiplexers Figure 7.4 Efﬁcient internal organization 1K /C24 memory chip138 MEMORY SYSTEM DESIGN IIConsider, example, design 4M bytes main memory subsystem using 1M bit chip. number required chips 32 chips. noted numberof address lines required 4M system 22, number data lines 8. Figure 7.5 shows block diagram intended memory subsystem basic building block used construct subsystem. memory subsystem arranged four rows, eight chips. schematic arrangement shown Figure 7.6. ﬁgure, least sig- niﬁcant 20 address lines 19/C0A0are used address basic building block 1M single bit chips. high-order two address lines A21–A20are used inputs 2–4 decoder order generate four enable lines; connected CE line eight chips constituting row. discussion main memory system design assumes use six- transistor static random cell. possible however use one-transistor dynamiccell. Dynamic memory depends storing logic values using capacitor togetherwith one transistor acts switch. use dynamic memory leads saving chip area. However, due possibility decay stored valuesTABLE 7.1 Impact Using Different Organizations Number Pins OrganizationNumber needed address linesNumber needed data lines 4K/C211 2 1 1K/C241 0 4 512/C289 8 256/C216 8 16 R/W R/W R/W20 address lines 22 address lines 1 data line 8 data lines CE Mode 0 Tri-state 1R e d1X 10 Write X=don’t care Tri-state = high impedanceCE4M bytes memory System (a) Intended memory system (b) Basic memory building block Figure 7.5 Block diagram required memory system basic building block7.1. MAIN MEMORY 139(leakage stored charge capacitor), dynamic memory requires periodical (every milliseconds) refreshment order restore stored logic values.Figure 7.7 illustrates dynamic memory array organization. read /write cir- cuitry Figure 7.7 performs functions sensing value bit line, ampli- fying it, refreshing value stored capacitor. order perform read operation, bit line precharged high (same static memory) word line activated. cause value stored capacitor appear bit line, thus appearing data line i. seen, read operation destructive; is, capacitor charged bit line. There- fore, every read operation followed write operation value. order perform write operation, intended value placed bit line word line activated. intended value 1, capacitor charged, intended value 0, capacitor discharged. Table 7.2 summarizes operation control circuitry.A21 A20 A19 -A0D7 D1 D0#7 #1 #0 #7 #1 #0 #7 #1 #0 #7 #1 #0 Figure 7.6 Organization 4M 8-bit memory using 1M 1-bit memory chips TABLE 7.2 Operation Control Circuitry CE R=/C22W W Operation 0 /C2 None 1 1 Read 1 0 Write /C2¼ don’t care140 MEMORY SYSTEM DESIGN IIAs discussed before, appropriate internal organization memory subsystem lead saving number IC pins required, important IC design factor. order reduce number pins required given dynamic memory subsystem, normal practice (as case static memory) todivide address lines row column address lines. addition, rowand column address lines transmitted pins, one otherin scheme known time-multiplexing . potentially cut number address pins required half. Due time-multiplexing address lines, benecessary add two extra control lines, is, row address strobe ( RAS) column address strobe ( CAS). two control lines used indicate memory chip row address lines valid column addresslines valid, respectively. Consider, example, design 1M /C21 dynamic memory subsystem. Figure 7.8 shows possible internal organization thememory cell array array organized 1024 /C21024. noted 10 address lines shown. used multi- plex rows columns address lines; 10 lines. rows col-umns latches used store row column addresses duration equalto memory cycle. case, memory access consist RAS row address, followed CAS column address. Figure 7.7 Dynamic memory array organization7.1. MAIN MEMORY 1417.2. VIRTUAL MEMORY concept virtual memory principle similar cache memory described Section 6.2. virtual memory system attempts optimize useof main memory (the higher speed portion) hard disk (the lowerspeed portion). effect, virtual memory technique using secondary sto-rage extend apparent limited size physical memory beyond actual physical size. usually case available physical memory space enough host parts given active program. parts pro-gram currently active brought main memory parts active stored magnetic disk. segment program con-taining word requested processor main memory time ofthe request, segment brought disk mainmemory. principles employed virtual memory design asthose employed cache memory. relevant principle keeping active segments high-speed main memory moving inactive segments back hard disk. Movement data disk main memory takes form pages. page collection memory words, moved thedisk MM processor requests accessing word page. typicalsize page modern computers ranges 2K 16K bytes. page faultoccurs page containing word required processor notexist MM brought disk. movement pages pro- grams data main memory disk totally transparent application programmer. operating system responsible movementof data programs. Figure 7.8 1024 /C21024 memory array organization142 MEMORY SYSTEM DESIGN IIIt useful mention point although based similar principles, signiﬁcant difference exists cache virtual memories. cache miss cause time penalty 5 10 times costly cache hit. pagefault, hand 1000 times costly page hit. thereforeunreasonable processor wait page fault page trans-ferred main memory. thousands instructions could exe-cuted modern processor page transfer. address issued processor order access given word correspond physical memory space. Therefore, address called vir- tual(logical )address . memory management unit (MMU) responsible translation virtual addresses corresponding physical addresses. Threeaddress translation techniques identiﬁed. direct-mapping ,associ- ative-mapping , set-associative-mapping . techniques, information main memory locations corresponding virtual pages keptin table called page table . page table stored main memory. information kept page table includes bit indicating validity ofa page, modiﬁcation page, authority accessing page. valid bitis set corresponding page actually loaded main memory. Valid bits pages reset computer ﬁrst powered on. Theother control bit kept page table dirty bit . set corres- ponding page altered residing main memory. residingin main memory given page altered, dirty bit reset. help deciding whether write contents page back disk (at time replacement) override contents another page. following discussion, concentrate address trans-lation techniques keeping mind use different control bits stored thepage table. 7.2.1. Direct Mapping Figure 7.9 illustrates address translation process according direct-mapping technique. case, virtual address issued processor divided intotwo ﬁelds: virtual page number offset ﬁelds. number bits inthe virtual page number ﬁeld N, number entries page table 2 N. virtual page number ﬁeld used directly address entry page table. corresponding page valid (as indicated valid bit), con-tents speciﬁed page table entry correspond physical page address.The latter extracted concatenated offset ﬁeld order form thephysical address word requested processor. If, hand, thespeciﬁed entry page table contain valid physical page number,then represents page fault. case, MMU bring thecorresponding page hard disk, load main memory, indicate validity page. translation process carried explained before.7.2. VIRTUAL MEMORY 143The main advantage direct-mapping technique simplicity measured terms direct addressing page table entries. main disadvantage expected large size page table. order overcome need fora large page table, associative-mapping technique, explained below, used. 7.2.2. Associative Mapping Figure 7.10 illustrates address translation according associative mapping technique. technique similar direct mapping virtual addressissued processor divided two ﬁelds: virtual page number theoffset ﬁelds. However, page table used associative mapping could far shorterthan direct mapping counterpart. Every entry page table divided two parts: virtual page number physical page number. match searched(associatively) virtual page number ﬁeld address virtual page numbers stored page table. match found, corresponding physical page number stored page table extracted concatenated offsetﬁeld order generate physical address word requested processor.If, hand, match could found, represents page fault. Inthis case, MMU bring corresponding page hard disk, loadit main memory, indicate validity page. translation processis carried explained before. main advantage associative-mapping technique expected shorter page table (compared direct-mapping technique) required translationprocess. main disadvantage search required matching virtual Figure 7.9 Direct-mapping virtual address translation144 MEMORY SYSTEM DESIGN IIpage number ﬁeld virtual page numbers stored page table. Although search done associatively, requires use added hardwareoverhead. possible compromise complexity associative mapping simplicity direct mapping set-associative mapping technique. Thishybrid technique explained below. 7.2.3. Set-Associative Mapping Figure 7.11 illustrates address translation according set-associative map- ping. case, virtual address issued processor divided threeﬁelds: tag, index, offset. page table used set-associative map-ping divided sets, consisting number entries. entry thepage table consists tag corresponding physical page address. Similarto direct mapping, index ﬁeld used directly determine set search conducted. number bits index ﬁeld S, number sets page table 2 S. set determined, search (similar associative mapping) conducted match tag ﬁeld entries speciﬁc set. match found, corresponding physicalpage address extracted concatenated offset ﬁeld order generatethe physical address word requested processor. If, hand, amatch could found, represents page fault. case, MMUwill bring corresponding page hard disk, load main memory, update corresponding set indicate validity page. translation process carried explained before. Figure 7.10 Associative mapping address translation7.2. VIRTUAL MEMORY 145The set-associative-mapping technique strikes compromise inefﬁ- ciency direct mapping, terms size page table, excessive hard- ware overhead associative mapping. also enjoys best two techniques: simplicity direct mapping efﬁciency associative mapping. noted address translation techniques extra main memory access required accessing page table. extra main memoryaccess could potentially saved copy small portion page table kept MMU. portion consists page table entries corre- spond recent accessed pages. case, address translationis attempted, search conducted ﬁnd whether virtual page number (orthe tag) virtual address ﬁeld could matched. small portion kept table look-aside buffer (TLB) cache MMU. explained below. 7.2.4. Translation Look-Aside Buffer (TLB) modern computer systems copy small portion page table kept processor chip. portion consists page table entries correspondto recently accessed pages. small portion kept translation look-aside buffer (TLB) cache. search TLB precedes page table. Therefore, virtual page ﬁeld ﬁrst checked entries Figure 7.11 Set-associative mapping address translation146 MEMORY SYSTEM DESIGN IITLB hope match found. hit TLB result generation physical address word requested processor, thus saving extramain memory access required access page table. noted misson TLB equivalent page fault. Figure 7.12 illustrates use theTLB virtual address translation process. typical size TLB therange 16 64 entries. small TLB size, hit ratio 90%is always possible. Owing limited size, search TLB done associa- tively, thus reducing required search time. illustrate effectiveness use TLB, let us consider case using TLB virtual memory system following speciﬁcations. Number entries TLB ¼16 Associative search time TLB ¼10 ns Main memory access time ¼50 ns TLB hit ratio ¼0.9 average access time ¼0.9(10þ50)þ0.1(10þ2*50)¼0.9*60þ 0.1*110¼65 ns. compared 100 ns access time needed absence TLB. noted simplicity, overlooked exist- ence cache illustration. clear discussion requests items exist main memory (page faults) occur, pages would broughtfrom hard disk main memory. eventually lead totally ﬁlledmain memory. arrival new page hard disk totally full main memory promote following question: main memory page removed (replaced) order make room incoming page(s)? Replace- ment algorithms (policies) explained next. Figure 7.12 Use TLB virtual address translation7.2. VIRTUAL MEMORY 147It noted Intel’s Pentium 4 processor 36-bit address bus, allows maximum main memory size 64 GB. According Intel’s speciﬁca- tions, virtual memory 64 TB (65,528 GB). increases processor’smemory access space 2 36to 246bytes. compared PowerPC 604 two 12-entry, two-way set-associative translation look-aside buffers (TLBs): one instructions data. virtual memory space istherefore ¼2 52¼4 Peta-bytes. 7.2.5. Replacement Algorithms (Policies) Basic implementation virtual memory concept demand paging . means operating system, programmer, controls swap- ping pages main memory required active pro-cesses. process needs nonresident page, operating system mustdecide resident page replaced requested page. techniqueused virtual memory makes decision called replacement policy . exists number possible replacement mechanisms. main objective mechanisms select removal page expectedly bereferenced near future. Random Replacement According replacement policy, page selected randomly replacement. simplest replacement mechanism. implemented using pseudo-random number generator generates numbersthat correspond possible page frames. time replacement, therandom number generated indicate page frame must replaced. Although simple, technique may result efﬁcient use main memory, is, low hit ratio h. Random replacement used Intel i860 family RISC processor. First-In-First-Out (FIFO) Replacement According replacement policy, page loaded others main memory selected forreplacement. basis page replacement technique time spentby given page residing main memory regardless pattern usage ofthat page. technique also simple. However, expected result accep- table performance, measured terms main memory hit ratio, page refer- ences made processor strict sequential order. illustrate use theFIFO mechanism, offer following example. Example Consider following reference string pages made processor: 6, 7, 8, 9, 7, 8, 9, 10, 8, 9, 10. particular, consider two cases: (a) number page frames allocated main memory TWO (b) number pageframes allocated THREE. Figure 7.13 illustrates trace reference stringfor two cases. seen ﬁgure, number page frames TWO, 11 page faults (these shown bold ﬁgure). number page frames increased THREE, number page148 MEMORY SYSTEM DESIGN IIfaults reduced ﬁve. Since ﬁve pages referenced, optimum con- dition. FIFO policy results best (minimum) page faults reference string strict order increasing page number references. Least Recently Used (LRU) Replacement According technique, page replacement based pattern usage given page residing main memory regardless time spent main memory. page notbeen referenced longest time residing main memory selectedfor replacement. LRU technique matches programs’ characteristics andtherefore expected result best possible performance terms main memory hit ratio. is, however, involved compared techniques. illustrate use LRU mechanism, offer following example. Example Consider following reference string pages made processor: 4, 7, 5, 7, 6, 7, 10, 4, 8, 5, 8, 6, 8, 11, 4, 9, 5, 9, 6, 9, 12, 4, 7, 5, 7. Assume number page frames allocated main memory FOUR. Compute thenumber page faults generated. trace main memory contents isshown Figure 7.14. Number page faults ¼17. presenting LRU, particular implementation, called stack-based LRU. implementation, recently accessed page represented by(a) (b)6 6 8 8 79 9 8 8 10 9 9 8 10 10 9 967 7 77 789 7 8 9 1 0 8 9 1 0 66 668 8 88 888 88810 10 10 10 7 7 77 777 9 99 9 99 9 99 7 8 9 10 8 9 10 Figure 7.13 FIFO replacement technique. (a) FIFO replacement using two page frames (#PFs¼11), (b) FIFO replacement using three page frames (#PFs ¼5) Figure 7.14 LRU replacement technique7.2. VIRTUAL MEMORY 149the top page rectangle. rectangles represent speciﬁc page frames FIFO diagram. Thus, reference generating page fault thetop row. noted pages allotted program pagereferences row change. number page faults changes. Thiswill make set pages memory npage frames subset set pages nþ1 page frames. fact, diagram could considered STACK data structure depth stack representing number page frames. page stack (i.e., found depth greater number page frames), page fault occurs. Example Consider case two-dimensional 8 /C28 array A. array stored row-major order. THREE page frames, compute many page faults generated following array-initialization loop. Assume LRU replacement algorithm used frames initially empty. Assume page size 16. ¼0t o7d J ¼0t o7d A[I, J] ¼0; End End arrangement array elements secondary storage shown Figure 7.15. sequence requests array elements ﬁrst TWO external loop executions follows: I¼0 J¼0, 1, 2, 3, 4, 5, 6, 7 00,a01,a02,a03,a04,a05,a06,a07 number page faults (PFs) ¼1 I¼1 J¼0, 1, 2, 3, 4, 5, 6, 7 a10,a11,a12,a13,a14,a15,a16,a17 number page faults (PFs) ¼1 analysis, clear one PF every external loop execution. makes total number PFs 8. noted thearray stored column-major, every internal loop execution would generateeight page faults, thus causing total number PFs become 64. Clock Replacement Algorithm modiﬁed FIFO algorithm. takes account time spent page residing main memory (similar FIFO) pattern usage page (similar LRU). tech- nique therefore sometimes called First-In-Not-Used-First-Out (FINUFO). keeping track time usage, technique uses pointer to150 MEMORY SYSTEM DESIGN IIFigure 7.15 Arrangement array elements secondary storage main memory built up7.2. VIRTUAL MEMORY 151indicate place incoming page used bit indicate usage given page. technique explained using following three steps. 1. used bit ¼1, reset bit, increment pointer repeat. 2. used bit ¼0, replace corresponding page increment pointer. 3. used bit SET page referenced initial loading. Example Consider following page requests (Fig. 7.16) THREE-page frames MM system using FINUFO technique: 2,3,2,4,6,2,5,6,1,4,6. Estimatethe hit ratio. estimated Hit Ratio ¼4/11. 7.2.6. Virtual Memory Systems Cache Memory typical computer system contain cache, virtual memory, TLB. virtual address received processor, number different scenarios canoccur, dependent availability requested item cache, mainmemory, secondary storage. Figure 7.17 shows general ﬂow diagram thedifferent scenarios. ﬁrst level address translation checks match received vir- tual address virtual addresses stored TLB. match occurs (TLB hit)then corresponding physical address obtained. physical address thenbe used access cache. match occurs (cache hit) element requestedby processor sent cache processor. If, hand, acache miss occurs, block containing targeted element copied themain memory cache (as discussed before) requested element sentto processor. scenario assumes TLB hit. TLB miss occurs, page table (PT) searched existence page containing targeted element themain memory. PT hit occurs, corresponding physical address gener-ated (as discussed before) search conducted block containingthe requested element (as discussed above). require updating TLB. hand PT miss takes place (indicating page fault), page containing targeted element copied disk main memory, ablock copied cache, element sent processor. last Figure 7.16 FINUFO replacement technique152 MEMORY SYSTEM DESIGN IIscenario require updating page table, main memory, cache. subsequent request virtual address processor result updatingthe TLB. 7.2.7. Segmentation segment block contiguous locations varying size. Segments used operating system (OS) relocate complete programs main disk memory. Segments shared programs. provide means pro-tection unauthorized access /or execution. possible enter seg- ments segments unless access speciﬁcally allowed. Datasegments code segments separated. also possible alterinformation code segment fetching instruction poss-ible execute data data segment. 7.2.8. Segment Address Translation order support segmentation, address issued processor consist segment number (base) displacement (or offset) within segment.Address translation performed directly via segment table. starting address targeted segment obtained adding segment number contents segment table pointer. One important content segment table Figure 7.17 Memory hierarchy accesses scenarios7.2. VIRTUAL MEMORY 153physical segment base address. Adding latter offset yields required physical address. Figure 7.18 illustrates segment address translation process. Possible additional information included segment table includes: 1. Segment length 2. Memory protection (read-only, execute-only, system-only, on) 3. Replacement algorithm (similar used paged systems)4. Placement algorithm (ﬁnding suitable place main memory hold incoming segment). Examples include (a) First ﬁt (b) Best ﬁt (c) Worst ﬁt 7.2.9. Paged Segmentation segmentation paging combined systems. segment divided number equal sized pages. basic unit transfer data main memory disk page, is, given time, themain memory may consist pages various segments. case, virtual address divided segment number ,apage number , displacement within page . Address translation explained except physical segment base address obtained segment table added virtualpage number order obtain appropriate entry page table. outputof page table page physical address, concatenated word ﬁeld virtual address results physical address. Figure 7.19 illustrates paged segmentation address translation.Segment Number Offset Segment Table Displacement Physical Segment Base Address Physical AddressΣΣ Figure 7.18 Segment address translation154 MEMORY SYSTEM DESIGN II7.2.10. Pentium Memory Management Pentium processor, segmentation paging individually available also disabled. Four distinct views memory exist: 1. Unsegmented unpaged memory 2. Unsegmented paged memory3. segmented unpaged memory 4. segmented paged memory segmentation, 16-bit segment number (two used protection) 32-bit offset produce segmented virtual address space equal 2 46¼64 terabytes. virtual address space divided two parts: one half, is,8K/C24 GB, global shared processes, half local distinct process. paging, two-level table lookup paging system used. First level page directory 1024 entries, is, 1024 page groups, page table FOUR MB length. page table contains 1024 entries; entry corresponds single 4 KB page.Segment Number Segment TableOffset Displacement Physical Segment Base Address Page Table Physical Address Physical AddressVirtual Page Number Σ Σ Σ Figure 7.19 Paged segmentation address translation7.2. VIRTUAL MEMORY 1557.3. READ-ONLY MEMORY Random access well cache memories examples volatile memories. volatile storage deﬁned one loses contents power turned off. Nonvolatile memory storages retain stored information power turned off. need volatile storage also need nonvo-latile storage. Computer system boot subroutines, microcode control, video game cartridges examples computer software require use nonvolatile storage. Read-only memory (ROM) also used realize combi-national logic functions. technology used implementing ROM chips evolved years. Early implementations ROMs called mask-programmed ROMs . case, made- to-order one time ROM programmed according speciﬁc encoding pattern sup-plied user. structure 4 /C24 CMOS ROM chip shown Figure 7.20. ﬁgure n-type transistor placed 1 stored. two-to-four address decoder used create four word lines; used activate row oftransistors. 1 appears word line, corresponding transistors beturned on, thus pulling corresponding bit line 0. inverter output Figure 7.20 Example 4 /C24 CMOS ROM chip156 MEMORY SYSTEM DESIGN IIbit lines used output 1 output pulled bit lines. Table 7.3 shows patterns stored four ROM locations. Mask-programmed ROMs primarily used store machine microcode, desk- top bootstrap loaders, video game cartridges. programmed manufacturer, mask-programmed ROMs inﬂexible. user would like program /her ROM site, different type ROM, called Programmable ROM (PROM) used. case, fuses, instead transis-tors, placed intersection word bit lines. user program thePROM selectively blowing appropriate fuses. done allow- ing high current ﬂow particular fuses, thus causing blow up. process known “burning ROM.” Although allows added ﬂexibility, PROM still restricted fact programmed (by user). third type ROM, calledErasable PROM (EPROM), reprogrammable; is, allows stored data beerased new data stored. order provide ﬂexibility, EPROMsare constructed using special type transistors. transistors able toassume one two statuses, normal disabled. disabled transistor acts like switch turned time. normal transistor programmed become open time inducing certain amount charge trappedunder gate. disabled transistor become normal removing induced charge. requires exposing transistors ultraviolet light.Exposing EPROM chip ultraviolet light lead erasure theentire chip contents. considered major drawback EPROMs. BothPROMs EPROMs used prototyping, moderate size systems. Flash EPROMs (FEPROMs) emerged strong contenders EPROMs. FEPROMs compact, faster, removable compared toEPROMs. erasure time FEPROM far faster EPROM. different type ROM, overcomes drawback EPROM, Electrically EPROM EEPROM. case, erasure EPROM bedone electrically and, moreover, selectively; is, contents selectivecells erased, leaving cells’ contents untouched. FEPROMsand EEPROMs used applications requiring occasional updating infor-mation, Programmable TVs, VCR, automotives. Table 7.4 summarizes main characteristics different types ROM discussed above.TABLE 7.3 Patterns Stored Four ROM Locations Address linesWord line activatedOutput pattern 00 W0 1001 01 W1 0110 10 W2 1010 11 W3 01017.3. READ-ONLY MEMORY 1577.4. SUMMARY discussion chapter continuation conducted Chapter 6. particular, chapter dedicated cover design aspects relate tothe internal external organization main memory. design staticRAM cell introduced emphasis read write operations. dis-cussion virtual memory started issues related address translation.Three address translation techniques discussed compared. direct, associative, set-associative techniques. use TLB improve average access time explained. Three replacement techniques intro-duced. FIFO, LRU, clock replacement. Segmented paged systemswere also introduced. discussion virtual memory ended explanationof virtual memory aspects Pentium IV processor. Toward end thechapter, touched number implementations ROMs. EXERCISES 1. Consider case computer system employing cache paged virtual memory shown (Fig. 7.21). One analyze system identifying FIVE combinations accesses. combi-nations? Determine probability access time case assumingthe following information. Compute also overall average access time. TLB address translation search 25 ns Cache search time determine whether address cache 25 nsCache access time 25 ns Main memory access time 250 ns Hard disk access time 100 ms TLB hit ratio 0.9 Cache hit ratio 0.95 Main memory hit ratio 0.8 2. 64 /C264 array words (elements) “normalized” follows. row, largest element found elements row divided byTABLE 7.4 Characteristics Different ROM Implementations ROM type Cost ProgrammabilityTypical applications Mask-programmed ROM Truly inexpensive manufacture Microcode PROM Inexpensive site Prototyping EPROM Moderate Many times Prototyping FEPROM Expensive Many times VCR & TVs EEPROM Truly expensive Many times VCR & TVs158 MEMORY SYSTEM DESIGN IIthis maximum value. Assume page virtual memory consists 64 words, 2K words main memory allocated storing data computation. Suppose takes 100 ms load page thedisk main memory page fault occurs. (a) Write simple piece code (in notational form) perform job. (b) many page faults would occur elements array stored column order virtual memory? (c) many page faults would occur elements array stored row order virtual memory? (d) Estimate total time needed perform normalization arrangements (b) (c). 3. Design 64M /C28-bit memory using number 16M /C21-bit static RAM chips. Assume individual chip chip select ( CS) line read/write ( R=/C22W W) line. Compute number chips required show complete connection diagram designed memory. 4. Consider following stream page requests: 1,2,3,4,5,1,2,3,4,5,1,2,3,4,5. Assume main memory consists FOUR page frames. Show traceof status page frames MM estimate hit ratio assumingeach following page replacement algorithms. (a) FIFO (b) LRU (c) FINUFO 5. Consider case two-dimensional 20 /C220 array A. array stored column-major. FIVE main memory page frames, compute many Figure 7.21 Computer system cache paged virtual memoryEXERCISES 159page faults generated following array-initialization loop. Assume LRU replacement algorithm used frames initially empty. Assume also page size 40 elements. ¼1t o2 0d J ¼1t o2 0d A[I, J] ¼0; 6. problem asked pick real-life computer memory system uses caching virtual memory schemes. job apply knowledge gained chapter concerning “Memory System Design Analysis” describing analyzing selected system. Examples may pick include limited Intel Pentium 4,The PowerPC, Alpha AXP 21064, on. Use examples illustrationsto support analysis speciﬁc possible. Remember empha-sis analysis basic design issues involved. useany reference material make sure include reference list. REFERENCES READING S. D. Burd, Systems Architecture , 3rd ed., Thomson Learning Ltd, Boston, 2001. H. Cragon, Memory Systems Pipelined Processors , Jones Bartlett: Sudbury, MA, 1996. V. C. Hamacher, Z. G. Vranesic, S. G. Zaky, Computer Organization , 5th ed., McGraw- Hill, NY, 2002. J. L. Hennessy, Patterson, D. A., Computer Architecture :A Quantitative Approach , Morgan Kaufmann, San Francisco, CA, 1996. V. P. Heuring, H. F. Jordan, Computer Systems Design Architecture , Addison- Wesley, NJ, USA, 1997. D. A. Patterson, J. L. Hennessy, Computer Organization & Design :The Hardware / Software Interface , Morgan Kaufmann, San Mateo, CA, 1994. H. S. Stone, High-Performance Computer Architecture , Addison-Wesley, Amsterdam, Netherlands, 1987. B. Wilkinson, Computer Architecture: Design Performance , Prentice-Hall, Hertfordshire, UK, 1996. Websites http://www.sysopt.com http://www.intel.com http://www.AcerHardware.com http://www.pmc-sierra.com /products /details /rm7000a http: //physinfo.ulb.ac.be /divers_html /PowerPC_Programming_Info /into_to_ppc /ppc2_ hardware.html160 MEMORY SYSTEM DESIGN II& CHAPTER 8 Input–Output Design Organization considered fundamental concepts related instruction set design, assembly language programming, processor design, memory design, nowturn attention issues related input–output (I /O) design organiz- ation. emphasized outset /O plays crucial role modern computer system. Therefore, clear understanding appreciation ofthe fundamentals /O operations, devices, interfaces great importance. Input–output (I /O) devices vary substantially characteristics. One dis- tinguishing factor among input devices (and also among output devices) data processing rate, deﬁned average number characters pro- cessed device per second. example, data processing rate aninput device keyboard 10 characters (bytes) /second, scanner send data rate 200,000 characters /second. Similarly, laser printer output data rate 100,000 characters /second, graphic display output data rate 30,000,000 characters /second. Striking character keyboard computer cause character (in form ASCII code) sent computer. amount time passed next character sent computer depend skill user even sometimes /her speed thinking. often case user knows /she wants input, sometimes need think touching next button keyboard. Therefore, input keyboardis slow burst nature waste time computer tospend valuable time waiting input slow input devices. mechanismis therefore needed whereby device interrupt processor askingfor attention whenever ready. called interrupt-driven communication computer /O devices (see Section 8.3). Consider case disk. typical disk capable transferring data rates exceeding several million bytes /second. would waste time trans- fer data byte byte even word word. Therefore, always case datais transferred form blocks, is, entire programs. also necessaryto provide mechanism allows disk transfer huge volume datawithout intervention CPU. allow CPU perform 161Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.useful operation(s) huge amount data transferred disk memory. essence direct memory access (DMA) mechanism discussed Section 8.4. begin discussion offering basic concepts Section 8.1. 8.1. BASIC CONCEPTS Figure 8.1 shows simple arrangement connecting processor memory given computer system input device, example, keyboard outputdevice graphic display. single bus consisting required address, data,and control lines used connect system’s components Figure 8.1. way processor memory exchange data explained Chapters 6 7. concerned way processor /O devices exchange data. indicated introduction part exists big difference rate processor process infor- mation input output devices. One simple way accommodate speed difference input device, example, keyboard, deposit thecharacter struck user register ( input register ), indicates avail- ability character processor. input character taken bythe processor, indicated input device order proceed inputthe next character, on. Similarly, processor character output(display), deposits speciﬁc register dedicated communication thegraphic display ( output register ). character taken graphic display, indicated processor proceed output thenext character, on. simple way communication processorand /O devices, called I/O protocol , requires availability input output registers. typical computer system, number input registers,each belonging speciﬁc input device. also number output registers, System BusProcessor Memory Output Device (Graphic Display)Input Device (Keyboard) Figure 8.1 single bus system162 INPUT–OUTPUT DESIGN ORGANIZATIONeach belonging speciﬁc output device. addition, mechanism according processor address input output registers must adopted.More one arrangement exists satisfy abovementioned requirements.Among these, two particular methods explained below. ﬁrst arrangement, /O devices assigned particular addresses, isolated address space assigned memory. execution input instruc- tionat input device address cause character stored input register device transferred speciﬁc register CPU. Similarly, theexecution output instruction output device address cause char- acter stored speciﬁc register CPU transferred output register output device. arrangement, called shared /O, shown schematically Figure 8.2. case, address data lines CPU sharedbetween memory /O devices. separate control line used. need executing input output instructions. atypical computer system, exists one input one output device. Therefore, need address decoder circuitry device identiﬁcation. also need status registers input output device. status input device, whether ready send data processor, stored status register device. Similarly, thestatus output device, whether ready receive data processor,should stored status register device. Input (output) registers,status registers, address decoder circuitry represent main components I/O interface (module). Address Bus Data Bus Memory Control Lines Input Device(s) Control Lines Output Device(s) Control LinesProcessor Memory Output Device (Graphic Display)Input Device (Keyboard) Figure 8.2 Shared /O arrangement8.1. BASIC CONCEPTS 163The main advantage shared /O arrangement separation memory address space /O devices. main disadvantage need special input andoutput instructions processor instruction set. shared /O arrangement mostly adopted Intel. second possible /O arrangement deal input andoutput registers regular memory locations. case, read operation address corresponding input register input device, example, Read Device 6 ,i equivalent performing input operation input register Device #6. Similarly, write operation address corresponding output register output device, example, Write Device 9 , equivalent performing output operation output register Device #9. arrangement called memory-mapped /O. shown Figure 8.3. main advantage memory-mapped /O use read write instructions processor perform input output operations, respectively. eliminates need introducing special /O instructions. main disadvantage memory-mapped /O need reserve certain part memory address space addressing /O devices, is, reduction available memory address space. memory-mapped /O mostly adopted Motorola. 8.2. PROGRAMMED /O section, present main hardware components required communi-cations processor /O devices. way according Address Bus Data Bus Control LinesProcessorMemory Output Device (Graphic Display)Input Device (Keyboard) Figure 8.3 Memory-mapped /O arrangement164 INPUT–OUTPUT DESIGN ORGANIZATIONcommunications take place (protocol) also indicated. protocol pro- grammed form routines run control CPU. Consider, forexample, input operation Device 6 (could keyboard) case shared I/O arrangement. Let us also assume eight different /O devices connected processor case (see Fig. 8.4). following protocol steps (program) followed: 1. processor executes input instruction device 6, example, INPUT 6 . effect executing instruction send device number address decoder circuitry input device order ident- ify speciﬁc input device involved. case, output deco-der Device #6 enabled, outputs decoders willbe disabled. 2. buffers (in ﬁgure assumed eight buffers) holding data speciﬁed input device (Device #6) enabled outputof address decoder circuitry. 3. data output enabled buffers available data bus. Figure 8.4 Example eight-I /O device connection processor8.2. PROGRAMMED /O 1654. instruction decoding gate data available data bus input particular register CPU, normally accumulator . Output operations performed way similar input operation explained above. difference direction data transfer, speciﬁc CPU register output register speciﬁed outputdevice. /O operations performed manner called programmed /O. performed CPU control. complete instruction fetch, decode,and execute cycle executed every input every output oper-ation. Programmed /O useful cases whereby one character time transferred, example, keyboard character mode printers. Although simple, programmed /O slow. One point overlooked description programmed /Oi handle substantial speed difference /O devices pro- cessor. mechanism adopted order ensure character sent output register output device, screen, overwritten theprocessor (due processor’s high speed) displayed char-acter available input register keyboard read processor.This brings issue status input output devices. mechanism implemented requires availability Status Bit (B in) interface input device Status Bit (Bin) interface output device. Whenever input device keyboard character available input register, indicates setting Bin¼1. program processor used continuously monitor Bin. program sees Bin¼1, interpret mean character available input register device. Reading character require executing protocol explained above. When- ever character read, program reset Bin¼0, thus avoiding multiple read character. similar manner, processor deposit characterin output register output device screen B out¼0. screen displayed character sets Bout¼1, indicating program monitors Boutthat screen ready receive next character. process checking status /O devices order determine readi- ness receiving /or sending characters, called software /O polling .Ahard- ware /O polling scheme shown Figure 8.5. Figure 8.5 Hardware polling scheme166 INPUT–OUTPUT DESIGN ORGANIZATIONIn ﬁgure, NI/O devices access interrupt line INR. Upon recognizing arrival request (called Interrupt Request) INR, pro- cessor polls devices determine requesting device. done thedLog2Nepolling lines. priority requesting device determine order addresses put polling lines. address highest priority device put ﬁrst, followed next priority, least priority device. addition /O polling, two mechanisms used carry /O operations. interrupt-driven /Oanddirect memory access (DMA). discussed next two sections. 8.3. INTERRUPT-DRIVEN /O often necessary normal ﬂow program interrupted, example, react abnormal events, power failure. interrupt also used toacknowledge completion particular course action, printer indicatingto computer completed printing character(s) input register andthat ready receive ot character(s). interrupt also used time-sharing systems allocate CPU time among different programs. instruction sets modern CPUs often include instruction(s) mimic actions hardware interrupts. CPU interrupted, required discontinue current activity, attend interrupting condition (serve interrupt), resume activity wherever stopped. Discontinuity processor’s current activity requiresﬁnishing executing current instruction, saving processor status (mostly theform pushing register values onto stack), transferring control (jump) whatis called interrupt service routine (ISR). service offered interrupt depend source interrupt. example, interrupt due power failure, action taken save values processor registers pointers resumption correct operation guaranteed upon powerreturn. case /O interrupt, serving interrupt means perform required data transfer. Upon ﬁnishing serving interrupt, processor shouldrestore original status popping relevant values stack. theprocessor returns normal state, enable sources interrupt again. One important point overlooked scenario issue ser- ving multiple interrupts , example, occurrence yet another interrupt processor currently serving interrupt. Response new interrupt willdepend upon priority newly arrived interrupt respect theinterrupt currently served. newly arrived interrupt priority lessthan equal currently served one, wait processorﬁnishes serving current interrupt. If, hand, newly arrived inter-rupt priority higher currently served interrupt, example,power failure interrupt occurring serving /O interrupt, processor push status onto stack serve higher priority interrupt. Correct handling multiple interrupts terms storing restoring correct processor status guaranteed due way push pop operations are8.3. INTERRUPT-DRIVEN /O 167performed. example, serve ﬁrst interrupt, STATUS 1 pushed onto stack. Upon receiving second interrupt, STATUS 2 pushed onto thestack. Upon serving second interrupt, STATUS 2 popped stackand upon serving ﬁrst interrupt, STATUS 1 popped stack. possible interrupting device identify processor sending code following interrupt request. code sent given /O device represent /O address memory address location start ISR device. scheme called vectored interrupt . 8.3.1. Interrupt Hardware discussion, assumed processor recognized occurrence interrupt proceeding serve it. Computers provided interrupt hardware capability form specialized interrupt lines processor. lines used send interrupt signals processor. case /O, exists one /O device. processor pro- vided mechanism enables handle simultaneous interrupt requestsand recognize interrupting device. Two basic schemes implementedto achieve task. ﬁrst scheme called daisy chain bus arbitration (DCBA) second called independent source bus arbitration (ISBA). According DCBA (see Fig. 8.6 a), I/O devices present interrupt requests interrupt request line INR (similar polling arrangement). Upon recognizing arrival interrupt request, processor, adaisy chained grant line (GL), sends grant requesting device start com- munication processor. GL goes devices starting theﬁrst device nearer processor going next device itreaches last device (Device #N). Device #1 put request, willhold grant signal start communication processor. If, hand, Device #1 interrupt request, pass grant signal device #2, repeat procedure, on. case multiple requests,the DCBA arrangement gives highest priority device physically nearer theprocessor. furthest device processor lowest priority. According ISBA (see Fig. 8.6 b), /O device interrupt request line, send interrupt request, independent devices.Similarly, /O device grant line, receives grant signal request start communicating processor. /O device priority ISBA depend device location. priority arbitra-tion circuitry needed order deal simultaneous interrupt requests. 8.3.2. Interrupt Operating Systems interrupt occurs, operating system gains control. operating system saves state interrupted process, analyzes interrupt, passes control appropriate routine handle interrupt. several168 INPUT–OUTPUT DESIGN ORGANIZATIONtypes interrupts, including /O interrupts. /O interrupt notiﬁes operating system /O device completed suspended operation needs service CPU. process interrupt, context current process must saved interrupt handling routine must invoked. process iscalled context switching . process context two parts: processor context memory context. processor context state CPU’s registers including program counter (PC), program status words (PSWs), registers. memory context state program’s memory including program data. interrupt handler routine processes different type ofinterrupt. operating system must provide programs save area contexts. also must provide organized way allocating deallocating memory theinterrupted process. interrupt handling routine ﬁnishes processing inter-rupt, CPU dispatched either interrupted process, highest priorityready process. depend whether interrupted process preemptive nonpreemptive. process nonpreemptive, gets CPU again. First con- text must restored, control returned interrupts process. Figure 8.6 Interrupt hardware schemes. ( a) Daisy chain interrupt arrangement (b) Independent interrupt arrangement8.3. INTERRUPT-DRIVEN /O 169Figure 8.7 shows layers software involved /O operations. First, pro- gram issues /O request via /O call. request passed /O device. device completes /O, interrupt sent interrupt handler invoked. Eventually, control relinquished back process initiated /O. Example 1: 80 386 Interrupt Architecture 80 /C286 processors two hardware interrupt pins. labeled INTR NMI. NMI nonmaskable interrupt, means cannot blocked processor must respond it. TheNMI input usually reserved critical system functions. INTR input mask-able interrupt request line CPU programmable interrupt controller(8259A PIC). Interrupts INTR enabled disabled using instructionsSTI (set interrupt ﬂag) CLI (clear interrupt ﬂag), respectively. Interrupt handlers called interrupt service routines (ISR). address interrupt service routine stored four consecutive memory locations inter-rupt vector table (IVT). IVT stores pointers ISR type interrupt.When interrupt occurs, 8-bit type number supplied processor,which identiﬁes appropriate entry table. interrupt generated device, goes PIC. Multiple interrupts may generated simultaneously. However, buffered PIC. ThePIC decides one interrupts forwarded CPU. Toinform CPU outstanding interrupt waiting processed, PIC sends interrupt request (INTR) CPU, then, appropriate time, responds interrupt acknowledgment (INTA). time, PIC willput 8-bit interrupt type number associated device bus thatthe CPU identify interrupt handler invoke. case severalinterrupts pending, PIC send next interrupt request CPU afterit receives end interrupt command current ISR. Figure 8.8 showsthe simple protocol used determine ISR invoked. computer designs used single PIC (PC XT), eight different inter- rupt requests allowed (IRQ0–IRQ7). Table 8.1 shows list standard interrupttype numbers typical devices. designed, second PIC added,HardwareDevice Independent SoftwareUser Processes I/O RequestI/O Reply Perform I/OWakeup driver I/O doneDevice Drivers Interrupt Handlers Figure 8.7 Layered /O software170 INPUT–OUTPUT DESIGN ORGANIZATIONincreasing number interrupt inputs 15. Figure 8.9 shows two PICS wired cascade. One PIC designated master becomes slave. shownin ﬁgure, slave interrupts input via IRQ1 master. general, eight different slaves accommodated single PIC. Example 2: ARM Interrupt Architecture ARM stands Advanced RISC Machines. ARM 16 /32-bit architecture used portable devices low power consumption reasonable performance. Interrupt requests ARM core collected controlled interrupt controller, calledATIC. interrupt controller provides interface core collectup 64 interrupt requests. usual sequence events interrupts follows. Interrupts would enabled source (such peripheral), enabled interrupt controller,and ﬁnally, enabled core. interrupt occurs source, signal isrouted interrupt controller ARM core. interrupt controller,the interrupt enabled disabled core assigned priorityDevice 8259A PIC CPU ISR1. IRQ# Interrupt 2. INTR3. INTA 4. INT # 5. Invoke8. Return7. End Interrupt 6. Service Figure 8.8 Interrupt handling 80 /C286 TABLE 8.1 Standard IBM-PC Interrupt Type Numbers Typical Devices DeviceIRQ no.Interrupt type number Programmable interval timer 0 08H Keyboard 1 09HCascading second PICs 2 Reserved Serial communication port (COM2) 3 0BH Serial communication port (COM1) 4 0CHFixed disk controller 5 0DH Floppy disk controller 6 0EH Parallel printer controller 7 0FH8.3. INTERRUPT-DRIVEN /O 171level. interrupt request reaches core, halt core normal processing routines allow interrupt request serviced. Among different interrupt requests ARM core handle IRQ FIQ requests. IRQ (normal interrupt request) used general-purpose inter-rupt handling. lower priority FIQ (fast interrupt request) masked FIQ sequence entered. FIQ used support high- speed data transfer channel processes.IRQ0 IRQ1 IRQ2 IRQ3 IRQ4 IRQ5 IRQ6 IRQ7D0-D7INT Address busINTR8259A Master INTA INTA CS SP/EN 8259A SlaveIRQ8 IRQ9 IRQ10 IRQ11 IRQ12 IRQ13 IRQ14 IRQ15D0-D7INT Address busINTA CS SP/EN Figure 8.9 Fifteen different interrupts supported two PICs wired cascade172 INPUT–OUTPUT DESIGN ORGANIZATIONSimilar 80 /C286, addresses interrupt handlers stored vector table, shown Table 8.2. example, IRQ detected core, accesses address 0 /C218 vector table executes instruction loaded address. Normally, instruction found 0 /C218 vector table form: LDR PC ,IRQ_Handler (load address IRQ interrupt handler PC). FIQ detected core, accesses address 0 /C21C vector table executes instruction loaded address. Normally, instruction foundat 0/C21C vector table form: LDR PC ,FIQ_Handler . interrupt occurs, following happens inside core: 1. CPSR (current program state register) copied SPSR (saved pro- gram status register) mode entered. 2. CPSR bits set appropriate mode entered, core set ARM state, relevant interrupt disable ﬂags set. 3. appropriate set banked registers banked in. 4. return address stored link register (of relevant mode). 5. PC set relevant vector address. example, IRQ interrupt detected, ARM core enables SPSR_irq CPSR, enters IRQ mode setting mode bits CSPR 10010, dis- ables Normal interrupts setting bit CPSR, saves address nextinstruction R14_irq, loads 0 /C218 PC. address 0 /C218, instruction load address interrupt handler PC. Similarly, FIQ interrupt detected, ARM core enables SPSR_ﬁq CPSR, enters FIQ mode setting mode bits CSPR 10001, disables Normal andFast interrupts setting F bits CPSR, saves address thenext instruction R14_ﬁq, loads 0 /C21C PC. address 0 /C21C, instruc- tion load address interrupt handler PC. MC9328MX1 /MXL AITC MC9328MX1 /MXL AITC contains twenty-six 32-bit registers, described Table 8.3. Using registers, AITC allows selection whether pending interrupt source create Normal interrupt (IRQ) Fast interrupt (FIQ) core. accomplished via theTABLE 8.2 Interrupt Vector Table Exception type Mode Address Reset Supervisor 0 /C200000000 Undeﬁned instructions Undeﬁned 0 /C200000004 Software interrupts (SWI) Supervisor 0 /C200000008 Prefetch abort Abort 0 /C20000000C Data abort Abort 0 /C200000010 IRQ (Normal interrupt) IRQ 0 /C200000018 FIQ (Fast interrupt) FIQ 0 /C20000001C8.3. INTERRUPT-DRIVEN /O 173TABLE 8.3 AITC Registers Register Description INTCNTL Conﬁgures speciﬁc control functions AITC. NIMASK Controls Normal interrupt mask level. Normal interrupt priority levels programmed NIMASK register masked. Normal interrupt priorities programmed via NIPRIORITY[7 : 0] registers. INTENNUM Provides hardware accelerated enabling interrupts. done programming register interrupt source desired enabled. immediately enable (set) interrupt source bit INTENABLEH /L register. INTDISNUM Provides hardware accelerated disabling interrupts. done programming register interrupt source isdesired disabled. immediately disable (clear) interrupt source bit INTENABLEH /L register. INTENABLEH Used enable pending interrupt source bits [63–32] core. INTENABLEL Used enable pending interrupt source bits [31–0] core. INTTYPEH Used select whether enabled pending interrupt source bit [63–32] create Normal interrupt Fast interrupt core. INTTYPEL Used select whether enabled pending interrupt source bit [31–0] create Normal interrupt Fast interrupt core. NIPRIORITY[7 : 0] Provides software prioritization Normal interrupts. NIVECSR Provides priority highest pending Normal interrupt provides source number highest pending Normal interrupt. FIVECSR Provides source number highest pending Fast interrupt. INTSRCH Reﬂects status interrupt request inputs (sources 63–32) interrupt controller. INTSCRL Reﬂects status interrupt request inputs (sources 31–0) interrupt controller. INTFRCH Allows software generation interrupts interrupt sources 63 32. INTFRCL Allows software generation interrupts interrupt sources 31 0. NIPNDH Reﬂects source number(s) pending Normal interrupt requests, interrupt sources 63 32. NIPNDL Reﬂects source number(s) pending Normal interrupt requests, interrupt sources 31 0. FIPNDH Reﬂects source number(s) pending Fast interrupt requests, interrupt sources 63 32. FIPNDL Reﬂects source number(s) pending Fast interrupt requests, interrupt sources 31 0.174 INPUT–OUTPUT DESIGN ORGANIZATIONINTTYPEH INTTYPEL registers. bit registers corresponds interrupt source available system. Setting bit select correspondinginterrupt source Fast interrupt, whereas clearing bit select corre-sponding bit Normal interrupt. INTTYPEL register, bit 0 correspondsto interrupt source 0, bit 1 corresponds interrupt source 1, bit31, corresponds interrupt source 31. INTTYPEH register, bit 0 cor-responds interrupt source 32, bit 1 corresponds interrupt source 33, bit 31, corresponds interrupt source 63. determining type pending interrupt, next step enable interrupt. done via INTENABLEH INTENABLEL registers. enable pending interrupt core, corresponding interrupt source bit theINTENABLEH INTENABLEL must set. Likewise, disable interrupt,clear bit. INTENABLEL register, bit 0 corresponds interrupt source0, bit 1 corresponds interrupt source 1, bit 31, correspondsto interrupt source 31. INTENABLEH register, bit 0 corresponds interrupt source 32, bit 1 corresponds interrupt source 33, bit 31, corresponds interrupt source 63. example, select interrupt source bit 15as Normal interrupt, clear bit 15 INTTYPEL register. Then, enable thisinterrupt, set bit 15 INTENABLEL register. Likewise, select interruptsource bit 45 Fast interrupt, set bit 13 INTTYPEH register. Then, toenable interrupt, set bit 13 INTENABLEH. AITC also allows pro-grammer prioritize pending Normal interrupt sources one 16 different priority levels. done NIPRIORITY[7:0] registers. 8.4. DIRECT MEMORY ACCESS (DMA) main idea direct memory access (DMA) enable peripheral devices cut “middle man” role CPU data transfer. allows peripheral devices transfer data directly memory without intervention CPU. peripheral devices access memory directly would allow CPU work,which would lead improved performance, especially cases large transfers. DMA controller piece hardware controls one peripheral devices. allows devices transfer data system’s memory withoutthe help processor. typical DMA transfer, event notiﬁes DMA controller data needs transferred memory. DMA CPU use memory bus one use memory time. DMA controller sends request CPU asking permission use bus. CPU returns acknowledgment DMA controller grantingit bus access. DMA take control bus independently conductmemory transfer. transfer complete DMA relinquishes control ofthe bus CPU. Processors support DMA provide one input signalsthat bus requester assert gain control bus one output signals CPU asserts indicate relinquished bus. Figure 8.10 shows DMA controller shares CPU’s memory bus.8.4. DIRECT MEMORY ACCESS (DMA) 175Direct memory access controllers require initialization CPU. Typical setup parameters include address source area, address destination area, length block, whether DMA controller generate processorinterrupt block transfer complete. DMA controller address reg-ister, word count register, control register. address register contains anaddress speciﬁes memory location data transferred. typicallypossible DMA controller automatically increment address register word transfer, next transfer next memory location. word count register holds number words transferred.The word count decremented one word transfer. control registerspeciﬁes transfer mode. Direct memory access data transfer performed burst mode single- cycle mode. burst mode, DMA controller keeps control bus allthe data transferred (from) memory (to) peripheral device.This mode transfer needed fast devices data transfer cannot stopped entire transfer done. single-cycle mode (cycle stealing), DMA controller relinquishes bus transfer one data word. mini-mizes amount time DMA controller keeps CPU controllingthe bus, requires bus request /acknowledge sequence performed every single transfer. overhead result degradation performance.The single-cycle mode preferred system cannot tolerate fewcycles added interrupt latency peripheral devices buffer large amounts data, causing DMA controller tie bus excessive amount time. following steps summarize DMA operations: 1. DMA controller initiates data transfer. 2. Data moved (increasing address memory, reducing count words moved).CPU MemoryDMA Controller DeviceDMA AcknowledgementDMA Request Control SignalsData BusAddress Bus Figure 8.10 DMA controller shares CPU’s memory bus176 INPUT–OUTPUT DESIGN ORGANIZATION3. word count reaches zero, DMA informs CPU termination means interrupt. 4. CPU regains access memory bus. DMA controller may multiple channels. channel associated address register count register. initiate data transfer device driversets DMA channel’s address count registers together direction ofthe data transfer, read write. transfer taking place, CPU free things. transfer complete, CPU interrupted. Direct memory access channels cannot shared device drivers. device driver must able determine DMA channel use. devices ﬁxed DMA channel, others ﬂexible, device drivercan simply pick free DMA channel use. Linux tracks usage DMA channels using vector dma_chan data structures (one per DMA channel). dma_chan data structure contains twoﬁelds, pointer string describing owner DMA channel ﬂag indi- cating DMA channel allocated not. 8.5. BUSES bus computer terminology represents physical connection used carry signal one point another. signal carried bus may represent address,data, control signal, power. Typically, bus consists number connectionsrunning together. connection called bus line . bus line normally ident- iﬁed number. Related groups bus lines usually identiﬁed name. Forexample, group bus lines 1 16 given computer system may used carry address memory locations, therefore identiﬁed address lines . Depending signal carried, exist least four types buses: address , data,control , power buses. Data buses carry data, control buses carry control signals, power buses carry power-supply /ground voltage. size (number lines) address, data, control bus varies one system another. Consider, example, bus connecting CPU memory givensystem, called CPU bus . size memory system 512M- word word 32 bits. system, size address bus log 2(512/C2220)¼29 lines, size data bus 32 lines, least one control line ( /C22R R=W) exist system. addition carrying control signals, control bus carry timing signals. signals used determine exact timing data transfer bus; is, determine given computer system component, asthe processor, memory, /O devices, place data bus receive data bus. bus synchronous data transfer bus controlled bus clock . clock acts timing reference bus signals. bus asynchronous data transfer bus based avail- ability data clock signal. Data transferred asynchronous8.5. BUSES 177bus using technique called handshaking . operations synchronous asyn- chronous buses explained below. understand difference synchronous asynchronous, let us con- sider case master CPU DMA source data trans- ferred slave /O device. following sequence events involving master slave: 1. Master: send request use bus 2. Master: request granted bus allocated master3. Master: place address /data bus 4. Slave: slave selected5. Master: signal data transfer 6. Slave: take data7. Master: free bus 8.5.1. Synchronous Buses synchronous buses, steps data transfer take place ﬁxed clock cycles. Everything synchronized bus clock clock signals made available toboth master slave. bus clock square wave signal. cycle starts atone rising edge clock ends next rising edge, beginningof next cycle. transfer may take multiple bus cycles depending speedparameters bus two ends transfer. One scenario would ﬁrst clock cycle, master puts address address bus, puts data data bus, asserts appropriate control lines.Slave recognizes address address bus ﬁrst cycle reads newvalue bus second cycle. Synchronous buses simple easily implemented. However, connect- ing devices varying speeds synchronous bus, slowest device deter-mine speed bus. Also, synchronous bus length could limited toavoid clock-skewing problems. 8.5.2. Asynchronous Buses ﬁxed clock cycles asynchronous buses. Handshaking used instead. Figure 8.11 shows handshaking protocol. master asserts data-ready line 1 23 41 23 4Data-ready Data-acceptData Data Data Data-Bus Figure 8.11 Asynchronous bus timing using handshaking protocol178 INPUT–OUTPUT DESIGN ORGANIZATION(point 1 ﬁgure) sees data-accept signal. slave sees data- ready signal, assert data-accept line (point 2 ﬁgure). rising thedata-accept line trigger falling data-ready line removal datafrom bus. falling data-ready line (point 3 ﬁgure) trigger thefalling data-accept line (point 4 ﬁgure). handshaking, iscalled fully interlocked, repeated data completely transferred. Asyn-chronous bus appropriate different speed devices. 8.5.3. Bus Arbitration Bus arbitration needed resolve conﬂicts two devices want become bus master time. short, arbitration process select-ing next bus master among multiple candidates. Conﬂicts resolved based fairness priority centralized distributed mechanisms. Centralized Arbitration centralized arbitration schemes, single arbiter used select next master. simple form centralized arbitration uses bus request line, bus grant line, bus busy line. lines shared bypotential masters, daisy-chained cascade. Figure 8.12 shows simple centralized arbitration scheme. ﬁgure, potential masters submit bus request time. ﬁxed priority set among masters left right. bus request received central bus arbiter, issues bus grant asserting bus grantline. potential master closest arbiter (potential master 1) seesthe bus grant signal, checks see made bus request. yes, takes overthe bus stops propagation bus grant signal further. made arequest, simple turn bus grant signal next master right (potential master 2), on. transaction complete, busy line deasserted. Instead using shared request grant lines, multiple bus request bus grant lines used. one scheme, master independent request grant line shown Figure 8.13. central arbiter employ priority-based fairness-based tiebreaker. Another scheme allows masters mul-tiple priority levels. priority level, bus request bus grant line. Within priority level, daisy chain used. scheme, device isattached daisy chain one priority level. arbiter receives multiple Central Bus ArbiterPotential Master 1Potential Master 2Potential Master n Bus BusyBus RequestBus Grant Figure 8.12 Centralized arbiter daisy-chain scheme8.5. BUSES 179bus requests different levels, grants bus level highest priority. Daisy chaining used among devices level. Figure 8.14shows example four devices included two priority levels. Potential master 1 potential master 3 daisy-chained level 1 potential master 2 potential master 4 daisy-chained level 2. Decentralized Arbitration decentralized arbitration schemes, priority-based arbitration usually used distributed fashion. potential master unique arbitration number, used resolving conﬂicts multiplerequests submitted. example, conﬂict always resolved favor device highest arbitration number. question deter- mine device highest arbitration number? One method request-ing device would make unique arbitration number available devices.Each device compares number arbitration number. device withthe smaller number always dismissed. Eventually, requester highestarbitration number survive granted bus access.Req-1 Grant-1 Req-2 Grant-2 Req-nGrant-nCentral Bus ArbiterPotential Master 1Potential Master 2Potential Master n Bus Busy Figure 8.13 Centralized arbiter independent request grant lines Central Bus Arbiter Request level 1 Bus BusyRequest level 2Grant level 1 Grant level 2Potential Master 1Potential Master 2Potential Master 3Potential Master 4 Figure 8.14 Centralized arbiter two priority levels (four devices)180 INPUT–OUTPUT DESIGN ORGANIZATION8.6. INPUT–OUTPUT INTERFACES interface data path two separate devices computer system. Inter- face buses classiﬁed based number bits transmitted agiven time serial versus parallel ports. serial port, 1 bit data trans-ferred time. Mice modems usually connected serial ports. parallelport allows 1 bit data processed once. Printers common peripheral devices connected parallel ports. Table 8.4 shows summary variety buses interfaces used personal computers. TABLE 8.4 Descriptions Buses Interfaces Used Personal Computers Bus/Interface Description PS/2 type port (or interface) used connect mice keyboards computer. PS /2 port sometimes called mouse port. Industry standard architecture (ISA)ISA originally 8-bit bus later expanded 16-bit bus 1984. 1993, Intel Microsoft introduced plug play ISA bus allowed computer automatically detect set computer ISA peripherals modem sound card. Extended industry standardarchitecture (EISA)EISA enhanced form ISA, allows 32-bit data transfers, maintaining support 8- 16-bit expansion boards. However, bus speed, like ISA, 8 MHz. EISA widely used, due high cost complicated nature. Micro channel architecture (MCA)MCA introduced IBM 1987. offered several additional features ISA 32-bit bus, automaticallyconﬁgured cards bus mastering greater efﬁciency. isslightly superior EISA, many expansion boards ever made ﬁt MCA speciﬁcations. VESA (Video electronics standardsassociation) local bus (VLB)The VESA, nonproﬁt organization founded NEC, released VLB 1992. 32-bit bus direct access system memory speed processor, commonly 486 CPU(33/40 MHz). VLB 2.0 later released 1994 64-bit bus bus speed 50 MHz. Peripheral component interconnect (PCI)PCI introduced Intel 1992, revised 1993 version 2.0, later revised 1995 PCI 2.1. 32-bit bus alsoavailable 64-bit bus today. Many modern expansion boards connected PCI slots. Advanced graphic port (AGP)AGP introduced Intel 1997. AGP 32-bit bus designed high demands 3D graphics. AGP direct line memory, allows 3D elements stored system memory instead video memory. AGP geared towards data-intensive graphics cards, 3D accelerators; design allows datathroughput rates 266 MB /s. (continued )8.6. INPUT–OUTPUT INTERFACES 1818.7. SUMMARY One major features computer system ability exchange data devices allow user interact system. chapter focused /O system way processor /O devices exchange data computer system. chapter described three ways organizing /O: programmed I/O, interrupt-driven /O, DMA. programmed /O, CPU handles trans- fers, take place registers devices. interrupt-driven /O, CPU handles data transfers /O module running concurrently. DMA, data transferred memory /O devices without intervention CPU. also studied two methods synchronization: polling interrupts. polling, processor polls device waiting /O complete. Clearly processor cycles wasted method. Using interrupts, processors free switch tasks /O. Devices assert interrupts /O complete. InterruptsTABLE 8.4 Continued Bus/Interface Description Universal serial bus (USB)USB external bus developed Intel, Compaq, DEC, IBM, Microsoft, NEC Northern Telcom. released 1996 Intel 430HX Triton II Mother Board. USB capability transferring 12 Mbps, supporting 127 devices. Many devices connected USB ports, support plug play. FireWire (IEEE 1394)FireWire type external bus, supports fast transfer rates: 400 Mbps. this, FireWire suitable connecting video devices, VCRs, computer. Small computer system interface(SCSI)SCSI type parallel interface commonly used mass storage devices. SCSI transfer data rates 4 MB /s; addition, several varieties SCSI support higher speeds: Fast SCSI (10 MB /s), Ultra SCSI Fast Wide SCSI (20 MB /s), well Ultra Wide SCSI (40 MB /s). Integrated drive electronics (IDE)IDE commonly used interface hard disk drives CD-ROM drives. less expensive SCSI, offers slightly less terms performance. Enhanced integrated drive electronics(EIDE)EIDE improved version IDE, offers better performance standard SCSI. offers transfer rates between4 16.6 MB /s. PCI-X PCI-X high performance bus designed meet increased /O demands technologies Fibre Channel, Gigabit Ethernet, Ultra3 SCSI. Communication network riser(CNR)CNR introduced Intel 2000. speciﬁcation supports audio, modem USB local area networking interfaces core logic chipsets.182 INPUT–OUTPUT DESIGN ORGANIZATIONincurs delay penalty. Two examples interrupt handling covered: 80 /C286 family ARM. chapter also covered buses interfaces. wide variety interfaces buses used personal computers summarized. EXERCISES 1. Conduct Internet search /O devices prepare table categorizing different devices separate categories, example input, output, character-based, block-based, on. every entry table, indicateits speed, interface, category. 2. advantages disadvantages isolated versus memory mapped /O. 3. Show data transfer disk memory conducted following /O schemes: programmed /O, interrupt-driven /O, DMA. Show steps taken case. 4. interrupt requires 50 ms overhead time, polling requires 5 ms per device, describe different situations seems better other. 5. entities computer system device driver communicate with? functions device driver? List operations. 6. types operations DMA used accelerate? 7. DMA module transferring data memory using cycle stealing device transmits data rate 19,200 bits per second. speed CPU 3 MIPS. much would DMA module affect perform- ance CPU. 8. Describe scenarios synchronous bus would outperform asyn- chronous bus vice versa. 9. Discuss advantages disadvantages different bus arbitration policies covered chapter. Prepare contract table compares arbitration techniques implementation operational aspects. REFERENCES READING I. Englander, Architecture Computer Hardware System Software , John Wiley, New York, 1996. C. Hamacher, Z. Vranesic, S. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002. V. Heuring, H. Jordan, Computer Systems Design Architecture , Addison Wesley, NJ, USA, 1997. S. Shiva, Computer Design Architecture , Harper Collins, MA, USA, 1991. A. Tanenbaum, Structured Computer Organization , 4th ed., Prentice Hall, NJ, USA, 1999.REFERENCES READING 183J. Uffenbeck, 80 /C286 Family, Design, Programming, Interfacing , 3rd ed., Prentice Hall, Essex, UK, 2002. Websites PCI Local Bus Speciﬁcations, www.pcisig.com /developers SCSI-3 Architecture Model (SAM), www.ansi.org Universal Serial Bus Speciﬁcation, www.usb.org /developers www.arm.comwww.motorola.com /semiconductors184 INPUT–OUTPUT DESIGN ORGANIZATION& CHAPTER 9 Pipelining Design Techniques exist two basic techniques increase instruction execution rate pro- cessor. increase clock rate, thus decreasing instruction executiontime, alternatively increase number instructions executedsimultaneously. Pipelining instruction-level parallelism examples thelatter technique. Pipelining owes origin car assembly lines. idea tohave one instruction processed processor sametime. Similar assembly line, success pipeline depends upon dividingthe execution instruction among number subunits (stages), perform- ing part required operations. possible division consider instruction fetch ( F), instruction decode ( D), operand fetch ( F), instruction execution ( E), store results ( S) subtasks needed execution instruction. case, possible ﬁve instructions pipeline thesame time, thus reducing instruction execution latency. Chapter, discussthe basic concepts involved designing instruction pipelines. Performancemeasures pipeline introduced. main issues contributing instructionpipeline hazards discussed possible solutions introduced. addition, introduce concept arithmetic pipelining together prob- lems involved designing pipeline. coverage concludes review ofa recent pipeline processor. 9.1. GENERAL CONCEPTS Pipelining refers technique given task divided number subtasks need performed sequence. subtask performed given functional unit. units connected serial fashion themoperate simultaneously. use pipelining improves performance comparedto traditional sequential execution tasks. Figure 9.1 shows illustration thebasic difference executing four subtasks given instruction (in casefetching F, decoding D, execution E, writing results W) using pipelining sequential processing. 185Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.It clear ﬁgure total time required process three instruc- tions ( I1,I2,I3) six time units four-stage pipelining used compared 12 time units sequential processing used. possible saving 50% execution time three instructions obtained. order formulatesome performance measures goodness pipeline processing series oftasks, space time chart (called Gantt’s chart) used. chart shows suc-cession subtasks pipe respect time. Figure 9.2 shows Gantt’schart. chart, vertical axis represents subunits (four case) andthe horizontal axis represents time (measured terms time unit requiredfor unit perform task). developing Gantt’s chart, assume time (T) taken subunit perform task same; call unit time . seen ﬁgure, 13 time units needed ﬁnish executing 10 instructions ( 1toI10). compared 40 time units sequential proces- sing used (ten instructions requiring four time units). following analysis, provide three performance measures good- ness pipeline. Speed-up S(n) ,Throughput U(n) , Efﬁciency E(n). noted analysis assume unit time T¼t units. 1.Speed-up (n) Consider execution mtasks (instructions) using n-stages (units) pipeline. seen, nþm/C01 time units required(a) Sequential Processing I1I1 I2 I3 I2 I3 TimeF1 D1 E1 W1 F1 D1 E1 W1F2 D2 E2 W2 F2 D2 E2 W2F3 D3 E3 W3 F3 123456789 1 0 1 1 1 2D3 E3 W3 (b) Pipelining Figure 9.1 Pipelining versus sequential processing U4 U3 U2 U1 I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10 123456789 1 0 1 1 1 2 1 3 Time Figure 9.2 space–time chart (Gantt chart)186 PIPELINING DESIGN TECHNIQUESto complete mtasks. Speed -up S(n)¼Time using sequential processing Time using pipeline processing¼m/C2n/C2t (nþm/C01)/C2t ¼m/C2n nþm/C01 Lim m!1S(n)¼n(i:e:,n-fold increase speed theoretically possible) 2.Throughput U (n) Throughput U (n)¼no:of tasks executed per unit time ¼m (nþm/C01)/C2t Lim m!1U(n)¼1 assuming t¼1 unit time 3.Efﬁciency E (n) Efﬁciency E (n)¼Ratio actual speed-up maximum speed-up ¼Speed -up n¼m nþm/C01 Lim m!1E(n)¼1 9.2. INSTRUCTION PIPELINE simple analysis made Section 9.1 ignores important aspect affect performance pipeline, is, pipeline stall . pipeline operation said stalled one unit (stage) requires time perform function, thusforcing stages become idle. Consider, example, case instruction fetch incurs cache miss . Assume also cache miss requires three extra time units . Figure 9.3 illustrates effect instruction 2incurring cache miss (assuming execution ten instructions I1toI10). U4 U3 U2 U1I1 I1 I1I2I3I4I5I6I7I8I9I10 I2I3I4I5I6I7I8I9I10 I2I3I4I5I6I7I8I9I10 I3 I1I2 I4I5I6I7I8I9I10 123456789 1 0 1 1 1 2 1 4 13 15 16 Figure 9.3 Effect cache miss pipeline9.2. INSTRUCTION PIPELINE 187The ﬁgure shows due extra time units needed instruction I2to fetched, pipeline stalls, is, fetching instruction I3and subsequent instruc- tions delayed. situations create known pipeline bubble (or pipe- linehazards ). creation pipeline bubble leads wasted unit times, thus leading overall increase number time units needed ﬁnish executing given number instructions. number time units needed execute 10instructions shown Figure 9.3 16 time units, compared 13 time units cache misses. Pipeline hazards take place number reasons. Among instruction dependency data dependency. explained below. 9.2.1. Pipeline “Stall” Due Instruction Dependency Correct operation pipeline requires operation performed stage MUST depend operation(s) performed stage(s). Instruction depen-dency refers case whereby fetching instruction depends resultsof executing previous instruction. Instruction dependency manifests execution conditional branch instruction. Consider, example, case “branch negative” instruction. case, next instruction fetch notbe known result executing “branch negative” instruction known. following discussion, assume instruction followinga conditional branch instruction fetched result executing thebranch instruction known (stored). following example shows effect ofinstruction dependency pipeline. Example 1 Consider execution ten instructions 1–I10on pipeline con- sisting four pipeline stages: IF(instruction fetch), ID(instruction decode), IE (instruction execute), IS(instruction results store). Assume instruction I4is conditional branch instruction executed, branch taken, is, branch condition(s) is(are) satisﬁed. Assume also branch instruction fetched, pipeline stalls result executingthe branch instruction stored. Show succession instructions pipeline;that is, show Gantt’s chart. Figure 9.4 shows required Gantt’s chart. Thebubble created due pipeline stall clearly shown ﬁgure. IE ID IFI1 I1 I1I2I3I4 I5I6I7I8I9I10 I2I3I4 I5I6I7I8I9I10 I2I3I4 I5I6I7I8I9I10 I1I2I3I4 I5I6I7I8I9I10 123456789 1 0 1 1 1 2 1 4 13 15 16 Figure 9.4 Instruction dependency effect pipeline188 PIPELINING DESIGN TECHNIQUES9.2.2. Pipeline “Stall” Due Data Dependency Data dependency pipeline occurs source operand instruction Iidepends results executing preceding instruction, Ij,i.j. noted although instruction Iican fetched, operand(s) may avail- able results instruction Ijare stored. following example shows effect data dependency pipeline. Example 2 Consider execution following piece code: † ADD R 1,R2,R3; R3 R1þR2 SL R 3; R3 SL(R3) SUB R 5,R6,R4; R4 R52R6 † piece code, ﬁrst instruction, call Ii, adds contents two registers R1andR2and stores result register R3. second instruction, call Iiþ1, shifts contents R3one bit position left stores result back intoR3. third instruction, call Iiþ2, stores result subtracting content ofR6from content R5in register R4. order show effect data dependency, assume pipeline consists ﬁve stages, IF,ID,OF,IE, andIS. case, OFstage represents operand fetch stage. functions remaining four stages remain explained before. Figure 9.5 shows Gantt’s chart piece code. shown ﬁgure, although instruction Iiþ1 successfully decoded time unit kþ2, instruction cannot pro- ceed OFunit time unit kþ3. operand fetched byIiþ1during time unit kþ3 content register R3, modiﬁed execution instruction Ii. However, modiﬁed value R3will available end time unit kþ4. require instruction Iiþ1 wait (at output IDunit) kþ5. Notice instruction Iiþ2will IE ID IiIiIiIiIi Ii+1Ii+1 Ii+2 kk + 1k+ 2k+ 3k+ 4k+ 5k+ 6Ii+2Ii+2Ii+2Ii+2 Ii+1Ii+1 Ii+1 Time Figure 9.5 write-after-write data dependency9.2. INSTRUCTION PIPELINE 189have also wait (at output IFunit) time instruction Iiþ1 proceeds ID. net result pipeline stall takes place due data dependency exists instruction Iiand instruction Iiþ1. data dependency presented example resulted register R3 destination instructions IiandIiþ1. called write-after-write data dependency. Taking consideration register written (or read from), total four different possibilities exist, including write-after-write case. three cases read-after-write, write-after-read, read-after-read. Among four cases, read-after-read case notlead pipeline stall. register read operation change content register. Among remaining three cases, write-after-write (see example) read-after-write lead pipeline stall. followingpiece code illustrates read-after-write case: † ADD R 1,R2,R3; R3 R1þR2 SUB R 3,1 ,R4; R4 R321 † case, ﬁrst instruction modiﬁes content register R3(through write operation) second instruction uses modiﬁed contents R3 (through read operation) load value register R4. two instructions proceeding within pipeline, care taken value register R3read second instruction updated value resulting execution previous instruction. Figure 9.6 shows Gantt’s chart case assuming ﬁrst instruction called Iiand second instruction called Iiþ1. clear operand second instruction cannot fetched time unitkþ3 delayed time unit kþ5. modi- ﬁed value content register R3will available time slot kþ5. IE ID IiIiIiIiIi Ii+1Ii+1 kk + 1k+ 2k+ 3k+ 4k+ 5k+ 6Ii+1Ii+1 Ii+1 Time Figure 9.6 read-after-write data dependency190 PIPELINING DESIGN TECHNIQUESFetching operand second instruction time slot kþ3 lead incorrect results. Example 3 Consider execution following sequence instructions ﬁve-stage pipeline consisting IF,ID,OF,IE, IS. required show succession instructions pipeline. I1!Load 21,R1; R1 21; I2!Load 5, R2; R2 5; I3!Sub R2, 1, R2 R2 R221; I4!Add R1,R2,R3; R3 R1þR2; I5!Add R4,R5,R6; R6 R4þR5; I6!SL R 3 R3 SL(R3) I7!Add R6,R4,R7; R7 R4þR6; example, following data dependencies observed: Instructions Type data dependency I3andI2 Read-after-write write-after-write (W-W) I4andI1 Read-after-write (R-W) I4andI3 Read-after-write (R-W) I6andI4 Read-after-write write-after-write (W-W) I7andI5 Read-after-write (R-W) Figure 9.7 illustrates progression instructions pipeline taking consideration data dependencies mentioned above. assumption made constructing Gantt’s chart Figure 9.7 fetching operand aninstruction depends results previous instruction execution delayeduntil operand available, is, result stored. total 16 time units arerequired execute given seven instructions taking consideration datadependencies among different instructions. IE ID IFI1 I1 I1I2I1I2 I3I3I3 I4I5I5 I6 I2 I4I5I4 I6I7I6I7 I7 I2I3 I4 I5I6 I7 I1I2I3I4 I5 I6I7 123456789 1 0 1 1 1 2 1 4 13 15 16 Figure 9.7 Gantt’s chart Example 39.2. INSTRUCTION PIPELINE 191Based results obtained above, compute speed-up throughput executing piece code given Example 3 as: Speed -up S(5)¼Time using sequential processing Time using pipeline processing¼7/C25 16¼2:19 Throughput U (5)¼No :of tasks executed per unit time ¼7 16¼0:44 discussion pipeline stall due instruction data dependencies reveal three main points problems associated dependen- cies. are: 1. instruction data dependencies lead added delay pipeline. 2. Instruction dependency lead fetching wrong instruction.3. Data dependency lead fetching wrong operand. exist number methods deal problems resulting instruction data dependencies. methods try prevent fetching thewrong instruction wrong operand others try reduce delay incurredin pipeline due existence instruction data dependency. number ofthese methods introduced below. Methods Used Prevent Fetching Wrong Instruction Operand Use NOP (No Operation) method used order prevent fetching wrong instruction, case instruction dependency, fetching wrong operand, case data dependency. Recall Example 1. example, execution sequence ten instructions 1–I10on pipeline consisting four pipeline stages: IF,ID,IE, ISwere considered. order show execution instructions pipeline, assumedthat branch instruction fetched, pipeline stalls result executing branch instruction stored. assumption needed order prevent fetching wrong instruction fetching branch instruction. Inreal-life situations, mechanism needed guarantee fetching appropriate instruction appropriate time. Insertion “ NOP ” instructions help carrying task. “ NOP ” instruction effect status processor. Example 4 Consider execution ten instructions 1–I10on pipeline con- sisting four pipeline stages: IF,ID,IE, IS. Assume instruction I4is con- ditional branch instruction executed, branch taken; is, branch condition satisﬁed.192 PIPELINING DESIGN TECHNIQUESIn order execute set instructions preventing fetching wrong instruction, assume speciﬁed number NOP instructions inserted follow instruction I4in sequence precede instruction I5. Figure 9.8 shows Gantt’s chart illustrating execution new sequence instructions (after inserting NOP instructions). ﬁgure shows insertion THREE NOP instructions instruction I4will guar- antee correct instruction fetch I4, case I5, fetched time slot number 8 result executing I4would stored condition branch would known. noted number NOP instructions needed equal ( n21), nis number pipeline stages. Example 4 illustrates use NOP instructions prevent fetching wrong instruction case instruction dependency. similar approach used prevent fetching wrong operand case data dependency. Consider execution following piece code ﬁve-stage pipeline ( IF,ID,OF,IE,IS). ADD R 1,R2,R3; R3 R1þR2 SUB R 3,1 ,R4; R4 R321 MOV R 5,R6; R6 R5 Note data dependency form read-after-write (R-W) ﬁrst two instructions. Fetching operand second instruction, is, fetching content R3, cannot proceed result ﬁrst instruction stored. order achieve that, NOP instructions inserted ﬁrst two instructions shown below. ADD R 1,R2,R3; R3 R1þR2 NOP NOPSUB R 3,1 ,R4; R4 R321 MOV R 5,R6; R6 R5 Execution modiﬁed sequence instructions shown Figure 9.9. ﬁgure shows use NOP guarantees time unit #6 instructionIS IE ID IFI1 I1 I1I2I3I4 I5I6I7I8I9I10 I2I3I4 I5I6I7I8I9I10 I2I3I4 I5I6I7I8I9I10 I1I2I3I4 I5I6I7I8I9I10 123456789 1 0 1 1 1 2 1 4 13 15 16Nop Nop Nop Nop Nop Nop Nop Nop Nop Nop Nop Nop Figure 9.8 use NOP instructions9.2. INSTRUCTION PIPELINE 193I2will fetch correct value R3. value stored result executing instruction I1during time unit #5. Methods Used Reduce Pipeline Stall Due Instruction Dependency Unconditional Branch Instructions order able reduce pipeline stall due unconditional branches, necessary identify unconditional branches early possible fetching wrong instruction. mayalso possible reduce stall reordering instruction sequence. Thesemethods explained below. REORDERING INSTRUCTIONS case, sequence instructions reor- dered correct instructions brought pipeline guaranteeingthe correctness ﬁnal results produced reordered set instructions. Con-sider, example, execution following group instructions 1,I2,I3,I4, I5.,.,Ij,Ijþ1,.,.on pipeline consisting three pipeline stages: IF,IE, IS. group instructions, I4is unconditional branch instruction whereby target instruction Ij. Execution group instructions sequence given lead incorrect fetching instruction I5after fetching instruction I4. However, consider execution reordered sequence I1,I4,I2,I3, I5.,.,Ij,Ijþ1,.,.. Execution reordered sequence using three-stage pipeline shown Figure 9.10. ﬁgure shows reordering instructions causes instruction Ijto fetched time unit #5, is, instruction I4has executed. Reorder- ing instructions done using “smart” compiler scan sequenceof code decide appropriate reordering instructions lead toIS IE ID IFI1 I2I1 I2 I3I3 123456789 1 0NOP NOP NOP NOP I1 I2I1 I2 I3I3 NOP NOP NOP NOP I1 I2I3 NOP NOP Figure 9.9 Use NOP data dependency IE 1234567I1 I3IjIj+1 I4I2I1 I3IjIj+1 I4I2I1 I3IjIj+1 I4I2 Figure 9.10 Instruction reordering194 PIPELINING DESIGN TECHNIQUESproducing correct ﬁnal results minimizing number time units lost due instruction dependency. One important condition must satisﬁedin order reordering instruction method produce correct results isthat set instructions swapped branch instruction hold nodata /or instruction dependency relationship among them. USE DEDICATED HARDWARE FETCH UNIT case, fetch unit assumed associated dedicated hardware unit capable recognizing unconditional branch instructions computing branch target address quickly possible. Consider, example, execution sequence ofinstructions illustrated above. Assume also fetch unit dedicatedhardware unit capable recognizing unconditional branch instructions comput-ing branch address using additional time units. Figure 9.11 shows Gantt’schart sequence instructions. ﬁgure shows correct sequence ofinstructions executed incurring extra unit times. assumption needing additional time units recognize branch instruc- tions computing target branch address unrealistic. typical cases, theadded hardware unit fetch unit require additional time unit(s) carryout task recognizing branch instructions computing target branchaddresses. extra time units needed hardware unit, instruc-tions executed, number extra time units needed may reducedand indeed may eliminated altogether. essence method shownbelow. PRECOMPUTING BRANCHES REORDERING INSTRUCTIONS method considered combination two methods discussed previous two sections above. case, dedicated hardware (used recognizebranch instructions computing target branch address) executes taskconcurrently execution instructions. Consider, example, thesame sequence instructions given above. Assume also dedicatedhardware unit requires one time unit carry task. case, reorderingof instructions become 1,I2,I4,I3,I5.,.,Ij,Ijþ1,.,.should produce correct results causing additional lost time units. illustrated using Gantt’s chart Figure 9.12. Notice time unit #4 used dedicated I4 IjIj+1 I3 I2 I1 I4 IjIj+1 I3 I2 I1 IE I4IjIj+1 I3 I2 I1 12345 6 7 Figure 9.11 Use additional hardware unit branch instruction recognition9.2. INSTRUCTION PIPELINE 195hardware unit compute target branch address concurrently fetching instruction I3. noted success method depends availability instructions executed concurrently dedicated hardware unit com-puting target branch address. case presented above, assumed thatreordering instructions provide instructions executed con-currently target branch computation. However, reordering notpossible, use instruction queue together prefetching instruc-tions help provide needed conditions. explained below. INSTRUCTION PREFETCHING method requires instructions fetched stored instruction queue needed. method also calls fetch unit required hardware needed recognize branch instructions compute target branch address. pipeline stalls due data dependencycausing new instructions fetched pipeline, fetch unit canuse time continue fetching instructions add instructionqueue. hand, delay fetching instructions occurs, forexample, due instruction dependency, prefetched instructions instruction queue used provide pipeline new instructions, thus eliminating otherwise lost time units due instruction dependency. Pro- viding appropriate instruction instruction queue pipeline usually done using called “dispatch unit.” technique prefetchingof instructions executing pipeline stall due instruction depen-dency called “branch folding.” Conditional Branch Instructions techniques discussed context unconditional branch instructions may work case conditional branch instructions. conditional branching target branch address known execution branch instruction completed.Therefore, number techniques used minimize number losttime units due instruction dependency represented conditional branching. DELAYED BRANCH Delayed branch refers case whereby possible ﬁll location(s) following conditional branch instruction, called branch delay slot(s) , useful instruction(s) executed targetIS I3 IjIj+1 I4 I2 I1 I3 IjIj+1 I4 I2 I1 IE I3IjIj+1 I4 I2 I1 12345 6 7 Figure 9.12 Branch folding196 PIPELINING DESIGN TECHNIQUESbranch address known. Consider, example, execution following program loop pipeline consisting two stages: Fetch ( F) Execute ( E). I1!Again :Load 5,R1; R1 5; I2! Sub R 2; R2 R221; I3! Bnn ; Branch result Negative; I4! Add R 4,R5,R3; R3 R4þR5; noted end ﬁrst loop, either instruction I1or instruction I4will fetched depending result executing instruction I3.T h e way situation dealt delay fetching next instruction result executing instruction I3is known. lead incurring extra delay pipeline. However, extra delay may avoided sequence instructions reordered become follows. : Sub R 2; R2 R221; Load 5,R1; R1 5; Bnn ; Branch result Negative; Add R 4,R5,R3; R3 R4þR5; Figure 9.13 shows Gantt’s chart executing modiﬁed piece code thecase R 2¼3 entering loop. ﬁgure indicates branching takes place one instruction later actual place branch instruction appears original instructionsequence, hence name “ delayed branch .” also clear Figure 9.13 reordering sequence instructions, possible ﬁll branch delaytime slot useful instruction, thus eliminating extra delay pipeline.It shown number studies “smart” compilers able makeuse one branch delay time slot 80% cases. use branchdelay time slots led improvement speed-up throughput processors using “smart” compilers. PREDICTION NEXT INSTRUCTION FETCH method tries reduce time unit(s) potentially lost due instruction dependency predicting next instruction fetch fetching conditional branch instruction. basisis branch outcomes random, would possible save about50% otherwise lost time. simple way carry technique E FI2 I1 I3 I2 I1 I3 I2 I1 I3 I2 I1 I3 I4 I1 I2 I3 I2 I1 I3 I2 I1 I3 I2 I1 I3 I4 1 2 3 4 5 6 7 8 11 12 13 14 10 9 Figure 9.13 Delayed branch9.2. INSTRUCTION PIPELINE 197assume whenever conditional branch encountered, system predicts branch taken (or alternatively taken). way, fetching ofinstructions sequential address order continue (or fetching instructionsstarting target branch instruction continue). completion thebranch instruction execution, results known decision haveto made whether instructions executed assuming thebranch taken (or taken) intended correct instruction sequence not. outcome decision one two possibilities. prediction correct, execution continue wasted time units. If, theother hand, wrong prediction made, care must taken suchthat status machine, measured terms memory register contents,should restored speculative execution took place. Prediction based scheme lead branch prediction decision every time given instruction encountered, hence name static branch prediction . simplest branch prediction scheme done compilation time. Another technique used branch prediction dynamic branch pre- diction. case, prediction done run time, rather compile time.When branch encountered, record checked ﬁnd whether thatsame branch encountered so, decision madeat time; is, branch taken taken. run time decision isthen made whether take take branch. making decision, two-state algorithm, “likely taken” (LTK) “likely taken” (LNK), followed. current state LTK branch taken, algorithm maintain LTK state; otherwise switch theLNK. If, hand, current state LNK branch taken,then algorithm maintain LNK state; otherwise switch theLTK state. simple algorithm work ﬁne, particularly branch isgoing backwards, example execution loop. will, however,lead misprediction control reaches last pass loop. robust algorithm uses four states used ARM 11 microarchitec- ture (see below). interesting notice combination dynamic static branch predic- tion techniques lead performance improvement. attempt use dynamic branch prediction ﬁrst made, possible, system resort tothe static prediction technique. Consider, example, ARM 11 microarchitecture (the ﬁrst implementation ARMv6 instruction set architecture). architecture uses dynamic /static branch prediction combination. record form 64-entry, four-state branch target address cache (BTAC) used help dynamic branch prediction ﬁnding whether given branch encountered before. branch encoun-tered, record also show whether frequently taken fre-quently taken. BTAC shows branch encountered before,then prediction made based previous outcome. four states are:strongly taken, weakly taken, strongly taken, weakly taken.198 PIPELINING DESIGN TECHNIQUESIn case record cannot found branch, static branch pre- diction procedure used. static branch prediction procedure investigates branch ﬁnd whether going backwards forwards. branch going back-wards assumed part loop branch assumed taken. Abranch going forwards taken. ARM 11 employs eight-stage pipeline.Every correctly predicted branch found lead typical saving ﬁve processorclock cycles. Around 80% branches found correctly predicted using dynamic /static combination ARM 11 architecture. pipeline features ARM 11 introduced next subsection. branch prediction technique based use 16K-entry branch history record employed UltraSPARC III RISC processor, 14-stage pipeline. How- ever, impact misprediction, terms number cycles lost due abranch misprediction reduced using following approach. predictionsthat branch taken branch target instructions fetched,the “fall-through” instructions prepared issue parallel use four-entry branch miss queue (BMQ). reduces misprediction penalty two cycles. UltraSPARC III achieved 95% success branch prediction.The pipeline features UltraSPARC III introduced next subsection. Methods Used Reduce Pipeline Stall Due Data Dependency Hardware Operand Forwarding Hardware operand forwarding allows result one ALU operation available another ALU operation cycle immediately follows. Consider following two instructions. ADD R 1,R2,R3; R3 R1þR2 SUB R 3,1 ,R4; R4 R321 easy notice exists read-after-write data dependency thesetwo instructions. Correct execution sequence ﬁve-stage pipeline ( IF,ID, OF,IE,IS) cause stall second instruction decoding result ﬁrst instruction stored R 3. time, operand second instruction, is, new value stored R3, fetched second instruction. However, possible result ﬁrst instruction forwarded ALU time unit stored R3, possible reduce stall time. illustrated Figure 9.14. assumption operand second instruction forwarded immedi- ately available stored R3requires modiﬁcation data path added feedback path created allow operand forwarding. modiﬁcation shown using dotted lines Figure 9.15. noted needed modiﬁcation achieve hardware operand forwarding isexpensive requires careful issuing control signals. also noted possible perform instruction decoding operand fetching time unit, lost time units.9.2. INSTRUCTION PIPELINE 199Software Operand Forwarding Operand forwarding alternatively performed software compiler. case, compiler “smart” enough make result(s) performing instructions quicklyavailable, operand(s), subsequent instruction(s). desirable featurerequires compiler perform data dependency analysis order determine operand(s) possibly made available (forwarded) subsequent instructions, thus reducing stall time. data dependency analysis requiresthe recognition basically three forms. explained using simpleexamples. STORE-FETCH case represents data dependency result instruction stored memory followed request fetch thesame result subsequent instruction. Consider following sequence twoinstructions: Store R 2,(R3); M[R3] R2 Load (R3),R4; R4 M[R3] sequence, operand needed second instruction (the contents ofmemory location whose address stored register R 3) already available register R2and therefore immediately (forwarded) moved register R4.I1I2I1I2I1 I2I1 I2 I1 I2 IFIDOFIEIS 12345 6 7 Figure 9.14 Hardware forwarding Figure 9.15 Hardware forwarding200 PIPELINING DESIGN TECHNIQUESWhen recognizes data dependency, “smart” compiler replace sequence following sequence: Store R 2,(R3); M[R3] R2 Move R 2,R4; R4 R2 FETCH-FETCH case represents data dependency data stored instruction also needed operand subsequent instruction. Consider thefollowing instruction sequence: Load (R 3),R2; R2 M[R3] Load (R3),R4; R4 M[R3] sequence, operand needed ﬁrst instruction (the contents memorylocation whose address stored register R 3) also needed operand second instruction. Therefore, operand immediately (forwarded) movedinto register R 4. recognizes data dependency, “smart” compiler replace sequence following sequence. Load (R3),R2; R2 M[R3] Move R 2,R4; R4 R2 STORE-STORE case data stored instruction overwrit- ten subsequent instruction. Consider following instruction sequence: Store R 2,(R3); M[R3] R2 Store R 4,(R3); M[R3] R4 sequence, results written ﬁrst instruction (the content ofregister R 2is written memory location whose address stored register R3) overwritten second instruction contents register R4. Assuming two instructions executed sequence result written theﬁrst instruction needed /O operation, example, DMA, sequence two instructions replaced following singleinstruction. Store R 4,(R3); M[R3] R4 9.3. EXAMPLE PIPELINE PROCESSORS section, brieﬂy present two pipeline processors use variety pipeline techniques presented chapter. focus coverage the9.3. EXAMPLE PIPELINE PROCESSORS 201pipeline features architectures. two processors ARM 1026EJ-S UltraSPARC III. 9.3.1. ARM 1026EJ-S Processor processor part family RISC processors designed Advanced RISC Machine (ARM) Company. series designed suit high-performance, low-cost, low-power embedded applications. ARM 022EJ-S integer core multiple execution units, thus allowing numberof instructions exist pipeline stage. also allows execution sim-ultaneous instructions. ARM 1026EJ-S deliver peak throughput oneinstruction per cycle. integer core consists following units: 1. Prefetch unit: unit responsible instruction fetch. also predicts outcome branches whenever possible. 2. Integer unit: unit responsible decoding instructions coming prefetch unit. unit contains barrel shifter, ALU, multiplier.It executes instructions MOV ,ADD , MUL . integer unit helps load /store unit execute load store instructions. also helps executing coprocessor instructions. 3. Load /Store unit: unit load store two registers (64 bits) per cycle. ARM 1022EJ-S pipeline processor whose ALU consists six stages. are: 1. Fetch stage: instruction cache access branch prediction instructions already fetched. 2. Issue stage: initial instruction decoding. 3. Decode stage: ﬁnal instruction decode, register read ALU operations, forwarding, initial interlock resolution. 4. Execute stage: data access address calculation, data processing shift, shift saturate, ALU operations, ﬁrst stage multiplication, ﬂag setting, condition code check, branch mispredict detection, store data register read. 5. Memory stage: data cache access, second stage multiplication, saturations. 6. Write stage: register write instruction retirement. arrangement, Fetch stage uses ﬁrst-in-ﬁrst-out (FIFO) buffer canhold three instructions. Issue Decode stages contain predictedbranch parallel one instruction. Execute, Memory, Write stages simultaneously contain following. 1. predicted branch 2. ALU multiply instruction3. Ongoing multiply load store multiple instructions4. Ongoing multicycle coprocessor instructions202 PIPELINING DESIGN TECHNIQUESThe prefetch unit operates Fetch stage fetch 64 bits every cycle instruction-side cache. can, however, issue one 32-bit instruction per cycle tothe integer unit. Pending instructions placed prefetch buffer prefetchunit. instruction prefetch buffer, branch prediction logic candecode see predictable branch. prefetch buffer hold tothree instructions enable prefetch unit to: 1. Detect branch instructions ahead fetch stage 2. Predict branches likely taken 3. Remove branches likely taken branch predicted taken, instruction address redirected branch target address. If, however, branch predicted taken,then next instruction fetched. case enough time completelyremove branch, fetch address redirected anyway, thus reducing branchpenalty. integer unit executes unpredictable branches. quickly obtain required address, dedicated fast branch adder used. done order avoid passing barrel shift. prefetch buffer ﬂushed following cases: 1. Entry exception processing sequence 2. load program counter (PC)3. arithmetic manipulation PC 4. Execution unpredicted branch 5. Detection erroneously predicted branch taken predicted branch case lead automatic ﬂush prefetch buffer. Mispredicted branches unpredicted taken branches lead athree-cycle penalty. 9.3.2. UltraSPARC III Processor UltraSPARC III based SUN SPARC-V9 RISC architectural speciﬁcations. number features characterize SPARC-V9. Among following: 1. simple instruction formats. instructions 32-bit. Memory access done exclusively using Load Store instructions. 2. addressing modes. Memory addressing two modes, RegisterþRegister Register þImmediate modes. 3. Triadic register operands. instructions operate two register operands one register constant operand. results cases stored third register. 4. Large window register ﬁle.9.3. EXAMPLE PIPELINE PROCESSORS 203The UltraSPARC III processor uses six independent units (see Fig. 9.16). are: 1. Instruction Issue Unit (IIU). unit predicts program ﬂows, fetches predicted path memory directs fetched instructions theexecution pipeline. Instructions forwarded either IEU FPU.The IIU incorporates four-way associative instruction cache, addresstranslation buffer, 16K-entry branch predictor. 2. Integer Execute Unit (IEU). unit executes integer instructions, including integer loading storing, integer arithmetic, logic, branch instructions. IEU capable executing four integer instructions concurrently cycle time.Instruction Issue Unit (IIU) Instruction Cache Instruction Queue Steering Logic Integer Execution Unit (IEU) Floating-Point Unit (FPU) Dependency/Trap FP Register File Add/Subtract Graphics unit Data Cache Unit (DCU) Prefetch Store System Interface Unit (SIU) External Memory Unit Data Switch Controller Eternal Cache TagsInteger Register File ALU Pipe Load/Store/Special DivideMultiply Data Write Snoop Pipe Controller SRAM DRAM Figure 9.16 functional units UltraSPARC III204 PIPELINING DESIGN TECHNIQUES3. Data Cache Unit (DCU). unit contains three different level-one (L1) data caches data address translation buffer. data caches are: demand fetch (a four-way associative 64KB 32-byte block size), pre-fetch cache (a four-way associative 2KB 64-byte block size), awrite cache (a four-way associative 2KB 64-byte block size). 4. Floating-Point Unit (FPU). unit executes ﬂoating-point graphical instructions. 5. External Memory Unit (EMU). unit controls access two off- chip memory modules. two off-chip modules level-two (L2)data cache main memory. 6. System Interface Unit (SIU). unit provides communication interface microprocessor system external it, mainmemory, /O devices, processors multiprocessing conﬁguration. UltraSPARC III 14-stage instruction pipeline. are: 1. Address Generation Unit (A). unit generates instruction fetch addresses. 2. Instruction Prefetch Unit (P). unit fetches second cycle instruc- tions cache accesses ﬁrst cycle branch prediction. 3. Instruction Fetch Unit (F). unit fetches second cycle instructions cache accesses second cycle branch prediction. F unit also performs virtual physical address translation. 4. Branch Target Calculation Unit (B). unit computes target address branches decodes ﬁrst cycle instructions. 5. Instruction Decode Unit (I). unit decodes second cycle instruc- tions directs queue. 6. Instruction Steer Unit (J). unit directs instructions appropriate execution unit. Integer instructions directed integer execution unit ﬂoating-point graphical instructions directed theﬂoating-point unit. 7. Register File Read Unit (R). unit reads operands integer register ﬁle. 8. Integer Execution Unit (E). unit executes integer instructions. 9. Date Cache Access Unit (C). unit accesses second cycle date cache, forwards load data word double word loads executes ﬁrst cycle ﬂoating-point instructions. 10. Memory Bypass Unit (M). unit loads data alignment half word bytes loads executes second cycle ﬂoating-point instructions. 11. Working Register File Write Unit (W). unit performs writes inte- ger register ﬁle executes third cycle ﬂoating-point instructions. 12. Pipe Extend Unit (X). unit extends integer pipeline precise ﬂoat- ing-point traps executes fourth cycle ﬂoating-point instructions.9.3. EXAMPLE PIPELINE PROCESSORS 20513. Trap Unit (T). unit reports traps upon occurrences. 14. Done Unit (D). unit writes architectural register ﬁle. Two main techniques employed UltraSPARC III dealing branches. explained below: Branch Prediction UltraSPARC III uses branch prediction technique combines static dynamic branch prediction techniques explained before. case, branch prediction takes place IIU unit. uses abranch prediction table hardware implementation dynamic predictionalgorithm. BRANCH PREDICTION TABLE branch prediction table (BPT) hardware implementation table two-bit ﬁnite state machine (FSM). saturated up–down counter. branch encountered, branch target address / branch history used ﬁnd table index location pre- diction branch found. branch condition predicted taken itcorresponds one two FSM states: strong taken weak taken. Thebranch condition predicted taken corresponds one two FSMstates: weak taken strong taken. counter incremented time branchis taken; otherwise decremented, hence name up–down counter. counterreaches strong taken state (11-state), stays long branch taken reaches strong taken (00-state), stays long branch taken, hence name saturation. BPT UltraSPARC III consists of16K-entry (16K 2-bit saturation up–down counters). GLOBAL SHARE DYNAMIC PREDICTION ALGORITHM global share ( gshare ) algorithm uses two levels branch-history information dynamically predictthe direction branches. ﬁrst level registers history last kbranches faced. represents global branching behavior. level implemented byproviding global branch history register. basically shift register entersa 1 every taken branch 0 every untaken branch. second level ofbranch history information registers branching last soccurrences speciﬁc pattern kbranches. information kept branch prediction table. gshare algorithm works taking lower bits branch target address XORing history register get index used prediction table. UltraSPARC III uses modiﬁed version gshare algorithm. modi- ﬁcation requires predictor pipelined two stages, is, originalgshare algorithm used, predictor would indexed old copy program counter (PC). modiﬁed gshare algorithm, time predictor accessed, eight counters read three low-order bits PC registerare used select one B pipeline stage.206 PIPELINING DESIGN TECHNIQUESInstruction Buffer (Queues) UltraSPARC III instruction issue unit (IIU) incorporates two instruction buffering queues: branch instruction queue (BIQ) branch miss queue (BMQ). introduced below. BRANCH INSTRUCTION QUEUE (BIQ) 20-entry queue allows fetch execution unit operate independently. fetch unit predicts theexecution path continuously ﬁlls BIQ. taken branch encountered,two fetch cycles lost ﬁll BIQ. BRANCH MISS QUEUE (BMQ) lost two cycles, sequential instructions already accessed buffered four-entry BMQ. thenfound branch mispredicted, instructions BMQ aredirected execution unit directly. 9.4. INSTRUCTION-LEVEL PARALLELISM Contrary pipeline techniques, instruction-level parallelism (ILP) based idea multiple issue processors (MIP). MIP multiple pipelined datapaths instruction execution. pipelines issue execute one instruc-tion per cycle. Figure 9.17 shows case processor three pipes. Forcomparison purposes, also show ﬁgure sequential thesingle pipeline case. clear ﬁgure limit number cycles per instruction case single pipeline CPI ¼1, MIP achieve CPI ,1. order make full use ILP, analysis made identify instruction data dependencies exist given program. analysis lead appropriate scheduling group instructions beissued simultaneously retaining program correctness. Static schedulingresults use long instruction word (VLIW) architectures, dynamicscheduling results use superscalar architectures. VLIW, instruction represents bundle many operations issued sim- ultaneously. compiler responsible checking dependencies makingthe appropriate groupings /scheduling operations. contrast super- scalar architectures, rely entirely hardware scheduling ofinstructions. Superscalar Architectures scalar machine able perform one arith- metic operation once. superscalar architecture (SPA) able fetch, decode, execute, store results several instructions time. trans-forming static sequential instruction stream dynamic parallel one, inorder execute number instructions simultaneously. Upon completion, theSPA reinforces original sequential instruction stream instructionscan completed original order. SPA instruction, processing consists fetch, decode, issue, commit stages. fetch stage, multiple instructions fetched simultaneously.9.4. INSTRUCTION-LEVEL PARALLELISM 207Branch prediction speculative execution also performed fetch stage. done order keep fetching instructions beyond branch andjump instructions. Decoding done two steps. Predecoding performed main memory cache responsible identifying branch instructions.Actual decoding used determine following instruction: (1) oper-ation performed; (2) location operands; (3) location wherethe results stored. issue stage, instructions among thedispatched ones start execution identiﬁed. commit stage,generated values /results written destination registers. crucial step processing instructions SPAs dependency analy- sis. complexity analysis grows quadratically instruction word size. puts limit degree parallelism achieved SPAs degree parallelism higher four impractical. Beyond this(a) Sequential Processing Time F1 I3I3 I2I2 I1I1 (b) PipeliningTime (c) Multiple issueTimeF1 D1 E1 W1 D1 E1 W1 F1 D1 E1 W1 F4 D4 E4 W4 F5 D5 E5 W5 F6 D6 E6 W6 F7 D7 E7 W7 F8 D8 E8 W8 F9 D9 E9 W9F2 F2D2 D2E2 E2W2 W2 F2 D2 E2 W2F3 D3 E3 W3 F3 D3 E3 W3 F3 D3 E3 W3123456789 1 0 1 1 1 2 Figure 9.17 Multiple issue versus pipelining versus sequential processing208 PIPELINING DESIGN TECHNIQUESlimit, dependence analysis scheduling must done compiler. basis VLIW approach. Long Instruction Word (VLIW) approach, compiler performs dependency analysis determines appropriate groupings /scheduling oper- ations. Operations performed simultaneously grouped long instruction word (VLIW). Therefore, instruction word made longenough order accommodate maximum possible degree parallelism.For example, IBM DAISY machine instruction word eight oper-ation long, called 8-issue machine. VLIW, resource binding done devoting ﬁeld instruction word one one functional unit. However, arrangement lead alimit mix instructions issued per cycle. ﬂexible approach allow given instruction ﬁeld occupied different kinds operations. example, Philips TriMedia machine, 5-issue machine, 27 functional units mapped 5-issue slot. IBM DAISY, every instruction implements multiway path selection scheme. case, ﬁrst 72 bits theVLIW called header contain information tree form, condition tests, branch targets. header followed eight 23-bit parcels, encod-ing operation. order solve problem providing operands largenumber functional units, IBM DAISY keeps eight identical copies register ﬁle, one eight functional units. 9.5. ARITHMETIC PIPELINE principles used instruction pipelining used order improve per- formance computers performing arithmetic operations add, subtract, multiply. case, principles used realize arithmetic cir-cuits inside ALU. section, elaborate use arithmetic pipe-line means speed arithmetic operations. start ﬁxed-pointarithmetic operations discuss ﬂoating-point operations. 9.5.1. Fixed-Point Arithmetic Pipelines basic ﬁxed point arithmetic operation performed inside ALU addition two n-bit operands A¼a n21an22/C1/C1/C1a2a1a0and B¼bn21bn22/C1/C1/C1b2b1b0. Addition two operands performed using number techniques.These techniques differ basically two attributes: degree complexity andachieved speed. two attributes somewhat contradictory; is, simplerealization may lead slower circuit complex realization may lead afaster circuit. Consider, example, carry ripple (CRTA) carry look-ahead (CLAA) adders. CRTA simple, slower, CLAA complex, faster.9.5. ARITHMETIC PIPELINE 209It possible modify CRTA way number pairs operands operated upon, is, pipelined, inside adder, thus improving overall speed addition CRTA. Figure 9.18 shows example amodiﬁed 4-bit CRTA. case, two operands B presented CRTA use synchronizing elements, clocked latches. latches guarantee movement partial carry values withinthe CRTA synchronized input subsequent stages adderwith higher order operand bits. example, arrival ﬁrst carry out(c 0) second pair bits ( a1and b1) synchronized input second full adder (counting low order bits high order bits) usinga latch. Although operation modiﬁed CRTA remains principle same; is, carry ripples adder, provision latches allows possi-bility presenting multiple sets pairs operands adder time.Consider, example, case adding Mpairs operands, whereby oper- ands pair n-bit. time needed perform addition pairs using nonpipelined CRTA given np¼M/C2n/C2Ta, Tais time needed perform single bit addition. compared timeneeded perform computation using pipelined CTRA givenbyT pp¼(nþM21)/C2Ta. example, M¼16 n¼64 bits, Tnp¼1024/C2TaandTpp¼79/C2Ta, thus resulting speed-up 13. extreme case whereby possible present unlimited number ofpairs operands ( M) CRTA time, speed reach 64, number bits operand. Figure 9.18 modiﬁed 4-bit CRTA210 PIPELINING DESIGN TECHNIQUES9.5.2. Floating-Point Arithmetic Pipelines Using similar approach, possible pipeline ﬂoating-point (FP) addition /sub- traction. case, pipeline organized around operations needed perform FP addition. main operations needed FP addition expo-nent comparison (EC), exponent alignment (EA), addition (AD), normalization(NZ). Therefore, possible pipeline organization four-stage pipeline, Figure 9.19 schematic pipeline FP adder Figure 9.20 Carry-save addition9.5. ARITHMETIC PIPELINE 211each performing operation EC, EA, AD, NZ. Figure 9.19 shows sche- matic pipeline FP adder. possible multiple sets FP operands pro- ceeding inside adder time, thus reducing overall time needed FP addition. Synchronizing latches needed, before, order synchronize operands input given stage FP adder. 9.5.3. Pipelined Multiplication Using Carry-Save Addition indicated before, one main problems addition fact carry ripple one stage next. Carry rippling stages beeliminated using method called carry-save addition. Consider case adding44, 28, 32, 79. possible way add without carry ripplethrough illustrated Figure 9.20. idea delay addition carry resulting intermediate stages last step addition. last stage carry-ripple stage employed. Figure 9.21 carry-save based multiplication two 8-bit operands MandQ AB Latches Partial Product Generator Circuit Full Adder Full Adder Full AdderFull AdderLatches Latches Latches CLAFull Adder Figure 9.22 Carry-save addition-based multiplication scheme212 PIPELINING DESIGN TECHNIQUESCarry-save addition used realize pipelined multiplication building block. Consider, example, multiplication two n-bit operands AandB. multiplication operation transformed addition shown Figure 9.21. ﬁgure illustrates case multiplying two 8-bit operands B. carry-save based multiplication scheme using principle shown Figure 9.21 shown Figure 9.22. scheme based idea producingthe set partial products needed adding using carry-save addition scheme. 9.6. SUMMARY chapter, considered basic principles involved designing pipe- line architectures. coverage started discussion number metrics used assess goodness pipeline. moved present ageneral discussion main problems need considered designinga pipelined architecture. particular considered two main problems: instructionand data dependency. effect two problems performance ofa pipeline elaborated. possible techniques used toreduce effect instruction data dependency introducedand illustrated. Two examples recent pipeline architectures, ARM 11 micro- architecture, UltraSPARC III Processor, presented. discus- sion chapter ended introduction ideas beused realizing pipeline arithmetic architectures. EXERCISES 1. Consider execution 500 instructions ﬁve-stage pipeline machine. Compute speed-up due use pipelining given probability instruction branch p¼0.3? must value pand expected number branch instructions speed-up least 4 ispossible? must value psuch speed-up least 5 poss- ible? Assume stage takes one cycle perform task. 2. Assume RISC machine executes one instruction per clock cycle branches executed. Delayed branch used three delay clockcycles. Consider execution 1000 instructions, 30% arebranch instructions, machine two cases. ﬁrst case theuse novice compiler able reduce extra clock cycleswasted due branch instructions. second case, smart compiler thatis able utilize 85% extra clock cycles used. Compute average number instructions per cycle case. Compute also percentage performance gain due use smart compiler.EXERCISES 2133. computer system four-stage pipeline consisting instruction fetch unit (F), instruction decode unit (D), instruction execution unit (E), write unit (W). Compute speed-up time P(4), throughput U(4), efﬁ- ciency z(4) pipeline executing code segment consisting 20 instruc- tions, given branch instructions occur follows: I3,I9,I10,I15,I20.A u e branch instruction fetched, pipeline stalls next instruction fetch known. Determine time required execute 20 instructions using two-way interleaved memory functions performed F, E, W units require use memory. isthe average number cycles per instruction cases? Use following space–time chart compute number time units. 4. Consider integer multiplication two 16-bit numbers MandQto produce product P. Show operation represented P¼P 15 i¼0Pi Pi¼M*Qi*2irepresents 32-bit partial product. Design pipeline unit perform operation using minimum number carry-save adders one carry-look-ahead adder. Show also design pipeline performing ﬂoating-point addition /subtraction. Give numerical examples support design. 5. computer system three-stage pipeline consisting Fetch unit (F), Decode unit (D), Execute (E) unit. Determine (using space–time chart) time required execute 20 sequential instructions using two-way interleaved memory three units require use memory simultaneously. 6. average instruction processing time ﬁve-stage instruc- tion pipeline 36 instructions conditional branch instructions occur follows: 5,I7,I10,I25,I27. Use space–time chart analytical model. 7. computer ﬁve-stage instruction pipeline one cycle each. ﬁve stages are: Instruction Fetch (IF), Instruction Decode (ID), OperandFetch (OF), Instruction Execution (IE), Operand Store (OS). Considerthe following code sequence, run computer. Load21,R1; R1 21; Load 5, R2; R2 5; Again: Sub R2, 1, R2 R2 R221; Add R1,R2,R3; R3 R1þR2; Bnn Again; branch result NotNegative; Add R4,R5,R6; R6 R4þR5; Add R6,R4,R7; R7 R4þR6;214 PIPELINING DESIGN TECHNIQUESa. Analyze execution piece code order calculate number cycles needed execute code without pipelining, assuming instruction requires exactly 5 cycles execute. b. Calculate (using Gantt’s chart) number cycles needed execute code pipeline described used. Assume noforwarding hardware branch instructions fetched, pipe-line “stall” target address calculated branch decision ismade. Ignore data dependency. c. Repeat (b) data dependency considered remaining con- ditions same. d. Calculate percentage improvement due use pipeline cases (b) (c). REFERENCES READING Overview UltraSPARC III Cu, Version 1.1 September 2003, White Paper, Sun Microsystems, 1–18. D. Brash, ARM Architecture Version 6 (ARMv6), ARM Ltd., January 2002, White Paper, 1–15. A. Clements, Principles Computer Hardware , 3rd ed., Oxford University Press, New York, 2000. D. Cormie, ARTM11 Microarchitecture, ARM Ltd., April 2002, White Paper, 1–9. K. Ebcioglu, J. Fritts S. Kosonocky, eight-issue tree VLIW processor dynamic binary translation, IEEE Proc. , ICCD, (1998). M. Flynn, Computer Architecture: Pipelined Parallel Processor Design , Jones Bartlett Publisher, New York, 1995. S. Furber, ARM System-on-Chip Architecture , Addison-Wesley, MA, USA, 2000. G. Goldman P. Tirumalai, UltraSPARC-III: advancement Ultra Computing, Proc. IEEE COMPCON’97 , p. 417. C. Hamacher, Z. Vranesic, S. Zaky, Computer Organization , 5th ed., McGraw-Hill, New York, 2002. V. Heuring H. Jordan, Computer Systems Design Architecture , Prentice-Hall, New Jersey, 1997. S. Hily A. Seznec, Branch prediction simultaneous multireading, Proc. IEEE PACT’96 , p. 170. J. Hoogerbrugge L. Augusteijn, Instruction scheduling TriMedia, J. Instruction-Level Parallelism (1999). W.-M. Hwu, Introduction predicted execution, IEEE Comput. , Vol. 31, No. 1, 49–50 (1998). D. Jaggar, ARM architecture systems, IEEE Micro , 17(4), 9–11 (1997). G. Lauthbatch T. Horel, UltraSPARC-III: Designing third generation 64-bit performance, IEEE Micro , May–June, 73–85 (1999).REFERENCES READING 215R. Nair, Optimal 2-bit branch predictors, IEEE Trans. Comput. , 698, 1995. R. Oehler R. Groves, IBM RISC system /6000 processor architecture, IBM J. Res. Dev. , 34; 23–36 (1990). B. Rau J. Fisher, Instruction-level parallel processing: history, overview perspective, J. Supercomput. , 7; 9–50 (1993). A. Scott, K. Burkhart, A. Kumar, R. Blumberg G. Ranson, Four-way superscalar PA-RISC processors, Hewlett-Packard J ., August (1997). J. Smith G. Sohi, microarchitecture superscalar processors, Proc. IEEE , Vol. 83, No. 12, 1609–1624 (1995). M. Tremblay, Increasing work, pushing clock, IEEE Comput. , Vol. 31, No. 1, 40–41 (1998). B. Wilkinson, Computer Architecture: Design Performance , 2nd ed., Prentice-Hall, Hert- fordshire, UK, 1996. Websites http://www.ar.com http://www.arm.com /support /White_Papers http://www.sun.com /ultrasparc216 PIPELINING DESIGN TECHNIQUES& CHAPTER 10 Reduced Instruction Set Computers (RISCs) chapter dedicated study reduced instruction set computers (RISCs). machines represent noticeable shift computer architecture paradigm. paradigm promotes simplicity rather complexity. RISC approach sub-stantiated number studies indicating assignment statements, conditional branching, procedure calls /return represent 90% complex operations long division represent 2% operations performed typical set benchmark programs. studies showed also among alloperations, procedure calls /return time-consuming. Based results, RISC approach calls enhancing architectures resourcesneeded make execution frequent time-consuming oper-ations efﬁcient. seed RISC approach started early mid-1970s. real-life manifestation appeared Berkeley RISC-I Stanford MIPS machines, introduced mid-1980s. Today, RISC-based machines reality characterized number common featuressuch simple reduced instruction set, ﬁxed instruction format, one instructionper machine cycle, pipeline instruction fetch /execute units, ample number general purpose registers (or alternatively optimized compiler code generation), Load /Store memory operations, hardwired control unit design. coverage chapterstarts discussion evolution RISC architectures. provide abrief discussion performance studies led adoption RISC paradigm. Overlapped Register Windows, essential concept RISC development, discussed. Toward end chapter provide detailson number RISC-based architectures, Berkeley RISC, StanfordMIPS, Compaq Alpha, SUN UltraSparc. 10.1. RISC /CISC EVOLUTION CYCLE term RISCs stands Reduced Instruction Set Computers. originally introduced notion mean architectures execute fast one 217Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.instruction per clock cycle. RISC started notion mid-1970s even- tually led development ﬁrst RISC machine, IBM 801 minicomputer.The launching RISC notion announces start new paradigm thedesign computer architectures. paradigm promotes simplicity computerarchitecture design. particular, calls going back basics rather provid-ing extra hardware support high-level languages. paradigm shift relates towhat known semantic gap , measure difference oper- ations provided high-level languages (HLLs) provided computerarchitectures. recognized wider semantic gap, larger number undesirable consequences. include (a) execution inefﬁciency, (b) excessive machine pro-gram size, (c) increased compiler complexity. expected conse-quences, conventional response computer architects add layers ofcomplexity newer architectures. include increasing number complex-ity instructions together increasing number addressing modes. archi- tectures resulting adoption “add complexity” known Complex Instruction Set Computers (CISCs). However, soon became apparent acomplex instruction set number disadvantages. include complexinstruction decoding scheme, increased size control unit, increasedlogic delays. drawbacks prompted team computer architects adopt theprinciple “less actually more.” number studies conducted inves-tigate impact complexity performance. discussed below. 10.2. RISCs DESIGN PRINCIPLES computer minimum number instructions disadvantage large number instructions executed realizing even simple function. result speed disadvantage. hand, computer inﬂated number instructions disadvantage complex decoding hence speed disadvantage. natural believe computer witha carefully selected reduced set instructions strike balance betweenthe two design alternatives. question becomes constitutes acarefully selected reduced set instructions? order arrive answer tothis question, necessary conduct in-depth studies number aspects computation. aspects include (a) operations frequently performed execution typical (benchmark) programs, (b) operations time consuming, (c) type operands frequently used. number early studies conducted order ﬁnd typical break- operations performed executing benchmark programs. esti- mated distribution operations shown Table 10.1. careful look estimated percentage operations performed reveals assignment statements, conditional branches, procedure calls constitute 90% total operations performed, operations, however complex may be, make remaining 10%.218 REDUCED INSTRUCTION SET COMPUTERS (RISCs)In addition ﬁndings, studies time–performance characteristics operations revealed among operations, procedure calls /return time-consuming. regards type operands used typical compu- tation, noticed majority references (no less 60%) madeto simple scalar variables less 80% scalars local variables(to procedures). observations typical program behavior led following conclusions: 1. Simple movement data (represented assignment statements), rather complex operations, substantial optimized. 2. Conditional branches predominant therefore careful attention paid sequencing instructions. particularly true isknown pipelining indispensable use. 3. Procedure calls /return time-consuming operations therefore mechanism devised make communication parametersamong calling called procedures cause least number instruc-tions execute. 4. prime candidate optimization mechanism storing accessing local scalar variables. conclusions led argument instead bringing instruc-tion set architecture closer HLLs, appropriate rather optimize performance time-consuming features typical HLL programs.This obviously call making architecture simpler rather complex. Remember complex operations long division represent small por- tion (less 2%) operations performed typical computation. Onethen ask question: achieve that? answer (a) keepingthe frequently accessed operands CPU registers (b) minimizing theregister-to-memory operations. two principles achieved using following mechanisms: 1. Use large number registers optimize operand referencing reduce processor memory trafﬁc.TABLE 10.1 Estimated Distribution Operations Operations Estimated percentage Assignment statements 35 Loops 5 Procedure calls 15 Conditional branches 40 Unconditional branches 3 Others 210.2. RISCs DESIGN PRINCIPLES 2192. Optimize design instruction pipelines minimum compiler code generation achieved (see Chapter 8). 3. Use simpliﬁed instruction set leave complex unnecessary instructions. following two approaches identiﬁed implement three mechanisms. 1. Software approach. Use compiler maximize register usage allocat- ing registers variables used given time period(this philosophy adopted Stanford MIPs machine). 2. Hardware approach. Use ample CPU registers variables held registers larger periods time (this philosophy adopted inthe Berkeley RISC machine). hardware approach necessitates useof new register organization, called overlapped register window . explained below. 10.3. OVERLAPPED REGISTER WINDOWS main idea behind use register windows minimize memory accesses. order achieve that, large number CPU registers needed. example,the number CPU general-purpose registers available original SPARCmachine (one earliest RISCs) 120. However, desirable haveonly subset registers visible given time addressedas set registers available. Therefore, CPU registers aredivided multiple small sets, assigned different procedure. procedure call automatically switch CPU use different ﬁxed-size window reg- isters. order minimize actual movement parameters among callingand called procedures, set registers divided three subsets: par-ameter registers, local registers, temporary registers. procedure call ismade, new overlapping window created temporary registersof caller physically parameter registers called pro-cedure. overlap allows parameters passed among procedure withoutactual movement data (Fig. 10.1). Parameter RegistersLocalRegistersTemporaryRegistersLevel j (Caller) Call/Return Level j+1 (called)ParametersRegistersLocalRegistersTemporaryRegisters Figure 10.1 Register window overlapping220 REDUCED INSTRUCTION SET COMPUTERS (RISCs)In addition, set ﬁxed number CPU registers identiﬁed global reg- isters available procedures. example, references registers 0 7 SPARC architecture refer unique global registers, refer- ences registers 8 31 indicate registers current window. currentwindow pointed using normally called current window pointer (CWP). Upon windows ﬁlled, register window wraps around, thusacting like “circular buffer.” Table 10.2 shows number windows window size number architectures. noted study conducted 1985 ﬁnd impact using register window performance Berkeley RISC. study, two versions machine studied. ﬁrst designed register win-dows second hypothetical Berkeley RISC implemented without win-dows. results study indicated decrease factor 2 4 (dependingon speciﬁc benchmark) memory trafﬁc due use register windows. 10.4. RISCs VERSUS CISCs choice RISC versus CISC depends totally factors must con- sidered computer designer. factors include size, complexity, andspeed. RISC architecture execute instructions perform samefunction performed CISC architecture. compensate drawback,RISC architectures must use chip area saved using complex instructiondecoders providing large number CPU registers, additional executionunits, instruction caches. use resources leads reduction inthe trafﬁc processor memory. hand, CISC archi- tecture richer complex instructions, require smaller number instructions RISC counterpart. However, CISC architecture requires acomplex decoding scheme hence subject logic delays. therefore reason-able consider RISC CISC paradigms differ primarily strategyused trade different design factors. little reason believe idea improves performance RISC architecture fail thing CISC architecture viceversa. example, one key issue RISC development use optimizing compiler reduce complexity hardware optimize use CPU registers. ideas applicable CISC compilers. IncreasingTABLE 10.2 Different Register Windows Characteristics ArchitectureNumber windowsNumber registers per window Berkeley RISC-I 8 16 Pyramids 16 32 SPARC 32 3210.4. RISCs VERSUS CISCs 221the number CPU registers could much improve performance CISC machine. could reason behind ﬁnding pure commercially availableRISC (or CISC) machine. unusual see RISC machine complexﬂoating-point instructions (see details SPARC architecture next sec-tion). equally expected see CISC machines making use register win- dows RISC idea. fact studies indicating CISC machine Motorola 680xx register window achieve 2 4 timesdecrease memory trafﬁc. factor achieved aRISC architecture, Berkeley RISC, due use register window. should, however, noted processor developers (except Intel associates) opted RISC processors. Computer system manufacturers Sun Microsystems using RISC processors products. However, compatibility PC-based market, companies still producing CISC-based products. Tables 10.3 10.4 show limited comparison example RISC CISC machine terms performance characteristics, respectively. elaborate comparison among number commercially available RISC CISC machines shown Table 10.5. worth mentioning point following set common character- istics among RISC machines observed: 1. Fixed-length instructions 2. Limited number instructions (128 less)3. Limited set simple addressing modes (minimum two: indexed PC-relative) 4. operations performed registers; memory operations5. two memory operations: Load StoreTABLE 10.3 RISC Versus CISC Performance ApplicationMIPS CPI (RISC)VAX CPI (CISC)CPI ratioInstruction ratio Spice 2G6 1.80 8.02 4.44 2.48 Matrix300 3.06 13.81 4.51 2.37 Nasa 7 3.01 14.95 4.97 2.10 Espresso 1.06 5.40 5.09 1.70 TABLE 10.4 RISC Versus CISC Characteristics CharacteristicVAX-11 (CISC)Berkeley RISC-1 (RISC) Number instructions 303 31 Instruction size (bits) 16-456 32Addressing modes 22 3 No. general purpose registers 16 138222 REDUCED INSTRUCTION SET COMPUTERS (RISCs)6. Pipelined instruction execution 7. Large number general-purpose registers use advanced compiler technology optimize register usage 8. One instruction per clock cycle9. Hardwired control unit design rather microprogramming 10.5. PIONEER (UNIVERSITY) RISC MACHINES section, present brief descriptions main architectural features two pioneer university-introduced RISC machines. ﬁrst machine BerkeleyRISC second Stanford MIPS machine. machines presentedas means show original RISC machines look also make readerappreciate advances made RISC machines development since inception. 10.5.1. Berkeley RISC two Berkeley RISC machines: RISC-I RISC-II. Unless otherwise mentioned, refer RISC-I discussion. RISC 32-bit LOAD /STORE architecture. 138 32-bit registers R 0–R137available users. ﬁrst ten registers R0–R9are global registers (seen procedures). Register R0 used synthesize addressing modes operations directly availableon machine. Registers R 10–R137are divided overlapping register window scheme 32 registers visible instant. 5-bit variable, called current window pointer (CWP) used point current register set. RISC instructions occupy full word (32 bits). RISC instruction set divided four categories. ALU (a total 12 instructions), Load /Store (a total 16 instructions), Branch & Call (a total seven instructions), specialinstructions (a total four instructions). examples RISC instructions are: 1. ALU: ADD R s,S,Rd;Rd RsþS 2. Load /Store: LDXW (Rx)S,Rd;Rd M[RxþS]TABLE 10.5 Summary Features Number RISC CISC Motorola 88110Alpha AXP 21264 PentiumPower PC 601 Company Motorola Compaq (DEC) Intel IBM Architecture RISC RISC CISC RISC # Registers(I) 32 80 64 32 Cache /D8 /8K B 6 4 /64 KB 8 /8K B 3 2 # Registers (GP /FP) 32 /32 31 /31 8 /83 2 /32 # Inst /cycle 2 1 2 3 # Pipelines (I /FP) NS 4 /25 /84 /6 Multiprocessing Support Yes Yes Yes10.5. PIONEER (UNIVERSITY) RISC MACHINES 2233. Branch & Call: JMPX COND ,( R x)S;PC RxþS;w h e r e COND condition 4. Special Instructions: GETPSW R d;Rd PSW arithmetic logical instructions three operands form Desti- nation :¼source1 op source2 (Fig. 10.2). LOAD STORE instructions may use either indicated formats DST register loaded stored. low order 19 bits instructions used determine effective address. Instructions load store 8-, 16-, 32-, 64-bit quantities 32-bit registers. Two methods provided calling procedures. CALL instruction uses 30-bit PC relative offset (Fig. 10.3). JMP instruction uses instruction formats used arithmetic logical operations allows return address put register. RISC uses three-address instruction format availability two- one-address instructions. two addressing modes. indexed mode PC relative modes. indexed mode used synthesize three modes. base-absolute (direct), register indirect, indexedfor linear byte array modes. RISC uses static two-stage pipeline: fetch execute. ﬂoating-point unit (FPU) contains thirty-two 32-bit registers hold 32 single precision (32-bit) ﬂoating-point operands, 16 double-precision (64-bit) operands, eight extended-precision (128-bit) operands. FPU execute 20 ﬂoat-ing-point instructions single-, double-, extended-precision usingthe ﬁrst instruction format used arithmetic. addition instructions loadingand storing FPUs registers, CPU also test FPUs registers branch con- ditionally results. RISC employs conventional MMU supporting single paged 32-bit address space. RISC four-bus organization shown Figure 10.4. 10.5.2. Stanford MIPS (Microprocessor Without Interlock Pipe Stages) MIPS 32-bit pipelined LOAD /STORE machine. uses ﬁve-stage pipeline consisting Instruction Fetch ( IF), Instruction Decode ( ID), Operand DecodeType Type Immediate Constant25 6 5 5 18 DST Op-Code SRC 1 0 DST Op-Code SRC 1 1FP-OP SRC 2 Figure 10.2 Three operand instructions formats used RISC Type PC-Relative Displacement23 0 Figure 10.3 Procedure call instruction RISC224 REDUCED INSTRUCTION SET COMPUTERS (RISCs)(OD), Operand Store /Execution ( OS/EX), Operand Fetch ( OF). ﬁrst three stages perform respectively instruction fetch, instruction decode, operand fetch. OS /EX stage sends operand memory case store instruction use ALU case instruction execution. stage receives operand caseof load instruction. MIPS uses mechanism called pipeline interlock order prevent instruction continuing needed operand available. Unlike Berkeley RISC, MIPS single set sixteen 32-bit general- purpose registers. MIPS compiler optimizes use registers whateverway best program currently compiled. addition 16 general-purpose registers, MIPS provided four additional registers order hold fourprevious PC values (to support backtracking restart case fault). ﬁfth reg-ister used hold future PC value (to support branch instructions). Four addressing modes used MIPS. immediate ,indexed ,based offset , base shifted . Four instruction groups identiﬁed MIPS. ALU,Load /Store ,Control , Special instructions. total 13 ALU instructions provided. include register-to-register two- three-operand formats (Fig. 10.5). total 10 LOAD /STORE instructions pro- vided. use 16 32 bits. latter case, indexed addressing used byadding 16-bit signed constant register using second format inFigure 10.5. total six control ﬂow instructions provided. include78× 32 Register File Shifter PSW IMMRegister Decoder BI AI Next PCPCLST PC Figure 10.4 RISC four-bus organization Op-Code SRC DST Immediate ConstantOp-Code SRC 1 SRC 2 DST SHIFT Function6 655 5555 6 16 Figure 10.5 Three-operand instructions used MIPS10.5. PIONEER (UNIVERSITY) RISC MACHINES 225jumps ,relative jumps , compare instructions. two special ﬂow instructions provided. support procedure interrupt linkage. examples MIPS instructions are: 1. ALU: Add src 1,src2,dst;dst src1þsrc2 2. Load /Store: Ld[src1þsrc2],dst;dst M[src1þsrc2] 3. Control: Jmp dst ;PC dst 4. Special Function: SavePC ;M[A] PC MIPS provide direct support ﬂoating-point operations. Floating- point operations done specialized coprocessor. Surprisingly,non-RISC instructions MULT DIV included use specialfunctional units. contents two registers multiplied divided the64-bit product kept two special registers LO HI. Procedure call made JUMP instruction shown Figure 10.6. instruction uses 26-bit jump target address. MIPS virtual address 32 bits long, thus allowing four Gwords virtual address space. virtual address divided 20-bit virtual pagenumber 12-bit offset within page. actual implementation MIPSwas restricted packaging constraints allowing 24 address pins; is, theactual physical address space 2 24¼16Mwords (32 bits each). support off-chip TLB address translation provided. MIPS organization shown Figure 10.7.Op-Code Jump Target62 6 Figure 10.6 Jump instruction format used MIPSMDR MAR PC-3 PC-2 PC-1 PC Register File (16 × 32) Barrel Shifter Hi Lo Registers Figure 10.7 MIPS organization226 REDUCED INSTRUCTION SET COMPUTERS (RISCs)10.6. EXAMPLE ADVANCED RISC MACHINES section, introduce two representative advanced RISC machines. emphasis coverage pipeline features branch handling mech-anisms used. 10.6.1. Compaq (Formerly DEC) Alpha 21264 Alpha 21264 (EV6) third generation Compaq (formerly DEC) RISC superscalar processor. full 64-bit processor. 21264 80-entry integer register ﬁleand 72-entry ﬂoating-point register ﬁle. employs two-level cache. L1 dataand instruction caches 64 KB each. organized two-way set-associ- ative manner. L2 data cache 1 16 MB (shared instructions data)organized using direct-mapping. block size 64 bytes. data cache receive combination two loads stores integer execution pipe every cycle. equivalent 64 KB on-chip data cache delivering16 bytes every cycle, hence twice clock speed processor. 21264memory system support 32 in-ﬂight loads, 32 in-ﬂight stores, 8in-ﬂight (64 byte) cache block ﬁlls 8 cache misses. 64 KB, two-way set-associative cache (both instruction data). also support two out-of-order operations (Fig. 10.8). 10.6.2. Alpha 21264 Pipeline Alpha 21264 instruction pipeline shown Figure 10.9. consists SEVEN stages. Fetch, Slot Assignment, Rename, Issue, Register Read,Execute, Memory stages. fetch stage fetch execute four instructions per cycle. block diagram fetch stage shown Figure 10.10. stage uses unique “block set” prediction technique. According technique, locations next four instructions set (there two sets) located, predicted. “block set” prediction technique combines speed advantages direct-mapped cache lower miss ratio two-way set-associative Cache L1 (Data 64 KB, 2-way) + (Instruction 64 KB, 2-way) (Block size 64 Byte) Cache L2 (off-chip) (Data 1-16 MB, direct) Main Memory (up 16 GB) Hard Disk (100s Terabytes) Figure 10.8 21264 memory hierarchy10.6. EXAMPLE ADVANCED RISC MACHINES 227cache. technique achieves 85% hit ratio. misprediction pen- alty single cycle. 21264 uses speculative branch prediction. Branch predic-tion 21264 two-level scheme. based observation branchesexhibit local global correlation. Local correlation makes use thebranch’s past behavior. Global correlation, hand, makes use past behavior previous branches. combined local /global prediction used 21264 correlates branch behavior pattern local branch history, is, execution single branch unique PC location, global branchhistory, is, execution previous branches. scheme dynamicallyselects local global branch history (Fig. 10.11). local branch predictor two tables. ﬁrst 1024 /C210 local history table entry holds 10-bit local history selected branch overthe last executions. local history table indexed instruction address (using PC). second table 1024 /C23 local prediction table entry 3-bit saturating counter predict branch outcome. branches’ retirement, 21264 updates local history table true branch directionand referenced counter. enhances possibility correct prediction called predictor training . global branch predictor 4096 /C22 global prediction table entry holds 2-bit saturating counter. keeps track global history last 12 branches. global branch prediction table indexed 4096 /C22 choice pre- diction table. branches’ retirement, 21264 updates referenced globalprediction counter, enhancing possibility correct prediction. Local prediction useful case alternating taken /not-taken sequence given branch. case, local history branch eventually resolveto pattern ten alternating zeros ones indicating success, failure, thebranch alternate encounters. branch executes multiple times, saturatesthe prediction counters corresponding local history values hence makesthe prediction correct.Fetch StageS #1SlotAssignmentStage #2RenameStageS #3 IssueStageS #4 RegisterReadStage #5ExecuteStageS #6 MemoryStageS #7 Figure 10.9 21264 instruction pipeline 64 KB 2-wayBlock/Set PredictionGlobalLocal Branch Predictor Instruction Cache Figure 10.10 21264 fetch stage228 REDUCED INSTRUCTION SET COMPUTERS (RISCs)Global prediction useful outcome branch inferred direction previous branches. Consider, example, case repeated invoca- tions two branches. ﬁrst branch checks value equal 1001 suc- ceeds, second branch checks value odd must also succeed. global history predictor learn pattern repeated invocations thesetwo branches. 2096/C22 choice predictor table entry holds 2-bit saturat- ing counter used implement selection (tournament) scheme. pre-dictions local global predictors differ, 21264 updates selected choice prediction entry support correct predictor. 21264 updates choice prediction table branch retires. slot assignment stage (S #2) simply assigns instructions slots associated integer ﬂoating-point queues. out-of-order (OOO) issue logic 21264 receives four fetched instruc- tions every cycle, renames remaps registers (to avoid unnecessary registerdependencies), queues instructions operands /or functional units become available. dynamically issues six instructions every cycle, four inte- ger two ﬂoating-point instructions. Register renaming means mapping instruc- tion virtual registers internal physical registers. 31 integer 31 ﬂoating-point registers visible users. registers renamed execution internal registers. instructions ﬁnished(retired) internal registers renamed back visible registers. Registerrenaming eliminates write-after-write write-after-read data dependencies. How-ever, preserves read-after-write dependencies necessary correctcomputation. list pending instructions maintained OOO queue logic. cycle, integer ﬂoating-point queues select instructions ready execute. selection made based scoreboard renamed reg- isters. scoreboard maintains status renamed registers tracking theLocal Prediction Table 1024 × 3Local History Table 1024 × 10 Branch PredictionPast HistoryChoice Prediction 4096 × 2Global Prediction 4096 × 2 PC MUX Figure 10.11 21264 selection branch predictor10.6. EXAMPLE ADVANCED RISC MACHINES 229progress single-cycle, multiple-cycle, variable-cycle instructions. Upon availability functional unit(s) load data results, scoreboard unit notiﬁesall instructions queue availability required register value. Eachqueue selects oldest data-ready functional-unit-ready instructions forexecution cycle. 21264 integer queue statically assigns instructions totwo four pipes, either upper lower pipe (Fig. 10.12). Alpha 21264 four integer two ﬂoating-point pipelines. allows processor dynamically issue six instructions cycle. Theissue (or queue) stage maintains inventory dynamicallyselect issue maximum six instructions. 20-entry integer issuequeue 15-entry ﬂoating-point issue queue. Instruction issue reordering takesplace issue stage. 21264 uses two integer ﬁles, 80-entry each, store duplicate register con- tents. Two pipes access single ﬁle form cluster. two clusters form four-way integer instruction execution. Results broadcasted cluster cluster. Instructions dynamically selected integer issue queue exe- cute given instruction pipe. instruction heuristically selected executeon cluster produces result. 21264 one 72-entry ﬂoating-pointregister ﬁle. ﬂoating-point register ﬁle, together two instruction executionpipes, form cluster. Figure 10.12 shows register read /execution pipes. ﬁnal note, indicate 21264 uses write-invalidate cache coherence mechanism level 2 cache provide support shared-memory multiprocessing. also supports following cache states: modiﬁed, owned, shared, exclusive, invalid. Floating-point Multiply Execution Floating-point Register File (72) ClusterClusterCluster Floating-point Add, Div, SQRT ExecutionInteger ExecutionInteger ExecutionInteger ExecutionInteger Execution Data Cache64 KB2-wayInteger Register File (80)Integer Register File (80) Figure 10.12 21264 execution pipes230 REDUCED INSTRUCTION SET COMPUTERS (RISCs)10.6.3. SUN UltraSPARC III UltraSPARC wIII high-performance superscalar RISC processor implements 64-bit SPARC w-V9 RISC architecture. exist number implementations SPARC III processor. include UltraSPARC IIIi UltraSPARC III Cu. coverage section independent ofany particular implementation. however refer speciﬁc implementationswhenever appropriate. UltraSPARC III third generation 64-bit SPARC wRISC microprocessor. supports 64-bit virtual address space 43-bit physical address space. TheUltraSPARC III employs multilevel cache architecture. example, Ultra-SPARC IIIi (and UltraSPARC III Cu) architecture 32 KB, four-way set- associative L1 instruction cache, 64 KB four-way set-associative L1 data cache, 2 KB prefetch cache, 2 KB write cache. UltraSPARC IIIi supports a1 MB four-way set-associative, uniﬁed instruction /data chip L2 cache. cache block size 64 bytes used UltraSPARC IIIi. UltraSPARCIII Cu architecture supports 1, 4, 8 MB two-way set-associative, uniﬁed instruc-tion/data external cache. Cache block size UltraSPARC III Cu varies 64 bytes (for 1 MB cache) 512 bytes (for 8 MB cache) (Fig. 10.13). UltraSPARC III uses two instruction TLBs accessed parallel three data TLBs accessed parallel. two instruction TLBsare organized 16-entry fully associative manner hold entries 8 KB,64 KB, 512 KB, 4 MB page sizes. 128-entry two-way set-associative TLBis used exclusively 8 KB page sizes. three data TLBs organized a16-entry associative manner 8 KB, 64 KB, 512 KB, 4 MB page sizes andtwo 512-entry two-way set-associative TLBs programmed hold one page size given time. UltraSPARC III uses write-allocate, write- back cache write policy. UltraSPARC III pipeline covered Chapter 9 (pages 203–207).On ﬁnal note, mentioned UltraSPARC III designed support one-to-four way multiprocessing. purpose, usestheJBus , supports small-scale multiprocessor system. JBus capable Main Memory (up 16 GB) Hard Disk (100s Terabytes)Cache L1 (Data 64 KB) + (Instruction 32 KB) + (2 KB Prefetch) + (2 Kbyte Write) Cache L2 (1-8 MB) Figure 10.13 UltraSPARC III memory hierarchy10.6. EXAMPLE ADVANCED RISC MACHINES 231of delivering high bandwidth needed networking embedded systems applications. JBus , processors attach coherent shared bus needed glue logic (Fig. 10.14). 10.7. SUMMARY RISC architecture saves extra chip area used CISC architectures decod- ing executing complex instructions. saved chip area used provide on-chip instruction cache used reduce instruction trafﬁc betweenthe processor memory. Common characteristics shared RISCdesigns are: limited simple instruction set, large number general purpose reg-isters /or use compiler technology optimize register usage, optim- ization instruction pipeline. essential RISC philosophy keep mostfrequently accessed operands registers minimize register-memory operations.This achieved using one two approaches: Software Approach, use com- piler maximize register usage allocating registers variables used given time period (this philosophy used Stanford MIPsmachine); Hardware Approach, use registers variables beheld registers larger periods time (this philosophy used Berke-ley RISC machine). Register windows multiple small sets registers, eachassigned different procedure. procedure call automatically switches theCPU use different ﬁxed-size window registers rather saving registersin memory call time. time, ONE window registers visible addressed set registers. Window overlapping requires temporary registers one level physically parameter regis-ters next level. overlap allows parameters passed without actualmovement data. worthwhile mentioning classiﬁcation processors entirely pure RISC entirely pure CISC becoming inappropriate may beirrelevant. actually counts much performance gain achieved byincluding element given design style. modern processors use calcu- lated combination elements design styles. decisive factor element(s) design style include made based trade-off betweenUltraSPARC III ProcessorUltraSPARC IIIProcessor UltraSPARC IIIProcessorUltraSPARC IIIProcessor JBUS 128 bit, 200 MHz Figure 10.14 four-way UltraSPARC III multiprocessor conﬁguration232 REDUCED INSTRUCTION SET COMPUTERS (RISCs)the required improvement performance expected added cost. number processors classiﬁed RISC employing number CISC features, suchas integer /ﬂoating-point division instructions. Similarly, exist processors classiﬁed CISC employing number RISC features, aspipelining. EXERCISES 1. main principles used construct RISC machine? 2. Contrast two approaches (the software hardware) used RISC machines minimize memory operations. 3. Explain, examples, concept register window window overlap- ping. Suggest different approach achieve results achieved using register window window overlapping. 4. purpose problem, required pick recent RISC pro- cessor choice. Submit small report (no less 5 10 pages length) summarizes main pipelining features used themain RISC features used. level coverage suitable asenior undergraduate student. Make sure precise neat yourcoverage. Use simple examples whenever possible. Provide accurate andmeaningful ﬁgures tables whenever possible. required cover aspects pipeline processor aspects RISC machine. 5. controversy RISC versus CISC never ends. Suppose represent advocate RISC approach; write least one-page critic CISCapproach showing disadvantages showing advantages RISCapproach. may want use real-life example machine performance asupport support RISC philosophy. 6. Repeat question 5 assuming advocate CISC philosophy. REFERENCES READING Overview UltraSPARC III Cu, Version 1.1 September 2003, White Paper, Sun Microsystems, 1–18. R. Colewell et al. Computers, complexity, controversy, IEEE Comput. , 18(9), 8–19 (1985). Z. Cvetanovic R. Kessler, Performance analysis Alpha 21264-based Compaq E40 system, Proc. 27th Annual International Symposium Computer Architecture , Vancou- ver, British Columbia, Canada, 192–202. Exploring Alpha Power Technical Computing. Compaq Report Compag High Performance Technical Computing, November 1999, pp. 1–28. G. Goldman P. Tirumalai, UltraSPARC-III: advancement ultra computing, Proc. IEEE COMPCON’97 , p. 417.REFERENCES READING 233J. Hennessy, VLSI processor architecture, IEEE Trans. Comput. , C-33(11), 1221–1246 (1984). J. Hennessy D. Patterson, Computer Architecture: Quantitative Approach , Morgan Kaufmann: San Mateo, San Francisco, CA, 1996. R. Kessler, Alpha 21264 Microprocessor, IEEE Micro , Vol. 19, issue 2, 24–36 (1999). R. Kessler, E. McLellan D. Webb, Alpha 21264 Microprocessor Architecture, International Conference Computer Design, Oct. 88, pp. 96–102. G. Lauthbatch T. Horel, UltraSPARC-III: Designing third generation 64-bit performance, IEEE Micro. , 73–85 (1999). R. Nair, Optimal 2-bit branch predictors, IEEE Trans. Comput. , 698 (1995). D. Patterson, Reduced instruction set computers, Commun. ACM , 28(1), 8–21 (1985). D. Patterson R. Ditzel, case reduced instruction set computer, Comput. Archi- tecture News , 8(6), 25–33 (1980). D. Patterson C. Sequin, VLSI RISC, IEEE Comput. , 15(9), 8–21 (1982). G. Radin, 801 minicomputer, IBM J. Res. Develop. , 27(3), 237–246 (1983). R. Sherburne, M. Katevenis, D. Patterson C. Sequin, 32-bit NMOS processor large register ﬁle, IEEE J. Solid-State Circuits , Sc-19(5), 682–689 (1984). A. Tanenbaum, Structured Computer Organization , 3rd ed., Prentice-Hall: Englewood Cliffs, New Jersey. Websites http://www.sun.com /processors /UltraSPARC-IIIi http://www.sun.com /processors /whitepapers234 REDUCED INSTRUCTION SET COMPUTERS (RISCs)& CHAPTER 11 Introduction Multiprocessors covered essential issues design analysis uniprocessors pointing main limitations single-stream machine, begin chap-ter pursue issue multiple processors. number processors (two ormore) connected manner allows share simultaneousexecution single task. main argument using multiprocessors tocreate powerful computers simply connecting many existing smaller ones. Amultiprocessor expected reach faster speed fastest uniprocessor. Inaddition, multiprocessor consisting number single uniprocessors expected cost-effective building high-performance single processor. additional advantage multiprocessor consisting nprocessors single processor fails, remaining fault-free n21 processors able provide continued service, albeit degraded performance. coverage inthis chapter starts section general concepts terminology used.We point different topologies used interconnecting multiple pro-cessors. Different classiﬁcation schemes computer architectures thenintroduced analyzed. introduce topology-based taxonomy inter- connection networks. Two memory-organization schemes MIMD (multiple instruction multiple data) multiprocessors also introduced. coverage inthis chapter ends touch analysis performance metrics multipro-cessors. noted interested readers referred elaborate dis-cussions multiprocessors Chapters 2 3 book Advanced ComputerArchitecture Parallel Processing (see reference list). 11.1. INTRODUCTION multiple processor system consists two processors connected manner allows share simultaneous (parallel) execution givencomputational task. Parallel processing advocated promising approachfor building high-performance computer systems. Two basic requirements areinevitable efﬁcient use employed processors. requirements 235Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.are (1) low communication overhead among processors executing given task (2) degree inherent parallelism task. number communication styles exist multiple processor networks. broadly classiﬁed according (1) communication model (CM) (2) thephysical connection (PC). According CM, networks classiﬁedas (1) multiple processors (single address space shared memory computation) or(2) multiple computers (multiple address space message passing computation). According PC, networks classiﬁed (1) bus-based (2) network- based multiple processors. Typical sizes systems summarized Table 11.1. organization performance multiple processor system greatly inﬂu- enced interconnection network used connect them. one hand, singleshared bus used interconnection network multiple processors. theother hand, crossbar switch used interconnection network. theﬁrst technique represents simple easy-to-expand topology, is, however, limitedin performance since allow one processor /memory transfer given time. crossbar provides full processor /memory distinct connections expensive. Multistage interconnection networks (MINs) strike balance limitation single, shared bus system expense acrossbar-based system. MIN one processor /memory connection established time. cost MIN considerably less thanthat crossbar, particularly large number processors /or memories. use multiple buses connect multiple processors multiple memory modules also suggested compromise limited single bus expensive crossbar. Figure 11.1 illustrates four types interconnection networks mentioned above. Interested readers referred book Advanced ComputerArchitecture Parallel Processing (see reference list). 11.2. CLASSIFICATION COMPUTER ARCHITECTURES classiﬁcation means order number objects categories, common features, among certain relationship(s) exist(s). regard, aclassiﬁcation scheme computer architectures aims categorizing architectures common features fall one category different categories represent distinct groups architectures. addition, TABLE 11.1 Typical Sizes Multiprocessor Systems Category SubcategoriesNumber processors Communication model Multiple processors 2–256 Multiple computers 8–256 Physical connection Bus-based 2–32 Network-based 8–256236 INTRODUCTION MULTIPROCESSORSa classiﬁcation scheme computer architecture provide basis infor- mation ordering basis predicting features given architecture. Two broad schemes exist computer architecture classiﬁcation. ﬁrst based external (morphological) features architectures second basedon evolutionary features architectures. ﬁrst scheme emphasizes theﬁnished form architectures, second scheme emphasizes way anarchitecture derived predecessor suggests speculative views successor. Morphological classiﬁcation provides basis predictive power, evolutionary classiﬁcation provides basis better understandingof architectures. Examining extent classiﬁcation scheme satisfyingits stated objective(s) could assess pros cons scheme. number classiﬁcation schemes proposed last three dec- ades. include Flynn’s classiﬁcation (1966), Kuck (1978), Hwangand Briggs (1984), Erlangen (1981), Giloi (1983), Skillicorn (1988), andthe Bell (1992). number brieﬂy discussed below. 11.2.1. Flynn’s Classiﬁcation Flynn’s classiﬁcation scheme based identifying two orthogonal streams computer. instruction data streams. instruction streamis deﬁned sequence instructions performed computer. datastream deﬁned data trafﬁc exchanged memory processing unit. According Flynn’s classiﬁcation, either instruction data streams single multiple. leads four distinct categories ofP P PM PPM p p p ppppp MMMMMMMM (c)(a)( b) (d)PMM Figure 11.1 Four types multiprocessor interconnection networks. ( a) Single-bus system, (b) multi-bus system, ( c) multi-stage interconnection network, ( d) crossbar system11.2. CLASSIFICATION COMPUTER ARCHITECTURES 237computer architectures: 1. Single-instruction single-data streams (SISD) 2. Single-instruction multiple-data streams (SIMD) 3. Multiple-instruction single-data streams (MISD)4. Multiple-instruction multiple-data streams (MIMD) Figure 11.2 shows orthogonal organization streams according Flynn’s classiﬁcation. Schematics four categories architectures resulting Flynn’s classi- ﬁcation shown Figure 11.3. Table 11.2 lists commercial machinesbelonging four categories. Observations Flynn’s Classiﬁcation 1. Flynn’s classiﬁcation among ﬁrst kind introduced must inspired subsequent classiﬁcations. 2. classiﬁcation helped categorizing architectures available introduced later. example, introduction SIMD MIMD machine models classiﬁcation must haveinspired architects introduce new machine models. 3. classiﬁcation stresses architectural relationship memory- processor level. architectural levels totally overlooked. 4. classiﬁcation stresses external (morphological) features architec- tures. information included revolutionary relationship archi-tectures belong category. 5. Owing pure abstractness, practically viable machine exempliﬁed MISD model introduced classiﬁcation (at least far). should,however, noted architects considered pipelined machines(and perhaps systolic-array computers) examples MISD. 6. important aspect lacking Flynn’s classiﬁcation issue machine performance. Although classiﬁcation gives impression thatmachines SIMD MIMD superior SISD MISDcounterparts, gives information relative performance SIMD MIMD machines. Data Stream Single Multiple Instruction Single SISD SIMD Stream Multiple MISD MIMD Figure 11.2 Flynn’s classiﬁcation238 INTRODUCTION MULTIPROCESSORSInstruction Memory Control Unit Processing Unit Data Memory (a) SISD (b) SIMD (d) MIMD(c) MISDData Stream Data Stream Data Stream Data Stream Data Stream Data StreamInstruction StreamInstruction StreamInstruction StreamInstruction Stream Instruction Stream Instruction StreamControl Unit Processing Unit Data Memory Processing Unit Data MemoryProcessing Unit Data Memory Instruction Memory Control Unit Instruction Memory Control Unit Instruction MemoryProcessing Unit Data Memory Control Unit Instruction Memory Processing Unit Data Memory Control Unit Instruction Memory Processing Unit Data Memory Control Unit Instruction Memory Processing Unit Data Memory Control Unit Instruction MemoryInstruction Stream Instruction Stream Figure 11.3 four architecture classes resulting Flynn’s taxonomy. ( a) SISD, (b) SIMD, ( c) MISD, ( d) MIMD11.2. CLASSIFICATION COMPUTER ARCHITECTURES 23911.2.2. Kuck Classiﬁcation Scheme Flynn’s taxonomy considered general classiﬁcation extended number computer architects. One extension classiﬁcation intro-duced D. J. Kuck 1978. classiﬁcation, Kuck extended instruction stream single (scalar array) multiple (scalar array) streams. data stream Kuck’s classiﬁcation called execution stream also extended include single (scalar array) multiple (scalar array) streams.The combination streams results total 16 categories architectures,as shown Table 11.3. main observation Flynn’s Kuck’s classiﬁcations cover entire architecture space. However, Flynn’s classiﬁcation emphasizes thedescription architectures instruction set level, Kuck’s classiﬁcation emphasizes description architectures hardware level. 11.2.3. Hwang Briggs Classiﬁcation Scheme main new contribution classiﬁcation due Hwang Briggs introduction concept classes . reﬁnement Flynn’s classi- ﬁcation. example, according Hwang Briggs, SISD category furtherreﬁned two subcategories: single functional unit SISD (SISD-S) multipleTABLE 11.2 Example Machines Flynn’s Classiﬁcation Classiﬁcation category Example machines SISD IBM 704, VAX 11 /780, CRAY-1 SIMD ILLIAC-IV, MPP, CM-2, STARAN MISD See observation 5 page 238MIMD Cm /C3, CRAY XMP, IBM 370 /168M TABLE 11.3 16-Architecture Categories Resulting Kuck’s Classiﬁcation Instruction streamExecution streams Single Multiple Scalar Array Scalar Array Single Scalar Uniprocessor Uniprocessor SIMD Array ILLIAC-IV Multiple Scalar NYU Ultracomputer Cray X MP Array240 INTRODUCTION MULTIPROCESSORSfunctional units SISD (SISD-M). MIMD category reﬁned loosely coupled MIMD (MIMD-L) tightly coupled MIMD (MIMD-T). SIMDcategory reﬁned word-sliced processing (SIMD-W) bit-slicedprocessing (SIMD-B). Therefore, Hwang Briggs classiﬁcation added levelto hierarchy machine classiﬁcation given machine beﬁrst classiﬁed SISD, SIMD, MIMD, classiﬁed according itsconstituent descendant. According Hwang Briggs’s taxonomy, always true predict SISD-M perform better SISD-S. is, however, doubtful predic-tion made respect SIMD-W SIMD-B. example, indi-cated using maximum degree potential parallelism performancemeasure, ILLAC-IV machine (SIMD-W) inferior MPP machine(SIMD-B). ﬁnal observation Hwang Briggs’s taxonomy sharedmemory systems (see Chapter 4 book Advanced Computer Architectureand Parallel Processing, see reference list) map naturally MIMD-T category, nonshared memory systems map MIMD-L category. 11.2.4. Erlangen Classiﬁcation Scheme simplest form, classiﬁcation scheme adds one level details internal structure computer, compared Flynn’s scheme. particular, thisscheme considers addition control (CNTL) processing (ALU)units, third subunit, called elementary logic unit (ELU), used charac- terize given computer architecture. ELU represents circuitry required toperform bit-level processing within ALU. architecture characterizedusing three-tuple system ( k,d,w) k¼number CNTLs, d¼number ALU units associated one control unit, w¼number ELUs per ALU (the width single data word). example, one models, theILLAC-IV made mesh connected array 64 64-bit ALUs controlledby Burroughs B6700 computer. According Erlangen, model theILLAC-IV characterized (1, 64, 64). Postulating pipelining exist three levels hardware processing, classiﬁcation includes three additional parameters. w 0¼the number pipeline stages per ALU, d0¼the number functional units per ALU, k0¼the number ELUs forming control unit. Given expected multi-unit nature three hardware processing levels, general six-tuplecan used characterize architecture follows: ( k/C2k 0,d/C2d0,w/C2w0). Figure 11.4 illustrates Erlangen classiﬁcation system. complex systems still characterized using Erlangen system using two additional operators, operator, denoted /C2, ALTERNATIVE operator, denoted _. example, architecture consisting two computational subunits six-tuple ( k0/C2k00,d0/C2d00,w0/C2w00) ( k1/C2k01,d1/C2d01,w1/C2w01) characterized using subunits ( k0/C2k00, d0/C2d00,w0/C2w00)/C2(k1/C2k01,d1/C2d01,w1/C2w01), architecture be11.2. CLASSIFICATION COMPUTER ARCHITECTURES 241expressed using either two subunits characterized ,k0/C2k00,d0/C2d00,w0- 0/C2w00._,k1/C2k01,d1/C2d01,w1/C2w01.. example, later design ILLAC-IV consisted two DEC PDP-10 front-end controller data accepted one PDP-10 time. version ILLAC-IV characterized (2, 1, 36) /C2(1, 64, 64). Now, since ILLAC-IV also work half-word mode whereby are128 32-bit processors rather 64 64-bit processors, overall character-ization ILLAC-IV given (2, 1, 36) /C2[(1, 64, 64) _(1, 128, 32)]. seen, classiﬁcation scheme regarded hierarchical classi- ﬁcation puts emphasis internal structure processing hardware.It provide basis classiﬁcation /or grouping computer architectures. particular, classiﬁcation overlooks interconnection amongdifferent units. 11.2.5. Skillicorn Classiﬁcation Scheme Owing inherent nature, Flynn’s classiﬁcation may end grouping computer systems similar architectural characteristics diverse functionality one class. observation main motive behind Skillicorn classiﬁcation introduced 1988. According classiﬁcation, abstract von Neumann machine modeled shown Figure 11.5. seen, abstract model includes two memory subdivisions, instruction memory (IM) datamemory (DM), addition instruction processor (IP) data processor(DP). developing classiﬁcation scheme, following possible interconnec-tion relationships considered: (IP–DP), (IP–IM), (DP–DM), (DP–IP).The interconnection scheme takes consideration type number con- nections among data processors, data memories, instruction processors, instruction memories. may exist no, one-to-many, many-to-many Figure 11.4 Erlangen classiﬁcation scheme242 INTRODUCTION MULTIPROCESSORSconnections. Table 11.4 illustrates different connection schemes identiﬁed classiﬁcation. Using given connection schemes, Skillicorn arrived 28 different classes. Sample classes shown Table 11.5. rightmost column table indicates corresponding Flynn’s class. Figure 11.6 illustrates four example classes accord- ing classiﬁcation. Major advantages Skillicorn classiﬁcation include (1) simplicity, (2) proper consideration interconnectivity among units, (3) ﬂexibility, (4)the ability represent current computer systems. However, classiﬁcationIPInstructions DP DP Operations Instructions Addresses Data Memory (DM)Instruction Memory (IM)Operand AddressesState Figure 11.5 Abstract model simple machine TABLE 11.4 Possible Connection Schemes Connection type Meaning1–1 connection two single units 1–n connection single unit nother units n–nn (1–1) connections n/C2nn (1–n) connections TABLE 11.5 Sample Connection Classes Class IP DP IP–DP IP–IM DP–DM DP–DP Description Flynn 1 1 1 1–1 1–1 1–1 None Von Neumann uniprocessorSISD 21 N 1 – n 1–1 n–nn/C2n Type 1 array processorsSIMD 31 N 1 – n 1–1 n/C2n None Type 2 array processorsSIMD 4N N n–nn –nn – n n/C2n Loosely coupled von NeumannMIMD 5N N n–nn –nn/C2n None Tightly coupled von NeumannMIMD11.2. CLASSIFICATION COMPUTER ARCHITECTURES 243(1) lacks inclusion operational aspects pipelining (2) difﬁculty predicting relative power machines belonging class withoutexplicit knowledge interconnection scheme used class. Multiple processor systems classiﬁed tightly coupled versus loosely coupled. tightly coupled system, processors equally access global memory. addition, processor may also local cachememory. loosely coupled system, memory divided among processorssuch processor memory attached it. However, pro-cessors still share memory address space. processor directlyaccess remote memory. Examples tightly coupled multiple processors includethe CMU C.mmp , Encore Computer Multimax , Sequent Corp. Balance series . Examples loosely coupled multiple processors include CMU Cm /C3, BBN Butterﬂy , IBM RP3. 11.3. SIMD SCHEMES Recall Flynn’s classiﬁcation results four basic architectures. Among those, SIMD MIMD frequently used constructing parallel architectures.In section, provide basic information SIMD paradigm. important outset indicate SIMD mostly designed exploit inherent parallelism encountered matrix (array) operations, required(a)( b) (c)( d)IP IP DP 1: n1: nnxn MIMIMD MDP IPDPnxn nxn MDPnxn IP IMIM Figure 11.6 Example connection classes. ( a) Array 1 class, ( b) array 2 class, ( c) tightly coupled multiprocessor, ( d) loosely coupled multiprocessor244 INTRODUCTION MULTIPROCESSORSin applications image processing. Famous real-life machines commercially constructed include ILLIAC-IV (1972), STARAN (1974), andthe MPP (1982). Two main SIMD conﬁgurations used real-life machines. shown Figure 11.7. ﬁrst scheme, processor local memory. Processors communicate interconnection network. intercon- nection network provide direct connection given pair pro- cessors, pair exchange data via intermediate processor. Figure 11.7 Two SIMD schemes. ( a) SIMD scheme 1, ( b) SIMD scheme 211.3. SIMD SCHEMES 245ILLIAC-IV used interconnection scheme. interconnection network ILLIAC-IV allowed processor communicate directly four neighbor-ing processors 8 /C28 matrix pattern ith processor communicate directly ( i21)th, ( iþ1)th, ( i28)th, ( iþ8)th processors. second SIMD scheme, processors memory modules communicate via interconnection network. Two processors transfer databetween via intermediate memory module(s) possibly via intermediate processor(s). Assume, example, processor iis connected memory modules (i21),i, ( iþ1). case, processor 1 communicate processor 5 via memory modules 2, 3, 4 intermediaries. BSP (Burroughs’ Scientiﬁc Processor) used second SIMD scheme. order illustrate effectiveness SIMD handling array operations, con- sider, example, operations adding corresponding elements two one-dimensional arrays AandBand storing results third one-dimensional array C. Assume also three arrays Nelements. Assume also SIMD scheme 1 used. Nadditions required done one step elements three arrays distributed 0contains elements A(0),B(0), C(0),M1contains elements A(1),B(1), C(1), ..., MN21contains elements A(N21),B(N21), C(N21). case, processors execute simultaneously add instruction form C AþB. executing single step processors, elements resultant array Cwill stored across memory modules M0will store C(0),M1will store C(1), ..., andMN21will store C(N21). customary formally represent SIMD machine terms ﬁve-tuples (N,C,I,M,F). meaning argument given below. 1.Nis number processing elements ( N¼2k,k/C211). 2.Cis set control instructions used control unit, example, do, for,step. 3.Iis set instructions executed active processing units. 4.Mis subset processing elements enabled. 5.Fis set interconnection functions determine communication links among processing elements. 11.4. MIMD SCHEMES MIMD machines use collection processors, memory, used collaborate executing given task. general, MIMD systems categorized based memory organization shared-memory andmessage-passing architectures. choice two categories depends onthe cost communication (relative computation) degree load imbalance application.246 INTRODUCTION MULTIPROCESSORS11.4.1. Shared Memory Organization recent growing interest distributed shared memory systems. shared memory provides attractive conceptual model interprocess inter- action even underlying hardware provides direct support. shared memory model one processors communicate reading writing locations ashared memory equally accessible processors. processor may haveregisters, buffers, caches, local memory banks additional memory resources. number basic issues design shared memory systems taken consideration. include access control, synchronization, protection,and security. Access control determines process accesses possible towhich resources. Access control models make required check every access request issued processors shared memory, contents access control table. latter contains ﬂags determine legality ofeach access attempt. access attempts resources, desiredaccess completed, disallowed access attempts illegal processes areblocked. Requests sharing processes may change contents accesscontrol table execution. ﬂags access control synchroniza-tion rules determine system’s functionality. Synchronization constraints limit thetime accesses sharing processes shared resources. Appropriate synchro- nization ensures information ﬂows properly ensures system functiona- lity. Protection system feature prevents processes making arbitraryaccess resources belonging processes. Sharing protection incom-patible; sharing allows access, whereas protection restricts it. Running two copies program two processors decrease per- formance relative single processor, due contention shared memory. performance degrades three, four, copies program execute time. shared memory computer system consists (1) set independent processors, (2) set memory modules, (3) interconnection network. simplest sharedmemory system consists one memory module (M) accessed twoprocessors P aandPb(Fig. 11.8). Requests arrive memory module two ports. arbitration unit within memory module passes requests toa memory controller. memory module busy single request arrives,then arbitration unit passes request memory controller requestis satisﬁed. module placed busy state request serviced. Pa Pb Figure 11.8 simple shared memory scheme11.4. MIMD SCHEMES 247If new request arrives memory busy servicing previous request, memory module sends wait signal memory controller processormaking new request. response, requesting processor may hold requeston line memory becomes free may repeat request timelater. arbitration unit receives two requests, selects one passesit memory controller. Again, denied request either held beserved next may repeated time later. arbitration unit may adequate organize use memory module two processors. main problem sequencing inter-actions memory accesses two processors. Consider followingtwo scenarios accessing memory location M(1000) two pro-cessors P aandPb(Fig. 11.9). Let us also assume initial value stored memory location M(1000) 150. Note cases, sequence instruc-tions performed processor same. difference twoscenarios relative time two processors update value M(1000). careful examination two scenarios show value stored location M(1000) ﬁrst scenario 151 storedvalue following second scenario 152. illustrative example presents case nonfunctional behavior simple shared memory system. example demonstrate basicrequirements success systems. requirements are: 1. mechanism conﬂict resolution among rival processors 2. technique specifying sequencing constraints3. mechanism enforcing sequencing speciﬁcations Approaches satisfying basic requirements covered Chapter 4 book Advanced Computer Architecture Parallel Processing (see reference list). use different interconnection networks shared memory multiprocessor system leads systems one following characteristics: 1. Shared memory architecture uniform memory access (UMA) 2. Cache-only memory architecture (COMA)3. Distributed shared memory architecture nonuniform memory access (NUMA) Cycle Processor Pa Processor Pb 1 23456 Scenario 1a← M(1000); M(1000) ← a;a← a+ 1;b← M(1000); M(1000) ← b;b← b+ 1;Cycle Processor P Processor Pb 123456 Scenario 2a← M(1000); M(1000) ← a;a← a+ 1; b← M(1000) M(1000) ← b;b← b+ 1; Figure 11.9 Potential shared memory problem248 INTRODUCTION MULTIPROCESSORSFigure 11.10 shows typical organization abovementioned three shared- memory architectures. UMA system, shared memory accessible processors interconnection network way single processoraccesses memory. Therefore, processors equal access time anymemory location. interconnection network used UMA singlebus, multiple bus, crossbar, multiport memory. NUMA system, processor part shared memory attached. memory single address space. Therefore, processor could accessany memory location directly using real address. However, access time tomodules depends distance processor. results nonuniformmemory access time. number architectures used interconnect processorsto memory modules NUMA. Among tree hierarchical busnetworks (see Chapter 2 book Advanced Computer Architecture andParallel Processing, see reference list). Similar NUMA, processor part shared memory COMA. However, case shared memory consists cache memory. ACOMA system requires data migrated processor requesting it. 11.4.2. Message-Passing Organization Message passing represents alternative method communication move- ment data among multiprocessors. Local, rather global, memories usedto communicate messages among processors. message deﬁned block ofrelated information travels among processors direct links. exist anumber models message passing. Examples message-passing systemsinclude cosmic cube, workstation cluster, transputer. introduction transputer system T212 1983 announced birth ﬁrst message-passing multiprocessor. Subsequently T414 announced 1985, Inmos introduced VISI transputer processor 1986. Two subsequent transputer products, T800 (1988) T9000 (1990), beenP MP PInter- Connection Network (a)P C P C P CInter- Connection Network (b)M MP P P MInter- Connection Network (c) Figure 11.10 Three examples shared-memory architectures. ( a) UMA, ( b) COMA, (c) NUMA11.4. MIMD SCHEMES 249introduced. cosmic cube message-passing multiprocessor designed Caltech period 1981–1985. represented ﬁrst hypercube multi-processor system made work. Wormhole routing messagepassing introduced 1987 alternative traditional store-and-forward routing order reduce size required buffers todecrease message latency. wormhole routing , packet divided smaller units called ﬂits(ﬂow control bits) ﬂitsmove pipeline fashion header ﬂitof packet leading way destination node. header ﬂit blocked due network congestion, theremaining ﬂits blocked well (see Chapter 5 book AdvancedComputer Architecture Parallel Processing (see reference list) Volume IIfor details). elimination need large global memory, usually reason slowdown overall system, together asynchronous nature, givemessage-passing schemes edge shared-memory schemes. Similar shared-memory multiprocessors, application programs divided smaller parts; executed individual processor concurrent manner. simple example message-passing multiprocessor architecture shown Figure 11.11. seen ﬁgure, processors use local bus (internal chan-nels) communicate local memories communicating otherprocessors via interconnection networks (external channels). Processes runningon given processor use internal channels exchange messages among themselves. Processes running different processors use external channels exchange mess- ages. scheme offers great deal ﬂexibility accommodating large number processors readily scalable. noted processand processor, executes it, considered two separate entities. Thesize process determined programmer described granu-larity, given by: Granularity¼ computation time communication time P1 M1 Pn Mn Interconnection Network Figure 11.11 Example message-passing multiprocessor architecture250 INTRODUCTION MULTIPROCESSORSThree types granularity distinguished. are: 1. Coarse granularity. process holds large number sequential instruc- tions takes substantial time execute. 2. Medium granularity. Since process communication overhead increases granularity decreases, medium granularity describes middle ground whereby communication overhead reduced order enable nodalcommunication take less amount time. 3. Fine granularity. process contains numbers sequential instruc- tions (as one instruction). Message-passing multiprocessors use mostly medium coarse granularity. Message-passing multiprocessors employ static networks local communica- tion. particular, hypercube networks receiving special attention foruse message-passing multiprocessor. nearest neighbor two-dimensionaland three-dimensional mesh networks potential used amessage-passing system well. Two important factors led suitabilityof hypercube mesh networks use message-passing networks. Thesefactors (1) ease VLSI implementation (2) suitability two- three-dimensional applications. Two important design factors must considered designing networks. (1) link bandwidth (2) network latency . link bandwidth deﬁned number bits transmitted per unit time (bits / second). network latency deﬁned time complete message transfer. example, links could unidirectional bidirectional transfer onebit several bits time. estimate network latency, must ﬁrst determine path setup time, depends number nodes path. actual transition time, depends message size, must also considered. information transfer given source network done two ways: 1. Circuit-switching networks. type network, buffer required node. path source destination ﬁrst determined. links along path reserved. information transfer, reserved links released use messages. Circuit-switching net-works characterized producing smallest amount delay. Inefﬁ-cient link utilization main disadvantage circuit-switchingnetworks. Circuit-switching networks are, therefore, advantageously usedonly case large message transfer. 2. Packet-switching networks. Here, messages divided smaller parts, called packets, transmitted nodes. node mustcontain enough buffers hold received packets transmitting them. complete path source destination may available start transmission. links become available, packets moved from11.4. MIMD SCHEMES 251a node node reach destination node. technique also known store-and-forward packet-switching technique. Although store-and-forward packet-switching networks eliminate need complete path start transmission, tend increase overall networklatency. packets expected stored node buffers waiting forthe availability outgoing links. order reduce size required buffers decrease incurred network latency, wormhole routing (see above) introduced. touched machine categories based Flynn’s classi- ﬁcations, provide introduction interconnection networks used machines. provide detailed coverage multiprocessor interconnectionnetworks Chapter 2 book Advanced Computer Architecture ParallelProcessing (see reference list). 11.5. INTERCONNECTION NETWORKS number classiﬁcation criteria exist interconnection networks (INs). Among criteria following. 11.5.1. Mode Operation According mode operation, INs classiﬁed synchronous versus asynchronous . synchronous mode operation, single global clock used components system whole system operating lock- step manner. Asynchronous mode operation, hand, require global clock. Handshaking signals used instead order coordinate theoperation asynchronous systems. synchronous systems tend slowercompared asynchronous systems, race hazard-free. 11.5.2. Control Strategy According control strategy, INs classiﬁed centralized versus decen- tralized . centralized control systems, single central control unit used over- see control operation components system. decentralized control, control function distributed among different components thesystem. function reliability central control unit become bottle- neck centralized control system. crossbar centralized system, multistage interconnection networks decentralized. 11.5.3. Switching Techniques Interconnection networks classiﬁed according switching mechanism circuit versus packet switching networks. circuit switching mechanism, a252 INTRODUCTION MULTIPROCESSORScomplete path established prior start communication source destination. established path remain existence thewhole communication period. packet switching mechanism, communicationbetween source destination takes place via messages divided intosmaller entities, called packets. way destination, packets besent one node another store-and-forward manner reachtheir destination. packet switching tends use network resources efﬁciently, compared circuit switching, suffers variable packet delays. 11.5.4. Topology According topology, INs classiﬁed static versus dynamic networks. dynamic networks, connections among inputs outputs made using switching elements. Depending switch settings, different interconnections estab-lished. static networks, direct ﬁxed paths exist nodes. noswitching elements (nodes) static networks. introduced general criteria classiﬁcation interconnection net- works, introduce possible taxonomy INs based topology. Figure 11.12, provide taxonomy. According shown taxonomy, INs classiﬁed either static dynamic. Static networks classiﬁed according interconnection patterns one-dimension (1D), two-dimension (2D), hypercubes (HCs). Dynamic net-works, hand, classiﬁed according scheme inter-connection bus-based versus switch-based. Bus-based INs classiﬁed singlebus multiple bus. Switch-based dynamic networks classiﬁedaccording structure interconnection network single-stage (SS), multi- stage (MS), crossbar networks. Multiprocessor interconnection networks explained detail Chapter 2 book Advanced Computer Architecture Parallel Processing (see reference list). Interconnection Networks Static SingleDynamic 1D 2D HC Bus-based Switch-based Crossbar MS SS Multiple Figure 11.12 topology-based taxonomy interconnection networks11.5. INTERCONNECTION NETWORKS 25311.6. ANALYSIS PERFORMANCE METRICS provided introduction architecture multiprocessors, pro- vide basic ideas performance issues multiprocessors. Interestedreaders referred Chapter 3 book Advanced Computer Architectureand Parallel Processing (see reference list) (Volume II) details. fundamental question usually asked much faster given problem solved using multiprocessors compared single processor? Thisquestion formulated speed-up factor deﬁned below. S(n)¼speed-up factor ¼Increase speed due use multiprocessor system consisting nprocessors ¼ Execution time using single processor Execution time using nprocessors related question efﬁciently nprocessors utilized. question formulated efﬁciency deﬁned below. E(n)¼Efﬁciency ¼S(n) n/C2100% executing tasks (programs) using multiprocessor, may assumed agiven task divided nequal subtasks executed one processor. Therefore, expected speed-up given S(n)¼n efﬁciency E(n)¼100%. assumption given task divided nequal subtasks, executed processor, unrealistic. Chapter 3 book Advanced Computer Architecture Parallel Processing(see reference list) meaningful computation models developed analyzed.A number performance metrics also introduced analyzed samechapter. 11.7. SUMMARY chapter, navigated number concepts system conﬁgurations related issues multiprocessing. particular, haveprovided general concepts terminology used context multiproces-sors. number taxonomies multiprocessors introduced andanalyzed. Two memory organization schemes introduced. shared-memory message-passing systems. addition, introduced different topologies used interconnecting multiple processors. Chapter 2254 INTRODUCTION MULTIPROCESSORSof book Advanced Computer Architecture Parallel Processing (see reference list) said interconnection networks theirperformance metrics. Shared-memory message-passing architectures areexplained Chapters 4 5, respectively, reference mentionedabove. EXERCISES 1. Consider ﬁve classiﬁcations computer architectures discussed chapter. required provide list showing advantages dis- advantages classiﬁcation view degree classi-ﬁcation satisﬁes purpose classiﬁcation needed. 2. required derive, ﬁve provided classiﬁcations, new classiﬁcation outperforms ﬁve classiﬁcations. Provide, atabular form, additional advantages eliminated shortcomings theproposed classiﬁcation. 3. Provide list main advantages disadvantages SIMD MIMD machines. 4. Provide list main advantages disadvantages shared-memory message-passing paradigms. 5. List three engineering applications, familiar, SIMD efﬁcient use, another three MIMD efﬁcient use. 6. Consider case connecting Nprocessors Nmemory modules using interconnection networks shown Figure 11.1. Assume time required processor access item memory module processors make request access distinct memory module.Compute worst-case possible delay expected four inter-connection networks. 7. mentioned given SIMD machine could characterized using ﬁve-tuple ( N,C,I,M,F). required select three different recent SIMD machines provide tabular form ﬁve-tuples thatcharacterizes them. 8. Assume simple addition two elements requires unit time. required compute execution time needed perform addition a40/C240 element array using following arrangements: (a) SIMD system 64 processing elements connected nearest- neighbor fashion. Consider processor local memory. (b) SIMD system 64 processing elements connected shared memory interconnection network. Ignore communicationtime.EXERCISES 255(c) MIMD computer system 64 independent elements accessing shared memory interconnection network. Ignore communication time. (d) Repeat (b) (c) communication time takes two time units. 9. Provide concise discussion suitability four attributes interconnection networks (mode operation, control strategy, switchingmechanism, topology) four different interconnection net-works shown Figure 11.1. Make sure justify suitability agiven attribute given interconnection network. 10. Consider case multiprocessor system consisting Nprocessors. Assume time needed processor execute given critical section tand frepresents fraction operations paral- lelized. Assume also single processor need time Tto execute task. Show total execution time using Nprocessors given N¼(1/C0f)/C2Tþf/C2T Nþt: number processors, N, needed order minimize total execution time TN. REFERENCES READING S. Abraham K. Padmanabhan, Performance direct binary n-cube network multi- processors, IEEE Trans. Comput. , 38(7), 1000–1011 (1989). P. Agrawal, V. Janakiram G. Pathak, Evaluating performance multicomputer conﬁgurations, IEEE Trans. Comput. , 19(5), 23–27 (1986). G. Almasi A. Gottlieb, Highly Parallel Computing , Benjamin Cummings, Redwood City, CA, USA, 1989. K. Al-Tawil, M. Abd-El-Barr F. Ashraf, survey comparison wormhole routing techniques mesh networks, IEEE Network , 11(2), 38–45 (1997). L. Bhuyan, Q. Yang D. Agrawal, Performance multiprocessor interconnection net- works, IEEE Comput. , 22(2), 25–37 (1989). W.-T. Chen J.-P. Sheu, Performance analysis multiple bus interconnection networks hierarchical requesting model, IEEE Trans. Comput. , 40(7), 834–842 (1991). S. Dasgupta, Computer Architecture: Modern Synthesis , Vol. 2: Advanced Topics, John Wiley, New York, 1989. A. Decegama, Technology Parallel Processing: Parallel Processing Architectures VLSI Hardware Volume 1 , Prentice-Hall, NJ, 1989. J. Dongarra, Experimental Parallel Computing Architectures , North-Holland, Amsterdam, 1987. A. Goyal T. Agerwala, Performance analysis future shared storage systems, IBM J. Res. Devel. , 28(1), 95–107 (1984).256 INTRODUCTION MULTIPROCESSORSJ.-Y. Juang B. Wah, contention-based bus-control scheme multiprocessor systems, IEEE Trans. Comput. , 40(9), 1046–1053 (1991). T. Lewis H. El-Rewini, Introduction Parallel Computing, Prentice-Hall, Englewood Cliffs, NJ, 1992. D. Linder J. Harden, adaptive fault tolerant wormhole routing strategy k-ary n-cubes, IEEE Trans. Comput. , 40(1), 2–12 (1991). L. Ni P. McKinely, survey wormhole routing techniques direct networks, IEEE Comput. , 26(2), 62–76 (1993). J. Patel, Performance processor–memory interconnections multiprocessor computer systems, IEEE Trans. , 28(9), 296–304 (1981). D. Reed R. Fujimoto, Multicomputer Networks: Message-Based Parallel Processing , MIT Press, MA, USA, 1987. H. El-Rewini T. Lewis, Distributed Parallel Computing, Manning & Prentice Hall, 1998. H. El-Rewini M. Abd-El-Barr, Advanced Computer Architecture Parallel Processing, John Wiley, Hoboken, NJ, USA, 2005. E. Sima, T. Fountain P. Kacsuk, Advanced Computer Architectures: Design Space Approach , Addison Wesley, MA, USA, 1996. H. Stone, High Performance Computer Architecture , 3rd ed., Addison Wesley, MA, USA, 1993. B. Wilkinson, Computer Architecture: Design Performance , 2nd ed., Prentice-Hall, Hertfordshire, UK, 1996. Q. Yang S. Zaky, Communication performance multiple-bus systems, IEEE Trans. Comput. , 37(7), 848–853 (1988). H. Youn C. Chen, comprehensive performance evaluation crossbar networks, IEEE Trans. Parallel Distribute Syst. , 4(5), 481–489 (1993). M. Zargham, Computer Architecture: Single Parallel Systems , Prentice-Hall, NJ, USA, 1996.REFERENCES READING 257& INDEX Access type, memory system design, 108–109 Accumulator-based processor assembly language programming, 38–40 input-output design, 166 Adders, 65–67, 73–74, 209–211 Addition operation addressing modes, 18–19CPU instruction cycle, 92–94 ﬂoating-point arithmetic pipelines, 211–212 two’ complement (2’ s) representation, 63–64 Address decoder circuitry, /O system design, 163 Address ﬁeld, addressing modes, 18 Addressing modes instruction set architecture, 18–26 autodecrement mode, 25–26 autoincrement mode, 23–24 direct (absolute) mode, 21–22immediate mode, 21 indexed mode, 23 indirect mode, 22 relative mode, 23 X86 family, assembly language programming for, 50–55 Address lines, input /output (I /O) buses, 177 Add-shift method, multiplication unsigned numbers, 68–70 Add/sub operations Booth’ algorithm, 71–72 ﬂoating-point arithmetic, 75–76 Advanced RISC Machines (ARM) interrupt architecture, input /output systems, 171–1731026EJ-S processor, pipeline design, 202–203 pipeline stall reduction, 198–199 Aligned exponents, ﬂoating-point arithmetic addition /subtraction, 75–76 Allocate policies, 127 Alpha 2164 pipeline, RISC design, 227–230 Amdahl’ law, performance analysis, 10–11 Arbitration buses, 179–180multiprocessor architecture, multiple- instruction multiple-data streams(MIMD), 248–249 Architecture classiﬁcations. See also Instruction set architecture evolution of, 4–5multiprocessors, 236–244 Erlangen classiﬁcation, 241–242 Flynn’ classiﬁcation, 237–240 Hwang Briggs classiﬁcation scheme, 240–241 Kuck classiﬁcation, 240 Skillicorn classiﬁcation, 242–244 Arithmetic Logic Unit (ALU) Booth’ algorithm, 71–72 central processing unit design, 83–85 datapath, 89–91horizontal vs. vertical microinstruc- tions, 103–104 three-bus organization, 90–91 two-bus organization, 90 data dependency pipeline stall reduction, 199–200 1026EJ-S processor pipeline design, 202–203 Arithmetic instructions, 27–28 X86 family, 50–55 259 Fundamentals Computer Organization Architecture , M. Abd-El-Barr H. El-Rewini ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.Arithmetic pipline design, 209–213 ﬁxed-point arithmetic, 209–211 ﬂoating-point arithmetic, 211–212 multiplication carry-save addition, 212–213 Array processors main memory unit, 137–142 ASCII code input-output design, 161 Assemblers, assembly language programming, 45 Assembly execution process, assembly language programming, 44–47 Assembly language programming assembler directives commands, 43–44 basic principles, 37–38 instruction mnemonics syntax, 40–43 programing assembly execution, 44–47 assemblers, 45 data structures, 45–46 linker loader, 46–47 simple machines, 38–40 X86 family example, 47–55 Associative memory cache memory organization, 118 virtual memory system, 144–145 Asynchronous buses input /output system design, 178–179 multiprocessor interconnection networks, 252 Autodecrement addressing mode, 25–26Autoincrement addressing mode, 23–24 Bandwidth parameters memory system design, 108–109 multiprocessors, 251–252 Base pointer register (BPR), X86 family, assembly language programming for, 50–55 Base radix, ﬂoating-point representation, 75 Base register sets, X86 family, assembly language programming for, 49–55 Benchmarks computer systems, 6–11 Berkeley RISC, design features, 223–224Bias, ﬂoating-point representation, 75 Binary coded decimal (BCD) system assembly language programming, simple machines, 39–40 computer architecture and, 4Binary digit (bit), memory locations operations, 15–18 Binary division structure, integer arithmetic, 73–74 Binary radices integer arithmetic, 63–74 number system, 59–63 Bit.SeeBinary digit (bit), memory locations operations Block ﬁeld protocol, cache memory organization, 115–116 Boolean equations, central processing unit design control unit, 96 hardwired implementation, 97–98 Booth’ algorithm, multiplication unsigned numbers, 70–72 Branch delay shots, pipeline stall reductions, conditional branch instructions, 196–197 “Branch negative” instruction, pipeline stall, instruction dependency, 188 Branch instruction queue (BIQ), UltraSPARC III RISC processor pipeline design, 207 Branch miss queue (BMQ) pipeline stall reduction, 199UltraSPARC III RISC processor pipeline design, 207 Branch prediction Alpha 2164 pipeline, 228–230pipeline stall reduction, 198–199 UltraSPARC III RISC processor, 206–207 Branch target address cache (BTAC), pipeline stall reduction, 198–199 Buffers instruction buffers, pipeline design, 207 translation look-aside buffer (TLB), 146–148 Buses, input /output (I /O) systems, 177–180 arbitration, 179–180 asynchronous buses, 178–179 synchronous buses, 178 Bus line connection, input /output (I /O) systems, 177 Bytes, memory locations operations, 16–18 Cache hit cache write policies, 125–127 virtual memory system, cache memory with, 152–153260 INDEXCache locking, PMC-Sierra RM7000A processor, 130 Cache memory, 109–130 combined spatial /temporal locality, 111–112 direct mapping, 113–116 fully associative memory, 116–118 mapping function, 112–113 organization, 113–121 real-life organization analysis, 128–130 Pentium IV processor cache, 128–129 PMC-Sierra RM7000A 64-bit MIPS RISC processor, 130 PowerPC 604 processor cache, 129 replacement techniques, 121–124 set-associative mapping, 118–121spatial locality, 111 temporal locality, 111 virtual memory systems with, 152–153 write policies, 124–127 Cache miss cache memory organization, 115–116 set-associative mapping, 121 instruction pipeline design, 187–188 write policies, 127 Cache-only memory architecture (COMA), multiprocessor architecture, multiple- instruction multiple-data streams (MIMD), 248–249 Cache unit, memory hierarchy, 107–109 Capacity parameters, memory system design, 108–109 Carry generate function, hardware structures addition subtraction, 65–67 Carry-look-ahead adders (CLAA) ﬁxed-point arithmetic pipelines, 209–211 hardware structures addition subtraction, 65–67 Carry propagate function, hardware structures addition andsubtraction, 64–67 Carry ripple adder (CRTA) ﬁxed-point arithmetic pipelines, 209–211 hardware structures addition subtraction, 65–67 Carry-save addition, pipelined multiplication, 212–213 Centralized bus arbitration, input /output system design, 179–180 Centralized interconnections, multiprocessor interconnection networks, 252Central processing unit (CPU) buses, 177–178 clock, performance analysis, 6–7 control unit, 95–104 hardwired implementation, 97–98 horizontal vs. vertical microinstructions, 101–104 microprogrammed unit, 98–100 datapath, 89–91 one-bus organization, 89 three-bus organization, 90–91 two-bus organization, 90 design criteria, basic principles, 83–85 input /output (I /O) system design, 161–162 direct memory access, 175–177 instruction cycle, 91–95 execute simple arithmetic operation, 92–94 fetch instructions, 92 interrupt handling, 94–95 interrupt-driven input /output (I /O) systems, 167–175 main memory and, 135–142memory locations operations, 16–18 overlapped register windows, 220–221programmed input /output design, 166–167 register set, 85–88 condition registers, 86instruction fetching registers, 86 memory access registers, 85–86 MIPS registers, 87–88 special-purpose address registers, 86 80x86 registers, 87 Characters input-output design, 161 Chip memory arrays, main memory unit, 137–142 Chip pin arrangements, main memory unit, 138–142 Circuitry. See also Adders main memory unit, 140–142 Circuit-switching networks, multiprocessor architecture interconnections, 252–253 multiple-instruction multiple-data streams (MIMD), 251–252 Classes multiprocessors, 240–241 Clock cycles per instruction (CPI), performance analysis, 7–8INDEX 261Clock rates central processing unit design, control unit, 95–104 computer architectures and, 4–5 performance analysis, 6–7 Clock replacement algorithm, virtual memory, 150–152 CMOS systems static memory units, 140–142 Column address strobe (CAS), main memory unit, 141–142 Combined temporal /spatial locality, cache memory, 111–112 Commands, assembly language programming, 43–44 Communication model (CM), multi- processor design, 236 Compaq (formerly DEC) alpha 21264 architecture, RISC design, 227 Complex instruction set computers (CISC) architecture examples of, 4RISC design vs, 221–223 RISC evolution cycle, 217–218 Computer arithmetic double-precision IEEE format, 78–79 ﬂoating-point arithmetic, 74–77 addition /subtraction, 75–76 division, 77 multiplication, 76–77 representation (scientiﬁc notation), 74–75 IEEE ﬂoating-point standard, 77–78 integer arithmetic, 63–74 addition, 63–64 division, 72–74 multiplication, 68–72 add-shift method, 68–70Booth’ algorithm, 70–72paper pencil method (unsigned numbers), 68 signed numbers, hardware structures, 64–67 subtraction, 64 two’ complement (2’ s) represen- tation, 63 number systems, 59–63 diminished radix complement, 62–63 negative integer representation, 61radix complement, 62 radix conversion algorithm, 60–61 sign-magnitude, 61 Conditional branch instructions, pipeline stall reductions, 196–199Condition registers, central processing unit design, 86 Context switching, interrupt-driven /O systems, 169–170 Control circuitry, main memory unit, 140–142 Control Data Corporation, history of, 3 Control memory (CM), central processing unit design, microprogrammed units, 100–104 Control strategy, multiprocessor inter- connection networks, 252 Control transfer instructions, X86 family, 51–55 Control unit (CU), central processing unit design, 83–85, 95–104 hardwired implementation, 97–98 horizontal vs. vertical microinstructions, 101–104 microprogrammed unit, 98–100 Cost parameters, memory system design, 108–109 Cray Research Corporation, history of, 3Current program state register (CPSR), advanced RISC machines interrupt architecture, 173 Current window pointer, Berkeley RISC, 223–224 Cycle count (CC), performance analysis, 6–7 Cycle time (CT) memory system design, 108–109 performance analysis, 6–7 Daisy chain bus arbitration (DCBa) input /output system design, 179–180 interrupt-driven /O device, 168 Data dependency, pipeline stall, 189–192 NOP prevention method, 192–194 reduction methods, 199–201 Data memory (DM), multiprocessor architecture, Skillicorn classiﬁcation, 242–244 Data movement instructions basic properties, 26–27 X86 family, 50–55 Data output register, input /output instructions and, 31 Datapath, central processing unit, 89–91 one-bus organization, 89 three-bus organization, 90–91 two-bus organization, 90 Data structures, assembly language programming, 45–46262 INDEXDecentralized arbitration, buses, 180 Decentralized interconnections, multi- processor interconnection networks, 252 Delayed branch, pipeline stall reductions, conditional branch instructions, 196–197 Demand paging, 148Dependency data dependency, 189–194, 200–201 instructional dependency, 188, 194–199 Destination registers, arithmetic logical instructions, 27–28 CPU design, 92–94 Devices per chip, evolution integration, table of, 5 Digital Equipment Corporation (DEC), history of, 3 Diminished radix complement, number systems, 62–63 Direct (absolute) addressing mode, 21–22 X86 family, assembly language programming for, 50–55 Directives, assembly language programming, 43–44 Direct mapping cache memory organization, 113–116 replacement techniques, 122–123 virtual memory systems, 143–144 Direct memory access (DMA) basic properties, 175–177 input /output (I /O) system design, 162, 175–177 Dirty bit cache write policies, 126–127virtual memory systems, 143 Distributed memory systems, history of, 3Division ﬂoating-point arithmetic, 77–78 integer arithmetic, 72–74 Double-point format, IEEE ﬂoating-point standard, 77–79 Dynamic branch prediction, pipeline stall reduction, conditional branch instructions, 198–199 Dynamic cells, main memory unit, 139–142 Dynamic interconnections, multiprocessor interconnections, 253 Dynamic scheduling, instruction-level parallelism, pipeline design, 207–209 Efﬁciency E(n) measurement, pipelining design, 186–18780x86 registers central processing unit design, 87 interrupt-driven /O systems, 170–171 Electronic Delay Storage Automatic Calculator (EDSAC), history of, 3 Electronic Discrete Variable Automatic Computer (EDVAC), history of, 2 Electronic Numerical Integrator Calculator (ENIAC) machine, history of, 2 Elementary logic unit (ELU), multiprocessor architecture, Erlangen classiﬁcation scheme, 241–242 Erasable PROM (EPROM), basic properties, 157–158 Erlangen classiﬁcation scheme, multipro- cessor architecture, 241–242 Evolutionary architecture classiﬁcation, multiprocessors, 237 Execution stream multiprocessors, 240 Execution time, performance analysis, 10Exponent ﬂoating-point representation, 75 IEEE ﬂoating-point standard, 78 Exponent alignment (EA) operation, ﬂoating-point arithmetic pipelines, 211–212 Exponent comparison (EC) operation, ﬂoating-point arithmetic pipelines, 211–212 Fetch-fetch operation, data dependency pipeline reduction, 201 Fetch stage Alpha 2164 pipeline, 227–230CPU instruction cycle, 82 microprogrammed control unit, 100–104 pipeline stall reduction, 195 1026EJ-S processor pipeline design, 202–203 prediction of, 197–198 FIQ requests, advanced RISC machines interrupt architecture, 172–173 First-in-ﬁrst-out (FIFO) replacement cache memory systems, 121–124 1026EJ-S processor pipeline design, 202–203 virtual memory systems, 148–149 First-in-not-used-ﬁrst-out (FINUFO), clock replacement algorithm, virtual memory, 150–152INDEX 263Fixed-point arithmetic history computers and, 2 pipelines design principles, 209–211 Flag register, X86 family, assembly language programming for, 49–55 Flash EPROMs (FEPROMs), basic properties, 157–158 Flits (ﬂow control hits), multiprocessor architecture, multiple-instruction multiple-data streams (MIMD), 250–252 Floating-point arithmetic, 74–77 addition /subtraction, 75–76 Alpha 2164 pipeline, 230Berkeley RISC, 224 division, 77 IEEE standard, 77–79multiplication, 76–77 pipeline design, 211–212 representation (scientiﬁc notation), 74–75 Flynn’ classiﬁcation scheme, multiproces- sor architecture, 237–240 Full-adder (FA), hardware structures addition subtraction, 65–67 Fully associative mapping, cache memory organization, 116–118 replacement techniques, 123, 125 Gantt’s chart pipeline stall conditional branch instructions, 197 data dependency, 190–192 instruction dependency, 188 pipelining design, 186 General purpose computer systems, 1 central processing unit design, 87 Global share dynamic prediction algorithm, UltraSPARC III RISC processor pipeline design, 206–207 Grant line (GL), interrupt-driven /O device, 168 Granularity, multiprocessor architecture, multiple- instruction multiple-data streams (MIMD), 250–252 Handshaking, buses, 178 Hardware operand forwarding, data dependency pipeline stall reduction, 199–200 Hardware structures addition /subtraction signed numbers, 64–67 binary division operations, 73–74interrupt-driven /O device, 168 I/O polling scheme, programmed input / output design, 166–167 pipeline stall reduction, fetch unit, 195 Hardwired control units, central processing unit design, 96–104 direct implementation, 97–98 Harvard Architecture, history of, 3Harvard Organization, PowerPC 604 processor cache, 129 Hexadecimal programming, simple machines, 40 Hexagonal base, number system, 59–63 Hierarchy parameters, memory system design, 107–109 High-level languages (HLLs), RISC design, 218–220 Historical background, computer systems, 2–4 Hit ratio cache memory, 110 memory hierarchy, 109 Horizontal microinstructions, central processing unit design, 101–104 Hwang /Briggs classiﬁcation scheme, multiprocessor architecture, 240–241 IBM systems, history of, 3 IEEE standard, ﬂoating-point standard, 77–79 ILLAC-IV design Erlangen classiﬁcation scheme, 242 multiprocessor architecture, single-instruction multiple-data streams (SIMD), 245–246 Immediate addressing mode, 20–21 Independent source bus arbitration (ISBA), interrupt-driven /O device, 168 Indexed addressing mode, 20–21, 23 Index register central processing unit design, 86deﬁned, 23 X86 family, 50–55 Indirect addressing mode, 22 X86 family, 50–55 Input data register input /output instructions and, 31 input /output (I /O) system design, 162–163 Input /output (I /O) system design basic concepts, 162–164 buses, 177–180 arbitration, 179–180 asynchronous buses, 178–179264 INDEXsynchronous buses, 178 central processing unit design, 84–85 design organization, 161–162 direct memory access, 175–177 instruction set architecture and, 1, 30–31 interfaces, 181–182 interrupt-driven /O, 167–175 ARM architecture, 171–173hardware, 168 MC9328MX1 /MXL AITC, 173– 175 operating systems, 168–175 80x86 architecture, 170–171 programmed /O, 164–167 Institute Advanced Study (IAS) machine, history of, 2 Instruction buffer, UltraSPARC III RISC processor pipeline design, 207 Instruction cycle, central processing unit design, 91–95 execute simple arithmetic operation, 92–94 fetch instructions, 92interrupt handling, 94–95 Instruction dependency, pipeline stall, 188 unconditional branch instructions, 194–196 Instruction-level parallelism (ILP), pipelining design, 207–209 superscalar architectures, 207–209 long instruction word (VLIW), 209 Instruction memory (IM), multiprocessor architecture, Skillicorn classiﬁcation, 242–244 Instruction pipeline design, 187–201 data dependency "stall," 189–201 hardware operand forwarding, 199–200 software operand forwarding, 200–201 instruction dependency "stall," 188 conditional branch instructions, 196–199 unconditional branch instructions, 194–196 wrong instruction /operand, prevention, 192–194 Instruction prefetching, pipeline stall reduction, 196 Instruction register (IR), central processing unit design, 83–86 one-bus organization, 89 Instruction reordering, pipeline stall reduction, 194–195precomputing branches, 195–196 Instruction set architecture addressing modes, 18–26 autodecrement mode, 25–26 autoincrement mode, 23–24 direct (absolute) mode, 21–22 immediate mode, 21 indexed mode, 23 indirect mode, 22 relative mode, 23 assembly language programming mnemonics syntax, 40–43simple machines, 38–40 basic principles, 15 deﬁned, 1 instruction types, 26–31 arithmetic logical instructions, 27–28 data movement instructions, 26–27 input /output instructions, 30–31 sequencing instructions, 28–30 memory locations operations, 15–18 programming examples, 31–33 Instruction types, instruction set architecture, 26–31 arithmetic logical instructions, 27–28 data movement instructions, 26–27 input /output instructions, 30–31 sequencing instructions, 28–30 Integer arithmetic, 63–74 addition, 63–64division, 72–74 multiplication, 68–72 add-shift method, 68–70 Booth’ algorithm, 70–72 paper pencil method (unsigned numbers), 68 signed numbers, hardware structures, 64–67 subtraction, 64 two’ complement (2’ s) representation, 63 Integer unit Alpha 2164 pipeline, 230 1026EJ-S processor pipeline design, 203 Integrated circuit (IC), main memory unit, 141–142 Intel microprocessors central processing unit design, 87 history of, 3real-life cache organization analysis, 128–129 X86 family, assembly language programming for, 48–55INDEX 265Interconnection networks, multiprocessor architecture, 252–253 Interrupt acknowledgement (INTA), interrupt-driven /O systems, 170–171 Interrupt-driven communication, input / output (I /O) systems, 161–162 Interrupt-driven input /output (I /O) systems, 167–175 ARM architecture, 171–173 hardware, 168 MC9328MX1 /MXL AITC, 173–175 operating systems, 168–17580x86 architecture, 170–171 Interrupt handling, CPU design, 94–95Interrupt service routine (ISR) interrupt-driven input /output (I /O) systems, 167–175 interrupt-driven /O systems, 170–171 Interrupt vector table (IVT) advanced RISC machines interrupt architecture, 173 interrupt-driven /O systems, 170–171 INTR pin, interrupt-driven /O systems, 80x86 architecture, 170–171 I/O protocol, input /output (I /O) system design, 162–163 IRQ request, advanced RISC machines interrupt architecture, 172–173 JBus , UltraSPARC III RISC processor design, 231–232 Keyboard, input /output device, 31 Kuck classiﬁcation scheme, multiprocessor architecture, 240 Language architecture, deﬁned, 1 Large-scale integration (LSI), evolution of, 5–6 Latency parameters, memory system design, 108–109 Least recently used (LRU) replacement cache memory, 121–124 virtual memory, 149–150 “Likely taken” (LNK) algorithm, pipeline stall reduction, conditional branch instructions, 198–199 “Likely taken” (LTK) algorithm, pipeline stall reduction, conditional branch instructions, 198–199 Linkers, assembly language programming, 46–47Loaders, assembly language programming, 46–47 Local area networks (LAN), history of, 3Locality reference, memory hierarchy, 108–109 Logical instructions, 27–28 X86 family, 50–55 Machine language, assembly language programming, 38 Main memory unit (MMU) basic properties, 135–142 fully associative mapping, 116–118 hierarchy parameters, 107–109 virtual memory, 142–155 associative mapping, 144–145 cache memory, 152–153 paged segmentation, 154–155 Pentium memory management, 155 replacement algorithms (policies), 148–152 clock replacement algorithm, 150–152 ﬁrst-in-ﬁrst-out (FIFO) replacement, 148–149 least recently used (LUR) replacement, 149–150 random replacement, 148 segment address translation, 153–154segmentation, 153 set-associative mapping, 145–146 translation look-aside buffer (TLB), 146–148 Mantissa, ﬂoating-point representation, 74–75 Many-to-one mapping technique, cache memory organization, 113–116 Mapping function, cache memory, 112–113 MARK computer systems, history of, 3Mask-programmed ROMs, 156–158 MC9328MX1 /MXL AITC, input /output systems, 173–175 Medium-scale integration (MSI), evolution of, 5–6 Memory access registers, central processing unit design, 85–86 Memory address register (MAR) central processing unit design, 85–86 interrupt handling, 94–95 one-bus organization, 89 fetch instructions, 92 main memory, 135–142 write operations and, 17–18 Memory data register (MDR) central processing unit design, 85–86266 INDEXinterrupt handling, 94–95 one-bus organization, 89 fetch instructions, 92 main memory, 135–142 write operations and, 17–18 Memory hierarchy, Alpha 2164 pipeline, 227–230 Memory indirect addressing, 22Memory interleaving, cache memory, 110–111 Memory locations operations, instruction set architecture, 15–18 Memory management unit (MMU), cache-mapping function, 112–113 Memory-mapped input /output, 31 I/O system design, 164 Memory operations, central processing unit design, microinstructions, 103–104 Memory system design basic concepts, 107–109buses, 177–180cache memory, 109–130 combined spatial /temporal locality, 111–112 direct mapping, 113–116 fully associative memory, 116–118 mapping function, 112–113 organization, 113–121 real-life organization analysis, 128–130 replacement techniques, 121–124 set-associative mapping, 118–121spatial locality, 111 temporal locality, 111 write policies, 124–127 hierachy parameters, 107–109 input /output (I /O) interfaces, 181–182 main memory, 135–142 read-only memory, 156–158 virtual memory, 142–155 associative mapping, 144–145cache memory, 152–153 paged segmentation, 154–155 Pentium memory management, 155 replacement algorithms (policies), 148–152 clock replacement algorithm, 150–152 ﬁrst-in-ﬁrst-out (FIFO) replacement, 148–149 least recently used (LUR) replacement, 149–150 random replacement, 148 segment address translation, 153–154segmentation, 153 set-associative mapping, 145–146 translation look-aside buffer (TLB), 146–148 Message-passing organization, multi- processor architecture, multiple-instruction multiple-data streams (MIMD), 249–252 Microinstructions, central processing unit design, 84–85 horizontal vs. vertical, 101–104 Microprocessor, history of, 3 Microprogrammed units, central processing unit design, 96–104 Million ﬂoating-point instructions per second (MFLOP), performanceanalysis, 9–10 Million instructions-per-second (MIPS) rate central processing unit registers, 87–88 performance analysis, 8–9 Minicomputers, history of, 3Miss ratio cache memory, 110memory hierarchy, 109 Mnemonics, assembly language programming, instruction set, 40–43 Morphological architecture classiﬁcation, multiprocessors, 237 signiﬁcant bit (MSB), ﬂoating-point representation, 75 Mouse, input /output device, 31 MPP system, multiprocessor architecture, single- instruction multiple-data streams (SIMD), 245–246 Multiple-instruction multiple-data streams (MIMD), multiprocessor architecture basic principles, 246–252 Flynn classiﬁcation, 238–240 Hwang /Briggs classiﬁcation scheme, 241 message-passing organization, 249–252 shared memory organization, 247–249 Multiple-instruction single-data streams (MISD), multiprocessor architecture, Flynn classiﬁcation, 238–240 Multiple interrupts, interrupt-driven input / output (I /O) systems, 168–175 Multiple issue processors (MIP), instruction- level parallelism, pipeline design, 207–209 Multiplication ﬂoating-point arithmetic, 76–77 integer arithmetic, 68–72INDEX 267Multiplication [ continued ] pipelined multiplication, carry-save addition, 212–213 Multiprocessors architecture classiﬁcations, 236–244 Erlangen classiﬁcation, 241–242 Flynn’ classiﬁcation, 237–240 Hwang Briggs classiﬁcation scheme, 240–241 Kuck classiﬁcation, 240 Skillicorn classiﬁcation, 242–244 basic principles, 235–236 interconnection networks, 252–253 MIMD architecture, 246–252 message-passing organization, 249–252 shared memory organization, 247–249 performance analysis, 254SIMD design, 244–246 (nþ1)-bit adder, binary division oper- ations, 73–74 Negative integer representation, number systems, 61 Network systems, history of, 3 NMI pins, interrupt-driven /O systems, 170–171 Non-blocking caches, real-life organization, 130 Non-restoring division algorithm, integer arithmetic, 73–74 Nonuniform memory access (NUMA), multiprocessor architecture, multiple-instruction multiple-data streams (MIMD), 248–249 Nonvolatile memory, 156 operation (NOP) method, pipeline stall, data dependency, 192–194 Normalization (NZ) operation, ﬂoating- point arithmetic pipelines,211–212 Normalized forms ﬂoating-point arithmetic addition / subtraction, 76 ﬂoating-point representation, 75 Number systems, 59–63 diminished radix complement, 62–63 negative integer representation, 61 radix complement, 62 radix conversion algorithm, 60–61sign-magnitude, 61Octagonal base, number system, 59–63One-address instruction, addressing modes, 18–19 One-bus organization, CPU datapath, 89 arithmetic operations, 93–94 Op-code addressing modes, 18 assembly language programming data structures, 45–46 mnemonics syntax, 41–43 central processing unit design, control unit, 96–104 Operands addressing modes, 18, 20–21 pipeline stall, data dependency, 192–196 hardware operand forwarding, 199–200 software operand forwarding, 200–201 Operating systems (OS) assembly language programming, assembler directives commands, 44 interrupt-driven /O systems, 168–171 virtual memory system, segmentation, 153 Operation modes, multiprocessor architec- ture, 252 Operations distribution, RISC design, 219–220 Out-of-order (OOO) issue logic, Alpha 2164 pipeline, 229–230 Output register, input /output (I /O) system design, 162–163 Overlapped register windows, RISC design, 220–222 Packet-switching networks, multiprocessor architecture interconnections, 252–253 multiple-instruction multiple-data streams (MIMD), 251–252 Paged segmentation, virtual memory system, 154–155 Page structure, virtual memory systems, 142–143 Page table virtual memory system cache memory with, 152–153 set-associative mapping, 145–146 virtual memory systems, 143 Paper pencil method, multiplication unsigned numbers, 68 Parallel computers, history of, 3268 INDEXPDP-8 minicomputer, history of, 3 Pentium processors real-life cache organization analysis, Pentium IV processor cache, 128–129 set-associative mapping, 121virtual memory management, 155 X86 family, assembly language programming for, 48–55 Performance measurement computer systems, 6–11 multiprocessor architectures, 254 pipelining design, 186–187 RISC vs. CISC, 222–223 Personal computer (PC), history of, 3Physical connection (PC), multiprocessor design, 236 Pipeline bubble (hazards), instruction pipeline design, 188 Pipelined multiplication, carry-save addition, 212–213 Pipeline interlock, Stanford microprocessor without interlock pipe stages (MIPS), 225–226 Pipeline stall data dependency, 189–192 NOP prevention method, 192–194reduction methods, 199–201 instruction dependency, 188 unconditional branch instructions, 194–196 instruction pipeline design, 187–188 Pipelining design Alpha 2164 pipeline, 227–230 arithmetic pipline, 209–213 ﬁxed-point arithmetic, 209–211 ﬂoating-point arithmetic, 211–212 multiplication carry-save addition, 212–213 example processors, 201–207 ARM 1026EJ-S processor, 202–203 UltraSPARC III processor, 203–207 general concepts, 185–187instruction-level parallelism, 207–209 superscalar architectures, 207–209very long instruction word (VLIW), 209 instruction pipeline, 187–201 data dependency "stall," 189–201 hardware operand forwarding, 199–200 software operand forwarding, 200–201 instruction dependency "stall," 188conditional branch instructions, 196–199 unconditional branch instructions, 194–196 wrong instruction /operand, prevention, 192–194 PMC-Sierra RM7000A 64-bit MIPS RISC processor, real-life cache organiz-ation, 130 Pop operations addressing modes, 19–20 central processing unit design, 86 Positional representation, number system, 59–63 PowerPC 604 processor cache, real-life cache organization analysis, 129 Precomputing branches, pipeline stall reduction, 195–196 Predictor training, Alpha 2164 pipeline, 228–230 Prefetch unit, 1026EJ-S processor pipeline design, 203 Primary memory, hierarchy parameters, 108–109 Processor control instructions, X86 family, 51–55 Program counter (PC) central processing unit design, 83–85 instruction register, 86 interrupt handling, 94–95 one-bus organization, 89 interrupt-driven /O systems, 169–170 relative addressing mode, 23 sequencing instructions, 28–30 Programmable ROM (PROM), basic properties, 157–158 Programmed input /output system, basic concepts, 164–167 Programming assembly language programming assembler directives commands, 43–44 basic principles, 37–38 instruction mnemonics syntax, 40–43 programing assembly execution, 44–47 assemblers, 45 data structures, 45–46 linker loader, 46–47 simple machines, 38–40 X86 family example, 47–55 instruction set architecture design, 31–33INDEX 269Program status word (PSW) register central processing unit design, 86 interrupt-driven /O systems, 169–170 Pseudo-operations, assembly language programming, assembler directives commands, 43–44 Push operations addressing modes, 19–20 central processing unit design, 86 Quaternary base, number system, 59–63 Radix complement, number systems, 62 Radix conversion algorithm, number systems, 60–61 Radix (radices), number systems, 59–63 RAM memory, integration technology and, 6Random access, memory hierarchy, 108–109 Random replacement algorithm, virtual memory systems, 148 Random selection, cache memory replace- ment, 121–124 Read-after-write data dependency, pipeline stall, 190–194 Read-only memory (ROM), basic properties, 156–158 Read operation cache miss, 127 main memory unit, 137–142 memory hierarchy, 108–109 memory locations operations, 16–18 Real-life cache organization analysis Pentium IV processor cache, 128–129 PMC-Sierra RM7000A 64-bit MIPS RISC processor, 130 PowerPC 604 processor cache, 129 Reduced instruction set computers (RISCs) advanced machines, 227–232 Alpha 21264 pipeline, 227–230 Compaq (formerly DEC) Alpha 21264, 227 SUN UltraSPARC III, 231–232 architecture examples of, 5 CISC vs., 221–223 design principles, 218–220 overlapped register windows, 220–221pioneer (university) machines, 223–226 Berkeley RISC, 223–224 Stanford MIPS, 224–226 RISC /CISC evolution cycle, 217–218 Register indirect addressing, 22 Register set, central processing unit design, 83–88condition registers, 86instruction fetching registers, 86 memory access registers, 85–86 MIPS registers, 87–88 special-purpose address registers, 86 80x86 registers, 87 Relative addressing mode, 23 Replacement algorithms, virtual memory, 148–152 clock replacement algorithm, 150–152 ﬁrst-in-ﬁrst-out (FIFO) replacement, 148–149 least recently used (LUR) replacement, 149–150 random replacement, 148 Replacement policy, 148 Replacement techniques, cache memory, 121–124 Representation (scientiﬁc notation), ﬂoating-point arithmetic, 74–75 Row address strobe (RAS), main memory unit, 141–142 runtime, Instruction set architecture and, 1 Scale integration, computer technology development and, 5–6 Scientiﬁc notation, ﬂoating-point arithmetic, 74–75 SEARCH algorithm, X86 family programming, 53–55 Seconardy memory, hierarchy parameters, 108–109 Segment address translation, virtual memory system, 153–154 Segmentation, virtual memory system, 153 Segment pointers, central processing unit design, 86 Semantic gap computer architecture and, 4RISC /CISC evolution cycle, 218 Sequencing instructions, computer architec- tures, 28–30 Sequential processing, pipelining vs., 186 Set-associative mapping cache memory organization, 118–121 replacement techniques, 123–124, 126 virtual memory system, 145–146 Set ﬁeld, cache memory organization, set-associative mapping, 118–121 Shared /O systems basic design, 163programmed input /output design, 165–167270 INDEXShared memory systems history of, 3 multiprocessor architecture, multiple- instruction multiple-data streams(MIMD), 247–249 Signed numbers, hardware structures addition subtraction, 64–67 Sign-magnitude, number system, 61Single-instruction multiple-data streams (SIMD), multiprocessor architecture basic features, 244–246 Flynn classiﬁcation, 238–240 Hwang /Briggs classiﬁcation scheme, 241 Single-instruction single-data streams (SISD), multiprocessor architecture,238–240 Hwang /Briggs classiﬁcation scheme, 240–241 Single-precision format, IEEE ﬂoating-point standard, 77–79 Skillicorn classiﬁcation, multiprocessor architecture, 242–244 Slave memory, deﬁned, 109 Small-scale integration (SSI), evolution of, 5–6 Software /O polling, programmed input / output design, 166–167 Software operand forwarding, data depen- dency pipeline reduction, 200–201 Solid-state memory, hierarchy parameters, 107–109 Source registers arithmetic logical instructions, 27–28 CPU instruction cycle, addition instruc- tions, 92–94 Space-time chart, pipelining design, 186 Spatial locality cache memory, 111 memory hierarchy, 108–109 Special purpose computer system, 1 Speculative execution, pipeline stall reduction, conditional branch instructions, 198–199 Speed-up (S( n)) measurements pipeline stall, data dependency, 192 pipelining design, 186–187 Speedup ( SU o), performance analysis, 10–11 Stack operation, addressing modes, 19–20 Stack point (SP) addressing modes, 19–20 central processing unit design, 86Stanford microprocessor without interlock pipe stages (MIPS), design principles,224–226 STARAN system, multiprocessor architecture, single-instruction multiple-data streams (SIMD), 245–246 Static branch prediction, pipeline stall reduction, conditional branch instructions, 198–199 Static CMOS technology, main memory units, 136–142 Static interconnections, multiprocessor interconnections, 253 Static scheduling, instruction-level parallelism, pipeline design, 207–209 Status bit, programmed input /output design, 166–167 Status ﬂags, X86 family, assembly language programming for, 49–55 Status registers, /O system design, 163 Store-fetch operation, data dependency pipeline reduction, 200–201 Store-store operation, data dependency pipeline reduction, 201 Subroutines, instruction set architecture design, 32–33 Subtraction, two’ complement (2’ s) representation, 64 Supercomputers, history of, 3Superscalar architectures (SPA), instruction- level parallelism, pipeline design, 207–209 Switching techniques, multiprocessor interconnection networks, 252–253 Synchronous buses input /output system design, 178 multiprocessor interconnection networks, 252 Syntax, assembly language programming, 40–43 Synthetic operations, assembly language programming, assembler directives commands, 44 System calls, assembly language programming, assembler directives commands, 44 Table look-aside buffer (TLB), virtual memory system, 146–148 cache memory with, 152–153set-associative mapping, 146–148INDEX 271Tag ﬁeld, cache memory organization direct mapping, 115–116 fully associative mapping, 116–118 set-associative mapping, 118–121 Technological development computing, evolution of, 5–6 Temporal locality cache memory, 111 memory hierarchy, 109 Tertiary memory, hierarchy parameters, 108–109 Three-address instruction, addressing modes, 18–19 Three-bus organization, CPU datapath, 90–91 Throughput U(n) measurement pipeline stall, data dependency, 192pipelining design, 186–187 Time-multiplexing, main memory unit, 141–142 Time units, instruction pipeline design, 187–188 Topology, multiprocessor interconnections, 253 Two-address instruction, addressing modes, 18–19 Two-bus organization, CPU datapath, 90 arithmetic operations, 93–94 Two’ complement (2’ s) representation, integer arithmetic, 63–74 UltraSPARC III RISC processor design principles, 231–232 pipeline design, 203–207 pipeline stall reduction, 199 Unconditional branch instructions, pipeline stall reductions, 194–196 Uniform memory access (UMA), multiprocessor architecture, multiple-instruction multiple-data streams (MIMD), 248–249 Unit time, pipelining design, 186 UNIVersal Automatic Computer (UNIVAC), history of, 3 Unsigned numbers, paper pencil multiplication, 68 Valid bit, cache memory organization, fully associative mapping, 118 VAX computer systems architecture of, 4 history of, 3 Vectored interrupt, interrupt-driven input / output (I /O) systems, 168Vertical microinstructions, central processing unit design, 101–104 large-scale integration (VLSI), evolution of, 5–6 long instruction word (VLIW) architecture, instruction-level parallel- ism, pipeline design, 207, 209 Virtual (logical) address, virtual memory systems, 143 Virtual memory, 142–155 associative mapping, 144–145 cache memory, 152–153 paged segmentation, 154–155 Pentium memory management, 155 replacement algorithms (policies), 148–152 clock replacement algorithm, 150–152 ﬁrst-in-ﬁrst-out (FIFO) replacement, 148–149 least recently used (LUR) replacement, 149–150 random replacement, 148 segment address translation, 153–154 segmentation, 153 set-associative mapping, 145–146 translation look-aside buffer (TLB), 146–148 Volatile memory, 156 Wafer-scale integration (WSI), evolution of, 5–6 Wide area networks (WAN), history of, 3 Word ﬁeld cache memory organization direct mapping, 115–116fully associative mapping, 116–118 set-associative mapping, 118–121 memory locations operations, 16–18 Wormhole routing, multiprocessor architecture, multiple-instructionmultiple-data streams (MIMD), 250–252 Write-after-write data dependency, pipeline stall, 189–192 Write-allocate scheme, cache misses, 127 Write-back policy cache hits, 125–127 cache miss, 127 Write-no-allocate policy, cache misses, 127 Write operation cache memory policies, 124–127main memory unit, 137–142memory hierarchy, 108–109272 INDEXmemory locations operations, 16–18 Write-through policy, cach hits, 125–127 X86 family, assembly language programming for, 47–55Z computers, historical background, 2 Zero-address instructions, addressing modes, 19–20INDEX 273