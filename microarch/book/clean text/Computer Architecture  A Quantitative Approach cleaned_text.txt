Computer Architecture Formulas 1.CPU time = Instruction count /H11003 Clock cycles per instruction /H11003 Clock cycle time 2. X n times faster Y: n = 3.Amdahl’s Law: Speedupoverall = = 4. 5. 6. 7.Availability = Mean time fail / (Mean time fail + Mean time repair) 8. Wafer yield accounts wafers bad need tested parameter called process-complexity factor, measure manufacturing difficulty. ranges 11.5 15.5 2011. 9.Means—arithmetic (AM), weighted arithmetic (WAM), geometric (GM): = WAM = GM = Timei execution time ith program total n workload, Weighti weighting ith program workload. 10.Average memory-access time = Hit time + Miss rate /H11003 Miss penalty 11.Misses per instruction = Miss rate /H11003 Memory access per instruction 12.Cache index size: 2index = Cache size /(Block size /H11003 Set associativity) 13.Power Utilization Effectiveness (PUE ) Warehouse Scale Computer = Rules Thumb 1.Amdahl/Case Rule: balanced computer system needs 1 MB main memory capacity 1 megabit per second I/O bandwidth per MIPS CPU performance. 2.90/10 Locality Rule: program executes 90% instructions 10% code. 3.Bandwidth Rule: Bandwidth grows least square mprovement latency. 4.2:1 Cache Rule: miss rate direct-mapped cache size N two-way set- associative cache size N/2. 5.Dependability Rule: Design single point failure. 6.Watt-Year Rule : fully burdened cost Watt per year Warehouse Scale Computer North America 2011, including cost amortizing power cooling infrastructure, $2.Execution timeYExecution timeX/ PerformanceXPerformanceY / = Execution timeold Execution timenew-------------------------------------------1 1F r c nenhanced –#)Fractionenhanced Speedupenhanced------------------------------------+-------------------------------------------------------------------------------------------- - Energydynamic 12/ Capacitive load Voltage2/H11003 /H11003 Powerdynamic 12/ Capacitive load/H11003 Voltage2Frequency switched/H11003 /H11003 Powerstatic Currentstatic Voltage/H11003 Die yield Wafer yield 1 1 Defects per unit area Die area /H11003 + ) (/N/H11003 = 1 n---T ei i1=n WeightiTimei/H11003 i1=n n Timei i1=n Total Facility Power Equipment Power------------------------------------------------- -( N NIn Praise Computer Architecture: Quantitative Approach Sixth Edition “Although important concepts architecture timeless, edition thoroughly updated latest technology developments, costs, examples,and references. Keeping pace recent developments open-sourced architec-ture, instruction set architecture used book updated use theRISC-V ISA. ” —from foreword Norman P. Jouppi, Google “Computer Architecture: Quantitative Approach classic that, like fine wine, keeps getting better. bought first copy finished undergraduatedegree remains one frequently referenced texts today. ” —James Hamilton, Amazon Web Service “Hennessy Patterson wrote first edition book graduate stu- dents built computers 50,000 transistors. Today, warehouse-size computerscontain many servers, consisting dozens independent processorsand billions transistors. evolution computer architecture rapidand relentless, Computer Architecture: Quantitative Approach kept pace, edition accurately explaining analyzing important emergingideas make field exciting. ” —James Larus, Microsoft Research “Another timely relevant update classic, also serving win- dow relentless exciting evolution computer architecture! newdiscussions edition slowing Moore's law implications forfuture systems must-reads computer architects practitionersworking broader systems. ” —Parthasarathy (Partha) Ranganathan, Google “I love ‘Quantitative Approach ’books written engineers, engineers. John Hennessy Dave Patterson show limits imposed bymathematics possibilities enabled materials science. teach real-world examples architects analyze, measure, compromise build working systems. sixth edition comes critical time: Moore ’s Law fading deep learning demands unprecedented compute cycles.The new chapter domain-specific architectures documents number prom-ising approaches prophesies rebirth computer architecture. Like thescholars European Renaissance, computer architects must understand ourown history, combine lessons history new techniquesto remake world. ” —Cliff Young, GoogleThis page intentionally left blankComputer Architecture Quantitative Approach Sixth EditionJohn L. Hennessy Professor Electrical Engineering Computer Science Stanford University, member faculty since 1977 was, 2000 2016, 10th President. currently serves Director Knight-Hennessy Fellow-ship, provides graduate fellowships potential future leaders. Hennessy Fellow ofthe IEEE ACM, member National Academy Engineering, National Acad-emy Science, American Philosophical Society, Fellow American Acad-emy Arts Sciences. Among many awards 2001 Eckert-Mauchly Award forhis contributions RISC technology, 2001 Seymour Cray Computer Engineering Award,and 2000 John von Neumann Award, shared David Patterson. also received 10 honorary doctorates. 1981, started MIPS project Stanford handful graduate students. completing project 1984, took leave university cofound MIPS Com- puter Systems, developed one first commercial RISC microprocessors. 2017, 5 billion MIPS microprocessors shipped devices ranging videogames palmtop computers laser printers network switches. Hennessy subse-quently led DASH (Director Architecture Shared Memory) project, prototypedthe first scalable cache coherent multiprocessor; many key ideas adoptedin modern multiprocessors. addition technical activities university responsibil-ities, continued work numerous start-ups, early-stage advisor andan investor. David A. Patterson became Distinguished Engineer Google 2016 40 years UC Berkeley professor. joined UC Berkeley immediately graduating UCLA. still spends day week Berkeley Emeritus Professor Computer Science. teaching honored Distinguished Teaching Award University ofCalifornia, Karlstrom Award ACM, Mulligan Education Medal Under-graduate Teaching Award IEEE. Patterson received IEEE Technical AchievementAward ACM Eckert-Mauchly Award contributions RISC, shared IEEEJohnson Information Storage Award contributions RAID. also shared IEEE Johnvon Neumann Medal C & C Prize John Hennessy. Like co-author, Patterson isa Fellow American Academy Arts Sciences, Computer History Museum, ACM, IEEE, elected National Academy Engineering, National Academy Sciences, Silicon Valley Engineering Hall Fame. served theInformation Technology Advisory Committee President United States, chairof CS division Berkeley EECS department, chair Computing ResearchAssociation, President ACM. record led Distinguished Service Awards fromACM, CRA, SIGARCH. currently Vice-Chair Board Directors RISC-VFoundation. Berkeley, Patterson led design implementation RISC I, likely first VLSI reduced instruction set computer, foundation commercial SPARC architec-ture. leader Redundant Arrays Inexpensive Disks (RAID) project, led dependable storage systems many companies. also involved Network Workstations (NOW) project, led cluster technology used Internet companiesand later cloud computing. current interests designing domain-specific archi-tectures machine learning, spreading word open RISC-V instruction set archi-tecture, helping UC Berkeley RISELab (Real-time Intelligent Secure Execution).Computer Architecture Quantitative Approach Sixth Edition John L. Hennessy Stanford University David A. Patterson University California, Berkeley Contributions Krste Asanovi /C19c University California, Berkeley Jason D. Bakos University South Carolina Robert P. Colwell R&E Colwell & Assoc. Inc. Abhishek Bhattacharjee Rutgers University Thomas M. Conte Georgia Tech Jos/C19e Duato Proemisa Diana Franklin University Chicago David Goldberg eBayNorman P. Jouppi Google Sheng Li Intel Labs Naveen Muralimanohar HP Labs Gregory D. Peterson University Tennessee Timothy M. Pinkston University Southern California Parthasarathy Ranganathan Google David A. Wood University Wisconsin –Madison Cliff Young Google Amr Zaky University Santa ClaraMorgan Kaufmann imprint Elsevier 50 Hampshire Street, 5th Floor, Cambridge, 02139, United States ©2019 Elsevier Inc. rights reserved. part publication may reproduced transmitted form means, electronic mechanical, including photocopying, recording, information storage retrieval system, without permission writingfrom publisher. Details seek permission, information Publisher ’s permissions policies arrangements organizations Copyright Clearance Center Copyright Licensing Agency, found website: www.elsevier.com/permissions . book individual contributions contained protected copyright Publisher (other may noted herein). Notices Knowledge best practice field constantly changing. new research experience broaden ourunderstanding, changes research methods, professional practices, medical treatment may become necessary. Practitioners researchers must always rely experience knowledge evaluating using information, methods, compounds, experiments described herein. using information methods mindful safety safety others, including parties professional responsibility. fullest extent law, neither Publisher authors, contributors, editors, assume liability injury and/or damage persons property matter products liability, negligence otherwise, fromany use operation methods, products, instructions, ideas contained material herein. Library Congress Cataloging-in-Publication Data catalog record book available Library Congress British Library Cataloguing-in-Publication Data catalogue record book available British Library ISBN: 978-0-12-811905-1 information Morgan Kaufmann publications visit website https://www.elsevier.com/books-and-journals Publisher: Katey Birtcher Acquisition Editor: Stephen Merken Developmental Editor: Nate McFadden Production Project Manager: Stalin Viswanathan Cover Designer: Christian J. Bilbow Typeset SPi Global, IndiaTo Andrea, Linda, four sonsThis page intentionally left blankForeword Norman P. Jouppi, Google Much improvement computer performance last 40 years provided computer architecture advancements leveraged Moore ’s Law Dennard scaling build larger parallel systems. Moore ’s Law observation maximum number transistors integratedcircuit doubles approximately every two years. Dennard scaling refers reduc-tion MOS supply voltage concert scaling feature sizes, astransistors get smaller, power density stays roughly constant. end ofDennard scaling decade ago, recent slowdown Moore ’s Law due combination physical limitations economic factors, sixth edition preeminent textbook field ’t timely. reasons. First, domain-specific architectures provide equivalent perfor- mance power benefits three historical generations Moore ’s Law Dennard scaling, provide better implementations thanmay ever possible future scaling general-purpose architectures. Andwith diverse application space computers today, many potentialareas architectural innovation domain-specific architectures. Second, high-quality implementations open-source architectures much lon- ger lifetime due slowdown Moore ’s Law. gives oppor- tunities continued optimization refinement, hence makes moreattractive. Third, slowing Moore ’s Law, different technology compo- nents scaling heterogeneously. Furthermore, new technologies as2.5D stacking, new nonvolatile memories, optical interconnects beendeveloped provide Moore ’s Law supply alone. use new technologies nonhomogeneous scaling effectively, fundamental design decisions need reexamined first principles. Hence important students, professors, practitioners industry skilled wide rangeof old new architectural techniques. told, believe mostexciting time computer architecture since industrial exploitation ofinstruction-level parallelism microprocessors 25 years ago. largest change edition addition new chapter domain- specific architectures. ’s long known customized domain-specific archi- tectures higher performance, lower power, require less silicon area general-purpose processor implementations. However general-purpose ixprocessors increasing single-threaded performance 40% per year (see Fig. 1.11), extra time market required develop custom architecture vs. using leading-edge standard microprocessor could cause custom architectureto lose much advantage. contrast, today single-core performance isimproving slowly, meaning benefits custom architectures willnot made obsolete general-purpose processors long time, ever.Chapter 7 covers several domain-specific architectures. Deep neural networks high computation requirements lower data precision requirements – combination benefit significantly custom architectures. Two example architectures implementations deep neural networks presented: one optimized inference second optimized training. Image processingis another example domain; also high computation demands benefitsfrom lower-precision data types. Furthermore, since often found mobiledevices, power savings custom architectures also valuable.Finally, nature reprogrammability, FPGA-based accelerators beused implement variety different domain-specific architectures singledevice. also benefit irregular applications frequently updated, like accelerating internet search. Although important concepts architecture timeless, edition thoroughly updated latest technology developments, costs, examples, andreferences. Keeping pace recent developments open-sourced architecture,the instruction set architecture used book updated use theRISC-V ISA. personal note, enjoying privilege working John grad- uate student, enjoying privilege working Dave Google. amazing duo!x■ForewordContents Foreword ix Preface xviiAcknowledgments xxv Chapter 1 Fundamentals Quantitative Design Analysis 1.1 Introduction 2 1.2 Classes Computers 6 1.3 Defining Computer Architecture 11 1.4 Trends Technology 18 1.5 Trends Power Energy Integrated Circuits 23 1.6 Trends Cost 29 1.7 Dependability 36 1.8 Measuring, Reporting, Summarizing Performance 39 1.9 Quantitative Principles Computer Design 48 1.10 Putting Together: Performance, Price, Power 55 1.11 Fallacies Pitfalls 58 1.12 Concluding Remarks 64 1.13 Historical Perspectives References 67 Case Studies Exercises Diana Franklin 67 Chapter 2 Memory Hierarchy Design 2.1 Introduction 78 2.2 Memory Technology Optimizations 84 2.3 Ten Advanced Optimizations Cache Performance 94 2.4 Virtual Memory Virtual Machines 118 2.5 Cross-Cutting Issues: Design Memory Hierarchies 126 2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 129 2.7 Fallacies Pitfalls 142 2.8 Concluding Remarks: Looking Ahead 146 2.9 Historical Perspectives References 148 xiCase Studies Exercises Norman P. Jouppi, Rajeev Balasubramonian, Naveen Muralimanohar, Sheng Li 148 Chapter 3 Instruction-Level Parallelism Exploitation 3.1 Instruction-Level Parallelism: Concepts Challenges 168 3.2 Basic Compiler Techniques Exposing ILP 176 3.3 Reducing Branch Costs Advanced Branch Prediction 182 3.4 Overcoming Data Hazards Dynamic Scheduling 191 3.5 Dynamic Scheduling: Examples Algorithm 201 3.6 Hardware-Based Speculation 208 3.7 Exploiting ILP Using Multiple Issue Static Scheduling 218 3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation 222 3.9 Advanced Techniques Instruction Delivery Speculation 228 3.10 Cross-Cutting Issues 240 3.11 Multithreading: Exploiting Thread-Level Parallelism Improve Uniprocessor Throughput 242 3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 247 3.13 Fallacies Pitfalls 258 3.14 Concluding Remarks: ’s Ahead? 264 3.15 Historical Perspective References 266 Case Studies Exercises Jason D. Bakos Robert P. Colwell 266 Chapter 4 Data-Level Parallelism Vector, SIMD, GPU Architectures 4.1 Introduction 282 4.2 Vector Architecture 283 4.3 SIMD Instruction Set Extensions Multimedia 304 4.4 Graphics Processing Units 310 4.5 Detecting Enhancing Loop-Level Parallelism 336 4.6 Cross-Cutting Issues 345 4.7 Putting Together: Embedded Versus Server GPUs Tesla Versus Core i7 346 4.8 Fallacies Pitfalls 353 4.9 Concluding Remarks 357 4.10 Historical Perspective References 357 Case Study Exercises Jason D. Bakos 357 Chapter 5 Thread-Level Parallelism 5.1 Introduction 368 5.2 Centralized Shared-Memory Architectures 377 5.3 Performance Symmetric Shared-Memory Multiprocessors 393xii ■Contents5.4 Distributed Shared-Memory Directory-Based Coherence 404 5.5 Synchronization: Basics 412 5.6 Models Memory Consistency: Introduction 417 5.7 Cross-Cutting Issues 422 5.8 Putting Together: Multicore Processors Performance 426 5.9 Fallacies Pitfalls 438 5.10 Future Multicore Scaling 442 5.11 Concluding Remarks 444 5.12 Historical Perspectives References 445 Case Studies Exercises Amr Zaky David A. Wood 446 Chapter 6 Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelism 6.1 Introduction 466 6.2 Programming Models Workloads Warehouse-Scale Computers 471 6.3 Computer Architecture Warehouse-Scale Computers 477 6.4 Efficiency Cost Warehouse-Scale Computers 482 6.5 Cloud Computing: Return Utility Computing 490 6.6 Cross-Cutting Issues 501 6.7 Putting Together: Google Warehouse-Scale Computer 503 6.8 Fallacies Pitfalls 514 6.9 Concluding Remarks 518 6.10 Historical Perspectives References 519 Case Studies Exercises Parthasarathy Ranganathan 519 Chapter 7 Domain-Specific Architectures 7.1 Introduction 540 7.2 Guidelines DSAs 543 7.3 Example Domain: Deep Neural Networks 544 7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator 557 7.5 Microsoft Catapult, Flexible Data Center Accelerator 567 7.6 Intel Crest, Data Center Accelerator Training 579 7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit 579 7.8 Cross-Cutting Issues 592 7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators 595 7.10 Fallacies Pitfalls 602 7.11 Concluding Remarks 604 7.12 Historical Perspectives References 606 Case Studies Exercises Cliff Young 606Contents ■xiiiAppendix Instruction Set Principles A.1 Introduction A-2 A.2 Classifying Instruction Set Architectures A-3 A.3 Memory Addressing A-7 A.4 Type Size Operands A-13 A.5 Operations Instruction Set A-15 A.6 Instructions Control Flow A-16 A.7 Encoding Instruction Set A-21 A.8 Cross-Cutting Issues: Role Compilers A-24 A.9 Putting Together: RISC-V Architecture A-33 A.10 Fallacies Pitfalls A-42 A.11 Concluding Remarks A-46 A.12 Historical Perspective References A-47 Exercises Gregory D. Peterson A-47 Appendix B Review Memory Hierarchy B.1 Introduction B-2 B.2 Cache Performance B-15 B.3 Six Basic Cache Optimizations B-22 B.4 Virtual Memory B-40 B.5 Protection Examples Virtual Memory B-49 B.6 Fallacies Pitfalls B-57 B.7 Concluding Remarks B-59 B.8 Historical Perspective References B-59 Exercises Amr Zaky B-60 Appendix C Pipelining: Basic Intermediate Concepts C.1 Introduction C-2 C.2 Major Hurdle Pipelining —Pipeline Hazards C-10 C.3 Pipelining Implemented? C-26 C.4 Makes Pipelining Hard Implement? C-37 C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations C-45 C.6 Putting Together: MIPS R4000 Pipeline C-55 C.7 Cross-Cutting Issues C-65 C.8 Fallacies Pitfalls C-70 C.9 Concluding Remarks C-71 C.10 Historical Perspective References C-71 Updated Exercises Diana Franklin C-71xiv ■ContentsOnline Appendices Appendix Storage Systems Appendix E Embedded Systems Thomas M. Conte Appendix F Interconnection Networks Timothy M. Pinkston Jos /C19e Duato Appendix G Vector Processors Depth Krste Asanovic Appendix H Hardware Software VLIW EPIC Appendix Large-Scale Multiprocessors Scientific Applications Appendix J Computer Arithmetic David Goldberg Appendix K Survey Instruction Set Architectures Appendix L Advanced Concepts Address Translation Abhishek Bhattacharjee Appendix Historical Perspectives References References R-1 Index I-1Contents ■xvThis page intentionally left blankPreface Wrote Book six editions book, goal describe basic principles underlying tomorrow ’s technological developments. excitement opportunities computer architecture abated, echo whatwe said field first edition: “It dreary science paper machines never work. No! ’s discipline keen intellectual interest, requiring balance marketplace forces cost-performance-power, leadingto glorious failures notable successes. ” primary objective writing first book change way people learn think computer architecture. feel goal still valid important. field changing daily must studied real examplesand measurements real computers, rather simply collection defini-tions designs never need realized. offer enthusiastic wel-come anyone came along us past, well arejoining us now. Either way, promise quantitative approach to, andanalysis of, real systems. earlier versions, strived produce new edition continue relevant professional engineers architects involved advanced computer architecture design courses. Like first edi-tion, edition sharp focus new platforms —personal mobile devices warehouse-scale computers —and new architectures —specifically, domain- specific architectures. much predecessors, edition aims demystifycomputer architecture emphasis cost-performance-energy trade-offsand good engineering design. believe field continued mature andmove toward rigorous quantitative foundation long-established scientific engineering disciplines. xviiThis Edition ending Moore ’s Law Dennard scaling profound effect computer architecture switch multicore. retain focus theextremes size computing, personal mobile devices (PMDs) cellphones tablets clients warehouse-scale computers offering cloudcomputing server. also maintain theme parallelism allits forms: data-level parallelism (DLP) Chapters 1and4,instruction-level par- allelism (ILP) inChapter 3 ,thread-level parallelism inChapter 5 , request- level parallelism (RLP) Chapter 6 . pervasive change edition switching MIPS RISC- V instruction set. suspect modern, modular, open instruction set maybecome significant force information technology industry. may becomeas important computer architecture Linux operating systems. newcomer edition Chapter 7 , introduces domain-specific architectures several concrete examples industry. before, first three appendices book give basics RISC-V instruction set, memory hierarchy, pipelining readers read book like Computer Organization Design . keep costs still sup- ply supplemental material interest readers, available online athttps://www.elsevier.com/books-and-journals/book-companion/9780128119051are nine appendices. pages appendices arein book! edition continues tradition using real-world examples demonstrate ideas, “Putting Together ”sections brand new. “Putting Together ”sections edition include pipeline organizations memory hier- archies ARM Cortex A8 processor, Intel core i7 processor, NVIDIA GTX-280 GTX-480 GPUs, one Google warehouse-scale computers. Topic Selection Organization before, taken conservative approach topic selection, aremany interesting ideas field reasonably covered treat- ment basic principles. steered away comprehensive survey every architecture reader might encounter. Instead, presentation focuses oncore concepts likely found new machine. key criterion remainsthat selecting ideas examined utilized successfully enoughto permit discussion quantitative terms. intent always focus material available equiv- alent form sources, continue emphasize advanced contentwherever possible. Indeed, several systems whose descriptions can- found literature. (Readers interested strictly basic introduc- tion computer architecture read Computer Organization Design: Hardware/Software Interface .)xviii ■PrefaceAn Overview Content Chapter 1 includes formulas energy, static power, dynamic power, integrated cir- cuit costs, reliability, availability. (These formulas also found frontinside cover.) hope topics used rest book.In addition classic quantitative principles computer design performancemeasurement, shows slowing performance improvement general-purposemicroprocessors, one inspiration domain-specific architectures. view instruction set architecture playing less role today 1990, moved material Appendix . uses RISC-V architecture. (For quick review, summary RISC-V ISA found back inside cover.) fans ISAs, Appendix K revised edition andcovers 8 RISC architectures (5 desktop server use 3 embedded use),the 80/C286, DEC VAX, IBM 360/370. move onto memory hierarchy Chapter 2 , since easy apply cost-performance-energy principles material, memory criticalresource rest chapters. past edition, Appendix B contains introductory review cache principles, available case need it. Chapter 2 discusses 10 advanced optimizations caches. chapter includes virtual machines, offer advantages protection, software management,and hardware management, play important role cloud computing. Inaddition covering SRAM DRAM technologies, chapter includes newmaterial Flash memory use stacked die packaging extend-ing memory hierarchy. PIAT examples ARM Cortex A8, isused PMDs, Intel Core i7, used servers. Chapter 3 covers exploitation instruction-level parallelism high- performance processors, including superscalar execution, branch prediction (including new tagged hybrid predictors), speculation, dynamic scheduling,and simultaneous multithreading. mentioned earlier, Appendix C review pipelining case need it. Chapter 3 also surveys limits ILP. Like Chapter 2 , PIAT examples ARM Cortex A8 Intel Core i7. third edition contained great deal Itanium VLIW, mate-rial Appendix H, indicating view architecture live upto earlier claims. increasing importance multimedia applications games video processing also increased importance architectures exploit datalevel parallelism. particular, rising interest computing using graph-ical processing units (GPUs), yet architects understand GPUs really work.We decided write new chapter large part unveil new style computerarchitecture. Chapter 4 starts introduction vector architectures, acts foundation build explanations multimedia SIMD instruc-tion set extensions GPUs. (Appendix G goes even depth vector architectures.) chapter introduces Roofline performance model uses compare Intel Core i7 NVIDIA GTX 280 GTX 480 GPUs.The chapter also describes Tegra 2 GPU PMDs.Preface ■xixChapter 5 describes multicore processors. explores symmetric distributed-memory architectures, examining organizational principles performance. primary additions chapter include comparison ofmulticore organizations, including organization multicore-multilevelcaches, multicore coherence schemes, on-chip multicore interconnect. Topicsin synchronization memory consistency models next. example theIntel Core i7. Readers interested depth interconnection networks shouldread Appendix F, interested larger scale multiprocessors scientificapplications read Appendix I. Chapter 6 describes warehouse-scale computers (WSCs). extensively revised based help engineers Google Amazon Web Services. Thischapter integrates details design, cost, performance WSCs archi-tects aware of. starts popular MapReduce programming modelbefore describing architecture physical implementation WSCs, includ-ing cost. costs allow us explain emergence cloud computing, wherebyit cheaper compute using WSCs cloud local datacenter.The PIAT example description Google WSC includes information published first time book. new Chapter 7 motivates need Domain-Specific Architectures (DSAs). draws guiding principles DSAs based four examples DSAs.Each DSA corresponds chips deployed commercial settings. Wealso explain expect renaissance computer architecture via DSAs giventhat single-thread performance general-purpose microprocessors stalled. brings us Appendices Athrough M. Appendix covers principles ISAs, including RISC-V, Appendix K describes 64-bit versions RISC V, ARM, MIPS, Power, SPARC multimedia extensions. also includes classic architectures (80x86, VAX, IBM 360/370) popular embed-ded instruction sets (Thumb-2, microMIPS, RISC V C). Appendix H related,in covers architectures compilers VLIW ISAs. mentioned earlier, Appendix B andAppendix C tutorials basic cach- ing pipelining concepts. Readers relatively new caching read Appen- dix B Chapter 2 , new pipelining read Appendix C Chapter 3 . Appendix D, “Storage Systems, ”has expanded discussion reliability availability, tutorial RAID description RAID 6 schemes, rarelyfound failure statistics real systems. continues provide introduction toqueuing theory I/O performance benchmarks. evaluate cost, perfor-mance, reliability real cluster: Internet Archive. “Putting Together ”example NetApp FAS6000 filer. Appendix E, Thomas M. Conte, consolidates embedded material one place. Appendix F, interconnection networks, revised Timothy M. Pinkston Jos /C19e Duato. Appendix G, written originally Krste Asanovi /C19c, includes description vector processors. think two appendices ofthe best material know topic.xx ■PrefaceAppendix H describes VLIW EPIC, architecture Itanium. Appendix describes parallel processing applications coherence protocols larger-scale, shared-memory multiprocessing. Appendix J, David Goldberg,describes computer arithmetic. Appendix L, Abhishek Bhattacharjee, new discusses advanced tech- niques memory management, focusing support virtual machines anddesign address translation large address spaces. growth inclouds processors, architectural enhancements becoming important. Appendix collects “Historical Perspective References ”from chapter single appendix. attempts give proper credit ideas chapter sense history surrounding inventions. like think ofthis presenting human drama computer design. also supplies referencesthat student architecture may want pursue. time, recom-mend reading classic papers field mentioned thesesections. enjoyable educational hear ideas directly thecreators. “Historical Perspective ”was one popular sections prior editions. Navigating Text single best order approach chapters appendices,except readers start Chapter 1 . ’t want read every- thing, suggested sequences: ■Memory Hierarchy: Appendix B ,Chapter 2 , Appendices M. ■Instruction-Level Parallelism: Appendix C ,Chapter 3 , Appendix H ■Data-Level Parallelism: Chapters 4,6, 7, Appendix G ■Thread-Level Parallelism: Chapter 5 , Appendices F ■Request-Level Parallelism: Chapter 6 ■ISA: Appendices K Appendix E read time, might work best read ISA cache sequences. Appendix J read whenever arithmetic moves you. read corresponding portion Appendix complete chapter. Chapter Structure material selected stretched upon consistent framework thatis followed chapter. start explaining ideas chapter. Theseideas followed “Crosscutting Issues ”section, feature shows ideas covered one chapter interact given chapters. isPreface ■xxifollowed “Putting Together ”section ties ideas together showing used real machine. Next sequence “Fallacies Pitfalls, ”which lets readers learn mistakes others. show examples common misunderstandings andarchitectural traps difficult avoid even know lyingin wait you. “Fallacies Pitfalls ”sections one popular sections book. chapter ends “Concluding Remarks ”section. Case Studies Exercises chapter ends case studies accompanying exercises. Authored experts industry academia, case studies explore key chapter concepts verify understanding increasingly challenging exercises. Instructors find case studies sufficiently detailed robust allow createtheir additional exercises. Brackets exercise ( <chapter.section >) indicate text sections primary relevance completing exercise. hope helps readers avoidexercises ’t read corresponding section, addition pro- viding source review. Exercises rated, give reader sense theamount time required complete exercise: [10] Less 5 min (to read understand) [15] 5 –15 min full answer [20] 15 –20 min full answer [25] 1 h full written answer[30] Short programming project: less 1 full day programming[40] Significant programming project: 2 weeks elapsed time[Discussion] Topic discussion others Solutions case studies exercises available instructors register textbooks.elsevier.com . Supplemental Materials variety resources available online https://www.elsevier.com/books/ computer-architecture/hennessy/978-0-12-811905-1 , including following: ■Reference appendices, guest authored subject experts, covering range advanced topics ■Historical perspectives material explores development key ideaspresented chapters textxxii ■Preface■Instructor slides PowerPoint ■Figures book PDF, EPS, PPT formats ■Links related material Web ■List errata New materials links resources available Web added regular basis. Helping Improve Book Finally, possible make money reading book. (Talk cost per- formance!) read Acknowledgments follow, see went great lengths correct mistakes. Since book goes many print-ings, opportunity make even corrections. uncover anyremaining resilient bugs, please contact publisher electronic mail(ca6bugs@mkp.com ). welcome general comments text invite send separate email address ca6comments @mkp.com . Concluding Remarks again, book true co-authorship, us writing half chap- ters equal share appendices. ’t imagine long would taken without someone else half work, offering inspiration taskseemed hopeless, providing key insight explain difficult concept, supply-ing over-the-weekend reviews chapters, commiserating weight ofour obligations made hard pick pen. Thus, again, share equally blame read. John Hennessy ■David PattersonPreface ■xxiiiThis page intentionally left blankAcknowledgments Although sixth edition book, actually created ten different versions text: three versions first edition (alpha, beta, final) two versions second, third, fourth editions (beta final). Along way, received help hundreds reviewers users. Eachof people helped make book better. Thus, chosen list ofthe people made contributions version book. Contributors Sixth Edition Like prior editions, community effort involves scores volunteers.Without help, edition would nearly polished. Reviewers Jason D. Bakos, University South Carolina; Rajeev Balasubramonian, Univer-sity Utah; Jose Delgado-Frias, Washington State University; Diana Franklin,The University Chicago; Norman P. Jouppi, Google; Hugh C. Lauer, WorcesterPolytechnic Institute; Gregory Peterson, University Tennessee; Bill Pierce, Hood College; Parthasarathy Ranganathan, Google; William H. Robinson, Van- derbilt University; Pat Stakem, Johns Hopkins University; Cliff Young, Google;Amr Zaky, University Santa Clara; Gerald Zarnett, Ryerson University;Huiyang Zhou, North Carolina State University. Members University California-Berkeley Par Lab RAD Lab gave frequent reviews Chapters 1,4, 6and shaped explanation GPUs WSCs: Krste Asanovi /C19c, Michael Armbrust, Scott Beamer, Sarah Bird, Bryan Catan- zaro, Jike Chong, Henry Cook, Derrick Coetzee, Randy Katz, Yunsup Lee, LeoMeyervich, Mark Murphy, Zhangxi Tan, Vasily Volkov, Andrew Waterman. Appendices Krste Asanovi /C19c, University California, Berkeley (Appendix G); Abhishek Bhattacharjee, Rutgers University (Appendix L); Thomas M. Conte, North Caro-lina State University (Appendix E); Jos /C19e Duato, Universitat Politècnica de xxvValència Simula (Appendix F); David Goldberg, Xerox PARC (Appendix J); Timothy M. Pinkston, University Southern California (Appendix F). Jos/C19e Flich Universidad Polit /C19ecnica de Valencia provided significant contri- butions updating Appendix F. Case Studies Exercises Jason D. Bakos, University South Carolina (Chapters 3and4); Rajeev Balasu- bramonian, University Utah (Chapter 2); Diana Franklin, University Chicago (Chapter 1 Appendix C); Norman P. Jouppi, Google, (Chapter 2); Naveen Muralimanohar, HP Labs (Chapter 2); Gregory Peterson, University ofTennessee (Appendix A); Parthasarathy Ranganathan, Google (Chapter 6); CliffYoung, Google (Chapter 7); Amr Zaky, University Santa Clara (Chapter 5andAppendix B). Jichuan Chang, Junwhan Ahn, Rama Govindaraju, Milad Hashemi assisted development testing case studies exercises Chapter 6. Additional Material John Nickolls, Steve Keckler, Michael Toksvig NVIDIA (Chapter 4 NVI- DIA GPUs); Victor Lee, Intel (Chapter 4 comparison Core i7 GPU); John Shalf, LBNL (Chapter 4 recent vector architectures); Sam Williams, LBNL (Roof- line model computers Chapter 4); Steve Blackburn Australian National University Kathryn McKinley University Texas Austin (Intel perfor- mance power measurements Chapter 5); Luiz Barroso, Urs H €olzle, Jimmy Clidaris, Bob Felderman, Chris Johnson Google (the Google WSC Chapter 6); James Hamilton Amazon Web Services (power distribution costmodel Chapter 6). Jason D. Bakos University South Carolina updated lecture slides edition. book could published without publisher, course. wish thank Morgan Kaufmann/Elsevier staff efforts support.For fifth edition, particularly want thank editors Nate McFadden andSteve Merken, coordinated surveys, development case studies exer-cises, manuscript reviews, updating appendices. must also thank university staff, Margaret Rowland Roxana Infante, countless express mailings, well holding fort Stan- ford Berkeley worked book. final thanks go wives suffering increasingly early mornings reading, thinking, writing.xxvi ■AcknowledgmentsContributors Previous Editions Reviewers George Adams, Purdue University; Sarita Adve, University Illinois Urbana- Champaign; Jim Archibald, Brigham Young University; Krste Asanovi /C19c, Massa- chusetts Institute Technology; Jean-Loup Baer, University Washington; PaulBarr, Northeastern University; Rajendra V. Boppana, University Texas, San Antonio; Mark Brehob, University Michigan; Doug Burger, University Texas, Austin; John Burger, SGI; Michael Butler; Thomas Casavant; Rohit Chan-dra; Peter Chen, University Michigan; classes SUNY Stony Brook, Car-negie Mellon, Stanford, Clemson, Wisconsin; Tim Coe, VitesseSemiconductor; Robert P. Colwell; David Cummings; Bill Dally; David Douglas;Jos/C19e Duato, Universitat Politècnica de València Simula; Anthony Duben, Southeast Missouri State University; Susan Eggers, University Washington;Joel Emer; Barry Fagin, Dartmouth; Joel Ferguson, University California, Santa Cruz; Carl Feynman; David Filo; Josh Fisher, Hewlett-Packard Laboratories; Rob Fowler, DIKU; Mark Franklin, Washington University (St. Louis); Kourosh Ghar-achorloo; Nikolas Gloy, Harvard University; David Goldberg, Xerox Palo AltoResearch Center; Antonio González, Intel Universitat Politècnica de Catalu-nya; James Goodman, University Wisconsin-Madison; Sudhanva Gurumurthi,University Virginia; David Harris, Harvey Mudd College; John Heinlein; MarkHeinrich, Stanford; Daniel Helman, University California, Santa Cruz; Mark D.Hill, University Wisconsin-Madison; Martin Hopkins, IBM; Jerry Huck, Hewlett-Packard Laboratories; Wen-mei Hwu, University Illinois Urbana- Champaign; Mary Jane Irwin, Pennsylvania State University; Truman Joe; NormJouppi; David Kaeli, Northeastern University; Roger Kieckhafer, University ofNebraska; Lev G. Kirischian, Ryerson University; Earl Killian; Allan Knies, Pur-due University; Knuth; Jeff Kuskin, Stanford; James R. Larus, MicrosoftResearch; Corinna Lee, University Toronto; Hank Levy; Kai Li, Princeton Uni-versity; Lori Liebrock, University Alaska, Fairbanks; Mikko Lipasti, Universityof Wisconsin-Madison; Gyula A. Mago, University North Carolina, Chapel Hill; Bryan Martin; Norman Matloff; David Meyer; William Michalson, Worcester Polytechnic Institute; James Mooney; Trevor Mudge, University Michigan;Ramadass Nagarajan, University Texas Austin; David Nagle, Carnegie Mel-lon University; Todd Narter; Victor Nelson; Vojin Oklobdzija, University Cal-ifornia, Berkeley; Kunle Olukotun, Stanford University; Bob Owens,Pennsylvania State University; Greg Papadapoulous, Sun Microsystems; JosephPfeiffer; Keshav Pingali, Cornell University; Timothy M. Pinkston, Universityof Southern California; Bruno Preiss, University Waterloo; Steven Przybylski; Jim Quinlan; Andras Radics; Kishore Ramachandran, Georgia Institute Tech- nology; Joseph Rameh, University Texas, Austin; Anthony Reeves, CornellUniversity; Richard Reid, Michigan State University; Steve Reinhardt, Universityof Michigan; David Rennels, University California, Los Angeles; Arnold L.Rosenberg, University Massachusetts, Amherst; Kaushik Roy, PurdueAcknowledgments ■xxviiUniversity; Emilio Salgueiro, Unysis; Karthikeyan Sankaralingam, University Texas Austin; Peter Schnorf; Margo Seltzer; Behrooz Shirazi, Southern Meth- odist University; Daniel Siewiorek, Carnegie Mellon University; J. P. Singh, Prin-ceton; Ashok Singhal; Jim Smith, University Wisconsin-Madison; Mike Smith,Harvard University; Mark Smotherman, Clemson University; Gurindar Sohi, Uni-versity Wisconsin-Madison; Arun Somani, University Washington; GeneTagliarin, Clemson University; Shyamkumar Thoziyoor, University NotreDame; Evan Tick, University Oregon; Akhilesh Tyagi, University North Car-olina, Chapel Hill; Dan Upton, University Virginia; Mateo Valero, Universidad Polit/C19ecnica de Cataluña, Barcelona; Anujan Varma, University California, Santa Cruz; Thorsten von Eicken, Cornell University; Hank Walker, Texas A&M; RoyWant, Xerox Palo Alto Research Center; David Weaver, Sun Microsystems;Shlomo Weiss, Tel Aviv University; David Wells; Mike Westall, Clemson Univer-sity; Maurice Wilkes; Eric Williams; Thomas Willis, Purdue University; MalcolmWing; Larry Wittie, SUNY Stony Brook; Ellen Witte Zegura, Georgia Institute ofTechnology; Sotirios G. Ziavras, New Jersey Institute Technology. Appendices vector appendix revised Krste Asanovi /C19c Massachusetts Institute Technology. floating-point appendix written originally David Gold-berg Xerox PARC. Exercises George Adams, Purdue University; Todd M. Bezenek, University Wisconsin-Madison (in remembrance grandmother Ethel Eshom); Susan Eggers;Anoop Gupta; David Hayes; Mark Hill; Allan Knies; Ethan L. Miller, Universityof California, Santa Cruz; Parthasarathy Ranganathan, Compaq Western ResearchLaboratory; Brandon Schwartz, University Wisconsin-Madison; Michael Scott;Dan Siewiorek; Mike Smith; Mark Smotherman; Evan Tick; Thomas Willis. Case Studies Exercises Andrea C. Arpaci-Dusseau, University Wisconsin-Madison; Remzi H. Arpaci-Dusseau, University Wisconsin-Madison; Robert P. Colwell, R&E Colwell &Assoc., Inc.; Diana Franklin, California Polytechnic State University, San LuisObispo; Wen-mei W. Hwu, University Illinois Urbana-Champaign; NormanP. Jouppi, HP Labs; John W. Sias, University Illinois Urbana-Champaign;David A. Wood, University Wisconsin-Madison. Special Thanks Duane Adams, Defense Advanced Research Projects Agency; Tom Adams; SaritaAdve, University Illinois Urbana-Champaign; Anant Agarwal; Davexxviii ■AcknowledgmentsAlbonesi, University Rochester; Mitch Alsup; Howard Alt; Dave Anderson; Peter Ashenden; David Bailey; Bill Bandy, Defense Advanced Research Projects Agency; Luiz Barroso, Compaq ’s Western Research Lab; Andy Bechtolsheim; C. Gordon Bell; Fred Berkowitz; John Best, IBM; Dileep Bhandarkar; Jeff Bier, BDTI; Mark Birman; David Black; David Boggs; Jim Brady; Forrest Brewer;Aaron Brown, University California, Berkeley; E. Bugnion, Compaq ’s Western Research Lab; Alper Buyuktosunoglu, University Rochester; Mark Callaghan;Jason F. Cantin; Paul Carrick; Chen-Chung Chang; Lei Chen, University Roch-ester; Pete Chen; Nhan Chu; Doug Clark, Princeton University; Bob Cmelik; John Crawford; Zarka Cvetanovic; Mike Dahlin, University Texas, Austin; Merrick Darley; staff DEC Western Research Laboratory; John DeRosa; LloydDickman; J. Ding; Susan Eggers, University Washington; Wael El-Essawy,University Rochester; Patty Enriquez, Mills; Milos Ercegovac; Robert Garner;K. Gharachorloo, Compaq ’s Western Research Lab; Garth Gibson; Ronald Green- berg; Ben Hao; John Henning, Compaq; Mark Hill, University Wisconsin-Madison; Danny Hillis; David Hodges; Urs H €olzle, Google; David Hough; Ed Hudson; Chris Hughes, University Illinois Urbana-Champaign; Mark John- son; Lewis Jordan; Norm Jouppi; William Kahan; Randy Katz; Ed Kelly; Richard Kessler; Les Kohn; John Kowaleski, Compaq Computer Corp; Dan Lambright;Gary Lauterbach, Sun Microsystems; Corinna Lee; Ruby Lee; Lewine;Chao-Huang Lin; Paul Losleben, Defense Advanced Research Projects Agency;Yung-Hsiang Lu; Bob Lucas, Defense Advanced Research Projects Agency;Ken Lutz; Alan Mainwaring, Intel Berkeley Research Labs; Al Marston; RichMartin, Rutgers; John Mashey; Luke McDowell; Sebastian Mirolo, Trimedia Cor-poration; Ravi Murthy; Biswadeep Nag; Lisa Noordergraaf, Sun Microsystems; Bob Parker, Defense Advanced Research Projects Agency; Vern Paxson, Center Internet Research; Lawrence Prince; Steven Przybylski; Mark Pullen, DefenseAdvanced Research Projects Agency; Chris Rowen; Margaret Rowland; GregSemeraro, University Rochester; Bill Shannon; Behrooz Shirazi; Robert Shom-ler; Jim Slager; Mark Smotherman, Clemson University; SMT research groupat University Washington; Steve Squires, Defense Advanced Research Pro-jects Agency; Ajay Sreekanth; Darren Staples; Charles Stapper; Jorge Stolfi; PeterStoll; students Stanford Berkeley endured first attempts cre- ating book; Bob Supnik; Steve Swanson; Paul Taysom; Shreekant Thakkar; Alexander Thomasian, New Jersey Institute Technology; John Toole, DefenseAdvanced Research Projects Agency; Kees A. Vissers, Trimedia Corporation;Willa Walker; David Weaver; Ric Wheeler, EMC; Maurice Wilkes; RichardZimmerman. John Hennessy ■David PattersonAcknowledgments ■xxix1.1 Introduction 2 1.2 Classes Computers 6 1.3 Defining Computer Architecture 11 1.4 Trends Technology 18 1.5 Trends Power Energy Integrated Circuits 23 1.6 Trends Cost 29 1.7 Dependability 36 1.8 Measuring, Reporting, Summarizing Performance 39 1.9 Quantitative Principles Computer Design 48 1.10 Putting Together: Performance, Price, Power 55 1.11 Fallacies Pitfalls 58 1.12 Concluding Remarks 64 1.13 Historical Perspectives References 67 Case Studies Exercises Diana Franklin 671 Fundamentals Quantitative Design Analysis iPod, phone, Internet mobile communicator …these three separate devices! calling iPhone! Today Apple going reinvent phone. is. Steve Jobs, January 9, 2007 New information communications technologies, particularhigh-speed Internet, changing way companies business, transforming public service delivery democratizing innovation. 10 percent increase high speed Internet connections, economic growth increases 1.3 percent. World Bank, July 28, 2009 Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00001-8 ©2019 Elsevier Inc. rights reserved.1.1 Introduction Computer technology made incredible progress roughly 70 years since first general-purpose electronic computer created. Today, less $500 purchase cell phone much performance world ’s fastest computer bought 1993 $50 million. rapid improvement come advances technology used build computers innovations incomputer design. Although technological improvements historically fairly steady, progress arising better computer architectures much less consistent.During first 25 years electronic computers, forces made major con-tribution, delivering performance improvement 25% per year. late1970s saw emergence microprocessor. ability microprocessorto ride improvements integrated circuit technology led higher rate ofperformance improvement —roughly 35% growth per year. growth rate, combined cost advantages mass-produced microprocessor, led increasing fraction computer business based microprocessors. addition, two significant changes computer market-place made easier ever succeed commercially new archi-tecture. First, virtual elimination assembly language programming reducedthe need object-code compatibility. Second, creation standardized,vendor-independent operating systems, UNIX clone, Linux, low-ered cost risk bringing new architecture. changes made possible develop successfully new set architec- tures simpler instructions, called RISC (Reduced Instruction Set Computer) architectures, early 1980s. RISC-based machines focused attentionof designers two critical performance techniques, exploitation instruc- tion-level parallelism (initially pipelining later multiple instruction issue) use caches (initially simple forms later usingmore sophisticated organizations optimizations). RISC-based computers raised performance bar, forcing prior architec- tures keep disappear. Digital Equipment Vax could not, replaced RISC architecture. Intel rose challenge, primarily translat- ing 80x86 instructions RISC-like instructions internally, allowing adoptmany innovations first pioneered RISC designs. transistor countssoared late 1990s, hardware overhead translating complexx86 architecture became negligible. low-end applications, cell phones,the cost power silicon area x86-translation overhead helped lead aRISC architecture, ARM, becoming dominant. Figure 1.1 shows combination architectural organizational enhancements led 17 years sustained growth performance annual rate 50% —a rate unprecedented computer industry. effect dramatic growth rate 20th century fourfold. First, significantly enhanced capability available computer users. Formany applications, highest-performance microprocessors outperformed thesupercomputer less 20 years earlier.2■Chapter One Fundamentals Quantitative Design Analysis15913182451801171832804816499931,2671,7793,0164,1956,0436,681 7,10811,86514,387 19,48421,871 24,12931,999 34,96739,419 40,96749,935 49,93549,870 110100100010,000100,000 1978 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 2008 2010 2012 2014 2016 2018Performance (vs. VAX-11/780) 25%/year52%/year23%/year 12%/year 3.5%/year IBM POWERstation 100, 150 MHzDigital Alphastation 4/266, 266 MHzDigital Alphastation 5/300, 300 MHzDigital Alphastation 5/500, 500 MHz AlphaServer 4000 5/600, 600 MHz 21164Digital AlphaServer 8400 6/575, 575 MHz 21264Professional Workstation XP1000, 667 MHz 21264AIntel VC820 motherboard, 1.0 GHz Pentium III processor IBM Power4, 1.3 GHz Intel Xeon EE 3.2 GHz AMD Athlon, 2.6 GHz Intel Core 2 Extreme 2 cores, 2.9 GHz Intel Core Duo Extreme 2 cores, 3.0 GHz Intel Core i7 Extreme 4 cores 3.2 GHz (boost 3.5 GHz) Intel Core i7 4 cores 3.4 GHz (boost 3.8 GHz)Intel Xeon 4 cores 3.6 GHz (Boost 4.0 GHz)Intel Xeon 4 cores 3.6 GHz (Boost 4.0 GHz)Intel Xeon 4 cores 3.7 GHz (Boost 4.1 GHz)Intel Core i7 4 cores 4.0 GHz (Boost 4.2 GHz)Intel Core i7 4 cores 4.0 GHz (Boost 4.2 GHz)Intel Core i7 4 cores 4.2 GHz (Boost 4.5 GHz) Intel Xeon 4 cores, 3.3 GHz (boost 3.6 GHz) Intel Xeon 6 cores, 3.3 GHz (boost 3.6 GHz) Intel D850EMVR motherboard (3.06 GHz, Pentium 4 processor Hyper-Threading Technology) AMD Athlon 64, 2.8 GHz Digital 3000 AXP/500, 150 MHz HP 9000/750, 66 MHz IBM RS6000/540, 30 MHz MIPS M2000, 25 MHz MIPS M/120, 16.7 MHz Sun-4/260, 16.7 MHz VAX 8700, 22 MHz AX-11/780, 5 MHz Figure 1.1 Growth processor performance 40 years. chart plots program performance relative VAX 11/780 measured SPEC integer benchmarks (see Section 1.8 ). Prior mid-1980s, growth processor performance largely technology-driven averaged 22% per year, doubling performance every 3.5 years. increase growth 52% starting 1986, doubling every 2 years, attributable advanced architectural organizational ideas typified RISC architectures. 2003 growth led dif-ference performance approximate factor 25 versus performance would occurred continued 22% rate. In2003 limits power due end Dennard scaling available instruction-level parallelism slowed uniprocessor performance 23% per year 2011, doubling every 3.5 years. (The fastest SPECintbase performance since 2007 automatic parallelization turned on, uniprocessor speed harder gauge. results limited single-chip systems usually four cores per chip.) 2011 2015,the annual improvement less 12%, doubling every 8 years part due limits parallelism Amdahl ’s Law. Since 2015, end Moore ’s Law, improvement 3.5% per year, doubling every 20 years! Performance floating-point-oriented calculations follows trends, typically 1% 2% higher annual growth shaded region. Figure 1.11 page 27 shows improvement clock rates eras. SPEC changed years, performance newer machines estimated scaling factor relates performance different versions SPEC: SPEC89, SPEC92, SPEC95, SPEC2000, SPEC2006. results SPEC2017 plot yet.1.1 Introduction ■3Second, dramatic improvement cost-performance led new classes computers. Personal computers workstations emerged 1980s availability microprocessor. past decade saw rise smart cellphones tablet computers, many people using primary com-puting platforms instead PCs. mobile client devices increasingly usingthe Internet access warehouses containing 100,000 servers, beingdesigned single gigantic computer. Third, improvement semiconductor manufacturing predicted Moore ’s law led dominance microprocessor-based computers across entire range computer design. Minicomputers, traditionally made off-the-shelf logic gate arrays, replaced servers made usingmicroprocessors. Even mainframe computers high-performance supercom-puters collections microprocessors. preceding hardware innovations led renaissance computer design, emphasized architectural innovation efficient use technologyimprovements. rate growth compounded 2003, high-performance microprocessors 7.5 times fast would obtained relying solely technology, including improved circuit design, is, 52% per year versus 35% per year. hardware renaissance led fourth impact, software development. 50,000-fold performance improvement since 1978 (seeFigure 1.1 ) allowed modern programmers trade performance productivity. place performance-oriented languages like C C++, much program-ming today done managed programming languages like Java Scala. More-over, scripting languages like JavaScript Python, even productive, gaining popularity along programming frameworks like AngularJS Django. maintain productivity try close performancegap, interpreters just-in-time compilers trace-based compiling repla-cing traditional compiler linker past. Software deployment chang-ing well, Software Service (SaaS) used Internet replacingshrink-wrapped software must installed run local computer. nature applications also changing. Speech, sound, images, video becoming increasingly important, along predictable response time critical user experience. inspiring example Google Translate. application lets hold cell phone point camera object, theimage sent wirelessly Internet warehouse-scale computer (WSC)that recognizes text photo translates native language.You also speak it, translate said audio outputin another language. translates text 90 languages voice 15 languages. Alas, Figure 1.1 also shows 17-year hardware renaissance over. fundamental reason two characteristics semiconductor processes true decades longer hold. 1974 Robert Dennard observed power density constant given area silicon even increased number transistors smallerdimensions transistor. Remarkably, transistors could go faster use less4■Chapter One Fundamentals Quantitative Design Analysispower. Dennard scaling ended around 2004 current voltage ’t keep dropping still maintain dependability integrated circuits. change forced microprocessor industry use multiple efficient pro- cessors cores instead single inefficient processor. Indeed, 2004 Intel can-celed high-performance uniprocessor projects joined others declaringthat road higher performance would via multiple processors per chiprather via faster uniprocessors. milestone signaled historic switch fromrelying solely instruction-level parallelism (ILP), primary focus firstthree editions book, data-level parallelism (DLP) thread-level par- allelism (TLP), featured fourth edition expanded fifth edition. fifth edition also added WSCs request-level parallelism (RLP), expanded edition. Whereas compiler hardware conspireto exploit ILP implicitly without programmer ’s attention, DLP, TLP, RLP explicitly parallel, requiring restructuring application thatit exploit explicit parallelism. instances, easy; many, isa major new burden programmers. Amdahl ’sL w (Section 1.9 ) prescribes practical limits number useful cores per chip. 10% task serial, maximum performance benefit parallelism 10 matter many cores put chip. second observation ended recently Moore ’s Law. 1965 Gordon Moore famously predicted number transistors per chip would doubleevery year, amended 1975 every two years. prediction lastedfor 50 years, longer holds. example, 2010 edition thisbook, recent Intel microprocessor 1,170,000,000 transistors. IfMoore ’s Law continued, could expected microprocessors 2016 18,720,000,000 transistors. Instead, equivalent Intel microprocessor 1,750,000,000 transistors, factor 10 Moore ’s Law would predicted. combination ■transistors longer getting much better slowing Moore ’s Law end Dinnard scaling, ■the unchanging power budgets microprocessors, ■the replacement single power-hungry processor several energy-efficient processors, ■the limits multiprocessing achieve Amdahl ’s Law caused improvements processor performance slow down, is, double every 20 years, rather every 1.5 years 1986 2003 (seeFigure 1.1 ). path left improve energy-performance-cost specialization. Future microprocessors include several domain-specific cores perform oneclass computations well, remarkably better general-purposecores. new Chapter 7 edition introduces domain-specific architectures .1.1 Introduction ■5This text architectural ideas accompanying compiler improve- ments made incredible growth rate possible past century, rea- sons dramatic change, challenges initial promising approachesto architectural ideas, compilers, interpreters 21st century. core isa quantitative approach computer design analysis uses empirical obser-vations programs, experimentation, simulation tools. style andapproach computer design reflected text. purpose chap-ter lay quantitative foundation following chapters appen-dices based. book written explain design style also stimulate contribute progress. believe approach serve computersof future worked implicitly parallel computers past. 1.2 Classes Computers changes set stage dramatic change view computing,computing applications, computer markets new century. sincethe creation personal computer seen striking changes waycomputers appear used. changes computer use ledto five diverse computing markets, characterized different applications,requirements, computing technologies. Figure 1.2 summarizes main- stream classes computing environments important characteristics. Internet Things/Embedded Computers Embedded computers found everyday machines: microwaves, washingmachines, printers, networking switches, automobiles. phrase FeaturePersonal mobile device (PMD)Desktop ServerClusters/warehouse- scale computerInternet things/ embedded Price system $100–$1000 $300–$2500 $5000 –$10,000,000 $100,000 –$200,000,000 $10–$100,000 Price microprocessor$10–$100 $50–$500 $200–$2000 $50–$250 $0.01–$100 Critical system design issuesCost, energy, mediaperformance,responsivenessPrice- performance,energy, graphicsperformanceThroughput, availability,scalability, energyPrice-performance, throughput, energyproportionalityPrice, energy, application-specificperformance Figure 1.2 summary five mainstream computing classes system characteristics. Sales 2015 included 1.6 billion PMDs (90% cell phones), 275 million desktop PCs, 15 million servers. total numberof embedded processors sold nearly 19 billion. total, 14.8 billion ARM-technology-based chips shipped 2015. Note wide range system price servers embedded systems, go USB keys network routers. servers, range arises need large-scale multiprocessor systems high-end trans-action processing.6■Chapter One Fundamentals Quantitative Design AnalysisInternet Things (IoT) refers embedded computers connected Internet, typically wirelessly. augmented sensors actuators, IoT devices collect useful data interact physical world, leading widevariety “smart ”applications, smart watches, smart thermostats, smart speakers, smart cars, smart homes, smart grids, smart cities. Embedded computers widest spread processing power cost. include 8-bit 32-bit processors may cost one penny, high-end64-bit processors cars network switches cost $100. Although range computing power embedded computing market large, price key factor design computers space. Performance requirements exist, course, primary goal often meets performance need minimumprice, rather achieving performance higher price. projectionsfor number IoT devices 2020 range 20 50 billion. book applies design, use, performance embedded processors, whether off-the-shelf microprocessors microprocessorcores assembled special-purpose hardware. Unfortunately, data drive quantitative design evaluation classes computers yet extended successfully embedded computing (see challenges EEMBC, example, Section 1.8 ). Hence left qualitative descriptions, fit well restof book. result, embedded material concentrated Appendix E. Webelieve separate appendix improves flow ideas text allowingreaders see differing requirements affect embedded computing. Personal Mobile Device Personal mobile device (PMD) term apply collection wireless devices multimedia user interfaces cell phones, tablet computers, on. Cost prime concern given consumer price wholeproduct hundred dollars. Although emphasis energy efficiencyis frequently driven use batteries, need use less expensive packag-ing—plastic versus ceramic —and absence fan cooling also limit total power consumption. examine issue energy power detailinSection 1.5 . Applications PMDs often web-based media-oriented, like previously mentioned Google Translate example. Energy size requirements lead use Flash memory storage ( Chapter 2 ) instead magnetic disks. processors PMD often considered embedded computers, keeping separate category PMDs platforms canrun externally developed software, share many characteristics ofdesktop computers. embedded devices limited hardware andsoftware sophistication. use ability run third-party software divid-ing line nonembedded embedded computers. Responsiveness predictability key characteristics media applica- tions. real-time performance requirement means segment application absolute maximum execution time. example, playing video a1.2 Classes Computers ■7PMD, time process video frame limited, since processor must accept process next frame shortly. applications, nuanced requirement exists: average time particular task constrained well asthe number instances maximum time exceeded. Suchapproaches —sometimes called soft real-time —arise possible miss time constraint event occasionally, long many missed.Real-time performance tends highly application-dependent. key characteristics many PMD applications need minimize memory need use energy efficiently. Energy efficiency driven battery power heat dissipation. memory substantial portion system cost, important optimize memory size cases. impor-tance memory size translates emphasis code size, since data size dic-tated application. Desktop Computing first, possibly still largest market dollar terms, desktop computing. Desktop computing spans low-end netbooks sell $300 high- end, heavily configured workstations may sell $2500. Since 2008, half desktop computers made year battery operated lap-top computers. Desktop computing sales declining. Throughout range price capability, desktop market tends driven optimize price-performance. combination performance (measured primarily terms comput e performance graphics perfor- mance) price system matters customers market, hence computer designers. result, newest, highest-performance microprocessors cost-reduced microp rocessors often appear first desktop systems (see Section 1.6 discussion issues affecting cost computers). Desktop computing also tends reasonably well characterized terms applications benchmarking, though increasing use web-centric, interac-tive applications poses new challenges performance evaluation. Servers shift desktop computing occurred 1980s, role servers grew toprovide larger-scale reliable file computing services. servershave become backbone large-scale enterprise computing, replacing tra-ditional mainframe. servers, different characteristics important. First, availability critical. (We discuss availability Section 1.7 .) Consider servers running ATM machines banks airline reservation systems. Failure server systems far catastrophic failure single desktop, since servers mustoperate seven days week, 24 hours day. Figure 1.3 estimates revenue costs downtime server applications.8■Chapter One Fundamentals Quantitative Design AnalysisA second key feature server systems scalability. Server systems often grow response increasing demand services support anexpansion functional requirements. Thus ability scale computingcapacity, memory, storage, I/O bandwidth server crucial. Finally, servers designed efficient throughput. is, overall per- formance server —in terms transactions per minute web pages served per second —is crucial. Responsiveness individual request remains important, overall efficiency cost-effectiveness, determined howmany requests handled unit time, key metrics servers.We return issue assessing performance different types computingenvironments Section 1.8 . Clusters/Warehouse-Scale Computers growth Software Service (SaaS) applications like search, social net- working, video viewing sharing, multiplayer games, online shopping, soon led growth class computers called clusters . Clusters col- lections desktop computers servers connected local area networks act asa single larger computer. node runs operating system, nodes com-municate using networking protocol. WSCs largest clusters, thatthey designed tens thousands servers act one. Chapter 6 describes class extremely large computers. Price-performance power critical WSCs since large. Chapter 6 explains, majority cost warehouse associated power cooling computers inside warehouse. annual amortizedcomputers networking gear cost WSC $40 million, usually replaced every years. buying thatApplicationCost downtime per hourAnnual losses downtime 1% (87.6 h/year)0.5% (43.8 h/year)0.1% (8.8 h/year) Brokerage service $4,000,000 $350,400,000 $175,200,000 $35,000,000 Energy $1,750,000 $153,300,000 $76,700,000 $15,300,000 Telecom $1,250,000 $109,500,000 $54,800,000 $11,000,000 Manufacturing $1,000,000 $87,600,000 $43,800,000 $8,800,000 Retail $650,000 $56,900,000 $28,500,000 $5,700,000 Health care $400,000 $35,000,000 $17,500,000 $3,500,000 Media $50,000 $4,400,000 $2,200,000 $400,000 Figure 1.3 Costs rounded nearest $100,000 unavailable system shown analyzing cost down- time (in terms immediately lost revenue), assuming three different levels availability, downtime isdistributed uniformly. data Landstrom (2014) collected analyzed Contingency Plan- ning Research.1.2 Classes Computers ■9much computing, need buy wisely, 10% improvement price- performance means annual savings $4 million (10% $40 million) per WSC; company like Amazon might 100 WSCs! WSCs related servers availability critical. example, Ama- zon.com $136 billion sales 2016. 8800 hours year, average revenue per hour $15 million. peak hour Christ- mas shopping, potential loss would many times higher. Chapter 6 explains, difference WSCs servers WSCs use redundant,inexpensive components building blocks, relying software layer catch isolate many failures happen computing scale deliver availability needed applications. Note scalability aWSC handled local area network connecting computers byintegrated computer hardware, case servers. Supercomputers related WSCs equally expensive, costing hundreds millions dollars, b ut supercomputers differ emphasi- zing floating-point performance running large, communication-intensive batch programs run weeks time. contrast, WSCs emphasize interactive applications, large-scale st orage, dependability, high Internet bandwidth. Classes Parallelism Parallel Architectures Parallelism multiple levels driving force computer design across allfour classes computers, energy cost primary constraints.There basically two kinds parallelism applications: 1.Data-level parallelism (DLP) arises many data items operated time. 2.Task-level parallelism (TLP) arises tasks work created operate independently largely parallel. Computer hardware turn exploit two kinds application parallelism four major ways: 1.Instruction-level parallelism exploits data-level parallelism modest levels compiler help using ideas like pipelining medium levels using ideaslike speculative execution. 2.Vector architectures, graphic processor units (GPUs), multimedia instruc- tion sets exploit data-level parallelism applying single instruction col- lection data parallel. 3.Thread-level parallelism exploits either data-level parallelism task-level par- allelism tightly coupled hardware model allows interaction parallel threads. 4.Request-level parallelism exploits parallelism among largely decoupled tasks specified programmer operating system.10 ■Chapter One Fundamentals Quantitative Design AnalysisWhen Flynn (1966) studied parallel computing efforts 1960s, found simple classification whose abbreviations still use today. target data-level parallelism task-level parallelism. looked parallelism theinstruction data streams called instructions constrainedcomponent multiprocessor placed computers one fourcategories: 1.Single instruction stream, single data stream (SISD) —This category uni- processor. programmer thinks standard sequential computer, exploit ILP. Chapter 3 covers SISD architectures use ILP techniques superscalar speculative execution. 2.Single instruction stream, multiple data streams (SIMD) —The instruc- tion executed multiple processors using different data streams. SIMD com-puters exploit data-level parallelism applying operations multiple items data parallel. processor data memory(hence, MD SIMD), single instruction memory controlprocessor, fetches dispatches instructions. Chapter 4 covers DLP three different architectures exploit it: vector architectures, multimediaextensions standard instruction sets, GPUs. 3.Multiple instruction streams, single data stream (MISD) —No commercial mul- tiprocessor type built date, rounds simple classification. 4.Multiple instruction streams, multiple data streams (MIMD) —Each processor fetches instructions operates data, targets task-level parallelism. general, MIMD flexible SIMD thus gen- erally applicable, inherently expensive SIMD. example,MIMD computers also exploit data-level parallelism, although overheadis likely higher would seen SIMD computer. overheadmeans grain size must sufficiently large exploit parallelism effi-ciently. Chapter 5 covers tightly coupled MIMD architectures, exploit thread-level parallelism multiple cooperating threads operate paral- lel.Chapter 6 covers loosely coupled MIMD architectures —specifically, clus- tersandwarehouse-scale computers —that exploit request-level parallelism , many independent tasks proceed parallel naturally little needfor communication synchronization. taxonomy coarse model, many parallel processors hybrids SISD, SIMD, MIMD classes. Nonetheless, useful put framework onthe design space computers see book. 1.3 Defining Computer Architecture task computer designer faces complex one: determine attributesare important new computer, design computer maximize1.3 Defining Computer Architecture ■11performance energy efficiency staying within cost, power, availabil- ity constraints. task many aspects, including instruction set design, func- tional organization, logic design, implementation. implementation mayencompass integrated circuit design, packaging, power, cooling. Optimizingthe design requires familiarity wide range technologies, com-pilers operating systems logic design packaging. decades ago, term computer architecture generally referred instruction set design. aspects computer design called implementa- tion, often insinuating implementation uninteresting less challenging. believe view incorrect. architect ’s designer ’s job much instruction set design, technical hurdles aspectsof project likely challenging encountered instructionset design. ’ll quickly review instruction set architecture describing larger challenges computer architect. Instruction Set Architecture: Myopic View Computer Architecture use term instruction set architecture (ISA) refer actual programmer-visible instruction set book. ISA serves boundarybetween software hardware. quick review ISA use examplesfrom 80x86, ARMv8, RISC-V illustrate seven dimensions ISA.The popular RISC processors come ARM (Advanced RISC Machine),which 14.8 billion chips shipped 2015, roughly 50 times manychips shipped 80x86 processors. Appendices K give detailson three ISAs. RISC-V ( “RISC Five ”) modern RISC instruction set developed University California, Berkeley, made free openly adoptablein response requests industry. addition full software stack (com-pilers, operating systems, simulators), several RISC-V implementa-tions freely available use custom chips field-programmable gate arrays.Developed 30 years first RISC instruction sets, RISC-V inherits ances-tors’good ideas —a large set registers, easy-to-pipeline instructions, lean set operations —while avoiding omissions mistakes. free open, elegant example RISC architectures mentioned earlier, 60 companies joined RISC-V foundation, includingAMD, Google, HP Enterprise, IBM, Microsoft, Nvidia, Qualcomm, Samsung,and Western Digital. use integer core ISA RISC-V exampleISA book. 1.Class ISA —Nearly ISAs today classified general-purpose register architectures, operands either registers memory locations. 80x86 16 general-purpose registers 16 hold floating-point data,while RISC-V 32 general-purpose 32 floating-point registers (seeFigure 1.4 ). two popular versions class register-memory ISAs,12 ■Chapter One Fundamentals Quantitative Design Analysissuch 80x86, access memory part many instructions, load-store ISAs, ARMv8 RISC-V, access memory load store instructions. ISAs announced since 1985 areload-store. 2.Memory addressing —Virtually desktop server computers, including 80x86, ARMv8, RISC-V, use byte addressing access memory operands.Some architectures, like ARMv8, require objects must aligned .A n access object size sbytes byte address Ais aligned Amod s¼0. (See Figure A.5 page A-8.) 80x86 RISC-V require alignment, accesses generally faster operands aligned. 3.Addressing modes —In addition specifying registers constant operands, addressing modes specify address memory object. RISC-V addressing modes Register, Immediate (for constants), Displacement, con-stant offset added register form memory address. 80x86supports three modes, plus three variations displacement: register(absolute), two registers (based indexed displacement), two registersRegister Name Use Saver x0 zero constant value 0 N.A. x1 ra Return address Caller x2 sp Stack pointer Callee x3 gp Global pointer – x4 tp Thread pointer – x5–x7 t0 –t2 Temporaries Caller x8 s0/fp Saved register/frame pointer Callee x9 s1 Saved register Callee x10–x11 a0 –a1 Function arguments/return values Caller x12–x17 a2 –a7 Function arguments Caller x18–x27 s2 –s11 Saved registers Callee x28–x31 t3 –t6 Temporaries Caller f0–f7 ft0 –ft7 FP temporaries Caller f8–f9 fs0 –fs1 FP saved registers Callee f10–f11 fa0 –fa1 FP function arguments/return values Caller f12–f17 fa2 –fa7 FP function arguments Caller f18–f27 fs2 –fs11 FP saved registers Callee f28–f31 ft8 –ft11 FP temporaries Caller Figure 1.4 RISC-V registers, names, usage, calling conventions. addition 32 general-purpose registers (x0 –x31), RISC-V 32 floating-point registers (f0 –f31) hold either 32-bit single-precision number 64-bit double-precision num- ber. registers preserved across procedure call labeled “Callee ”saved.1.3 Defining Computer Architecture ■13where one register multiplied size operand bytes (based scaled index displacement). like last three modes, minus displacement field, plus register indirect, indexed, based scaled index.ARMv8 three RISC-V addressing modes plus PC-relative addressing,the sum two registers, sum two registers one register ismultiplied size operand bytes. also autoincrement andautodecrement addressing, calculated address replaces contentsof one registers used forming address. 4.Types sizes operands —Like ISAs, 80x86, ARMv8, RISC-V support operand sizes 8-bit (ASCII character), 16-bit (Unicode characteror half word), 32-bit (integer word), 64-bit (double word long integer),and IEEE 754 floating point 32-bit (single precision) 64-bit (double precision). 80x86 also supports 80-bit floating point (extended double precision). 5.Operations —The general categories operations data transfer, arithmetic logical, control (discussed next), floating point. RISC-V simple andeasy-to-pipeline instruction set architecture, representative RISCarchitectures used 2017. Figure 1.5 summarizes integer RISC-V ISA, Figure 1.6 lists floating-point ISA. 80x86 much richer larger set operations (see Appendix K). 6.Control flow instructions —Virtually ISAs, including three, support conditional branches, unconditional jumps, procedure calls, returns. Allthree use PC-relative addressing, branch address specified anaddress field added PC. small differences. RISC-V conditional branches ( BE,BNE, etc.) test contents registers, 80x86 ARMv8 branches test condition code bits set side effectsof arithmetic/logic operations. ARMv8 RISC-V procedure call placesthe return address register, whereas 80x86 call ( CALLF ) places return address stack memory. 7.Encoding ISA —There two basic choices encoding: fixed length variable length . ARMv8 RISC-V instructions 32 bits long, simplifies instruction decoding. Figure 1.7 shows RISC-V instruction for- mats. 80x86 encoding variable length, ranging 1 18 bytes.Variable-length instructions take less space fixed-length instructions, program compiled 80x86 usually smaller program compiled RISC-V. Note choices mentioned previously affect howthe instructions encoded binary representation. example, num-ber registers number addressing modes significantimpact size instructions, register field addressingmode field appear many times single instruction. (Note ARMv8and RISC-V later offered extensions, called Thumb-2 RV64IC, thatprovide mix 16-bit 32-bit length instructions, respectively, reduce program size. Code size compact versions RISC architectures smaller 80x86. See Appendix K.)14 ■Chapter One Fundamentals Quantitative Design AnalysisInstruction type/opcode Instruction meaning Data transfers Move data registers memory, integer FP special registers; memory address mode 12-bit displacement +contents GPR lb,lbu,sb Load byte, load byte unsigned, store byte (to/from integer registers) lh,lhu,sh Load half word, load half word unsigned, store half word (to/from integer registers) lw,lwu,sw Load word, load word unsigned, store word (to/from integer registers) ld,sd Load double word, store double word (to/from integer registers) flw,fld,fsw,fsd Load SP float, load DP float, store SP float, store DP float fmv._.x ,fmv.x ._ Copy from/to integer register to/from floating-point register; “__”¼S single- precision, double-precision csrrw ,csrrwi ,csrrs , csrrsi ,csrrc ,csrrciRead counters write status registers, include counters: clock cycles, time, instructions retired Arithmetic/logical Operations integer logical data GPRsadd,addi ,addw ,addiw Add, add immediate (all immediates 12 bits), add 32-bits & sign-extend 64 bits, add immediate 32-bits sub,subw Subtract, subtract 32-bits mul,mulw ,mulh ,mulhsu , mulhuMultiply, multiply 32-bits only, multiply upper half, multiply upper half signed- unsigned, multiply upper half unsigned div,divu ,rem,remu Divide, divide unsigned, remainder, remainder unsigned divw ,divuw ,remw ,remuw Divide remainder: previously, divide lower 32-bits, producing 32-bit sign-extended result and,andi And, immediate or,ori,xor,xori Or, immediate, exclusive or, exclusive immediate lui Load upper immediate; loads bits 31-12 register immediate, sign-extends auipc Adds immediate bits 31 –12 zeros lower bits PC; used JALR transfer control 32-bit address sll,slli ,srl,srli ,sra, sraiShifts: shift left logical, right logical, right arithmetic; variable immediate forms sllw ,slliw ,srlw ,srliw , sraw ,sraiwShifts: previously, shift lower 32-bits, producing 32-bit sign-extended result slt,slti ,sltu ,sltiu Set less than, set less immediate, signed unsigned Control Conditional branches jumps; PC-relative registerbeq,bne,blt,bge,bltu , bgeuBranch GPR equal/not equal; less than; greater equal, signed unsigned jal,jalr Jump link: save PC+4, target PC-relative ( JAL) register ( JALR ); specify x0as destination register, acts simple jump ecall Make request supporting execution environment, usually OS ebreak Debuggers used cause control transferred back debugging environment fence ,fence.i Synchronize threads guarantee ordering memory accesses; synchronize instructions data stores instruction memory Figure 1.5 Subset instructions RISC-V. RISC-V base set instructions (R64I) offers optional exten- sions: multiply-divide (RVM), single-precision floating point (RVF), double-precision floating point (RVD). figure includes RVM next one shows RVF RVD. Appendix gives much detail RISC-V.1.3 Defining Computer Architecture ■15Instruction type/opcode Instruction meaning Floating point FP operations DP SP formats fadd.d ,fadd.s Add DP, SP numbers fsub.d ,fsub.s Subtract DP, SP numbers fmul.d ,fmul.s Multiply DP, SP floating point fmadd.d ,fmadd.s ,fnmadd.d , fnmadd.sMultiply-add DP, SP numbers; negative multiply-add DP, SP numbers fmsub.d ,fmsub.s ,fnmsub.d , fnmsub.sMultiply-sub DP, SP numbers; negative multiply-sub DP, SP numbers fdiv.d ,fdiv.s Divide DP, SP floating point fsqrt.d ,fsqrt.s Square root DP, SP floating point fmax.d ,fmax.s ,fmin.d , fmin.sMaximum minimum DP, SP floating point fcvt._._ ,fcvt._._u , fcvt._u._Convert instructions: FCVT.x.y converts type xto type y, xandyare L(64-bit integer), W(32-bit integer), D(DP), S(SP). Integers unsigned ( U) feq._ ,flt._ ,fle._ Floating-point compare floating-point registers record Boolean result integer register; “__”¼Sfor single-precision, Dfor double-precision fclass.d ,fclass.s Writes integer register 10-bit mask indicates class floating-point number ( /C0∞,+∞,/C00, +0, NaN, …) fsgnj._ ,fsgnjn._ , fsgnjx._Sign-injection instructions changes sign bit: copy sign bit source, oppositive sign bit source, XOR 2 sign bits Figure 1.6 Floating point instructions RISC-V. RISC-V base set instructions (R64I) offers optional extensions single-precision floating point (RVF) double-precision floating point (RVD). SP ¼single precision; DP¼double precision. R-type0 7 6 12 11 15 14 20 19 25 24 31 I-type S-type U-typeopcode rd rs1 rs2 funct3 funct7 opcode imm [4:0] imm [4:1|11]rs1 rs1rs2 rs2funct3 funct3imm [11:5]opcode rd rs1 funct3 imm [11:0] opcode rd imm [31:12] J-type opcode rd imm [20|10:1|11|19:12]B-type opcode imm [10:5] imm [12] Figure 1.7 base RISC-V instruction set architecture formats. instructions 32 bits long. R format integer register-to-register operations, ADD, SUB, on. format loads immediate oper- ations, LD ADDI. B format branches J format jumps link. format stores. separate format stores allows three register specifiers (rd, rs1, rs2) always samelocation formats. U format wide immediate instructions (LUI, AUIPC).16 ■Chapter One Fundamentals Quantitative Design AnalysisThe challenges facing computer architect beyond ISA design par- ticularly acute present, differences among instruction sets small distinct application areas. Therefore, starting fourthedition book, beyond quick review, bulk instruction set mate-rial found appendices (see Appendices K). Genuine Computer Architecture: Designing Organization Hardware Meet Goals Functional Requirements implementation computer two components: organization hard- ware. term organization includes high-level aspects computer ’s design, memory system, memory interconnect, design ofthe internal processor CPU (central processing unit —where arithmetic, logic, branching, data transfer implemented). term microarchitecture also used instead organization. example, two processors instruc-tion set architectures different organizations AMD Opteron IntelCore i7. processors implement 80x86 instruction set, verydifferent pipeline cache organizations. switch multiple processors per microprocessor led term core also used processors. Instead saying multiprocessor microprocessor, theterm multicore caught on. Given virtually chips multiple processors, term central processing unit, CPU, fading popularity. Hardware refers specifics computer, including detailed logic design packaging technology computer. Often line computerscontains computers identical instruction set architectures similarorganizations, differ detailed hardware implementation. exam-ple, Intel Core i7 (see Chapter 3 ) Intel Xeon E7 (see Chapter 5 ) nearly identical offer different clock rates different memory systems, mak-ing Xeon E7 effective server computers. book, word architecture covers three aspects computer design —instruction set architecture, organization microarchitecture, hardware. Computer architects must design computer meet functional requirements well price, power, performance, availability goals. Figure 1.8 summarizes requirements consider designing new computer. Often, architects also mustdetermine functional requirements are, major task. Therequirements may specific features inspired market. Application software typically drives choice certain functional requirements determining computer used. large body software exists particular instruc-tion set architecture, architect may decide new computer imple-ment existing instruction set. presence large market particularclass applications might encourage designers incorporate requirementsthat would make computer competitive market. Later chapters examinemany requirements features depth.1.3 Defining Computer Architecture ■17Architects must also aware important trends technology use computers trends affect future cost also thelongevity architecture. 1.4 Trends Technology instruction set architecture prevail, must designed survive rapidchanges computer technology. all, successful new instruction setFunctional requirements Typical features required supported Application area Target computer Personal mobile device Real-time performance range tasks, including interactive performance graphics, video, audio; energy efficiency ( Chapters 2 –5and7;Appendix ) General-purpose desktop Balanced performance range tasks, including interactive performance graphics, video, audio ( Chapters 2 –5;Appendix ) Servers Support databases transaction processing; enhancements reliability availability; support scalability ( Chapters 2 ,5, 7; Appendices A, D, F) Clusters/warehouse-scale computersThroughput performance many independent tasks; error correction memory; energy proportionality ( Chapters 2 ,6, 7; Appendix F) Internet things/embedded computingOften requires special support graphics video (or application-specific extension); power limitations power control may required; real-time constraints (Chapters 2 ,3,5, 7; Appendices E) Level software compatibility Determines amount existing software computerAt programming language flexible designer; need new compiler ( Chapters 3 ,5, 7;Appendix ) Object code binary compatibleInstruction set architecture completely defined —little flexibility —but investment needed software porting programs ( Appendix ) Operating system requirements Necessary features support chosen OS (Chapter 2 ;Appendix B ) Size address space important feature ( Chapter 2 ); may limit applications Memory management Required modern OS; may paged segmented ( Chapter 2 ) Protection Different OS application needs: page versus segment; virtual machines ( Chapter 2 ) Standards Certain standards may required marketplaceFloating point Format arithmetic: IEEE 754 standard (Appendix J), special arithmetic graphics signal processing I/O interfaces I/O devices: Serial ATA, Serial Attached SCSI, PCI Express (Appendices F)Operating systems UNIX, Windows, Linux, CISCO IOSNetworks Support required different networks: Ethernet, Infiniband (Appendix F)Programming languages Languages (ANSI C, C++, Java, Fortran) affect instruction set ( Appendix ) Figure 1.8 Summary important functional requirements architect faces. left-hand column describes class requirement, right-hand column gives specific examples. right-hand col-umn also contains references chapters appendices deal specific issues.18 ■Chapter One Fundamentals Quantitative Design Analysisarchitecture may last decades —for example, core IBM mainframe use 50 years. architect must plan technology changes increase lifetime successful computer. plan evolution computer, designer must aware rapid changes implementation technology. Five implementation technologies, whichchange dramatic pace, critical modern implementations: ■Integrated circuit logic technology —Historically, transistor density increased 35% per year, quadrupling somewhat four years. Increases die size less predictable slower, ranging 10% 20% per year. combined effect traditional growth rate transistor count chipof 40% –55% per year, doubling every 18 –24 months. trend popularly known Moore ’s Law. Device speed scales slowly, discuss below. Shockingly, Moore ’s Law more. number devices per chip still increasing, decelerating rate. Unlike Moore ’s Law era, expect doubling time stretched new technol-ogy generation. ■Semiconductor DRAM (dynamic random-access memory) —This technology foundation main memory, discuss Chapter 2 . growth DRAM slowed dramatically, quadrupling every three years past. 8-gigabit DRAM shipping 2014, 16-gigabit DRAM ’t reach state 2019, looks like 32-gigabit DRAM ( Kim, 2005 ).Chapter 2 mentions several technologies may replace DRAM hits capacity wall. ■Semiconductor Flash (electrically erasable programmable read-only mem- ory)—This nonvolatile semiconductor memory standard storage device PMDs, rapidly increasing popularity fueled rapid growthrate capacity. recent years, capacity per Flash chip increased about50%–60% per year, doubling roughly every 2 years. Currently, Flash memory 8 –10 times cheaper per bit DRAM. Chapter 2 describes Flash memory. ■Magnetic disk technology —Prior 1990, density increased 30% per year, doubling three years. rose 60% per year thereafter, increased 100% per year 1996. 2004 2011, dropped back 40%per year, doubled every two years. Recently, disk improvement slowedto less 5% per year. One way increase disk capacity add plat-ters areal density, already seven platters within theone-inch depth 3.5-inch form factor disks. room mostone two platters. last hope real density increase use smalllaser disk read-write head heat 30 nm spot 400 °C written magnetically cools. unclear whether Heat Assisted Magnetic Recording manufactured economically reliably, althoughSeagate announced plans ship HAMR limited production 2018. HAMRis last chance continued improvement areal density hard disk1.4 Trends Technology ■19drives, 8 –10 times cheaper per bit Flash 200 –300 times cheaper per bit DRAM. technology central server- warehouse-scale storage, discuss trends detail Appendix D. ■Network technology —Network performance depends performance switches performance transmission system. discuss trends networking Appendix F. rapidly changing technologies shape design computer that, speed technology enhancements, may lifetime 3 –5 years. Key tech- nologies Flash change sufficiently designer must plan thesechanges. Indeed, designers often design next technology, knowing that,when product begins shipping volume, following technology may bethe cost-effective may performance advantages. Traditionally, costhas decreased rate density increases. Although technology improves continuously, impact increases discrete leaps, threshold allows new capability reached. example, MOS technology reached point early 1980s 25,000 50,000 transistors could fit single chip, became possible builda single-chip, 32-bit microprocessor. late 1980s, first-level caches could goon chip. eliminating chip crossings within processor pro-cessor cache, dramatic improvement cost-performance energy-performance possible. design simply unfeasible technologyreached certain point. multicore microprocessors increasing numbers ofcores generation, even server computers increasingly headed toward sin- gle chip processors. technology thresholds rare sig- nificant impact wide variety design decisions. Performance Trends: Bandwidth Latency shall see Section 1.8 ,bandwidth orthroughput total amount work done given time, megabytes per second disk transfer. contrast,latency orresponse time time start completion event, milliseconds disk access. Figure 1.9 plots relative improve- ment bandwidth latency technology milestones microprocessors,memory, networks, disks. Figure 1.10 describes examples milestones detail. Performance primary differentiator microprocessors networks, seen greatest gains: 32,000 –40,000 /C2in bandwidth 50 –90/C2in latency. Capacity generally important performance memory anddisks, capacity improved more, yet bandwidth advances 400 –2400/C2are still much greater gains latency 8 –9/C2. Clearly,bandwidthhasoutpaced latencyacrossthesetechnologiesandwilllikely continue so. simple rule thumb bandwidth grows least thesquare improvement latency. Computer designers plan accordingly.20 ■Chapter One Fundamentals Quantitative Design AnalysisScaling Transistor Performance Wires Integrated circuit processes characterized feature size , min- imum size transistor wire either xorydimension. Feature sizes decreased 10 μm 1971 0.016 μm 2017; fact, switched units, production 2017 referred “16 nm, ”and 7 nm chips under- way. Since transistor count per square millimeter silicon determined thesurface area transistor, density transistors increases quadratically alinear decrease feature size.110100100010,000100,000 1 10 100Relative bandwidth improvement Relative Latenc ImprovementMicroprocessor MemoryNetwork Disk (Latency improvement = Bandwidth improvement) Figure 1.9 Log-log plot bandwidth latency milestones Figure 1.10 relative first milestone. Note latency improved 8 –91/C2, bandwidth improved 400 –32,000 /C2. Except networking, note modest improvements latency bandwidth three technologies six years since last edition: 0% –23% latency 23% –70% bandwidth. Updated Patterson, D., 2004. Latency lags band- width. Commun. ACM 47 (10), 71 –75.1.4 Trends Technology ■21Microprocessor 16-Bit address/ bus, microcoded32-Bit address/ bus, microcoded5-Stage pipeline, on-chip & caches, FPU2-Way superscalar, 64-bit busOut-of-order 3-way superscalarOut-of-order superpipelined, on-chip L2 cacheMulticore OOO 4-way chip L3 cache, Turbo Product Intel 80286 Intel 80386 Intel 80486 Intel Pentium Intel Pentium Pro Intel Pentium 4 Intel Core i7Year 1982 1985 1989 1993 1997 2001 2015 Die size (mm 2) 47 43 81 90 308 217 122 Transistors 134,000 275,000 1,200,000 3,100,000 5,500,000 42,000,000 1,750,000,000 Processors/chip 1 1 1 1 1 1 4 Pins 68 132 168 273 387 423 1400 Latency (clocks) 6 5 5 5 10 22 14 Bus width (bits) 16 32 32 64 64 64 196 Clock rate (MHz) 12.5 16 25 66 200 1500 4000 Bandwidth (MIPS) 2 6 25 132 600 4500 64,000Latency (ns) 320 313 200 76 50 15 4 Memory module DRAM Page mode DRAMFast page mode DRAMFast page mode DRAMSynchronous DRAMDouble data rate SDRAMDDR4 SDRAM Module width (bits) 16 16 32 64 64 64 64 Year 1980 1983 1986 1993 1997 2000 2016Mbits/DRAM chip 0.06 0.25 1 16 64 256 4096 Die size (mm 2) 35 45 70 130 170 204 50 Pins/DRAM chip 16 16 18 20 54 66 134 Bandwidth (MBytes/s) 13 40 160 267 640 1600 27,000 Latency (ns) 225 170 125 75 62 52 30 Local area network Ethernet Fast EthernetGigabit Ethernet10 Gigabit Ethernet100 Gigabit Ethernet400 Gigabit Ethernet IEEE standard 802.3 803.3u 802.3ab 802.3ac 802.3ba 802.3bs Year 1978 1995 1999 2003 2010 2017 Bandwidth (Mbits/seconds) 10 100 1000 10,000 100,000 400,000 Latency ( μs) 3000 500 340 190 100 60 Hard disk 3600 RPM 5400 RPM 7200 RPM 10,000 RPM 15,000 RPM 15,000 RPMProduct CDC WrenI 94145-36Seagate ST41600Seagate ST15150Seagate ST39102Seagate ST373453Seagate ST600MX0062 Year 1983 1990 1994 1998 2003 2016Capacity (GB) 0.03 1.4 4.3 9.1 73.4 600 Disk form factor 5.25 in. 5.25 in. 3.5 in. 3.5 in. 3.5 in. 3.5 in. Media diameter 5.25 in. 5.25 in. 3.5 in. 3.0 in. 2.5 in. 2.5 in. Interface ST-412 SCSI SCSI SCSI SCSI SAS Bandwidth (MBytes/s) 0.6 4 9 24 86 250 Latency (ms) 48.3 17.1 12.7 8.8 5.7 3.6 Figure 1.10 Performance milestones 25 –40 years microprocessors, memory, networks, disks. microprocessor milestones several generations IA-32 processors, going 16-bit bus, microcoded 80286 64-bit bus, multicore, out-of-order execution, superpipelined Core i7. Memory module milestones go 16-bit- wide, plain DRAM 64-bit-wide double data rate version 3 synchronous DRAM. Ethernet advanced 10 Mbits/sto 400 Gbits/s. Disk milestones based rotation speed, improving 3600 15,000 RPM. case best- case bandwidth, latency time simple operation assuming contention. Updated Patterson, D., 2004. Latency lags bandwidth. Commun. ACM 47 (10), 71 –75.The increase transistor performance, however, complex. feature sizes shrink, devices shrink quadratically horizontal dimension also shrink vertical dimension. shrink vertical dimension requires areduction operating voltage maintain correct operation reliability thetransistors. combination scaling factors leads complex interrelationshipbetween transistor performance process feature size. first approximation, inthe past transistor performance improved linearly decreasing feature size. factthattransistor count improves quadratically linear increase tran- sistor performance challenge opportunity computer architects created! early days microprocessors, higher rate improvement density used move quickly 4-bit, 8-bit, 16-bit,to 32-bit, 64-bit microprocessors. recently, density improvements sup-ported introduction multiple processors per chip, wider SIMD units, manyof innovations speculative execution caches found Chapters 2 –5. Although transistors generally improve performance decreased feature size, wires integrated circuit not. particular, signal delay wireincreases proportion product resistance capacitance. course, feature size shrinks, wires get shorter, resistance capacitance per unit length get worse. relationship complex, since resistance capaci-tance depend detailed aspects process, geometry wire, loadingon wire, even adjacency structures. occasional processenhancements, introduction copper, provide one-timeimprovements wire delay. general, however, wire delay scales poorly compared transistor perfor- mance, creating additional challenges designer. addition power dissipation limit, wire delay become major design obstacle large inte- grated circuits often critical transistor switching delay. Largerand larger fractions clock cycle consumed propagationdelay signals wires, power plays even greater role wire delay. 1.5 Trends Power Energy Integrated Circuits Today, energy biggest challenge facing computer designer nearlyevery class computer. First, power must brought distributed aroundthe chip, modern microprocessors use hundreds pins multiple intercon-nect layers power ground. Second, power dissipated heat must removed. Power Energy: Systems Perspective system architect user think performance, power, energy? viewpoint system designer, three primary concerns. First, maximum power processor ever requires? Meeting demand important ensuring correct operation. example, processor1.5 Trends Power Energy Integrated Circuits ■23attempts draw power power-supply system provide (by drawing current system supply), result typically voltage drop, cause devices malfunction. Modern processors vary widely inpower consumption high peak currents; hence provide voltage indexingmethods allow processor slow regulate voltage within widermargin. Obviously, decreases performance. Second, sustained power consumption? metric widely called thethermal design power (TDP) determines cooling requirement. TDP neither peak power, often 1.5 times higher, actual aver- age power consumed given computation, likely lower still. typical power supply system typically sized exceed theTDP, cooling system usually designed match exceed TDP. Failureto provide adequate cooling allow junction temperature processor toexceed maximum value, resulting device failure possibly permanentdamage. Modern processors provide two features assist managing heat, sincethe highest power (and hence heat temperature rise) exceed long-termaverage specified TDP. First, thermal temperature approaches junction temperature limit, circuitry lowers clock rate, thereby reducing power. technique successful, second thermal overload trap activatedto power chip. third factor designers users need consider energy energy efficiency. Recall power simply energy per unit time: 1 watt ¼1 joule per second. metric right one comparing processors: energy power?In general, energy always better metric tied specific task andthe time required task. particular, energy complete workload equal average power times execution time workload. Thus, want know two processors efficient given task, need compare energy consumption (not power) executing task.For example, processor may 20% higher average power consumptionthan processor B, executes task 70% time needed byB, energy consumption 1.2 /C20.7¼0.84, clearly better. One might argue large server cloud, sufficient consider average power, since workload often assumed infinite, mis- leading. cloud populated processor Bs rather As, cloud would less work amount energy expended. Using energyto compare alternatives avoids pitfall. Whenever fixed workload,whether warehouse-size cloud smartphone, comparing energy theright way compare computer alternatives, electricity bill thecloud battery lifetime smartphone determined energyconsumed. power consumption useful measure? primary legitimate use constraint: example, air-cooled chip might limited 100 W. used metric workload fixed, ’s variation true metric energy per task.24 ■Chapter One Fundamentals Quantitative Design AnalysisEnergy Power Within Microprocessor CMOS chips, traditional primary energy consumption switch- ing transistors, also called dynamic energy. energy required per transistor proportional product capacitive load driven transistor andthe square voltage: Energydynamic /Capacitive load /C2Voltage2 equation energy pulse logic transition 0 !1!0o r 1!0!1. energy single transition (0 !1o r1!0) then: Energydynamic /1=2/C2Capacitive load /C2Voltage2 power required per transistor product energy transition multiplied frequency transitions: Power dynamic /1=2/C2Capacitive load /C2Voltage2/C2Frequency switched fixed task, slowing clock rate reduces power, energy. Clearly, dynamic power energy greatly reduced lowering volt- age, voltages dropped 5 V 1 V 20 years. capac-itive load function number transistors connected output thetechnology, determines capacitance wires transistors. Example microprocessors today designed adjustable voltage, 15% reduction voltage may result 15% reduction frequency. would impact dynamic energy dynamic power? Answer capacitance unchanged, answer energy ratio voltages Energynew Energyold¼Voltage /C20:85 ðÞ2 Voltage2¼0:852¼0:72 reduces energy 72% original. power, add ratio frequencies Power new Power old¼0:72/C2Frequency switched /C20:85 ðÞ Frequency switched¼0:61 shrinking power 61% original. move one process next, increase number tran- sistors switching frequency change dominate decreasein load capacitance voltage, leading overall growth power consump-tion energy. first microprocessors consumed less watt, first1.5 Trends Power Energy Integrated Circuits ■2532-bit microprocessors (such Intel 80386) used 2 W, whereas 4.0 GHz Intel Core i7-6700K consumes 95 W. Given heat must dissi- pated chip 1.5 cm side, near limit becooled air, stuck nearly decade. Given preceding equation, would expect clock frequency growth slow ’t reduce voltage increase power per chip. Figure 1.11 shows indeed case since 2003, even microprocessorsinFigure 1.1 highest performers year. Note period flatter clock rates corresponds period slow performance improvement range Figure 1.1 . Distributing power, removing heat, preventing hot spots become increasingly difficult challenges. Energy major constraint tousing transistors; past, raw silicon area. Therefore modern 110100100010,000 1978 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 2006 2008 2010 2012 2014 2016 2018Clock rate (MHz)Intel Pentium4 Xeon 3200 MHz 2003Intel Skylake Core i7 4200 MHz 2017 Intel Pentium III 1000 MHz 2000 Digital Alpha 21164A 500 MHz 1996 Digital Alpha 21064 150 MHz 1992 MIPS M2000 25 MHz 1989 Digital VAX-11/780 5 MHz 1978Sun-4 SPARC 16.7 MHz 1986 15%/year40%/year2%/year Figure 1.11 Growth clock rate microprocessors Figure 1.1 .Between 1978 1986, clock rate improved less 15% per year performance improved 22% per year. “renaissance period ”of 52% per- formance improvement per year 1986 2003, clock rates shot almost 40% per year. Since then, clock rate nearly flat, growing less 2% per year, single processor performance improvedrecently 3.5% per year.26 ■Chapter One Fundamentals Quantitative Design Analysismicroprocessors offer many techniques try improve energy efficiency despite flat clock rates constant supply voltages: 1.Do nothing well. microprocessors today turn clock inactive modules save energy dynamic power. example, floating-pointinstructions executing, clock floating-point unit disabled. Ifsome cores idle, clocks stopped. 2.Dynamic voltage-frequency scaling (DVFS). second technique comes directly preceding formulas. PMDs, laptops, even servers periods low activity need operate highest clockfrequency voltages. Modern microprocessors typically offer clockfrequencies voltages operate use lower power energy.Figure 1.12 plots potential power savings via DVFS server work- load shrinks three different clock rates: 2.4, 1.8, 1 GHz. overallserver power savings 10% –15% two steps. 3.Design typical case. Given PMDs laptops often idle, mem- ory storage offer low power modes save energy. example, DRAMshave series increasingly lower power modes extend battery life PMDsand laptops, proposals disks mode spinsmore slowly unused save power. However, cannot access DRAMsor disks modes, must return fully active mode read write,no matter low access rate. mentioned, microprocessors PCs designed instead heavy use high operating temperatures, relying on-chip temperature sensors detect activity reduced automat-ically avoid overheating. “emergency slowdown ”allows manufacturers design typical case rely safety mechanism some-one really run programs consume much power typical. 100Power (% peak) 80 604020 01 GHz DVS savings (%) 1.8 GHz 2.4 GHz Idle 7 14 21 29 36 43 50 57 64 71 79 86 93 100 Compute load (%) Figure 1.12 Energy savings server using AMD Opteron microprocessor, 8 GB DRAM, one ATA disk. 1.8 GHz, server handle two-thirds workload without causing service-level violations, 1 GHz, safely han- dle one-third workload (Figure 5.11 Barroso H €olzle, 2009 ).1.5 Trends Power Energy Integrated Circuits ■274.Overclocking . Intel started offering Turbo mode 2008, chip decides safe run higher clock rate short time, possibly cores, temperature starts rise. example, 3.3 GHz Core i7 runin short bursts 3.6 GHz. Indeed, highest-performing microprocessorseach year since 2008 shown Figure 1.1 offered temporary overclock- ing 10% nominal clock rate. single-threaded code, thesemicroprocessors turn cores one run faster. Note that,although operating system turn Turbo mode, notificationonce enabled, programmers may surprised see programs vary performance room temperature! Although dynamic power traditionally thought primary source power dissipation CMOS, static power becoming important issue becauseleakage current flows even transistor off: Power static/Current static/C2Voltage is, static power proportional number devices. Thus increasing number transistors increases power even idle, current leakage increases processors smaller transistor sizes. aresult, low-power systems even turning power supply ( power gat- ing) inactive modules order control loss leakage. 2011 goal leakage 25% total power consumption, leakage inhigh-performance designs sometimes far exceeding goal. Leakage ashigh 50% chips, part large SRAM caches need power maintain storage values. (The SRAM static.) hope stop leakage turn power chips ’subsets. Finally, processor portion whole energy cost sys- tem, make sense use faster, less energy-efficient processor allow restof system go sleep mode. strategy known race-to-halt . importance power energy increased scrutiny effi- ciency innovation, primary evaluation tasks per joule per-formance per watt, contrary performance per mm 2of silicon past. new metric affects approaches parallelism, see Chapters 4and5. Shift Computer Architecture Limits Energy transistor improvement decelerates, computer architects must look elsewhere improved energy efficiency. Indeed, given energy budget, easy todayto design microprocessor many transistors cannot turnedon time. phenomenon called dark silicon , much chip cannot unused ( “dark”) moment time thermal con- straints. observation led architects reexamine fundamentals pro-cessors ’design search greater energy-cost performance. Figure 1.13 , lists energy cost area cost building blocks modern computer, reveals surprisingly large ratios. example, 32-bit28 ■Chapter One Fundamentals Quantitative Design Analysisfloating-point addition uses 30 times much energy 8-bit integer add. area difference even larger, 60 times. However, biggest difference inmemory; 32-bit DRAM access takes 20,000 times much energy 8-bit addition. small SRAM 125 times energy-efficient DRAM, demonstrates importance careful uses caches memory buffers. new design principle minimizing energy per task combined relative energy area costs Figure 1.13 inspired new direction com- puter architecture, describe Chapter 7 . Domain-specific processors save energy reducing wide floating-point operations deploying special-pur-pose memories reduce accesses DRAM. use saving provide10–100 (narrower) integer arithmetic units traditional processor. Although processors perform limited set tasks, perform remarkably faster energy efficiently general-purpose processor. Like hospital general practitioners medical specialists, computers energy-aware world likely combinations general-purpose cores thatcan perform task special-purpose cores things extremely welland even cheaply. 1.6 Trends Cost Although costs tend less important computer designs —specifically supercomputers —cost-sensitive designs growing significance. Indeed, past 35 years, use technology improvements lower cost, well asincrease performance, major theme computer industry.Relative energy cost 1 Energy numbers Mark Horowitz *Computing’s Energy problem (and it)*. ISSCC 2014 Area numbers ynthesized result usin g Desi gn compiler TSMC 45nm tech node. FP units used Desi gnWare Librar y.10 100 1000 10000 1 10 100 1000Relative area cost Operation: 8b Add 0.03 0.05 0.1 0.4 0.9 0.2 3.1 1.1 3.7 5 64016b Add 16b FB Add 32b FB Add32b Add 8b Mult 32b Mult 16b FB Mult 32b FB Mult 32b SRAM Read (8KB) 32b DRAM ReadEnergy (pJ) Area ( µm2) 36 67 137 1360 4184 282 3495 1640 7700 N/A N/A Figure 1.13 Comparison energy die area arithmetic operations energy cost accesses SRAM DRAM. [Azizi][Dally]. Area TSMC 45 nm technology node.1.6 Trends Cost ■29Textbooks often ignore cost half cost-performance costs change, thereby dating books, issues subtle differ across industry segments. Nevertheless, ’s essential computer architects under- standing cost factors order make intelligent decisions whethera new feature included designs cost issue. (Imagine archi-tects designing skyscrapers without information costs steel beams andconcrete!) section discusses major factors influence cost computer factors changing time. Impact Time, Volume, Commoditization cost manufactured computer component decreases time even withoutsignificant improvements basic implementation technology. underlyingprinciple drives costs learning curve —manufacturing costs decrease time. learning curve best measured change yield—the percentage manufactured devices survives testing procedure. Whether chip, board, system, designs twice yield havehalf cost. Understanding learning curve improves yield critical projecting costs product ’s life. One example price per megabyte DRAM dropped long term. Since DRAMs tend priced close relation-ship cost —except periods shortage oversupply —price cost DRAM track closely. Microprocessor prices also drop time, less standard- ized DRAMs, relationship price cost complex. aperiod significant competition, price tends track cost closely, although micro-processor vendors probably rarely sell loss. Volume second key factor determining cost. Increasing volumes affect cost several ways. First, decrease time needed get learn-ing curve, partly proportional number systems (or chips) man-ufactured. Second, volume decreases cost increases purchasing manufacturing efficiency. rule thumb, designers estimated costs decrease 10% doubling volume. Moreover, volumedecreases amount development costs must amortized com-puter, thus allowing cost selling price closer still make profit. Commodities products sold multiple vendors large volumes essentially identical. Virtually products sold shelves gro-cery stores commodities, standard DRAMs, Flash memory, monitors,and keyboards. past 30 years, much personal computer industry become commodity business focused building desktop laptop com- puters running Microsoft Windows. many vendors ship virtually identical products, market highly competitive. course, competition decreases gap cost selling30 ■Chapter One Fundamentals Quantitative Design Analysisprice, also decreases cost. Reductions occur commodity market volume clear product definition, allows multiple suppliers compete building components commodity product. result, over-all product cost lower competition among suppliers thecomponents volume efficiencies suppliers achieve. rivalryhas led low end computer business able achieve betterprice-performance sectors yielded greater growth lowend, although limited profits (as typical commodity business). Cost Integrated Circuit would computer architecture book section integrated circuitcosts? increasingly competitive computer marketplace standardparts—disks, Flash memory, DRAMs, —are becoming significant por- tion system ’s cost, integrated circuit costs becoming greater portion cost varies computers, especially high-volume, cost-sensitive portion market. Indeed, PMDs ’increasing reliance whole systems chip (SOC), cost integrated circuits much cost PMD. Thus computer designers must understand costs chips order understand costs current computers. Although costs integrated circuits dropped exponentially, basic process silicon manufacture unchanged: wafer still tested chopped intodiesthat packaged (see Figures 1.14 –1.16). Therefore cost pack- aged integrated circuit Cost integrated circuit ¼Cost die + Cost testing die + Cost packaging final test Final test yield section, focus cost dies, summarizing key issues testing packaging end. Learning predict number good chips per wafer requires first learn- ing many dies fit wafer learning predict percentage ofthose work. simple predict cost: Cost die ¼Cost wafer Dies per wafer /C2Die yield interesting feature initial term chip cost equation sen- sitivity die size, shown below. number dies per wafer approximately area wafer divided area die. accurately estimated Dies per wafer ¼π/C2Wafer diameter =2 ðÞ2 Die area/C0π/C2Wafer diameterﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2/C2Die areap first term ratio wafer area ( πr2) die area. second compensates “square peg round hole ”problem —rectangular dies near periphery1.6 Trends Cost ■31Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Core Memory Controller Core Core Core Core Memory Controller DDRIO DDRIO 3x Intel® UPI, 3x16 PCIe Gen3, 1x4 DMI3 Figure 1.15 components microprocessor die Figure 1.14 labeled functions. Figure 1.14 Photograph Intel Skylake microprocessor die, evaluated inChapter 4 .32 ■Chapter One Fundamentals Quantitative Design Analysisof round wafers. Dividing circumference ( πd) diagonal square die approximately number dies along edge. Example Find number dies per 300 mm (30 cm) wafer die 1.5 cm side die 1.0 cm side. Answer die area 2.25 cm2: Dies per wafer ¼π/C230=2ðÞ2 2:25/C0π/C230ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2/C22:25p ¼706:9 2:25/C094:2 2:12¼270 area larger die 2.25 times bigger, roughly 2.25 many smaller dies per wafer: Dies per wafer ¼π/C230=2ðÞ2 1:00/C0π/C230ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 2/C21:00p ¼706:9 1:00/C094:2 1:41¼640 However, formula gives maximum number dies per wafer. critical question is: fraction good dies wafer, die yield ?A simple model integrated circuit yield, assumes defects randomly Figure 1.16 200 mm diameter wafer RISC-V dies designed SiFive. two types RISC-V dies using older, larger processing line. FE310 die 2.65 mm/C22.72 mm SiFive test die 2.89 mm /C22.72 mm. wafer contains 1846 former 1866 latter, totaling 3712 chips.1.6 Trends Cost ■33distributed wafer yield inversely proportional complexity fabrication process, leads following: Die yield ¼Wafer yield /C21=1 + Defects per unit area /C2Die area ðÞN Bose-Einstein formula empirical model developed looking yield many manufacturing lines (Sydow, 2006), still applies today. Wafer yield accounts wafers completely bad need tested. simplicity, ’ll assume wafer yield 100%. Defects per unit area measure random manufacturing defects occur. 2017 value typically 0.08 –0.10 defects per square inch 28-nm node 0.10 –0.30 newer 16 nm node depends maturity process (recallthe learning curve mentioned earlier). metric versions 0.012 –0.016 defects per square centimeter 28 nm 0.016 –0.047 16 nm. Finally, N parameter called process-complexity factor, measure manufacturingdifficulty. 28 nm processes 2017, N 7.5 –9.5. 16 nm process, N ranges 10 14. Example Find die yield dies 1.5 cm side 1.0 cm side, assuming defect density 0.047 per cm 2andNis 12. Answer total die areas 2.25 1.00 cm2. larger die, yield Die yield ¼1=1+0 :047/C22:25 ðÞ12/C2270¼120 smaller die, yield Die yield ¼1=1+0 :047/C21:00 ðÞ12/C2640¼444 bottom line number good dies per wafer. Less half large dies good, nearly 70% small dies good. Although many microprocessors fall 1.00 2.25 cm2, low-end embedded 32-bit processors sometimes small 0.05 cm2, processors used embedded control (for inexpensive IoT devices) often less 0.01 cm2, high-end server GPU chips large 8 cm2. Given tremendous price pressures commodity products DRAM SRAM, designers included redundancy way raise yield. anumber years, DRAMs regularly included redundant memory cells certain number flaws accommodated. Designers used sim- ilar techniques standard SRAMs large SRAM arrays used cacheswithin microprocessors. GPUs 4 redundant processors 84 samereason. Obviously, presence redundant entries used boost yieldsignificantly.34 ■Chapter One Fundamentals Quantitative Design AnalysisIn 2017 processing 300 mm (12-inch) diameter wafer 28-nm technol- ogy costs $4000 $5000, 16-nm wafer costs $7000. Assuming processed wafer cost $7000, cost 1.00 cm2die would around $16, cost per die 2.25 cm2die would $58, almost four times cost die little twice large. computer designer remember chip costs? manufactur- ing process dictates wafer cost, wafer yield, defects per unit area, solecontrol designer die area. practice, number defects perunit area small, number good dies per wafer, therefore cost per die, grows roughly square die area. computer designer affects die size, thus cost, functions included excluded die andby number I/O pins. part ready use computer, die must tested (to separate good dies bad), packaged, tested packag-ing. steps add significant costs, increasing total half. preceding analysis focused variable costs producing functional die, appropriate high-volume integrated circuits. is, however, one important part fixed costs significantly affect cost integrated circuit low volumes (less 1 million parts), namely, costof mask set. step integrated circuit process requires separate mask.Therefore, modern high-density fabrication processes 10 metallayers, mask costs $4 million 16 nm $1.5 million 28 nm. good news semiconductor companies offer “shuttle runs ”to dramat- ically lower costs tiny test chips. lower costs putting many smalldesigns onto single die amortize mask costs, later split dies smaller pieces project. Thus TSMC delivers 80 –100 untested dies 1.57 /C21.57 mm 28 nm process $30,000 2017. Although die tiny, offer architect millions transistors play with. example, sev-eral RISC-V processors would fit die. Although shuttle runs help prototyping debugging runs, ’t address small-volume production tens hundreds thousands parts.Because mask costs likely continue increase, designers incorpo-rating reconfigurable logic enhance flexibility part thus reduce cost implications masks. Cost Versus Price commoditization computers, margin cost manufac- ture product price product sells shrinking. margins pay company ’s research development (R&D), marketing, sales, manufacturing equipment maintenance, building rental, cost financing, pretaxprofits, taxes. Many engineers surprised find companies spendonly 4% (in commodity PC business) 12% (in high-end server business)of income R&D, includes engineering.1.6 Trends Cost ■35Cost Manufacturing Versus Cost Operation first four editions book, cost meant cost build computer price meant price purchase computer. advent WSCs, containtens thousands servers, cost operate computers significant addi-tion cost purchase. Economists refer two costs capital expenses(CAPEX) operational expenses (OPEX). AsChapter 6 shows, amortized purchase price servers networks half monthly cost operate WSC, assuming short lifetimeof equipment 3 –4 years. 40% monthly operational costs power use amortized infrastructure distribute power cool equipment, despite infrastructure amortized 10 –15 years. Thus, lower operational costs WSC, computer architects need use energyefficiently. 1.7 Dependability Historically, integrated circuits one reliable components com-puter. Although pins may vulnerable, faults may occur commu-nication channels, failure rate inside chip low. conventionalwisdom changing head feature sizes 16 nm smaller, bothtransient faults permanent faults becoming commonplace, archi-tects must design systems cope challenges. section gives quickoverview issues dependability, leaving official definition terms approaches Section D.3 Appendix D. Computers designed constructed different layers abstraction. descend recursively computer seeing components enlargethemselves full subsystems run individual transistors. Althoughsome faults widespread, like loss power, many limited singlecomponent module. Thus utter failure module one level may con-sidered merely component error higher-level module. distinction ishelpful trying find ways build dependable computers. One difficult question deciding system operating properly. theoretical point became concrete popularity Internet services. Infra-structure providers started offering service level agreements (SLAs) service level objectives (SLOs) guarantee networking power service would dependable. example, would pay customer penalty notmeet agreement hours per month. Thus SLA could used decidewhether system down. Systems alternate two states service respect SLA: 1.Service accomplishment , service delivered specified. 2.Service interruption , delivered service different SLA.36 ■Chapter One Fundamentals Quantitative Design AnalysisTransitions two states caused failures (from state 1 state 2) orrestorations (2 1). Quantifying transitions leads two main mea- sures dependability: ■Module reliability measure continuous service accomplishment (or, equivalently, time failure) reference initial instant. Therefore themean time failure (MTTF) reliability measure. reciprocal MTTF rate failures, generally reported failures per billion hours operation, orFIT(forfailures time ). Thus MTTF 1,000,000 hours equals 10 9/106or 1000 FIT. Service interruption measured mean time repair (MTTR). Mean time failures (MTBF) simply sum MTTF+MTTR. Although MTBF widely used, MTTF often appropriate term. Ifa collection modules exponentially distributed lifetimes —meaning age module important probability failure —the overall failure rate collection sum failure rates modules. ■Module availability measure service accomplishment respect alternation two states accomplishment interruption. Fornonredundant systems repair, module availability Module availability ¼MTTF MTTF + MTTRðÞ Note reliability availability quantifiable metrics, rather syn- onyms dependability. definitions, estimate reliability asystem quantitatively make assumptions reliability com-ponents failures independent. Example Assume disk subsystem following components MTTF: ■10 disks, rated 1,000,000-hour MTTF ■1 ATA controller, 500,000-hour MTTF ■1 power supply, 200,000-hour MTTF ■1 fan, 200,000-hour MTTF ■1 ATA cable, 1,000,000-hour MTTF Using simplifying assumptions lifetimes exponentially distributed failures independent, compute MTTF system whole. Answer sum failure rates Failure rate system¼10/C21 1,000,000+1 500,000+1 200,000+1 200,000+1 1,000,000 ¼1 0+2+5+5+1 1,000,000 hours¼23 1,000,000¼23,000 1,000,000,000 hours1.7 Dependability ■37or 23,000 FIT. MTTF system inverse failure rate MTTF system¼1 Failure rate system¼1,000,000,000 hours 23,000¼43,500 hours 5 years. primary way cope failure redundancy, either time (repeat operation see still erroneous) resources (have components totake one failed). component replaced system fully repaired, dependability system assumed good new. Let’s quantify benefits redundancy example. Example Disk subsystems often redundant power supplies improve dependability. Using preceding components MTTFs, calculate reliability redundantpower supplies. Assume one power supply sufficient run disk subsys-tem adding one redundant power supply. Answer need formula show expect tolerate failure still provide service. simplify calculations, assume lifetimes thecomponents exponentially distributed dependency betweenthe component failures. MTTF redundant power supplies mean timeuntil one power supply fails divided chance fail thefirst one replaced. Thus, chance second failure repair small,then MTTF pair large. Since two power supplies independent failures, mean time one supply fails MTTF power supply /2.A good approximation probability second failure MTTR mean time power supply fails.Therefore reasonable approximation redundant pair power supplies MTTF power supply pair ¼MTTF power supply =2 MTTR power supply MTTF power supply¼MTTF2 power supply =2 MTTR power supply¼MTTF2 power supply 2/C2MTTR power supply Using preceding MTTF numbers, assume takes average 24 hours human operator notice power supply failed replace it, reli- ability fault tolerant pair power supplies MTTF power supply pair ¼MTTF2 power supply 2/C2MTTR power supply¼200 ,0002 2/C224ﬃ830,000,000 making pair 4150 times reliable single power supply. quantified cost, power, dependability computer technology, ready quantify performance.38 ■Chapter One Fundamentals Quantitative Design Analysis1.8 Measuring, Reporting, Summarizing Performance say one computer faster another one is, mean? user cell phone may say computer faster program runs less time,while Amazon.com administrator may say computer faster com-pletes transactions per hour. cell phone user wants reduce response time—the time start completion event —also referred execution time . operator WSC wants increase throughput —the total amount work done given time. comparing design alternatives, often want relate performance two different computers, say, X Y. phrase “X faster ”is used mean response time execution time lower X Yfor given task. particular, “Xi ntimes fast ”will mean Execution time Execution time X¼n Since execution time reciprocal performance, following relationship holds: n¼Execution time Execution time X¼1 Performance 1 Performance X¼Performance X Performance phrase “the throughput X 1.3 times fast ”signifies number tasks completed per unit time computer X 1.3 times numbercompleted Y. Unfortunately, time always metric quoted comparing perfor- mance computers. position consistent reliable measure performance execution time real programs, proposed alter-natives time metric real programs items measured even-tually led misleading claims even mistakes computer design. Even execution time defined different ways depending count. straightforward definition time called wall-clock time , response time ,o relapsed time , latency complete task, including storage accesses, memory accesses, input/output activities, operating system over- head—everything. multiprogramming, processor works another pro- gram waiting I/O may necessarily minimize elapsed time ofone program. Thus need term consider activity. CPU time recognizes distinction means time processor computing, notincluding time waiting I/O running programs. (Clearly, response time seen bythe user elapsed time program, CPU time.) Computer users routinely run programs would perfect can- didates evaluate new computer. evaluate new system, users would simply compare execution time workloads —the mixture programs1.8 Measuring, Reporting, Summarizing Performance ■39and operating system commands users run computer. happy situation, however. must rely methods evaluate computers, often evaluators, hoping methods predict performance fortheir usage new computer. One approach benchmark programs, areprograms many companies use establish relative performance theircomputers. Benchmarks best choice benchmarks measure performance real applications, asGoogle Translate mentioned Section 1.1 . Attempts running programs much simpler real application led performance pitfalls. Examplesinclude ■Kernels , small, key pieces real applications. ■Toy programs , 100-line programs beginning programming assignments, Quicksort. ■Synthetic benchmarks , fake programs invented try match profile behavior real applications, Dhrystone. three discredited today, usually compiler writer architect conspire make computer appear faster stand-in programs real applications. Regrettably authors —who dropped fallacy using synthetic benchmarks characterize performance fourth edition ofthis book since thought computer architects agreed disreputable — synthetic program Dhrystone still widely quoted benchmark forembedded processors 2017! Another issue conditions benchmarks run. One way improve performance benchmark benchmark-specific compiler flags; flags often caused transformations would illegal many programs would slow performance others. restrict pro-cess increase significance results, benchmark developers typicallyrequire vendor use one compiler one set flags programsin language (such C++ C). addition question compilerflags, another question whether source code modifications allowed. arethree different approaches addressing question: 1.No source code modifications allowed. 2.Source code modifications allowed essentially impossible. example, database benchmarks rely standard database programs tens millions lines code. database companies highly unlikely makechanges enhance performance one particular computer. 3.Source modifications allowed, long altered version produces output.40 ■Chapter One Fundamentals Quantitative Design AnalysisThe key issue benchmark designers face deciding allow modification source whether modifications reflect real practice provide useful insight users, whether changes simply reduce accuracy bench-marks predictors real performance. see Chapter 7 , domain- specific architects often follow third option creating processors forwell-defined tasks. overcome danger placing many eggs one basket, collections benchmark applications, called benchmark suites , popular measure perfor- mance processors variety applications. course, collections good constituent individual benchmarks. Nonetheless, key advan- tage suites weakness one benchmark lessened thepresence benchmarks. goal benchmark suite char-acterize real relative performance two computers, particularly programsnot suite customers likely run. cautionary example Electronic Design News Embedded Microproces- sor Benchmark Consortium (or EEMBC, pronounced “embassy ”) benchmarks. set 41 kernels used predict performance different embedded applications: automotive/industrial, consumer, networking, office automation, telecommunications. EEMBC reports unmodified performance “full fury ” performance, almost anything goes. benchmarks use smallkernels, reporting options, EEMBC reputationof good predictor relative performance different embedded computersin field. lack success Dhrystone, EEMBC trying toreplace, sadly still used. One successful attempts create standardized benchmark appli- cation suites SPEC (Standard Performance Evaluation Corporation), roots efforts late 1980s deliver better benchmarks forworkstations. computer industry evolved time, needfor different benchmark suites, SPEC benchmarks cover manyapplication classes. SPEC benchmark suites reported results arefound http://www.spec.org . Although focus discussion SPEC benchmarks many following sections, many benchmarks also developed PCs running Windows operating system. Desktop Benchmarks Desktop benchmarks divide two broad classes: processor-intensive bench- marks graphics-intensive benchmarks, although many graphics benchmarksinclude intensive processor activity. SPEC originally created benchmark set focusing processor performance (initially called SPEC89), evolved sixth generation: SPEC CPU2017, follows SPEC2006, SPEC2000,SPEC95 SPEC92, SPEC89. SPEC CPU2017 consists set 10 integerbenchmarks (CINT2017) 17 floating-point benchmarks (CFP2017).Figure 1.17 describes current SPEC CPU benchmarks ancestry.1.8 Measuring, Reporting, Summarizing Performance ■41GNU C compiler Perl interpreter Route planning General data compression Discrete Event simulation - computer network XML HTML conversion via XSLT Video compression Artificial Intelligence: alpha-beta tree search (Chess) Artificial Intelligence: Monte Carlo tree search (Go) Artificial Intelligence: recursive solution generator (Sudoku) Explosion modelingXZ omnetpp xalancbmk h264ref sjeng gobmk astar hmmer libquantum bwaves cactuBSSN namd povray lbm wrf gamess wupwise apply galgel mesa art equake facerec ammp lucas fma3d sixtrackparest blender cam4 imagick nab fotonik3d romsmilc zeusmp gromacs leslie3d dealII soplex calculix GemsFDTD tonto sphinx3mcf bzip2 vortex gzip eon twolf vortex vpr crafty parserperlgcc espresso li eqntott compress sc go ijpeg m88ksim fpppp tomcatv doduc nasa7 spice matrix300 swim su2cor wave5apsi mgrid applu turb3dhydro2dX264 deepsjeng leela exchange2 Physics: relativity Molecular dynamics Ray tracing Fluid dynamicsWeather forecasting 3D rendering animation Atmosphere modeling Image manipulation Molecular dynamics Computational Electromagnetics Regional ocean modelingBiomedical imaging: optical tomography finite elementsSPEC89 SPEC95Benchmark name SPEC generation SPEC92 SPEC2000 SPEC2006 SPEC2017 Figure 1.17 SPEC2017 programs evolution SPEC benchmarks time, integer programs line floating- point programs line. 10 SPEC2017 integer programs, 5 written C, 4 C++., 1 Fortran. floating-point programs, split 3 Fortran, 2 C++, 2 C, 6 mixed C, C++, Fortran. figure shows 82 programs 1989, 1992, 1995, 2000, 2006, 2017 releases. Gcc senior citizen group. 3 integer programs 3 floating-point programs survived three generations. Although carried generation generation, version program changes either theinput size benchmark often expanded increase running time avoid perturbation measurement domination theexecution time factor CPU time. benchmark descriptions left SPEC2017 apply earlier versions. Programs row different generations SPEC generally related; example, fpppp CFD code like bwaves.42 ■Chapter One Fundamentals Quantitative Design AnalysisSPEC benchmarks real programs modified portable minimize effect I/O performance. integer benchmarks vary part C compiler go program video compression. floating-point benchmarksinclude molecular dynamics, ray tracing, weather forecasting. SPECCPU suite useful processor benchmarking desktop systems andsingle-processor servers. see data many programs throughoutthis book. However, programs share little modern programming lan-guages environments Google Translate application Section 1.1 describes. Nearly half written least partially Fortran! even statically linked instead dynamically linked like real pro- grams. Alas, SPEC2017 applications may real, arenot inspiring. ’s clear SPECINT2017 SPECFP2017 capture exciting computing 21st century. InSection 1.11 , describe pitfalls occurred developing SPEC CPUbenchmark suite, well challenges maintaining useful pre-dictive benchmark suite. SPEC CPU2017 aimed processor performance, SPEC offers many benchmarks. Figure 1.18 lists 17 SPEC benchmarks active 2017. Server Benchmarks servers multiple functions, multiple types benchmarks. simplest benchmark perhaps processor throughput-oriented benchmark.SPEC CPU2017 uses SPEC CPU benchmarks construct simple throughputbenchmark processing rate multiprocessor measured run- ning multiple copies (usually many processors) SPEC CPU benchmark converting CPU time rate. leads measurementcalled SPECrate, measure request-level parallelism Section1.2. measure thread-level parallelism, SPEC offers call high-performance computing benchmarks around OpenMP MPI well foraccelerators GPUs (see Figure 1.18 ). SPECrate, server applications benchmarks signifi- cant I/O activity arising either storage network traffic, including bench- marks file server systems, web servers, database transaction- processing systems. SPEC offers file server benchmark (SPECSFS) anda Java server benchmark. (Appendix discusses file I/O system bench-marks detail.) SPECvirt_Sc2013 evaluates end-to-end performance virtua-lized data center servers. Another SPEC benchmark measures power, weexamine Section 1.10 . Transaction-processing (TP) benchmarks measure ability system handle transactions consist database accesses updates. Airline reserva- tion systems bank ATM systems typical simple examples TP; sophisticated TP systems involve complex databases decision-making.1.8 Measuring, Reporting, Summarizing Performance ■43In mid-1980s, group concerned engineers formed vendor-independent Transaction Processing Council (TPC) try create realistic fair benchmarksfor TP. TPC benchmarks described http://www.tpc.org . first TPC benchmark, TPC-A, published 1985 since replaced enhanced several different benchmarks. TPC-C, initially created 1992, simulates complex query environment. TPC-H models ad hoc decision support —the queries unrelated knowledge past queries cannot used optimize future queries. TPC-DI benchmark, new data integration (DI)task also known ETL, important part data warehousing. TPC-E anonline transaction processing (OLTP) workload simulates brokerage firm ’s customer accounts.Category Name Measures performance Cloud Cloud_IaaS 2016 Cloud using NoSQL database transaction K-Means clustering using map/reduce CPU CPU2017 Compute-intensive integer floating-point workloads Graphics workstation performanceSPECviewperf®12 3D graphics systems running OpenGL Direct X SPECwpc V2.0 Workstations running professional apps Windows OS SPECapcSM 3ds Max 2015 ™ 3D graphics running proprietary Autodesk 3ds Max 2015 app SPECapcSM Maya®2012 3D graphics running proprietary Autodesk 3ds Max 2012 app SPECapcSM PTC Creo 3.0 3D graphics running proprietary PTC Creo 3.0 appSPECapcSM Siemens NX 9.0 10.03D graphics running proprietary Siemens NX 9.0 10.0 app SPECapcSM SolidWorks 2015 3D graphics systems running proprietary SolidWorks 2015 CAD/CAM app High performance computingACCEL Accelerator host CPU running parallel applications using OpenCL OpenACC MPI2007 MPI-parallel, floating-point, compute-intensive programs running clusters SMPs OMP2012 Parallel apps running OpenMP Java client/server SPECjbb2015 Java serversPower SPECpower_ssj2008 Power volume server class computers running SPECjbb2015 Solution File Server (SFS)SFS2014 File server throughput response time SPECsfs2008 File servers utilizing NFSv3 CIFS protocols Virtualization SPECvirt_sc2013 Datacenter servers used virtualized server consolidation Figure 1.18 Active benchmarks SPEC 2017.44 ■Chapter One Fundamentals Quantitative Design AnalysisRecognizing controversy traditional relational databases “No SQL”storage solutions, TPCx-HS measures systems using Hadoop file system running MapReduce programs, TPC-DS measures decision support systemthat uses either relational database Hadoop-based system. TPC-VMS andTPCx-V measure database performance virtualized systems, TPC-Energyadds energy metrics existing TPC benchmarks. TPC benchmarks measure performance transactions per second. addition, include response time requirement throughput performanceis measured response time limit met. model real-world sys- tems, higher transaction rates also associated larger systems, terms users database transactions applied. Finally, thesystem cost benchmark system must included well allow accuratecomparisons cost-performance. TPC modified pricing policy thereis single specification TPC benchmarks allow verification ofthe prices TPC publishes. Reporting Performance Results guiding principle reporting performance measurements repro- ducibility —list everything another experimenter would need duplicate results. SPEC benchmark report requires extensive description com-puter compiler flags, well publication baseline andthe optimized results. addition hardware, software, baseline tuningparameter descriptions, SPEC report contains actual performance times, shown tabular form graph. TPC benchmark report even complete, must include results benchmarking audit costinformation. reports excellent sources finding real costs com-puting systems, since manufacturers compete high performance cost-performance. Summarizing Performance Results practical computer design, one must evaluate myriad design choices theirrelative quantitative benefits across suite benchmarks believed relevant.Likewise, consumers trying choose computer rely performance mea-surements benchmarks, ideally similar users ’applications. cases, useful measurements suite benchmarks theperformance important applications similar one benchmarksin suite variability performance understood. best case, suite resembles statistically valid sample application space, sample requires benchmarks typically found suitesand requires randomized sampling, essentially benchmark suite uses.1.8 Measuring, Reporting, Summarizing Performance ■45Once chosen measure performance benchmark suite, want able summarize performance results suite unique num- ber. simple approach computing summary result would compare thearithmetic means execution times programs suite. alternativewould add weighting factor benchmark use weighted arith-metic mean single number summarize performance. One approach touse weights make programs execute equal time reference com-puter, biases results toward performance characteristics ref-erence computer. Rather pick weights, could normalize execution times reference computer dividing time reference computer time thecomputer rated, yielding ratio proportional performance. SPEC uses thisapproach, calling ratio SPECRatio. particularly useful propertythat matches way benchmark computer performance throughout thistext—namely, comparing performance ratios. example, suppose SPECRatio computer benchmark 1.25 times fast computer B;then know 1:25¼SPECRatio SPECRatio B¼Execution time reference Execution time Execution time reference Execution time B¼Execution time B Execution time A¼Performance Performance B Notice execution times reference computer drop choice reference computer irrelevant comparisons made ratio, approach consistently use. Figure 1.19 gives example. SPECRatio ratio rather absolute execution time, mean must computed using geometric mean. (Because SPECRatios units, comparing SPECRatios arithmetically meaningless.) Theformula Geometric mean ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1sample ins case SPEC, sample iis SPECRatio program i. Using geometric mean ensures two important properties: 1.The geometric mean ratios ratio geometric means. 2.The ratio geometric means equal geometric mean perfor- mance ratios, implies choice reference computer isirrelevant. Therefore motivations use geometric mean substantial, especially use performance ratios make comparisons.46 ■Chapter One Fundamentals Quantitative Design AnalysisExample Show ratio geometric means equal geometric mean performance ratios reference computer SPECRatio matter. Answer Assume two computers B set SPECRatios each. Geometric mean Geometric mean B¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1SPECRatio ins ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1SPECRatio B ins ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1SPECRatio SPECRatio B ins ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Yn i¼1Execution time reference Execution time Ai Execution time reference Execution time Binvuuuuut¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃY n i¼1Execution time Bi Execution time Ains ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn i¼1Performance Ai Performance Bins is, ratio geometric means SPECRatios B geo- metric mean performance ratios B benchmarks suite. Figure 1.19 demonstrates validity using examples SPEC. BenchmarksSun Ultra Enterprise 2 time (seconds)AMD A10- 6800K time (seconds)SPEC 2006Cint ratioIntel Xeon E5-2690 time (seconds)SPEC 2006Cint ratioAMD/Intel times (seconds)Intel/AMD SPEC ratios perlbench 9770 401 24.36 261 37.43 1.54 1.54 bzip2 9650 505 19.11 422 22.87 1.20 1.20gcc 8050 490 16.43 227 35.46 2.16 2.16mcf 9120 249 36.63 153 59.61 1.63 1.63gobmk 10,490 418 25.10 382 27.46 1.09 1.09hmmer 9330 182 51.26 120 77.75 1.52 1.52sjeng 12,100 517 23.40 383 31.59 1.35 1.35libquantum 20,720 84 246.08 3 7295.77 29.65 29.65h264ref 22,130 611 36.22 425 52.07 1.44 1.44omnetpp 6250 313 19.97 153 40.85 2.05 2.05astar 7020 303 23.17 209 33.59 1.45 1.45xalancbmk 6900 215 32.09 98 70.41 2.19 2.19Geometric mean 31.91 63.72 2.00 2.00 Figure 1.19 SPEC2006Cint execution times (in seconds) Sun Ultra 5 —the reference computer SPEC2006 —and execution times SPECRatios AMD A10 Intel Xeon E5-2690. final two columns show ratios execution times SPEC ratios. figure demonstrates irrelevance reference computer relativeperformance. ratio execution times identical ratio SPEC ratios, ratio geometric means (63.7231.91/20.86 ¼2.00) identical geometric mean ratios (2.00). Section 1.11 discusses libquantum, whose performance orders magnitude higher SPEC benchmarks.1.8 Measuring, Reporting, Summarizing Performance ■471.9 Quantitative Principles Computer Design seen define, measure, summarize performance, cost, dependability, energy, power, explore guidelines principles areuseful design analysis computers. section introduces importantobservations design, well two equations evaluate alternatives. Take Advantage Parallelism Using parallelism one important methods improving perfor-mance. Every chapter book example performance enhanced exploitation parallelism. give three brief examples here, expounded later chapters. first example use parallelism system level. improve throughput performance typical server benchmark, SPECSFS TPC-C, multiple processors multiple storage devices used. workload ofhandling requests spread among processors storage devices,resulting improved throughput. able expand memory numberof processors storage devices called scalability , valuable asset servers. Spreading data across many storage devices parallel reads writes enables data-level parallelism. SPECSFS also relies request-level parallelism touse many processors, whereas TPC-C uses thread-level parallelism faster pro-cessing database queries. level individual processor, taking advantage parallelism among instructions critical achieving high performance. One simplest ways todo pipelining. (Pipelining explained detail Appendix C major focus Chapter 3 .) basic idea behind pipelining overlap instruction execution reduce total time complete instruction sequence. key insight pipelining every instruction depends immediatepredecessor, executing instructions completely partially parallel may bepossible. Pipelining best-known example ILP. Parallelism also exploited level detailed digital design. example, set-associative caches use multiple banks memory typicallysearched parallel find desired item. Arithmetic-logical units use carry-lookahead, uses parallelism speed process computing sums linear logarithmic number bits per operand. examples data-level parallelism . Principle Locality Important fundamental observations come properties programs. important program property regularly exploit principle local- ity: programs tend reuse data instructions used recently. widely held rule thumb program spends 90% execution time 10%of code. implication locality predict reasonable48 ■Chapter One Fundamentals Quantitative Design Analysisaccuracy instructions data program use near future based accesses recent past. principle locality also applies data accesses, though strongly code accesses. Two different types locality observed. Temporal locality states recently accessed items likely accessed soon. Spatial locality says items whose addresses near one another tend referenced closetogether time. see principles applied Chapter 2 . Focus Common Case Perhaps important pervasive principle computer design focus common case: making design trade-off, favor frequent case infrequent case. principle applies determining spend resources, impact improvement higher occurrence commonplace. Focusing common case works energy well resource allo- cation performance. instruction fetch decode unit processormay used much frequently multiplier, optimize first. workson dependability well. database server 50 storage devices every pro-cessor, storage dependability dominate system dependability. addition, common case often simpler done faster infrequent case. example, adding two numbers processor, expect overflow rare circumstance therefore improve performanceby optimizing common case overflow. emphasis may slowdown case overflow occurs, rare, overall performancewill improved optimizing normal case. see many cases principle throughout text. applying simple principle, decide frequent case much per-formance improved making case faster. fundamental law, called Amdahl ’sL w , used quantify principle. Amdahl ’s Law performance gain obtained improving portion com- puter calculated using Amdahl ’s Law. Amdahl ’s Law states perfor- mance improvement gained using faster mode execution islimited fraction time faster mode used. Amdahl ’s Law defines speedup gained using particular feature. speedup? Suppose make enhancement com- puter improve performance used. Speedup ratio Speedup ¼Performance entire task using enhancement possible Performance entire task without using enhancement Alternatively, Speedup ¼Execution time entire task without using enhancement Execution time entire task using enhancement possible1.9 Quantitative Principles Computer Design ■49Speedup tells us much faster task run using computer enhance- ment contrary original computer. Amdahl ’s Law gives us quick way find speedup enhance- ment, depends two factors: 1.The fraction computation time original computer con- verted take advantage enhancement —For example, 40 seconds execution time program takes 100 seconds total use anenhancement, fraction 40/100. value, call Fraction enhanced , always less equal 1. 2.The improvement gained enhanced execution mode, is, much faster task would run enhanced mode used entire pro- gram —This value time original mode time enhanced mode. enhanced mode takes, say, 4 seconds portion program,while 40 seconds original mode, improvement 40/4 10. Wecall value, always greater 1, Speedup enhanced . execution time using original computer enhanced mode time spent using unenhanced portion computer plus time spent usingthe enhancement: Execution time new¼Execution time old/C2 1/C0Fraction enhanced ðÞ +Fraction enhanced Speedupenhanced/C18/C19 overall speedup ratio execution times: Speedupoverall¼Execution time old Execution time new¼1 1/C0Fraction enhanced ðÞ +Fraction enhanced Speedupenhanced Example Suppose want enhance processor used web serving. new processor 10 times faster computation web serving application thanthe old processor. Assuming original processor busy computation40% time waiting I/O 60% time, overall speedupgained incorporating enhancement? Answer Fraction enhanced ¼0:4; Speedupenhanced ¼10; Speedupoverall¼1 0:6+0:4 10¼1 0:64/C251:56 Amdahl ’s Law expresses law diminishing returns: incremental improve- ment speedup gained improvement portion computationdiminishes improvements added. important corollary Amdahl ’s Law enhancement usable fraction task, ’t speed task reciprocal 1 minus fraction.50 ■Chapter One Fundamentals Quantitative Design AnalysisA common mistake applying Amdahl ’s Law confuse “fraction time con- verted use enhancement ”and“fraction time enhancement use .” If, instead measuring time could use enhancement compu- tation, measure time enhancement use, results incorrect! Amdahl ’s Law serve guide much enhancement improve performance distribute resources improve cost-performance. Thegoal, clearly, spend resources proportional time spent. Amdahl ’s Law particularly useful comparing overall system performance two alternatives, also applied compare two processor design alterna- tives, following example shows. Example common transformation required graphics processors square root. Imple- mentations floating-point (FP) square root vary significantly performance,especially among processors designed graphics. Suppose FP square root(FSQRT) responsible 20% execution time critical graphics bench-mark. One proposal enhance FSQRT hardware speed operationby factor 10. alternative try make FP instructions thegraphics processor run faster factor 1.6; FP instructions responsible forhalf execution time application. design team believes make FP instructions run 1.6 times faster effort required fast square root. Compare two design alternatives. Answer compare two alternatives comparing speedups: SpeedupFSQRT¼1 1/C00:2ðÞ +0:2 10¼1 0:82¼1:22 SpeedupFP¼1 1/C00:5ðÞ +0:5 1:6¼1 0:8125¼1:23 Improving performance FP operations overall slightly better higher frequency. Amdahl ’s Law applicable beyond performance. Let ’s redo reliability example page 39 improving reliability power supply via redundancy 200,000-hour 830,000,000-hour MTTF, 4150 /C2better. Example calculation failure rates disk subsystem Failure rate system¼10/C21 1,000,000+1 500,000+1 200,000+1 200,000+1 1,000,000 ¼1 0+2+5+5+1 1,000,000 hours¼23 1,000,000 hours1.9 Quantitative Principles Computer Design ■51Therefore fraction failure rate could improved 5 per million hours 23 whole system, 0.22. Answer reliability improvement would Improvementpower supply pair ¼1 1/C00:22ðÞ +0:22 4150¼1 0:78¼1:28 Despite impressive 4150 /C2improvement reliability one module, system ’s perspective, change measurable small benefit. preceding examples, needed fraction consumed new improved version; often difficult measure times directly. next section, see another way comparisons based useof equation decomposes CPU execution time three separatecomponents. know alternative affects three components,we determine overall performance. Furthermore, often possible tobuild simulators measure components hardware actuallydesigned. Processor Performance Equation Essentially computers constructed using clock running constant rate.These discrete time events called clock periods ,clocks ,cycles ,o rclock cycles . Computer designers refer time clock period duration (e.g., 1 ns) orby rate (e.g., 1 GHz). CPU time program expressed two ways: CPU time ¼CPU clock cycles program /C2Clock cycle time CPU time ¼CPU clock cycles program Clock rate addition number clock cycles needed execute program, also count number instructions executed —theinstruction path length instruction count (IC). know number clock cycles instruction count, calculate average number clock cycles per instruction (CPI). easier work with, deal simple processorsin chapter, use CPI. Designers sometimes also use instructions per clock (IPC), inverse CPI. CPI computed CPI¼CPU clock cycles program Instruction count processor figure merit provides insight different styles instruction sets implementations, use extensively next fourchapters.52 ■Chapter One Fundamentals Quantitative Design AnalysisBy transposing instruction count preceding formula, clock cycles defined IC /C2CPI. allows us use CPI execution time formula: CPU time ¼Instruction count /C2Cycles per instruction /C2Clock cycle time Expanding first formula units measurement shows pieces fit together: Instructions Program/C2Clock cycles Instruction/C2Seconds Clock cycle¼Seconds Program¼CPU time formula demonstrates, processor performance dependent upon three characteristics: clock cycle (or rate), clock cycles per instruction, instructioncount. Furthermore, CPU time equally dependent three characteristics; example, 10% improvement one leads 10% improvement CPU time. Unfortunately, difficult change one parameter complete isolation others basic technologies involved changing characteristic areinterdependent: ■Clock cycle time —Hardware technology organization ■CPI—Organization instruction set architecture ■Instruction count —Instruction set architecture compiler technology Luckily, many potential performance improvement techniques primarily enhance one component processor performance small predictable impacts theother two. designing processor, sometimes useful calculate number total processor clock cycles CPU clock cycles ¼Xn i¼1ICi/C2CPI IC irepresents number times instruction iis executed program CPI irepresents average number clocks per instruction instruction i. form used express CPU time CPU time ¼Xn i¼1ICi/C2CPI ! /C2Clock cycle time overall CPI CPI¼Xn i¼1ICi/C2CPI Instruction count¼Xn i¼1ICi Instruction count/C2CPI latter form CPI calculation uses individual CPI iand fraction occurrences instruction program (i.e., IC i/C4Instruction count). must include pipeline effects, cache misses, memory system1.9 Quantitative Principles Computer Design ■53inefficiencies, CPI ishould measured calculated table back reference manual. Consider performance example page 52, modified use measure- ments frequency instructions instruction CPI values, which,in practice, obtained simulation hardware instrumentation. Example Suppose made following measurements: Frequency FP operations ¼25% Average CPI FP operations ¼4.0 Average CPI instructions ¼1.33 Frequency FSQRT ¼2% CPI FSQRT ¼20 Assume two design alternatives decrease CPI FSQRT 2 decrease average CPI FP operations 2.5. Compare two designalternatives using processor performance equation. Answer First, observe CPI changes; clock rate instruction count remain identical. start finding original CPI neither enhancement: CPI original¼Xn i¼1CPI i/C2ICi Instruction count/C18/C19 ¼4/C225% ðÞ +1:33/C275% ðÞ ¼ 2:0 compute CPI enhanced FSQRT subtracting cycles saved original CPI: CPI new FPSQR ¼CPI original/C02%/C2CPI old FPSQR /C0CPI new FPSQR only/C0/C1 ¼2:0/C02%/C220/C02ðÞ ¼ 1:64 compute CPI enhancement FP instructions way summing FP non-FP CPIs. Using latter gives us CPI new FP¼75%/C21:33 ðÞ +2 5%/C22:5 ðÞ ¼ 1:625 Since CPI overall FP enhancement slightly lower, performance marginally better. Specifically, speedup overall FP enhancement Speedupnew FP¼CPU time original CPU time new FP¼IC/C2Clock cycle /C2CPI original IC/C2Clock cycle /C2CPI new FP ¼CPI original CPI new FP¼2:00 1:625¼1:23 Happily, obtained speedup using Amdahl ’s Law page 51.54 ■Chapter One Fundamentals Quantitative Design AnalysisIt often possible measure constituent parts processor performance equation. isolated measurements key advantage using processorperformance equation versus Amdahl ’s Law previous example. particular, may difficult measure things fraction execution time whicha set instructions responsible. practice, would probably computed summing product instruction count CPI instruc- tions set. Since starting point often individual instruction count andCPI measurements, processor performance equation incredibly useful. use processor performance equation design tool, need able measure various factors. existing processor, easy obtain exe-cution time measurement, know default clock speed. challengelies discovering instruction count CPI. processors include countersfor instructions executed clock cycles. periodically monitoring counters, also possible attach execution time instruction count seg- ments code, helpful programmers trying understand andtune performance application. Often designers programmers wantto understand performance fine-grained level available fromthe hardware counters. example, may want know CPI itis. cases, simulation techniques used like processors arebeing designed. Techniques help energy efficiency, dynamic voltage fre- quency scaling overclocking (see Section 1.5 ), make equation harder use, clock speed may vary measure program. simpleapproach turn features make results reproducible. Fortunately,as performance energy efficiency often highly correlated —taking less time run program generally saves energy —it’s probably safe consider perfor- mance without worrying impact DVFS overclocking results. 1.10 Putting Together: Performance, Price, Power “Putting Together ”sections appear near end every chapter, provide real examples use principles chapter. section, look measures performance power-performance small servers using theSPECpower benchmark. Figure 1.20 shows three multiprocessor servers evaluating along price. keep price comparison fair, Dell PowerEdge servers.The first PowerEdge R710, based Intel Xeon /C285670 micro- processor clock rate 2.93 GHz. Unlike Intel Core i7-6700 Chapters 2–5, 20 cores 40 MB L3 cache, Intel chip 22 cores 55 MB L3 cache, although cores identical. selected two- socket system —so 44 cores total —with 128 GB ECC-protected 2400 MHz DDR4 DRAM. next server PowerEdge C630, processor,number sockets, DRAM. main difference smaller rack-mountablepackage: “2U”high (3.5 inches) 730 versus “1U”(1.75 inches) 630.1.10 Putting Together: Performance, Price, Power ■55The third server cluster 16 PowerEdge 630 connected together 1 Gbit/s Ethernet switch. running Oracle Java HotSpotversion 1.7 Java Virtual Machine (JVM) Microsoft Windows Server 2012R2 Datacenter version 6.3 operating system. Note forces benchmarking (see Section 1.11 ), unusually configured servers. systems Figure 1.20 little memory rel- ative amount computation, tiny 120 GB solid-state disk. inexpensive add cores ’t need add commensurate increases mem- ory storage! Rather run statically linked C programs SPEC CPU, SPECpower uses modern software stack written Java. based SPECjbb, repre-sents server side business applications, performance measured thenumber transactions per second, called ssj_ops forserver side Java operations per second . exercises processor server, SPEC CPU, also caches, memory system, even multiprocessor interconnection system. addition, exercises JVM, including JIT runtime compiler garbage collector, well portions underlying operating system. last two rows Figure 1.20 show, performance winner cluster 16 R630s, hardly surprise since far expensive. Theprice-performance winner PowerEdge R630, barely beats cluster at213 versus 211 ssj-ops/ $. Amazingly, 16 node cluster within 1% price-performances single node despite 16 times large.System 1 System 2 System 3 Component Cost (% Cost) Cost (% Cost) Cost (% Cost) Base server PowerEdge R710 $653 (7%) PowerEdge R815 $1437 (15%) PowerEdge R815 $1437 (11%) Power supply 570 W 1100 W 1100 WProcessor Xeon X5670 $3738 (40%) Opteron 6174 $2679 (29%) Opteron 6174 $5358 (42%) Clock rate 2.93 GHz 2.20 GHz 2.20 GHz Total cores 12 24 48Sockets 2 2 4Cores/socket 6 12 12 DRAM 12 GB $484 (5%) 16 GB $693 (7%) 32 GB $1386 (11%) Ethernet Inter. Dual 1-Gbit $199 (2%) Dual 1-Gbit $199 (2%) Dual 1-Gbit $199 (2%) Disk 50 GB SSD $1279 (14%) 50 GB SSD $1279 (14%) 50 GB SSD $1279 (10%) Windows OS $2999 (32%) $2999 (33%) $2999 (24%) Total $9352 (100%) $9286 (100%) $12,658 (100%) Max ssj_ops 910,978 926,676 1,840,450Max ssj_ops/ $ 97 100 145 Figure 1.20 Three Dell PowerEdge servers measured prices July 2016. calculated cost processors subtracting cost second processor. Similarly, calculated overall cost memory seeing cost extra memory was. Hence base cost server adjusted removing estimated cost default processor memory. Chapter 5 describes multisocket systems connected together, Chapter 6 describes clusters connected together.56 ■Chapter One Fundamentals Quantitative Design AnalysisWhile benchmarks (and computer architects) care per- formance systems peak load, computers rarely run peak load. Indeed, Figure 6.2 Chapter 6 shows results measuring utilization tens thousands servers 6 months Google, less 1% operate aver-age utilization 100%. majority average utilization 10%and 50%. Thus SPECpower benchmark captures power target workloadvaries peak 10% intervals way 0%, called Active Idle. Figure 1.21 plots ssj_ops (SSJ operations/second) per watt average power target load varies 100% 0%. Intel R730 always lowest power single node R630 best ssj_ops per watt across target workload level. Since watts ¼joules/second, metric proportional SSJ operations per joule: ssj_operations =second Watt¼ssj_operations =second Joule=second¼ssj_operations Joule 050100150200250300350 02000400060008000100001200014000 100% 90% 80% 70% 60% 50% 40% 30% 20% 10% Active idle Watts Target Workloadssj_ops/wattDell 630 44 cores perf/watt Dell 730 44 cores pe rf/watt Dell 630 cluster 704 cores perf/watt Dell 630 cluster 704 cores watts/node Dell 630 44 cores watts Dell 730 44 cores watts Figure 1.21 Power-performance three servers Figure 1.20 .Ssj_ops/watt values left axis, three columns associated it, watts right axis, three lines associated it. hor-izontal axis shows target workload, varies 100% Active Idle. single node R630 bestssj_ops/watt workload level, R730 consumes lowest power level.1.10 Putting Together: Performance, Price, Power ■57To calculate single number use compare power efficiency sys- tems, SPECpower uses Overall ssj _ops=watt¼X ssj_opsX power overall ssj_ops/watt three servers 10,802 R730, 11,157 R630, 10,062 cluster 16 R630s. Therefore single node R630 best power-performance. Dividing price servers, ssj_ops/watt/$1,000 879 R730, 899 R630, 789 (per node) 16-node cluster R630s. Thus, adding power, single-node R630 still firstplace performance/price, single-node R730 significantly moreefficient 16-node cluster. 1.11 Fallacies Pitfalls purpose section, found every chapter, explainsome commonly held misbeliefs misconceptions avoid. callsuch misbeliefs fallacies. discussing fallacy, try give counterex- ample. also discuss pitfalls —easily made mistakes. Often pitfalls general- izations principles true limited context. purpose sections help avoid making errors computers design. Pitfall exponential laws must come end . first go Dennard scaling. Dennard ’s 1974 observation power density constant transistors got smaller. transistor ’s linear region shrank factor 2, current voltage also reduced factor 2,and power used fell 4. Thus chips could designed operate faster andstill use less power. Dennard scaling ended 30 years observed, notbecause transistors ’t continue get smaller integrated circuit dependability limited far current voltage could drop. threshold voltage driven low static power became significant fraction overall power. next deceleration hard disk drives. Although law disks, past 30 years maximum areal density hard drives —which deter- mines disk capacity —improved 30% –100% per year. recent years, less 5% per year. Increasing density per drive come primarilyfrom adding platters hard disk drive. Next venerable Moore ’s Law. ’s since number transistors per chip doubled every one two years. example, DRAM chip introduced 2014 contained 8B transistors, ’t 16B transistor DRAM chip mass production 2019, Moore ’s Law predicts 64B tran- sistor DRAM chip. Moreover, actual end scaling planar logic transistor even pre- dicted end 2021. Figure 1.22 shows predictions physical gate length58 ■Chapter One Fundamentals Quantitative Design Analysisof logic transistor two editions International Technology Roadmap Semiconductors (ITRS). Unlike 2013 report projected gate lengths reach 5 nm 2028, 2015 report projects length stopping 10 nmby 2021. Density improvements thereafter would come ways otherthan shrinking dimensions transistors. ’s dire ITRS suggests, companies like Intel TSMC plans shrink 3 nm gate lengths, butthe rate change decreasing. Figure 1.23 shows changes increases bandwidth time micro- processors DRAM —which affected end Dennard scaling Moore ’s Law —as well disks. slowing technology improvements apparent dropping curves. continued networking improvement isdue advances fiber optics planned change pulse amplitude modu-lation (PAM-4) allowing two-bit encoding transmit information at400 Gbit/s.0510152025 2013 2015 2017 2019 2021 2023 2024 2025 2027 2028 2030 YearPhysical gate length (nm)2013 report 2015 report Figure 1.22 Predictions logic transistor dimensions two editions ITRS report. reports started 2001, 2015 last edition, group disbanded waning interest. companiesthat produce state-of-the-art logic chips today GlobalFoundaries, Intel, Samsung, TSMC, whereas 19 first ITRS report released. four companies left, sharing plans hard sustain. IEEE Spectrum, July 2016, “Transistors stop shrinking 2021, Moore ’s Law Roadmap Predicts, ” Rachel Courtland.1.11 Fallacies Pitfalls ■59Fallacy Multiprocessors silver bullet . switch multiple processors per chip around 2005 come breakthrough dramatically simplified parallel programming made easy build multicore computers. change occurred optiondue ILP walls power walls. Multiple processors per chip guar-antee lower power; ’s certainly feasible design multicore chip uses power. potential ’s possible continue improve performance replacing high-clock-rate, inefficient core several lower-clock-rate, effi-cient cores. technology shrink transistors improves, shrink capac-itance supply voltage bit get modest increase the110100100010,000100,000 1975 1980 1985 1990 1995 2000 2005 2010 2015 2020Relative Bandwidth Improvement YearMicroprocessor MemoryNetwork Disk Figure 1.23 Relative bandwidth microprocessors, networks, memory, disks time, based data Figure 1.10 .60 ■Chapter One Fundamentals Quantitative Design Analysisnumber cores per generation. example, past years, Intel adding two cores per generation higher-end chips. see Chapters 4and5, performance programmer ’s bur- den. programmers ’La-Z-Boy era relying hardware designer make programs go faster without lifting finger officially over. programmerswant programs go faster generation, must make pro-grams parallel. popular version Moore ’s law —increasing performance gen- eration technology —is programmers. Pitfall Falling prey Amdahl ’s heartbreaking law . Virtually every practicing computer architect knows Amdahl ’s Law. Despite this, almost occasionally expend tremendous effort optimizing feature beforewe measure usage. overall speedup disappointing recallthat measured first spent much effort enhancing it! Pitfall single point failure . calculations reliability improvement using Amdahl ’s Law page 53 show dependability stronger weakest link chain. matter howmuch dependable make power supplies, example, thesingle fan limit reliability disk subsystem. Amdahl ’s Law observation led rule thumb fault-tolerant systems make sure everycomponent redundant single component failure could bring downthe whole system. Chapter 6 shows software layer avoids single points failure inside WSCs. Fallacy Hardware enhancements increase performance also improve energy efficiency, worst energy neutral . Esmaeilzadeh et al. (2011) measured SPEC2006 one core 2.67 GHz Intel Core i7 using Turbo mode ( Section 1.5 ). Performance increased factor 1.07 clock rate increased 2.94 GHz (or factor 1.10), i7used factor 1.37 joules factor 1.47 watt hours! Fallacy Benchmarks remain valid indefinitely . Several factors influence usefulness benchmark predictor real per- formance, change time. big factor influencing usefulness abenchmark ability resist “benchmark engineering ”or“benchmarketing. ” benchmark becomes standardized popular, tremendous pres-sure improve performance targeted optimizations aggressive interpre-tation rules running benchmark. Short kernels programs spendtheir time small amount code particularly vulnerable. example, despite best intentions, initial SPEC89 benchmark suite included small kernel, called matrix300, consisted eight different300/C2300 matrix multiplications. kernel, 99% execution time single line (see SPEC, 1989 ). IBM compiler optimized inner loop1.11 Fallacies Pitfalls ■61(using good idea called blocking , discussed Chapters 2and4), performance improved factor 9 prior version compiler! benchmark tested compiler tuning not, course, good indication overall perfor-mance, typical value particular optimization. Figure 1.19 shows ignore history, may forced repeat it. SPEC Cint2006 updated decade, giving compiler writers sub-stantial time hone optimizers suite. Note SPEC ratios allbenchmarks libquantum fall within range 16 –52 AMD computer 22 78 Intel. Libquantum runs 250 times faster AMD 7300 times faster Intel! “miracle ”is result optimizations Intel compiler automatically parallelizes code across 22 cores optimizesmemory using bit packing, packs together multiple narrow-range inte-gers save memory space thus memory bandwidth. drop benchmarkand recalculate geometric means, AMD SPEC Cint2006 falls 31.9 26.5and Intel 63.7 41.4. Intel computer 1.5 times fast theAMD computer instead 2.0 include libquantum, surely closer totheir real relative performances. SPECCPU2017 dropped libquantum. illustrate short lives benchmarks, Figure 1.17 page 43 lists status 82 benchmarks various SPEC releases; Gcc lone sur-vivor SPEC89. Amazingly, 70% programs SPEC2000 orearlier dropped next release. Fallacy rated mean time failure disks 1,200,000 hours almost 140 years, disks practically never fail . current marketing practices disk manufacturers mislead users. MTTF calculated? Early process, manufacturers put thousands disks room, run months, count number fail. Theycompute MTTF total number hours disks worked cumulativelydivided number failed. One problem number far exceeds lifetime disk, commonly assumed five years 43,800 hours. large MTTF makesome sense, disk manufacturers argue model corresponds user whobuys disk keeps replacing disk every 5 years —the planned lifetime disk. claim many customers (and great-grandchildren) next century, average would replace disk 27 times afailure, 140 years. useful measure percentage disks fail, called theannual failure rate . Assume 1000 disks 1,000,000-hour MTTF disks used 24 hours day. replaced failed disks newone reliability characteristics, number would fail year(8760 hours) Failed disks ¼Number disks /C2Time period MTTF¼1000 disks /C28760 hours =drive 1,000,000 hours =failure¼9 Stated alternatively, 0.9% would fail per year, 4.4% 5-year lifetime.62 ■Chapter One Fundamentals Quantitative Design AnalysisMoreover, high numbers quoted assuming limited ranges temper- ature vibration; exceeded, bets off. survey disk drives real environments ( Gray van Ingen, 2005 ) found 3% –7% drives failed per year, MTTF 125,000 –300,000 hours. even larger study found annual disk failure rates 2% –10% ( Pinheiro et al., 2007 ). Therefore real-world MTTF 2 –10 times worse manufacturer ’s MTTF. Fallacy Peak performance tracks observed performance . universally true definition peak performance “the performance level computer guaranteed exceed. ”Figure 1.24 shows percentage peak performance four programs four multiprocessors. varies 5% 58%.Since gap large vary significantly benchmark, peak perfor-mance generally useful predicting observed performance. Paratec plasma ph ysics33%54%58% 20% 6%10%54% LBMHD materials scienceCactus astroph ysicsGTC magnetic fusion0%30% 20% 10%40%50%Percentage peak performance60%70%Power4 Itanium 2 NEC earth simulator Cray X1 34% 11%34% 7%6% 6% 5%16% 11% Figure 1.24 Percentage peak performance four programs four multiprocessors scaled 64 processors. Earth Simulator X1 vector processors (see Chapter 4 Appendix G). deliver higher fraction peak performance, also highest peak performance lowest clock rates. Except forthe Paratec program, Power 4 Itanium 2 systems delivered 5% 10% peak. Oliker, L., Canning, A., Carter, J., Shalf, J., Ethier, S., 2004. Scientific computations modern parallel vector systems. In: Proc. ACM/IEEE Conf. Supercomputing, November 6 –12, 2004, Pittsburgh, Penn., p. 10.1.11 Fallacies Pitfalls ■63Pitfall Fault detection lower availability . apparently ironic pitfall computer hardware fair amount state may always critical proper operation. example, fatalif error occurs branch predictor, performance may suffer. processors try exploit ILP aggressively, operations needed correct execution program. Mukherjee et al. (2003) found less 30% operations potentially critical path theSPEC2000 benchmarks. observation true programs. register “dead”in pro- gram—that is, program write register read —then errors matter. crash program upon detection transientfault dead register, would lower availability unnecessarily. Sun Microsystems Division Oracle lived pitfall 2000 L2 cache included parity, error correction, Sun E3000 Sun E10000 systems. SRAMs used build caches intermittent faults, whichparity detected. data cache modified, processor wouldsimply reread data cache. designers protect thecache ECC (error-correcting code), operating system choice butto report error dirty data crash program. Field engineers found noproblems inspection 90% cases. reduce frequency errors, Sun modified Solaris operating sys- tem “scrub ”the cache process proactively wrote dirty data memory. processor chips enough pins add ECC, theonly hardware option dirty data duplicate external cache, using thecopy without parity error correct error. pitfall detecting faults without providing mechanism correct them. engineers unlikely design another computer without ECC onexternal caches. 1.12 Concluding Remarks chapter introduced number concepts provided quantitativeframework expand throughout book. Starting last edi-tion, energy efficiency constant companion performance. InChapter 2 , start all-important area memory system design. examine wide range techniques conspire make memory look infi- nitely large still fast possible. ( Appendix B provides introductory material caches readers without much experience background withthem.) later chapters, see hardware-software cooperation hasbecome key high-performance memory systems, high-performance pipelines. chapter also covers virtual machines, increasinglyimportant technique protection. InChapter 3 , look ILP, pipelining simplest com- mon form. Exploiting ILP one important techniques building64 ■Chapter One Fundamentals Quantitative Design Analysishigh-speed uniprocessors. Chapter 3 begins extensive discussion basic concepts prepare wide range ideas examined chap- ters. Chapter 3 uses examples span 40 years, drawing one first supercomputers (IBM 360/91) fastest processors market 2017.It emphasizes called dynamic orruntime approach exploiting ILP. also talks limits ILP ideas introduces multithreading, fur-ther developed Chapters 4and5.Appendix C provides introductory mate- rial pipelining readers without much experience background inpipelining. (We expect review many readers, including introductory text, Computer Organization Design: Hardware/Soft- ware Interface .) Chapter 4 explains three ways exploit data-level parallelism. classic oldest approach vector architecture, start lay principlesof SIMD design. (Appendix G goes greater depth vector architectures.) Wenext explain SIMD instruction set extensions found desktop micropro-cessors today. third piece in-depth explanation modern graphicsprocessing units (GPUs) work. GPU descriptions written pro- grammer ’s perspective, usually hides computer really works. section explains GPUs insider ’s perspective, including mapping GPU jargon traditional architecture terms. Chapter 5 focuses issue achieving higher performance using multiple processors, multiprocessors. Instead using parallelism overlap individualinstructions, multiprocessing uses parallelism allow multiple instruction streamsto executed simultaneously different processors. focus domi-nant form multiprocessors, shared-memory multiprocessors, though intro- duce types well discuss broad issues arise multiprocessor. explore variety techniques, focusing theimportant ideas first introduced 1980s 1990s. Chapter 6 introduces clusters goes depth WSCs, com- puter architects help design. designers WSCs professional descen-dants pioneers supercomputers, Seymour Cray, aredesigning extreme computers. WSCs contain tens thousands servers, andthe equipment building holds cost nearly $200 million. con- cerns price-performance energy efficiency earlier chapters apply WSCs, quantitative approach making decisions. Chapter 7 new edition. introduces domain-specific architectures path forward improved performance energy efficiency given theend Moore ’s Law Dennard scaling. offers guidelines build effec- tive domain-specific architectures, introduces exciting domain deep neuralnetworks, describes four recent examples take different approaches toaccelerating neural networks, compares cost-performance. book comes abundance material online (see Preface details), reduce cost introduce readers variety advancedtopics. Figure 1.25 shows all. Appendices –C, appear book, review many readers.1.12 Concluding Remarks ■65In Appendix D, move away processor-centric view discuss issues storage systems. apply similar quantitative approach, one basedon observations system behavior using end-to-end approach perfor-mance analysis. appendix addresses important issue store retrieve data efficiently using primarily lower-cost magnetic storage technol- ogies. focus examining performance disk storage systems typ-ical I/O-intensive workloads, OLTP benchmarks mentioned thischapter. extensively explore advanced topics RAID-based systems, whichuse redundant disks achieve high performance high availability.Finally, Appendix introduces queuing theory, gives basis tradingoff utilization latency. Appendix E applies embedded computing perspective ideas chapters early appendices. Appendix F explores topic system interconnect broadly, including wide area system area networks allow computers communicate. Appendix H reviews VLIW hardware software, which, contrast, less popular EPIC appeared scene last edition. Appendix describes large-scale multiprocessors use high-performance computing. Appendix J appendix remains first edition, covers computer arithmetic. Appendix K provides survey instruction architectures, including 80x86, IBM 360, VAX, many RISC architectures, including ARM,MIPS, Power, RISC-V, SPARC. Appendix L new discusses advanced techniques memory manage- ment, focusing support virtual machines design address translationAppendix Title Instruction Set Principles B Review Memory HierarchiesC Pipelining: Basic Intermediate ConceptsD Storage SystemsE Embedded SystemsF Interconnection NetworksG Vector Processors DepthH Hardware Software VLIW EPICI Large-Scale Multiprocessors Scientific ApplicationsJ Computer ArithmeticK Survey Instruction Set ArchitecturesL Advanced Concepts Address TranslationM Historical Perspectives References Figure 1.25 List appendices.66 ■Chapter One Fundamentals Quantitative Design Analysisfor large address spaces. growth cloud processors, architec- tural enhancements becoming important. describe Appendix next. 1.13 Historical Perspectives References Appendix (available online) includes historical perspectives key ideaspresented chapters text. historical perspective sectionsallow us trace development idea series machines todescribe significant projects. ’re interested examining initial develop- ment idea processor want reading, references provided end history. chapter, see Section M.2, “The Early Development Computers, ”for discussion early development digital computers performance measurement methodologies. read historical material, ’ll soon come realize one important benefits youth computing, compared many engineeringfields, pioneers still alive —we learn history simply asking them! Case Studies Exercises Diana Franklin Case Study 1: Chip Fabrication Cost Concepts illustrated case study ■Fabrication Cost ■Fabrication Yield ■Defect Tolerance Redundancy Many factors involved price computer chip. Intel spending $7b l l n complete Fab 42 fabrication facility 7 nm technology. case study, weexplore hypothetical company situation different design deci-sions involving fabrication technology, area, redundancy affect cost chips. 1.1 [10/10] <1.6>Figure 1.26 gives hypothetical relevant chip statistics influence cost several current chips. next exercises, exploring theeffect different possible design decisions Intel chips. ChipDie Size (mm2)Estimated defect rate (per cm2) NManufacturing size (nm)Transistors (billion) Cores BlueDragon 180 0.03 12 10 7.5 4 RedDragon 120 0.04 14 7 7.5 4Phoenix 8200 0.04 14 7 12 8 Figure 1.26 Manufacturing cost factors several hypothetical current future processors.Case Studies Exercises Diana Franklin ■67a.[10]<1.6>What yield Phoenix chip? b.[10]<1.6>Why Phoenix higher defect rate BlueDragon? 1.2 [20/20/20/20] <1.6>They sell range chips factory, need decide much capacity dedicate chip. Imagine willsell two chips. Phoenix completely new architecture designed 7 nm tech-nology mind, whereas RedDragon architecture 10 nm Blue- Dragon. Imagine RedDragon make profit $15 per defect-free chip. Phoenix make profit $30 per defect-free chip. wafer 450 mm diameter. a.[20]<1.6>How much profit make wafer Phoenix chips? b.[20]<1.6>How much profit make wafer RedDragon chips? c.[20]<1.6>If demand 50,000 RedDragon chips per month 25,000 Phoenix chips per month, facility fabricate 70 wafers month, many wafers make chip? 1.3 [20/20] <1.6>Your colleague AMD suggests that, since yield poor, might make chips cheaply released multiple versions samechip, different numbers cores. example, could sell Phoenix 8, Phoenix4, Phoenix2, Phoenix1, contain 8, 4, 2, 1 cores chip, respectively. eight cores defect-free, sold Phoenix8. Chips four seven defect-free cores sold Phoenix4, two three defect-free cores sold Phoenix2. simplification, calculate yield single core yield chip 1/8 area original Phoenix chip. view yield independent probability single core defect free. Calculate yield configuration probability corre-sponding number cores defect free. a.[20]<1.6>What yield single core defect free well yield Phoenix 4, Phoenix2and Phoenix1? b.[5]<1.6>Using results part a, determine chips think would worthwhile package sell, why. c.[10]<1.6>If previously cost $20 dollars per chip produce Phoenix8, cost new Phoenix chips, assuming additionalcosts associated rescuing trash? d.[20]<1.6>You currently make profit $30 defect-free Phoenix 8,a n sell Phoenix4chip $25. much profit per Phoenix8 chip consider (i) purchase price Phoenix4chips entirely profit (ii) apply profit Phoenix4chips Phoenix8chip proportion many produced? Use yields calculated part Problem 1.3a,not problem 1.1a.68 ■Chapter One Fundamentals Quantitative Design AnalysisCase Study 2: Power Consumption Computer Systems Concepts illustrated case study ■Amdahl ’s Law ■Redundancy ■MTTF ■Power Consumption Power consumption modern systems dependent variety factors, includ- ing chip clock frequency, efficiency, voltage. following exercisesexplore impact power energy different design decisions usescenarios have. 1.4 [10/10/10/10] <1.5>A cell phone performs different tasks, including stream- ing music, streaming video, reading email. tasks perform different computing tasks. Battery life overheating two common problems cellphones, reducing power energy consumption critical. problem,we consider user using phone full computingcapacity. problems, evaluate unrealistic scenario thecell phone specialized processing units. Instead, quad-core, general-purpose processing unit. core uses 0.5 W full use. email-related tasks,the quad-core 8 /C2as fast necessary. a.[10]<1.5>How much dynamic energy power required compared running full power? First, suppose quad-core operates 1/8 time idle rest time. is, clock disabled 7/8 time, leakage occurring time. Compare totaldynamic energy well dynamic power core running. b.[10]<1.5>How much dynamic energy power required using fre- quency voltage scaling? Assume frequency voltage reducedto 1/8 entire time. c.[10]<1.6, 1.9 >Now assume voltage may decrease 50% original voltage. voltage referred voltage floor , voltage lower lose state. Therefore, frequency keepdecreasing, voltage cannot. dynamic energy power savingsin case? d.[10]<1.5>How much energy used dark silicon approach? involves creating specialized ASIC hardware major task powerCase Studies Exercises Diana Franklin ■69gating elements use. one general-purpose core would provided, rest chip would filled specialized units. email, one core would operate 25% time turnedcompletely power gating 75% time. other75% time, specialized ASIC unit requires 20% energy acore would running. 1.5 [10/10/10] <1.5>As mentioned Exercise 1.4, cell phones run wide variety applications. ’ll make assumptions exercise previous one, 0.5 W per core quad core runs email 3 /C2as fast. a.[10]<1.5>Imagine 80% code parallelizable. much would frequency voltage single core need increased order exe-cute speed four-way parallelized code? b.[10]<1.5>What reduction dynamic energy using frequency voltage scaling part a? c.[10]<1.5>How much energy used dark silicon approach? approach, hardware units power gated, allowing turn entirely(causing leakage). Specialized ASICs provided perform samecomputation 20% power general-purpose processor. Imagine core power gated. video game requires two ASICS two cores. much dynamic energy require compared baselineof parallelized four cores? 1.6 [10/10/10/10/10/20] <1.5,1.9 >General-purpose processes optimized general-purpose computing. is, optimized behavior gener-ally found across large number applications. However, domain isrestricted somewhat, behavior found across large number targetapplications may different general-purpose applications. One appli-cation deep learning neural networks. Deep learning applied manydifferent applications, fundamental building block inference —using learned information make decisions —is across all. Inference operations largely parallel, currently performed graphics proces- sing units, specialized toward type computation, toinference particular. quest performance per watt, Google cre-ated custom chip using tensor processing units accelerate inference operationsin deep learning. 1This approach used speech recognition image recognition, example. problem explores trade-offs pro-cess, general-purpose processor (Haswell E5-2699 v3) GPU (NVIDIAK80), terms performance cooling .If heat removed com- puter efficiently, fans blow hot air back onto computer, cold air. Note: differences processor —on-chip memory DRAM also come play. Therefore statistics system level, chip level. 1Cite paper website: https://drive.google.com/file/d/0Bx4hafXDDq2EMzRNcy1vSUxtcEk/view .70 ■Chapter One Fundamentals Quantitative Design Analysisa.[10]<1.9>If Google ’s data center spends 70% time workload 30% time workload B running GPUs, speedup TPU system GPU system? b.[10]<1.9>If Google ’s data center spends 70% time workload 30% time workload B running GPUs, percentage Max IPS achieve three systems? c.[15]<1.5, 1.9 >Building (b), assuming power scales linearly idle busy power IPS grows 0% 100%, performance perwatt TPU system GPU system? d.[10]<1.9>If another data center spends 40% time workload A, 10% time workload B, 50% time workload C, thespeedups GPU TPU systems general-purpose system? e.[10]<1.5>A cooling door rack costs $4000 dissipates 14 kW (into room; additional cost required get room). manyHaswell-, NVIDIA-, Tensor-based servers cool one coolingdoor, assuming TDP Figures 1.27 and1.28? f.[20]<1.5>Typical server farms dissipate maximum 200 W per square foot. Given server rack requires 11 square feet (including front backclearance), many servers part (e) placed single rack, andhow many cooling doors required? System ChipThroughput % Max IPS AB C B C General-purpose Haswell E5-2699 v3 5482 13,194 12,000 42% 100% 90% Graphics processor NVIDIA K80 13,461 36,465 15,000 37% 100% 40%Custom ASIC TPU 225,000 280,000 2000 80% 100% 1% Figure 1.28 Performance characteristics general-purpose processor, graphical processing unit-based custom ASIC-based system two neural-net workloads (cite ISCA paper). Workloads B published results. Workload C fictional, general-purpose application.System Chip TDP Idle power Busy power General-purpose Haswell E5-2699 v3 504 W 159 W 455 W Graphics processor NVIDIA K80 1838 W 357 W 991 WCustom ASIC TPU 861 W 290 W 384 W Figure 1.27 Hardware characteristics general-purpose processor, graphical processing unit-based custom ASIC-based system, including measured power (cite ISCA paper).Case Studies Exercises Diana Franklin ■71Exercises 1.7 [10/15/15/10/10] <1.4, 1.5 >One challenge architects design created today require several years implementation, verification, testing beforeappearing market. means architect must project tech-nology like several years advance. Sometimes, difficult do. a.[10]<1.4>According trend device scaling historically observed Moore ’s Law, number transistors chip 2025 many times number 2015? b.[15]<1.5>The increase performance mirrored trend. perfor- mance continued climb rate 1990s, approximately whatperformance would chips VAX-11/780 2025? c.[15]<1.5>At current rate increase mid-2000s, updated projection performance 2025? d.[10]<1.4>What limited rate growth clock rate, architects extra transistors increase performance? e.[10]<1.4>The rate growth DRAM capacity also slowed down. 20 years, DRAM capacity improved 60% year. 8 Gbit DRAM wasfirst available 2015, 16 Gbit available 2019, cur-rent DRAM growth rate? 1.8 [10/10] <1.5>You designing system real-time application specific deadlines must met. Finishing computation faster gains nothing.You find system execute necessary code, worst case, twiceas fast necessary. a.[10]<1.5>How much energy save execute current speed turn system computation complete? b.[10]<1.5>How much energy save set voltage frequency half much? 1.9 [10/10/20/20] <1.5>Server farms Google Yahoo! provide enough compute capacity highest request rate day. Imagine ofthe time servers operate 60% capacity. Assume power scale linearly load; is, servers operating 60% capacity, consume 90% maximum power. servers could turned off,but would take long restart response load. new system hasbeen proposed allows quick restart requires 20% maximumpower “barely alive ”state. a.[10]<1.5>How much power savings would achieved turning 60% servers? b.[10]<1.5>How much power savings would achieved placing 60% servers “barely alive ”state?72 ■Chapter One Fundamentals Quantitative Design Analysisc.[20]<1.5>How much power savings would achieved reducing volt- age 20% frequency 40%? d.[20]<1.5>How much power savings would achieved placing 30% servers “barely alive ”state 30% off? 1.10 [10/10/20] <1.7>Availability important consideration designing servers, followed closely scalability throughput. a.[10]<1.7>We single processor failure time (FIT) 100. mean time failure (MTTF) system? b.[10]<1.7>If takes one day get system running again, avail- ability system? c.[20]<1.7>Imagine government, cut costs, going build super- computer inexpensive computers rather expensive, reliable com-puters. MTTF system 1000 processors? Assume one fails, fail. 1.11 [20/20/20] <1.1, 1.2, 1.7 >In server farm used Amazon eBay, single failure cause entire system crash. Instead, reduce number requests satisfied one time. a.[20]<1.7>If company 10,000 computers, MTTF 35 days, experiences catastrophic failure 1/3 computers fail, isthe MTTF system? b.[20]<1.1, 1.7 >If costs extra $1000, per computer, double MTTF, would good business decision? Show work. c.[20]<1.2>Figure 1.3 shows, average, cost downtimes, assuming cost equal times year. retailers, however, Christmasseason profitable (and therefore costly time lose sales). Ifa catalog sales center twice much traffic fourth quarter everyother quarter, average cost downtime per hour fourthquarter rest year? 1.12 [20/10/10/10/15] <1.9>In exercise, assume considering enhanc- ing quad-core machine adding encryption hardware it. computingencryption operations, 20 times faster normal mode execution.We define percentage encryption percentage time original execution spent performing encryption operations. specialized hard- ware increases power consumption 2%. a.[20]<1.9>Draw graph plots speedup percentage compu- tation spent performing encryption. Label y-axis “Net speedup ”and label x-axis “ Percent encryption. ” b.[10]<1.9>With percentage encryption adding encryption hard- ware result speedup 2? c.[10]<1.9>What percentage time new execution spent encryption operations speedup 2 achieved?Case Studies Exercises Diana Franklin ■73d.[15]<1.9>Suppose measured percentage encryption 50%. hardware design group estimates speed encryption hard- ware even significant additional investment. wonder whetheradding second unit order support parallel encryption operations wouldbe useful. Imagine original program, 90% encryptionoperations could performed parallel. speedup providingtwo four encryption units, assuming parallelization allowed limitedto number encryption units? 1.13 [15/10] <1.9>Assume make enhancement computer improves mode execution factor 10. Enhanced mode used 50% time,measured percentage execution time enhanced mode use . Recall Amdahl ’s Law depends fraction original, unenhanced exe- cution time could make use enhanced mode. Thus cannot directly use 50% measurement compute speedup Amdahl ’s Law. a.[15]<1.9>What speedup obtained fast mode? b.[10]<1.9>What percentage original execution time converted fast mode? 1.14 [20/20/15] <1.9>When making changes optimize part processor, often case speeding one type instruction comes cost slowing something else. example, put complicated fast floating-point unit,that takes space, something might moved farther away themiddle accommodate it, adding extra cycle delay reach unit. Thebasic Amdahl ’s Law equation take account trade-off. a.[20]<1.9>If new fast floating-point unit speeds floating-point opera- tions by, average, 2x, floating-point operations take 20% originalprogram ’s execution time, overall speedup (ignoring penalty instructions)? b.[20]<1.9>Now assume speeding floating-point unit slowed data cache accesses, resulting 1.5x slowdown (or 2/3 speedup). Data cacheaccesses consume 10% execution time. overall speedup now? c.[15]<1.9>After implementing new floating-point operations, per- centage execution time spent floating-point operations? percent-age spent data cache accesses? 1.15 [10/10/20/20] <1.10>Your company bought new 22-core processor, tasked optimizing software processor.You run four applications system, resource requirements equal. Assume system application characteristics listed Table 1.1 . Table 1.1 Four applications Application B C % resources needed 41 27 18 14 % parallelizable 50 80 60 9074 ■Chapter One Fundamentals Quantitative Design AnalysisThe percentage resources assuming run serial. Assume parallelize portion program X, speedup portion X. a.[10]<1.10>How much speedup would result running application entire 22-core processor, compared running serially? b.[10]<1.10>How much speedup would result running application entire 22-core processor, compared running serially? c.[20]<1.10>Given application requires 41% resources, stat- ically assign 41% cores, overall speedup run paral- lelized everything else run serially? d.[20]<1.10>What overall speedup four applications statically assigned cores, relative percentage resource needs, andall run parallelized? e.[10]<1.10>Given acceleration parallelization, new percentage resources applications receiving, considering active time ontheir statically-assigned cores? 1.16 [10/20/20/20/25] <1.10>When parallelizing application, ideal speedup speeding number processors. limited two things: percentageof application parallelized cost communication.Amdahl ’s Law takes account former latter. a.[10]<1.10>What speedup Nprocessors 80% application parallelizable, ignoring cost communication? b.[20]<1.10>What speedup eight processors if, every processor added, communication overhead 0.5% original execution time. c.[20]<1.10>What speedup eight processors if, every time number processors doubled, communication overhead increasedby 0.5% original execution time? d.[20]<1.10>What speedup Nprocessors if, every time num- ber processors doubled, communication overhead increased 0.5%of original execution time? e.[25]<1.10>Write general equation solves question: number processors highest speedup application P% ofthe original execution time parallelizable, and, every time number ofprocessors doubled, communication increased 0.5% originalexecution time?Case Studies Exercises Diana Franklin ■752.1 Introduction 78 2.2 Memory Technology Optimizations 84 2.3 Ten Advanced Optimizations Cache Performance 94 2.4 Virtual Memory Virtual Machines 118 2.5 Cross-Cutting Issues: Design Memory Hierarchies 126 2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 129 2.7 Fallacies Pitfalls 142 2.8 Concluding Remarks: Looking Ahead 146 2.9 Historical Perspectives References 148 Case Studies Exercises Norman P. Jouppi, Rajeev Balasubramonian, Naveen Muralimanohar,and Sheng Li 1482 Memory Hierarchy Design Ideally one would desire indefinitely large memory capacity particular …word would immediately available … are…forced recognize possibility constructing hierarchy memories greater capacity preceding less quickly accessible. A. W. Burks, H. H. Goldstine, J. von Neumann, Preliminary Discussion Logical Design Electronic Computing Instrument (1946). Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00002-X ©2019 Elsevier Inc. rights reserved.2.1 Introduction Computer pioneers correctly predicted programmers would want unlimited amounts fast memory. economical solution desire memory hierar-chy, takes advantage locality trade-offs cost-performance ofmemory technologies. principle locality, presented first chapter, saysthat programs access code data uniformly. Locality occurs time(temporal locality) space (spatial locality). principle plus guideline given implementation technology power budget, smaller hardware made faster led hierarchies based memories different speeds andsizes. Figure 2.1 shows several different multilevel memory hierarchies, including typical sizes speeds access. Flash next generation memory technol-ogies continue close gap disks cost per bit, technologies likelyto increasingly replace magnetic disks secondary storage. Figure 2.1 shows, technologies already used many personal computers increasingly inservers, advantages performance, power, density significant. fast memory expensive, memory hierarchy organized several levels —each smaller, faster, expensive per byte next lower level, farther processor. goal provide memorysystem cost per byte almost low cheapest level memoryand speed almost fast fastest level. cases (but all), datacontained lower level superset next higher level. property,called inclusion property , always required lowest level hierar- chy, consists main memory case caches secondary storage (disk Flash) case virtual memory. importance memory hierarchy increased advances per- formance processors. Figure 2.2 plots single processor performance projections historical performance improvement time access main memory.The processor line shows increase memory requests per second average(i.e., inverse latency memory references), memory lineshows increase DRAM accesses per second (i.e., inverse DRAMaccess latency), assuming single DRAM single memory bank. reality complex processor request rate uniform, memory system typically multiple banks DRAMs channels. Although gap inaccess time increased significantly many years, lack significant perfor-mance improvement single processors led slowdown growth ofthe gap processors DRAM. high-end processors multiple cores, bandwidth requirements greater single cores. Although single-core bandwidth grown moreslowly recent years, gap CPU memory demand DRAM band- width continues grow numbers cores grow. modern high-end desktop processor Intel Core i7 6700 generate two data memory referencesper core clock cycle. four cores 4.2 GHz clock rate, i7 cangenerate peak 32.8 billion 64-bit data memory references per second, addi-tion peak instruction demand 12.8 billion 128-bit instruction78 ■Chapter Two Memory Hierarchy DesignSize: Speed:4– 64 GB 25 – 50 us1– 2 GB 50 –100 ns256 KB 5-10 ns64 KB 1 ns1000 bytes 300 ps Memory hierarchy laptop desktopLaptop DesktopLevel 2 Cache referenceLevel 1 Cache referenceRegister referenceMemory referenceFlash memory referenceCPU RegistersMemory StorageMemory busL1 C c heL2 C c Memory hierarchy personal mobile device 256 KB 3–10 ns64 KB 1 nsSize: Speed:256 GB-1 TB 50-100 uS4 –16 GB 50 –100 ns4-8 MB 10 – 20 ns1000 bytes 300 psLevel 1 Cache referenceRegister referenceMemory referenceFlash memory referenceLevel 2 Cache referenceLevel 3 Cache referenceCPU RegistersMemory StorageMemory busL1 C c h eL2 C c h eL3 C c h e 256 KB 3–10 ns64 KB 1 nsSize: Speed:256 GB-2 TB 50-100 uS8–64 GB 50 –100 ns8-32 MB 10 – 20 ns2000 bytes 300 ps Memory hierarchy serverSize: Speed:16–64 TB 5 –10 ms32–256 GB 50 –100 ns16-64 MB 10 – 20 ns256 KB 3–10 ns64 KB 1 ns4000 bytes 200 psLevel 1 Cache referenceRegister referenceMemory reference Disk memory referenceLevel 2 Cache referenceLevel 3 Cache referenceCPU RegistersMemoryDisk storage I/O busMemory busL1 C c h eL2 C c h eL3 C c h e Flash storage 1-16 TB 100-200 usFlash memory reference(A) (B) (C) Figure 2.1 levels typical memory hierarchy personal mobile device (PMD), cell phone tablet (A), laptop desktop computer (B), server (C). move farther away processor, memory level becomes slower larger. Note time units change factor 109from pico- seconds milliseconds case magnetic disks size units change factor 1010from thou- sands bytes tens terabytes. add warehouse-sized computers, opposed servers, capacity scale would increase three six orders magnitude. Solid-state drives (SSDs) composed Flash areused exclusively PMDs, heavily laptops desktops. many desktops, primary storage system SSD, expansion disks primarily hard disk drives (HDDs). Likewise, many servers mix SSDs HDDs.2.1 Introduction ■79references; total peak demand bandwidth 409.6 GiB/s! incredible bandwidth achieved multiporting pipelining caches; using threelevels caches, two private levels per core shared L3; usinga separate instruction data cache first level. contrast, peak band-width DRAM main memory, using two memory channels, 8% thedemand bandwidth (34.1 GiB/s). Upcoming versions expected L4 DRAM cache using embedded stacked DRAM (see Sections 2.2 and2.3). Traditionally, designers memory hierarchies focused optimizing average memory access time, determined cache access time, miss rate, andmiss penalty. recently, however, power become major consideration. Inhigh-end microprocessors, may 60 MiB on-chip cache, alarge second- third-level cache consume significant power leakagewhen operating (called static power ) active power, performing read write (called dynamic power ), described Section 2.3 . problem even acute processors PMDs CPU less aggressive power budget may 20 50 times smaller. cases, caches accountfor 25% 50% total power consumption. Thus designs must considerboth performance power trade-offs, examine chapter.1100 101000Performance10,000100,000 2010 2005 1980 2000 1995 YearProcessor Memory 1990 1985 2015 Figure 2.2 Starting 1980 performance baseline, gap performance, measured difference time processor memory requests (for single processor core) latency DRAM access, plotted time. mid-2017, AMD, Intel Nvidia announced chip sets using versions HBMtechnology. Note vertical axis must logarithmic scale record size processor-DRAM performance gap. memory baseline 64 KiB DRAM 1980, 1.07 per year performance improvement latency (see Figure 2.4 page 88). processor line assumes 1.25 improvement per year 1986, 1.52 improve-ment 2000, 1.20 improvement 2000 2005, small improve- ments processor performance (on per-core basis) 2005 2015. see, 2010 memory access times DRAM improved slowly consistently;since 2010 improvement access time reduced, compared earlier periods, although continued improvements bandwidth. See Figure 1.1 Chapter 1 information.80 ■Chapter Two Memory Hierarchy DesignBasics Memory Hierarchies: Quick Review increasing size thus importance gap led migration basics memory hierarchy undergraduate courses computer architecture,and even courses operating systems compilers. Thus ’ll start quick review caches operation. bulk chapter, however,describes advanced innovations attack processor —memory performance gap. word found cache, word must fetched lower level hierarchy (which may another cache main memory) placed cache continuing. Multiple words, called block (orline), moved efficiency reasons, likely needed soondue spatial locality. cache block includes tagto indicate memory address corresponds to. key design decision blocks (or lines) placed cache. popular scheme set associative , setis group blocks cache. block first mapped onto set, block placed any-where within set. Finding block consists first mapping block address set searching set —usually parallel —to find block. set chosen address data: Block addressðÞ MOD Number sets cacheðÞ nblocks set, cache placement called n-way set associative . end points set associativity names. direct-mapped cache one block per set (so block always placed location), fully associative cache one set (so block placed anywhere). Caching data read easy copy cache mem- ory identical. Caching writes difficult; example, thecopy cache memory kept consistent? two main strategies.Awrite-through cache updates item cache andwrites update main memory. write-back cache updates copy cache. block replaced, copied back memory. write strategies canuse write buffer allow cache proceed soon data placed buffer rather wait full latency write data memory. One measure benefits different cache organizations miss rate. Miss rate simply fraction cache accesses result miss —that is, number accesses miss divided number accesses. gain insights causes high miss rates, inspire better cache designs, three Cs model sorts misses three simple categories: ■Compulsory —The first access block cannot cache, block must brought cache. Compulsory misses occur even infinite-sized cache. ■Capacity —If cache cannot contain blocks needed execution program, capacity misses (in addition compulsory misses) occurbecause blocks discarded later retrieved.2.1 Introduction ■81■Conflict —If block placement strategy fully associative, conflict mis- ses (in addition compulsory capacity misses) occur block may discarded later retrieved multiple blocks map set andaccesses different blocks intermingled. Figure B.8 page 24 shows relative frequency cache misses broken three Cs. mentioned Appendix B , three C ’s model conceptual, although insights usually hold, definitive model explaining thecache behavior individual references. see Chapters 3and5, multithreading multiple cores add com- plications caches, increasing potential capacity misses well asadding fourth C, coherency misses due cache flushes keep multiple caches coherent multiprocessor; consider issues Chapter 5 . However, miss rate misleading measure several reasons. Therefore designers prefer measuring misses per instruction rather misses per memory reference (miss rate). two related: Misses Instruction¼Miss rate /C2Memory accesses Instruction count¼Miss rate /C2Memory accesses Instruction (This equation often expressed integers rather fractions, misses per 1000 instructions.) problem measures ’t factor cost miss. better measure average memory access time , Average memory access time ¼Hit time + Miss rate /C2Miss penalty hit time time hit cache miss penalty time replace block memory (that is, cost miss). Average memory access time isstill indirect measure performance; although better measure missrate, substitute execution time. Chapter 3 see specu- lative processors may execute instructions miss, thereby reducing effective miss penalty. use multithreading (introduced Chapter 3 ) also allows processor tolerate misses without forced idle. exam-ine shortly, take advantage latency tolerating techniques, need cachesthat service requests handling outstanding miss. material new you, quick review moves quickly, see Appendix B . covers introductory material depth includes examples caches real computers quantitative evaluations theireffectiveness. Section B.3 Appendix B presents six basic cache optimizations, quickly review here. appendix also gives quantitative examples benefitsof optimizations. also comment briefly power implications ofthese trade-offs. 1.Larger block size reduce miss rate —The simplest way reduce miss rate take advantage spatial locality increase block size. Larger blocks82 ■Chapter Two Memory Hierarchy Designreduce compulsory misses, also increase miss penalty. larger blocks lower number tags, slightly reduce static power. Larger block sizes also increase capacity conflict misses, especially insmaller caches. Choosing right block size complex trade-off thatdepends size cache miss penalty. 2.Bigger caches reduce miss rate —The obvious way reduce capacity misses increase cache capacity. Drawbacks include potentially longer hit time ofthe larger cache memory higher cost power. Larger caches increaseboth static dynamic power. 3.Higher associativity reduce miss rate —Obviously, increasing associativity reduces conflict misses. Greater associativity come cost increasedhit time. see shortly, associativity also increases power consumption. 4.Multilevel caches reduce miss penalty —A difficult decision whether make cache hit time fast, keep pace high clock rate proces-sors, make cache large reduce gap processoraccesses main memory accesses. Adding another level cache original cache memory simplifies decision. first-level cache small enough match fast clock cycle time, yet second-level(or third-level) cache large enough capture many accesses wouldgo main memory. focus misses second-level caches leads largerblocks, bigger capacity, higher associativity. Multilevel caches morepower-efficient single aggregate cache. L1 L2 refer, respectively,to first- second-level caches, redefine average memory accesstime: Hit time L1+ Miss rate L1/C2Hit time L2+ Miss rate L2/C2Miss penaltyL2 ðÞ 5.Giving priority read misses writes reduce miss penalty —A write buffer good place implement optimization. Write buffers create haz- ards hold updated value location needed read miss — is, read-after-write hazard memory. One solution check thecontents write buffer read miss. conflicts, thememory system available, sending read writes reduces misspenalty. processors give reads priority writes. choice littleeffect power consumption. 6.Avoiding address translation indexing cache reduce hit time — Caches must cope translation virtual address processor toa physical address access memory. (Virtual memory covered inSections 2.4 B.4.) common optimization use page offset —the part identical virtual physical addresses —to index cache, described Appendix B , page B.38. virtual index/physical tag method introduces system complications and/or limitations size struc-ture L1 cache, advantages removing translation lookasidebuffer (TLB) access critical path outweigh disadvantages.2.1 Introduction ■83Note preceding six optimizations potential disadvantage lead increased, rather decreased, average memory access time. rest chapter assumes familiarity preceding material details Appendix B . “Putting Together ”section, examine memory hierarchy microprocessor designed high-end desktop smallerserver, Intel Core i7 6700, well one designed use PMD, ArmCortex-53, basis processor used several tablets smart-phones. Within classes, significant diversity approachbecause intended use computer. Although i7 6700 cores bigger caches Intel proces- sors designed mobile uses, processors similar architectures. proces-sor designed small servers, i7 6700, larger servers, theIntel Xeon processors, typically running large number concurrent processes,often different users. Thus memory bandwidth becomes important, andthese processors offer larger caches aggressive memory systems boostthat bandwidth. contrast, PMDs serve one user generally also smaller oper- ating systems, usually less multitasking (running several applications simulta- neously), simpler applications. PMDs must consider performance andenergy consumption, determines battery life. dive moreadvanced cache organizations optimizations, one needs understand thevarious memory technologies evolving. 2.2 Memory Technology Optimizations …the one single development put computers feet invention reliable form memory, namely, core memory. …Its cost reasonable, reliable and, reliable, could duecourse made large. (p. 209) Maurice Wilkes. Memoirs Computer Pioneer (1985) section describes technologies used memory hierarchy, specifically building caches main memory. technologies SRAM (static random- access memory), DRAM (dynamic random-access memory), Flash. last used alternative hard disks, characteristics basedon semiconductor technology, appropriate include section. Using SRAM addresses need minimize access time caches. cache miss occurs, however, need move data main memory asquickly possible, requires high bandwidth memory. high memorybandwidth achieved organizing many DRAM chips make themain memory multiple memory banks making memory bus wider, both. allow memory systems keep bandwidth demands modern processors, memory innovations started happening inside DRAM chips84 ■Chapter Two Memory Hierarchy Designthemselves. section describes technology inside memory chips innovative, internal organizations. describing technologies options, need introduce terminology. introduction burst transfer memories, widely used Flash DRAM, memory latency quoted using two measures —access time cycle time. Access time time read requested desired word arrives, cycle time minimum time unrelated requests memory. Virtually computers since 1975 used DRAMs main memory SRAMs cache, one three levels integrated onto processor chip CPU. PMDs must balance power performance, havemore modest storage needs, PMDs use Flash rather disk drives, decisionincreasingly followed desktop computers well. SRAM Technology first letter SRAM stands static . dynamic nature circuits DRAM requires data written back read —thus difference access time cycle time well need refresh. SRAMsdon’t need refresh, access time close cycle time. SRAMs typically use six transistors per bit prevent information disturbed read. SRAM needs minimal power retain charge standby mode. earlier times, desktop server systems used SRAM chips primary, secondary, tertiary caches. Today, three levels caches inte-grated onto processor chip. high-end server chips, may manyas 24 cores 60 MiB cache; systems often configured with128–256 GiB DRAM per processor chip. access times large, third-level, on-chip caches typically two eight times second-level cache. Evenso, L3 access time usually least five times faster DRAM access. On-chip, cache SRAMs normally organized width matches block size cache, tags stored parallel block. allows anentire block read written single cycle. capability partic-ularly useful writing data fetched miss cache writingback block must evicted cache. access time cache(ignoring hit detection selection set associative cache) proportionalto number blocks cache, whereas energy consumption dependsboth number bits cache (static power) number blocks (dynamic power). Set associative caches reduce initial access time mem- ory size memory smaller, increase time hit detectionand block selection, topic cover Section 2.3 . DRAM Technology early DRAMs grew capacity, cost package necessary address lines issue. solution multiplex address lines, thereby2.2 Memory Technology Optimizations ■85cutting number address pins half. Figure 2.3 shows basic DRAM orga- nization. One-half address sent first row access strobe (RAS). half address, sent column access strobe (CAS), follows it. names come internal chip organization, memory organized rectangular matrix addressed rows columns. additional requirement DRAM derives property signified first letter, D,f r dynamic . pack bits per chip, DRAMs use single transistor, effectively acts capacitor, store bit. two implica-tions: first, sensing wires detect charge must precharged, setsthem “halfway ”between logical 0 1, allowing small charge stored cell cause 0 1 detected sense amplifiers. reading, row placedinto row buffer, CAS signals select portion row read DRAM. reading row destroys information, must written back row longer needed. write back happens overlapped fashion, butin early DRAMs, meant cycle time new row could read waslarger time read row access portion row. addition, prevent loss information charge cell leaks away (assuming read written), bit must “refreshed ”periodically. For- tunately, bits row refreshed simultaneously reading thatrow writing back. Therefore every DRAM memory system must access every row within certain time window, 64 ms. DRAM controllers include hardware refresh DRAMs periodically. requirement means memory system occasionally unavailable sending signal telling every chip refresh. time refreshis row activation precharge also writes row back (which takesColumn Rd/Wr PreAct RowBank Figure 2.3 Internal organization DRAM. Modern DRAMs organized banks, 16 DDR4. bank consists series rows. Sending ACT (Activate)command opens bank row loads row row buffer. row buffer, transferred successive column addresses whatever width DRAM (typically 4, 8, 16 bits DDR4) specifying block trans-fer starting address. Precharge commend (PRE) closes bank row readies new access. command, well block transfers, synchro- nized clock. See next section discussing SDRAM. row column signalsare sometimes called RAS CAS, based original names signals.86 ■Chapter Two Memory Hierarchy Designroughly 2/3 time get datum column select needed), required row DRAM. memory matrix DRAM conceptually square, number steps refresh usually square root ofthe DRAM capacity. DRAM designers try keep time spent refreshing lessthan 5% total time. far presented main memory operatedlike Swiss train, consistently delivering goods exactly according schedule.In fact, SDRAMs, DRAM controller (usually processor chip) tries tooptimize accesses avoiding opening new rows using block transfer whenpossible. Refresh adds another unpredictable factor. Amdahl suggested rule thumb memory capacity grow linearly processor speed keep balanced system. Thus 1000 MIPS processor shouldhave 1000 MiB memory. Processor designers rely DRAMs supply thatdemand. past, expected fourfold improvement capacity every threeyears, 55% per year. Unfortunately, performance DRAMs growing amuch slower rate. slower performance improvements arise primarily ofsmaller decreases row access time, determined issues aspower limitations charge capacity (and thus size) individual mem- ory cell. discuss performance trends detail, need describe major changes occurred DRAMs starting mid-1990s. Improving Memory Performance Inside DRAM Chip: SDRAMs Although early DRAMs included buffer allowing multiple column accesses single row, without requiring new row access, used asynchronous interface, meant every column access transfer involved overheadto synchronize controller. mid-1990s, designers added clock sig-nal DRAM interface repeated transfers would bear over-head, thereby creating synchronous DRAM (SDRAM). addition reducing overhead, SDRAMs allowed addition burst transfer mode multipletransfers occur without specifying new column address. Typically, eight ormore 16-bit transfers occur without sending new addresses placing DRAM burst mode. inclusion burst mode transfers meant significant gap bandwidth stream random accessesversus access block data. overcome problem getting bandwidth memory DRAM density increased, DRAMS made wider. Initially, offered afour-bit transfer mode; 2017, DDR2, DDR3, DDR DRAMS 4, 8,or 16 bit buses. early 2000s, innovation introduced: double data rate (DDR), allows DRAM transfer data rising falling edge memory clock, thereby doubling peak data rate. Finally, SDRAMs introduced banks help power management, improve access time, allow interleaved overlapped accesses different banks.2.2 Memory Technology Optimizations ■87Access different banks overlapped other, bank row buffer. Creating multiple banks inside DRAM effectively adds another segment address, consists bank number, row address, col-umn address. address sent designates new bank, bank mustbe opened, incurring additional delay. management banks rowbuffers completely handled modern memory control interfaces, whena subsequent access specifies row open bank, access happenquickly, sending column address. initiate new access, DRAM controller sends bank row number (called Activate SDRAMs formerly called RAS —row select). com- mand opens row reads entire row buffer. column addresscan sent, SDRAM transfer one data items, dependingon whether single item request burst request. accessing newrow, bank must precharged. row bank, pre-charge delay seen; however, row another bank, closing rowand precharging overlap accessing new row. synchronous DRAMs,each command cycles requires integral number clock cycles. 1980 1995, DRAMs scaled Moore ’s Law, doubling capacity every 18 months (or factor 4 3 years). mid-1990s 2010, capacityincreased slowly roughly 26 months doubling. 2010 to2016, capacity doubled! Figure 2.4 shows capacity access time various generations DDR SDRAMs. DDR1 DDR3, access timesimproved factor 3, 7% per year. DDR4 improves powerand bandwidth DDR3, similar access latency. AsFigure 2.4 shows, DDR sequence standards. DDR2 lowers power DDR1 dropping voltage 2.5 1.8 V offers higher clock rates: 266, 333, 400 MHz. DDR3 drops voltage 1.5 V maximumclock speed 800 MHz. (As discuss next section, GDDR5 graphics Best case access time (no precharge) Precharge needed Production year Chip size DRAM type RAS time (ns) CAS time (ns) Total (ns) Total (ns) 2000 256M bit DDR1 21 21 42 63 2002 512M bit DDR1 15 15 30 452004 1G bit DDR2 15 15 30 452006 2G bit DDR2 10 10 20 302010 4G bit DDR3 13 13 26 392016 8G bit DDR4 13 13 26 39 Figure 2.4 Capacity access times DDR SDRAMs year production. Access time random memory word assumes new row must opened. row different bank, assume bank precharged;if row open, precharge required, access time longer. number banks increased, ability hide precharge time also increased. DDR4 SDRAMs initially expected 2014, begin production early 2016.88 ■Chapter Two Memory Hierarchy DesignRAM based DDR3 DRAMs.) DDR4, shipped volume early 2016, expected 2014, drops voltage 1 –1.2 V maximum expected clock rate 1600 MHz. DDR5 unlikely reach production quantities 2020 later. introduction DDR, memory designers increasing focused band- width, improvements access time difficult. Wider DRAMs, bursttransfers, double data rate contributed rapid increases memory band-width. DRAMs commonly sold small boards called dual inline memory modules (DIMMs) contain 4 –16 DRAM chips normally organized 8 bytes wide (+ ECC) desktop server systems. DDR SDRAMs packaged DIMMs, confusingly labeled peak DIMM band- width. Therefore DIMM name PC3200 comes 200 MHz /C22/C28 bytes, 3200 MiB/s; populated DDR SDRAM chips. Sustaining confusion,the chips labeled number bits per second rather clock rate, 200 MHz DDR chip called DDR400. Figure 2.5 shows relationships ’I/O clock rate, transfers per second per chip, chip bandwidth, chip name, DIMM bandwidth, DIMM name. Reducing Power Consumption SDRAMs Power consumption dynamic memory chips consists dynamic powerused read write static standby power; depend operatingvoltage. advanced DDR4 SDRAMs, operating voltage droppedto 1.2 V, significantly reducing power versus DDR2 DDR3 SDRAMs. Theaddition banks also reduced power row single bank read.Standard I/O clock rate transfers/s DRAM name MiB/s/DIMM DIMM name DDR1 133 266 DDR266 2128 PC2100 DDR1 150 300 DDR300 2400 PC2400DDR1 200 400 DDR400 3200 PC3200DDR2 266 533 DDR2-533 4264 PC4300DDR2 333 667 DDR2-667 5336 PC5300DDR2 400 800 DDR2-800 6400 PC6400DDR3 533 1066 DDR3-1066 8528 PC8500DDR3 666 1333 DDR3-1333 10,664 PC10700DDR3 800 1600 DDR3-1600 12,800 PC12800DDR4 1333 2666 DDR4-2666 21,300 PC21300 Figure 2.5 Clock rates, bandwidth, names DDR DRAMS DIMMs 2016. Note numerical relationship columns. third column twice second, fourth uses number third column inthe name DRAM chip. fifth column eight times third column, rounded version number used name DIMM. DDR4 saw significant first use 2016.2.2 Memory Technology Optimizations ■89In addition changes, recent SDRAMs support power-down mode, entered telling DRAM ignore clock. Power-down mode dis-ables SDRAM, except internal automatic refresh (without enteringpower-down mode longer refresh time cause contents mem-ory lost). Figure 2.6 shows power consumption three situations 2 GB DDR3 SDRAM. exact delay required return low power modedepends SDRAM, typical delay 200 SDRAM clock cycles. Graphics Data RAMs GDRAMs GSDRAMs (Graphics Graphics Synchronous DRAMs) spe-cial class DRAMs based SDRAM designs tailored handling higherbandwidth demands graphics processing units. GDDR5 based DDR3 withearlier GDDRs based DDR2. graphics processor units (GPUs; seeChapter 4 ) require bandwidth per DRAM chip CPUs, GDDRs several important differences: 1.GDDRs wider interfaces: 32-bits versus 4, 8, 16 current designs. 2.GDDRs higher maximum clock rate data pins. allow higher transfer rate without incurring signaling problems, GDRAMS normally connect directly GPU attached soldering board, unlike DRAMs, normally arranged expandable array DIMMs. Altogether, characteristics let GDDRs run two five times bandwidth per DRAM versus DDR3 DRAMs.0100200300400500600 Low power modeTypical usageFully activePower mW Background powerActivate powerRead, write, terminatepower Figure 2.6 Power consumption DDR3 SDRAM operating three condi- tions: low-power (shutdown) mode, typical system mode (DRAM active 30% time reads 15% writes), fully active mode, DRAM continuously reading writing. Reads writes assume bursts eight transfers. data based Micron 1.5V 2GB DDR3-1066, although similar savings occur DDR4 SDRAMs.90 ■Chapter Two Memory Hierarchy DesignPackaging Innovation: Stacked Embedded DRAMs newest innovation 2017 DRAMs packaging innovation, rather circuit innovation. places multiple DRAMs stacked adjacent fashion embedded within package processor. (Embedded DRAM also used refer designs place DRAM processor chip.) Placing DRAM processor package lowers access latency (by shortening delay DRAMs processor) potentially increases band- width allowing faster connections processor DRAM; thus several producers called high bandwidth memory (HBM) . One version technology places DRAM die directly CPU die using solder bump technology connect them. Assuming adequate heat manage- ment, multiple DRAM dies stacked fashion. Another approach stacks DRAMs abuts CPU single package using substrate (interposer) containing connections. Figure 2.7 shows two different inter- connection schemes. Prototypes HBM allow stacking eight chips demonstrated. special versions SDRAMs, package could contain 8 GiB memory data transfer rates 1 TB/s. 2.5D tech- nique currently available. chips must specifically manufactured stack, quite likely early uses high-end server chipsets. applications, may possible internally package enough DRAM satisfy needs application. example, version Nvidia GPU used node special-purpose cluster design developed using HBM, likely HBM become successor GDDR5 higher-end appli- cations. cases, may possible use HBM main memory, although cost limitations heat removal issues currently rule technology embedded applications. next section, consider possibility using HBM additional level cache. DRAM DRAM Vertical stackin g (3D) Interposer stackin g (2.5D )xPUxPU Interposer Figure 2.7 Two forms die stacking. 2.5D form available now. 3D stacking development faces heat management challenges due CPU.2.2 Memory Technology Optimizations ■91Flash Memory Flash memory type EEPROM (electronically erasable programmable read- memory), normally read-only erased. key prop-erty Flash memory holds contents without power. focus onNAND Flash, higher density Flash suitable forlarge-scale nonvolatile memories; drawback access sequential andwriting slower, explain below. Flash used secondary storage PMDs manner disk functions laptop server. addition, PMDs limited amount DRAM, Flash may also act level memory hierarchy, much greater extent might desktop server mainmemory might 10 –100 times larger. Flash uses different architecture different properties stan- dard DRAM. important differences 1.Reads Flash sequential read entire page, 512 bytes, 2 KiB, 4 KiB. Thus NAND Flash long delay access first byte random address (about 25 μS), supply remainder page block 40 MiB/s. comparison, DDR4 SDRAM takes 40 ns tothe first byte transfer rest row 4.8 GiB/s. Comparing thetime transfer 2 KiB, NAND Flash takes 75 μS, DDR SDRAM takes less 500 ns, making Flash 150 times slower. Compared mag-netic disk, however, 2 KiB read Flash 300 500 times faster. Fromthese numbers, see Flash candidate replace DRAM formain memory, candidate replace magnetic disk. 2.Flash memory must erased (thus name flash “flash”erase process) overwritten, erased blocks rather individual bytes words. requirement means data must written Flash, entire block must assembled, either new data merging data tobe written rest block ’s contents. writing, Flash 1500 times slower SDRAM, 8 –15 times fast magnetic disk. 3.Flash memory nonvolatile (i.e., keeps contents even power applied) draws significantly less power reading writing (fromless half standby mode zero completely inactive). 4.Flash memory limits number times given block written, typically least 100,000. ensuring uniform distribution written blocksthroughout memory, system maximize lifetime Flash memorysystem. technique, called write leveling , handled Flash memory controllers. 5.High-density NAND Flash cheaper SDRAM expensive disks: roughly $2/GiB Flash, $20 $40/GiB SDRAM, $0.09/GiB magnetic disks. past five years, Flash decreased cost rate almost twice fast magnetic disks.92 ■Chapter Two Memory Hierarchy DesignLike DRAM, Flash chips include redundant blocks allow chips small numbers defects used; remapping blocks handled Flash chip. Flash controllers handle page transfers, provide caching pages, handle writeleveling. rapid improvements high-density Flash critical devel- opment low-power PMDs laptops, also significantly changedboth desktops, increasingly use solid state disks, large servers, whichoften combine disk Flash-based storage. Phase-Change Memory Technology Phase-change memory (PCM) active research area decades. Thetechnology typically uses small heating element change state bulk sub-strate crystalline form amorphous form, differentresistive properties. bit corresponds crosspoint two-dimensional net-work overlays substrate. Reading done sensing resistance betweenan x point (thus alternative name memristor ), writing accomplished applying current change phase material. absence active device (such transistor) lead lower costs greater density NAND Flash. 2017 Micron Intel began delivering Xpoint memory chips believed based PCM. technology expected much betterwrite durability NAND Flash and, eliminating need erase pagebefore writing, achieve increase write performance versus NAND toa factor ten. Read latency also better Flash perhaps factor of2–3. Initially, expected priced slightly higher Flash, advan- tages write performance write durability may make attractive, especially SSDs. technology scale well able achieve additional costreductions, may solid state technology depose magnetic disks,which reigned primary bulk nonvolatile store 50 years. Enhancing Dependability Memory Systems Large caches main memories significantly increase possibility errorsoccurring fabrication process dynamically operation.Errors arise change circuitry repeatable called hard errors orpermanent faults. Hard errors occur fabrication, well circuit change operation (e.g., failure Flash memory cell manywrites). DRAMs, Flash memory, SRAMs manufactured withspare rows small number manufacturing defects accommodatedby programming replacement defective row spare row. Dynamicerrors, changes cell ’s contents, change circuitry, called soft errors ortransient faults . Dynamic errors detected parity bits detected fixed use error correcting codes (ECCs). instruction caches read-only, parity2.2 Memory Technology Optimizations ■93suffices. larger data caches main memory, ECC used allow errors detected corrected. Parity requires one bit overhead detect single error sequence bits. multibit error would undetectedwith parity, number bits protected parity bit must limited. One paritybit per 8 data bits typical ratio. ECC detect two errors correct singleerror cost 8 bits overhead per 64 data bits. large systems, possibility multiple errors well complete fail- ure single memory chip becomes significant. Chipkill introduced IBMto solve problem, many large systems, IBM SUN servers Google Clusters, use technology. (Intel calls version SDDC.) Similar nature RAID approach used disks, Chipkill distributes dataand ECC information complete failure single memory chip behandled supporting reconstruction missing data remainingmemory chips. Using analysis IBM assuming 10,000 processor serverwith 4 GiB per processor yields following rates unrecoverable errors threeyears operation: ■Parity only: 90,000, one unrecoverable (or undetected) failure every17 minutes. ■ECC only: 3500, one undetected unrecoverable failure every 7.5 hours. ■Chipkill: one undetected unrecoverable failure every 2 months. Another way look find maximum number servers (each 4 GiB) protected achieving error rate demon-strated Chipkill. parity, even server one processor anunrecoverable error rate higher 10,000-server Chipkill protected system. ForECC, 17-server system would failure rate 10,000-serverChipkill system. Therefore Chipkill requirement 50,000 –100,00 servers warehouse-scale computers (see Section 6.8 Chapter 6 ). 2.3 Ten Advanced Optimizations Cache Performance preceding average memory access time formula gives us three metrics cache optimizations: hit time, miss rate, miss penalty. Given recent trends,we add cache bandwidth power consumption list. classify 10advanced cache optimizations examine five categories based metrics: 1.Reducing hit time —Small simple first-level caches way-prediction. techniques also generally decrease power consumption. 2.Increasing cache bandwidth —Pipelined caches, multibanked caches, non- blocking caches. techniques varying impacts power consumption.94 ■Chapter Two Memory Hierarchy Design3.Reducing miss penalty —Critical word first merging write buffers. optimizations little impact power. 4.Reducing miss rate —Compiler optimizations. Obviously improvement compile time improves power consumption. 5.Reducing miss penalty miss rate via parallelism —Hardware prefetching compiler prefetching. optimizations generally increase power con- sumption, primarily prefetched data unused. general, hardware complexity increases go optimi- zations. addition, several optimizations require sophisticated compilertechnology, final one depends HBM. conclude summaryof implementation complexity performance benefits 10 tech-niques presented Figure 2.18 page 113. straight- forward, cover briefly; others require description. First Optimization: Small Simple First-Level Caches Reduce Hit Time Power pressure fast clock cycle power limitations encourages limited size first-level caches. Similarly, use lower levels associativity reduceboth hit time power, although trade-offs complex thoseinvolving size. critical timing path cache hit three-step process addressing tag memory using index portion address, comparing read tag value address, setting multiplexor choose correct data item cache set associative. Direct-mapped caches overlap tag check transmis-sion data, effectively reducing hit time. Furthermore, lower levels associa-tivity usually reduce power fewer cache lines must accessed. Although total amount on-chip cache increased dramatically new generations microprocessors, clock rate impact arising froma larger L1 cache, size L1 caches recently increased either slightly ornot all. many recent processors, designers opted associativity rather larger caches. additional consideration choosing associativity possibility eliminating address aliases; discuss topic shortly. One approach determining impact hit time power consumption advance building chip use CAD tools. CACTI program estimate theaccess time energy consumption alternative cache structures CMOSmicroprocessors within 10% detailed CAD tools. given minimumfeature size, CACTI estimates hit time caches function cache size,associativity, number read/write ports, complex parameters. Figure 2.8 shows estimated impact hit time cache size associativity varied. Depending cache size, parameters, model suggests thatthe hit time direct mapped slightly faster two-way set associative andthat two-way set associative 1.2 times fast four-way four-way 1.42.3 Ten Advanced Optimizations Cache Performance ■95times fast eight-way. course, estimates depend technology well size cache, CACTI must carefully aligned technology; Figure 2.8 shows relative tradeoffs one technology. Example Using data Figure B.8 Appendix B andFigure 2.8 , determine whether 32 KiB four-way set associative L1 cache faster memory access time a32 KiB two-way set associative L1 cache. Assume miss penalty L2 is15 times access time faster L1 cache. Ignore misses beyond L2. Whichhas faster average memory access time? Answer Let access time two-way set associative cache 1. Then, two- way cache, Average memory access time 2-way¼Hit time + Miss rate /C2Miss penalty ¼1+0:038/C215¼1:383.0 2.52.0 1.5 1.0 0.5 0 Cache size16 KB 32 KB 64 KB 128 KB 256 KB1-way 8-way2-way 4-wayRelative access time microseconds Figure 2.8 Relative access times generally increase cache size associativity increased. data come CACTI model 6.5 Tarjan et al. (2005) . data assume typical embedded SRAM technology, single bank, 64-byte blocks. assumptions cache layout complex trade-offs inter- connect delays (that depend size cache block accessed) cost oftag checks multiplexing lead results occasionally surprising, lower access time 64 KiB two-way set associativity versus direct mapping. Sim- ilarly, results eight-way set associativity generate unusual behavior cache sizeis increased. observations highly dependent technology anddetailed design assumptions, tools CACTI serve reduce search space. results relative; nonetheless, likely shift move recent denser semiconductor technologies.96 ■Chapter Two Memory Hierarchy DesignFor four-way cache, access time 1.4 times longer. elapsed time miss penalty 15/1.4 ¼10.1. Assume 10 simplicity: Average memory access time 4-way¼Hit time 2-way/C21:4 + Miss rate /C2Miss penalty ¼1:4+0:037/C210¼1:77 Clearly, higher associativity looks like bad trade-off; however, cache access modern processors often pipelined, exact impact clock cycletime difficult assess. Energy consumption also consideration choosing cache size associativity, Figure 2.9 shows. energy cost higher associativity ranges factor 2 negligible caches 128 256 KiB goingfrom direct mapped two-way set associative. energy consumption become critical, designers focused ways reduce energy needed cache access. addition associativity, theother key factor determining energy used cache access number blocks cache determines number “rows”that accessed. designer could reduce number rows increasing block size(holding total cache size constant), could increase miss rate, especiallyin smaller L1 caches. 7.08.09.010.0 6.0 5.0 4.0 3.0 1.02.0 01-way 8-way2-way 4-wayRelative energy per read nano joules Cache size16 KB 32 KB 64 KB 128 KB 256 KB Figure 2.9 Energy consumption per read increases cache size associativity increased. previous figure, CACTI used modeling technology parameters. large penalty eight-way set associative caches due tothe cost reading eight tags corresponding data parallel.2.3 Ten Advanced Optimizations Cache Performance ■97An alternative organize cache banks access activates portion cache, namely bank desired block resides. primary use multibanked caches increase bandwidth cache,an optimization consider shortly. Multibanking also reduces energy becauseless cache accessed. L3 caches many multicores logically uni-fied, physically distributed, effectively act multibanked cache. Basedon address request, one physical L3 caches (a bank) actuallyaccessed. discuss organization Chapter 5 . recent designs, three factors led use higher associativity first-level caches despite energy access time costs. First, many processors take least 2 clock cycles access cache thus impactof longer hit time may critical. Second, keep TLB criticalpath (a delay would larger associated increased associativity),almost L1 caches virtually indexed. limits size cache tothe page size times associativity bits within page areused index. solutions problem indexing cachebefore address translation completed, increasing associativity, also benefits, attractive. Third, introduction multi- threading (see Chapter 3 ), conflict misses increase, making higher associativity attractive. Second Optimization: Way Prediction Reduce Hit Time Another approach reduces conflict misses yet maintains hit speed direct- mapped cache. way prediction , extra bits kept cache predict way (or block within set) nextcache access. prediction means mul- tiplexor set early select desired block, clock cycle, singletag comparison performed parallel reading cache data. miss resultsin checking blocks matches next clock cycle. Added block cache block predictor bits. bits select blocks try next cache access. predictor correct, cacheaccess latency fast hit time. not, tries block, changes way predictor, latency one extra clock cycle. Simulations suggest set prediction accuracy excess 90% two-way set associative cache and80% four-way set associative cache, better accuracy I-caches thanD-caches. Way prediction yields lower average memory access time two-way set associative cache least 10% faster, quite likely. Wayprediction first used MIPS R10000 mid-1990s. popular inprocessors use two-way set associativity used several ARM pro-cessors, four-way set associative caches. fast processors, may challenging implement one-cycle stall critical keeping way prediction penalty small. extended form way prediction also used reduce power con- sumption using way prediction bits decide cache block actually98 ■Chapter Two Memory Hierarchy Designaccess (the way prediction bits essentially extra address bits); approach, might called way selection, saves power way prediction cor- rect adds significant time way misprediction, access, justthe tag match selection, must repeated. optimization likely tomake sense low-power processors. Inoue et al. (1999) estimated usingthe way selection approach four-way set associative cache increases theaverage access time I-cache 1.04 D-cache 1.13 theSPEC95 benchmarks, yields average cache power consumption relativeto normal four-way set associative cache 0.28 I-cache 0.35 D-cache. One significant drawback way selection makes difficult pipeline cache access; however, energy concerns mounted, schemesthat require powering entire cache make increasing sense. Example Assume half many D-cache accesses I-cache accesses I-cache D-cache responsible 25% 15% processor ’s power consumption normal four-way set associative implementation. Determine ifway selection improves performance per watt based estimates thepreceding study. Answer I-cache, savings power 25 /C20.28¼0.07 total power, D-cache 15 /C20.35¼0.05 total savings 0.12. way prediction version requires 0.88 power requirement standard four-way cache. Theincrease cache access time increase I-cache average access time plusone-half increase D-cache access time, 1.04+0.5 /C20.13¼1.11 times lon- ger. result means way selection 0.90 performance standardfour-way cache. Thus way selection improves performance per joule slightly ratio 0.90/0.88 ¼1.02. optimization best used power rather performance key objective. Third Optimization: Pipelined Access Multibanked Caches Increase Bandwidth optimizations increase cache bandwidth either pipelining cache access widening cache multiple banks allow multiple accesses per clock;these optimizations dual superpipelined superscalar approaches toincreasing instruction throughput. optimizations primarily targeted L1, access bandwidth constrains instruction throughput. Multiple banks also used L2 L3 caches, primarily power-management technique. Pipelining L1 allows higher clock cycle, cost increased latency. example, pipeline instruction cache access Intel Pentium processorsin mid-1990s took 1 clock cycle; Pentium Pro Pentium III themid-1990s 2000, took 2 clock cycles; Pentium 4, whichbecame available 2000, current Intel Core i7, takes 4 clock cycles.Pipelining instruction cache effectively increases number pipeline stages,2.3 Ten Advanced Optimizations Cache Performance ■99leading greater penalty mispredicted branches. Correspondingly, pipelining data cache leads clock cycles issuing load using data (see Chapter 3 ). Today, processors use pipelining L1, simple case separating access hit detection, many high-speedprocessors three levels cache pipelining. easier pipeline instruction cache data cache pro- cessor rely high performance branch prediction limit latency effects.Many superscalar processors issue execute one memory refer-ence per clock (allowing load store common, processors allowmultiple loads). handle multiple data cache accesses per clock, divide cache independent banks, supporting independent access. Banks originally used improve performance main memory usedinside modern DRAM chips well caches. Intel Core i7 fourbanks L1 (to support 2 memory accesses per clock). Clearly, banking works best accesses naturally spread across banks, mapping addresses banks affects behavior ofthe memory system. simple mapping works well spread addressesof block sequentially across banks, called sequential interleaving . example, four banks, bank 0 blocks whose address modulo 4 0, bank 1 blocks whose address modulo 4 1, on. Figure 2.10 shows interleaving. Multiple banks also way reduce power consump-tion caches DRAM. Multiple banks also useful L2 L3 caches, different reason. multiple banks L2, handle one outstanding L1 miss,if banks conflict. key capability support nonblocking caches,our next optimization. L2 Intel Core i7 eight banks, Arm Cortex processors used L2 caches 1 –4 banks. mentioned earlier, multibanking also reduce energy consumption. Fourth Optimization: Nonblocking Caches Increase Cache Bandwidth pipelined computers allow out-of-order execution (discussed Chapter 3 ), processor need stall data cache miss. example, processor could0 4 8 12Bank 0Block addressBlock address 1 5 9 13Bank 1Block address 2 6 10 14Bank 2Block address 3 7 11 15Bank 3 Figure 2.10 Four-way interleaved cache banks using block addressing. Assuming 64 bytes per block, addresses would multiplied 64 get byteaddressing.100 ■Chapter Two Memory Hierarchy Designcontinue fetching instructions instruction cache waiting data cache return missing data. nonblocking cache orlockup-free cache esca- lates potential benefits scheme allowing data cache continueto supply cache hits miss. “hit miss ”optimization reduces effective miss penalty helpful miss instead ignoring therequests processor. subtle complex option cache may furtherlower effective miss penalty overlap multiple misses: “hit multiple miss ”or“miss miss ”optimization. second option beneficial memory system service multiple misses; high-performance pro- cessors (such Intel Core processors) usually support both, whereas many lower-end processors provide limited nonblocking support L2. examine effectiveness nonblocking caches reducing cache miss penalty, Farkas Jouppi (1994) study assuming 8 KiB caches 14-cycle miss penalty (appropriate early 1990s). observed reductionin effective miss penalty 20% SPECINT92 benchmarks 30% forthe SPECFP92 benchmarks allowing one hit miss. Li et al. (2011) updated study use multilevel cache, modern assumptions miss penalties, larger demanding SPECCPU2006 benchmarks. study done assuming model based asingle core Intel i7 (see Section 2.6 ) running SPECCPU2006 benchmarks. Figure 2.11 shows reduction data cache access latency allowing 1, 2, 64 hits miss; caption describes details memorysystem. larger caches addition L3 cache since earlierstudy reduced benefits SPECINT2006 benchmarks showingan average reduction cache latency 9% SPECFP2006 bench- marks 12.5%. Example important floating-point programs: two-way set associativity hit one miss primary data caches? integer programs?Assume following average miss rates 32 KiB data caches: 5.2% forfloating-point programs direct-mapped cache, 4.9% programs two-way set associative cache, 3.5% integer programs direct-mapped cache, 3.2% integer programs two-way set associative cache. Assumethe miss penalty L2 10 cycles, L2 misses penalties same. Answer floating-point programs, average memory stall times Miss rate DM/C2Miss penalty ¼5:2%/C210¼0:52 Miss rate 2-way/C2Miss penalty ¼4:9%/C210¼0:49 cache access latency (including stalls) two-way associativity 0.49/0.52 94% direct-mapped cache. Figure 2.11 caption indicates hit one miss reduces average data cache access latency floating-point programsto 87.5% blocking cache. Therefore, floating-point programs, the2.3 Ten Advanced Optimizations Cache Performance ■101direct-mapped data cache supporting one hit one miss gives better perfor- mance two-way set-associative cache blocks miss. integer programs, calculation Miss rate DM/C2Miss penalty ¼3:5%/C210¼0:35 Miss rate 2-way/C2Miss penalty ¼3:2%/C210¼0:32 data cache access latency two-way set associative cache thus 0.32/0.35 91% direct-mapped cache, reduction access latency allow- ing hit one miss 9%, making two choices equal. real difficulty performance evaluation nonblocking caches cache miss necessarily stall processor. case, difficult judge impact single miss thus calculate average memoryaccess time. effective miss penalty sum misses thenonoverlapped time processor stalled. benefit nonblocking cachesis complex, depends upon miss penalty multiple misses, thememory reference pattern, many instructions processor executewith miss outstanding. general, out-of-order processors capable hiding much miss penalty L1 data cache miss hits L2 cache capable40%50%60%70%80%90%100% bzip2 gcc mcf hmmer sjeng libquantum h264ref omnetpp astar gamess zeusmp milc gromacs cactusADM namd soplex povray calculix GemsFDTD tonto lbm wrf sphinx3 SPECFP SPECINTCache access latencyHit-under-2-misses Hit-under-64-misses Hit-under-1-miss Figure 2.11 effectiveness nonblocking cache evaluated allowing 1, 2, 64 hits cache miss 9 SPECINT (on left) 9 SPECFP (on right) benchmarks. data memory system modeled Intel i7 consists 32 KiB L1 cache four-cycle access latency. L2 cache (shared instructions) 256 KiB 10-clock cycle access latency. L3 2 MiB 36-cycle access latency. caches eight-way set associative 64-byte block size. Allowing one hitunder miss reduces miss penalty 9% integer benchmarks 12.5% floating point. Allowing second hit improves results 10% 16%, allowing 64 results little additional improvement.102 ■Chapter Two Memory Hierarchy Designof hiding significant fraction lower-level cache miss. Deciding many outstanding misses support depends variety factors: ■The temporal spatial locality miss stream, determines whether miss initiate new access lower-level cache memory. ■The bandwidth responding memory cache. ■To allow outstanding misses lowest level cache (where themiss time longest) requires supporting least many misses ahigher level, miss must initiate highest level cache. ■The latency memory system. following simplified example illustrates key idea. Example Assume main memory access time 36 ns memory system capable sustained transfer rate 16 GiB/s. block size 64 bytes, maximumnumber outstanding misses need support assuming maintain thepeak bandwidth given request stream accesses never conflict. prob-ability reference colliding one previous four 50%, assume access wait earlier access completes, estimate number maximum outstanding references. simplicity, ignore time misses. Answer first case, assuming maintain peak bandwidth, memory system support (16 /C210) 9/64¼250 million references per second. reference takes 36 ns, support 250 /C2106/C236/C210/C09¼9 references. probability collision greater 0, need outstanding ref-erences, cannot start work colliding references; memory system needs independent references, fewer! approximate, sim- ply assume half memory references issued memory.This means must support twice many outstanding references, 18. Li, Chen, Brockman, Jouppi ’s study, found reduction CPI integer programs 7% one hit miss about12.7% 64. floating-point programs, reductions 12.7% forone hit miss 17.8% 64. reductions track fairly closely thereductions data cache access latency shown Figure 2.11 . Implementing Nonblocking Cache Although nonblocking caches potential improve performance, nontrivial implement. Two initial types challenges arise: arbitrating conten- tion hits misses, tracking outstanding misses knowwhen loads stores proceed. Consider first problem. blocking cache,misses cause processor stall accesses cache occur2.3 Ten Advanced Optimizations Cache Performance ■103until miss handled. nonblocking cache, however, hits collide misses returning next level memory hierarchy. allow multiple outstanding misses, almost recent processors do, even possible formisses collide. collisions must resolved, usually first giving priorityto hits misses, second ordering colliding misses (if occur). second problem arises need track multiple outstanding mis- ses. blocking cache, always know miss returning, onlyone outstanding. nonblocking cache, rarely true. first glance,you might think misses always return order, simple queue could kept match returning miss longest outstanding request. Consider, however, miss occurs L1. may generate either hit miss L2; ifL2 also nonblocking, order misses returned L1 willnot necessarily order originally occurred. Multi-core multiprocessor systems nonuniform cache access timesalso introduce complication. miss returns, processor must know load store caused miss, instruction go forward; must know cache data placed (as well setting tags block). recent processors, information kept set registers, typically called Miss Status Handling Registers (MSHRs) . allow noutstanding misses, benMSHRs, holding information miss goes cache value tag bits miss, well information indicatingwhich load store caused miss (in next chapter, see thisis tracked). Thus, miss occurs, allocate MSHR handling miss,enter appropriate information miss, tag memory request index MSHR. memory system uses tag returns data, allowing cache system transfer data tag information appropri-ate cache block “notify ”the load store generated miss data available resume operation. Nonblocking caches clearly requireextra logic thus cost energy. difficult, however, assesstheir energy costs exactly may reduce stall time, thereby decreasingexecution time resulting energy consumption. addition preceding issues, multiprocessor memory systems, whether within single chip multiple chips, must also deal complex implemen- tation issues related memory coherency consistency. Also, cache mis-ses longer atomic (because request response split may beinterleaved among multiple requests), possibilities deadlock. theinterested reader, Section I.7 online Appendix deals issues detail. Fifth Optimization: Critical Word First Early Restart Reduce Miss Penalty technique based observation processor normally needs one word block time. strategy impatience: ’t wait full104 ■Chapter Two Memory Hierarchy Designblock loaded sending requested word restarting processor. two specific strategies: ■Critical word first —Request missed word first memory send processor soon arrives; let processor continue execution whilefilling rest words block. ■Early restart —Fetch words normal order, soon requested word block arrives, send processor let processor continueexecution. Generally, techniques benefit designs large cache blocks benefit low unless blocks large. Note caches normally continue satisfy accesses blocks rest block filled. However, given spatial locality, good chance next reference rest block. nonblocking caches, miss penalty notsimple calculate. second request critical word first, effec-tive miss penalty nonoverlapped time reference secondpiece arrives. benefits critical word first early restart depend sizeof block likelihood another access portion block yet fetched. example, SPECint2006 running i7 6700, uses early restart critical word first, one reference made ablock outstanding miss (1.23 references average range 0.5to 3.0). explore performance i7 memory hierarchy detail inSection 2.6 . Sixth Optimization: Merging Write Buffer Reduce Miss Penalty Write-through caches rely write buffers, stores must sent next lower level hierarchy. Even write-back caches use simple buffer whena block replaced. write buffer empty, data full address written buffer, write finished processor ’s perspective; processor continues working write buffer prepares write wordto memory. buffer contains modified blocks, addresses bechecked see address new data matches address valid writebuffer entry. so, new data combined entry. Write merging name optimization. Intel Core i7, among many others, uses writemerging. buffer full address match, cache (and processor) must wait buffer empty entry. optimization uses memory efficiently multiword writes usually faster writes performedone word time. Skadron Clark (1997) found even merging four-entry write buffer generated stalls led 5% –10% performance loss.2.3 Ten Advanced Optimizations Cache Performance ■105The optimization also reduces stalls write buffer full. Figure 2.12 shows write buffer without write merging. Assume four entries write buffer, entry could hold four 64-bit words.Without optimization, four stores sequential addresses would fill bufferat one word per entry, even though four words merged fit exactlywithin single entry write buffer. Note input/output device registers often mapped physical address space. I/O addresses cannot allow write merging separate I/O registers may act like array words memory. example, mayrequire one address data word per I/O register rather use multiword writesusing single address. side effects typically implemented marking thepages requiring nonmerging write caches.100 108116124Write address 1 111V 0 000V 0 000V 0 000V 100Write address 1 0 0 0V 1 0 0 0V 1 0 0 0V 1 0 0 0VMem[100] Mem[100]Mem[108] Mem[108]Mem[116] Mem[116]Mem[124] Mem[124] Figure 2.12 illustration write merging, write buffer top use write merging write buffer bottom does. four writes merged single buffer entry write merging; without it, buffer full even though three-fourths entry wasted. buffer four entries, entry holds four 64-bit words. address entry left, valid bit (V) indicatingwhether next sequential 8 bytes entry occupied. (Without write merging,the words right upper part figure would used instructions wrote multiple words time.)106 ■Chapter Two Memory Hierarchy DesignSeventh Optimization: Compiler Optimizations Reduce Miss Rate Thus far, techniques required changing hardware. next technique reduces miss rates without hardware changes. magical reduction comes optimized software —the hardware designer ’s favorite solution! increasing performance gap processors main memory inspired compiler writers scrutinize memory hierarchyto see compile time optimizations improve performance. again, researchis split improvements instruction misses improvements data mis-ses. optimizations presented next found many modern compilers. Loop Interchange programs nested loops access data memory nonsequential order. Simply exchanging nesting loops make code access data order stored. Assuming arrays fit thecache, technique reduces misses improving spatial locality; reordering max-imizes use data cache block discarded. example, xis two-dimensional array size [5000,100] allocated x[i,j] andx[i,j +1] adjacent (an order called row major array laid rows), two pieces following code show accesses optimized: /* */ (j ¼0; j <100; j ¼j+1 ) (i ¼0; <5000; ¼i+1 ) x[i][j] ¼2 * x[i][j]; /* */ (i ¼0; <5000; ¼i+1 ) (j ¼0; j <100; j ¼j+1 ) x[i][j] ¼2 * x[i][j]; original code would skip memory strides 100 words, revised version accesses words one cache block going nextblock. optimization improves cache performance without affecting num-ber instructions executed. Blocking optimization improves temporal locality reduce misses. deal-ing multiple arrays, arrays accessed rows columns.Storing arrays row row ( row major order ) column column ( column major order ) solve problem rows columns used every loop iteration. orthogonal accesses mean transformations loop interchange still leave plenty room improvement.2.3 Ten Advanced Optimizations Cache Performance ■107Instead operating entire rows columns array, blocked algorithms operate submatrices blocks . goal maximize accesses data loaded cache data replaced. following code example,which performs matrix multiplication, helps motivate optimization: /* */ (i ¼0; <N; ¼i+1 ) (j ¼0; j <N; j ¼j+1 ) {r¼0; (k ¼0; k <N ;k=k+1 ) r¼r+y[i][k]*z[k][j]; x[i][j] ¼r; }; two inner loops read N-by-Nelements z, read Nelements row yrepeatedly, write one row Nelements x.Figure 2.13 gives snapshot accesses three arrays. dark shade indicates recent access, light shade indicates older access, white means yet accessed. number capacity misses clearly depends Nand size cache. hold three N-by-Nmatrices, well, provided cache conflicts. cache hold one N-by-N matrix one row N, least theith row yand array zmay stay cache. Less misses may occur xandz. worst case, would 2 N 3+N2memory words accessed N3operations. ensure elements accessed fit cache, original code changed compute submatrix size BbyB. Two inner loops compute steps size Brather full length xandz.Bis called theblocking factor . (Assume xis initialized zero.)0 123 4 510 2345xj i0 123 4 510 2345yk i0 123 4 5102 3 4 5zj k Figure 2.13 snapshot three arrays x, y, andzwhen N56 i51.The age accesses array elements indicated shade: white means yet touched, light means older accesses, dark means neweraccesses. elements yandzare read repeatedly calculate new elements x. variables i,j, k shown along rows columns used access arrays.108 ■Chapter Two Memory Hierarchy Design/* */ (jj ¼0; jj <N; jj ¼jj + B) (kk ¼0; kk <N; kk ¼kk + B) (i ¼0; <N; ¼i+1 ) (j ¼jj; j <min(jj + B,N); j ¼j+1 ) {r¼0; (k ¼kk; k <min(kk + B,N); k ¼k+1 ) r¼r + y[i][k]*z[k][j]; x[i][j] = x[i][j] + r; }; Figure 2.14 illustrates accesses three arrays using blocking. Looking capacity misses, total number memory words accessed 2 N3/B+N2. total improvement approximate factor B. Therefore blocking exploits combination spatial temporal locality, ybenefits spatial locality zbenefits temporal locality. Although example uses square block (BxB), could also use rectangular block, would nec-essary matrix square. Although aimed reducing cache misses, blocking also used help register allocation. taking small blocking size block beheld registers, minimize number loads stores program. shall see Section 4.8 Chapter 4 , cache blocking absolutely nec- essary get good performance cache-based processors running applications using matrices primary data structure. Eighth Optimization: Hardware Prefetching Instructions Data Reduce Miss Penalty Miss Rate Nonblocking caches effectively reduce miss penalty overlapping execution memory access. Another approach prefetch items processorrequests them. instructions data prefetched, either directly 0 123 4510 2345xj i0 123 4510 2345yk i0 123 45102 3 4 5zj k Figure 2.14 age accesses arrays x, y, z B53.Note that, contrast Figure 2.13 , smaller number elements accessed.2.3 Ten Advanced Optimizations Cache Performance ■109the caches external buffer quickly accessed main memory. Instruction prefetch frequently done hardware outside cache. Typically, processor fetches two blocks miss: requested block next consec-utive block. requested block placed instruction cache returns, andthe prefetched block placed instruction stream buffer. requested block ispresent instruction stream buffer, original cache request canceled, theblock read stream buffer, next prefetch request issued. similar approach applied data accesses ( Jouppi, 1990 ).Palacharla Kessler (1994) looked set scientific programs considered multiple stream buffers could handle either instructions data. found eightstream buffers could capture 50% –70% misses processor two 64 KiB four-way set associative caches, one instructions data. Intel Core i7 supports hardware prefetching L1 L2 common case prefetching accessing next line. earlier Intelprocessors used aggressive hardware prefetching, resulted reducedperformance applications, causing sophisticated users turn capability. Figure 2.15 shows overall performance improvement subset SPEC2000 programs hardware prefetching turned on. Note figure 1.001.201.401.601.802.002.20 gap1.16 mcf1.45 fam3d1.18 wupwise1.20 galgel1.21 facerec1.26Performance improvement swim1.29 applu1.32 SPECint2000 SPECfp2000lucas1.40 equake1.97 mgrid1.49 Figure 2.15 Speedup hardware prefetching Intel Pentium 4 hardware prefetching turned 2 12 SPECint2000 benchmarks 9 14 SPECfp2000 benchmarks. programs benefit prefetching shown; prefetching speeds missing 15 SPECCPU benchmarks less than15% ( Boggs et al., 2004 ).110 ■Chapter Two Memory Hierarchy Designincludes 2 12 integer programs, includes majority SPECCPU floating-point programs. return evaluation prefetch- ing i7 Section 2.6 . Prefetching relies utilizing memory bandwidth otherwise would unused, interferes demand misses, actually lower performance.Help compilers reduce useless prefetching. prefetching workswell, impact power negligible. prefetched data usedor useful data displaced, prefetching negative impacton power. Ninth Optimization: Compiler-Controlled Prefetching Reduce Miss Penalty Miss Rate alternative hardware prefetching compiler insert prefetch instruc- tions request data processor needs it. two flavors ofprefetch: ■Register prefetch loads value register. ■Cache prefetch loads data cache register. Either faulting ornonfaulting ; is, address cause exception virtual address faults protection violations. Usingthis terminology, normal load instruction could considered “faulting register prefetch instruction. ”Nonfaulting prefetches simply turn no-ops would normally result exception, want. effective prefetch “semantically invisible ”to program: ’t change contents registers memory, andit cannot cause virtual memory faults. processors today offer nonfaulting cache prefetches. sectionassumes nonfaulting cache prefetch, also called nonbinding prefetch. Prefetching makes sense processor proceed prefetching data; is, caches stall continue supply instructions data waiting prefetched data return. would expect, data cache computers normally nonblocking. Like hardware-controlled prefetching, goal overlap execution prefetching data. Loops important targets lend themselvesto prefetch optimizations. miss penalty small, compiler unrolls theloop twice, schedules prefetches execution. misspenalty large, uses software pipelining (see Appendix H) unrolls many timesto prefetch data future iteration. Issuing prefetch instructions incurs instruction overhead, however, com- pilers must take care ensure overheads exceed benefits.By concentrating references likely cache misses, programs canavoid unnecessary prefetches improving average memory access timesignificantly.2.3 Ten Advanced Optimizations Cache Performance ■111Example following code, determine accesses likely cause data cache misses. Next, insert prefetch instructions reduce misses. Finally, calculate thenumber prefetch instructions executed misses avoided prefetching.Let’s assume 8 KiB direct-mapped data cache 16-byte blocks, write-back cache write allocate. elements aandbare 8 bytes long double-precision floating-point arrays. 3 rows 100 columns aand 101 rows 3 columns b. Let’s also assume cache start program. (i ¼0; <3; ¼i+1 ) (j ¼0; j <100; j ¼j+1 ) a[i][j] ¼b[j][0] * b[j + 1][0]; Answer compiler first determine accesses likely cause cache misses; otherwise, waste time issuing prefetch instructions data would hits. Elements aare written order stored memory, awill benefit spatial locality: even values jwill miss odd values hit. ahas 3 rows 100 columns, accesses lead 3/C2(100/2), 150 misses. array bdoes benefit spatial locality accesses order stored. array bdoes benefit twice temporal locality: elements accessed iteration i, iteration juses value bas last iteration. Ignoring potential conflict misses, misses bwill b[j+1][0] accesses i¼0, also first access b[j][0] j¼0. jgoes 0 99 i¼0, accesses blead 100+1, 101 misses. Thus loop miss data cache approximately 150 times aplus 101 times b, 251 misses. simplify optimization, worry prefetching first accesses loop. may already cache, pay misspenalty first elements aorb. worry suppressing prefetches end loop try prefetch beyond end (a[i] [100]…a[i][106] ) end b(b[101][0] …b[107][0] ). faulting prefetches, could take luxury. Let ’s assume miss penalty large need start prefetching least, say, seven itera-tions advance. (Stated alternatively, assume prefetching benefit untilthe eighth iteration.) underline changes preceding code needed addprefetching. (j ¼0; j <100; j ¼j + 1) { prefetch(b[j + 7][0]);/* b(j,0) 7 iterations later */prefetch(a[0][j + 7]);/* a(0,j) 7 iterations later */ a[0][j] ¼b[j][0] * b[j + 1][0];};112 ■Chapter Two Memory Hierarchy Designfor (i ¼1; <3; ¼i+1 ) (j ¼0; j <100; j ¼j + 1) { prefetch(a[i][j + 7]);/* a(i,j) + 7 iterations */a[i][j] ¼b[j][0] * b[j + 1][0];} revised code prefetches a[i][7] a[i][99] andb[7][0] b[100][0] , reducing number nonprefetched misses ■7 misses elements b[0][0] ,b[1][0] ,…,b[6][0] first loop ■4 misses ([7/2]) elements a[0][0] ,a[0][1] ,…,a[0][6] first loop (spatial locality reduces misses 1 per 16-byte cache block) ■4 misses ([7/2]) elements a[1][0] ,a[1][1] ,…,a[1][6] second loop ■4 misses ([7/2]) elements a[2][0] ,a[2][1] ,…,a[2][6] second loop total 19 nonprefetched misses. cost avoiding 232 cache misses executing 400 prefetch instructions, likely good trade-off. Example Calculate time saved preceding example. Ignore instruction cache misses assume conflict capacity misses data cache. Assume prefetches overlap cache misses, thereby transferringat maximum memory bandwidth. key loop times ignoring cachemisses: original loop takes 7 clock cycles per iteration, first prefetch looptakes 9 clock cycles per iteration, second prefetch loop takes 8 clock cyclesper iteration (including overhead outer loop). miss takes 100 clockcycles. Answer original doubly nested loop executes multiply 3 /C2100 300 times. loop takes 7 clock cycles per iteration, total 300 /C27 2100 clock cycles plus cache misses. Cache misses add 251 /C2100 25,100 clock cycles, giving total 27,200 clock cycles. first prefetch loop iterates 100 times; 9 clock cyclesper iteration total 900 clock cycles plus cache misses. add 11 /C2100 1100 clock cycles cache misses, giving total 2000. second loop executes2/C2100 200 times, 8 clock cycles per iteration, takes 1600 clock cycles plus 8 /C2100 800 clock cycles cache misses. gives total 2400 clock cycles. prior example, know code executes 400 prefetch instructions 2000+2400 4400 clock cycles execute two loops.If assume prefetches completely overlapped rest exe-cution, prefetch code 27,200/4400, 6.2 times faster.2.3 Ten Advanced Optimizations Cache Performance ■113Although array optimizations easy understand, modern programs likely use pointers. Luk Mowry (1999) demonstrated compiler-based prefetching sometimes extended pointers well. Of10 programs recursive data structures, prefetching pointers nodeis visited improved performance 4% –31% half programs. hand, remaining programs still within 2% original performance.The issue whether prefetches data already cache whetherthey occur early enough data arrive time needed. Many processors support instructions cache prefetch, high-end proces- sors (such Intel Core i7) often also type automated prefetch hardware. Tenth Optimization: Using HBM Extend Memory Hierarchy general-purpose processors servers likely want memory packaged HBM packaging, proposed in-package DRAMs used build massive L4 caches, upcoming technologiesranging 128 MiB 1 GiB more, considerably current on-chipL3 caches. Using large DRAM-based caches raises issue: thetags reside? depends number tags. Suppose use a64B block size; 1 GiB L4 cache requires 96 MiB tags —far static memory exists caches CPU. Increasing block size 4 KiB, yields dramatically reduced tag store 256 K entries less than1 MiB total storage, probably acceptable, given L3 caches of4–16 MiB next-generation, multicore processors. large block sizes, however, two major problems. First, cache may used inefficiently content many blocks needed; called fragmentation problem , also occurs virtual mem- ory systems. Furthermore, transferring large blocks inefficient much data unused. Second, large block size, number distinct blocks held DRAM cache much lower, result misses,especially conflict consistency misses. One partial solution first problem add sublocking .Subblocking allow parts block invalid, requiring fetched miss. Sub-blocking, however, nothing address second problem. tag storage major drawback using smaller block size. One pos- sible solution difficulty store tags L4 HBM. first glance seems unworkable, requires two accesses DRAM L4 access: one tags one data itself. long access timefor random DRAM accesses, typically 100 processor clock cycles, anapproach discarded. Loh Hill (2011) proposed clever solution thisproblem: place tags data row HBM SDRAM.Although opening row (and eventually closing it) takes large amount time,the CAS latency access different part row one-third new rowaccess time. Thus access tag portion block first, hit,114 ■Chapter Two Memory Hierarchy Designthen use column access choose correct word. Loh Hill (L-H) pro- posed organizing L4 HBM cache SDRAM row consists set tags (at head block) 29 data segments, making 29-way set associa-tive cache. L4 accessed, appropriate row opened tags areread; hit requires one column access get matching data. Qureshi Loh (2012) proposed improvement called alloy cache reduces hit time. alloy cache molds tag data together uses direct mapped cache structure. allows L4 access time reduced toa single HBM cycle directly indexing HBM cache burst transfer tag data. Figure 2.16 shows hit latency alloy cache, L-H scheme, SRAM based tags. alloy cache reduces hit time thana factor 2 versus L-H scheme, return increase miss rate afactor 1.1 –1.2. choice benchmarks explained caption. Unfortunately, schemes, misses require two full DRAM accesses: one get initial tag follow-on access main memory (which even 0255075100125 mcf_r lbm_r soplex_r milc_r omnet_r bwaves_r Benchmarksgcc_r libqntm_r sphinx_r gems_rLH-Cache SRAM-TagsAlloy cacheAverage hit latency Figure 2.16 Average hit time latency clock cycles L-H scheme, currently-impractical scheme using SRAM tags, alloy cache organization. SRAM case, assume SRAM accessible time L3 checked L4 accessed. average hit latencies 43 (alloy cache), 67 (SRAM tags), 107 (L-H). 10 SPECCPU2006 benchmarks used memory-intensive ones; would run twice fast L3 perfect.2.3 Ten Advanced Optimizations Cache Performance ■115slower). could speed miss detection, could reduce miss time. Two different solutions proposed solve problem: one uses map keeps track blocks cache (not location block, justwhether present); uses memory access predictor predicts likelymisses using history prediction techniques, similar used global branchprediction (see next chapter). appears small predictor predict likelymisses high accuracy, leading overall lower miss penalty. Figure 2.17 shows speedup obtained SPECrate memory- intensive benchmarks used Figure 2.16 . alloy cache approach outperforms LH scheme even impractical SRAM tags, combination fast access time miss predictor good prediction results lead shortertime predict miss, thus lower miss penalty. alloy cache performsclose Ideal case, L4 perfect miss prediction minimal hit time. 11.11.21.31.41.5 64 MB 128 MB 256 MB 512 MB 1 GB L4 cache sizeLH-Cache SRAM-Tags Alloy cacheSpedup SPECRate Ideal Figure 2.17 Performance speedup running SPECrate benchmark LH scheme, SRAM tag scheme, ideal L4 (Ideal); speedup 1 indicates improvement L4 cache, speedup 2 wouldbe achievable L4 perfect took access time. 10 memory-intensive benchmarks used benchmark run eight times. accompanying miss prediction scheme used. Ideal case assumes 64-byte block requested L4 needs accessed transferred prediction accuracy L4is perfect (i.e., misses known zero cost).116 ■Chapter Two Memory Hierarchy DesignHBM likely widespread use variety different configurations, containing entire memory system high-performance, special- purpose systems use L4 cache larger server configurations. Cache Optimization Summary techniques improve hit time, bandwidth, miss penalty, miss rate gen-erally affect components average memory access equation wellas complexity memory hierarchy. Figure 2.18 summarizes tech- niques estimates impact complexity, + meaning technique TechniqueHit timeBand- widthMiss penaltyMiss ratePower consumptionHardware cost/ complexity Comment Small simple caches+ – + 0 Trivial; widely used Way-predicting caches + + 1 Used Pentium 4 Pipelined & banked caches– + 1 Widely used Nonblocking caches + + 3 Widely used Critical word first early restart+ 2 Widely used Merging write buffer + 1 Widely used write Compiler techniques reduce cache misses+ 0 Software challenge, many compilers handlecommon linear algebra calculations Hardware prefetching instructions data++ – 2 instr., 3 dataMost provide prefetch instructions; modern high-end processors alsoautomatically prefetch hardware Compiler-controlled prefetching+ + 3 Needs nonblocking cache; possible instructionoverhead; many CPUs HBM additional level cache+/–– + + 3 Depends new packaging technology. Effects depend heavily hit rate improvements Figure 2.18 Summary 10 advanced cache optimizations showing impact cache performance, power con- sumption, complexity. Although generally technique helps one factor, prefetching reduce misses done sufficiently early; not, reduce miss penalty. + means technique improves factor, /C0means hurts factor, blank means impact. complexity measure subjective, 0 easiestand 3 challenge.2.3 Ten Advanced Optimizations Cache Performance ■117improves factor, /C0meaning hurts factor, blank meaning impact. Generally, technique helps one category. 2.4 Virtual Memory Virtual Machines virtual machine taken efficient, isolated duplicate realmachine. explain notions idea virtual machine monitor (VMM) …a VMM three essential characteristics. First, VMM provides environment programs essentially identical withthe original machine; second, programs run environment show worstonly minor decreases speed; last, VMM complete control ofsystem resources. Gerald Popek Robert Goldberg, “Formal requirements virtualizable third generation architectures, ” Communications ACM (July 1974). Section B.4 Appendix B describes key concepts virtual memory. Recall virtual memory allows physical memory treated cache sec-ondary storage (which may either disk solid state). Virtual memory movespages two levels memory hierarchy, caches move blocksbetween levels. Likewise, TLBs act caches page table, eliminating theneed memory access every time address translated. Virtual memoryalso provides separation processes share one physical memory buthave separate virtual address spaces. Readers ensure understand functions virtual memory continuing. section, focus additional issues protection privacy processes sharing processor. Security privacy two mostvexing challenges information technology 2017. Electronic burglaries, ofteninvolving lists credit card numbers, announced regularly, ’s widely believed many go unreported. course, problems arise pro-gramming errors allow cyberattack access data unable toaccess. Programming errors fact life, modern complex software systems, occur significant regularity. Therefore researchers practitioners looking improved ways make computing systems moresecure. Although protecting information limited hardware, viewreal security privacy likely involve innovation computer architectureas well systems software. section starts review architecture support protecting pro- cesses via virtual memory. describes added protectionprovided virtual machines, architecture requirements virtual machines, performance virtual machine. see Chapter 6 , virtual machines foundational technology cloud computing.118 ■Chapter Two Memory Hierarchy DesignProtection via Virtual Memory Page-based virtual memory, including TLB caches page table entries, primary mechanism protects processes other. Sections B.4 B.5inAppendix B review virtual memory, including detailed description protec- tion via segmentation paging 80x86. section acts quick review;if it’s quick, please refer denoted Appendix B sections. Multiprogramming, several programs running concurrently share computer, led demands protection sharing among programs andto concept process . Metaphorically, process program ’s breathing air living space —that is, running program plus state needed continue running it. instant, must possible switch one process another.This exchange called process switch orcontext switch . operating system architecture join forces allow processes share hardware yet interfere other. this, architecture must limitwhat process access running user process yet allow operating sys-tem process access more. minimum, architecture must following: 1.Provide least two modes, indicating whether running process user process operating system process. latter process sometimes calledakernel process supervisor process. 2.Provide portion processor state user process use write. stateincludesa user/supervisormode bit, anexceptionenable/disable bit,andmemory protection information. Users prevented writing statebecause operating system cannot control user processes users give them-selves supervisor privileges, disable exceptions, change memory protection. 3.Provide mechanisms whereby processor go user mode super- visor mode vice versa. first direction typically accomplished asystem call , implemented special instruction transfers control ded- icated location supervisor code space. PC saved point thesystem call, processor placed supervisor mode. return user mode like subroutine return restores previous user/supervisor mode. 4.Provide mechanisms limit memory accesses protect memory state process without swap process disk context switch. Appendix describes several memory protection schemes, far popular adding protection restrictions page virtual memory. Fixed-sized pages, typically 4 KiB, 16 KiB, larger, mapped virtual addressspace physical address space via page table. protection restrictions areincluded page table entry. protection restrictions might determinewhether user process read page, whether user process write page, whether code executed page. addition, process can2.4 Virtual Memory Virtual Machines ■119neither read write page page table. OS update page table, paging mechanism provides total access protection. Paged virtual memory means every memory access logically takes least twice long, one memory access obtain physical address secondaccess get data. cost would far dear. solution rely theprinciple locality; accesses locality, address translations accesses must also locality. keeping address translations spe-cial cache, memory access rarely requires second access translate address.This special address translation cache referred TLB. TLB entry like cache entry tag holds portions virtual address data portion holds physical page address, protection field, validbit, usually use bit dirty bit. operating system changes bits bychanging value page table invalidating corresponding TLBentry. entry reloaded page table, TLB gets accuratecopy bits. Assuming computer faithfully obeys restrictions pages maps vir- tual addresses physical addresses, would seem done. Newspaper headlines suggest otherwise. reason ’re done depend accuracy operating system well hardware. Today ’s operating systems consist tens mil- lions lines code. bugs measured number per thousand lines ofcode, thousands bugs production operating systems. Flaws OShave led vulnerabilities routinely exploited. problem possibility enforcing protection could much costly past led look protection model much smaller code base full OS, virtual machines. Protection via Virtual Machines idea related virtual memory almost old virtual machines (VMs). first developed late 1960s, remained importantpart mainframe computing years. Although largely ignored thedomain single-user computers 1980s 1990s, recently gainedpopularity ■the increasing importance isolation security modern systems; ■the failures security reliability standard operating systems; ■the sharing single computer among many unrelated users, datacenter cloud; ■the dramatic increases raw speed processors, make overheadof VMs acceptable. broadest definition VMs includes basically emulation methods provide standard software interface, Java VM. interested in120 ■Chapter Two Memory Hierarchy DesignVMs provide complete system-level environment binary instruction set architecture (ISA) level. often, VM supports ISA under- lying hardware; however, also possible support different ISA, suchapproaches often employed migrating ISAs order allowsoftware departing ISA used ported newISA. focus VMs ISA presented VM andthe underlying hardware match. VMs called (operating) system virtual machines . IBM VM/370, VMware ESX Server, Xen examples. pre- sent illusion users VM entire computer themselves, including copy operating system. single computer runs multiple VMs support number different operating systems (OSes). conventionalplatform, single OS “owns ”all hardware resources, VM, multiple OSes share hardware resources. software supports VMs called virtual machine monitor (VMM) hypervisor ; VMM heart virtual machine technology. underlying hardware platform called host, resources shared among guest VMs. VMM determines map virtual resources physical resources: physical resource may time-shared, partitioned, even emulated software. VMM much smaller traditional OS; isolation portion VMM isperhaps 10,000 lines code. general, cost processor virtualization depends workload. User- level processor-bound programs, SPECCPU2006, zero virtualizationoverhead OS rarely invoked, everything runs native speeds.Conversely, I/O-intensive workloads generally also OS-intensive executemany system calls (which I/O requires) privileged instructions result high virtualization overhead. overhead determined number instructions must emulated VMM slowly emu-lated. Therefore, guest VMs run ISA host, assumehere, goal architecture VMM run almost instructionsdirectly native hardware. hand, I/O-intensive workloadis also I/O-bound , cost processor virtualization completely hidden low processor utilization often waiting I/O. Although interest VMs improving protection, VMs provide two benefits commercially significant: 1.Managing software —VMs provide abstraction run complete software stack, even including old operating systems DOS. typicaldeployment might VMs running legacy OSes, many running cur-rent stable OS release, testing next OS release. 2.Managing hardware —One reason multiple servers applica- tion running compatible version operating system sep-arate computers, separation improve dependability. VMs allowthese separate software stacks run independently yet share hardware, therebyconsolidating number servers. Another example newer VMMs support migration running VM different computer, either to2.4 Virtual Memory Virtual Machines ■121balance load evacuate failing hardware. rise cloud computing made ability swap entire VM another physical processor increasingly useful. two reasons cloud-based servers, Amazon ’s, rely virtual machines. Requirements Virtual Machine Monitor must VM monitor do? presents software interface guest software, must isolate state guests other, must protect guest software (including guest OSes). qualitative requirements ■Guest software behave VM exactly running thenative hardware, except performance-related behavior limitations offixed resources shared multiple VMs. ■Guest software able directly change allocation real systemresources. To“virtualize ”the processor, VMM must control everything — access privileged state, address translation, I/O, exceptions interrupts —even though guest VM OS currently running temporarily using them. example, case timer interrupt, VMM would suspend cur- rently running guest VM, save state, handle interrupt, determine guestVM run next, load state. Guest VMs rely timer interrupt areprovided virtual timer emulated timer interrupt VMM. charge, VMM must higher privilege level guest VM, generally runs user mode; also ensures execution anyprivileged instruction handled VMM. basic requirements sys- tem virtual machines almost identical previously mentioned paged virtual memory: ■At least two processor modes, system user. ■A privileged subset instructions available system mode, result-ing trap executed user mode. system resources must controllableonly via instructions. Instruction Set Architecture Support Virtual Machines VMs planned design ISA, ’s relatively easy reduce number instructions must executed VMM long takes emulate them. architecture allows VM execute directly onthe hardware earns title virtualizable , IBM 370 architecture proudly bears label.122 ■Chapter Two Memory Hierarchy DesignHowever, VMs considered desktop PC-based server applications fairly recently, instruction sets created without virtua- lization mind. culprits include 80x86 original RISC archi-tectures, although latter fewer issues 80x86 architecture. Recentadditions x86 architecture attempted remedy earlier shortcom-ings, RISC V explicitly includes support virtualization. VMM must ensure guest system interacts virtual resources, conventional guest OS runs user mode program top theVMM. Then, guest OS attempts access modify information related hardware resources via privileged instruction —for example, reading writing page table pointer —it trap VMM. VMM effect appropriate changes corresponding real resources. Therefore, instruction tries read write sensitive informa- tion traps executed user mode, VMM intercept support avirtual version sensitive information guest OS expects. absence support, measures must taken. VMM must take special precautions locate problematic instructions ensure behave correctly executed guest OS, thereby increasing complexity VMM reducing performance running VM. Sections 2.5 2.7give concrete examples problematic instructions 80x86 architecture. One attractive extension allows VM OS operate different privilegelevels, distinct user level. introducing additionalprivilege level, OS operations —e.g., exceed permissions granted user program require intervention VMM (becausethey cannot affect VM) —can execute directly without overhead trapping invoking VMM. Xen design, examine shortly, makes use three privilege levels. Impact Virtual Machines Virtual Memory I/O Another challenge virtualization virtual memory, guest OS everyVM manages set page tables. make work, VMM separates notions realandphysical memory (which often treated synonymously) makes real memory separate, intermediate level virtual memory andphysical memory. (Some use terms virtual memory, physical memory , machine memory name three levels.) guest OS maps virtual mem- ory real memory via page tables, VMM page tables map guests ’ real memory physical memory. virtual memory architecture specifiedeither via page tables, IBM VM/370 80x86, via TLB structure,as many RISC architectures. Rather pay extra level indirection every memory access, VMM maintains shadow page table maps directly guest virtual address space physical address space hardware. detecting mod-ifications guest ’s page table, VMM ensure shadow page table2.4 Virtual Memory Virtual Machines ■123entries used hardware translations correspond guest OS environment, exception correct physical pages substituted real pages guest tables. Therefore VMM must trap attempt theguest OS change page table access page table pointer. com-monly done write protecting guest page tables trapping access thepage table pointer guest OS. previously noted, latter happens naturallyif accessing page table pointer privileged operation. IBM 370 architecture solved page table problem 1970s additional level indirection managed VMM. guest OS keeps page tables before, shadow pages unnecessary. AMD implemen- ted similar scheme 80x86. virtualize TLB many RISC computers, VMM manages real TLB copy contents TLB guest VM. pull off,any instructions access TLB must trap. TLBs Process ID tags sup-port mix entries different VMs VMM, thereby avoiding flushingof TLB VM switch. Meanwhile, background, VMM supports amapping VMs ’virtual Process IDs real Process IDs. Section L.7 online Appendix L describes additional details. final portion architecture virtualize I/O. far difficult part system virtualization increasing number I/Odevices attached computer andthe increasing diversity I/O device types. Another difficulty sharing real device among multiple VMs, yetanother comes supporting myriad device drivers required, espe-cially different guest OSes supported VM system. VM illu-sion maintained giving VM generic versions type I/O device driver, leaving VMM handle real I/O. method mapping virtual-to-physical I/O device depends type device. example, physical disks normally partitioned VMM tocreate virtual disks guest VMs, VMM maintains mapping virtualtracks sectors physical ones. Network interfaces often sharedbetween VMs short time slices, job VMM keep trackof messages virtual network addresses ensure guest VMs receiveonly messages intended them. Extending Instruction Set Efficient Virtualization Better Security past 5 –10 years, processor designers, including AMD Intel (and lesser extent ARM), introduced instruction set extensions effi-ciently support virtualization. Two primary areas performance improvement handling page tables TLBs (the cornerstone virtual memory) I/O, specifically handling interrupts DMA. Virtual memory perfor-mance enhanced avoiding unnecessary TLB flushes using nestedpage table mechanism, employed IBM decades earlier, rather complete124 ■Chapter Two Memory Hierarchy Designset shadow page tables (see Section L.7 Appendix L). improve I/O per- formance, architectural extensions added allow device directly use DMA move data (eliminating potential copy VMM) allow deviceinterrupts commands handled guest OS directly. extensionsshow significant performance gains applications intensive either theirmemory-management aspects use I/O. broad adoption public cloud systems running critical applica- tions, concerns risen security data applications. mali-cious code able access higher privilege level data must kept secure compromises system. example, running credit card processing application, must absolutely certain malicious users cannotget access credit card numbers, even using hardwareand intentionally attack OS even VMM. use virtualiza-tion, prevent accesses outside user data different VM, andthis provides significant protection compared multiprogrammed environment.That might enough, however, attacker compromises VMM canfind information observations another VMM. example, suppose attacker penetrates VMM; attacker remap memory access portion data. Alternatively, attack might rely Trojan horse (see Appendix B ) intro- duced code access credit cards. Trojan horse isrunning VM credit card processing application, Trojan horseonly needs exploit OS flaw gain access critical data. cyberat-tacks used form Trojan horse, typically exploiting OS flaw, thateither effect returning access attacker leaving CPU still privilege mode allows attacker upload execute code part OS. either case, attacker obtains control CPU and, using thehigher privilege mode, proceed access anything within VM. Note thatencryption alone prevent attacker. data memory unen-crypted, typical, attacker access data. Furthermore,if attacker knows encryption key stored, attacker freelyaccess key access encrypted data. recently, Intel introduced set instruction set extensions, called software guard extensions (SGX), allow user programs create enclaves , por- tions code data always encrypted decrypted use andonly key provided user code. enclave alwaysencrypted, standard OS operations virtual memory I/O access theenclave (e.g., move page) cannot extract information. enclaveto work, code data required must part enclave. Althoughthe topic finer-grained protection around decades, gotten littletraction high overhead solutions efficient less intrusive acceptable. rise cyberattacks amount confidential information online led reexamination tech-niques improving fine-grained security. Like Intel ’s SGX, IBM AMD ’s recent processors support on-the-fly encryption memory.2.4 Virtual Memory Virtual Machines ■125An Example VMM: Xen Virtual Machine Early development VMs, number inefficiencies became apparent. example, guest OS manages virtual-to-real page mapping, mapping isignored VMM, performs actual mapping physical pages. Inother words, significant amount wasted effort expended keep theguest OS happy. reduce inefficiencies, VMM developers decided thatit may worthwhile allow guest OS aware running aVM. example, guest OS could assume real memory large virtualmemory memory management required guest OS. Allowing small modifications guest OS simplify virtualization referred paravirtualization , open source Xen VMM good exam- ple. Xen VMM, used Amazon ’s web services data centers, pro- vides guest OS virtual machine abstraction similar thephysical hardware, drops many troublesome pieces. example, toavoid flushing TLB, Xen maps upper 64 MiB address spaceof VM. Xen allows guest OS allocate pages, checking surethe guest OS violate protection restrictions. protect guest OS user programs VM, Xen takes advantage four protection levels available 80x86. Xen VMM runs highest privilege level (0),the guest OS runs next level (1), applications run lowest priv-ilege level (3). OSes 80x86 keep everything privilege levels 0 3. subsetting work properly, Xen modifies guest OS use prob- lematic portions architecture. example, port Linux Xen changesabout 3000 lines, 1% 80x86-specific code. changes, how-ever, affect application binary interfaces guest OS. simplify I/O challenge VMs, Xen assigned privileged virtual machines hardware I/O device. special VMs called driver domains . (Xen calls VMs “domains. ”) Driver domains run physical device drivers, although interrupts still handled VMM sent tothe appropriate driver domain. Regular VMs, called guest domains , run simple vir- tual device drivers must communicate physical device drivers thedriver domains channel access physical I/O hardware. Data sentbetween guest driver domains page remapping. 2.5 Cross-Cutting Issues: Design Memory Hierarchies section describes four topics discussed chapters fundamentalto memory hierarchies. Protection, Virtualization, Instruction Set Architecture Protection joint effort architecture operating systems, architects modify awkward details existing instruction set architectures vir- tual memory became popular. example, support virtual memory IBM126 ■Chapter Two Memory Hierarchy Design370, architects change successful IBM 360 instruction set architecture announced 6 years before. Similar adjustments made today accommodate virtual machines. example, 80x86 instruction POPF loads flag registers top stack memory. One flags Interrupt Enable (IE) flag. Untilrecent changes support virtualization, running POPF instruction user mode, rather trapping it, simply changed flags except IE. systemmode, change IE flag. guest OS runs user mode insidea VM, problem, OS would expect see changed IE. Extensions 80x86 architecture support virtualization eliminated problem. Historically, IBM mainframe hardware VMM took three steps improve performance virtual machines: 1.Reduce cost processor virtualization. 2.Reduce interrupt overhead cost due virtualization. 3.Reduce interrupt cost steering interrupts proper VM without invoking VMM. IBM still gold standard virtual machine technology. example, IBM mainframe ran thousands Linux VMs 2000, Xen ran 25 VMs 2004(Clark et al., 2004 ). Recent versions Intel AMD chipsets added special instructions support devices VM mask interrupts lower levels VM steer interrupts appropriate VM. Autonomous Instruction Fetch Units Many processors out-of-order execution even simply deep pipelines decouple instruction fetch (and sometimes initial decode), using aseparate instruction fetch unit (see Chapter 3 ). Typically, instruction fetch unit accesses instruction cache fetch entire block decoding indi-vidual instructions; technique particularly useful instructionlength varies. instruction cache accessed blocks, longer makes sense compare miss rates processors access instruction cache per instruction. addition, instruction fetch unit may prefetch blocks intothe L1 cache; prefetches may generate additional misses, may actuallyreduce total miss penalty incurred. Many processors also include data prefetch-ing, may increase data cache miss rate, even decreasing totaldata cache miss penalty. Speculation Memory Access One major techniques used advanced pipelines speculation, whereby aninstruction tentatively executed processor knows whether really needed. techniques rely branch prediction, incorrect requires that2.5 Cross-Cutting Issues: Design Memory Hierarchies ■127the speculated instructions flushed pipeline. two separate issues memory system supporting speculation: protection performance. speculation, processor may generate memory references, willnever used instructions result incorrect speculation.Those references, executed, could generate protection exceptions. Obviously,such faults occur instruction actually executed. nextchapter, see “speculative exceptions ”are resolved. speculative processor may generate accesses instruction datacaches, subsequently use results accesses, speculation may increase cache miss rates. prefetching, however, speculation may actually lower total cache miss penalty. use speculation, like theuse prefetching, makes misleading compare miss rates seen pro-cessors without speculation, even ISA cache structures otherwiseidentical. Special Instruction Caches One biggest challenges superscalar processors supply instruc-tion bandwidth. designs translate instructions micro-operations,such recent Arm i7 processors, instruction bandwidth demands andbranch misprediction penalties reduced keeping small cache recently translated instructions. explore technique greater depth next chapter. Coherency Cached Data Data found memory cache. long processor solecomponent changing reading data cache stands proces-sor memory, little danger processor seeing old stale copy. see, multiple processors I/O devices raise opportunity copiesto inconsistent read wrong copy. frequency cache coherency problem different multiprocessors I/O. Multiple data copies rare event I/O —one avoided when- ever possible —but program running multiple processors want copies data several caches. Performance multiprocessor pro-gram depends performance system sharing data. TheI/O cache coherency question this: I/O occur com- puter—between I/O device cache I/O device main memory? input puts data cache output reads data cache,both I/O processor see data. difficulty approach interferes processor cause processor stall I/O. Input may also interfere cache displacing information new datathat unlikely accessed soon.128 ■Chapter Two Memory Hierarchy DesignThe goal I/O system computer cache prevent stale data problem interfering little possible. Many systems therefore prefer I/O occur directly main memory, main memory acting I/O buffer.If write-through cache used, memory would up-to-date copy ofthe information, would stale data issue output. (This benefit areason processors used write through.) However, today write usuallyfound first-level data caches backed L2 cache uses write back. Input requires extra work. software solution guarantee blocks input buffer cache. page containing buffer marked noncachable, operating system always input page. Alternatively, operating system flush buffer addresses cachebefore input occurs. hardware solution check I/O addresses inputto see cache. match I/O addresses cache, thecache entries invalidated avoid stale data. approaches also beused output write-back caches. Processor cache coherency critical subject age multicore proces- sors, examine detail Chapter 5 . 2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 section reveals ARM Cortex-A53 (hereafter called A53) Intel Core i76700 (hereafter called i7) memory hierarchies shows performance components set single-threaded benchmarks. examine theCortex-A53 first simpler memory system; go detailfor i7, tracing memory reference detail. section presumes thatreaders familiar organization two-level cache hierarchy using vir-tually indexed caches. basics memory system explained detailinAppendix B , readers uncertain organization system strongly advised review Opteron example Appendix B . understand organization Opteron, brief explanation A53 sys- tem, similar, easy follow. ARM Cortex-A53 Cortex-A53 configurable core supports ARMv8A instruction setarchitecture, includes 32-bit 64-bit modes. Cortex-A53 isdelivered IP (intellectual property) core. IP cores dominant formof technology delivery embedded, PMD, related markets; billions ofARM MIPS processors created IP cores. Note IP cores different cores Intel i7 AMD Athlon multicores. IP core (which may multicore) designed incorporated withother logic (thus core chip), including application-specific processors2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■129(such encoder decoder video), I/O interfaces, memory interfaces, fabricated yield processor optimized particular application. example, Cortex-A53 IP core used variety tablets smartphones; itis designed highly energy-efficient, key criteria battery-based PMDs. TheA53 core capable configured multiple cores per chip use inhigh-end PMDs; discussion focuses single core. Generally, IP cores come two flavors. Hard cores optimized par- ticular semiconductor vendor black boxes external (but still on-chip)interfaces. Hard cores typically allow parametrization logic outside core, L2 cache sizes, IP core cannot modified. Soft cores usually delivered form uses standard library logic elements. softcore compiled different semiconductor vendors also modi-fied, although extensive modifications difficult complexityof modern-day IP cores. general, hard cores provide higher performance andsmaller die area, soft cores allow retargeting vendors bemore easily modified. Cortex-A53 issue two instructions per clock clock rates 1.3 GHz. supports two-level TLB two-level cache; Figure 2.19 sum- marizes organization memory hierarchy. critical term returnedfirst, processor continue miss completes; memory systemwith four banks supported. D-cache 32 KiB page size of4 KiB, physical page could map two different cache addresses; aliasesare avoided hardware detection miss Section B.3 Appendix B . Figure 2.20 shows 32-bit virtual address used index TLB caches, assuming 32 KiB primary caches 1 MiB secondary cache 16 KiB page size. Structure Size OrganizationTypical miss penalty (clock cycles) Instruction MicroTLB 10 entries Fully associative 2 Data MicroTLB 10 entries Fully associative 2L2 Unified TLB 512 entries 4-way set associative 20L1 Instruction cache 8 –64 KiB 2-way set associative; 64-byte block 13 L1 Data cache 8 –64 KiB 2-way set associative; 64-byte block 13 L2 Unified cache 128 KiB 2 MiB 16-way set associative; LRU 124 Figure 2.19 memory hierarchy Cortex A53 includes multilevel TLBs caches. page map cache keeps track location physical page set virtual pages; reduces L2 TLB miss penalty. L1 caches virtually indexed physically tagged; L1 cache L2 use write-back policy defaulting allocate write. Replacement policy LRU approximation caches. Miss penalties L2 higher botha MicroTLB L1 miss occur. L2 main memory bus 64 –128 bits wide, miss penalty larger narrow bus.130 ■Chapter Two Memory Hierarchy DesignVirtual address <32> Physical address <32> L2 tag compare address <16> L2 cache index <10> Block offset <6>Real page number <16> L2 cache tag <16> L2 data <64 bytes>=? =?To CPUTo CPU CPU L1 cache CPUL1 cache tag <19> L1 data <64 bytes>TLB tag <16>Instruction TLBVirtual address <32> Physical address <32>Page offset <16> Virtual page number <16> Page offset <16> Virtual page number <16>Real page number <16> =?To CPU CPUL1 cache index <10> Block offset <6> L1 cache index <10> Block offset <6>L1 cache tag <18> L1 data <64 bytes>TLB tag <16> L2 (see part b below) instruction access path (A) (B) data access pathInstruction cache8 2 Data TLB Data cache7 3 =?L2 TLB 7 9Real page number <16> TLB tag <9> Figure 2.20 virtual address, physical data blocks ARM Cortex-A53 caches TLBs, assuming 32- bit addresses. top half (A) shows instruction access; bottom half (B) shows data access, including L2. TLB (instruction data) fully associative 10 entries, using 64 KiB page example. L1 I- cache two-way set associative, 64-byte blocks 32 KiB capacity; L1 D-cache 32 KiB, four-way set asso- ciative, 64-byte blocks. L2 TLB 512 entries four-way set associative. L2 cache 16-way set asso-ciative 64-byte blocks 128 cKiB 2 MiB capacity; 1 MiB L2 shown. figure ’t show valid bits protection bits caches TLB.Performance Cortex-A53 Memory Hierarchy memory hierarchy Cortex-A8 measured 32 KiB primary caches 1 MiB L2 cache running SPECInt2006 benchmarks. instruc-tion cache miss rates SPECInt2006 small even L1:close zero 1% them. low rate probably resultsfrom computationally intensive nature SPECCPU programs two-way set associative cache eliminates conflict misses. Figure 2.21 shows data cache results, significant L1 L2 miss rates. L1 rate varies factor 75, 0.5% 37.3% median miss rate 2.4%. global L2 miss rate varies factor 180, 0.05% 9.0% median 0.3%. MCF, known cache buster,sets upper bound significantly affects mean. Remember L2global miss rate significantly lower L2 local miss rate; example,the median L2 stand-alone miss rate 15.1% versus global miss rate 0.3%. Using miss penalties Figure 2.19, Figure 2.22 shows average pen- alty per data access. Although L1 miss rates seven times higher thanthe L2 miss rate, L2 penalty 9.5 times high, leading L2 misses slightly dominating benchmarks stress memory system. next chapter, examine impact cache misses overall CPI. hmmer h264ref libquantumbzip2gobmk xalancbmkgccastar omnetppmcfsjeng perlbench0.0%5.0%10.0%15.0%20.0%25.0%30.0%35.0%40.0%L1 data miss rate L2 data miss rate Figure 2.21 data miss rate ARM 32 KiB L1 global data miss rate 1 MiB L2 using SPECInt2006 benchmarks significantly affected applications. Applications larger memory footprints tend higher miss rates L1 L2. Note L2 rate global miss rate counting allreferences, including hit L1. MCF known cache buster.132 ■Chapter Two Memory Hierarchy DesignThe Intel Core i7 6700 i7 supports x86-64 instruction set architecture, 64-bit extension 80x86 architecture. i7 out-of-order execution processor includes fourcores. chapter, focus memory system design performancefrom viewpoint single core. system performance multiprocessor designs, including i7 multicore, examined detail Chapter 5 . core i7 execute four 80x86 instructions per clock cycle, using multiple issue, dynamically scheduled, 16-stage pipeline, wedescribe detail Chapter 3 . i7 also support two simultaneous threads per processor, using technique called simultaneous multithreading,described Chapter 4 . 2017 fastest i7 clock rate 4.0 GHz (in Turbo Boost mode), yielded peak instruction execution rate 16 billion instruc-tions per second, 64 billion instructions per second four-core design. course, big gap peak sustained performance, see next chapters. i7 support three memory channels, consisting separate set DIMMs, transfer parallel. Using DDR3-1066(DIMM PC8500), i7 peak memory bandwidth 25 GB/s.hmmer h264ref libquantumbzip2gobmk xalancbmkgccastar omnetppmcfsjeng perlbench02468Miss penalty per data reference 10121416 L2 data average memory penalty L1 data average memory penalty Figure 2.22 average memory access penalty per data memory reference coming L1 L2 shown A53 processor running SPECInt2006. Although miss rates L1 significantly higher, L2 miss penalty, five times higher, means L2 misses contribute significantly.2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■133i7 uses 48-bit virtual addresses 36-bit physical addresses, yielding maximum physical memory 36 GiB. Memory management handled two-level TLB (see Appendix B , Section B.4), summarized Figure 2.23 . Figure 2.24 summarizes i7 ’s three-level cache hierarchy. first-level caches virtually indexed physically tagged (see Appendix B , Section B.3), L2 L3 caches physically indexed. versionsof i7 6700 support fourth-level cache using HBM packaging. Figure 2.25 labeled steps access memory hierarchy. First, PC sent instruction cache. instruction cache index 2Index¼Cache size Block size /C2Set associativity¼32K 64/C28¼64¼26 Characteristic Instruction TLB Data DLB Second-level TLB Entries 128 64 1536 Associativity 8-way 4-way 12-wayReplacement Pseudo-LRU Pseudo-LRU Pseudo-LRUAccess latency 1 cycle 1 cycle 8 cyclesMiss 9 cycles 9 cycles Hundreds cycles access page table Figure 2.23 Characteristics i7 ’s TLB structure, separate first-level instruction data TLBs, backed joint second-level TLB. first-level TLBs support standard 4 KiB page size, well limited number entries oflarge 2 –4 MiB pages; 4 KiB pages supported second-level TLB. i7 ability handle two L2 TLB misses parallel. See Section L.3 online Appendix L discussion multilevel TLBs support multiple page sizes. Characteristic L1 L2 L3 Size 32 KiB I/32 KiB 256 KiB 2 MiB per core Associativity 8-way 4-way 16-wayAccess latency 4 cycles, pipelined 12 cycles 44 cycles Replacement scheme Pseudo-LRU Pseudo-LRU Pseudo-LRU ordered selection algorithm Figure 2.24 Characteristics three-level cache hierarchy i7. three caches use write back block size 64 bytes. L1 L2 caches separate core, whereas L3 cache shared among cores chip total of2 MiB per core. three caches nonblocking allow multiple outstanding writes. merging write buffer used L1 cache, holds data event line present L1 written. (That is, L1 write miss cause theline allocated.) L3 inclusive L1 L2; explore property detailwhen explain multiprocessor caches. Replacement variant pseudo-LRU; case L3, block replaced always lowest numbered way whose access bit off. quite random easy compute.134 ■Chapter Two Memory Hierarchy DesignData <128x4> Data <512>Virtual page number <36> Data <64>Instruction <128> <128> <7><64> <30>Page offset <12> PCCPU 2:1 mux <20> Tag<10>L2 C C H E C C H EData virtual page number <36> Page offset <12> <6> Index Block offset C C H EI L B L2 L B <24>=? 8:1 mux8:1 mux 12:1 mux2 1 3 55 6 8 9716 10V <1>D <1> V <1>D <1>Tag <21> V <1>D <1>Tag <17>4:1 mux=? =?<7> <6> DC C H ED L B 8:1 mux<4> Prot<1> V 4:1 mux(64 PTEs 4 banks)(128 PTEs 8 banks)<31> Tag<24> Physical address<4> Prot<1> V<32> Tag<24> Physical address <4> Prot<1> V<29> Tag<24> Physical address =?(1536 PTEs 12 banks) (512 blocks 8 banks)Data <128 ×4> (512 blocks 8 banks)Data <64> (4K blocks 4 banks) Data <512> <17> <13>L311 12 13 16:1 mux=? (128K blocks 16 banks)<64> <64> DIMM DIMMM NM E RY15Memory Interface<64>DIMM 14 164 Index Tag IndexV <1>D <1>Tag <24>Index Block offset<6> <6><24> <28> Figure 2.25 Intel i7 memory hierarchy steps instruction data access. show reads. Writes similar, except misses handled simply placing data write buffer, L1 cache write-allocated.2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■135or 6 bits. page frame instruction ’s address (36 ¼48/C012 bits) sent instruction TLB (step 1). time, 12-bit page offset vir-tual address sent instruction cache (step 2). Notice eight-wayassociative instruction cache, 12 bits needed cache address: 6 bits toindex cache plus 6 bits block offset 64-byte block, aliasesare possible. previous versions i7 used four-way set associativeI-cache, meaning block corresponding virtual address could actuallybe two different places cache, corresponding physical address could either 0 1 location. instructions pose prob- lem even instruction appeared cache two different locations,the two versions must same. duplication, aliasing, data isallowed, cache must checked page map changed, aninfrequent event. Note simple use page coloring (see Appendix B , Section B.3) eliminate possibility aliases. even-address virtualpages mapped even-address physical pages (and odd pages),then aliases never occur low-order bit virtual phys- ical page number identical. instruction TLB accessed find match address valid page table entry (PTE) (steps 3 4). addition translating address, theTLB checks see PTE demands access result exceptionbecause access violation. instruction TLB miss first goes L2 TLB, contains 1536 PTEs 4 KiB page sizes 12-way set associative. takes 8 clock cycles toload L1 TLB L2 TLB, leads 9-cycle miss penalty including initial clock cycle access L1 TLB. L2 TLB misses, hardware algorithm used walk page table update TLB entry.Sections L.5 L.6 online Appendix L describe page table walkers pagestructure caches. worst case, page memory, operatingsystem gets page secondary storage. millions instructionscould execute page fault, operating system swap another pro-cess one waiting run. Otherwise, TLB exception, instruc-tion cache access continues. index field address sent eight banks instruction cache (step 5). instruction cache tag 36 bits /C06 bits (index) /C06 bits (block offset), 24 bits. four tags valid bits compared physical page framefrom instruction TLB (step 6). i7 expects 16 bytes instructionfetch, additional 2 bits used 6-bit block offset select appro-priate 16 bytes. Therefore 6+2 8 bits used send 16 bytes instructions tothe processor. L1 cache pipelined, latency hit 4 clock cycles(step 7). miss goes second-level cache. mentioned earlier, instruction cache virtually addressed physi- cally tagged. second-level caches physically addressed, phys-ical page address TLB composed page offset make anaddress access L2 cache. L2 index 2Index¼Cache size Block size /C2Set associativity¼256K 64/C24¼1024¼210136 ■Chapter Two Memory Hierarchy Designso 30-bit block address (36-bit physical address /C06-bit block offset) divided 20-bit tag 10-bit index (step 8). again, index tag sentto four banks unified L2 cache (step 9), compared parallel. Ifone matches valid (step 10), returns block sequential order theinitial 12-cycle latency rate 8 bytes per clock cycle. L2 cache misses, L3 cache accessed. four-core i7, 8 MiB L3, index size 2Index¼Cache size Block size /C2Set associativity¼8M 64/C216¼8192¼213 13-bit index (step 11) sent 16 banks L3 (step 12). L3 tag, 36 /C0(13+6) ¼17 bits, compared physical address TLB (step 13). hit occurs, block returned initial latency 42clock cycles, rate 16 bytes per clock placed L1 L3. L3 misses, memory access initiated. instruction found L3 cache, on-chip memory controller must get block main memory. i7 three 64-bit memory channelsthat act one 192-bit channel, one memory controllerand address sent channels (step 14). Wide transfers happenwhen channels identical DIMMs. channel supports fourDDR DIMMs (step 15). data return placed L3 L1 (step16) L3 inclusive. total latency instruction miss serviced main memory approximately 42 processor cycles determine L3 miss occurred, plusthe DRAM latency critical instructions. single-bank DDR4-2400SDRAM 4.0 GHz CPU, DRAM latency 40 ns 160 clock cyclesto first 16 bytes, leading total miss penalty 200 clock cycles. Thememory controller fills remainder 64-byte cache block rate 16bytes per I/O bus clock cycle, takes another 5 ns 20 clock cycles. second-level cache write-back cache, miss lead old block written back memory. i7 10-entry merging write buffer writes back dirty cache lines next level cache unusedfor read. write buffer checked miss see cache line exists thebuffer; so, miss filled buffer. similar buffer used betweenthe L1 L2 caches. initial instruction load, data address sentto data cache data TLBs, acting much like instruction cache access. Suppose instruction store instead load. store issues, data cache lookup like load. miss causes block placed write buffer L1 cache allocate block write miss. hit, store update L1 (or L2) cache later, knownto nonspeculative. time, store resides load-store queue, partof out-of-order control mechanism processor. I7 also supports prefetching L1 L2 next level hierarchy. cases, prefetched line simply next block cache.By prefetching L1 L2, high-cost unnecessary fetches memory areavoided.2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■137Performance i7 memory system evaluate performance i7 cache structure using SPECint2006 benchmarks. data section collected Professor Lu Peng PhD student Qun Liu, Louisiana State University. analysis based earlier work (see Prakash Peng, 2008 ). complexity i7 pipeline, use autonomous instruction fetch unit, speculation, instruction data prefetch, makes hard tocompare cache performance simpler processors. mentioned page110, processors use prefetch generate cache accesses independent ofthe memory accesses performed program. cache access generatedbecause actual instruction access data access sometimes called demand access distinguish prefetch access . Demand accesses come speculative instruction fetches speculative data accesses,some subsequently canceled (see Chapter 3 detailed description speculation instruction graduation). speculative processor generates atleast many misses in-order nonspeculative processor, typically more.In addition demand misses, prefetch misses instructionsand data. i7 ’s instruction fetch unit attempts fetch 16 bytes every cycle, com- plicates comparing instruction cache miss rates multiple instructions fetched every cycle (roughly 4.5 average). fact, entire 64-byte cache lineis read subsequent16-byte fetches require additional accesses. Thus missesare tracked basis 64-byte blocks. 32 KiB, eight-way set associativeinstruction cache leads low instruction miss rate SPECint2006programs. If, simplicity, measure miss rate SPECint2006 numberof misses 64-byte block divided number instructions complete, themiss rates 1% except one benchmark (XALANCBMK), 2.9% miss rate. 64-byte block typically contains 16 –20 instructions, effective miss rate per instruction much lower, depending degree spatiallocality instruction stream. frequency instruction fetch unit stalled waiting I-cache misses similarly small (as percentage total cycles) increasing to2% two benchmarks 12% XALANCBMK, highestI-cache miss rate. next chapter, see stalls IFU contributeto overall reductions pipeline throughput i7. L1 data cache interesting even trickier evaluate addition effects prefetching speculation, L1 data cache notwrite-allocated, writes cache blocks present treated asmisses. reason, focus memory reads. performance monitormeasurements i7 separate prefetch accesses demand accesses, butonly keep demand accesses instructions graduate. effect spec-ulative instructions graduate negligible, although pipeline effectsprobably dominate secondary cache effects caused speculation; return issue next chapter.138 ■Chapter Two Memory Hierarchy DesignTo address issues, keeping amount data reasonable, Figure 2.26 shows L1 data cache misses two ways: 1.The L1 miss rate relative demand references given L1 miss rate includ- ing prefetches speculative loads/L1 demand read references thoseinstructions graduate.ASTARBZIP2GCC HMMER LIBQUANTUMMCF OMNETPP PERLBENCHSJENG XALANCBMKH264REFGOBMK0%L1 miss rate prefetches demand reads L1 miss rate demand reads 5%10%15%Miss rate20%25%30%35%40%45% 11% 3%5% 2%18% 4%3% 1%5% 1%5% 1%35% 11%41% 22% 15% 7% 2%1% 1%1%6% 3% Figure 2.26 L1 data cache miss rate SPECint2006 benchmarks shown two ways relative demand L1 reads: one including demand prefetch accesses one including demand accesses.The i7 separates L1 misses block present cache L1 misses block already outstanding prefetched L2; treat latter group hits would hit blocking cache. data, like rest section, collected Professor Lu Peng PhD student Qun Liu, Louisiana StateUniversity, based earlier studies Intel Core Duo processors (see Peng et al., 2008).2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■1392.The demand miss rate given L1 demand misses/L1 demand read references, measurements instructions graduate. average, miss rate including prefetches 2.8 times high demand- miss rate. Comparing data earlier i7 920, thesame size L1, see miss rate including prefetches higher neweri7, number demand misses, likely cause stall, areusually fewer. understand effectiveness aggressive prefetch mechanisms i7, let ’s look measurements prefetching. Figure 2.27 shows fraction L2 requests prefetches versus demand requests prefetchmiss rate. data probably astonishing first glance: roughly1.5 times many prefetches L2 demand requests, come directlyfrom L1 misses. Furthermore, prefetch miss rate amazingly high, anaverage miss rate 58%. Although prefetch ratio varies considerably, pre-fetch miss rate always significant. first glance, might conclude thedesigners made mistake: prefetching much, miss rate high. Notice, however, benchmarks higher prefetch ratios (ASTAR, BZIP2, HMMER, LIBQUANTUM, OMNETPP) also show thegreatest gap prefetch miss rate demand miss rate, thana factor 2 case. aggressive prefetching trading prefetch misses,which occur earlier, demand misses, occur later; result, pipe-line stall less likely occur due prefetching. Similarly, consider high prefetch miss rate. Suppose majority prefetches actually useful (this hard measure involves tracking individual cache blocks), prefetch miss indicates likely L2 cache miss future. Uncovering handling miss earlier via prefetch likely toreduce stall cycles. Performance analysis speculative superscalars, likethe i7, shown cache misses tend primary cause pipeline stalls,because hard keep processor going, especially longer running L2 andL3 misses. Intel designers could easily increase size caches with-out incurring energy cycle time impacts; thus use aggressive pre-fetching try lower effective cache miss penalties interesting alternative approach. combination L1 demand misses prefetches going L2, roughly 17% loads generate L2 request. Analyzing L2 performancerequires including effects writes (because L2 write-allocated), wellas prefetch hit rate demand hit rate. Figure 2.28 shows miss rates L2 caches demand prefetch accesses, versus number L1references (reads writes). L1, prefetches significant contributor,generating 75% L2 misses. Comparing L2 demand miss rate earlier i7 implementations (again L2 size) shows i7 6700 lower L2 demand miss rate approximate factor 2, may well justifythe higher prefetch miss rate.140 ■Chapter Two Memory Hierarchy DesignBecause cost miss memory 100 cycles average data miss rate L2 combining prefetch demand misses 7%, L3 obvi-ously critical. Without L3 assuming one-third instructions loads stores, L2 cache misses could add two cycles per instruction CPI! Obviously, prefetching past L2 would make sense without L3. comparison, average L3 data miss rate 0.5% still significant less one-third L2 demand miss rate 10 times less L1 demandmiss rate. two benchmarks (OMNETPP MCF) L3 miss rateASTARBZIP2GCC HMMER LIBQUANTUMMCF OMNETPP PERLBENCHSJENG XALANCBMKH264REFGOBMK0 0%10%20%30%40%50%60%70%80%90%100% 0.51.01.5Prefetches LA/All L2 demand references Prefetch miss rate 2.02.53.03.54.05.0 4.5 Prefetches/demand accesses Prefetches miss ratio Figure 2.27 fraction L2 requests prefetches shown via columns left axis. right axis line shows prefetch hit rate. data, like rest section, collected Professor Lu Peng PhD student Qun Liu, Louisiana State University, based earlier studies Intel Core Duo andother processors (see Peng et al., 2008).2.6 Putting Together: Memory Hierarchies ARM Cortex-A53 Intel Core i7 6700 ■141above 0.5%; two cases, miss rate 2.3% likely dominates performance losses. next chapter, examine relationship i7 CPI cache misses, well pipeline effects. 2.7 Fallacies Pitfalls naturally quantitative computer architecture disciplines, mem- ory hierarchy would seem less vulnerable fallacies pitfalls. Yet wewere limited lack warnings, lack space! Fallacy Predicting cache performance one program another . Figure 2.29 shows instruction miss rates data miss rates three programs SPEC2000 benchmark suite cache size varies. Depending L2 miss rate astar bzip2gcc hmmer libquantummcf omnetppperlbenchsjeng xalancbmkh264ref gobmk0%2%4%6%8%10%12%14%16%18%20%22% 2%7% 1%3% 1%10% 0%1% 0%1% 0%3% 0%4%12%22% 4%11% 0%1% 0%0%1%3%L2 demand miss rate L2 prefetch miss rate Figure 2.28 L2 demand miss rate prefetch miss rate, shown relative references L1, also includes prefetches, speculative loads complete, program-generated loads stores (demand references). data, like rest section, collected Professor Lu Peng PhD student QunLiu, Louisiana State University.142 ■Chapter Two Memory Hierarchy Designprogram, data misses per thousand instructions 4096 KiB cache 9, 2, 90, instruction misses per thousand instructions 4 KiB cache 55,19, 0.0004. Commercial programs databases significant missrates even large second-level caches, generally case theSPECCPU programs. Clearly, generalizing cache performance one program another unwise. Figure 2.24 reminds us, great deal variation, even predictions relative miss rates integer floating-point-intensive programs wrong, mcf sphnix3 remind us! Pitfall Simulating enough instructions get accurate performance measures memory hierarchy . really three pitfalls here. One trying predict performance large cache using small trace. Another program ’s locality behavior con- stant run entire program. third program ’s locality behavior may vary depending input. Figure 2.30 shows cumulative average instruction misses per thousand instructions five inputs single SPEC2000 program. inputs, theaverage memory rate first 1.9 billion instructions different fromthe average miss rate rest execution. Pitfall delivering high memory bandwidth cache-based system . Caches help average cache memory latency may deliver high memory bandwidth application must go main memory. architect mustdesign high bandwidth memory behind cache applications. willrevisit pitfall Chapters 4and5.020406080100120140160Misses per 1000 instructions 4 16 64 256 1024 4096 Cache size (KB)D: lucas D: gcc D: gap I: gapI: gcc I: lucas Figure 2.29 Instruction data misses per 1000 instructions cache size varies 4 KiB 4096 KiB. Instruction misses gcc 30,000 –40,000 times larger lucas, and, conversely, data misses lucas 2 –60 times larger gcc. programs gap, gcc, lucas SPEC2000 benchmark suite.2.7 Fallacies Pitfalls ■1430123456789Instruction misses per 1000 references Instruction misses per 1000 references0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1.1 1.2 1.3 1.41 2, 3, 4, 5 1.5 1.6 1.7 1.8 1.9 Instructions (billions) 0123456789 0 2 4 6 8 1 01 21 41 61 82 02 22 42 62 83 03 23 43 63 84 04 2 Instructions (billions )5 23 4 1 Figure 2.30 Instruction misses per 1000 references five inputs perl bench- mark SPEC2000. little variation misses little difference five inputs first 1.9 billion instructions. Running completion shows misses vary life program depend input. top graph shows running average misses first 1.9 billion instructions, starts atabout 2.5 ends 4.7 misses per 1000 references five inputs. bot-tom graph shows running average misses run completion, takes 16 –41 billion instructions depending input. first 1.9 billion instructions, misses per 1000 references vary 2.4 7.9 depending input. simulationswere Alpha processor using separate L1 caches instructions data, two-way 64 KiB LRU, unified 1 MiB direct-mapped L2 cache.144 ■Chapter Two Memory Hierarchy DesignPitfall Implementing virtual machine monitor instruction set architecture ’t designed virtualizable . Many architects 1970s 1980s ’t careful make sure instructions reading writing information related hardware resource informa-tion privileged. laissez faire attitude causes problems VMMs architectures, including 80x86, use example. Figure 2.31 describes 18 instructions cause problems paravirtuali- zation ( Robin Irvine, 2000 ). two broad classes instructions ■read control registers user mode reveal guest operating system running virtual machine (such POPF mentioned earlier) ■check protection required segmented architecture assume operating system running highest privilege level. Virtual memory also challenging. 80x86 TLBs support process ID tags, RISC architectures, expensive VMMand guest OSes share TLB; address space change typically requires aTLB flush. Problem category Problem 80x86 instructions Access sensitive registers without trapping running user modeStore global descriptor table register ( SGDT ) Store local descriptor table register ( SLDT ) Store interrupt descriptor table register ( SIDT ) Store machine status word ( SMSW ) Push flags ( PUSHF ,PUSHFD ) Pop flags ( POPF ,POPFD ) accessing virtual memory mechanisms user mode,instructions fail 80x86 protection checksLoad access rights segment descriptor ( LAR) Load segment limit segment descriptor ( LSL) Verify segment descriptor readable ( VERR ) Verify segment descriptor writable ( VERW ) Pop segment register ( POP CS ,POP SS ,…) Push segment register ( PUSH CS ,PUSH SS ,…) Far call different privilege level ( CALL ) Far return different privilege level ( RET) Far jump different privilege level ( JMP) Software interrupt ( INT) Store segment selector register ( STR) Move to/from segment registers ( MOVE ) Figure 2.31 Summary 18 80x86 instructions cause problems virtualization (Robin Irvine, 2000 ).The first five instructions top group allow program user mode read control register, descriptor table register without causinga trap. pop flags instruction modifies control register sensitive informationbut fails silently user mode. protection checking segmented archi- tecture 80x86 downfall bottom group instruc- tions checks privilege level implicitly part instruction execution reading acontrol register. checking assumes OS must highest privilege level, case guest VMs. MOVE segment register tries modify control state, protection checking foils well.2.7 Fallacies Pitfalls ■145Virtualizing I/O also challenge 80x86, part supports memory-mapped I/O separate I/O instructions, importantly large number variety types devices devicedrivers PCs VMM handle. Third-party vendors supply owndrivers, may properly virtualize. One solution conventionalVM implementations load real device drivers directly VMM. simplify implementations VMMs 80x86, AMD Intel proposed extensions architecture. Intel ’s VT-x provides new execu- tion mode running VMs, architected definition VM state, instructions swap VMs rapidly, large set parameters select circumstances VMM must invoked. Altogether, VT-x adds 11 new instructions the80x86. AMD ’s Secure Virtual Machine (SVM) provides similar functionality. turning mode enables VT-x support (via new VMXON instruc- tion), VT-x offers four privilege levels guest OS lower priority thanthe original four (and fix issues like problem POPF instruction mentioned earlier).VT-xcapturesallthestatesofavirtualmachineintheVirtualMachineControlState (VMCS) provides atomic instructions save restore VMCS. addition critical state, VMCS includes configuration information deter- mine invoke VMM specifically caused VMM beinvoked. reduce number times VMM must invoked, mode addsshadow versions sensitive registers adds masks check see whethercritical bits sensitive register changed trapping. reduce costof virtualizing virtual memory, AMD ’s SVM adds additional level indirection, called nested page tables , makes shadow page tables unnecessary (see Section L.7 Appendix L). 2.8 Concluding Remarks: Looking Ahead past thirty years several predictions eminent [sic]cessation rate improvement computer performance. Every pre-diction wrong. wrong hinged unstated assump-tions overturned subsequent events. So, example, failure toforesee move discrete components integrated circuits led predic- tion speed light would limit computer speeds several orders mag- nitude slower now. prediction memory wall probablywrong suggests start thinking “out box. ” Wm. A. Wulf Sally A. McKee, Hitting Memory Wall: Implications Obvious, Department Computer Science, University Virginia (December 1994). paper introduced term memory wall. possibility using memory hierarchy dates back earliest days general-purpose digital computers late 1940s early 1950s. Virtual mem-ory introduced research computers early 1960s IBM main- frames 1970s. Caches appeared around time. basic concepts146 ■Chapter Two Memory Hierarchy Designhave expanded enhanced time help close access time gap main memory processors, basic concepts remain. One trend causing significant change design memory hierar- chies continued slowdown density access time DRAMs. thepast 15 years, trends observed even obvi-ous past 5 years. increases DRAM bandwidth beenachieved, decreases access time come much slowly almost van-ished DDR4 DDR3. end Dennard scaling well slow-down Moore ’s Law contributed situation. trenched capacitor design used DRAMs also limiting ability scale. may well case packaging technologies stacked memory dominant sourceof improvements DRAM access bandwidth latency. Independently improvements DRAM, Flash memory playing much larger role. PMDs, Flash dominated 15 years became stan-dard laptops almost 10 years ago. past years, many desktops haveshipped Flash primary secondary storage. Flash ’s potential advantage DRAMs, specifically absence per-bit transistor control writing, also Achilles heel. Flash must use bulk erase-rewrite cycles consider- ably slower. result, although Flash become fastest growing form ofsecondary storage, SDRAMs still dominate main memory. Although phase-change materials basis memory around while,they haveneverbeenseriouscompetitors eitherformagnetic disksorforFlash.The recent announcement Intel Micron cross-point technology maychangethis.Thetechnology appearstohaveseveraladvantagesoverFlash,includingthe elimination slow erase-to-write cycle greater longevity terms. could technology finally technology replaces electro- mechanical disks dominated bulk storage 50 years! years, variety predictions made coming memory wall (see previously cited quote paper), would lead seriouslimits processor performance. Fortunately, extension caches multiplelevels (from 2 4), sophisticated refill prefetch schemes, greater com-piler programmer awareness importance locality, tremendousimprovements DRAM bandwidth (a factor 150 times since mid- 1990s) helped keep memory wall bay. recent years, combination access time constraints size L1 (which limited clock cycle) andenergy-related limitations size L2 L3 raised new challenges. Theevolution i7 processor class 6 –7 years illustrates this: caches size i7 6700 first generation i7 processors! moreaggressive use prefetching attempt overcome inability increase L2and L3. Off-chip L4 caches likely become important areless energy-constrained on-chip caches. addition schemes relying multilevel caches, introduction out-of- order pipelines multiple outstanding misses allowed available instruction-level parallelism hide memory latency remaining cache-based system.The introduction multithreading thread-level parallelism takes astep providing parallelism thus latency-hiding2.8 Concluding Remarks: Looking Ahead ■147opportunities. likely use instruction- thread-level parallelism important tool hiding whatever memory delays encountered modern multilevel cache systems. One idea periodically arises use programmer-controlled scratch- pad high-speed visible memories, see used GPUs.Such ideas never made mainstream general-purpose processors sev-eral reasons: First, break memory model introducing address spaceswith different behavior. Second, unlike compiler-based programmer-basedcache optimizations (such prefetching), memory transformations scratch- pads must completely handle remapping main memory address space scratchpad address space. makes transformations difficult andlimited applicability. GPUs (see Chapter 4 ), local scratchpad memories heavily used, burden managing currently falls programmer.For domain-specific software systems use memories, perfor-mance gains significant. likely HBM technologies thus beused caching large, general-purpose computers quite possibility asthe main working memories graphics similar systems. domain-specific architectures become important overcoming limitations arising end Dennard ’s Law slowdown Moore ’s Law (see Chapter 7 ), scratchpad memories vector-like register sets likely see use. implications end Dennard ’s Law affect DRAM proces- sor technology. Thus, rather widening gulf processors mainmemory, likely see slowdown technologies, leading sloweroverall growth rates performance. New innovations computer architecture andin related software together increase performance efficiency key continuing performance improvements seen past 50 years. 2.9 Historical Perspectives References Section M.3 (available online) examine history caches, virtual mem- ory, virtual machines. IBM plays prominent role history three. References reading included. Case Studies Exercises Norman P. Jouppi, Rajeev Balasubramonian, Naveen Muralimanohar, Sheng Li Case Study 1: Optimizing Cache Performance via Advanced Techniques Concepts illustrated case study ■Nonblocking Caches ■Compiler Optimizations Caches ■Software Hardware Prefetching ■Calculating Impact Cache Performance Complex Processors148 ■Chapter Two Memory Hierarchy DesignThe transpose matrix interchanges rows columns; concept illustrated here: A11 A11 A21 A31 A41 A12 A22 A32 A42 A13 A23 A33 A43 A14 A24 A34 A44A12 A22 A23 A24A13 A14 A21 A31 A32 A33 A34 A41 A42 A43 A44⇒ simple C loop show transpose: (i = 0; <3; i++) { (j = 0; j <3; j++) { output[j][i] = input[i][j]; } } Assume input output matrices stored row major order (row major order means row index changes fastest). Assume executing 256 /C1256 double-precision transpose processor 16 KB fully associative (don ’t worry cache conflicts) least recently used (LRU) replace- ment L1 data cache 64-byte blocks. Assume L1 cache misses pre-fetches require 16 cycles always hit L2 cache, L2 cache process request every 2 processor cycles. Assume iteration pre- ceding inner loop requires 4 cycles data present L1 cache. Assumethat cache write-allocate fetch-on-write policy write misses. Unreal-istically, assume writing back dirty cache blocks requires 0 cycles. 2.1 [10/15/15/12/20] <2.3>For preceding simple implementation, execution order would nonideal input matrix; however, applying loop interchangeoptimization would create nonideal order output matrix. loop interchange sufficient improve performance, must blocked instead. a.[10]<2.3>What minimum size cache take advantage blocked execution? b.[15]<2.3>How relative number misses blocked unblocked versions compare preceding minimum-sized cache? c.[15]<2.3>Write code perform transpose block size parameter B uses B/C1Bblocks. d.[12]<2.3>What minimum associativity required L1 cache consistent performance independent arrays ’position memory? e.[20]<2.3>Try blocked nonblocked 256 /C1256 matrix transpositions computer. closely results match expectations based whatyou know computer ’s memory system? Explain discrepancies possible.Case Studies Exercises ■1492.2 [10]<2.3>Assume designing hardware prefetcher preceding unblocked matrix transposition code. simplest type hardware prefetcher prefetches sequential cache blocks miss. complicated “nonunit stride ” hardware prefetchers analyze miss reference stream detect prefetchnonunit strides. contrast, software prefetching determine nonunit strides eas-ily determine unit strides. Assume prefetches write directly cache andthat “pollution ”(overwriting data must used data prefetched). best performance given nonunit stride prefetcher, steady stateof inner loop, many prefetches must outstanding given time? 2.3 [15/20] <2.3>With software prefetching, important careful prefetches occur time use also minimize number outstandingprefetches live within capabilities microarchitecture minimize cache pollution. complicated fact different processors dif- ferent capabilities limitations. a.[15]<2.3>Create blocked version matrix transpose software prefetching. b.[20]<2.3>Estimate compare performance blocked unblocked transpose codes without software prefetching. Case Study 2: Putting Together: Highly Parallel Memory Systems Concept illustrated case study ■Cross-Cutting Issues: Design Memory Hierarchies program Figure 2.32 used evaluate behavior memory sys- tem. key accurate timing program stride throughmemory invoke different levels hierarchy. Figure 2.32 shows code C. first part procedure uses standard utility get accurate measureof user CPU time; procedure may changed work systems. second part nested loop read write memory different strides cache sizes. get accurate cache timing, code repeated manytimes. third part times nested loop overhead besubtracted overall measured times see long accesses were. Theresults output .csv file format facilitate importing spreadsheets. may need change CACHE_MAX depending question answer- ing size memory system measuring. Running programin single-user mode least without active applications give con- sistent results. code Figure 2.32 derived program written Andrea Dusseau University California-Berkeley based adetailed description found Saavedra-Barrera (1992) . modified fix number issues modern machines run Microsoft150 ■Chapter Two Memory Hierarchy Design#include "stdafx.h" #include <stdio.h> #include <time.h>#define ARRAY_MIN (1024) /* 1/4 smallest cache */#define ARRAY_MAX (4096*4096) /* 1/4 largest cache */int x[ARRAY_MAX]; /* array going stride */ double get_seconds() { /* routine read time seconds */ __time64_t ltime;_time64( &ltime );return (double) ltime; }int label(int i) {/* generate text labels */ (i<1e3) printf("%1dB,",i);else (i<1e6) printf("%1dK,",i/1024);else (i<1e9) printf("%1dM,",i/1048576);else printf("%1dG,",i/1073741824); return 0; } int _tmain(int argc, _TCHAR* argv[]) {int register nextstep, i, index, stride;int csize;double steps, tsteps;double loadtime, lastsec, sec0, sec1, sec; /* timing variables */ /* Initialize output */ printf(" ,"); (stride=1; stride <= ARRAY_MAX/2; stride=stride*2) label(stride*sizeof(int)); printf("\n"); /* Main loop configuration */ (csize=ARRAY_MIN; csize <= ARRAY_MAX; csize=csize*2) { label(csize*sizeof(int)); /* print cache size loop */for (stride=1; stride <= csize/2; stride=stride*2) { /* Lay path memory references array */ (index=0; index < csize; index=index+stride) x[index] = index + stride; /* pointer next */ x[index-stride] = 0; /* loop back beginning */ /* Wait timer roll */ lastsec = get_seconds(); sec0 = get_seconds(); (sec0 == lastsec); /* Walk path array twenty seconds */ /* gives 5% accuracy second resolution */steps = 0.0; /* number steps taken */nextstep = 0; /* start beginning path */ sec0 = get_seconds(); /* start timer */ { /* repeat collect 20 seconds */ (i=stride;i!=0;i=i-1) { /* keep samples */ nextstep = 0; nextstep = x[nextstep]; /* dependency */ (nextstep != 0); }steps = steps + 1.0; /* count loop iterations */sec1 = get_seconds(); /* end timer */ } ((sec1 - sec0) < 20.0); /* collect 20 seconds */ sec = sec1 - sec0; /* Repeat empty loop loop subtract overhead */ tsteps = 0.0; /* used match no. iterations */sec0 = get_seconds(); /* start timer */ { /* repeat no. iterations */ (i=stride;i!=0;i=i-1) { /* keep samples */ index = 0; index = index + stride; (index < csize); } tsteps = tsteps + 1.0; sec1 = get_seconds(); /* - overhead */ } (tsteps<steps); /* = no. iterations */ sec = sec - (sec1 - sec0); loadtime = (sec*1e9)/(steps*csize);/* write results .csv format Excel */printf("%4.1f,", (loadtime<0.1) ? 0.1 : loadtime); }; /* end inner loop */ printf("\n");}; /* end outer loop */ return 0; } Figure 2.32 C program evaluating memory system.Case Studies Exercises ■151Visual C++. downloaded http://www.hpl.hp.com/research/cacti/ aca_ch2_cs2.c . preceding program assumes program addresses track physical addresses, true machines use virtually addressed caches,such Alpha 21264. general, virtual addresses tend follow physicaladdresses shortly rebooting, may need reboot machine orderto get smooth lines results. answer following questions, assume thatthe sizes components memory hierarchy powers 2. Assume thatthe size page much larger size block second-level cache (if one) size second-level cache block greater equal size block first-level cache. example output programis plotted Figure 2.33 ; key lists size array exercised. 2.4 [12/12/12/10/12] <2.6>Using sample program results Figure 2.33 : a.[12]<2.6>What overall size block size second-level cache? b.[12]<2.6>What miss penalty second-level cache? c.[12]<2.6>What associativity second-level cache? d.[10]<2.6>What size main memory? e.[12]<2.6>What paging time page size 4 KB?Read (ns)1000 100 10 1 4B 16B 64B 256B 4K 1K 16K 64K 256K 4M 1M 16M 64M 256M Stride8K 16K32K64K128K256K512K1M2M4M8M16M32M 64M 128M256M512M Figure 2.33 Sample results program Figure 2.32.152 ■Chapter Two Memory Hierarchy Design2.5 [12/15/15/20] <2.6>If necessary, modify code Figure 2.32 measure following system characteristics. Plot experimental results elapsed time they-axis memory stride x-axis. Use logarithmic scales axes, draw line cache size. a.[12]<2.6>What system page size? b.[15]<2.6>How many entries TLB? c.[15]<2.6>What miss penalty TLB? d.[20]<2.6>What associativity TLB? 2.6 [20/20] <2.6>In multiprocessor memory systems, lower levels memory hierarchy may able saturated single processor ableto saturated multiple processors working together. Modify code inFigure 2.32 , run multiple copies time. determine: a.[20]<2.6>How many actual processors computer system many system processors additional multithreaded contexts? b.[20]<2.6>How many memory controllers system have? 2.7 [20]<2.6>Can think way test characteristics instruc- tion cache using program? Hint: compiler may generate large number nonobvious instructions piece code. Try use simple arithmetic instruc-tions known length instruction set architecture (ISA). Case Study 3: Studying Impact Various Memory System Organizations Concepts illustrated case study ■DDR3 memory systems ■Impact ranks, banks, row buffers performance power ■DRAM timing parameters processor chip typically supports DDR3 DDR4 memory channels. focus single memory channel case study explore per- formance power impacted varying several parameters. Recall thechannel populated one DIMMs. DIMM supports one moreranks —a rank collection DRAM chips work unison service single command issued memory controller. example, rank may composedof 16 DRAM chips, chip deals 4-bit input output everychannel clock edge. chip referred /C24 (by four) chip. examples, rank may composed 8 /C28 chips 4 /C216 chips —note case, rank handle data placed 64-bit memory channel. rank partitioned 8 (DDR3) 16 (DDR4) banks. bank arow buffer essentially remembers last row read bank. ’sa n example typical sequence memory commands performing read froma bank:Case Studies Exercises ■153(i)The memory controller issues Precharge command get bank ready access new row. precharge completed time tRP. (ii)The memory controller issues Activate command read appro- priate row bank. activation completed time tRCD row deemed part row buffer. (iii)The memory controller issue column-read CAS command places specific subset row buffer memory channel. time CL, first 64 bits data burst placed memory channel.A burst typically includes eight 64-bit transfers memory channel, per-formed rising falling edges 4 memory clock cycles (referred astransfer time). (iv)If memory controller wants access data different row bank, referred row buffer miss, repeats steps (i) –(iii). now, assume CL elapsed, Precharge step (i) issued; somecases, additional delay must added, ignore delay here. Ifthe memory controller wants access another block data row, referred row buffer hit, simply issues another CAS command. Two back-to-back CAS commands separated least 4 cycles thatthe first data transfer complete second data transfer begin. Note memory controller issue commands different banks successive cycles perform many memory reads/writes parallel sitting idle waiting tRP, tRCD, CL elapse single bank. sub-sequent questions, assume tRP ¼tRCD ¼CL¼13 ns, memory channel frequency 1 GHz, is, transfer time 4 ns. 2.8 [10]<2.2>What read latency experienced memory controller row buffer miss? 2.9 [10]<2.2>What latency experienced memory controller row buffer hit? 2.10 [10]<2.2>If memory channel supports one bank memory access pattern dominated row buffer misses, utilization memorychannel? 2.11 [15]<2.2>Assuming 100% row buffer miss rate, minimum number banks memory channel support order achieve 100% mem-ory channel utilization? 2.12 [10]<2.2>Assuming 50% row buffer miss rate, minimum number banks memory channel support order achieve 100% memory channel utilization? 2.13 [15]<2.2>Assume executing application four threads threads exhibit zero spatial locality, is, 100% row buffer miss rate. Every 200 ns, four threads simultaneously inserts read operation the154 ■Chapter Two Memory Hierarchy Designmemory controller queue. average memory latency experienced memory channel supports one bank? memory channel supported four banks? 2.14 [10]<2.2>From questions, learned benefits downsides growing number banks? 2.15 [20]<2.2>Now let ’s turn attention memory power. Download copy Micron power calculator link: https://www.micron.com/ /C24/media/ documents/products/power-calculator/ddr3_power_calc.xlsm . spreadsheet preconfigured estimate power dissipation single 2 Gb /C28 DDR3 SDRAM memory chip manufactured Micron. Click “Summary ”tab see power breakdown single DRAM chip default usage conditions(reads occupy channel 45% cycles, writes occupy channel 25%of cycles, row buffer hit rate 50%). chip consumes 535 mW, andthe breakdown shows half power expended Activate oper- ations, 38% CAS operations, 12% background power. Next, click “System Config ”tab. Modify read/write traffic row buffer hit rate observe changes power profile. example, thedecrease power channel utilization 35% (25% reads 10% writes),or row buffer hit rate increased 80%? 2.16 [20]<2.2>In default configuration, rank consists eight /C28 2 Gb DRAM chips. rank also comprise16 /C24 chips 4 /C216 chips. also vary capacity DRAM chip —1 Gb, 2 Gb, 4 Gb. selections made “DDR3 Config ”tab Micron power calculator. Tabulate total power consumed rank organization. power-efficient approach constructing rank given capacity? Exercises 2.17 [12/12/15] <2.3>The following questions investigate impact small simple caches using CACTI assume 65 nm (0.065 m) technology. (CACTIis available online form http://quid.hpl.hp.com:9081/cacti/ .) a.[12]<2.3>Compare access times 64 KB caches 64-byte blocks single bank. relative access times two-way four-way setassociative caches compared direct mapped organization? b.[12]<2.3>Compare access times four-way set associative caches 64-byte blocks single bank. relative access times 32 and64 KB caches compared 16 KB cache? c.[15]<2.3>For 64 KB cache, find cache associativity 1 8 lowest average memory access time given misses per instruction certain workload suite 0.00664 direct-mapped, 0.00366 two-way set associative, 0.000987 four-way set associative, 0.000266 eight-way set associative cache. Overall, 0.3 data references per instruction.Assume cache misses take 10 ns models. calculate hit time inCase Studies Exercises ■155cycles, assume cycle time output using CACTI, corresponds maximum frequency cache operate without bubbles pipeline. 2.18 [12/15/15/10] <2.3>You investigating possible benefits way- predicting L1 cache. Assume 64 KB four-way set associative single-banked L1 data cache cycle time limiter system. alternative cache orga- nization, considering way-predicted cache modeled 64 KB direct-mapped cache 80% prediction accuracy. Unless stated otherwise, assume thata mispredicted way access hits cache takes one cycle. Assume themiss rates miss penalties question 2.8 part (c). a.[12]<2.3>What average memory access time current cache (in cycles) versus way-predicted cache? b.[15]<2.3>If components could operate faster way-predicted cache cycle time (including main memory), would impact onperformance using way-predicted cache? c.[15]<2.3>Way-predicted caches usually used instruction caches feed instruction queue buffer. Imagine want try outway prediction data cache. Assume 80% prediction accuracyand subsequent operations (e.g., data cache access instructions, dependent operations) issued assuming correct way prediction. Thus way misprediction necessitates pipe flush replay trap, requires15 cycles. change average memory access time per load instructionwith data cache way prediction positive negative, much it? d.[10]<2.3>As alternative way prediction, many large associative L2 caches serialize tag data access required dataset arrayneeds activated. saves power increases access time. UseCACTI ’s detailed web interface 0.065 process 1 MB four-way set associative cache 64-byte blocks, 144 bits read out, 1 bank, 1read/write port, 30 bit tags, ITRS-HP technology global wires. Whatis ratio access times serializing tag data access compared parallel access? 2.19 [10/12] <2.3>You asked investigate relative performance banked versus pipelined L1 data cache new microprocessor. Assume 64 KB two-way set associative cache 64-byte blocks. pipelined cache wouldconsist three pipe stages, similar capacity Alpha 21264 data cache.A banked implementation would consist two 32 KB two-way set associativebanks. Use CACTI assume 65 nm (0.065 m) technology answer fol-lowing questions. cycle time output web version shows whatfrequency cache operate without bubbles pipeline. a.[10]<2.3>What cycle time cache comparison access time, many pipe stages cache take (to two decimal places)? b.[12]<2.3>Compare area total dynamic read energy per access pipelined design versus banked design. State takes less area andwhich requires power, explain might be.156 ■Chapter Two Memory Hierarchy Design2.20 [12/15] <2.3>Consider usage critical word first early restart L2 cache misses. Assume 1 MB L2 cache 64-byte blocks refill path 16 bytes wide. Assume L2 written 16 bytes every 4processor cycles, time receive first 16 byte block memory con-troller 120 cycles, additional 16 byte block main memory requires 16cycles, data bypassed directly read port L2 cache. Ignoreany cycles transfer miss request L2 cache requested data theL1 cache. a.[12]<2.3>How many cycles would take service L2 cache miss without critical word first early restart? b.[15]<2.3>Do think critical word first early restart would important L1 caches L2 caches, factors would contribute theirrelative importance? 2.21 [12/12] <2.3>You designing write buffer write-through L1 cache write-back L2 cache. L2 cache write data bus 16 B wide per-form write independent cache address every four processor cycles. a.[12]<2.3>How many bytes wide write buffer entry be? b.[15]<2.3>What speedup could expected steady state using merging write buffer instead nonmerging buffer zeroing memoryby execution 64-bit stores instructions could issued inparallel stores blocks present L2 cache? c.[15]<2.3>What would effect possible L1 misses number required write buffer entries systems blocking nonblocking caches? 2.22 [20]<2.1, 2.2, 2.3 >A cache acts filter. example, every 1000 instruc- tions program, average 20 memory accesses may exhibit low enough locality cannot serviced 2 MB cache. 2 MB cache saidto MPKI (misses per thousand instructions) 20, largelytrue regardless smaller caches precede 2 MB cache. Assume fol-lowing cache/latency/MPKI values: 32 KB/1/100, 128 KB/2/80, 512 KB/4/50,2 MB/8/40, 8 MB/16/10. Assume accessing off-chip memory systemrequires 200 cycles average. following cache configurations, calculatethe average time spent accessing cache hierarchy. observe downsides cache hierarchy shallow deep? a.32 KB L1; 8 MB L2; off-chip memory b.32 KB L1; 512 KB L2; 8 MB L3; off-chip memory c.32 KB L1; 128 KB L2; 2 MB L3; 8 MB L4; off-chip memory 2.23 [15]<2.1, 2.2, 2.3 >Consider 16 MB 16-way L3 cache shared two programs B. mechanism cache monitors cache missrates program allocates 1 –15 ways program over- number cache misses reduced. Assume program MPKI 100when assigned 1 MB cache. additional 1 MB assigned programCase Studies Exercises ■157A reduces MPKI 1. Program B MPKI 50 assigned 1 MB cache; additional 1 MB assigned program B reduces MPKI 2. best allocation ways programs B? 2.24 [20]<2.1, 2.6 >You designing PMD optimizing low energy. core, including 8 KB L1 data cache, consumes 1 W whenever hiber- nation. core perfect L1 cache hit rate, achieves average CPI 1 given task, is, 1000 cycles execute 1000 instructions. additionalcycle accessing L2 beyond adds stall cycle core. Based thefollowing specifications, size L2 cache achieves lowestenergy PMD (core, L1, L2, memory) given task? a.The core frequency 1 GHz, L1 MPKI 100. b.A 256 KB L2 latency 10 cycles, MPKI 20, background power 0.2 W, L2 access consumes 0.5 nJ. c.A 1 MB L2 latency 20 cycles, MPKI 10, background power 0.8 W, L2 access consumes 0.7 nJ. d.The memory system average latency 100 cycles, background power 0.5 W, memory access consumes 35 nJ. 2.25 [15]<2.1, 2.6 >You designing PMD optimized low power. Qual- itatively explain impact cache hierarchy (L2 memory) power overallapplication energy design L2 cache with: a.Small block size b.Small cache size c.High associativity 2.30 [10/10] <2.1, 2.2, 2.3 >The ways set viewed priority list, ordered high priority low priority. Every time set touched, list bereorganized change block priorities. view, cache management policies decomposed three sub-policies: Insertion, Promotion, Victim Selection. Insertion defines newly fetched blocks placed prioritylist. Promotion defines block ’s position list changed every time touched (a cache hit). Victim Selection defines entry list evicted tomake room new block cache miss. a.Can frame LRU cache policy terms Insertion, Promotion, Victim Selection sub-policies? b.Can define Insertion Promotion policies may competitive worth exploring further? 2.31 [15]<2.1, 2.3 >In processor running multiple programs, last-level cache typically shared programs. leads interference, whereone program ’s behavior cache footprint impact cache available programs. First, problem quality-of-service (QoS) perspective,where interference leads program receiving fewer resources lower158 ■Chapter Two Memory Hierarchy Designperformance promised, say operator cloud service. Second, problem terms privacy. Based interference sees, program infer memory access patterns programs. referred timing chan-nel, form information leakage one program others exploitedto compromise data privacy reverse-engineer competitor ’s algorithm. policies add last-level cache behavior one program isimmune behavior programs sharing cache? 2.32 [15]<2.3>A large multimegabyte L3 cache take tens cycles access long wires traversed. example, may take20 cycles access 16 MB L3 cache. Instead organizing 16 MB cache suchthat every access takes 20 cycles, organize cache array ofsmaller cache banks. banks may closer processor core, others may further. leads nonuniform cache access (NUCA), 2 MB cache may accessible 8 cycles, next 2 MB 10 cycles,and last 2 MB accessed 22 cycles. new policies youintroduce maximize performance NUCA cache? 2.33 [10/10/10] <2.2>Consider desktop system processor connected 2 GB DRAM error-correcting code (ECC) . Assume one memory channel width 72 bits (64 bits data 8 bits ECC). a.[10]<2.2>How many DRAM chips DIMM 1 Gb DRAM chips used, many data I/Os must DRAM one DRAMconnects DIMM data pin? b.[10]<2.2>What burst length required support 32 B L2 cache blocks? c.[10]<2.2>Calculate peak bandwidth DDR2-667 DDR2-533 DIMMs reads active page excluding ECC overhead. 2.34 [10/10] <2.2>A sample DDR2 SDRAM timing diagram shown Figure 2.34 . tRCD time required activate row bank, column addressstrobe (CAS) latency (CL) number cycles required read columnin row. Assume RAM standard DDR2 DIMM ECC, having72 data lines. Also assume burst lengths 8 read 8 bits, total 64 Bfrom DIMM. Assume tRCD = CAS (or CL) clock_frequency , clock_frequency = transfers_per_second/2 . on-chip latency ACT B0, RxRD B0, Cx Data outCAS latencytRCDData outCMD/ ADDClock Data Figure 2.34 DDR2 SDRAM timing diagram.Case Studies Exercises ■159on cache miss levels 1 2 back, including DRAM access, 20 ns. a.[10]<2.2>How much time required presentation activate command last requested bit data DRAM transitions valid invalid DDR2-667 1 Gb CL ¼5 DIMM? Assume every request, automatically prefetch another adjacent cache line inthe page. b.[10]<2.2>What relative latency using DDR2-667 DIMM read requiring bank activate versus one already open page, including thetime required process miss inside processor? 2.35 [15]<2.2>Assume DDR2-667 2 GB DIMM CL ¼5 available 130 DDR2-533 2 GB DIMM CL ¼4 available 100. Assume two DIMMs used system, rest system costs 800. Consider theperformance system using DDR2-667 DDR2-533 DIMMs aworkload 3.33 L2 misses per 1K instructions, assume 80% allDRAM reads require activate. cost-performance entire system using different DIMMs, assuming one L2 miss outstanding time in-order core CPI 1.5 including L2 cache miss memoryaccess time? 2.36 [12]<2.2>You provisioning server eight-core 3 GHz CMP execute workload overall CPI 2.0 (assuming L2 cache miss refillsare delayed). L2 cache line size 32 bytes. Assuming system usesDDR2-667 DIMMs, many independent memory channels providedso system limited memory bandwidth bandwidth required issometimes twice average? workloads incur, average, 6.67 L2 missesper 1 K instructions. 2.37 [15]<2.2>Consider processor four memory channels. consec- utive memory blocks placed bank, placed dif-ferent banks different channels? 2.38 [12/12] <2.2>A large amount (more third) DRAM power due page activation (see http://download.micron.com/pdf/technotes/ddr2/TN4704.pdf andhttp://www.micron.com/systemcalc ). Assume building system 2 GB memory using either 8-bank 2 Gb /C28 DDR2 DRAMs 8-bank 1 Gb /C28 DRAMs, speed grade. use page size 1 KB, last-level cache line size 64 bytes. Assume DRAMs notactive precharged standby dissipate negligible power. Assume thatthe time transition standby active significant. a.[12]<2.2>Which type DRAM would expected provide higher system performance? Explain why. b.[12]<2.2>How 2 GB DIMM made 1 Gb /C28 DDR2 DRAMs com- pare DIMM similar capacity made 1 Gb /C24 DDR2 DRAMs terms power?160 ■Chapter Two Memory Hierarchy Design2.39 [20/15/12] <2.2>To access data typical DRAM, first activate appropriate row. Assume brings entire page size 8 KB row buffer. select particular column row buffer. subsequentaccesses DRAM page, skip activation step; oth-erwise, close current page precharge bitlines nextactivation. Another popular DRAM policy proactively close page andprecharge bitlines soon access over. Assume every read writeto DRAM size 64 bytes DDR bus latency (data Figure 2.33 ) sending 512 bits Tddr. a.[20]<2.2>Assuming DDR2-667, takes five cycles precharge, five cycles activate, four cycles read column, value rowbuffer hit rate ( r) choose one policy another get best access time? Assume every access DRAM separated enough time finish random new access. b.[15]<2.2>If 10% total accesses DRAM happen back back contiguously without time gap, decision change? c.[12]<2.2>Calculate difference average DRAM energy per access two policies using previously calculated row buffer hit rate.Assume precharging requires 2 nJ activation requires 4 nJ that100 pJ/bit required read write row buffer. 2.40 [15]<2.2>Whenever computer idle, either put standby (where DRAM still active) let hibernate. Assume that, hibernate, haveto copy contents DRAM nonvolatile medium Flash. read-ing writing cache line size 64 bytes Flash requires 2.56 J DRAMrequires 0.5 nJ, idle power consumption DRAM 1.6 W (for 8 GB),how long system idle benefit hibernating? Assume main memory size 8 GB. 2.41 [10/10/10/10/10] <2.4>Virtual machines (VMs) potential adding many beneficial capabilities computer systems, improved total cost ownership (TCO) availability. Could VMs used provide following capabilities? so, could facilitate this? a.[10]<2.4>Test applications production environments using development machines? b.[10]<2.4>Quick redeployment applications case disaster failure? c.[10]<2.4>Higher performance I/O-intensive applications? d.[10]<2.4>Fault isolation different applications, resulting higher availability services? e.[10]<2.4>Performing software maintenance systems applications running without significant interruption? 2.42 [10/10/12/12] <2.4>Virtualmachinescanloseperformancefromanumberofevents, execution privileged instructions, TLB misses, traps, I/O.Case Studies Exercises ■161These events usually handled system code. Thus one way estimating slowdown running VM percentage application executiontime system versus user mode. example, application spending 10% itsexecution system mode might slow 60% running VM.Figure 2.35 lists early performance various system calls native execu- tion, pure virtualization, paravirtualization LMbench using Xen onan Itanium system times measured microseconds (courtesy MatthewChapman University New South Wales). a.[10]<2.4>What types programs would expected smaller slowdowns running VMs? b.[10]<2.4>If slowdowns linear function system time, given preceding slowdown, much slower would program spending 20% itsexecution system time expected run? c.[12]<2.4>What median slowdown system calls table pure virtualization paravirtualization? d.[12]<2.4>Which functions table largest slowdowns? think cause could be? 2.43 [12]<2.4>Popek Goldberg ’s definition virtual machine said would indistinguishable real machine except performance. ques-tion, use definition find access native execution processor running virtual machine. Intel VT-x technology effec- tively provides second set privilege levels use virtual machine.What would virtual machine running top another virtual machine todo, assuming VT-x technology? 2.44 [20/25] <2.4>With adoption virtualization support x86 architecture, virtual machines actively evolving becoming mainstream. Compare andcontrast Intel VT-x AMD ’s AMD-V virtualization technologies.Benchmark Native Pure Para Null call 0.04 0.96 0.50 Null I/O 0.27 6.32 2.91Stat 1.10 10.69 4.14Open/close 1.99 20.43 7.71Install signal handler 0.33 7.34 2.89Handle signal 1.69 19.26 2.36Fork 56.00 513.00 164.00Exec 316.00 2084.00 578.00Fork+exec sh 1451.00 7790.00 2360.00 Figure 2.35 Early performance various system calls native execution, pure virtualization, paravirtualization.162 ■Chapter Two Memory Hierarchy Design(Information AMD-V found http://sites.amd.com/us/business/it- solutions/virtualization/Pages/resources.aspx .) a.[20]<2.4>Which one could provide higher performance memory- intensive applications large memory footprints? b.[25]<2.4>Information AMD ’s IOMMU support virtualized I/O found http://developer.amd.com/documentation/articles/pages/892006101. aspx. Virtualization Technology input/output memory manage- ment unit (IOMMU) improve virtualized I/O performance? 2.45 [30]<2.2, 2.3 >Since instruction-level parallelism also effectively exploited in-order superscalar processors long instruction word (VLIW) processors speculation, one important reason building out- of-order (OOO) superscalar processor ability tolerate unpredictable mem- ory latency caused cache misses. Thus think hardware supporting OOO issue part memory system. Look floorplan Alpha 21264 Figure 2.36 find relative area integer floating-point issue queues mappers versus caches. queues schedule instructions issue, Float map queue Memory controllerBus interface unit Data control buses Memory controller Data cache Instruction cacheInteger mapper Integer queue Integer unit (cluster 0)Integer unit (cluster 1)Floating-point units Instruction fetch BIU Figure 2.36 Floorplan Alpha 21264 [Kessler 1999].Case Studies Exercises ■163and mappers rename register specifiers. Therefore necessary additions support OOO issue. 21264 L1 data instruction caches chip, 64 KB two-way set associative. Use OOO superscalar sim-ulator SimpleScalar ( http://www.cs.wisc.edu/ /C24mscalar/simplescalar. html) memory-intensive benchmarks find much performance lost area issue queues mappers used additional L1 data cache areain in-order superscalar processor, instead OOO issue model 21264.Make sure aspects machine similar possible make thecomparison fair. Ignore increase access cycle time larger caches effects larger data cache floorplan chip. (Note com- parison totally fair, code scheduled thein-order processor compiler.) 2.46 [15]<2.2, 2.7 >As discussed Section 2.7 , Intel i7 processor aggres- sive prefetcher. potential disadvantages designing prefetcher isextremely aggressive? 2.47 [20/20/20] <2.6>The Intel performance analyzer VTune used make many measurements cache behavior. free evaluation version VTune onboth Windows Linux downloaded http://software.intel.com/en- us/articles/intel-vtune-amplifier-xe/ . program ( aca_ch2_cs2.c ) used Case Study 2 modified work VTune boxon Microsoft Visual C++. program downloaded http://www. hpl.hp.com/research/cacti/aca_ch2_cs2_vtune.c . Special VTune functions inserted exclude initialization loop overhead performance analysis process. Detailed VTune setup directions given README sec-tion program. program keeps looping 20 seconds every config-uration. following experiment, find effects data size cacheand overall processor performance. Run program VTune Intel proces-sor input dataset sizes 8 KB, 128 KB, 4 MB, 32 MB, keep astride 64 bytes (stride one cache line Intel i7 processors). Collect statistics onoverall performance L1 data cache, L2, L3 cache performance. a.[20]<2.6>List number misses per 1K instruction L1 data cache, L2, L3 dataset size processor model speed. Based results, say L1 data cache, L2, L3 cache sizes processor? Explain observations. b.[20]<2.6>List instructions per clock (IPC) dataset size processor model speed. Based results, say theL1, L2, L3 miss penalties processor? Explain observations. c.[20]<2.6>Run program VTune input dataset size 8 KB 128 KB Intel OOO processor. List number L1 data cache andL2 cache misses per 1K instructions CPI configurations. Whatcan say effectiveness memory latency hiding techniques inhigh-performance OOO processors? Hint: need find L1 data cache miss latency processor. recent Intel i7 processors, approxi-mately 11 cycles.164 ■Chapter Two Memory Hierarchy DesignThis page intentionally left blank3.1 Instruction-Level Parallelism: Concepts Challenges 168 3.2 Basic Compiler Techniques Exposing ILP 176 3.3 Reducing Branch Costs Advanced Branch Prediction 182 3.4 Overcoming Data Hazards Dynamic Scheduling 191 3.5 Dynamic Scheduling: Examples Algorithm 201 3.6 Hardware-Based Speculation 208 3.7 Exploiting ILP Using Multiple Issue Static Scheduling 218 3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation 222 3.9 Advanced Techniques Instruction Delivery Speculation 228 3.10 Cross-Cutting Issues 240 3.11 Multithreading: Exploiting Thread-Level Parallelism Improve Uniprocessor Throughput 242 3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 247 3.13 Fallacies Pitfalls 258 3.14 Concluding Remarks: ’s Ahead? 264 3.15 Historical Perspective References 266 Case Studies Exercises Jason D. Bakos Robert P. Colwell 2663 Instruction-Level Parallelism Exploitation “Who ’s first? ” “America. ” “Who ’s second? ” “Sir, second. ” Dialog two observers sailing race 1851, later named “The America ’s Cup, ” inspiration John Cocke ’s naming IBM research processor “America, ”the first superscalar processor, precursor PowerPC. Thus, IA-64 gambles that, future, power critical limitation, massive resources …will penalize clock speed, path length, CPI factors. view clearly skeptical … Marty Hopkins (2000) , IBM Fellow Early RISC pioneer commenting 2000 new Intel Itanium, joint development Intel HP. Itanium used static ILP approach (see Appendix H) massive investment Intel. never accounted 0.5% Intel ’s microprocessor sales. Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00003-1 ©2019 Elsevier Inc. rights reserved.3.1 Instruction-Level Parallelism: Concepts Challenges processors since 1985 used pipelining overlap execution instructions improve performance. potential overlap among instructionsis called instruction-level parallelism (ILP), instructions eval- uated parallel. chapter Appendix H, look wide range tech-niques extending basic pipelining concepts increasing amount ofparallelism exploited among instructions. chapter considerably advanced level material basic pipelining Appendix C . thoroughly familiar ideas Appendix C , review appendix venturing chapter. start chapter looking limitation imposed data control hazards turn topic increasing ability compiler theprocessor exploit parallelism. sections introduce large number concepts,which build throughout chapter next. basicmaterial chapter could understood without ideas first two sections, basic material important later sections chapter. two largely separable approaches exploiting ILP: (1) approach relies hardware help discover exploit parallelism dynamically,and (2) approach relies software technology find parallelism staticallyat compile time. Processors using dynamic, hardware-based approach, includ-ing recent Intel many ARM processors, dominate desktop servermarkets. personal mobile device market, approaches used inprocessors found tablets high-end cell phones. IOT space, power cost constraints dominate performance goals, designers exploit lower levels instruction-level parallelism. Aggressive compiler-based approacheshave attempted numerous times beginning 1980s recentlyin Intel Itanium series, introduced 1999. Despite enormous efforts, suchapproaches successful domain-specific environments inwell-structured scientific applications significant data-level parallelism. past years, many techniques developed one approach exploited within design relying primarily other. chapter intro- duces basic concepts approaches. discussion limitations ILP approaches included chapter, limitations directlyled movement toward multicore. Understanding limitations remainsimportant balancing use ILP thread-level parallelism. section, discuss features programs processors limit amount parallelism exploited among instructions, well thecritical mapping program structure hardware structure, keyto understanding whether program property actually limit performance circumstances. value CPI (cycles per instruction) pipelined processor sum base CPI contributions stalls: Pipeline CPI¼Ideal pipeline CPI + Structural stalls + Data hazard stalls + Control stalls168 ■Chapter Three Instruction-Level Parallelism ExploitationTheideal pipeline CPI measure maximum performance attainable implementation. reducing terms right-hand side, decreasethe overall pipeline CPI or, alternatively, increase IPC (instructions per clock).The preceding equation allows us characterize various techniques com- ponent overall CPI technique reduces. Figure 3.1 shows techniques examine chapter Appendix H, well topics covered theintroductory material Appendix C . chapter, see tech- niques introduce decrease ideal pipeline CPI increase importanceof dealing hazards. Instruction-Level Parallelism? techniques chapter exploit parallelism among instructions. Theamount parallelism available within basic block —a straight-line code sequence branches except entry branches except exit —is quite small. typical RISC programs, average dynamic branch frequency isoften 15% 25%, meaning three six instructions exe-cute pair branches. instructions likely dependupon one another, amount overlap exploit within basic block islikely less average basic block size. obtain substantial performanceenhancements, must exploit ILP across multiple basic blocks. simplest common way increase ILP exploit parallel- ism among iterations loop. type parallelism often called loop-levelTechnique Reduces Section Forwarding bypassing Potential data hazard stalls C.2 Simple branch scheduling prediction Control hazard stalls C.2Basic compiler pipeline scheduling Data hazard stalls C.2, 3.2 Basic dynamic scheduling (scoreboarding) Data hazard stalls true dependences C.7Loop unrolling Control hazard stalls 3.2 Advanced branch prediction Control stalls 3.3 Dynamic scheduling renaming Stalls data hazards, output dependences, antidependences3.4 Hardware speculation Data hazard control hazard stalls 3.6 Dynamic memory disambiguation Data hazard stalls memory 3.6 Issuing multiple instructions per cycle Ideal CPI 3.7, 3.8 Compiler dependence analysis, software pipelining, trace schedulingIdeal CPI, data hazard stalls H.2, H.3 Hardware support compiler speculation Ideal CPI, data hazard stalls, branch hazard stalls H.4, H.5 Figure 3.1 major techniques examined Appendix C , Chapter 3, Appendix H shown together component CPI equation technique affects.3.1 Instruction-Level Parallelism: Concepts Challenges ■169parallelism . simple example loop adds two 1000-element arrays completely parallel: (i=0; <=999; i=i+1) x[i] = x[i] + y[i]; Every iteration loop overlap iteration, although within loop iteration, little opportunity overlap. examine number techniques converting loop-level parallelism instruction-level parallelism. Basically, techniques workby unrolling loop either statically compiler (as next section) ordynamically hardware (as Sections 3.5 and3.6). important alternative method exploiting loop-level parallelism use SIMD vector processors graphics processing units (GPUs), covered Chapter 4 . SIMD instruction exploits data-level parallelism operating small moderate number data items parallel (typicallytwo eight). vector instruction exploits data-level parallelism operatingon many data items parallel using parallel execution units deep pipe-line. example, preceding code sequence, simple form requiresseven instructions per iteration (two loads, add, store, two address updates,and branch) total 7000 instructions, might execute one-quarter many instructions SIMD architecture four data items processed per instruction. vector processors, sequence might take four instruc-tions: two instructions load vectors xandyfrom memory, one instruction add two vectors, instruction store back result vector. course,these instructions would pipelined relatively long latencies, theselatencies may overlapped. Data Dependences Hazards Determining one instruction depends another critical determining howmuch parallelism exists program parallelism exploited. particular, exploit instruction-level parallelism, must determine instructions executed parallel. two instructions parallel , execute simultaneously pipeline arbitrary depth without causing anystalls, assuming pipeline sufficient resources (and thus structural hazardsexist). two instructions dependent, parallel must executedin order, although may often partially overlapped. key cases isto determine whether instruction dependent another instruction. Data Dependences three different types dependences: data dependences (also called true data dependences), name dependences , control dependences . instruction j isdata-dependent instruction iif either following holds:170 ■Chapter Three Instruction-Level Parallelism Exploitation■Instruction iproduces result may used instruction j. ■Instruction jis data-dependent instruction k, instruction kis data- dependent instruction i. second condition simply states one instruction dependent another exists chain dependences first type two instructions.This dependence chain long entire program. Note depen-dence within single instruction (such add x1,x1,x1 ) considered dependence. example, consider following RISC-V code sequence increments vector values memory (starting 0(x1) ending last element 0(x2) ) scalar register f2. Loop: fld f0,0(x1) //f0=array element fadd.d f4,f0,f2 //add scalar f2fsd f4,0(x1) //store result addi x1,x1, /C08 //decrement pointer 8 bytes bne x1,x2,Loop //branch x1 6¼x2 data dependences code sequence involve floating-point data: Loop: fld f0,0(x1) //f0=array element fadd.d f4,f0,f2 //add scalar f2fsd f4,0(x1) //store result integer data: addi x1,x1,-8 //decrement pointer //8 bytes (per DW) bne x1,x2,Loop//branch x1ax2 preceding dependent sequences, shown arrows, instruction depends previous one. arrows following exam-ples show order must preserved correct execution. arrow points instruction must precede instruction arrowhead points to. two instructions data-dependent, must execute order cannot execute simultaneously completely overlapped. dependence implies thatthere would chain one data hazards two instructions.(See Appendix C brief description data hazards, define precisely pages.) Executing instructions simultaneously causea processor pipeline interlocks (and pipeline depth longer distancebetween instructions cycles) detect hazard stall, thereby reducing eliminating overlap. processor without interlocks relies compiler scheduling, compiler cannot schedule dependent instructions way that3.1 Instruction-Level Parallelism: Concepts Challenges ■171they completely overlap program execute correctly. pres- ence data dependence instruction sequence reflects data dependence source code instruction sequence generated. effectof original data dependence must preserved. Dependences property programs. Whether given dependence results actual hazard detected whether hazard actually causes stallare properties pipeline organization. difference critical understand- ing instruction-level parallelism exploited. data dependence conveys three things: (1) possibility hazard, (2) order results must calculated, (3) upper bound much parallelism possibly exploited. limits explored pitfallonpage 262 Appendix H detail. data dependence limit amount instruction-level parallel- ism exploit, major focus chapter overcoming limitations. Adependence overcome two different ways: (1) maintaining depen-dence avoiding hazard, (2) eliminating dependence transformingthe code. Scheduling code primary method used avoid hazard without altering dependence, scheduling done compiler hardware. data value may flow instructions either registers memory locations. data flow occurs register, detecting thedependence straightforward register names fixed instruc-tions, although gets complicated branches intervene correctnessconcerns force compiler hardware conservative. Dependences flow memory locations difficult detect two addresses may refer location look different: exam- ple,100(x4) and20(x6) may identical memory addresses. addition, effective address load store may change one execution instruc-tion another (so 20(x4) and20(x4) may different), compli- cating detection dependence. chapter, examine hardware detecting data dependences involve memory locations, see techniques also limita-tions. compiler techniques detecting dependences critical unco- vering loop-level parallelism. Name Dependences second type dependence name dependence. name dependence occurs two instructions use register memory location, called name , flow data instructions associated name. Thereare two types name dependences instruction ithatprecedes instruc- tion jin program order: 1.Anantidependence instruction iand instruction joccurs instruc- tion jwrites register memory location instruction ireads. original172 ■Chapter Three Instruction-Level Parallelism Exploitationordering must preserved ensure ireads correct value. example onpage 171 , antidependence fsd andaddi register x1. 2.Anoutput dependence occurs instruction iand instruction jwrite register memory location. ordering instructions must preserved ensure value finally written corresponds instruction j. antidependences output dependences name dependences, opposed true data dependences, value transmitted theinstructions. name dependence true dependence, instructionsinvolved name dependence execute simultaneously reordered, ifthe name (register number memory location) used instructions changedso instructions conflict. renaming easily done register operands, called register renaming . Register renaming done either statically compiler dynamically hardware. describing dependences arising branches,let’s examine relationship dependences pipeline data hazards. Data Hazards hazard exists whenever name data dependence instructions, close enough overlap execution would change theorder access operand involved dependence. depen- dence, must preserve called program order —that is, order instructions would execute executed sequentially one time determinedby original source program. goal software hardware tech-niques exploit parallelism preserving program order affects outcome program. Detecting avoiding hazards ensures neces- sary program order preserved. Data hazards, informally described Appendix C , may classified one three types, depending order read write accesses instructions. convention, hazards named ordering program must preserved pipeline. Consider two instructions iandj,with preceding jin program order. possible data hazards ■RAW (read write) —jtries read source iwrites it, jincor- rectly gets oldvalue. hazard common type corresponds true data dependence. Program order must preserved ensure j receives value i. ■WAW (write write) —jtries write operand written i. writes end performed wrong order, leaving value writ- ten irather value written jin destination. hazard cor- responds output dependence. WAW hazards present pipelinesthat write one pipe stage allow instruction proceed evenwhen previous instruction stalled.3.1 Instruction-Level Parallelism: Concepts Challenges ■173■WAR (write read) —jtries write destination read i,soi incorrectly gets newvalue. hazard arises antidependence (or name dependence). WAR hazards cannot occur static issue pipelines — even deeper pipelines floating-point pipelines —because reads early (in ID pipeline Appendix C ) writes late (in WB pipe- line Appendix C ). WAR hazard occurs either instruc- tions write results early instruction pipeline andother instructions read source late pipeline, instructions reordered, aswe see chapter. Note RAR (read read) case hazard. Control Dependences last type dependence control dependence. control dependence deter- mines ordering instruction, i,with respect branch instruction instruction iis executed correct program order be. Every instruction, except first basic block program, control-dependent set branches, general, control dependences mustbe preserved preserve program order. One simplest examples controldependence dependence statements “then”part statement branch. example, code segment p1 { S1; }; p2 { S2; } S1is control-dependent p1, S2is control-dependent p2but onp1. general, two constraints imposed control dependences: 1.An instruction control-dependent branch cannot moved branch execution longer controlled branch. example, cannot take instruction portion statement moveit statement. 2.An instruction control-dependent branch cannot moved branch execution controlled branch. example, cannot take statement statement move portion. processors preserve strict program order, ensure control depen- dences also preserved. may willing execute instructions shouldnot executed, however, thereby violating control dependences, ifwe174 ■Chapter Three Instruction-Level Parallelism Exploitationcan without affecting correctness program. Thus control depen- dence critical property must preserved. Instead, two properties critical program correctness —and normally preserved maintaining data control dependences —are exception behavior data flow . Preserving exception behavior means changes ordering instruction execution must change exceptions raised program.Often relaxed mean reordering instruction execution must notcause new exceptions program. simple example shows maintain-ing control data dependences prevent situations. Consider code sequence: add x2,x3,x4 beq x2,x0,L1ld x1,0(x2) L1: case, easy see maintain data dependence involv- ingx2, change result program. Less obvious fact ignore control dependence move load instruction branch, load instruction may cause memory protection exception. Notice data dependence prevents us interchanging beqz ld; control dependence. allow us reorder instructions (and still preservethe data dependence), want ignore exception branch istaken. Section 3.6 , look hardware technique, speculation , allows us overcome exception problem. Appendix H looks software tech-niques supporting speculation. second property preserved maintenance data dependences con- trol dependences data flow. data flow actual flow data values among instructions produce results consume them. Branchesmake data flow dynamic allow source data giveninstruction come many points. Put another way, insufficient justmaintain data dependences instruction may data-dependent morethan one predecessor. Program order determines predecessor willactually deliver data value instruction. Program order ensured main- taining control dependences. example, consider following code fragment: add x1,x2,x3 beq x4,x0,L sub x1,x5,x6 L: ... x7,x1,x8 example, value x1used orinstruction depends whether branch taken not. Data dependence alone sufficient preserve correct-ness. orinstruction data-dependent add andsub instructions, preserving order alone insufficient correct execution.3.1 Instruction-Level Parallelism: Concepts Challenges ■175Instead, instructions execute, data flow must preserved: branch taken, value x1computed sub used theor, branch taken, value x1computed add used or. preserving control dependence oron branch, prevent illegal change data flow. similar reasons, sub instruc- tion cannot moved branch. Speculation, helps excep-tion problem, also allow us lessen impact control dependencewhile still maintaining data flow, see Section 3.6 . Sometimes determine violating control dependence cannot affect either exception behavior data flow. Consider following code sequence: add x1,x2,x3beq x12,x0,skipsub x4,x5,x6add x5,x4,x9 skip: x7,x8,x9 Suppose knew register destination subinstruction ( x4) unused instruction labeled skip . (The property whether value used upcoming instruction called liveness. )I fx4were unused, changing value ofx4just branch would affect data flow x4would dead (rather live) code region skip . Thus, x4were dead existing sub instruction could generate exception (other processor resumes process), could move sub instruction branch data flow could affected change. branch taken, sub instruction execute useless, affect program results. type code scheduling also form ofspeculation ,often called software speculation, compiler betting branch outcome; case, bet branch usually taken.More ambitious compiler speculation mechanisms discussed Appendix H. Normally, clear say speculation speculative whether mechanism hardware software mechanism; clear, bestto say “hardware speculation ”or“software speculation. ” Control dependence preserved implementing control hazard detection causes control stalls. Control stalls eliminated reduced varietyof hardware software techniques, examine Section 3.3 . 3.2 Basic Compiler Techniques Exposing ILP section examines use simple compiler technology enhance proces- sor’s ability exploit ILP. techniques crucial processors use static issue static scheduling. Armed compiler technology, shortlyexamine design performance processors using static issuing. AppendixH investigate sophisticated compiler associated hardware schemesdesigned enable processor exploit instruction-level parallelism.176 ■Chapter Three Instruction-Level Parallelism ExploitationBasic Pipeline Scheduling Loop Unrolling keep pipeline full, parallelism among instructions must exploited find- ing sequences unrelated instructions overlapped pipeline. Toavoid pipeline stall, execution dependent instruction must separatedfrom source instruction distance clock cycles equal pipelinelatency source instruction. compiler ’s ability perform scheduling depends amount ILP available program latencies ofthe functional units pipeline. Figure 3.2 shows FP unit latencies assume chapter, unless different latencies explicitly stated. assume standard five-stage integer pipeline branches delay one clock cycle. assume functional units fully pipelined replicated (asmany times pipeline depth) operation type issuedon every clock cycle structural hazards. section, look compiler increase amount avail- able ILP transforming loops. example serves illustrate importanttechnique well motivate powerful program transformationsdescribed Appendix H. rely following code segment, adds scalar vector: (i=999; >=0; i=i/C01) x[i] = x[i] + s; see loop parallel noticing body iteration independent. formalize notion Appendix H describe testwhether loop iterations independent compile time. First, let ’s look per- formance loop, shows use parallelism improve performance RISC-V pipeline preceding latencies. first step translate preceding segment RISC-V assembly language. following code segment, x1is initially address element array highest address, f2contains scalar value s.R e g e r x2is precom- puted Regs[x2]+8 address last element operate on. Instruction producing result Instruction using result Latency clock cycles FP ALU op Another FP ALU op 3 FP ALU op Store double 2Load double FP ALU op 1Load double Store double 0 Figure 3.2 Latencies FP operations used chapter. last column number intervening clock cycles needed avoid stall. numbers similar average latencies would see FP unit. latency floating-point load store 0 result load bypassed without stalling store.We continue assume integer load latency 1 integer ALU operation latency 0 (which includes ALU operation branch).3.2 Basic Compiler Techniques Exposing ILP ■177The straightforward RISC-V code, scheduled pipeline, looks like this: Loop: fld f0,0(x1) //f0=array element fadd.d f4,f0,f2 //add scalar f2fsd f4,0(x1) //store resultaddi x1,x1, /C08 //decrement pointer //8 bytes (per DW) bne x1,x2,Loop //branch x1 6¼x2 Let’s start seeing well loop run scheduled sim- ple pipeline RISC-V latencies Figure 3.2 . Example Show loop would look RISC-V, scheduled unscheduled, including stalls idle clock cycles. Schedule delays floating-pointoperations. Answer Without scheduling, loop execute follows, taking nine cycles: Clock cycle issued Loop: fld f0,0(x1) 1 stall 2 fadd.d f4,f0,f2 3 stall 4 stall 5 fsd f4,0(x1) 6 addi x1,x1, /C08 7 bne x1,x2,Loop 8 schedule loop obtain two stalls reduce time seven cycles: Loop: fld f0,0(x1) addi x1,x1, /C08 fadd.d f4,f0,f2 stallstallfsd f4,8(x1)bne x1,x2,Loop stalls fadd.d use fsd, repositioning addi pre- vents stall fld. previous example, complete one loop iteration store back one array element every seven clock cycles, actual work operating arrayelement takes three (the load, add, store) seven clock cycles.178 ■Chapter Three Instruction-Level Parallelism ExploitationThe remaining four clock cycles consist loop overhead —theaddi andbne— two stalls. eliminate four clock cycles, need get operations relative number overhead instructions. simple scheme increasing number instructions relative branch overhead instructions loop unrolling . Unrolling simply replicates loop body multiple times, adjusting loop termination code. Loop unrolling also used improve scheduling. eliminates branch, allows instructions different iterations scheduled together.In case, eliminate data use stalls creating additional independent instructions within loop body. simply replicated instructions unrolled loop, resulting use registers could prevent us fromeffectively scheduling loop. Thus want use different registers foreach iteration, increasing required number registers. Example Show loop unrolled four copies loop body, assuming x1/C0x2(that is, size array) initially multiple 32, means number loop iterations multiple 4. Eliminate obviously redundantcomputations reuse registers. Answer result merging addi instructions dropping unnec- essary bne operations duplicated unrolling. Note x2must set Regs[x2]+32 starting address last four elements. Loop: fld f0,0(x1) fadd.d f4,f0,f2fsd f4,0(x1) //drop addi & bnefld f6,/C08(x1) fadd.d f8,f6,f2fsd f8,/C08(x1) //drop addi & bne fld f0,/C016(x1) fadd.d f12,f0,f2 fsd f12,/C016(x1) //drop addi & bne fld f14,/C024(x1) fadd.d f16,f14,f2fsd f16,/C024(x1) addi x1,x1, /C032 bne x1,x2,Loop eliminated three branches three decrements x1. addresses loads stores compensated allow addi instructions x1 merged. optimization may seem trivial, not; requires symbolicsubstitution simplification. Symbolic substitution simplification rear-range expressions allow constants collapsed, allowing expressionsuch (( i+1)+1) rewritten ( i+(1+1)) simplified ( i+2).3.2 Basic Compiler Techniques Exposing ILP ■179We see general forms optimizations eliminate dependent computations Appendix H. Without scheduling, every FP load operation unrolled loop followed dependent operation thus cause stall. unrolled loop run in26 clock cycles —eachfld 1 stall, fadd.d 2, plus 14 instruction issue cycles —or 6.5 clock cycles four elements, sched- uled improve performance significantly. Loop unrolling normally done early compilation process redundant computations exposed andeliminated optimizer. real programs, usually know upper bound loop. Sup- pose n,and want unroll loop make kcopies body. Instead single unrolled loop, generate pair consecutive loops. first executes ( n mod k) times body original loop. second unrolled body surrounded outer loop iterates ( n/k) times. (As see Chapter 4 , technique similar technique called strip mining , used com- pilers vector processors.) large values n, execution time spent unrolled loop body. previous example, unrolling improves performance loop eliminating overhead instructions, although increases code size substantially.How unrolled loop perform scheduled pipeline describedearlier? Example Show unrolled loop previous example scheduled pipeline latencies Figure 3.2 . Answer Loop: fld f0,0(x1) fld f6,/C08(x1) fld f0,/C016(x1) fld f14,/C024(x1) fadd.d f4,f0,f2fadd.d f8,f6,f2fadd.d f12,f0,f2 fadd.d f16,f14,f2 fsd f4,0(x1)fsd f8,/C08(x1) fsd f12,16(x1)fsd f16,8(x1)addi x1,x1, /C032 bne x1,x2,Loop execution time unrolled loop dropped total 14 clock cycles, 3.5 clock cycles per element, compared 8 cycles per element beforeany unrolling scheduling 6.5 cycles unrolled scheduled.180 ■Chapter Three Instruction-Level Parallelism ExploitationThe gain scheduling unrolled loop even larger original loop. increase arises unrolling loop exposes computationthat scheduled minimize stalls; preceding code stalls.Scheduling loop fashion necessitates realizing loads stores independent interchanged. Summary Loop Unrolling Scheduling Throughout chapter Appendix H, look variety hardware software techniques allow us take advantage instruction-level parallelismto fully utilize potential functional units processor. key mostof techniques know ordering among instructions maybe changed. example, made many changes, us, humanbeings, obviously allowable. practice, process must performed methodical fashion either compiler hardware. obtain final unrolled code, make following decisions transformations: ■Determine unrolling loop would useful finding loop iter-ations independent, except loop maintenance code. ■Use different registers avoid unnecessary constraints would forced byusing registers different computations (e.g., name dependences). ■Eliminate extra test branch instructions adjust loop terminationand iteration code. ■Determine loads stores unrolled loop interchanged byobserving loads stores different iterations independent.This transformation requires analyzing memory addresses finding thatthey refer address. ■Schedule code, preserving dependences needed yield resultas original code. key requirement underlying transformations understanding one instruction depends another instructions changed reordered given dependences. Three different effects limit gains loop unrolling: (1) decrease amount overhead amortized unroll, (2) code size limitations, and(3) compiler limitations. Let ’s consider question loop overhead first. unrolled loop four times, generated sufficient parallelism among theinstructions loop could scheduled stall cycles. fact, 14clock cycles, 2 cycles loop overhead: addi , maintains index value, bne, terminates loop. loop unrolled eight times, overhead reduced 1/2 cycle per element 1/4. second limit unrolling resulting growth code size. larger loops, code size growth may concern, particularly causes increasein instruction cache miss rate.3.2 Basic Compiler Techniques Exposing ILP ■181Another factor often important code size potential shortfall registers created aggressive unrolling scheduling. secondary effect results instruction scheduling large code segments called reg- ister pressure. arises scheduling code increase ILP causes number live values increase. aggressive instruction scheduling, may bepossible allocate live values registers. transformed code, the-oretically faster, may lose advantage leads shortageof registers. Without unrolling, aggressive scheduling sufficiently limited bybranches register pressure rarely problem. combination unrolling aggressive scheduling can, however, cause problem. problem becomes especially challenging multiple-issue processors require exposure ofmore independent instruction sequences whose execution overlapped.In general, use sophisticated high-level transformations, whose potentialimprovements difficult measure detailed code generation, led tosignificant increases complexity modern compilers. Loop unrolling simple useful method increasing size straight- line code fragments scheduled effectively. transformation useful variety processors, simple pipelines like examined far multiple-issue superscalars VLIWs explored later chapter. 3.3 Reducing Branch Costs Advanced Branch Prediction need enforce control dependences branch hazards stalls, branches hurt pipeline performance. Loop unrolling one way reduce number branch hazards; also reduce performance losses branches predicting behave. Appendix C , examine sim- ple branch predictors rely either compile-time information theobserved dynamic behavior single branch isolation. number ofinstructions flight increased deeper pipelines issues per clock,the importance accurate branch prediction grown. section, weexamine techniques improving dynamic prediction accuracy. sectionmakes extensive use simple 2-bit predictor covered Section C.2, critical reader understand operation predictor proceeding. Correlating Branch Predictors 2-bit predictor schemes Appendix C use recent behavior single branch predict future behavior branch. may possible improvethe prediction accuracy also look recent behavior branches rather branch trying predict. Consider small code fragmentfrom eqntott benchmark, member early SPEC benchmark suites dis-played particularly bad branch prediction behavior:182 ■Chapter Three Instruction-Level Parallelism Exploitationif (aa==2) aa=0; (bb==2) bb=0; (aa!=bb) { RISC-V code would typically generate code frag- ment assuming aaandbbare assigned registers x1andx2: addi x3,x1, –2 bnez x3,L1 //branch b1 (aa!=2)add x1,x0,x0 //aa=0 L1: addi x3,x2, –2 bnez x3,L2 //branch b2 (bb!=2)add x2,x0,x0 //bb=0 L2: sub x3,x1,x2 //x3=aa-bb beqz x3,L3 //branch b3 (aa==bb) Let’s label branches b1, b2, b3. key observation behavior branch b3 correlated behavior branches b1 b2. Clearly, nei-ther branches b1 b2 taken (i.e., conditions evaluate true aa andbbare assigned 0), b3 taken, aaandbbare clearly equal. predictor uses behavior single branch predict out-come branch never capture behavior. Branch predictors use behavior branches make prediction called correlating predictors ortwo-level predictors . Existing correlating pre- dictors add information behavior recent branches decidehow predict given branch. example, (1,2) predictor uses behavior ofthe last branch choose among pair 2-bit branch predictors predictinga particular branch. general case, ( m,n) predictor uses behavior lastmbranches choose 2 mbranch predictors, n-bit pre- dictor single branch. attraction type correlating branch predictoris yield higher prediction rates 2-bit scheme requires trivial amount additional hardware. simplicity hardware comes simple observation: global history recent mbranches recorded m-bit shift register, bit records whether branch taken taken. branch-prediction buffer indexed using concatenation low-order bitsfrom branch address m-bit global history. example, (2,2) buffer 64 total entries, 4 low-order address bits branch (word address)and 2 global bits representing behavior two recently executed branches form 6-bit index used index 64 counters. combin- ing local global information concatenation (or simple hash function),we index predictor table result get prediction fast wecould standard 2-bit predictor, shortly.3.3 Reducing Branch Costs Advanced Branch Prediction ■183How much better correlating branch predictors work compared standard 2-bit scheme? compare fairly, must compare predictors use number state bits. number bits an(m,n) predictor 2m/C2n/C2Number prediction entries selected branch address 2-bit predictor global history simply (0,2) predictor. Example many bits (0,2) branch predictor 4K entries? many entries (2,2) predictor number bits? Answer predictor 4K entries 20/C22/C24K¼8K bits many branch-selected entries (2,2) predictor total 8K bits prediction buffer? know 22/C22/C2Number prediction entries selected branch ¼8K Therefore number prediction entries selected branch ¼1K. Figure 3.3 compares misprediction rates earlier (0,2) predictor 4K entries (2,2) predictor 1K entries. see, correlating pre-dictor outperforms simple 2-bit predictor total number state bits, also often outperforms 2-bit predictor unlimited number entries. Perhaps best-known example correlating predictor McFarling ’s gshare predictor. gshare index formed combining address thebranch recent conditional branch outcomes using exclusive-OR, essentially acts hash branch address branch history.The hashed result used index prediction array 2-bit counters, shown inFigure 3.4 . gshare predictor works remarkably well simple predictor, often used baseline comparison sophisticated predictors. Predictors combine local branch information global branch history alsocalled alloyed predictors hybrid predictors . Tournament Predictors: Adaptively Combining Local Global Predictors primary motivation correlating branch predictors came observa- tion standard 2-bit predictor, using local information, failed someimportant branches. Adding global history could help remedy situation.Tournament predictors take insight next level, using multiple predic- tors, usually global predictor local predictor, choosing them184 ■Chapter Three Instruction-Level Parallelism Exploitationwith selector, shown Figure 3.5 .Aglobal predictor uses recent branch history index predictor, local predictor uses address branch index. Tournament predictors another form hybrid oralloyed predictors. Tournament predictors achieve better accuracy medium sizes (8K –32K bits) also effectively use large numbers prediction bits. Existing tour-nament predictors use 2-bit saturating counter per branch choose among twodifferent predictors based predictor (local, global, even time-varying mix) effective recent predictions. simple 2-bit predic-tor, saturating counter requires two mispredictions changing identityof preferred predictor. advantage tournament predictor ability select right predic- tor particular branch, particularly crucial integer benchmarks.nASA7 matrix300 tomcatv doducSPEC89 benchmarksspice fpppp gcc espresso eqntott li 0% 2% 4% 6% 8% 10% 12% 14% 16% 18% Frequenc mis predictions1% 0%1% 0% 0%0% 1% 0%1% 5% 5% 5% 9% 9% 5% 9% 9% 5% 12% 11% 11% 5% 5% 4% 18% 18% 6% 10% 10% 5%1024 entries: (2,2)Unlimited entries: 2 bits per entry4096 entries: 2 bits per entry Figure 3.3 Comparison 2-bit predictors. noncorrelating predictor 4096 bits first, followed noncor- relating 2-bit predictor unlimited entries 2-bit predictor 2 bits global history total 1024entries. Although data older version SPEC, data recent SPEC benchmarks would show similar differences accuracy.3.3 Reducing Branch Costs Advanced Branch Prediction ■185Branch history Branch address Prediction10-bit shift register recent branch result (not taken/taken) Exclusive OR1024 2-bit predictors10 10 10 Figure 3.4 gshare predictor 1024 entries, standard 2-bit predictor. Branch history Predictionm u xGlobal predictorsBranch address Selector Local predictors Figure 3.5 tournament predictor using branch address index set 2-bit selection counters, choose local global predictor. case, index selector table current branch address. two tables also 2-bit predictors indexed global history branch address, respec- tively. selector acts like 2-bit predictor, changing preferred predictor branch address two mis- predicts occur row. number bits branch address used index selector table localpredictor table equal length global branch history used index global prediction table. Note misprediction bit tricky need change selector table either global local predictor.186 ■Chapter Three Instruction-Level Parallelism ExploitationA typical tournament predictor select global predictor almost 40% time SPEC integer benchmarks less 15% time SPEC FP benchmarks. addition Alpha processors pioneered tournament pre-dictors, several AMD processors used tournament-style predictors. Figure 3.6 looks performance three different predictors (a local 2-bit predictor, correlating predictor, tournament predictor) different num-bers bits using SPEC89 benchmark. local predictor reaches limitfirst. correlating predictor shows significant improvement, tourna-ment predictor generates slightly better performance. recent versions SPEC, results would similar, asymptotic behavior would reached slightly larger predictor sizes. local predictor consists two-level predictor. top level local history table consisting 1024 10-bit entries; 10-bit entry corresponds tothe recent 10 branch outcomes entry. is, branch taken10 times row, entry local history table 1s. Ifthe branch alternately taken untaken, history entry consists alternating0s 1s. 10-bit history allows patterns 10 branches discovered predicted. selected entry local history table used index table 1K entries consisting 3-bit saturating counters, provide local pre-diction. combination, uses total 29K bits, leads high accuracy 6%7%8% 5% 4% 3%2% 1% 0%Conditional branch misprediction rate Total predictor sizeLocal 2-bit predictors Correlating predictors Tournament predictors 512 480 448 416 384 352 320 288 256 224 192 160 128 96 64 32 0 Figure 3.6 misprediction rate three different predictors SPEC89 versus size predictor kilobits. predictors local 2-bit predictor, correlating predictor optimally structured use global local information point graph, tournament predictor. Although data arefor older version SPEC, data recent SPEC benchmarks show similar behavior, perhaps convergingto asymptotic limit slightly larger predictor sizes.3.3 Reducing Branch Costs Advanced Branch Prediction ■187branch prediction requiring fewer bits single level table prediction accuracy. Tagged Hybrid Predictors best performing branch prediction schemes 2017 involve combining multiple predictors track whether prediction likely associated withthe current branch. One important class predictors loosely based algo-rithm statistical compression called PPM (Prediction Partial Matching).PPM (see Jim /C19enez Lin, 2001), like branch prediction algorithm, attempts predict future behavior based history. class branch predictors, whichwe call tagged hybrid predictors (see Seznec Michaud, 2006), employs series global predictors indexed different length histories. example, shown Figure 3.7 , five-component tagged hybrid predictor five prediction tables: P(0), P(1), . . . P(4), P( i) accessed using hash ofBase predictorpredP(0) P(1) tag =?hashpcpc h[0:L(1)] hash predP(2) tag =?hashpc h[0:L(2)] hash predP(3) tag =?hashpc h[0:L(3)] hash pred PredictionP(4) tag =?hashpc h[0:L(4)] hash Figure 3.7 five-component tagged hybrid predictor five separate prediction tables, indexed hash branch address segment recent branch history length 0 –4 labeled “h”in figure. hash simple exclusive-OR, gshare. predictor 2-bit (or possibly 3-bit) predictor. tags aretypically 4 –8 bits. chosen prediction one longest history tags also match.188 ■Chapter Three Instruction-Level Parallelism Exploitationthe PC history recent ibranches (kept shift register, h, gshare). use multiple history lengths index separate predictors first critical difference. second critical difference use tags tablesP(1) P(4). tags short 100% matches required:a small tag 4 –8 bits appears gain advantage. prediction P(1), . . . P(4) used tags match hash branch address global branch history. predictors P(0 …n) standard 2-bit predictor. practice 3-bit counter, requires three mispredictions changea prediction, gives slightly better results 2-bit counter. prediction given branch predictor longest branch his- tory also matching tags. P(0) always matches uses tags andbecomes default prediction none P(1) P( n) match. tagged hybrid version predictor also includes 2-bit use field thehistory-indexed predictors. use field indicates whether prediction wasrecently used therefore likely accurate; use field period-ically reset entries old predictions cleared. Many details areinvolved implementing style predictor, especially handle mispre- dictions. search space optimal predictor also large number predictors, exact history used indexing, size pre-dictor variable. Tagged hybrid predictors (sometimes called TAGE —TAgged GEometic — predictors) earlier PPM-based predictors winners recentannual international branch-prediction competitions. predictors outperformgshare tournament predictors modest amounts memory (32 – 64 KiB), addition, class predictors seems able effectively use larger prediction caches deliver improved prediction accuracy. Another issue larger predictors initialize predictor. could initialized randomly, case, take fair amount execution time tofill predictor useful predictions. predictors (including many recentpredictors) include valid bit, indicating whether entry predictor beenset “unused state. ”In latter case, rather use random prediction, could use method initialize prediction entry. example, someinstruction sets contain bit indicates whether associated branch expected taken not. days dynamic branch prediction, hint bits prediction; recent processors, hint bit used set initial prediction. could also set initial prediction basis branch direc-tion: forward going branches initialized taken, backward goingbranches, likely loop branches, initialized taken. pro-grams shorter running times processors larger predictors, initialsetting measurable impact prediction performance. Figure 3.8 shows hybrid tagged predictor significantly outperforms gshare, especially less predictable programs like SPECint server appli- cations. figure, performance measured mispredicts per thousandinstructions; assuming branch frequency 20% –25%, gshare mispredict rate (per branch) 2.7% –3.4% multimedia benchmarks, tagged3.3 Reducing Branch Costs Advanced Branch Prediction ■189hybrid predictor misprediction rate 1.8% –2.2%, roughly one-third fewer mispredicts. Compared gshare, tagged hybrid predictors complex toimplement probably slightly slower need check multipletags choose prediction result. Nonetheless, deeply pipelined processorswith large penalties branch misprediction, increased accuracy outweighsthose disadvantages. Thus many designers higher-end processors optedto include tagged hybrid predictors newest implementations. Evolution Intel Core i7 Branch Predictor mentioned previous chapter, six generations Intel Core i7processors 2008 (Core i7 920 using Nehalem microarchitecture) and2016 (Core i7 6700 using Skylake microarchitecture). combi-nation deep pipelining multiple issues per clock, i7 many instruc-tions in-flight (up 256, typically least 30). makes branchprediction critical, area Intel making constantimprovements. Perhaps performance-critical nature branch predictor, Intel tended keep details branch predictors highly secret.0 SPECfp1.3011234Misses per one thousand instructions5678 SPECint3.2996.607 MultiMedia4.456.778 Server2.9396.52TAGE gshare 0.59 Figure 3.8 comparison misprediction rate (measured mispredicts per 1000 instructions executed) tagged hybrid versus gshare. predictors use total number bits, although tagged hybrid uses storage tags, gshare contains tags. benchmarks consist traces SPECfp SPECint, aseries multimedia server benchmarks. latter two behave like SPECint.190 ■Chapter Three Instruction-Level Parallelism ExploitationEven older processors Core i7 920 introduced 2008, released limited amounts information. section, briefly describe known compare performance predictors Core i7 920 withthose latest Core i7 6700. Core i7 920 used two-level predictor smaller first-level predictor, designed meet cycle constraints predicting branch every clock cycle, alarger second-level predictor backup. predictor combines three differentpredictors: (1) simple 2-bit predictor, introduced Appendix C (and used preceding tournament predictor); (2) global history predictor, like saw; (3) loop exit predictor. loop exit predictor uses counter predict exact number taken branches (which number loop itera-tions) branch detected loop branch. branch, best pre-diction chosen among three predictors tracking accuracy eachprediction, like tournament predictor. addition multilevel main predictor,a separate unit predicts target addresses indirect branches, stack predictreturn addresses also used. Although even less known predictors newest i7 processors, good reason believe Intel employing tagged hybrid predictor. One advantage predictor combines functions threesecond-level predictors earlier i7. tagged hybrid predictor differenthistory lengths subsumes loop exit predictor well local global his-tory predictor. separate return address predictor still employed. cases, speculation causes challenges evaluating predic- tor mispredicted branch easily lead another branch fetchedand mispredicted. keep things simple, look number mispredictions percentage number successfully completed branches (those result misspeculation). Figure 3.9 shows data SPEC- PUint2006 benchmarks. benchmarks considerably larger SPEC89or SPEC2000, result misprediction rates higher thanthose Figure 3.6 even powerful combination predictors. branch misprediction leads ineffective speculation, contributes wastedwork, see later chapter. 3.4 Overcoming Data Hazards Dynamic Scheduling simple statically scheduled pipeline fetches instruction issues it, unlessthere data dependence instruction already pipeline thefetched instruction cannot hidden bypassing forwarding. (Forward-ing logic reduces effective pipeline latency certain dependences donot result hazards.) data dependence cannot hidden, hazard detection hardware stalls pipeline starting instruction uses result. new instructions fetched issued dependence cleared. section, explore dynamic scheduling , technique hard- ware reorders instruction execution reduce stalls maintaining data3.4 Overcoming Data Hazards Dynamic Scheduling ■191flow exception behavior. Dynamic scheduling offers several advantages. First, allows code compiled one pipeline mind run efficiently different pipeline, eliminating need multiple binaries recompile adifferent microarchitecture. today ’s computing environment, much software third parties distributed binary form, advantage sig-nificant. Second, enables handling cases dependences unknown atcompile time; example, may involve memory reference data-dependent branch, may result modern programming environmentthat uses dynamic linking dispatching. Third, perhaps importantly, allows processor tolerate unpredictable delays, cache misses, executing code waiting miss resolve. Section 3.6 ,w e explore hardware speculation, technique additional performance advantages,which builds dynamic scheduling. see, advantages dynamicscheduling gained cost significant increase hardware complexity. Although dynamically scheduled processor cannot change data flow, tries avoid stalling dependences present. contrast, static pipelinescheduling compiler (covered Section 3.2 ) tries minimize stalls sep- arating dependent instructions lead hazards. course,0.0% astar bzip2gcc gobmkh264ref hmmer libquantummcf omnetppperlbenchsjeng xalancbmk1.0%2.0%3.0%4.0%Branch misprediction rate5.0%6.0%7.0%i7 6700 i7 9208.0%9.0% Figure 3.9 misprediction rate integer SPECCPU2006 benchmarks Intel Core i7 920 6700. misprediction rate computed ratio completed branches mispredicted versus completed branches. could understate misprediction rate somewhat branch mispredicted led another mispredicted branch (which executed), counted one misprediction.On average, i7 920 mispredicts branches 1.3 times often i7 6700.192 ■Chapter Three Instruction-Level Parallelism Exploitationcompiler pipeline scheduling also used code destined run proces- sor dynamically scheduled pipeline. Dynamic Scheduling: Idea major limitation simple pipelining techniques use in-order instruc- tion issue execution: instructions issued program order, instruc-tion stalled pipeline, later instructions proceed. Thus, adependence two closely spaced instructions pipeline, lead toa hazard, stall result. multiple functional units, unitscould lie idle. instruction jdepends long-running instruction i,currently execution pipeline, instructions jmust stalled iis finished jcan execute. example, consider code: fdiv.d f0,f2,f4 fadd.d f10,f0,f8fsub.d f12,f8,f14 Thefsub.d instruction cannot execute dependence fadd.d fdiv.d causes pipeline stall; yet, fsub.d data-dependent any- thing pipeline. hazard creates performance limitation elim-inated requiring instructions execute program order. classic five-stage pipeline, structural data hazards could checked instruction decode (ID): instruction could execute withouthazards, issued ID, recognition data hazards beenresolved. allow us begin executing fsub.d preceding example, must separate issue process two parts: checking structural hazards andwaiting absence data hazard. Thus still use in-order instruction issue(i.e., instructions issued program order), want instruction begin exe- cution soon data operands available. pipeline out-of-order execution , implies out-of-order completion . Out-of-order execution introduces possibility WAR WAW hazards, exist five-stage integer pipeline logical extension anin-order floating-point pipeline. Consider following RISC-V floating-pointcode sequence: fdiv.d f0,f2,f4fmul.d f6,f0,f8 fadd.d f0,f10,f14 antidependence fmul.d fadd.d (for register f0), pipeline executes fadd.d fmul.d (which wait- ing fdiv.d ), violate antidependence, yielding WAR hazard. Likewise, avoid violating output dependences, write f0by fadd.d fdiv.d completes, WAW hazards must handled. see, hazards avoided use register renaming.3.4 Overcoming Data Hazards Dynamic Scheduling ■193Out-of-order completion also creates major complications handling excep- tions. Dynamic scheduling out-of-order completion must preserve exception behavior sense exactly exceptions would arise program executed strict program order actually arise. Dynamically scheduled processors preserve exception behavior delaying notification associ-ated exception processor knows instruction next onecompleted. Although exception behavior must preserved, dynamically scheduled pro- cessors could generate imprecise exceptions. exception imprecise processor state exception raised look exactly instruc- tions executed sequentially strict program order. Imprecise exceptions canoccur two possibilities: 1.The pipeline may already completed instructions later program order instruction causing exception. 2.The pipeline may yet completed instructions earlier program order instruction causing exception. Imprecise exceptions make difficult restart execution exception. Rather address problems section, discuss solution thatprovides precise exceptions context processor speculation Section 3.6 . floating-point exceptions, solutions used, dis- cussed Appendix J. allow out-of-order execution, essentially split ID pipe stage simple five-stage pipeline two stages: 1.Issue—Decode instructions, check structural hazards. 2.Read operands —Wait data hazards, read operands. instruction fetch stage precedes issue stage may fetch either instruction register queue pending instructions; instructions thenissued register queue. execution stage follows read operands stage, five-stage pipeline. Execution may take multiple cycles, depend- ing operation. distinguish instruction begins execution completes execution ; two times, instruction execution. pipeline allows multiple instructions execution time; without capa-bility, major advantage dynamic scheduling lost. multiple instruc-tions execution requires multiple functional units, pipelined functionalunits, both. two capabilities —pipelined functional units multiple functional units —are essentially equivalent purposes pipeline control, assume processor multiple functional units. dynamically scheduled pipeline, instructions pass issue stage order (in-order issue); however, stalled bypass each194 ■Chapter Three Instruction-Level Parallelism Exploitationother second stage (read operands) thus enter execution order. Scoreboarding technique allowing instructions execute order sufficient resources data dependences; named CDC6600 scoreboard, developed capability. focus sophis-ticated technique, called Tomasulo ’s algorithm. primary difference Tomasulo ’s algorithm handles antidependences output dependences effec- tively renaming registers dynamically. Additionally, Tomasulo ’s algorithm extended handle speculation , technique reduce effect control dependences predicting outcome branch, executing instructions predicted destination address, taking corrective actions prediction wrong. use scoreboarding probably sufficient support sim-pler processors, sophisticated, higher performance processors make use ofspeculation. Dynamic Scheduling Using Tomasulo ’s Approach IBM 360/91 floating-point unit used sophisticated scheme allow out-of- order execution. scheme, invented Robert Tomasulo, tracks oper-ands instructions available minimize RAW hazards introduces reg-ister renaming hardware minimize WAW WAR hazards. Although thereare many variations scheme recent processors, rely two keyprinciples: dynamically determining instruction ready execute andrenaming registers avoid unnecessary hazards. IBM’s goal achieve high floating-point performance instruction set compilers designed entire 360 computer family, rather specialized compilers high-end processors. 360 architecturehad four double-precision floating-point registers, limited effective-ness compiler scheduling; fact another motivation Tomasuloapproach. addition, IBM 360/91 long memory accesses longfloating-point delays, Tomasulo ’s algorithm designed overcome. end section, see Tomasulo ’s algorithm also support overlapped execution multiple iterations loop. explain algorithm, focuses floating-point unit load- store unit, context RISC-V instruction set. primary differencebetween RISC-V 360 presence register-memory instructions inthe latter architecture. Tomasulo ’s algorithm uses load functional unit, significant changes needed add register-memory addressing modes. TheIBM 360/91 also pipelined functional units, rather multiple functionalunits, describe algorithm multiple functional units. Itis simple conceptual extension also pipeline functional units. RAW hazards avoided executing instruction operands available, exactly simpler scoreboarding approach provides.WAR WAW hazards, arise name dependences, eliminated byregister renaming. Register renaming eliminates hazards renaming all3.4 Overcoming Data Hazards Dynamic Scheduling ■195destination registers, including pending read write earlier instruction, out-of-order write affect instructions depend earlier value operand. compiler could typically implementsuch renaming, enough registers available ISA. original360/91 four floating-point registers, Tomasulo ’s algorithm cre- ated overcome shortage. Whereas modern processors 32 –64 floating- point integer registers, number renaming registers available recentimplementations hundreds. better understand register renaming eliminates WAR WAW haz- ards, consider following example code sequence includes potential WAR WAW hazards: fdiv.d f0,f2,f4 fadd.d f6,f0,f8 fsd f6,0(x1) fsub.d f8,f10,f14fmul.d f6,f10,f8 two antidependences: fadd.d fsub.d fsd fmul.d . also output dependence thefadd.d fmul.d , leading three possible hazards: WAR hazards use f8byfadd.d use fsub.d , well WAW hazard fadd.d may finish later fmul.d . also three true data dependences: fdiv.d fadd.d , fsub.d fmul.d , fadd.d fsd. three name dependences eliminated register renaming. simplicity, assume existence two temporary registers, SandT. Using Sand T, sequence rewritten without dependences fdiv.d f0,f2,f4 fadd.d S,f0,f8 fsd S,0(x1) fsub.d T,f10,f14fmul.d f6,f10,T addition, subsequent uses f8must replaced register T. example, renaming process done statically compiler. Finding anyuses f8that later code requires either sophisticated compiler analysis hardware support may intervening branches pre-ceding code segment later use f8. see, Tomasulo ’s algorithm handle renaming across branches. Tomasulo ’s scheme, register renaming provided reservation stations , buffer operands instructions waiting issue associated functional units. basic idea reservation station fetches buffers operand soon available, eliminating need get operand froma register. addition, pending instructions designate reservation station thatwill provide input. Finally, successive writes register overlap in196 ■Chapter Three Instruction-Level Parallelism Exploitationexecution, last one actually used update register. instructions issued, register specifiers pending operands renamed names reservation station, provides register renaming. reservation stations real registers, technique even eliminate hazards arising name dependences could elim-inated compiler. explore components Tomasulo ’s scheme, return topic register renaming see exactly renamingoccurs eliminates WAR WAW hazards. use reservation stations, rather centralized register file, leads two important properties. First, hazard detection execution control distributed: information held reservation stations functional unitdetermines instruction begin execution unit. Second, results arepassed directly functional units reservation stations arebuffered, rather going registers. bypassing done acommon result bus allows units waiting operand loaded simul-taneously (on 360/91, called common data bus , CDB). pipelines issue multiple instructions per clock also multiple execution units, one result bus needed. Figure 3.10 shows basic structure Tomasulo-based processor, includ- ing floating-point unit load/store unit; none execution con-trol tables shown. reservation station holds instruction beenissued awaiting execution functional unit. operand values thatinstruction computed, also stored entry; otherwise, thereservation station entry keeps names reservation stations pro-vide operand values. load buffers store buffers hold data addresses coming going memory behave almost exactly like reservation stations, dis-tinguish necessary. floating-point registers connectedby pair buses functional units single bus store buffers.All results functional units memory sent common databus, goes everywhere except load buffer. reservation stations havetag fields, employed pipeline control. describe details reservation stations algorithm, let’s look steps instruction goes through. three steps, although one take arbitrary number clock cycles: 1.Issue—Get next instruction head instruction queue, maintained FIFO order ensure maintenance correct data flow. thereis matching reservation station empty, issue instruction stationwith operand values, currently registers. anempty reservation station, structural hazard, instruction issue stalls station buffer freed. operands reg- isters, keep track functional units produce operands. steprenames registers, eliminating WAR WAW hazards. (This stage some-times called dispatch dynamically scheduled processor.)3.4 Overcoming Data Hazards Dynamic Scheduling ■1972.Execute —If one operands yet available, monitor com- mon data bus waiting computed. operand becomesavailable, placed reservation station awaiting it. oper-ands available, operation executed corresponding functionalunit. delaying instruction execution operands available, RAWhazards avoided. (Some dynamically scheduled processors call step“issue, ”but use name “execute, ”which used first dynamically scheduled processor, CDC 6600.)From instruction unit Floating-point operationsFP registers Reservation stations FP adders FP multipliers3 2 12 1 Common data bus (CDB)Operation busOperand busesLoad/store operations Address unit Load buffers Memory unitAddress DataInstruction queue Store buffers Figure 3.10 basic structure RISC-V floating-point unit using Tomasulo ’s algorithm. Instructions sent instruction unit instruction queue issued first-in, first-out (FIFO) order. Thereservation stations include operation actual operands, well information used detecting resolving hazards. Load buffers three functions: (1) hold components effective address com- puted, (2) track outstanding loads waiting memory, (3) hold results completed loads thatare waiting CDB. Similarly, store buffers three functions: (1) hold components effective address computed, (2) hold destination memory addresses outstanding stores waiting data value store, (3) hold address value store memory unit available. results either theFP units load unit put CDB, goes FP register file well reservation stations store buffers. FP adders implement addition subtraction, FP multipliers multiplication division.198 ■Chapter Three Instruction-Level Parallelism ExploitationNotice several instructions could become ready clock cycle functional unit. Although independent functional units could begin execution clock cycle different instructions, thanone instruction ready single functional unit, unit chooseamong them. floating-point reservation stations, choice may madearbitrarily; loads stores, however, present additional complication. Loads stores require two-step execution process. first step com- putes effective address base register available, effectiveaddress placed load store buffer. Loads load buffer exe- cute soon memory unit available. Stores store buffer wait value stored sent memory unit. Loads stores aremaintained program order effective address calculation, whichwill help prevent hazards memory. preserve exception behavior, instruction allowed initiate execu- tion branch precedes instruction program order completed.This restriction guarantees instruction causes exception duringexecution really would executed. processor using branch predic- tion (as dynamically scheduled processors do), means processor must know branch prediction correct allowing instructionafter branch begin execution. processor records occurrence ofthe exception, actually raise it, instruction start execution butnot stall enters Write Result. Speculation provides flexible complete method handle exceptions, delay making enhancement show specula-tion handles problem later. 3.Write result —When result available, write CDB registers reservation stations (including store buffers) wait-ing result. Stores buffered store buffer value stored store address available; result written soon memory unit free. data structures detect eliminate hazards attached reserva- tion stations, register file, load store buffers slightly dif- ferent information attached different objects. tags essentially names foran extended set virtual registers used renaming. example, tag field isa 4-bit quantity denotes one five reservation stations one five loadbuffers. combination produces equivalent 10 registers (5 reservation sta-tions+5 load buffers) designated result registers (as opposed thefour double-precision registers 360 architecture contains). processorwith real registers, want renaming provide even larger set virtual registers, often numbering hundreds. tag field describes reservation station contains instruction produce result needed source operand. instruction issued waiting source operand, refers operand reservation station number instruction write3.4 Overcoming Data Hazards Dynamic Scheduling ■199the register assigned. Unused values, zero, indicate oper- already available registers. reservation stations actual register numbers, WAW WAR hazards eliminated renamingresults using reservation station numbers. Although Tomasulo ’s scheme res- ervation stations used extended virtual registers, approaches coulduse register set additional registers structure like reorder buffer,which see Section 3.6 . Tomasulo ’s scheme, well subsequent methods look sup- porting speculation, results broadcast bus (the CDB), monitored reservation stations. combination common result bus retrieval results bus reservation stations implements forward-ing bypassing mechanisms used statically scheduled pipeline. so,however, dynamically scheduled scheme, Tomasulo ’s algorithm, intro- duces one cycle latency source result matching aresult use cannot done end Write Result stage, opposedto end Execute stage simpler pipeline. Thus, dynamically sched-uled pipeline, effective latency producing instruction consum- ing instruction least one cycle longer latency functional unit producing result. important remember tags Tomasulo scheme refer buffer unit produce result; register names discarded aninstruction issues reservation station. (This key difference Toma-sulo’s scheme scoreboarding: scoreboarding, operands stay registers read producing instruction completes consuminginstruction ready execute.) reservation station seven fields: ■Op—The operation perform source operands S1 S2. ■Qj, Qk —The reservation stations produce corresponding source operand; value zero indicates source operand already availablein Vj Vk, unnecessary. ■Vj, Vk —The value source operands. Note one V fields Q field valid operand. loads, Vk field used hold offset field. ■A—Used hold information memory address calculation load store. Initially, immediate field instruction stored here; address calculation, effective address stored here. ■Busy—Indicates reservation station accompanying functional unit occupied. register file field, Qi: ■Qi—The number reservation station contains operation whose result stored register. value Qi blank (or 0), no200 ■Chapter Three Instruction-Level Parallelism Exploitationcurrently active instruction computing result destined register, meaning value simply register contents. load store buffers field, A, holds result effec- tive address first step execution completed. next section, first consider examples show mechanisms work examine detailed algorithm. 3.5 Dynamic Scheduling: Examples Algorithm examine Tomasulo ’s algorithm detail, let ’s consider examples help illustrate algorithm works. Example Show information tables look like following code sequence first load completed written result: 1. fld f6,32(x2) 2. fld f2,44(x3)3. fmul.d f0,f2,f44. fsub.d f8,f2,f65. fdiv.d f0,f0,f6 6. fadd.d f6,f8,f2 Answer Figure 3.11 shows result three tables. numbers appended names Add, Mult, Load stand tag reservation station —Add1 tag result first add unit. addition, included instructionstatus table. table included help understand algorithm; isnotactually part hardware. Instead, reservation station keeps state operation issued. Tomasulo ’s scheme offers two major advantages earlier simpler schemes: (1) distribution hazard detection logic, (2) elimination stalls WAW WAR hazards. first advantage arises distributed reservation stations use CDB. multiple instructions waiting single result, instruc-tion already operand, instructions released simulta-neously broadcast result CDB. centralized register filewere used, units would read results registers reg-ister buses available. second advantage, elimination WAW WAR hazards, accom- plished renaming registers using reservation stations process storing operands reservation station soon available. example, code sequence Figure 3.11 issues fdiv.d fadd.d , even though WAR hazard involving f6. hazard is3.5 Dynamic Scheduling: Examples Algorithm ■201eliminated one two ways. First, instruction providing value fdiv.d completed, Vk store result, allowing fdiv.d execute independent fadd.d (this case shown). hand, fld hasn’t completed, Qk point Load1 reservation station, fdiv.d instruction independent fadd.d . Thus, either case, thefadd.d issue begin executing. uses result fdiv.d point reservation station, allowing fadd.d complete store value registers without affecting fdiv.d . Instruction status Instruction Issue Execute Write result fld f6,32(x2) √√ √ fld f2,44(x3) √√ fmul.d f0,f2,f4 √ fsub.d f8,f2,f6 √ fdiv.d f0,f0,f6 √ fadd.d f6,f8,f2 √ Reservation stations Name Busy Op Vj Vk Qj Qk Load1 Load2 Yes Load 44 + Regs[x3] Add1 Yes SUB Mem[32 + Regs[x2]] Load2 Add2 Yes ADD Add1 Load2Add3 NoMult1 Yes MUL Regs[f4] Load2 Mult2 Yes DIV Mem[32 + Regs[x2]] Mult1 Register status Field f0 f2 f4 f6 f8 f10 f12 … f30 Qi Mult1 Load2 Add2 Add1 Mult2 Figure 3.11 Reservation stations register tags shown instructions issued first load instruction completed written result CDB. second load completed effective address calculation waiting memory unit. use array Regs[ ] refer register file andthe array Mem[ ] refer memory. Remember operand specified either Q field V field time. Notice fadd.d instruction, WAR hazard WB stage, issued could complete fdiv.d initiates.202 ■Chapter Three Instruction-Level Parallelism ExploitationWe’ll see example elimination WAW hazard shortly. let ’s first look earlier example continues execution. example, ones follow chapter, assume following latencies: load 1 clockcycle, add 2 clock cycles, multiply 6 clock cycles, divide 12 clockcycles. Example Using code segment previous example ( page 201 ), show status tables look like fmul.d ready write result. Answer result shown three tables Figure 3.12 . Notice fadd.d com- pleted operands fdiv.d copied, thereby overcoming WAR hazard. Notice even load f6wasfdiv.d , add f6 could executed without triggering WAW hazard. Instruction status Instruction Issue Execute Write result fld f6,32(x2) √√ √ fld f2,44(x3) √√ √ fmul.d f0,f2,f4 √√ fsub.d f8,f2,f6 √√ √ fdiv.d f0,f0,f6 √ fadd.d f6,f8,f2 √√ √ Reservation stations Name Busy Op Vj Vk Qj Qk Load1 Load2 NoAdd1 NoAdd2 Add3 Mult1 Yes MUL Mem[44 + Regs[x3]] Regs[f4] Mult2 Yes DIV Mem[32 + Regs[x2]] Mult1 Register status Field f0 f2 f4 f6 f8 f10 f12 … f30 Qi Mult1 Mult2 Figure 3.12 Multiply divide instructions finished.3.5 Dynamic Scheduling: Examples Algorithm ■203Tomasulo ’s Algorithm: Details Figure 3.13 specifies checks steps instruction must go through. mentioned earlier, loads stores go hrough functional unit effective address computation proceeding independent load store buffers. Loads take second execution step access memory go WriteResult send value memory register file and/or waitingreservation stations. Stores complete ir execution Write Result stage, writes result memory. Notice writes occur Write Result,whether destination register memory. restriction simplifies Tomasulo ’s algorithm critical extension speculation Section 3.6 . Tomasulo ’s Algorithm: Loop-Based Example understand full power eliminating WAW WAR hazards dynamic renaming registers, must look loop. Consider following sim- ple sequence multiplying elements array scalar f2: Loop: fld f0,0(x1) fmul.d f4,f0,f2fsd f4,0(x1) addi x1,x1, /C08 bne x1,x2,Loop // branches x1 6¼x2 predict branches taken, using reservation stations allow multiple executions loop proceed once. advantage gained without changing code —in effect, loop unrolled dynamically hard- ware using reservation stations obtained renaming act additionalregisters. Let’s assume issued instruc tions two successive iterations loop, none floating-point load/stores operations com-pleted. Figure 3.14 shows reservation stations, register status tables, load store buffers point. (The integer ALU operation ignored, assumed branch predicted taken.) system reaches state, two copies loop could sustained CPI close 1.0, providedthe multiplies could complete four cl ock cycles. latency six cycles, additional iterations need processed steady state bereached. requires reservatio n stations hold ins tructions execution. see later chapter, extended multiple issue instructions, Tomasulo ’s approach sustain one instruction per clock. load store done safely order, provided access dif- ferent addresses. load store access address, one two thingshappens:204 ■Chapter Three Instruction-Level Parallelism ExploitationInstruction state Wait Action bookkeeping Issue FP operationStation rempty (RegisterStat[rs].Qi 6¼0) {RS[r].Qj RegisterStat[rs].Qi} else {RS[r].Vj Regs[rs]; RS[r].Qj 0}; (RegisterStat[rt].Qi 6¼0) {RS[r].Qk RegisterStat[rt].Qi else {RS[r].Vk Regs[rt]; RS[r].Qk 0}; RS[r].Busy yes; RegisterStat[rd].Q r; Load store Buffer rempty (RegisterStat[rs].Qi 6¼0) {RS[r].Qj RegisterStat[rs].Qi} else {RS[r].Vj Regs[rs]; RS[r].Qj 0}; RS[r].A imm; RS[r].Busy yes; Load RegisterStat[rt].Qi r; Store (RegisterStat[rt].Qi 6¼0) {RS[r].Qk RegisterStat[rs].Qi} else {RS[r].Vk Regs[rt]; RS[r].Qk 0}; Execute FP operation(RS[r].Qj = 0) (RS[r].Qk = 0)Compute result: operands Vj Vk Load/storestep 1 RS[r].Qj ¼0&ris head load-store queueRS[r].A RS[r].Vj + RS[r].A; Load step 2 Load step 1 complete Read Mem[RS[r].A] Write result FP operation loadExecution complete r& CDB available8x(if (RegisterStat[x].Qi=r) {Regs[x] result; RegisterStat[x].Qi 0}); 8x(if (RS[x].Qj=r) {RS[x].Vj result;RS[x].Qj 0}); 8x(if (RS[x].Qk=r) {RS[x].Vk result;RS[x].Qk 0}); RS[r].Busy no; Store Execution complete r& RS[r].Qk = 0Mem[RS[r].A] RS[r].Vk; RS[r].Busy no; Figure 3.13 Steps algorithm required step. issuing instruction, rdis des- tination, rsandrtare source register numbers, imm sign-extended immediate field, ris reser- vation station buffer instruction assigned to. RSis reservation station data structure. value returned FP unit load unit called result .RegisterStat register status data structure (not register file, Regs[] ). instruction issued, destination register Qi field set number buffer reservation station instruction issued. operands availablein registers, stored V fields. Otherwise, Q fields set indicate reservation station thatwill produce values needed source operands. instruction waits reservation station operands available, indicated zero Q fields. Q fields set zero either instruction issued instruction instruction depends completes write back. aninstruction finished execution CDB available, write back. buffers, registers, reservation stations whose values Qj Qk completing reservation station update values CDB mark Q fields indicate values received. Thus CDB broadcast resultto many destinations single clock cycle, waiting instructions operands, begin execution next clock cycle. Loads go two steps execute, stores perform slightly differently Write Result, may wait value store. Remember that, preserve exception behav-ior, instructions allowed execute branch earlier program order yet completed. concept program order maintained issue stage, restriction usually implemented preventing instruction leaving issue step pending branch already pipeline. InSection 3.6 , see speculation support removes restriction.■The load store program order interchanging results WAR hazard. ■The store load program order interchanging results ina RAW hazard. Similarly, interchanging two stores address results WAW hazard. Therefore, determine load executed given time, processor check whether uncompleted store precedes load program orderInstruction status Instruction iteration Issue Execute Write result fld f0,0(x1) 1 √√ fmul.d f4,f0,f2 1 √ fsd f4,0(x1) 1 √ fld f0,0(x1) 2 √√ fmul.d f4,f0,f2 2 √ fsd f4,0(x1) 2 √ Reservation stations Name Busy Op Vj Vk Qj Qk Load1 Yes Load Regs[x1] + 0 Load2 Yes Load Regs[x1]/C08 Add1 NoAdd2 NoAdd3 NoMult1 Yes MUL Regs[f2] Load1 Mult2 Yes MUL Regs[f2] Load2 Store1 Yes Store Regs[x1] Mult1 Store2 Yes Store Regs[x1]/C08 Mult2 Register status Field f0 f2 f4 f6 f8 f10 f12 … f30 Qi Load2 Mult2 Figure 3.14 Two active iterations loop instruction yet completed. Entries multiplier reser- vation stations indicate outstanding loads sources. store reservation stations indicate thatthe multiply destination source value store.206 ■Chapter Three Instruction-Level Parallelism Exploitationshares data memory address load. Similarly, store must wait unexecuted loads stores earlier program order share data memory address. consider method eliminate restrictioninSection 3.9 . detect hazards, processor must computed data memory address associated earlier memory operation. simple, necessar-ily optimal, way guarantee processor addresses performthe effective address calculations program order. (We really need keepthe relative order stores memory references; is, loads reordered freely.) Let’s consider situation load first. perform effective address calculation program order, load completed effective addresscalculation, check whether address conflict examining Afield active store buffers. load address matches address anyactive entries store buffer, load instruction sent load bufferuntil conflicting store completes. (Some implementations bypass valuedirectly load pending store, reducing delay RAW hazard.) Stores operate similarly, except processor must check conflicts load buffers store buffers conflicting stores cannot bereordered respect either load store. dynamically scheduled pipeline yield high performance, pro- vided branches predicted accurately —an issue addressed previous section. major drawback approach complexity Toma-sulo scheme, requires large amount hardware. particular, reservation station must contain associative buffer, must run high speed, well complex control logic. performance also limitedby single CDB. Although additional CDBs added, CDB mustinteract reservation station, nd associative ta g-matching hard- ware would duplicated station CDB. the1990s, high-end processors could take advantage dynamic scheduling(and extension speculation); ever, recently even processors designed PMDs using techniques, processors high-end desktops small servers hundreds buffers support dynamic scheduling. Tomasulo ’s scheme, two different techniques combined: renaming architectural registers larger set registers buffering sourceoperands register file. Source operand buffering resolves WAR hazardsthat arise operand available registers. see later, isalso possible eliminate WAR hazards renaming register togetherwith buffering result outstanding references earlier versionof register remain. approach used discuss hardware speculation. Tomasulo ’s scheme unused many years 360/91, widely adopted multiple-issue processors starting 1990s severalreasons:3.5 Dynamic Scheduling: Examples Algorithm ■2071.Although Tomasulo ’s algorithm designed caches, presence caches, inherently unpredictable delays, become one major motivations dynamic scheduling. Out-of-order execution allows proces-sors continue executing instructions awaiting completion cachemiss, thus hiding part cache miss penalty. 2.As processors became aggressive issue capability designers concerned performance difficult-to-schedule code (such asmost nonnumeric code), techniques register renaming, dynamic sched-uling, speculation became important. 3.It achieve high performance without requiring compiler target code specific pipeline structure, valuable property era shrink-wrappedmass market software. 3.6 Hardware-Based Speculation try exploit instruction-level parallelism, maintaining control depen-dences becomes increasing burden. Branch prediction reduces direct stallsattributable branches, processor executing multiple instructions perclock, predicting branches accurately may sufficient generate thedesired amount instruction-level parallelism. wide-issue processor may needto execute branch every clock cycle maintain maximum performance. Thusexploiting parallelism requires overcome limitation control dependence. Overcoming control dependence done speculating outcome branches executing program guesses correct. mech-anism represents subtle, important, extension branch prediction dynamic scheduling. particular, speculation, fetch, issue, exe- cute instructions, branch predictions always correct; dynamic scheduling fetches issues instructions. course, need mech-anisms handle situation peculation incorrect. Appendix H discusses variety mechanisms su pporting speculation compiler. section, explore hardware speculation, extends ideas dynamic scheduling. Hardware-based speculation combines three key ideas: (1) dynamic branch prediction choose instructions execute, (2) speculation allowthe execution instructions control dependences resolved(with ability undo effects incorrectly speculated sequence),and (3) dynamic scheduling deal scheduling different combina- tions basic blocks. (In comparison, dynamic scheduling without speculation partially overlaps basic blocks requires branch beresolved actually executing instructions successor basicblock.)208 ■Chapter Three Instruction-Level Parallelism ExploitationHardware-based speculation follows predicted flow data values choose execute instructions. method executing programs essen- tially data flow execution : Operations execute soon operands available. extend Tomasulo ’s algorithm support speculation, must separate bypassing results among instructions, needed execute instructionspeculatively, actual completion instruction. making sepa-ration, allow instruction execute bypass results otherinstructions, without allowing instruction perform updates cannot undone, know instruction longer speculative. Using bypassed value like performing speculative register read know whether instruction providing source register value pro-viding correct result instruction longer speculative. aninstruction longer speculative, allow update register file mem-ory; call additional step instruction execution sequence instruction commit . key idea behind implementing speculation allow instructions exe- cute order force commit order prevent irrevo- cable action (such updating state taking exception) instructioncommits. Therefore, add speculation, need separate processof completing execution instruction commit, instructions may finishexecution considerably ready commit. Adding commit phaseto instruction execution sequence requires additional set hardware buffersthat hold results instructions finished execution com-mitted. hardware buffer, call reorder buffer , also used pass results among instructions may speculated. reorder buffer (ROB) provides additional registers way reservation stations Tomasulo ’s algorithm extend register set. ROB holds result instruction time operation associated withthe instruction completes time instruction commits. ROB thereforeis source operands instructions, reservation stations provideoperands Tomasulo ’s algorithm. key difference Tomasulo ’s algo- rithm, instruction writes result, subsequently issued instructions find result register file. speculation, register file updated instruction commits (and know definitively instruction shouldexecute); thus, ROB supplies operands interval completion ofinstruction execution instruction commit. ROB similar storebuffer Tomasulo ’s algorithm, integrate function store buffer ROB simplicity. Figure 3.15 shows hardware structure processor including ROB. entry ROB contains four fields: instruction type, destination field, value field, ready field. instruction type field indicates whether instruction branch (and destination result), store (which mem-ory address destination), register operation (ALU operation load, hasregister destinations). destination field supplies register number (for loads3.6 Hardware-Based Speculation ■209and ALU operations) memory address (for stores) instruction result written. value field used hold value instruction resultuntil instruction commits. see example ROB entries shortly.Finally, ready field indicates instruction completed execution,and value ready. ROB subsumes store buffers. Stores still execute two steps, second step performed instruction commit. Although renaming functionof reservation stations replaced ROB, still need place buffer oper- ations (and operands) time issue time begin execution.From instruction unit FP registers Reservation stations FP adders FP multipliers3 212 1 Common data bus (CDB)Operation busOperand buses Address unit Load buffers Memory unitReorder buffer Data Reg # Store data Address Load dataStore addressFloating-point operationsLoad/storeoperationsInstruction queue Figure 3.15 basic structure FP unit using Tomasulo ’s algorithm extended handle speculation. Comparing Figure 3.10 onpage 198 , implemented Tomasulo ’s algorithm, see major change addition ROB elimination store buffer, whose function integrated ROB. mechanism extended allow multiple issues per clock making CDB wider allow mul-tiple completions per clock.210 ■Chapter Three Instruction-Level Parallelism ExploitationThis function still provided reservation stations. every instruction position ROB commits, tag result using ROB entry number rather using reservation station number. tagging requires ROBassigned instruction must tracked reservation station. Later sec-tion, explore alternative implementation uses extra registers renam-ing queue replaces ROB decide instructions commit. four steps involved instruction execution: 1.Issue—Get instruction instruction queue. Issue instruction empty reservation station empty slot ROB; send operands reservation station available either registersor ROB. Update control entries indicate buffers use. Thenumber ROB entry allocated result also sent reservationstation number used tag result placed theCDB. either reservations full ROB full, instructionissue stalled available entries. 2.Execute —If one operands yet available, monitor CDB waiting register computed. step checks RAW haz-ards. operands available reservation station, execute theoperation. Instructions may take multiple clock cycles stage, loads still require two steps stage. Stores need base register step, execution store point effective addresscalculation. 3.Write result —When result available, write CDB (with ROB tag sent instruction issued) CDB ROB, well asto reservation stations waiting result. Mark reservation station asavailable. Special actions required store instructions. value bestored available, written Value field ROB entry thestore. value stored available yet, CDB must monitoreduntil value broadcast, time Value field ROB entry store updated. simplicity assume occurs Write Result stage store; discuss relaxing requirement later. 4.Commit —This final stage completing instruction, result remains. (Some processors call commit phase “completion ”or “graduation. ”) three different sequences actions commit depend- ing whether committing instruction branch incorrect predic-tion, store, instruction (normal commit). normal commit caseoccurs instruction reaches head ROB result presentin buffer; point, processor updates register result andremoves instruction ROB. Committing store similar except memory updated rather result register. branch incorrect prediction reaches head ROB, indicates speculation waswrong. ROB flushed execution restarted correct successorof branch. branch correctly predicted, branch finished.3.6 Hardware-Based Speculation ■211Once instruction commits, entry ROB reclaimed, register memory destination updated, eliminating need ROB entry. ROB fills, simply stop issuing instructions entry made free. let ’s examine scheme would work example used Toma-sulo’s algorithm. Example Assume latencies floating-point functional units earlier exam- ples: add 2 clock cycles, multiply 6 clock cycles, divide 12 clock cycles.Using following code segment, one used generate Figure 3.12 , show status tables look like fmul.d ready go commit. fld f6,32(x2) fld f2,44(x3)fmul.d f0,f2,f4fsub.d f8,f2,f6fdiv.d f0,f0,f6 fadd.d f6,f8,f2 Answer Figure 3.16 shows result three tables. Notice although fsub.d instruction completed execution, commit fmul.d com- mits. reservation stations register status field contain basic infor-mation Tomasulo ’s algorithm (see page 200 description fields). differences reservation station numbers replaced withROB entry numbers Qj Qk fields, well register status fields,and added Dest field reservation stations. Dest field designates ROB entry destination result produced reservation station entry. preceding example illustrates key important difference pro- cessor speculation processor dynamic scheduling. Compare thecontent Figure 3.16 Figure 3.12 page 184, shows code sequence operation processor Tomasulo ’s algorithm. key difference that, preceding example, instruction earliest uncom-pleted instruction ( fmul.d preceding example) allowed complete. con- trast, Figure 3.12 thefsub.d andfadd.d instructions also completed. One implication difference processor ROB dynamically execute code maintaining precise interrupt model. exam- ple, fmul.d instruction caused interrupt, could simply wait reached head ROB take interrupt, flushing pendinginstructions ROB. instruction commit happens order, thisyields precise exception. contrast, example using Tomasulo ’s algorithm, fsub.d fadd.d instructions could complete fmul.d raised excep- tion. result registers f8andf6(destinations fsub.d and212 ■Chapter Three Instruction-Level Parallelism Exploitationfadd.d instructions) could overwritten, case interrupt would imprecise. users architects decided imprecise floating-point excep- tions acceptable high-performance processors program likely terminate; see Appendix J discussion topic. typesReorder buffer Entry Busy Instruction State Destination Value 1N fld f6,32(x2) Commit f6 Mem[32 + Regs[x2]] 2N fld f2,44(x3) Commit f2 Mem[44 + Regs[x3]] 3 Yes fmul.d f0,f2,f4 Write result f0 #2/C2Regs[f4] 4 Yes fsub.d f8,f2,f6 Write result f8 #2/C0#1 5 Yes fdiv.d f0,f0,f6 Execute f0 6 Yes fadd.d f6,f8,f2 Write result f6 #4 + #2 Reservation stations Name Busy Op Vj Vk Qj Qk Dest Load1 Load2 NoAdd1 NoAdd2 NoAdd3 NoMult1 fmul.d Mem[44 + Regs[x3]] Regs[f4] #3 Mult2 Yes fdiv.d Mem[32 + Regs[x2]] #3 #5 FP register status Field f0 f1 f2 f3 f4 f5 f6 f7 f8 f10 Reorder # 3 6 4 5 Busy Yes Yes … Yes Yes Figure 3.16 time fmul.d ready commit, two fld instructions committed, although several others completed execution. Thefmul.d head ROB, two fld instructions ease understanding. fsub.d andfadd.d instructions commit fmul.d instruc- tion commits, although results instructions available used sources instructions. Thefdiv.d execution, completed solely longer latency fmul.d . Value column indicates value held; format #X used refer value field ROB entry X. Reorderbuffers 1 2 actually completed shown informational purposes. show entries load/store queue, entries kept order.3.6 Hardware-Based Speculation ■213of exceptions, page faults, much difficult accommodate imprecise program must transparently resume execution han- dling exception. use ROB in-order instruction commit provides precise exceptions, addition supporting speculative execution, next example shows. Example Consider code example used earlier Tomasulo ’s algorithm shown Figure 3.14 execution: Loop: fld f0,0(x1) fmul.d f4,f0,f2fsd f4,0(x1)addi x1,x1, /C08 bne x1,x2,Loop //branches x1 6¼x2 Assume issued instructions loop twice. Let ’s also assume fld andfmul.d first iteration committed instructions completed execution. Normally, store would wait ROB effective address operand ( x1in example) value (f4in example). considering floating-point pipeline, assume effective address store computed time instructionis issued. Answer Figure 3.17 shows result two tables. neither register values memory values actually written instruction commits, processor easily undo speculative actionswhen branch found mispredicted. Suppose branch bne taken first time Figure 3.17 . instructions prior branch simply commit reaches head ROB; branch reaches headof buffer, buffer simply cleared processor begins fetchinginstructions path. practice, processors speculate try recover early possible branch mispredicted. recovery done clearing ROB allentries appear mispredicted branch, allowing beforethe branch ROB continue, restarting fetch correct branch successor. speculative processors, performance sensitive branch prediction impact misprediction higher. Thus theaspects handling branches —prediction accuracy, latency misprediction detection, misprediction recovery time —increase importance. Exceptions handled recognizing exception ready commit. speculated instruction raises exception, exception recordedin ROB. branch misprediction arises instruction havebeen executed, exception flushed along instruction the214 ■Chapter Three Instruction-Level Parallelism ExploitationROB cleared. instruction reaches head ROB, know longer speculative exception really taken. also try handle exceptions soon arise earlier branches resolved, butthis challenging case exceptions branch mispredict and,because occurs less frequently, critical. Figure 3.18 shows steps execution instruction, well conditions must satisfied proc eed step actions taken. show case mispredicted branches resolved commit.Although speculation seems like simple addition dynamic scheduling, comparison Figure 3.18 comparable figure Tomasulo ’sa l g - rithm Figure 3.13 shows speculation adds significant complications control. addition, remember branch mispredictions somewhatmore complex. important difference stores handled speculative processor versus Tomasulo ’s algorithm. Tomasulo ’s algorithm, store update memory reaches Write Result (which ensures effec-tive address calculated) data value store available. speculative processor, store updates memory reaches head ofReorder buffer Entry Busy Instruction State Destination Value 1N fld f0,0(x1) Commit f0 Mem[0 + Regs[x1]] 2N fmul.d f4,f0,f2 Commit f4 #1/C2Regs[f2] 3 Yes fsd f4,0(x1) Write result 0 + Regs[x1] #2 4 Yes addi x1,x1, /C08 Write result x1 Regs[x1] /C08 5 Yes bne x1,x2,Loop Write result 6 Yes fld f0,0(x1) Write result f0 Mem[#4] 7 Yes fmul.d f4,f0,f2 Write result f4 #6/C2Regs[f2] 8 Yes fsd f4,0(x1) Write result 0 + #4 #7 9 Yes addi x1,x1, /C08 Write result x1 #4/C08 10 Yes bne x1,x2,Loop Write result FP register status Field f0 f1 f2 f3 f4 F5 f6 F7 f8 Reorder # 6 Busy Yes Yes … Figure 3.17 fld andfmul.d instructions committed, although others completed execution. Thus reservation stations busy none shown. remaining instructions committed quickly possible. first two reorder buffers empty, shown completeness.3.6 Hardware-Based Speculation ■215Status Wait Action bookkeeping Issue instructions FP operationsand stores FP operations Loads StoresReservation station (r) ROB (b) availableif (RegisterStat[rs].Busy)/*in-flight instr. writes rs*/ {h RegisterStat[rs].Reorder; (ROB[h].Ready)/* Instr completed already */ {RS[r].Vj ROB[h].Value; RS[r].Qj 0;} else {RS[r].Qj h;} /* wait instruction */ } else {RS[r].Vj Regs[rs]; RS[r].Qj 0;}; RS[r].Busy yes; RS[r].Dest b; ROB[b].Instruction opcode; ROB[b].Dest rd;ROB[b].Ready no; (RegisterStat[rt].Busy) /*in-flight instr writes rt*/ {h RegisterStat[rt].Reorder; (ROB[h].Ready)/* Instr completed already */ {RS[r].Vk ROB[h].Value; RS[r].Qk 0;} else {RS[r].Qk h;} /* wait instruction */ } else {RS[r].Vk Regs[rt]; RS[r].Qk 0;}; RegisterStat[rd].Reorder b; RegisterStat[rd].Busy yes; ROB[b].Dest rd; RS[r].A imm; RegisterStat[rt].Reorder b; RegisterStat[rt].Busy yes; ROB[b].Dest rt; RS[r].A imm; Execute FP op(RS[r].Qj == 0) (RS[r].Qk == 0)Compute results —operands Vj Vk Load step 1 (RS[r].Qj == 0) storesearlier queueRS[r].A RS[r].Vj + RS[r].A; Load step 2 Load step 1 done stores earlier ROB different addressRead Mem[RS[r].A] Store (RS[r].Qj == 0) store queue headROB[h].Address RS[r].Vj + RS[r].A; Write result storeExecution done rand CDB availableb RS[r].Dest; RS[r].Busy no; 8x(if (RS[x].Qj==b) {RS[x].Vj result; RS[x].Qj 0}); 8x(if (RS[x].Qk==b) {RS[x].Vk result; RS[x].Qk 0}); ROB[b].Value result; ROB[b].Ready yes; Store Execution done rand (RS[r].Qk == 0)ROB[h].Value RS[r].Vk; Commit Instruction head ROB (entry h) andROB[h].ready == yesd ROB[h].Dest; /* register dest, exists */ (ROB[h].Instruction==Branch) {if (branch mispredicted) {clear ROB[h], RegisterStat; fetch branch dest;};} else (ROB[h].Instruction==Store) {Mem[ROB[h].Destination] ROB[h].Value;} else /* put result register destination */ {Regs[d] ROB[h].Value;}; ROB[h].Busy no; /* free ROB entry */ /* free dest register one else writing */if (RegisterStat[d].Reorder==h) {RegisterStat[d].Busy no;}; Figure 3.18 Steps algorithm required step. issuing instruction, rdis des- tination, rsandrtare sources, ris reservation station allocated, bis assigned ROB entry, head entry ROB. RSis reservation station data structure. value returned reservation station called theresult .Register-Stat register data structure, Regs represents actual registers, ROB reorder buffer data structure.216 ■Chapter Three Instruction-Level Parallelism Exploitationthe ROB. difference ensures memory updated instruc- tion longer speculative. Figure 3.18 one significant simplification stores, unneeded practice. Figure 3.18 requires stores wait Write Result stage register source operand whose value stored; value movedfrom Vk field store ’s reservation station Value field store’s ROB entry. reality, however, value stored need arrive store commits placed directly store ’s ROB entry sourcing instruction. accomplished hardware track source value stored available store ’s R n r ya n ds e r c h n gt h eR Bo ne v e r yi n r u c nc p l e nt ol kfor dependent stores. addition complicated, adding two effects: would need add field ROB, Figure 3.18 , already small font, would even longer! Although Figure 3.18 makes simplification, examples, allow store pass Write Result stage simply wait forthe value ready commits. Like Tomasulo ’s algorithm, must avoid hazards memory. WAW WAR hazards memory eliminated speculation theactual updating memory occurs order, store head theROB, earlier loads stores still pending. RAW hazards throughmemory maintained two restrictions: 1.Not allowing load initiate second step execution active ROB entry occupied store Destination field matches value field load 2.Maintaining program order computation effective address load respect earlier stores Together, two restrictions ensure load accesses memory location written earlier store cannot perform memory access untilthe store written data. speculative processors actually bypassthe value store load directly RAW hazard occurs.Another approach predict potential collisions using form value prediction; consider Section 3.9 . Although explanation speculative execution focused floating point, techniques easily extend integer registers functional units.Indeed, programs tend code branch behavior isless predictable, speculation may useful integer programs. Additionally,these techniques extended work multiple-issue processor allowingmultiple instructions issue commit every clock. fact, speculation probably interesting processors less ambitious techniques probably exploit sufficient ILP within basic blocks assisted acompiler.3.6 Hardware-Based Speculation ■2173.7 Exploiting ILP Using Multiple Issue Static Scheduling techniques preceding sections used eliminate data, control stalls, achieve ideal CPI one. improve performance further, wantto decrease CPI less one, CPI cannot reduced one weissue one instruction every clock cycle. goal multiple-issue processors , discussed next sections, allow multiple instructions issue clock cycle. Multiple-issue processorscome three major flavors: 1.Statically scheduled superscalar processors 2.VLIW (very long instruction word) processors 3.Dynamically scheduled superscalar processors two types superscalar processors issue varying numbers instructions per clock use in-order execution statically scheduled out-of-orderexecution dynamically scheduled. VLIW processors, contrast, issue fixed number instructions formatted either one large instruction fixed instruction packet parallelismamong instructions explicitly indicated instruction. VLIW processors areinherently statically scheduled compiler. Intel HP created IA-64 architecture, described Appendix H, also introduced name EPIC (explicitly parallel instruction computer) architectural style. Although statically scheduled supers calars issue varying rather fixed number instructions per clock, actually closer concept toVLIWs approaches rely compiler schedule code forthe processor. diminishin g advantages statically scheduled superscalar issue width grows, statically scheduled superscalars usedprimarily narrow issue widths, norma lly two instructions. Beyond width, designers choose implement either VLIW dynamically scheduled superscalar. similarities hardware requiredcompiler technology, focus VLIWs section, see themagain Chapter 7 . insights section easily extrapolated stat- ically scheduled superscalar. Figure 3.19 summarizes basic approach es multiple issue distinguishing character istics shows processors use approach. Basic VLIW Approach VLIWs use multiple, independent functional units. Rather attempting issue multiple, independent instructions units, VLIW packages multipleoperations one long instruction requires instructions the218 ■Chapter Three Instruction-Level Parallelism Exploitationissue packet satisfy constraints. fundamental differ- ence two approaches, assume multiple operations placedin one instruction, original VLIW approach. advantage VLIW increases maximum issue rate grows, focus wider issue processor. Indeed, simple two-issue processors, theoverhead superscalar probably minimal. Many designers would probablyargue four-issue processor manageable overhead, see later chapter, growth overhead major factor limiting wider issue processors. Let’s consider VLIW processor instructions contain five operations, including one integer operation (which could also branch), two floating-pointoperations, two memory references. instruction would set fieldsfor functional unit —perhaps 16 –24 bits per unit, yielding instruction length 80 120 bits. comparison, Intel Itanium 1 2 con-tain six operations per instruction packet (i.e., allow concurrent issue two three-instruction bundles, Appendix H describes). keep functional units busy, must enough parallelism code sequence fill available operation slots. parallelism uncovered byunrolling loops scheduling code within single larger loop body. Ifthe unrolling generates straight-line code, local scheduling techniques, operate single basic block, used. finding exploiting parallel-ism require scheduling code across branches, substantially complex globalCommon nameIssue structureHazard detection SchedulingDistinguishing characteristic Examples Superscalar (static)Dynamic Hardware Static In-order execution Mostly embedded space: MIPS ARM,including Cortex-A53 Superscalar (dynamic)Dynamic Hardware Dynamic out-of-order execution, speculationNone present Superscalar (speculative)Dynamic Hardware Dynamic speculationOut-of-order execution speculationIntel Core i3, i5, i7; AMD Phenom; IBM Power 7 VLIW/LIW Static Primarily softwareStatic hazards determined indicated compiler (often implicitly)Most examples signal processing, TI C6x EPIC Primarily staticPrimarily softwareMostly static hazards determined indicated explicitlyby compilerItanium Figure 3.19 five primary approaches use multiple-issue processors primary characteristics distinguish them. chapter focused hardware-intensive techniques, form superscalar. Appendix H focuses compiler-based approaches. EPIC approach, embodied IA-64 architecture, extends many concepts early VLIW approaches, providing blend static dynamicapproaches.3.7 Exploiting ILP Using Multiple Issue Static Scheduling ■219scheduling algorithm must used. Global scheduling algorithms complex structure, also must deal significantly com- plicated trade-offs optimization, moving code across branches isexpensive. Appendix H, discuss trace scheduling , one global scheduling techniques developed specifically VLIWs; also explore special hardwaresupport allows conditional branches eliminated, extending use-fulness local scheduling enhancing performance global scheduling. now, rely loop unrolling generate long, straight-line code sequences use local scheduling build VLIW instructions focus well processors operate. Example Suppose VLIW could issue two memory references, two FP oper- ations, one integer operation branch every clock cycle. Show unrolledversion loop x[i] = x[i] + (see page 158 RISC-V code) processor. Unroll many times necessary eliminate stalls. Answer Figure 3.20 shows code. loop unrolled make seven copies body, eliminates stalls (i.e., completely empty issue cycles), runsin 9 cycles unrolled scheduled loop. code yields running rate ofseven results 9 cycles, 1.29 cycles per result, nearly twice fast thetwo-issue superscalar Section 3.2 used unrolled scheduled code. Memory reference 1Memory reference 2 FP operation 1 FP operation 2Integer operation/branch fld f0,0(x1) fld f6,-8(x1) fld f10,-16(x1) fld f14,-24(x1)fld f18,-32(x1) fld f22,-40(x1) fadd.d f4,f0,f2 fadd.d f8,f6,f2fld f26,-48(x1) fadd.d f12,f0,f2 fadd.d f16,f14,f2 fadd.d f20,f18,f2 fadd.d f24,f22,f2 fsd f4,0(x1) fsd f8,-8(x1) fadd.d f28,f26,f24fsd f12,-16(x1) fsd f16,-24(x1) addi x1,x1,-56fsd f20,24(x1) fsd f24,16(x1)fsd f28,8(x1) bne x1,x2,Loop Figure 3.20 VLIW instructions occupy inner loop replace unrolled sequence. code takes 9 cycles assuming correct branch prediction. issue rate 23 operations 9 clock cycles, 2.5 operations per cycle. efficiency, percentage available slots contained operation, 60%. achieve issue rate requires larger number registers RISC-V would normally use loop. preceding VLIW code sequencerequires least eight FP registers, whereas code sequence base RISC-V processor use astwo FP registers many five unrolled scheduled.220 ■Chapter Three Instruction-Level Parallelism ExploitationFor original VLIW model, technical logistical problems made approach less efficient. technical problems increase incode size limitations lockstep operation. Two different elements com-bine increase code size substantially VLIW. First, generating enough oper- ations straight-line code fragment requires ambitiously unrolling loops (as earlier examples), thereby increasing code size. Second, whenever instructions arenot full, unused functional units translate wasted bits instruction encod-ing. Appendix H, examine software scheduling approaches, softwarepipelining,thatcanachievethebenefitsofunrollingwithoutasmuchcodeexpansion. combat code size increase, clever encodings sometimes used. example, may one large immediate field use functionalunit. Another technique compress instructions main memory expand read cache decoded. Appendix H, show techniques, well document significant code expansion seen IA-64. Early VLIWs operated lockstep; hazard-detection hardware all. structure dictated stall functional unit pipeline must cause theentire processor stall functional units kept synchro-nized. Although compiler might able schedule deterministicfunctional units prevent stalls, predicting data accesses would encountera cache stall scheduling difficult do. Thus caches needed blocking causing allthe functional units stall. issue rate num- ber memory references became large, synchronization restriction becameunacceptable. recent processors, functional units operate inde-pendently, compiler used avoid hazards issue time, hardwarechecks allow unsynchronized execution instructions issued. Binary code compatibility also major logistical problem general- purpose VLIWs run third-party software. strict VLIW approach,the code sequence makes use instruction set definition detailed pipeline structure, including functional units latencies. Thus differ- ent numbers functional units unit latencies require different versions thecode. requirement makes migrating successive implementations, orbetween implementations different issue widths, difficult asuperscalar design. course, obtaining improved performance new super-scalar design may require recompilation. Nonetheless, ability run old binaryfiles practical advantage superscalar approach. domain-specificarchitectures, examine Chapter 7 , problem serious applications written specifically architectural configuration. EPIC approach, IA-64 architecture primary example, provides solutions many problems encountered early general-purposeVLIW designs, including extensions aggressive software speculation andmethods overcome limitation hardware dependence preservingbinary compatibility. major challenge multiple-issue processors try exploit large amounts ILP. parallelism comes unrolling simple loops FP3.7 Exploiting ILP Using Multiple Issue Static Scheduling ■221programs, original loop probably could run efficiently vector processor (described next chapter). clear multiple-issue pro- cessor preferred vector processor applications; costs sim-ilar, vector processor typically speed faster. potentialadvantages multiple-issue processor versus vector processor former ’s ability extract parallelism less structured code easily cache allforms data. reasons, multiple-issue approaches become pri-mary method taking advantage instruction-level parallelism, vectorshave become primarily extension processors. 3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation far seen individual mechanisms dynamic scheduling, multiple issue, speculation work. section, put three together, yields amicroarchitecture quite similar modern microprocessors. simplicitywe consider issue rate two instructions per clock, concepts nodifferent modern processors issue three instructions per clock. Let’s assume want extend Tomasulo ’s algorithm support multiple- issue superscalar pipeline separate integer, load/store, floating-point units (both FP multiply FP add), initiate operation everyclock. want issue instructions reservation stations orderbecause could lead violation program semantics. gain fulladvantage dynamic scheduling, allow pipeline issue combi-nation two instructions clock, using scheduling hardware actuallyassign operations integer floating-point unit. interactionof integer floating-point instructions crucial, also extend Tomasulo ’s scheme deal integer floating-point functional units reg- isters, well incorporating speculative execution. Figure 3.21 shows, basic organization similar processor speculation one issueper clock, except issue completion logic must enhanced allowmultiple instructions processed per clock. Issuing multiple instructions per clock dynamically scheduled processor (with without speculation) complex simple reason mul-tiple instructions may depend one another. this, tables must updated instructions parallel; otherwise, tables incorrect dependence may lost. Two different approaches used issue multiple instructions per clock dynamically scheduled processor, rely observation thatthe key assigning reservation station updating pipeline control tables.One approach run step half clock cycle two instructions beprocessed one clock cycle; approach cannot easily extended handlefour instructions per clock, unfortunately.222 ■Chapter Three Instruction-Level Parallelism ExploitationA second alternative build logic necessary handle two instructions once, including possible dependences instructions.Modern superscalar processors issue four instructions per clock mayinclude approaches: pipeline widen issue logic. keyobservation cannot simply pipeline away problem. makinginstruction issues take multiple clocks new instructions issuing everyclock cycle, must able assign reservation station update thepipeline tables dependent instruction issuing next clock use updated information.From instruction unit Integer FP registers Reservation stations FP adders FP multipliers3 212 1 Common data bus (CDB)Operation busOperand buses Address unit Load buffers Memory unitData Reg #Reorder buffer Store data Address Load dataStore addressFloating-point operationsLoad/storeoperationsInstruction queue Integer unit2 1 Figure 3.21 basic organization multiple issue processor speculation. case, organization could allow FP multiply, FP add, integer, load/store issues simultaneously (assuming one issue per clock per functional unit). Note several datapaths must widened support multiple issues: CDB, operandbuses, and, critically, instruction issue logic, shown figure. last difficult problem, discuss text.3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation ■223This issue step one fundamental bottlenecks dynamically scheduled superscalars. illustrate complexity process, Figure 3.22 shows issue logic one case: issuing load followed dependent FP oper-ation. logic based Figure 3.18 page 197, represents one case. modern superscalar, every possible combination dependent instruc-tions allowed issue clock cycle must considered. Becausethe number possibilities climbs square number instructions thatcan issued clock, issue step likely bottleneck attempts gobeyond four instructions per clock. generalize detail Figure 3.22 describe basic strategy updating issue logic reservation tables dynamically scheduledsuperscalar nissues per clock follows: 1.Assign reservation station reorder buffer every instruction might issued next issue bundle. assignment done theinstruction types known simply preallocating reorder buffer entriessequentially instructions packet using navailable reorder buffer entries ensuring enough reservation stations available issuethe whole bundle, independent contains. limiting numberof instructions given class (say, one FP, one integer, one load, one store), necessary reservation stations preallocated. sufficient reser- vation stations available (such next instructions theprogram one instruction type), bundle broken, subsetof instructions, original program order, issued. remainder theinstructions bundle placed next bundle potential issue. 2.Analyze dependences among instructions issue bundle. 3.If instruction bundle depends earlier instruction bundle, use assigned reorder buffer number update reservation table forthe dependent instruction. Otherwise, use existing reservation table andreorder buffer information update reservation table entries issuinginstruction. course, makes preceding complicated done par- allel single clock cycle! back-end pipeline, must able complete commit mul- tiple instructions per clock. steps somewhat easier issue problemsbecause multiple instructions actually commit clock cycle musthave already dealt resolved dependences. see, designershave figured handle complexity: Intel i7, examine inSection 3.12 , uses essentially scheme described speculative mul- tiple issue, including large number reservation stations, reorder buffer, load store buffer also used handle nonblocking cache misses. performance viewpoint, show concepts fit together example.224 ■Chapter Three Instruction-Level Parallelism ExploitationAction bookkeeping Comments (RegisterStat[rs1].Busy)/*in-flight instr. writes rs*/ {h RegisterStat[rs1].Reorder; (ROB[h].Ready)/* Instr completed already */ {RS[x1].Vj ROB[h].Value; RS[x1].Qj 0;} else {RS[x1].Qj h;} /* wait instruction */ } else {RS[x1].Vj Regs[rs]; RS[x1].Qj 0;}; RS[x1].Busy yes; RS[x1].Dest b1; ROB[b1].Instruction Load; ROB[b1].Dest rd1; ROB[b1].Ready no; RS[r].A imm1; RegisterStat[rt1].Reorder b1; RegisterStat[rt1].Busy yes; ROB[b1].Dest rt1;Updating reservation tables load instruction, single source operand. thisis first instruction issue bundle, looks different would normally happen aload. RS[x2].Qj b1;} /* wait load instruction */ know first operand FP operation load, step simply updates thereservation station point theload. Notice dependence must analyzed fly ROB entries must allocatedduring issue step reservation tables correctly updated. (RegisterStat[rt2].Busy) /*in-flight instr writes rt*/ {h RegisterStat[rt2].Reorder; (ROB[h].Ready)/* Instr completed already */ {RS[x2].Vk ROB[h].Value; RS[x2].Qk 0;} else {RS[x2].Qk h;} /* wait instruction */ } else {RS[x2].Vk Regs[rt2]; RS[x2].Qk 0;}; RegisterStat[rd2].Reorder b2; RegisterStat[rd2].Busy yes; ROB[b2].Dest rd2;Because assumed second operand FPinstruction prior issue bundle, step looks like would single-issue case. course, ifthis instruction dependent onsomething issue bundle, tables would need updated using assigned reservationbuffer. RS[x2].Busy yes; RS[x2].Dest b2; ROB[b2].Instruction FP operation; ROB[b2].Dest rd2; ROB[b2].Ready no;This section simply updates tables FP operation independent load. course, instructions issuebundle depended FP operation (as could happen four-issue superscalar), updatesto reservation tables thoseinstructions would effected instruction. Figure 3.22 issue steps pair dependent instructions (called 1 2), instruction 1 FP load instruction 2 FP operation whose first operand result load instruction; x1andx2are assigned reservation stations instructions; b1andb2are assigned reorder buffer entries. issuing instructions, rd1 andrd2 destinations; rs1,rs2, rt2 sources (the load one source); x1andx2are reservation stations allocated; b1andb2 assigned ROB entries. RSis reservation station data structure. RegisterStat register data structure, Regs represents actual regis- ters, ROB reorder buffer data structure. Notice need assigned reorder buffer entries logic operate properly, recall updates happen single clock cycle parallel, sequentially.3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation ■225Example Consider execution following loop, increments element integer array, two-issue processor, without speculation withspeculation: Loop: ld x2,0(x1) //x2=array element addi x2,x2,1 //increment x2sd x2,0(x1) //store resultaddi x1,x1,8 //increment pointerbne x2,x3,Loop //branch last Assume separate integer functional units effective address cal- culation, ALU operations, branch condition evaluation. Create tablefor first three iterations loop processors. Assume two instructions type commit per clock. Answer Figures 3.23 3.24 show performance two-issue, dynamically scheduled processor, without speculation. case, branch Iteration number InstructionsIssues clock cycle numberExecutes clock cycle numberMemory access clock cycle numberWrite CDB clock cycle number Comment 1 ld x2,0(x1) 1 2 3 4 First issue 1 addi x2,x2,1 1 5 6 Wait ld 1 sd x2,0(x1) 2 3 7 Wait addi 1 addi x1,x1,8 2 3 4 Execute directly 1 bne x2,x3,Loop 3 7 Wait addi 2 ld x2,0(x1) 4 8 9 10 Wait bne 2 addi x2,x2,1 4 11 12 Wait ld 2 sd x2,0(x1) 5 9 13 Wait addi 2 addi x1,x1,8 5 8 9 Wait bne 2 bne x2,x3,Loop 6 13 Wait addi 3 ld x2,0(x1) 7 14 15 16 Wait bne 3 addi x2,x2,1 7 17 18 Wait ld 3 sd x2,0(x1) 8 15 19 Wait addi 3 addi x1,x1,8 8 14 15 Wait bne 3 bne x2,x3,Loop 9 19 Wait addi Figure 3.23 time issue, execution, writing result dual-issue version pipeline without spec- ulation. Note ldfollowing bne cannot start execution earlier must wait branch out- come determined. type program, data-dependent branches cannot resolved earlier, shows thestrength speculation. Separate functional units address calculation, ALU operations, branch-condition eval-uation allow multiple instructions execute cycle. Figure 3.24 shows example speculation.226 ■Chapter Three Instruction-Level Parallelism Exploitationcan critical performance limiter, spec ulation helps significantly. third branch speculative processor executes clock cycle 13, whereas exe- cutes clock cycle 19 nonspeculative pipeline. completionrate nonspeculative pipeline fa lling behind issue rate rapidly, nonspeculative pipeline stall wh en iterations issued. performance nonspeculative pro cessor could improved allowing load instructions complete effectiv e address calculation branch decided, unless speculative memory accesses allowed, improve-ment gain 1 clock per iteration. example clearly shows speculation advantageous data-dependent branches, otherwise would limit performance. advan-tage depends, however, accurate branch prediction. Incorrect speculation doesnot improve performance; fact, typically harms performance and, shallsee, dramatically lowers energy efficiency.Iteration number InstructionsIssues clock numberExecutes clock numberRead access clock numberWrite CDB clock numberCommits clock number Comment 1 ld x2,0(x1) 1 2 3 4 5 First issue 1 addi x2,x2,1 1 5 6 7 Wait ld 1 sd x2,0(x1) 2 3 7 Wait addi 1 addi x1,x1,8 2 3 4 8 Commit order 1 bne x2,x3,Loop 3 7 8 Wait addi 2 ld x2,0(x1) 4 5 6 7 9 execute delay 2 addi x2,x2,1 4 8 9 10 Wait ld 2 sd x2,0(x1) 5 6 10 Wait addi 2 addi x1,x1,8 5 6 7 11 Commit order 2 bne x2,x3,Loop 6 10 11 Wait addi 3 ld x2,0(x1) 7 8 9 10 12 Earliest possible 3 addi x2,x2,1 7 11 12 13 Wait ld 3 sd x2,0(x1) 8 9 13 Wait addi 3 addi x1,x1,8 8 9 10 14 Executes earlier 3 bne x2,x3,Loop 9 13 14 Wait addi Figure 3.24 time issue, execution, writing result dual-issue version pipeline specu- lation. Note ldfollowing bne start execution early speculative.3.8 Exploiting ILP Using Dynamic Scheduling, Multiple Issue, Speculation ■2273.9 Advanced Techniques Instruction Delivery Speculation high-performance pipeline, especially one multiple issues, predicting branches well enough; actually able deliver ahigh-bandwidth instruction stream. recent multiple-issue processors, hasmeant delivering 4 –8 instructions every clock cycle. look methods increasing instruction delivery bandwidth first. turn set key issuesin implementing advanced speculation techniques, including use registerrenaming versus reorder buffers, aggressiveness speculation, tech- nique called value prediction , attempts predict result computation could enhance ILP. Increasing Instruction Fetch Bandwidth multiple-issue processor require average number instructionsfetched every clock cycle least large average throughput.Of course, fetching instructions requires wide enough paths theinstruction cache, difficult aspect handling branches. section, look two methods dealing branches discuss modern processors integrate instruction prediction prefetchfunctions. Branch-Target Buffers reduce branch penalty simple five-stage pipeline, well dee- per pipelines, must know whether as-yet-undecoded instruction branch and, so, next program counter (PC) be. instruction abranch know next PC be, branch penaltyof zero. branch-prediction cache stores predicted address nextinstruction branch called branch-target buffer orbranch-target cache. Figure 3.25 shows branch-target buffer. branch-target buffer predicts next instruction address send decoding instruction, must know whether fetched instruction predicted taken branch. PC fetched instruction matches address prediction buffer, corresponding predictedPC used next PC. hardware branch-target buffer essentiallyidentical hardware cache. matching entry found branch-target buffer, fetching begins imme- diately predicted PC. Note unlike branch-prediction buffer, predic-tive entry must matched instruction predicted PC sent228 ■Chapter Three Instruction-Level Parallelism Exploitationout known whether instruction even branch. processor check whether entry matched PC, wrong PC would sent outfor instructions branches, resulting worse performance. need tostore predicted-taken branches branch-target buffer unta-ken branch simply fetch next sequential instruction, abranch. Figure 3.26 shows steps using branch-target buffer simple five- stage pipeline. see figure, branch delay abranch-prediction entry found buffer prediction correct. Other-wise, penalty least two clock cycles. Dealing mispre-dictions misses significant challenge typically haltinstruction fetch rewrite buffer entry. Thus want make pro-cess fast minimize penalty.Look upPredicted PC Number entries branch- target buffer No: instruction predicted taken branch; proceed normally= Yes: instruction taken branch predicted PC used next PCPC instruction fetch Figure 3.25 branch-target buffer. PC instruction fetched matched set instruc- tion addresses stored first column; represent addresses known branches. PC matches one entries, instruction fetched taken branch, second field, predicted PC, contains prediction next PC branch. Fetching begins immediately address. third field, optional, may used extra prediction state bits.3.9 Advanced Techniques Instruction Delivery Speculation ■229To evaluate well branch-target buffer works, first must determine penalties possible cases. Figure 3.27 contains information simple five-stage pipeline. Example Determine total branch penalty branch-target buffer assuming penalty cycles individual mispredictions Figure 3.27 . Make following assump- tions prediction accuracy hit rate: ■Prediction accuracy 90% (for instructions buffer). ■Hit rate buffer 90% (for branches predicted taken).IF ID EXSend PC memory branch-target buffer Entry found branch-target buffer?No Normal instruction executionYes Send predicted PC instruction taken branch? Taken branch? Enter branch instruction address next PC branch- target bufferMispredicted branch, kill fetched instruction; restart fetch target; delete entry target bufferBranch correctly predicted; continue execution stallsYes Yes Figure 3.26 steps involved handling instruction branch-target buffer.230 ■Chapter Three Instruction-Level Parallelism ExploitationAnswer compute penalty looking probability two events: branch predicted taken ends taken, branch taken foundin buffer. carry penalty two cycles. Probability branch buffer, actually takenðÞ ¼ Percent buffer hit rate /C2Percent incorrect predictions ¼90%/C210%¼0:09 Probability branch buffer, actually takenðÞ ¼ 10% Branch penalty¼0:09 + 0 :10ðÞ/C22 Branch penalty¼0:38 improvement dynamic branch prediction grow pipeline length, thus branch delay grows; addition, better predictors yielda greater performance advantage. Modern high-performance processors havebranch misprediction delays order 15 clock cycles; clearly, accurate pre-diction critical! One variation branch-target buffer store one target instruc- tions instead of, addition to, predicted target address . variation two potential advantages. First, allows branch-target buffer access take longer time successive instruction fetches, possibly allowing larger branch-target buffer. Second, buffering actual target instructions allowsus perform optimization called branch folding . Branch folding used obtain 0-cycle unconditional branches sometimes 0-cycle conditionalbranches. see, Cortex A-53 uses single-entry branch target cachethat stores predicted target instructions. Consider branch-target buffer buffers instructions predicted path accessed address unconditional branch. function unconditional branch change PC. Thus, branch-target buffer signals hit indicates branch Instruction buffer Prediction Actual branch Penalty cycles Yes Taken Taken 0 Yes Taken taken 2No Taken 2No taken 0 Figure 3.27 Penalties possible combinations whether branch buffer actually does, assuming store taken branches buffer. branch penalty everything correctly predicted branch found target buffer. branch correctly predicted, penalty equal 1 clock cycle update buffer correct information (during instruction cannot fetched) 1 clock cycle, needed, restart fetching thenext correct instruction branch. branch found taken, 2-cycle penalty encountered, time buffer updated.3.9 Advanced Techniques Instruction Delivery Speculation ■231unconditional, pipeline simply substitute ins truction branch-target buffer place instruction returned cache (which unconditional branch). processor issuing multipleinstructions per cycle, buffer need supply multiple instructionsto obtain maximum benefit. cases, may possible eliminate cost conditional branch. Specialized Branch Predictors: Predicting Procedure Returns, Indirect Jumps, Loop Branches try increase opportunity accuracy speculation, face chal- lenge predicting indirect jumps, is, jumps whose destination address variesat runtime. High-level language programs generate jumps indirectprocedure calls, select case statements, FORTRAN-computed gotos,although many indirect jumps simply come procedure returns. example,for SPEC95 benchmarks, procedure returns account 15% branches vast majority indirect jumps average. object- oriented languages C++ Java, procedure returns even fre-quent. Thus focusing procedure returns seems appropriate. Though procedure returns predicted branch-target buffer, accuracy prediction technique low procedure calledfrom multiple sites calls one site clustered time. Forexample, SPEC CPU95, aggressive branch predictor achieves accu-racy less 60% return branches. overcome problem, designs use small buffer return addresses operating stack. structure caches recent return addresses, pushing return address stack call popping one return. cache sufficientlylarge (i.e., large maximum call epth), predict returns per- fectly. Figure 3.28 shows performance return buffer 0 –16 elements number SPEC CPU95 benchmarks. use similarreturn predictor examine studies ILP Section 3.10 .B ht h e Intel Core processors AMD Phenom processors return address predictors. large server applications, indirect jumps also occur various function calls control transfers. Predicting targets branches simple aprocedure return. processors opted add specialized predictors allindirect jumps, whereas others rely branch target buffer. Although simple predictor like gshare good job predicting many conditional branches, tailored predicting loop branches, especiallyfor long running loops. observed earlier, Intel Core i7 920 used spe- cialized loop branch predictor. emergence tagged hybrid predictors, good predicting loop branches, recent designers optedto put resources larger tagged hybrid predictors rather separate loopbranch predictor.232 ■Chapter Three Instruction-Level Parallelism ExploitationIntegrated Instruction Fetch Units meet demands multiple-issue processors, many recent designers chosen implement integrated instruction fetch unit separate autonomousunit feeds instructions rest pipeline. Essentially, amounts torecognizing characterizing instruction fetch simple single pipe stage giventhe complexities multiple issue longer valid. Instead, recent designs used integrated instruction fetch unit inte- grates several functions: 1.Integrated branch prediction —The branch predictor becomes part instruction fetch unit constantly predicting branches, drivethe fetch pipeline.Misprediction frequency70% 60% 50%40%30%20% 010% 0% Return address buffer entries1248 1 6Go m88ksimcc1CompressXlispIjpegPerlVortex Figure 3.28 Prediction accuracy return address buffer operated stack number SPEC CPU95 benchmarks. accuracy fraction return addresses predicted correctly. buffer 0 entries implies standard branch prediction used. call depths typically large, exceptions, modest bufferworks well. data come Skadron et al. (1999) use fix-up mechanism prevent corruption cached return addresses.3.9 Advanced Techniques Instruction Delivery Speculation ■2332.Instruction prefetch —To deliver multiple instructions per clock, instruction fetch unit likely need fetch ahead. unit autonomously manages prefetching instructions (see Chapter 2 discussion techniques this), integrating branch prediction. 3.Instruction memory access buffering —When fetching multiple instructions per cycle, variety complexities encountered, including difficulty thatfetching multiple instructions may require accessing multiple cache lines. Theinstruction fetch unit encapsulates complexity, using prefetch try hidethe cost crossing cache blocks. instruction fetch unit also provides buff-ering, essentially acting on-demand unit provide instructions theissue stage needed quantity needed. Virtually high-end processors use separate instruction fetch unit con- nected rest pipeline buffer containing pending instructions. Speculation: Implementation Issues Extensions section, explore five issues involve design trade-offs chal-lenges multiple-issue speculation, starting use register renaming, approach sometimes used instead reorder buffer. discuss one important possible extension speculation control flow: idea calledvalue prediction. Speculation Support: Register Renaming Versus Reorder Buffers One alternative use reorder buffer (ROB) explicit use alarger physical set registers combined register renaming. approachbuilds concept renaming used Tomasulo ’s algorithm extends it. Tomasulo ’s algorithm, values architecturally visible registers (x0, . . . r31 f0, . . . f31) contained, point execution, combination register set reservation stations. addition speculation, register values may also temporarily reside ROB. either case, processor issue new instructions period time, existinginstructions commit, register values appear register file,which directly corresponds architecturally visible registers. register-renaming approach, extended set physical registers used hold architecturally visible registers well temporary values. Thusthe extended registers replace function ROB reservationstations; queue ensure instructions complete order needed. instruction issue, renaming process maps names architectural registers physical register numbers extended register set, allocating anew unused register destination. WAW WAR hazards avoidedby renaming destination register, speculation recovery handled234 ■Chapter Three Instruction-Level Parallelism Exploitationbecause physical register holding instruction destination become architectural register instruction commits. Therenaming map simple data structure supplies physical register number register currently corresponds specified architectural reg-ister, function performed register status table Tomasulo ’s algorithm. instruction commits, renaming table permanently updated indi-cate physical register corresponds actual architectural register, thuseffectively finalizing update processor state. Although ROB notnecessary register renaming, hardware must still track instructions queue-like structure update renaming table strict order. advantage renaming approach versus ROB approach instruction commit slightly simplified requires two simpleactions: (1) record mapping architectural register numberand physical register number longer speculative, (2) free physicalregisters used hold “older”value architectural register. design reservation stations, station freed instruction usingit completes execution, ROB entry freed corresponding instruction commits. register renaming, deallocating registers complex free physical register, must know longer corresponds anarchitectural register uses physical register outstand-ing. physical register corresponds architectural register architec-tural register rewritten, causing renaming table point elsewhere. is, ifno renaming entry points particular physical register, longer corre-sponds architectural register. may, however, still outstanding uses physical register. processor determine whether case examining source register specifiers instructions functional unitqueues. given physical register appear source des-ignated architectural register, may reclaimed reallocated. Alternatively, processor simply wait another instruction writes architectural register commits. point, furtheruses older value outstanding. Although method may tie physicalregister slightly longer necessary, easy implement used recent superscalars. One question may asking ever know registers architectural registers constantly changing? time whenthe program executing, matter. clearly cases, however,where another process, operating system, must able know exactlywhere contents certain architectural register reside. understand thiscapability provided, assume processor issue instructions someperiod time. Eventually instructions pipeline commit, map- ping architecturally visible registers physical registers become stable. point, subset physical registers contains archi-tecturally visible registers, value physical register associatedwith architectural register unneeded. easy move architectural3.9 Advanced Techniques Instruction Delivery Speculation ■235registers fixed subset physical registers values commu- nicated another process. register renaming reorder buffers continue used high-end pro- cessors, feature ability many 100 instructions(including loads stores waiting cache) flight. Whether renaming areorder buffer used, key complexity bottleneck dynamically scheduledsuperscalar remains issuing bundles instructions dependences within thebundle. particular, dependent instructions issue bundle must issued withthe assigned virtual registers instructions depend. strategy instruction issue register renaming similar used multiple issue reorder buffers (see page 205) deployed, follows: 1.The issue logic reserves enough physical registers entire issue bundle (say, four registers four-instruction bundle one register resultper instruction). 2.The issue logic determines dependences exist within bundle. dependence exist within bundle, register renaming structureis used determine physical register holds, hold, resulton instruction depends. dependence exists within bundle,the result earlier issue bundle, register renaming table correct register number. 3.If instruction depends instruction earlier bundle, pre-reserved physical register result placed used update information issuing instruction. Note reorder buffer case, issue logic must determine dependences within bundle update renaming tables single clock,and before, complexity larger number instructions perclock becomes chief limitation issue width. Challenge Issues per Clock Without speculation, little motivation try increase issue ratebeyond two, three, possibly four issues per clock resolving brancheswould limit average issue rate smaller number. processor includesaccurate branch prediction speculation, might conclude increasing theissue rate would attractive. Duplicating functional units straightforwardassuming silicon capacity power; real complications arise issue stepand correspondingly commit step. Commit step dual issuestep, requirements similar, let ’s take look happen six-issue processor using register renaming. Figure 3.29 shows six-instruction code sequence issue step must do. Remember must occur single clock cycle, processor tomaintain peak rate six issues per clock! dependences must detected,236 ■Chapter Three Instruction-Level Parallelism Exploitationthe physical registers must assigned, instructions must rewritten using physical register numbers: one clock. example makes clearwhy issue rates grown 3 –4 4 –8 past 20 years. com- plexity analysis required issue cycle grows square theissue width, new processor typically targeted higher clock ratethan last generation! register renaming reorder bufferapproaches duals, complexities arise independent implemen-tation scheme. Much Speculate One significant advantages speculation ability uncover events thatwould otherwise stall pipeline early, cache misses. potentialadvantage, however, comes significant potential disadvantage. Speculationis free. takes time energy, recovery incorrect speculation fur-ther reduces performance. addition, support higher instruction execution rate needed benefit speculation, processor must additional resources, take silicon area power. Finally, speculation causes anexceptional event occur, cache translation lookaside buffer(TLB) miss, potential significant performance loss increases, eventwould occurred without speculation. maintain advantage minimizing disadvantages, pipelines speculation allow low-cost exceptional events (such afirst-level cache miss) handled speculative mode. expensive exceptional event occurs, second-level cache miss TLB miss, processor waitInstr. # InstructionPhysical register assigned destinationInstruction physical register numbersRename map changes 1 add x1,x2,x3 p32 add p32,p2,p3 x1->p32 2 sub x1,x1,x2 p33 sub p33,p32,p2 x1->p33 3 add x2,x1,x2 p34 add p34,p33,x2 x2->p34 4 sub x1,x3,x2 p35 sub p35,p3,p34 x1->p35 5 add x1,x1,x2 p36 add p36,p35,p34 x1->p36 6 sub x1,x3,x1 p37 sub p37,p3,p36 x1->p37 Figure 3.29 example six instructions issued clock cycle happen. instructions shown program order: 1 –6; are, however, issued 1 clock cycle! notation piis used refer physical register; contents register point determined renaming map. sim-plicity, assume physical registers holding architectural registers x1, x2, x 3are initially p1, p2, andp3(they could physical register). instructions issued physical register numbers, shown column four. rename map, appears last column, shows map would change instruc-tions issued sequentially. difficulty renaming replacement architectural registers physical renaming registers happens effectively 1 cycle, sequentially. issue logic must find depen- dences “rewrite ”the instruction parallel.3.9 Advanced Techniques Instruction Delivery Speculation ■237until instruction causing event longer speculative handling event. Although may slightly degrade performance programs, avoids significant performance losses others, especially suffer ahigh frequency events coupled less-than-excellent branch prediction. 1990s potential downsides speculation less obvious. pro- cessors evolved, real costs speculation become apparent,and limitations wider issue speculation obvious. returnto issue shortly. Speculating Multiple Branches examples considered chapter, possible resolve abranch speculate another. Three different situations benefitfrom speculating multiple branches simultaneously: (1) high branch fre-quency, (2) significant clustering branches, (3) long delays functionalunits. first two cases, achieving high performance may mean multiple branches speculated, may even mean handling one branch per clock. Database programs less structured integer computations, oftenexhibit properties, making speculation multiple branches important. Like-wise, long delays functional units raise importance speculating onmultiple branches way avoid stalls longer pipeline delays. Speculating multiple branches slightly complicates process specula- tion recovery straightforward otherwise. 2017, processor yetcombined full speculation resolving multiple branches per cycle, unlikely costs would justified terms performance versus complexity power. Speculation Challenge Energy Efficiency impact speculation energy efficiency? first glance, one mightargue using speculation always decreases energy efficiency wheneverspeculation wrong, consumes excess energy two ways: 1.Instructions speculated whose results needed generate excess work processor, wasting energy. 2.Undoing speculation restoring state processor continue exe- cution appropriate address consumes additional energy would needed without speculation. Certainly, speculation raise power consumption, could control speculation, would possible measure cost (or least dynamic powercost). But, speculation lowers execution time increases theaverage power consumption, total energy consumed may less. Thus, understand impact speculation energy efficiency, need look often speculation leading unnecessary work. significant number unneeded instructions executed, unlikely speculation will238 ■Chapter Three Instruction-Level Parallelism Exploitationimprove running time comparable amount. Figure 3.30 shows fraction instructions executed misspeculation subset SPEC2000benchmarks using sophisticated branch predictor. see, fraction executed misspeculated instructions small scientific code significant (about 30% average) integer code. Thus unlikely speculation isenergy-efficient integer applications, end Dennard scaling makesimperfect speculation problematic. Designers could avoid speculation, tryto reduce misspeculation, think new approaches, spec-ulating branches known highly predictable. Address Aliasing Prediction Address aliasing prediction technique predicts whether two stores load store refer memory address. two references refer tothe address, may safely interchanged. Otherwise, must waituntil memory addresses accessed instructions known. weneed actually predict address values, whether values conflict,the prediction reasonably accurate small predictors. Address predictionrelies ability speculative processor recover misprediction; is, actual addresses predicted different (and thus alias) turn (and thus aliases), processor simply restarts sequence,0% 164.gzip175.vpr 176.gcc 181.mcf186.crafty 168.wupwise171.swim 172.mgrid 173.applu 177.mesaMisspeculation45% 40% 35%30% 25%20% 15%10% 5% Figure 3.30 fraction instructions executed result misspeculation typically much higher integer programs (the first five) versus FP programs (the last five).3.9 Advanced Techniques Instruction Delivery Speculation ■239just though mispredicted branch. Address value speculation used several processors already may become universal future. Address prediction simple restricted form value prediction, attempts predict value produced instruction. Value predic-tion could, highly accurate, eliminate data flow restrictions achievehigher rates ILP. Despite many researchers focusing value prediction thepast 15 years dozens papers, results never sufficiently attractiveto justify general value prediction real processors. 3.10 Cross-Cutting Issues Hardware Versus Software Speculation hardware-intensive approaches speculation chapter softwareapproaches Appendix H provide alternative approaches exploiting ILP. Someof trade-offs, limitations, approaches listed here: ■To speculate extensively, must able disambiguate memory references.This capability difficult compile time integer programs con-tain pointers. hardware-based scheme, dynamic runtime disambiguation memory addresses done using techniques saw earlier Tomasulo ’s algorithm. disambiguation allows us move loads past stores runtime.Support speculative memory references help overcome conserva-tism compiler, unless approaches used carefully, over-head recovery mechanisms may swamp advantages. ■Hardware-based speculation works better control flow unpredictableand hardware-based branch prediction superior software-basedbranch prediction done compile time. properties hold many integerprograms, misprediction rates dynamic predictors usually lessthan one-half static predictors. speculated instructions may slow computation prediction incorrect, differ- ence significant. One result difference even statically scheduledprocessors normally include dynamic branch predictors. ■Hardware-based speculation maintains completely precise exception modeleven speculated instructions. Recent software-based approaches haveadded special support allow well. ■Hardware-based speculation require compensation bookkeepingcode, needed ambitious software speculation mechanisms. ■Compiler-based approaches may benefit ability see thecode sequence, resulting better code scheduling purely hardware-driven approach. ■Hardware-based speculation dynamic scheduling require differ-ent code sequences achieve good performance different implementations240 ■Chapter Three Instruction-Level Parallelism Exploitationof architecture. Although advantage hardest quantify, may important one long run. Interestingly, one moti- vations IBM 360/91. hand, recent explicitly parallelarchitectures, IA-64, added flexibility reduces hardwaredependence inherent code sequence. major disadvantage supporting speculation hardware complex- ity additional hardware resources required. hardware cost must eval-uated complexity compiler software-based approach amount usefulness simplifications processor relies compiler. designers tried combine dynamic compiler-based approaches achieve best each. combination generate interest-ing obscure interactions. example, conditional moves combined withregister renaming, subtle side effect appears. conditional move annulledmust still copy value destination register renamed earlier inthe instruction pipeline. subtle interactions complicate design ver- ification process also reduce performance. Intel Itanium processor ambitious computer ever designed based software support ILP speculation. deliver thehopes designers, especially general-purpose, nonscientific code. Asdesigners ’ambitions exploiting ILP reduced light difficulties described page 244, architectures settled hardware-based mechanismswith issue rates three four instructions per clock. Speculative Execution Memory System Inherent processors support speculative execution conditional instruc-tions possibility generating invalid addresses would occur withoutspeculative execution. would incorrect behavior protectionexceptions taken, also benefits speculative execution would beswamped false exception overhead. Therefore memory system must identify speculatively executed instructions conditionally executed instructions suppress corresponding exception. similar reasoning, cannot allow instructions cause cache stall miss because, again, unnecessary stalls could overwhelm thebenefits speculation. Thus processors must matched nonblock-ing caches. reality, penalty miss goes DRAM large speculated misses handled next level on-chip cache (L2 L3). Figure 2.5 page 84 shows well-behaved scientific programs, compiler sustain multiple outstanding L2 misses cut L2 miss penalty effectively. Onceagain, work, memory system behind cache must match goalsof compiler number simultaneous memory accesses.3.10 Cross-Cutting Issues ■2413.11 Multithreading: Exploiting Thread-Level Parallelism Improve Uniprocessor Throughput topic cover section, multithreading, truly cross-cutting topic, relevance pipelining superscalars, graphics processingunits ( Chapter 4 ), multiprocessors ( Chapter 5 ). thread like process state current program counter, threads typically share theaddress space single process, allowing thread easily access data otherthreads within process. Multithreading technique whereby multiplethreads share processor without requiring intervening process switch. ability switch threads rapidly enables multithreading used hide pipeline memory latencies. next chapter, see multithreading provides advan- tages GPUs. Finally, Chapter 5 explore combination multithreading multiprocessing. topics closely interwoven multithreadingis primary technique exposing parallelism hardware. strictsense, multithreading uses thread-level parallelism, thus properly subjectofChapter 5 , role improving pipeline utilization GPUs moti- vates us introduce concept here. Although increasing performance using ILP great advantage reasonably transparent programmer, seen, ILP canbe quite limited difficult exploit applications. particular, withreasonable instruction issue rates, cache misses go memory off-chipcaches unlikely hidden available ILP. course, processoris stalled waiting cache miss, utilization functional units dropsdramatically. attempts cover long memory stalls ILP limited effectiveness, natural ask whether forms parallelism applica-tion could used hide memory delays. example, online transaction pro-cessing system natural parallelism among multiple queries updates thatare presented requests. course, many scientific applications contain naturalparallelism often model three-dimensional, parallel structure ofnature, structure exploited using separate threads. Even desktopapplications use modern Windows-based operating systems often mul- tiple active applications running, providing source parallelism. Multithreading allows multiple threads share functional units single processor overlapping fashion. contrast, general method exploitthread-level parallelism (TLP) multiprocessor multiple indepen- dent threads operating parallel. Multithreading, however, notduplicate entire processor multiprocessor does. Instead, multithreadingshares processor core among set threads, duplicating privatestate, registers program counter. see Chapter 5 , many recent processors incorporate multiple processor cores single chip provide multithreading within core.242 ■Chapter Three Instruction-Level Parallelism ExploitationDuplicating per-thread state processor core means creating separate register file separate PC thread. memory shared virtual memory mechanisms, already support multiprogram-ming. addition, hardware must support ability change differentthread relatively quickly; particular, thread switch much effi-cient process switch, typically requires hundreds thousands pro-cessor cycles. course, multithreading hardware achieve performanceimprovements, program must contain multiple threads (we sometimes say thatthe application multithreaded) could execute concurrent fashion. threads identified either compiler (typically language paral- lelism constructs) programmer. three main hardware approaches multithreading: fine-grained, coarse-grained, simultaneous. Fine-grained multithreading switches threads clock cycle, causing execution instructions multiplethreads interleaved. interleaving often done round-robin fashion,skipping threads stalled time. One key advantage fine-grained multithreading hide throughput losses arise short long stalls instructions threads executed one thread stalls, even stall cycles. primarydisadvantage fine-grained multithreading slows executionof individual thread thread ready execute without stallswill delayed instructions threads. trades increase multi-threaded throughput loss performance (as measured latency) asingle thread. SPARC T1 T5 processors (originally made Sun, made Oracle Fujitsu) use fine-grained multithreading. processors tar- geted multithreaded workloads transaction processing web services.The T1 supported 8 cores per processor 4 threads per core, T5supports 16 cores 128 threads per core. Later versions (T2 –T5) also supported 4–8 processors. NVIDIA GPUs, look next chapter, also make use fine-grained multithreading. Coarse-grained multithreading invented alternative fine-grained multithreading. Coarse-grained multithreading switches threads costly stalls, level two three cache misses. instructions threads issued thread encounters costly stall, coarse-grainedmultithreading relieves need thread-switching essentially free ismuch less likely slow execution one thread. Coarse-grained multithreading suffers, however, major drawback: limited ability overcome throughput losses, especially shorter stalls.This limitation arises pipeline start-up costs coarse-grained multi-threading. processor coarse-grained multithreading issues instruc- tions single thread, stall occurs, pipeline see bubble new thread begins executing. start-up overhead, coarse-grainedmultithreading much useful reducing penalty high-coststalls, pipeline refill negligible compared stall time. Several research3.11 Multithreading: Exploiting Thread-Level Parallelism Improve Uniprocessor Throughput ■243projects explored coarse-grained multithreading, major current proces- sors use technique. common implementation multithreading called simultaneous multithreading (SMT). Simultaneous multithreading variation fine-grained multithreading arises naturally fine-grained multithreading implemen-ted top multiple-issue, dynamically scheduled processor. otherforms multithreading, SMT uses thread-level parallelism hide long-latencyevents processor, thereby increasing usage functional units. Thekey insight SMT register renaming dynamic scheduling allow mul- tiple instructions independent threads executed without regard dependences among them; resolution dependences handled bythe dynamic scheduling capability. Figure 3.31 conceptually illustrates differences processor ’s ability exploit resources superscalar following processor configurations: ■A superscalar multithreading support ■A superscalar coarse-grained multithreading Superscalar Coarse MT Fine MT SMTTimeExecution slots Figure 3.31 four different approaches use functional unit execution slots superscalar processor. horizontal dimension represents instruction execution capability clock cycle. vertical dimension rep-resents sequence clock cycles. empty (white) box indicates corresponding execution slot unused clock cycle. shades gray black correspond four different threads multithreading processors. Black also used indicate occupied issue slots case superscalar without multithreading support.The Sun T1 T2 (aka Niagara) processors fine-grained, multithreaded processors, Intel Core i7 IBM Power7 processors use SMT. T2 8 threads, Power7 4, Intel i7 2. existing SMTs, instructions issue one thread time. difference SMT subsequent decision execute aninstruction decoupled could execute operations coming several different instructions sameclock cycle.244 ■Chapter Three Instruction-Level Parallelism Exploitation■A superscalar fine-grained multithreading ■A superscalar simultaneous multithreading superscalar without multithreading support, use issue slots lim- ited lack ILP, including ILP hide memory latency. length L2 L3 cache misses, much processor left idle. coarse-grained multithreaded superscalar, long stalls partially hidden switching another thread uses resources processor. Thisswitching reduces number completely idle clock cycles. coarse-grainedmultithreaded processor, however, thread switching occurs astall. new thread start-up period, likely fullyidle cycles remaining. fine-grained case, interleaving threads eliminate fully empty slots. addition, issuing thread changed every clock cycle, longerlatency operations hidden. instruction issue execution con-nected, thread issue many instructions ready. narrow issuewidth, problem (a cycle either occupied not), fine-grained multithreading works perfectly single issue processor, SMT wouldmake sense. Indeed, Sun T2, two issues per clock, arefrom different threads. eliminates need implement complex dynamic scheduling approach relies instead hiding latency threads. one implements fine-grained threading top multiple-issue, dynami- cally schedule processor, result SMT. existing SMT implementations,all issues come one thread, although instructions different threads caninitiate execution cycle, using dynamic scheduling hardware todetermine instructions ready. Although Figure 3.31 greatly simplifies real operation processors, illustrate potential performanceadvantages multithreading general SMT wider issue, dynamically scheduled processors. Simultaneous multithreading uses insight dynamically scheduled processor already many hardware mechanisms needed support themechanism, including large virtual register set. Multithreading built ontop out-of-order processor adding per-thread renaming table, keepingseparate PCs, providing capability instructions multiple threadsto commit. Effectiveness Simultaneous Multithreading Superscalar Processors key question is, much performance gained implementing SMT? question explored 2000 –2001, researchers assumed dynamic superscalars would get much wider next five years, supporting six eightissues per clock speculative dynamic scheduling, many simultaneous loadsand stores, large primary caches, four eight contexts simultaneous issue3.11 Multithreading: Exploiting Thread-Level Parallelism Improve Uniprocessor Throughput ■245and retirement multiple contexts. processor gotten close combination. result, simulation research results showed gains multiprogrammed workloads two times unrealistic. practice, existing implemen-tations SMT offer two four contexts fetching issue onlyone, four issues per clock. result gain SMT alsomore modest. Esmaeilzadeh et al. (2011) extensive insightful set measurements examined performance energy benefits using SMT single i7 920 core running set multithreaded applications. Intel i7 920 supported SMT two threads per core, recent i7 6700. changes betweenthe i7 920 6700 relatively small unlikely significantly changethe results shown section. benchmarks used consist collection parallel scientific applications set multithreaded Java programs DaCapo SPEC Java suite, assummarized Figure 3.32 .Figure 3.31 shows ratios performance energy efficiency benchmarks run one core i7 920 SMT turned on. (We plot energy efficiency ratio, inverse energy consumption, that, like speedup, higher ratio better.) harmonic mean speedup Java benchmarks 1.28, despite two benchmarks see small gains. two benchmarks, pjbb2005 trade-beans, multithreaded, limited parallelism. included becausethey typical multithreaded benchmark might run SMT pro-cessor hope extracting performance, find limitedamounts. PARSEC benchmarks obtain somewhat better speedups full set Java benchmarks (harmonic mean 1.31). tradebeans pjbb2005 omitted, Java workload would actually significantly better speedup (1.39)than PARSEC benchmarks. (See discussion implication using har-monic mean summarize results caption Figure 3.33 .) Energy consumption determined combination speedup increase power consumption. Java benchmarks, average, SMT delivers thesame energy efficiency non-SMT (average 1.0), brought bythe two poor performing benchmarks; without pjbb2005 tradebeans, aver- age energy efficiency Java benchmarks 1.06, almost good PARSEC benchmarks. PARSEC benchmarks, SMT reduces energy by1/C0(1/1.08)¼7%. energy-reducing performance enhancements dif- ficult find. course, static power associated SMT paid cases, thus results probably slightly overstate energy gains. results clearly show SMT extensive support aggressive speculative processor improve performance energy-efficient fashion. In2011, balance offering multiple simpler cores fewer sophis- ticated cores shifted favor cores, core typically three- four-issue superscalar SMT supporting two four threads. Indeed,Esmaeilzadeh et al. (2011) show energy improvements SMT even larger Intel i5 (a processor similar i7, smaller caches a246 ■Chapter Three Instruction-Level Parallelism Exploitationlower clock rate) Intel Atom (an 80x86 processor originally designed netbook PMD market, focused low-end PCs, described inSection 3.13 ). 3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 section, explore design two multiple issue processors: ARM Cortex-A53 core, used basis several tablets cell phones, andthe Intel Core i7 6700, high-end, dynamically scheduled, speculative processorintended high-end desktops server applications. begin simplerprocessor.blackscholes Prices portfolio options Black-Scholes PDE bodytrack Tracks markerless human bodycanneal Minimizes routing cost chip cache-aware simulated annealingfacesim Simulates motions human face visualization purposesferret Search engine finds set images similar query imagefluidanimate Simulates physics fluid motion animation SPH algorithmraytrace Uses physical simulation visualizationstreamcluster Computes approximation optimal clustering data pointsswaptions Prices portfolio swap options Heath –Jarrow –Morton framework vips Applies series transformations imagex264 MPG-4 AVC/H.264 video encodereclipse Integrated development environmentlusearch Text search toolsunflow Photo-realistic rendering systemtomcat Tomcat servlet containertradebeans Tradebeans Daytrader benchmarkxalan XSLT processor transforming XML documentspjbb2005 Version SPEC JBB2005 (but fixed problem size rather time) Figure 3.32 parallel benchmarks used examine multithreading, well Chapter 5 examine multiprocessing i7. top half chart consists PARSEC benchmarks collected Bienia et al. (2008) . PARSEC benchmarks meant indicative compute-intensive, parallel appli- cations would appropriate multicore processors. lower half consists multithreaded Java benchmarks DaCapo collection (see Blackburn et al., 2006 ) pjbb2005 SPEC. benchmarks contain parallelism; Java benchmarks DaCapo SPEC Java workloads usemultiple threads little true parallelism and, hence, used here. See Esmaeilzadeh et al. (2011) additional information characteristics benchmarks, relative measurements Chapter 5 .3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■247The ARM Cortex-A53 A53 dual-issue, statically scheduled superscalar dynamic issue detec- tion, allows processor issue two instructions per clock. Figure 3.34 shows basic pipeline structure pipeline. nonbranch, integer instruc-tions, eight stages: F1, F2, D1, D2, D3/ISS, EX1, EX2, WB, asdescribed caption. pipeline order, instruction initiate exe-cution results available proceeding instructions haveinitiated. Thus, next two instructions dependent, proceed theappropriate execution pipeline, serialized get thebeginning pipeline. scoreboard-based issue logic indicates result first instruction available, second instruction issue.2.00 1.75 1.50 1.25 1.00 0.75i7 SMT performance energy efficiency ratio Eclipse Sunflow TomcatXalan BlackscholesBodytrackCannealFerret FluidanimateRaytrace StreamclusterSwaptions×264Energy efficiency Speedup Lusearch TradebeansPjbb2005 FacesimVips Figure 3.33 speedup using multithreading one core i7 processor averages 1.28 Java benchmarks 1.31 PARSEC benchmarks (using unweighted harmonic mean, implies work- load total time spent executing benchmark single-threaded base set same). energy efficiency averages 0.99 1.07, respectively (using harmonic mean). Recall anything 1.0 forenergy efficiency indicates feature reduces execution time increases average power. Two Java benchmarks experience little speedup significant negative energy efficiency issue. Turbo Boost cases. data collected analyzed Esmaeilzadeh et al. (2011) using Oracle (Sun) HotSpot build 16.3-b01 Java 1.6.0 Virtual Machine gcc v4.4.1 native compiler.248 ■Chapter Three Instruction-Level Parallelism ExploitationThe four cycles instruction fetch include address generation unit pro- duces next PC either incrementing last PC one four predictors: 1.A single-entry branch target cache containing two instruction cache fetches (the next two instructions following branch, assuming prediction correct).This target cache checked first fetch cycle, hits; next two instructions supplied target cache. case hit correct prediction, branch executed delay cycles. 2.A 3072-entry hybrid predictor, used instructions hit branch target cache, operating F3. Branches handled predic- tor incur 2-cycle delay. 3.A 256-entry indirect branch predictor operates F4; branches pre- dicted predictor incur three-cycle delay predicted correctly. 4.An 8-deep return stack, operating F4 incurring three-cycle delay.Floating Point executeInteger execute load-store Instruction fetch & predict Instruction DecodeAGU + TLB Instruction cacheF1 F3 F2 Writeback D1Iss Ex1 Ex2 Wr ALU pipe 0 ALU pipe 1 MAC pipe Divide pipe Load pipe MUL/DIV/SQRT pipe ALU pipeHybrid predictor Indirect predictor Early decode13-Entry instruction queueF4 D2Main decodeLate decode D3IssueInteger register file Store pipe NEON register file F1 F2 F3 F4 F5 Figure 3.34 basic structure A53 integer pipeline 8 stages: F1 F2 fetch instruction, D1 D2 basic decoding, D3 decodes complex instructions overlapped first stage ofthe execution pipeline (ISS). ISS, Ex1, EX2, WB stages complete integer pipeline. Branches use four different predictors, depending type. floating-point execution pipeline 5 cycles deep, addition 5 cycles needed fetch decode, yielding 10 stages total.3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■249Branch decisions made ALU pipe 0, resulting branch misprediction penalty 8 cycles. Figure 3.35 shows misprediction rate SPECint2006. amount work wasted depends misprediction rate issue rate sustained time mispredicted branch followed.AsFigure 3.36 shows, wasted work generally follows misprediction rate, though may larger occasionally shorter. Performance A53 Pipeline A53 ideal CPI 0.5 dual-issue structure. Pipeline stalls arise three sources: 1.Functional hazards, occur two adjacent instructions selected issue simultaneously use functional pipeline. A53 stat-ically scheduled, compiler try avoid conflicts. suchhmmer0% h264ref lib quantum perlbench jeng bzip2 gobmk xalancbmk gcc astar omnet pp mcf2%4%6%8%10%12%Branch misprediction rate14%16%18%20%22% Figure 3.35 Misprediction rate A53 branch predictor SPECint2006.250 ■Chapter Three Instruction-Level Parallelism Exploitationinstructions appear sequentially, serialized beginning execution pipeline, first instruction begin execution. 2.Data hazards, detected early pipeline may stall either instructions (if first cannot issue, second always stalled) secondof pair. Again, compiler try prevent stalls possible. 3.Control hazards, arise branches mispredicted. TLB misses cache misses also cause stalls. instruction side, TLB cache miss causes delay filling instruction queue, likely leading downstream stall pipeline. course, depends whether L1miss, might largely hidden instruction queue full timeof miss, L2 miss, takes considerably longer. data side, acache TLB miss cause pipeline stall load store thathmmer0% h264ref lib quantum perlbench jeng bzip2 gobmk xalancbmk gcc astar omnet pp mcf2%4%6%8%10%12%% Wasted work14%16%18%20%22% Figure 3.36 Wasted work due branch misprediction A53. A53 in-order machine, amount wasted work depends variety factors, including data dependences cache misses, cause stall.3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■251caused miss cannot proceed pipeline. subsequent instruc- tions thus stalled. Figure 3.37 shows CPI estimated contribu- tions various sources. A53 uses shallow pipeline reasonably aggressive branch predictor, leading modest pipeline losses, allowing processor achieve high clock rates modest power consumption. comparison i7, A53 con-sumes approximately 1/200 power quad core processor! Intel Core i7 i7 uses aggressive out-of-order speculative microarchitecture deeppipelines goal achieving high instruction throughput combining mul-tiple issue high clock rates. first i7 processor introduced 2008; i76700 sixth generation. basic structure i7 similar, successivehmmer h264ref lib quantum perlbench jeng bzip2 gobmk xalancbmk gcc astar omnet pp mcfMemory hierarchy stalls Pipeline stalls Ideal CPI 0.97 1.04 1.071.17 1.221.33 1.391.75 1.762.143.378.56 012345678910 Figure 3.37 estimated composition CPI ARM A53 shows pipeline stalls significant outweighed cache misses poorest performing programs. estimate obtained using L1 L2 miss rates penalties compute L1 L2 generated stalls per instruction. subtracted theCPI measured detailed simulator obtain pipeline stalls. Pipeline stalls include three hazards.252 ■Chapter Three Instruction-Level Parallelism Exploitationgenerations enhanced performance changing cache strategies (e.g., aggressiveness prefetching), increasing memory bandwidth, expanding num- ber instructions flight, enhancing branch prediction, improving graphicssupport. early i7 microarchitectures used reservations stations reorderbuffers out-of-order, speculative pipeline. Later microarchitectures, includ-ing i7 6700, use register renaming, reservations stations acting func-tional unit queues reorder buffer simply tracking control information. Figure 3.38 shows overall structure i7 pipeline. examine pipeline starting instruction fetch continuing instruction commit, following steps labeled figure. 1.Instruction fetch —The processor uses sophisticated multilevel branch predic- tor achieve balance speed prediction accuracy. also areturn address stack speed function return. Mispredictions cause penaltyof 17 cycles. Using predicted address, instruction fetch unit fetches16 bytes instruction cache. 2.The 16 bytes placed predecode instruction buffer —In step, pro- cess called macro-op fusion executed. Macro-op fusion takes instruction combinations compare followed branch fuses sin-gle operation, issue dispatch one instruction. certain spe- cial cases fused, since must know use first result second instruction (i.e., compare branch). study Intel Corearchitecture (which many fewer buffers), Bird et al. (2007) discovered macrofusion significant impact performance integer programsresulting 8% –10% average increase performance programs showing negative results. little impact FP programs; fact, abouthalf SPECFP benchmarks showed negative results macro-op fusion.The predecode stage also breaks 16 bytes individual x86 instructions. predecode nontrivial length x86 instruction 1 17 bytes predecoder must look number bytesbefore knows instruction length. Individual x86 instructions (includingsome fused instructions) placed instruction queue. 3.Micro-op decode —Individual x86 instructions translated micro-ops. Micro-ops simple RISC-V-like instructions executed directlyby pipeline; approach translating x86 instruction set simpleoperations easily pipelined introduced Pentium Pro in1997 used since. Three decoders handle x86 instructionsthat translate directly one micro-op. x86 instructions complex semantics, microcode engine used produce micro-op sequence; produce four micro-ops every cycle con-tinues necessary micro-op sequence generated. micro-ops placed according order x86 instructions 64-entrymicro-op buffer.3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■2534.The micro-op buffer preforms loop stream detection microfusion —If small sequence instructions (less 64 instructions) com-prises loop, loop stream detector find loop directly issuethe micro-ops buffer, eliminating need instruction fetch instruction decode stages activated. Microfusion combines256 KB unified l2 cache (4-way)Register alias table allocator 224-Entry reorder buffer 97-Entry reservation stationRetirement register file ALU shift SSE shuffle ALU 128-bit FMUL FDIV128-bit FMUL FDIV128-bit FMUL FDIVSSE shuffle ALUSSE shuffle ALUMemory order buffer (72 load; 56 stores pending)ALU shiftALU shiftLoad addressStore addressStore data Store & loadMicro -codeComplex macro-op decoder 64-Entry micro-op loop stream detect bufferSimple macro-op decoderSimple macro-op decoderSimple macro-op decoder128-Entry inst. TLB (8-way) Instruction fetch hardwareInstruction queue32 KB Inst. cache (8-way associative) Pre-decode +macro-op fusion, fetch buffer 64-Entry data TLB (4-way associative)32-KB dual-ported data cache (8-way associative)1536-Entry unified L2 TLB (12-way) 8 MB core shared inclusive L3 cache (16-way associative)Uncore arbiter (handles scheduling clock/power state differences) Figure 3.38 Intel Core i7 pipeline structure shown memory system components. total pipeline depth 14 stages, branch mispredictions typically costing 17 cycles, extra cycles likely due time reset branch predictor. six independent functional units begin execution ready micro-opin cycle. four micro-ops processed register renaming table.254 ■Chapter Three Instruction-Level Parallelism Exploitationinstruction pairs ALU operation dependent store issues single reservation station (where still issue indepen- dently), thus increasing usage buffer. Micro-op fusion producessmaller gains integer programs larger ones FP, results varywidely. different results integer FP programs macro andmicro fusion, probably arise patterns recognized fused frequency occurrence integer versus FP programs. i7, whichhas much larger number reorder buffer entries, benefits bothtechniques likely smaller. 5.Perform basic instruction issue —Looking register location register tables, renaming register s, allocating reorder buffer entry, fetching results registers reorder buffer sending micro-ops reservation stations. four micro-ops pro- cessed every clock cycle; assigned next available reorder bufferentries. 6.The i7 uses centralized reservation station shared six functional units. six micro-ops may dispatched functional units every clock cycle. 7.Micro-ops executed individual function units, results sent back waiting reservation station well register retirementunit, update register state known instruc-tion longer speculative. entry corresponding instruction thereorder buffer marked complete. 8.When one instructions head reorder buffer marked complete, pending writes register retirement unit exe-cuted, instructions removed reorder buffer. addition changes branch predictor, major changes first generation i7 (the 920, Nehalem microarchitecture) sixth generation(i7 6700, Skylake microarchitecture) sizes various buffers, renam-ing registers, resources allow many outstanding instructions.Figure 3.39 summarizes differences. Performance i7 earlier sections, examined performance i7 ’s branch predictor also performance SMT. section, look single-thread pipelineperformance. presence aggressive speculation well non-blocking caches, difficult accurately attribute gap idealized per-formance actual performance. extensive queues buffers 6700 reduce probability stalls lack reservation stations, renaming registers, reorder buffers significantly. Indeed, even earlier i7 920 withnotably fewer buffers, 3% loads delayed reser-vation station available.3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■255Thus losses come either branch mispredicts cache misses. cost branch mispredict 17 cycles, whereas cost L1 miss about10 cycles. L2 miss slightly three times costly L1 miss, andan L3 miss costs 13 times L1 miss costs (130 –135 cycles). Although processor attempt find alternative instructions execute L2 L3 misses, likely buffers fill miss completes,causing processor stop issuing instructions. Figure 3.40 shows overall CPI 19 SPECCPUint2006 benchmarks compared CPI earlier i7 920. average CPI i7 6700 is0.71, whereas almost 1.5 times better i7 920, 1.06. differencederives improved branch prediction reduction demand miss rates(see Figure 2.26 page 135). understand 6700 achieves significant improvement CPI, let ’s look benchmarks achieve largest improvement. Figure 3.41 shows five benchmarks CPI ratio 920 least 1.5 times higherthan 6700. Interestingly, three benchmarks show significantimprovement branch prediction accuracy (1.5 more); however, threebenchmarks (HMMER, LIBQUANTUM, SJENG) show equal slightlyhigher L1 demand miss rates i7 6700. misses likely arise becausethe aggressive prefetching replacing cache blocks actually used. type behavior reminds designers challenges maximizing performance complex speculative multiple issue processors: rarely significant perfor-mance achieved tuning one part microarchitecture!Resource i7 920 (Nehalem) i7 6700 (Skylake) Micro-op queue (per thread) 28 64 Reservation stations 36 97Integer registers NA 180FP registers NA 168Outstanding load buffer 48 72Outstanding store buffer 32 56Reorder buffer 128 256 Figure 3.39 buffers queues first generation i7 latest generation i7. Nehalem used reservation station plus reorder buffer organization. later microarchitectures, reservation stations serve scheduling resources, register renaming used rather reorder buffer; reorder buffer Skylake microarchitecture serves buffer control information. choicesof size various buffers renaming registers, appearing sometimes arbi- trary, likely based extensive simulation.256 ■Chapter Three Instruction-Level Parallelism Exploitationhmmer00.711.06 0.540.74 0.681.23i7 6700 i7 920 h264ref libquantum perlbench jeng bzip2 gobmk xalancbmk gcc astar omnetpp mcf0.511.522.53Cycles per instruction 0.811.02 0.420.59 0.470.61 0.410.44 0.380.650.600.92 0.760.771.442.67 1.372.12 Figure 3.40 CPI SPECCPUint2006 benchmarks i7 6700 i7 920. data section collected Professor Lu Peng PhD student Qun Liu, Louisiana State University. Benchmark CPI ratio (920/6700)Branch mispredict ratio (920/6700)L1 demand miss ratio (920/6700) ASTAR 1.51 1.53 2.14 GCC 1.82 2.54 1.82 MCF 1.85 1.27 1.71 OMNETPP 1.55 1.48 1.96PERLBENCH 1.70 2.11 1.78 Figure 3.41 analysis five integer benchmarks largest performance gap i7 6700 920. five benchmarks show improvement branch prediction rate reduction L1 demand miss rate.3.12 Putting Together: Intel Core i7 6700 ARM Cortex-A53 ■2573.13 Fallacies Pitfalls fallacies focus difficulty predicting performance energy effi- ciency extrapolating single measures clock rate CPI. alsoshow different architectural approaches radically different behaviorsfor different benchmarks. Fallacy easy predict performance energy efficiency two different versions instruction set architecture, hold technologyconstant. Intel offers processor low-end Netbook PMD space called Atom 230, implements 64-bit 32-bit versions x86 architecture.The Atom statically scheduled, 2-issue superscalar, quite similar micro-architecture ARM A8, single-core predecessor A53. Interestingly,both Atom 230 Core i7 920 fabricated 45 nmIntel technology. Figure 3.42 summarizes Intel Core i7 920, ARM Cortex- A8, Intel Atom 230. similarities provide rare opportunity todirectly compare two radically different microarchitectures instruction set holding constant underlying fabrication technology. comparison, need say little Atom 230. Atom processors implement x86 architecture using standard tech- nique translating x86 instructions RISC-like instructions (as every x86implementation since mid-1990s done). Atom uses slightly pow-erful microoperation, allows arithmetic operation paired loador store; capability added later i7s use macrofusion. Thismeans average typical instruction mix, 4% instructions require one microoperation. microoperations executed 16-deep pipeline capable issuing two instructions per clock, order, asin ARM A8. dual-integer ALUs, separate pipelines FP addand FP operations, two memory operation pipelines, supporting moregeneral dual execution ARM A8 still limited in-order issuecapability. Atom 230 32 KiB instruction cache 24 KiB data cache,both backed shared 512 KiB L2 die. (The Atom 230 also supportsmultithreading two threads, consider single-threaded comparisons.) might expect two processors, implemented technol- ogy instruction set, would exhibit predictable behavior, termsof relative performance energy consumption, meaning power perfor-mance would scale close linearly. examine hypothesis using three sets ofbenchmarks. first set group Java single-threaded benchmarks comefrom DaCapo benchmarks SPEC JVM98 benchmarks (seeEsmaeilzadeh et al. (2011) discussion benchmarks measurements). second third sets benchmarks SPEC CPU2006 consist integer FP benchmarks, respectively.258 ■Chapter Three Instruction-Level Parallelism ExploitationAs see Figure 3.43 , i7 significantly outperforms Atom. benchmarks least four times faster i7, two SPECFP benchmarks areover 10 times faster, one SPECINT benchmark runs eight times faster!Because ratio clock rates two processors 1.6, advan-tage comes much lower CPI i7 920: factor 2.8 Java bench-marks, factor 3.1 SPECINT benchmarks, factor 4.3 theSPECFP benchmarks. average power consumption i7 920 43 W, average power consumption Atom 4.2 W, one-tenth power! Combining performance power leads energy efficiencyadvantage Atom typically 1.5 times better often2 times better! comparison two processors using underlyingtechnology makes clear performance advantages aggressiveArea Specific characteristicIntel i7 920 ARM A8 Intel Atom 230 Four cores, FP One core, FP One core, FP Physical chip propertiesClock rate 2.66 GHz 1 GHz 1.66 GHz Thermal design power 130 W 2 W 4 WPackage 1366-pin BGA 522-pin BGA 437-pin BGA Memory systemTLBTwo-level Two-level four-way set associativeAll four-way set associative One-level fully associative 128 I/64 16 I/16 512 L2 32 I/32 64 L2 CachesThree-level 32 KiB/32 KiB Two-level Two-level 256 KiB 16/16 32/32 KiB 32/24 KiB2–8 MiB 128 KiB –1 MiB 512 KiB Peak memory BW 17 GB/s 12 GB/sec 8 GB/s Pipeline structurePeak issue rate4 ops/clock fusion 2 ops/clock 2 ops/clock Pipe line schedulingSpeculating orderIn-order dynamic issueIn-order dynamic issue Branch prediction Two-levelTwo-level 512-entry BTB4 K global history8-entry return stack Two-level Figure 3.42 overview four-core Intel i7 920, example typical ARM A8 processor chip (with 256 MiB L2, 32 KiB L1s, floating point), Intel ARM 230, clearly showing difference designphilosophy processor intended PMD (in case ARM) netbook space (in case Atom) processor use servers high-end desktops. Remember, i7 includes four cores, higher performance one-core A8 Atom. processors implemented comparable45 nm technology.3.13 Fallacies Pitfalls ■25911 10 9 8 7 6 5 4 3 2 1 0 Fop Luindex antlr Bloat _201_compress _202_jess _209_db _213_javac _212_mpegaudio _228_jack 400.perlbench 401.bzip2 403.gcc 429.mcf 445.gobmk 456.hmmer 458.sjeng 462.libquantum 464.h264ref 470.omnetpp 473.astar 483.xalancbmk 416.gamess 433.milc 434.zeusmp 435.gromacs 436.cactus ADM 437.leslie3d 444.namd 447.dealll 450.soplex 453.povray 454.calculix 459.gams FDTD 465.tonto 470.ibm 482.sphinx3i7 920 Atom 230 performance energy ratioEnergy efficiency Speedup Figure 3.43 relative performance energy efficiency set single-threaded benchmarks shows i7 920 4 10 times faster Atom 230 2 times lesspower-efficient average! Performance shown columns i7 relative Atom, execution time (i7)/execution time (Atom). Energy shown line Energy (Atom)/Energy (i7). i7 never beats Atom energy efficiency, although essentially good four benchmarks, three floating point. data shown werecollected Esmaeilzadeh et al. (2011) . SPEC benchmarks compiled optimization using standard Intel compiler, Java benchmarks use Sun (Oracle) Hotspot Java VM. one core active i7, rest deep power saving mode. Turbo Boost used i7, increases performance advantage slightly decreases relative energy efficiency.260 ■Chapter Three Instruction-Level Parallelism Exploitationsuperscalar dynamic scheduling speculation come significant dis- advantage energy efficiency. Fallacy Processors lower CPIs always faster. Fallacy Processors faster clock rates always faster. key product CPI clock rate determines performance. high clock rate obtained deeply pipelining processor must maintain lowCPI get full benefit faster clock. Similarly, simple processor high clock rate low CPI may slower. saw previous fallacy, performance energy efficiency diverge significantly among processors designed different environments evenwhen ISA. fact, large differences performance showup even within family processors company designed forhigh-end applications. Figure 3.44 shows integer FP performance two different implementations x86 architecture Intel, well ver-sion Itanium architecture, also Intel. Pentium 4 aggressively pipelined processor ever built Intel. used pipeline 20 stages, seven functional units, cachedmicro-ops rather x86 instructions. relatively inferior performance, giventhe aggressive implementation, clear indication attempt exploitmore ILP (there could easily 50 instructions flight) failed. Pentium ’s power consumption similar i7, although transistor count lower,as primary caches half large i7, included 2 MiB sec-ondary cache tertiary cache. Intel Itanium VLIW-style architecture, despite potential decrease complexity compared dynamically scheduled superscalars, neverattained competitive clock rates mainline x86 processors (although itappears achieve overall CPI similar i7). examining theseresults, reader aware use different implementation technol-ogies, giving i7 advantage terms transistor speed hence clock ratefor equivalently pipelined processor. Nonetheless, wide variation ProcessorImplementation technologyClock rate PowerSPECCInt2006 baseSPECCFP2006 baseline Intel Pentium 4 670 90 nm 3.8 GHz 115 W 11.5 12.2 Intel Itanium 2 90 nm 1.66 GHz 104 W approx. 70 W one core14.5 17.3 Intel i7 920 45 nm 3.3 GHz 130 W total approx. 80 W onecore35.5 38.4 Figure 3.44 Three different Intel processors vary widely. Although Itanium processor two cores i7 four, one core used benchmarks; Power column thermal design power estimates onlyone core active multicore cases.3.13 Fallacies Pitfalls ■261performance —more three times Pentium i7 —is astonishing. next pitfall explains significant amount advantage comes from. Pitfall Sometimes bigger dumber better. Much attention early 2000s went building aggressive processors exploit ILP, including Pentium 4 architecture, used deepest pipelineever seen microprocessor, Intel Itanium, highest peakissue rate per clock ever seen. quickly became clear main lim-itation exploiting ILP often turned memory system. Although spec- ulative out-of-order pipelines fairly good hiding significant fraction 10- 15-cycle miss penalties first-level miss, could little hidethe penalties second-level miss that, going main memory, likelyto 50 –100 clock cycles. result designs never came close achieving peak instruction throughput despite large transistor counts extremely sophisti-cated clever techniques. Section 3.15 discusses dilemma turning away aggressive ILP schemes multicore, another change exemplified pitfall. Instead trying hide even memory latency ILP, designers simply used transistors build much larger caches.Both Itanium 2 i7 use three-level caches compared two-levelcache Pentium 4, third-level caches 9 8 MiB compared tothe 2 MiB second-level cache Pentium 4. Needless say, building largercaches lot easier designing 20+-stage Pentium 4 pipeline, basedon data Figure 3.44 , seems effective. Pitfall sometimes smarter better bigger dumber. One surprising results past decade branch prediction. emergence hybrid tagged predictors shown sophisticated pre-dictor outperform simple gshare predictor number bits (see Figure 3.8 page 171). One reason result surprising tagged predictor actually stores fewer predictions, also consumes bits storetags, whereas gshare large array predictions. Nonetheless, appearsthat advantage gained misusing prediction one branch anotherbranch justifies allocation bits tags versus predictions. Pitfall Believing large amounts ILP available, right techniques. attempts exploit large amounts ILP failed several reasons, one important ones, designers initially accept, hard find large amounts ILP conventionally structured pro-grams, even speculation. famous study David Wall 1993 (seeWall, 1993 ) analyzed amount ILP available variety idealistic conditions. summarize results processor configuration roughly five ten times capability advanced processors 2017. Wall ’s study extensively documented variety different approaches,262 ■Chapter Three Instruction-Level Parallelism Exploitationand reader interested challenge exploiting ILP read complete study. aggressive processor consider following characteristics: 1.Up 64 instruction issues dispatches per clock noissue restric- tions, 8 times total issue width widest processor 2016 (theIBM Power8) 32 times many loads stores allowedper clock! discussed, serious complexity powerproblems large issue rates. 2.A tournament predictor 1K entries 16-entry function return pre- dictor. predictor comparable best predictors 2016; pre-dictor primary bottleneck. Mispredictions handled one cycle, limit ability speculate. 3.Perfect disambiguation memory references done dynamically —this ambitious perhaps attainable small window sizes. 4.Register renaming 64 additional integer 64 additional FP registers, somewhat less aggressive processor 2011. study assumes latency one cycle instructions (versus 15or processors like i7 Power8), effective number renameregisters five times larger either processors. Figure 3.45 shows result configuration vary window size. configuration complex expensive existing implementations,especially terms number instruction issues. Nonetheless, gives useful upper limit future implementations might yield. data figures likely optimistic another reason. issue restrictions amongthe 64 instructions: example, may memory references. one wouldeven contemplate capability processor near future. addition,remember interpreting results, cache misses non-unit latencies werenot taken account, effects significant impacts. startling observation Figure 3.45 preceding realistic processor constraints, effect window size integer programs severe FP programs. result points key difference two types programs. availability loop-level parallelism two FPprograms means amount ILP exploited higher, forinteger programs factors —such branch prediction, register renaming, less parallelism, start —are important limitations. observation critical market growth past decade —transaction pro- cessing, web servers, like —depended integer performance, rather floating point. Wall’s study believed some, 10 years later, reality sunk in, combination modest performance increases significanthardware resources major energy issues coming incorrect speculationforced change direction. return discussion concludingremarks.3.13 Fallacies Pitfalls ■2633.14 Concluding Remarks: ’s Ahead? 2000 began focus exploiting instruction-level parallelism peak. first five years new century, became clear ILP approach likely peaked new approaches would needed. 2005 Intel theother major processor manufacturers revamped approach focus mul-ticore. Higher performance would achieved thread-level parallelismrather instruction-level parallelism, responsibility using the101010 89 1515 13 810 111212 11 9 14223552 47 912151617 56 45 34 22 14gcc espresso li fppppBenchmarks doduc tomcatv 0 10 20 Instruction issues per cycle30 40 50 60Infinite 256 128 64 32Window size Figure 3.45 amount parallelism available versus window size variety integer floating- point programs 64 arbitrary instruction issues per clock. Although fewer renaming registers window size, fact operations 1-cycle latency number renaming registers equals issue width allows processor exploit parallelism within entire window.264 ■Chapter Three Instruction-Level Parallelism Exploitationprocessor efficiently would largely shift hardware software programmer. change significant change processor architec- ture since early days pipelining instruction-level parallelism 25+years earlier. period, designers began explore use data-level parallelism another approach obtaining performance. SIMD extensionsenabled desktop server microprocessors achieve moderate performanceincreases graphics similar functions. importantly, graphics proces-sing units (GPUs) pursued aggressive use SIMD, achieving significant perfor- mance advantages applications extensive data-level parallelism. scientific applications, approaches represent viable alternative moregeneral, less efficient, thread-level parallelism exploited multicores. Thenext chapter explores developments use data-level parallelism. Many researchers predicted major retrenchment use ILP, predicting two issue superscalar processors larger numbers cores would thefuture. advantages, however, slightly higher issue rates ability ofspeculative dynamic scheduling deal unpredictable events, level- one cache misses, led moderate ILP (typically 4 issues/clock) primary building block multicore designs. addition SMT effective-ness (both performance energy efficiency) cemented position ofthe moderate issue, out-of-order, speculative approaches. Indeed, even theembedded market, newest processors (e.g., ARM Cortex-A9 Cortex-A73) introduced dynamic scheduling, speculation, wider issues rates. highly unlikely future processors try increase width issue significantly. simply inefficient viewpoint silicon utilization power efficiency. Consider data Figure 3.46 show five proces- sors IBM Power series. decade, modestimprovement ILP support Power processors, dominant portion Power4 Power5 Power6 Power7 Power8 Introduced 2001 2004 2007 2010 2014 Initial clock rate (GHz) 1.3 1.9 4.7 3.6 3.3 GHzTransistor count (M) 174 276 790 1200 4200Issues per clock 55768 Functional units per core 8 8 9 12 16SMT threads per core 02248 Cores/chip 2228 1 2 SMT threads per core 02248 Total on-chip cache (MiB) 1.5 2 4.1 32.3 103.0 Figure 3.46 Characteristics five generations IBM Power processors. except Power6, static in-order, dynamically scheduled; processors support two load/store pipelines. Power6 functional units Power5 except decimal unit. Power7 Power8 use embedded DRAM L3 cache. Power9 described briefly; expands caches supports off-chip HBM.3.14 Concluding Remarks: ’s Ahead? ■265of increase transistor count (a factor 10 Power4 Power8) went increasing caches number cores per die. Even expansion SMT support seems focus increase ILPthroughput: ILP structure Power4 Power8 went 5 issues 8,from 8 functional units 16 (but increasing original 2 load/storeunits), whereas SMT support went nonexistent 8 threads/processor.A similar trend observed across six generations i7 processors, wherealmost additional silicon gone supporting cores. next twochapters focus approaches exploit data-level thread-level parallelism. 3.15 Historical Perspective References Section M.5 (available online) features discussion development ofpipelining instruction-level parallelism. provide numerous referencesfor reading exploration topics. Section M.5 covers bothChapter 3 Appendix H. Case Studies Exercises Jason D. Bakos Robert P. Colwell Case Study: Exploring Impact Microarchitectural Techniques Concepts illustrated case study ■Basic Instruction Scheduling, Reordering, Dispatch ■Multiple Issue Hazards ■Register Renaming ■Out-of-Order Speculative Execution ■Where Spend Out-of-Order Resources tasked designing new processor microarchitecture try- ing determine best allocate hardware resources. hard-ware software techniques learned Chapter 3 apply? Youhave list latencies functional units memory, well somerepresentative code. boss somewhat vague performancerequirements new design, know experience that, else beingequal, faster usually better. Start basics. Figure 3.47 provides sequence instructions list latencies. 3.1 [10]<3.1, 3.2 >What baseline performance (in cycles, per loop iteration) code sequence Figure 3.47 new instruction ’s execution could be266 ■Chapter Three Instruction-Level Parallelism Exploitationinitiated previous instruction ’s execution completed? Ignore front-end fetch decode. Assume execution stall lack next instruction, one instruction/cycle issued. Assume branch istaken, one-cycle branch delay slot. 3.2 [10]<3.1, 3.2 >Think latency numbers really mean —they indicate number cycles given function requires produce output. overall pipe-line stalls latency cycles functional unit, least guar-anteed pair back-to-back instructions (a “producer ”followed “consumer ”) execute correctly. instruction pairs pro- ducer/consumer relationship. Sometimes two adjacent instructions nothingto other. many cycles would loop body code sequence inFigure 3.47 require pipeline detected true data dependences stalled those, rather blindly stalling everything one functionalunit busy? Show code <stall >inserted necessary accom- modate stated latencies. ( Hint: instruction latency +2 requires two<stall >cycles inserted code sequence.) Think thisLatencies beyond single cycle Memory LD +3 Memory SD +1Integer ADD, SUB +0Branches +1fadd.d +2fmul.d +4fdiv.d +10 Loop: fld f2,0(Rx) I0: fmul.d f2,f0,f2I1: fdiv.d f8,f2,f0I2: fld f4,0(Ry)I3: fadd.d f4,f0,f4I4: fadd.d f10,f8,f2I5: fsd f4,0(Ry)I6: addi Rx,Rx,8I7: addi Ry,Ry,8I8: sub x20,x4,RxI9: bnz x20,Loop Figure 3.47 Code latencies Exercises 3.1 3.6.Case Studies Exercises Jason D. Bakos Robert P. Colwell ■267way: one-cycle instruction latency 1+0, meaning zero extra wait states. So, latency 1+1 implies one stall cycle; latency 1+ NhasNextra stall cycles. 3.3 [15]<3.1, 3.2 >Consider multiple-issue design. Suppose two execu- tion pipelines, capable beginning execution one instruction per cycle, enough fetch/decode bandwidth front end stall execution. Assume results immediately forwarded one execution unitto another, itself. assume reason execution pipelinewould stall observe true data dependency. many cycles theloop require? 3.4 [10]<3.1, 3.2 >In multiple-issue design Exercise 3.3, may rec- ognized subtle issues. Even though two pipelines exact sameinstruction repertoire, neither identical interchangeable, thereis implicit ordering must reflect ordering instruc-tions original program. instruction N+1 begins execution Execution Pipe 1 time instruction Nbegins Pipe 0, N+1 happens require shorter execution latency N, N+1 complete N (even though program ordering would implied otherwise). Recite leasttwo reasons could hazardous require special considerationsin microarchitecture. Give example two instructions code inFigure 3.47 demonstrate hazard. 3.5 [20]<3.1, 3.2 >Reorder instructions improve performance code Figure 3.47 . Assume two-pipe machine Exercise 3.3 out-of- order completion issues Exercise 3.4 dealt successfully. Justworry observing true data dependences functional unit latencies now. many cycles reordered code take? 3.6 [10/10/10] <3.1, 3.2 >Every cycle initiate new operation pipe lost opportunity, sense hardware living potential. a.[10]<3.1, 3.2 >In reordered code Exercise 3.5, fraction cycles, counting pipes, wasted (did initiate new op)? b.[10]<3.1, 3.2 >Loop unrolling one standard compiler technique finding parallelism code, order minimize lost opportunities perfor- mance. Hand-unroll two iterations loop reordered code Exer-cise 3.5. c.[10]<3.1, 3.2 >What speedup obtain? (For exercise, color N+1 iteration ’s instructions green distinguish Nth iteration ’s instructions; actually unrolling loop, would reassignregisters prevent collisions iterations.) 3.7 [15]<3.4>Computers spend time loops, multiple loop itera- tions great places speculatively find work keep CPU resources busy.Nothing ever easy, though; compiler emitted one copy loop ’s code, even though multiple iterations handling distinct data, will268 ■Chapter Three Instruction-Level Parallelism Exploitationappear use registers. keep multiple iterations ’register usages colliding, rename registers. Figure 3.48 shows example code would like hardware rename. compiler could simply unrolled theloop used different registers avoid conflicts, expect hardwareto unroll loop, must also register renaming. How? Assume hard-ware pool temporary registers (call Tregisters, assume 64 them, T0through T63) substitute registers designated compiler. rename hardware indexed src (source) register designation, value table Tregister last destination targeted register. (Think table values producers, src reg- isters consumers; ’t much matter producer puts result long consumers find it.) Consider code sequence Figure 3.48 . Every time see destination register code, substitute next availableT, beginning T9. update src registers accordingly, true data dependences maintained. Show resulting code. ( Hint: seeFigure 3.49 .) 3.8 [20]<3.4>Exercise 3.7 explored simple register renaming: hardware register renamer sees source register, substitutes destination Tregister last instruction targeted source register. rename table seesa destination register, substitutes next available Tfor it, superscalar designs need handle multiple instructions per clock cycle every stage machine, including register renaming. SimpleScalar processor would therefore look src register mappings instruction allocate e w dest mapping per clock cycle. Superscalar processors must able well, must also ensure dest -to-src relationships two concurrent instructions handled correctly. Consider samplecode sequence Figure 3.50 . Assume would like simultaneously Loop: fld f2,0(Rx) I0: fmul.d f5,f0,f2I1: fdiv.d f8,f0,f2I2: fld f4,0(Ry)I3: fadd.d f6,f0,f4I4: fadd.d f10,f8,f2I5: sd f4,0(Ry) Figure 3.48 Sample code register renaming practice. I0: fld T9,0(Rx) I1: fmul.d T10,F0,T9... Figure 3.49 Expected output register renaming.Case Studies Exercises Jason D. Bakos Robert P. Colwell ■269rename first two instructions. assume next two available Treg- isters used known beginning clock cycle twoinstructions renamed. Conceptually, want first instruc- tion rename table lookups update table per destination ’sT register. second instruction would exactly thing, anyinter-instruction dependency would thereby handled correctly. ’s enough time write Tregister designation renaming table look second instruction, clock cycle. Thatregister substitution must instead done live (in parallel register renametable update). Figure 3.51 shows circuit diagram, using multiplexers com- parators, accomplish necessary on-the-fly register renaming. task show cycle-by-cycle state rename table every instruction code shown Figure 3.50 . Assume table starts every entry equal index ( T0=0 ;T1=1 ,…)(Figure 3.51 ).I0: fmul.d f5,f0,f2 I1: fadd.d f9,f5,f4I2: fadd.d f5,f5,f2I3: fdiv.d f2,f9,f0 Figure 3.50 Sample code superscalar register renaming. Rename table 0 12345Next available register dst=F4 src1=F1 src2=F2dst=F1 src1=F2 src2=F3dst=T9 src1=T19 src2=T38 dst=T10 src1=T9 src2=T19 (Similar mux src2)YNThis 9 appears renametable nextclock cycle I1 dst =I2 src?(As per instr 1)I1 I219 29 8 9 62 63910. . . . . . . . . 21 38 Figure 3.51 Initial state register renaming table.270 ■Chapter Three Instruction-Level Parallelism Exploitation3.9 [5]<3.4>If ever get confused register renamer do, go back assembly code ’re executing, ask happen right result obtained. example, consider three-way superscalarmachine renaming three instructions concurrently: addi x1, x1, x1 addi x1, x1, x1 addi x1, x1, x1 value x1starts 5, value sequence executed? 3.10 [20]<3.4, 3.7 >Very long instruction word (VLIW) designers basic choices make regarding architectural rules register use. Suppose VLIW designed self-draining execution pipelines: operation initiated, results appear destination register Lcycles later (where Lis latency operation). never enough registers, temptationto wring maximum use registers exist. Consider Figure 3.52 . loads 1+2 cycle latency, unroll loop once, show VLIW capable oftwo loads two adds per cycle use minimum number registers, theabsence pipeline interruptions stalls. Give example event that, inthe presence self-draining pipelines, could disrupt pipelining yield wrong results. 3.11 [10/10/10] <3.3>Assume five-stage single-pipeline microarchitecture (fetch, decode, execute, memory, write-back) code Figure 3.53 . ops one cycle except LWandSW, 1+2 cycles, branches, 1+1 cycles. forwarding. Show phases instruction per clockcycle one iteration loop. a.[10]<3.3>How many clock cycles per loop iteration lost branch overhead? b.[10]<3.3>Assume static branch predictor, capable recognizing back- ward branch Decode stage. many clock cycles wastedon branch overhead? Loop: lw x1,0(x2); lw x3,8(x2) <stall > <stall > addi x10,x1,1; addi x11,x3,1sw x1,0(x2); sw x3,8(x2)addi x2,x2,8sub x4,x3,x2bnz x4,Loop Figure 3.52 Sample VLIW code two adds, two loads, two stalls.Case Studies Exercises Jason D. Bakos Robert P. Colwell ■271c.[10]<3.3>Assume dynamic branch predictor. many cycles lost correct prediction? 3.12 [15/20/20/10/20] <3.4, 3.6 >Let’s consider dynamic scheduling might achieve here. Assume microarchitecture shown Figure 3.54 . Assume arithmetic-logical units (ALUs) arithmetic ops ( fmul.d, fdiv.d, fadd.d, addi, sub ) branches, Reservation Station (RS) dispatch, most, one operation functional unit per cycle (one op ALU plus one memory op fld/fsd). a.[15]<3.4>Suppose instructions sequence Figure 3.47 present RS, renaming done. Highlight instruc- tions code register renaming would improve performance. ( Hint: look read-after-write write-after-write hazards. Assume func-tional unit latencies Figure 3.47 .) b.[20]<3.4>Suppose register-renamed version code part (a) resident RS clock cycle N, latencies given Figure 3.47 . Show RS dispatch instructions order, clock clock, toLoop: lw x1,0(x2) addi x1,x1, 1sw x1,0(x2)addi x2,x2,4sub x4,x3,x2bnz x4,Loop Figure 3.53 Code loop Exercise 3.11. Reservation stationALU 0 Instructions decoder 1 2ALU 1 LD/ST Mem Figure 3.54 Microarchitecture Exercise 3.12.272 ■Chapter Three Instruction-Level Parallelism Exploitationobtain optimal performance code. (Assume RS restrictions part (a). Also assume results must written RS ’re available use —no bypassing.) many clock cycles code sequence take? c.[20]<3.4>Part (b) lets RS try optimally schedule instructions. reality, whole instruction sequence interest usually present theRS. Instead, various events clear RS, new code sequence streams infrom decoder, RS must choose dispatch has. Suppose theRS empty. cycle 0, first two register-renamed instructions thissequence appear RS. Assume takes one clock cycle dispatch anyop, assume functional unit latencies Exercise 3.2. Fur-ther assume front end (decoder/register-renamer) continue supply two new instructions per clock cycle. Show cycle-by-cycle order dispatch RS. many clock cycles code sequence require now? d.[10]<3.4>If wanted improve results part (c), would helped most: (1) Another ALU? (2) Another LD/ST unit? (3) Full bypassing ofALU results subsequent operations? (4) Cutting longest latency half?What ’s speedup? e.[20]<3.6>Now let ’s consider speculation, act fetching, decoding, executing beyond one conditional branches. motivation thisis twofold: dispatch schedule came part (c) lots nops,and know computers spend time executing loops (which impliesthe branch back top loop pretty predictable). Loops tell us tofind work do; sparse dispatch schedule suggests opportuni- ties work earlier before. part (d) found critical path loop. Imagine folding second copy path onto sched-ule got part (b). many clock cycles would required twoloops ’worth work (assuming instructions resident RS)? (Assume functional units fully pipelined.) Exercises 3.13 [25]<3.7, 3.8 >In exercise, explore performance trade-offs three processors employ different types multithreading (MT). ofthese processors superscalar, uses in-order pipelines, requires fixed three-cycle stall following loads branches, identical L1 caches. Instructions thread issued cycle read program order mustnot contain data control dependences. ■Processor superscalar simultaneous MT architecture, capable issuingup two instructions per cycle two threads. ■Processor B fine-grained MT architecture, capable issuing fourinstructions per cycle single thread switches threads anypipeline stall.Case Studies Exercises Jason D. Bakos Robert P. Colwell ■273■Processor C coarse-grained MT architecture, capable issuing eight instructions per cycle single thread switches threads L1 cache miss. application list searcher, scans region memory specific value stored R9between address range specified R16 andR17. par- allelized evenly dividing search space four equal-sized contiguousblocks assigning one search thread block (yielding four threads). Mostof thread ’s runtime spent following unrolled loop body: loop: lw x1,0(x16) lw x2,8(x16)lw x3,16(x16)lw x4,24(x16)lw x5,32(x16)lw x6,40(x16)lw x7,48(x16) lw x8,56(x16) beq x9,x1,match0beq x9,x2,match1beq x9,x3,match2beq x9,x4,match3beq x9,x5,match4beq x9,x6,match5beq x9,x7,match6 beq x9,x8,match7 DADDIU x16,x16,#64blt x16,x17,loop Assume following: ■A barrier used ensure threads begin simultaneously. ■The first L1 cache miss occurs two iterations loop. ■None BEQAL branches taken. ■The BLT always taken. ■All three processors schedule threads round-robin fashion. Determine many cycles required processor complete firsttwo iterations loop. 3.14 [25/25/25] <3.2, 3.7 >In exercise, look software techniques extract instruction-level parallelism (ILP) common vector loop. followingloop so-called DAXPY loop (double-precision aXplus Y) central operation Gaussian elimination. following code implements DAXPYoperation, Y¼aX+Y, vector length 100. Initially, R1is set base address array XandR2is set base address Y: addi x4,x1,#800 ; x1 = upper bound X274 ■Chapter Three Instruction-Level Parallelism Exploitationfoo: fld F2,0(x1) ; (F2) = X(i) fmul.d F4,F2,F0 ; (F4) = a*X(i) fld F6,0(x2) ; (F6) = Y(i)fadd.d F6,F4,F6 ; (F6) = a*X(i) + Y(i)fsd F6,0(x2) ; Y(i) = a*X(i) + Y(i)addi x1,x1,#8 ; increment X indexaddi x2,x2,#8 ; increment indexsltu x3,x1,x4 ; test: continue loop?bnez x3,foo ; loop needed Assume functional unit latencies shown following table. Assume one-cycle delayed branch resolves ID stage. Assume results fullybypassed. Instruction producing result Instruction using result Latency clock cycles FP multiply FP ALU op 6 FP add FP ALU op 4 FP multiply FP store 5FP add FP store 4Integer operations loads 2 a.[25]<3.2>Assume single-issue pipeline. Show loop would look unscheduled compiler compiler scheduling bothfloating-point operation branch delays, including stalls idle clockcycles. execution time (in cycles) per element result vector,Y, unscheduled scheduled? much faster must clock proces-sor hardware alone match performance improvement achieved thescheduling compiler? (Neglect possible effects increased clock speed memory system performance.) b.[25]<3.2>Assume single-issue pipeline. Unroll loop many times necessary schedule without stalls, collapsing loop overhead instruc- tions. many times must loop unrolled? Show instruction sched-ule. execution time per element result? c.[25]<3.7>Assume VLIW processor instructions contain five operations, shown Figure 3.20 . compare two degrees loop unrolling. First, unroll loop 6 times extract ILP schedule withoutany stalls (i.e., completely empty issue cycles), collapsing loop overheadinstructions, repeat process unroll loop 10 times. Ignorethe branch delay slot. Show two schedules. execution time perelement result vector schedule? percent operation slots used schedule? much size code differ two schedules? total register demand twoschedules?Case Studies Exercises Jason D. Bakos Robert P. Colwell ■2753.15 [20/20] <3.4, 3.5, 3.7, 3.8 >In exercise, look variations Tomasulo ’s algorithm perform running loop Exercise 3.14. functional units (FUs) described following table. FU type Cycles EX Number FUs Number reservation stations Integer 1 1 5 FP adder 10 1 3FP multiplier 15 1 2 Assume following: ■Functional units pipelined. ■There forwarding functional units; results communicated common data bus (CDB). ■The execution stage (EX) effective address calculation memory access loads stores. Thus, pipeline IF/ID/IS/EX/WB. ■Loads require one clock cycle. ■The issue (IS) write-back (WB) result stages require one clock cycle. ■There five load buffer slots five store buffer slots. ■Assume Branch Equal Zero (BNEZ) instruction requires one clock cycle. a.[20]<3.4–3.5>For problem use single-issue Tomasulo MIPS pipeline ofFigure 3.10 pipeline latencies preceding table. Show number stall cycles instruction clock cycle instructionbegins execution (i.e., enters first EX cycle) three iterations loop.How many cycles loop iteration take? Report answer formof table following column headers: ■Iteration (loop iteration number) ■Instruction ■Issues (cycle instruction issues) ■Executes (cycle instruction executes) ■Memory access (cycle memory accessed) ■Write CDB (cycle result written CDB) ■Comment (description event instruction waiting) Show three iterations loop table. may ignore first instruction. b.[20]<3.7, 3.8 >Repeat part (a) time assume two-issue Tomasulo algorithm fully pipelined floating-point unit (FPU). 3.16 [10]<3.4>Tomasulo ’s algorithm disadvantage: one result compute per clock per CDB. Use hardware configuration latencies previous question find code sequence 10 instructions Toma- sulo’s algorithm must stall due CDB contention. Indicate occurs sequence.276 ■Chapter Three Instruction-Level Parallelism Exploitation3.17 [20]<3.3>An ( m,n) correlating branch predictor uses behavior recent mexecuted branches choose 2mpredictors, n- bit predictor. two-level local predictor works similar fashion, keepstrack past behavior individual branch predict future behavior. design trade-off involved predictors: correlating predictors require little memory history, allows maintain 2-bit predictorsfor large number individual branches (reducing probability branch instructions reusing predictor), local predictors require substan- tially memory keep history thus limited tracking relativelysmall number branch instructions. exercise, consider (1,2) correlatingpredictor track four branches (requiring 16 bits) versus (1,2) local pre-dictor track two branches using amount memory. fol-lowing branch outcomes, provide prediction, table entry used make theprediction, updates table result prediction, final mis-prediction rate predictor. Assume branches point taken. Initialize predictor following: Correlating predictor Entry Branch Last outcome Prediction 0 0 one misprediction 1 0 NT NT21 N T3 1 NT T42 T5 2 NT T6 3 NT one misprediction7 3 NT NT Local predictor Entry Branch Last 2 outcomes (right recent) Prediction 0 0 T,T one misprediction 1 0 T,NT NT2 0 NT,T NT3 0 NT T4 1 T,T T5 1 T,NT one misprediction6 1 NT,T NT7 1 NT,NT NTCase Studies Exercises Jason D. Bakos Robert P. Colwell ■2773.18 [10]<3.9>Suppose deeply pipelined processor, imple- ment branch-target buffer conditional branches only. Assume mis- prediction penalty always four cycles buffer miss penalty always threecycles. Assume 90% hit rate, 90% accuracy, 15% branch frequency. Howmuch faster processor branch-target buffer versus processor thathas fixed two-cycle branch penalty? Assume base clock cycle per instruction(CPI) without branch stalls one. 3.19 [10/5] <3.9>Consider branch-target buffer penalties zero, two, two clock cycles correct conditional branch prediction, incorrect prediction,and buffer miss, respectively. Consider branch-target buffer design distin-guishes conditional unconditional branches, storing target address aconditional branch target instruction unconditional branch. a.[10]<3.9>What penalty clock cycles unconditional branch found buffer? b.[10]<3.9>Determine improvement branch folding unconditional branches. Assume 90% hit rate, unconditional branch frequency 5%, two-cycle penalty buffer miss. much improvement gained enhancement? high must hit rate enhancement provide aperformance gain?Branch PC (word address) Outcome 454 543 NT777 NT543 NT777 NT454 T777 NT454 T543 T278 ■Chapter Three Instruction-Level Parallelism ExploitationThis page intentionally left blank4.1 Introduction 282 4.2 Vector Architecture 283 4.3 SIMD Instruction Set Extensions Multimedia 304 4.4 Graphics Processing Units 310 4.5 Detecting Enhancing Loop-Level Parallelism 336 4.6 Cross-Cutting Issues 345 4.7 Putting Together: Embedded Versus Server GPUs Tesla Versus Core i7 346 4.8 Fallacies Pitfalls 353 4.9 Concluding Remarks 357 4.10 Historical Perspective References 357 Case Study Exercises Jason D. Bakos 3574 Data-Level Parallelism Vector, SIMD, GPUArchitectures call algorithms data parallel algorithms parallelism comes simultaneous operations across large sets data rather multiple threads control. W. Daniel Hillis Guy L. Steele, “Data parallel algorithms, ”Commun. ACM (1986) plowing field, would rather use: two strong oxen 1024 chickens? Seymour Cray, Father Supercomputer (arguing two powerful vector processors versus many simple processors) Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00004-3 ©2019 Elsevier Inc. rights reserved.4.1 Introduction question single instruction multiple data (SIMD) architecture, Chapter 1 introduced, always wide set applications sig- nificant data-level parallelism (DLP). Five years SIMD classification wasproposed (Flynn, 1966), answer matrix-oriented computations ofscientific computing also media-oriented image sound processing andmachine learning algorithms, see Chapter 7 . Since multiple instruc- tion multiple data (MIMD) architecture needs fetch one instruction per data oper- ation, single instruction multiple data (SIMD) potentially energy-efficientsince single instruction launch many data operations. two answers makeSIMD attractive personal mobile devices well servers. Finally, perhapsthe biggest advantage SIMD versus MIMD programmer continues tothink sequentially yet achieves parallel speedup parallel data operations. chapter covers three variations SIMD: vector architectures, multimedia SIMD instruction set extensions, graphics processing units (GPUs). 1 first variation, predates two 30 years, extends pipelined execution many data operations. vector architectures easier understand compile SIMD variations, consid-ered expensive microprocessors recently. Part expense intransistors, part cost sufficient dynamic random access memory(DRAM) bandwidth, given widespread reliance caches meet memory per-formance demands conventional microprocessors. second SIMD variation borrows SIMD name mean basically simultaneous parallel data operations found instruction set architectures support multimedia applications. x86 architectures, theSIMD instruction extensions started MMX (multimedia extensions) in1996, followed several SSE (streaming SIMD extensions) versionsin next decade, continue day AVX (advanced vectorextensions). get highest computation rate x86 computer, oftenneed use SIMD instructions, especially floating-point programs. third variation SIMD comes graphics accelerator community, offering higher potential performance found traditional multicore com- puters today. Although GPUs share features vector architectures, havetheir distinguishing characteristics, part ecosystem whichthey evolved. environment system processor system memory inaddition GPU graphics memory. fact, recognize distinc-tions, GPU community refers type architecture heterogeneous . 1This chapter based material Appendix F, “Vector Processors, ”by Krste Asanovic, Appendix G, “Hardware Software VLIW EPIC ”from 5th edition book; material Appendix A, “Graphics Computing GPUs, ”by John Nickolls David Kirk, 5th edition Computer Organization Design ; lesser extent material “Embracing Extending 20th-Century Instruction Set Architectures, ”by Joe Gebis David Patterson, IEEE Computer , April 2007.282 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesFor problems lots data parallelism, three SIMD variations share advantage easier programmer classic parallel MIMD programming. goal chapter architects understand vector general multimedia SIMD, well similarities differences vectorand GPU architectures. vector architectures supersets multimediaSIMD instructions, including better model compilation, GPUsshare several similarities vector architectures, start vector architecturesto set foundation following two sections. next section introduces vector architectures, Appendix G goes much deeper subject. 4.2 Vector Architecture efficient way execute vectorizable application vectorprocessor. Jim Smith, International Symposium Computer Architecture (1994) Vector architectures grab sets data elements scattered memory, place large sequential register files, operate data register files, thendisperse results back memory. single instruction works vectors ofdata, results dozens register-register operations independent dataelements. large register files act compiler-controlled buffers, hide mem- ory latency leverage memory bandwidth. vector loads stores deeply pipelined, program pays long memory latency per vec-tor load store versus per element, thus amortizing latency over, say, 32elements. Indeed, vector programs strive keep memory busy. power wall leads architects value architectures deliver good performance without energy design complexity costs highly out-of-order superscalar processors. Vector instructions natural match trendbecause architects use increase performance simple in-order scalar processors without greatly raising energy demands design complexity. prac- tice, developers express many programs ran well complex out-of-order designs efficiently data-level parallelism form vectorinstructions, Kozyrakis Patterson (2002) showed. RV64V Extension begin vector processor consisting primary components Figure 4.1 shows. loosely based 40-year-old Cray-1, one first supercomputers. time writing edition, RISC- V vector instruction set extension RVV still development. (The vector extension called RVV, RV64V refers RISC-V base instructions4.2 Vector Architecture ■283plus vector extension.) show subset RV64V, trying capture essence pages. primary components instruction set architecture RV64V following: ■Vector registers —Each vector register holds single vector, RV64V 32 them, 64 bits wide. vector register file needs provide enoughports feed vector functional units. ports allow high degree overlap among vector operations different vector registers. read write ports, total least 16 read ports 8 write ports, connected tothe functional unit inputs outputs pair crossbar switches. One way toMain memory Vector registers Scalar registersFP add/subtract FP multiply FP divide Integer LogicalVector load/store Figure 4.1 basic structure vector architecture, RV64V, includes RISC-V scalar architecture. also 32 vector registers, functional units vector functional units. vector scalar registers significant number read write ports allow multiple simultaneous vector operations. set crossbar switches (thick gray lines) connects ports inputs outputs vector functional units.284 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesincrease register file bandwidth compose multiple banks, work well relatively long vectors. ■Vector functional units —Each unit fully pipelined implementation, start new operation every clock cycle. control unit needed detect hazards, structural hazards functional units data hazards register accesses. Figure 4.1 shows assume implementation RV64V five functional units. simplicity, focus floating-pointfunctional units section. ■Vector load/store unit —The vector memory unit loads stores vector memory. vector loads stores fully pipelined ourhypothetical RV64V implementatio n words moved vector registers memory bandwidth one word per clockcycle, initial latency. unit would also normally handle scalarloads stores. ■A set scalar registers —Scalar registers likewise provide data input vector functional units, well compute addresses pass vector load/store unit. normal 31 general-purpose registers 32 floating-point registers RV64G. One input vector functional unitslatches scalar values read scalar register file. Figure 4.2 lists RV64V vector instruc tions use section. description Figure 4.2 assumes input operands vector registers, also versions inst ructions operand scalar register ( xio rfi). RV64V uses suffix .vv vectors, .vs second operand scalar, .sv first scalar register. Thus three va lid RV64V instructions: vsub.vv ,vsub.vs ,a n vsub.sv . (Add commutative ope rations first two versions, vadd.sv andvadd.sv would redundant.) operands determine version struction, usually let assembler supply appropriate suffix. vector functional unit gets copy sca-lar value instruction issue time. Although traditional vector architectures ’t support narrow data types efficiently, vectors naturally accommodate varying data sizes ( Kozyrakis Patterson, 2002 ). Thus, vector register 32 64-bit elements, 128 /C216- bit elements, even 256 /C28-bit elements equally valid views. hardware multiplicity vector architecture useful multimedia applications aswell scientific applications. Note RV64V instructions Figure 4.2 omit data type size! innovation RV64V associate data type data size vector register , rather normal approach instruction supplying informa- tion. Thus, executing vector instructions, program configures vector registers used specify data type widths. Figure 4.3 lists options RV64V.4.2 Vector Architecture ■285Mnemonic Name Description vadd ADD Add elements V[rs1] V[rs2], put result V[rd] vsub SUBtract Subtract elements V[rs2] frpm V[rs1], put result V[rd] vmul MULtiply Multiply elements V[rs1] V[rs2], put result V[rd] vdiv DIVide Divide elements V[rs1] V[rs2], put result V[rd] vrem REMainder Take remainder elements V[rs1] V[rs2], put result V[rd] vsqrt SQuare RooT Take square root elements V[rs1], put result V[rd] vsll Shift Left Shift elements V[rs1] left V[rs2], put result V[rd] vsrl Shift Right Shift elements V[rs1] right V[rs2], put result V[rd] vsra Shift Right ArithmeticShift elements V[rs1] right V[rs2] extending sign bit, put result V[rd] vxor XOR Exclusive elements V[rs1] V[rs2], put result V[rd] vor Inclusive elements V[rs1] V[rs2], put result V[rd] vand Logical elements V[rs1] V[rs2], put result V[rd] vsgnj SiGN source Replace sign bits V[rs1] sign bits V[rs2], put result V[rd] vsgnjn Negative SiGN sourceReplace sign bits V[rs1] complemented sign bits V[rs2], put result V[rd] vsgnjx Xor SiGN sourceReplace sign bits V[rs1] xor sign bits V[rs1] V[rs2], put result V[rd] vld Load Load vector register V[rd] memory starting address R[rs1] vlds Strided Load Load V[rd] address R[rs1] stride R[rs2] (i.e., R[rs1]+i /C2R[rs2]) vldx Indexed Load (Gather)Load V[rs1] vector whose elements R[rs2]+V[rs2] (i.e., V[rs2] index) vst Store Store vector register V[rd] memory starting address R[rs1] vsts Strided Store Store V[rd] memory address R[rs1] stride R[rs2] (i.e., R[rs1]+i /C2R[rs2]) vstx Indexed Store (Scatter)Store V[rs1] memory vector whose elements R[rs2]+V[rs2] ( i.e., V[rs2] index) vpeq Compare ¼ Compare elements V[rs1] V[rs2]. equal, put 1 corresponding 1-bit element p[rd]; otherwise, put 0 vpne Compare ! ¼ Compare elements V[rs1] V[rs2]. equal, put 1 corresponding 1-bit element p[rd]; otherwise, put 0 vplt Compare < Compare elements V[rs1] V[rs2]. less than, put 1 corresponding 1- bit element p[rd]; otherwise, put 0 vpxor Predicate XOR Exclusive 1-bit elements p[rs1] p[rs2], put result p[rd] vpor Predicate Inclusive 1-bit elements p[rs1] p[rs2], put result p[rd] vpand Predicate Logical 1-bit elements p[rs1] p[rs2], put result p[rd] setvl Set Vector LengthSet vl destination register smaller mvl source regsiter Figure 4.2 RV64V vector instructions. use R instruction format. vector operation two operands shown operands vector ( .vv), also versions second operand scalar register ( .vs) and, makes difference, first operand scalar register second vector register ( .sv). type width operands determined configuring vector register rather supplied instruction. addition vector registers predicate registers, two vectorcontrol status registers (CSRs), vland vctype , discussed below. strided indexed data transfers also explained later. completed, RV64 surely instructions, ones figure included.One reason dynamic register typing many instructions required conventional vector architecture supports variety. Given com-binations data types sizes Figure 4.3 , dynamic register typing, Figure 4.2 would several pages long! Dynamic typing also lets programs disable unused vector registers. conse- quence, enabled vector registers allocated vector memory long vectors. example, assume 1024 bytes vector memory, 4 vector registers enabled type 64 –bit floats, processor would give vector register 256 bytes 256/8 ¼32 elements. valiue called maximum vector length (mvl), set processor cannot changed sofware. One complaint vector architectures larger state means slower context switch time. implementation RV64V increases state factor 3:from 2 /C232/C28¼512 bytes 2 /C232/C21024¼1536 bytes. pleasant side effect dynamic register typing program configure vector registers dis- abled used, need save restore context switch. third benefit dynamic register typing conversions different size operands implicit depending configuration registers ratherthan additional explicit conversion instructions. ’ll see example benefit next section. names vld andvst denote vector load vector store, load store entire vectors data. One operand vector register loaded stored; operand, RV64G general-purpose register, start- ing address vector memory. Vector needs registers beyond vectorregisters themselves. vector-length register vlis used natural vector length equal mvl, vector-type register vctype records register types, predicate registers p iare used loops involve statements. ’ll see action following example. vector instruction, system perform operations vector data elements many ways, including operating many elements simultaneously. flexibility lets vector designs use slow wide execution units achieve high performance low power. Furthermore, independence elements withina vector instruction set allows scaling functional units without performing addi-tional costly dependency checks, superscalar processors require.Integer 8, 16, 32, 64 bits Floating point 16, 32, 64 bits Figure 4.3 Data sizes supported RV64V assuming also single- double-precision floating-point extensions RVS RVD. Adding RVV RISC-V design means scalar unit must also add RVH, scalar instruction extension support half-precision (16-bit) IEEE 754 floating point. RV32Vwould doubleword scalar operations, could drop 64-bit integers vector unit. RISC-V implementation ’t include RVS RVD, could omit vec- tor floating-point instructions.4.2 Vector Architecture ■287How Vector Processors Work: Example best understand vector processor looking vector loop RV64V. Let’s take typical vector problem, use throughout section: Y=a /C2X+Y XandYare vectors, initially resident memory, ais scalar. problem theSAXPY orDAXPY loop forms inner loop Linpack benchmark (Dongarra et al., 2003 ). (SAXPY stands ingle-precision /C2Xplus , DAXPY ouble precision /C2Xplus .) Linpack collection linear alge- bra routines, Linpack benchmark consists routines performingGaussian elimination. now, let us assume number elements, length , vector reg- ister (32) matches length vector operation interested in. (Thisrestriction lifted shortly.) Example Show code RV64G RV64V DAXPY loop. example, assume X 32 elements starting addresses XandY x5andx6, respectively. (A subsequent example covers 32 elements.) Answer RISC-V code: fld f0,a # Load scalar addi x28,x5,#256 # Last address load Loop: fld f1,0(x5) # Load X[i] fmul.d f1,f1,f0 # /C2X[i] fld f2,0(x6) # Load Y[i] fadd.d f2,f2,f1 # /C2X[i] + Y[i] fsd f2,0(x6) # Store Y[i]addi x5,x5,#8 # Increment index Xaddi x6,x6,#8 # Increment index Ybne x28,x5,Loop # Check done RV64V code DAXPY: vsetdcfg 4*FP64 # Enable 4 DP FP vregs fld f0,a # Load scalar vld v0,x5 # Load vector Xvmul v1,v0,f0 # Vector-scalar multvld v2,x6 # Load vector Yvadd v3,v1,v2 # Vector-vector addvst v3,x6 # Store sumvdisable # Disable vector regs Note assembler determines version vector operations gen- erate. multiply scalar operand, generates vmul.vs , whereas add ’t, generates vadd.vv .288 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesThe initial instruction configures first four vector registers hold 64-bit floating-point data. last instruction disables vector registers. contextswitch happened last instructio n, additional state save. dramatic difference preceding scalar vector code vector processor greatly reduces dynamic instruction bandwidth, executingonly 8 instructions versus 258 RV64G. reduction occurs vector operations work 32 elements overhead instructions constitute nearly half loop RV64G present RV64V code. compilerproduces vector instructions sequence, resulting code spendsmuch time running vector mode, code said vectorized orvector- izable . Loops vectorized dependences iterations loop, called loop-carried dependences (seeSection 4.5 ). Another important difference RV64G RV64V frequency pipeline interlocks simple implementation RV64G. straightforward RV64G code, every fadd.d must wait fmul.d , every fsd must wait fadd.d. vector processor, vector instruction stall first element vector, subsequent elements flow smoothlydown pipeline. Thus pipeline stalls required per vector instruction , rather per vector element . Vector architects call forwarding element- dependent operations chaining , dependent operations “chained ” together. example, pipeline stall frequency RV64G about32/C2higher RV64V. Software pipelining, loop unrolling (Appendix H), out-of-order execution reduce pipeline stalls RV64G; however, large difference instruction bandwidth cannot reduced substantially. Let’s show dynamic register typing discussing performance code. Example common use multiply-accumulate operations multiply using narrow data accumulate wider size increase accuracy sum products.Show preceding code would change Xandawere single-precision instead double-precision floating point. Next, show changes code switch X,Y, afrom floating-point type integers. Answer changes underlined following code. Amazingly, code works two small changes: configuration instruction includes one single-precisionvector, scalar load single-precision: vsetdcfg 1*FP32,3 *FP64 # 1 32b, 3 64b vregs flw f0,a # Load scalar vld v0,x5 # Load vector Xvmul v1,v0,f0 # Vector-scalar multvld v2,x6 # Load vector Yvadd v3,v1,v2 # Vector-vector addvst v3,x6 # Store sumvdisable # Disable vector regs4.2 Vector Architecture ■289Note RV64V hardware implicitly perform conversion narrower single-precision wider double-precision setup. switch integers almost easy, must use integer load instruction integer register hold scalar value: vsetdcfg 1*X32,3*X64 # 1 32b, 3 64b int reg lw x7,a # Load scalar vld v0,x5 # Load vector X vmul v1,v0,x7 # Vector-scalar mult vld v2,x6 # Load vector Yvadd v3,v1,v2 # Vector-vector addvst v3,x6 # Store sumvdisable # Disable vector regs Vector Execution Time execution time sequence vector operations primarily depends threefactors: (1) length operand vectors, (2) structural hazards among theoperations, (3) data dependences. Given vector length initiation rate, rate vector unit consumes new operands produces new results, compute time single vector instruction. modern vector computers vector functional units multiple par- allel pipelines (or lanes ) produce two results per clock cycle, may also functional units fully pipelined. sim-plicity, RV64V implementation one lane initiation rate one ele-ment per clock cycle individual operations. Thus execution time clockcycles single vector instruction approximately vector length. simplify discussion vector execution vector performance, use notion convoy , set vector instructions could potentially execute together. instructions convoy must contain structural haz- ards; hazards present, instructions would need serialized andinitiated different convoys. Thus vld following vmul pre- ceding example convoy. soon see, estimateperformance section code counting number convoys. keep thisanalysis simple, assume convoy instructions must complete execution instructions (scalar vector) begin execution. might seem addition vector instruction sequences structural hazards, sequences read-after-write dependency hazards also inseparate convoys. However, chaining allows convoy sinceit allows vector operation start soon individual elements vectorsource operand become available: results first functional unit thechain “forwarded ”to second functional unit. practice, often imple- ment chaining allowing processor read write particular vector reg- ister time, albeit different elements. Early implementations of290 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectureschaining worked like forwarding scalar pipelining, restricted timing source destination instructions chain. Recent implementa- tions use flexible chaining , allows vector instruction chain essentially active vector instruction, assuming ’t generate structural hazard. modern vector architectures support flexible chaining, weassume throughout chapter. turn convoys execution time, need metric estimate length convoy. called chime , simply unit time taken execute one convoy. Thus vector sequence consists mconvoys executes mchimes; vector length n, simple RV64V implementation, approxi- mately m/C2nclock cycles. chime approximation ignores processor-specific overheads, many dependent vector length. Therefore measuring time chimes abetter approximation long vectors short ones. use chimemeasurement, rather clock cycles per result, indicate explicitly areignoring certain overheads. know number convoys vector sequence, know execution time chimes. One source overhead ignored measuring chimes limitation initiating multiple vector instructions single clock cycle. one vectorinstruction initiated clock cycle (the reality vector processors),the chime count underestimate actual execution time convoy. Becausethe length vectors typically much greater number instructions inthe convoy, simply assume convoy executes one chime. Example Show following code sequence lays convoys, assuming single copy vector functional unit: vld v0,x5 # Load vector X vmul v1,v0,f0 # Vector-scalar multiplyvld v2,x6 # Load vector vadd v3,v1,v2 # Vector-vector add vst v3,x6 # Store sum many chimes vector sequence take? many cycles per FLOP (floating-point operation) needed, ignoring vector instruction issue overhead? Answer first convoy starts first vld instruction. vmul dependent firstvld, chaining allows convoy. second vld instruction must separate convoy structural hazard load/store unit prior vld instruction. vadd dependent second vld, convoy via chain- ing. Finally, vst structural hazard vld second convoy, must go third convoy. analysis leads following layout vector instructions convoys:4.2 Vector Architecture ■2911.vld vmul 2.vld vadd 3.vst sequence requires three convoys. sequence takes three chimes two floating-point operations per result, number cycles per FLOP is1.5 (ignoring vector instruction issue overhead). Note that, although allow thevld andvmul execute first convoy, vector machines take 2 clock cycles initiate instructions. example shows chime approximation reasonably accurate long vectors. example, 32-element vectors, time chimes 3, sothe sequence would take 32 /C23 96 clock cycles. overhead issuing convoys two separate clock cycles would small. Another source overhead far significant issue limitation. important source overhead ignored chime model vector start-up time, latency clock cycles pipeline full. start-up time principally determined pipelining latency vector functional unit. RV64V, use pipeline depths Cray-1, although latencies modern processors tended increase, especially vector loads. Allfunctional units fully pipelined. pipeline depths 6 clock cycles forfloating-point add, 7 floating-point multiply, 20 floating-point divide,and 12 vector load. Given vector basics, next several sections give optimizations either improve performance increase types programs run wellon vector architectures. particular, answer questions: ■How vector processor execute single vector faster one element perclock cycle? Multiple elements per clock cycle improve performance. ■How vector processor handle programs vector lengths notthe maximum vector length (mvl)? application vec-tors ’t match architecture vector length, need efficient solution common case. ■What happens statement inside code vectorized?More code vectorize efficiently handle conditional statements. ■What vector processor need memory system? Without suffi-cient memory bandwidth, vector execution futile. ■How vector processor handle multiple dimensional matrices? pop-ular data structure must vectorize vector architectures well. ■How vector processor handle sparse matrices? popular data struc-ture must vectorize also.292 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectures■How program vector computer? Architectural innovations mismatch programming languages compilers may get widespread use. rest section introduces optimizations vector archi- tecture, Appendix G goes greater depth. Multiple Lanes: Beyond One Element per Clock Cycle critical advantage vector instruction set allows software pass alarge amount parallel work hardware using single short instruction.One vector instruction include scores independent operations yet beencoded number bits conventional scalar instruction. par-allel semantics vector instruction allow implementation execute theseelemental operations using deeply pipelined functional unit, RV64V implementation ’ve studied far; array parallel functional units; com- bination parallel pipelined functional units. Figure 4.4 illustrates improve vector performance using parallel pipelines execute vector addinstruction. RV64V instruction set property vector arithmetic instruc- tions allow element Nof one vector register take part operations element Nfrom vector registers. dramatically simplifies design highly parallel vector unit, structured multiple parallel lanes . traffic highway, increase peak throughput vector unit adding lanes. Figure 4.5 shows structure four-lane vector unit. Thus going four lanes one lane reduces number clocks chimefrom 32 8. multiple lanes advantageous, applications thearchitecture must support long vectors; otherwise, execute quickly thatyou’ll run instruction bandwidth, requiring ILP techniques (see Chapter 3 )t supply enough vector instructions. lane contains one portion vector register file one execution pipeline vector functional unit. vector functional unit executes vector instructions rate one element group per cycle using multiplepipelines, one per lane. first lane holds first element (element 0) allvector registers, first element vector instruction itssource destination operands located first lane. allocation allowsthe arithmetic pipeline local lane complete operation without commu-nicating lanes. Avoiding interlane communication reduces wiringcost register file ports required build highly parallel execution unit helps explain vector computers complete 64 operations per clock cycle (2 arithmetic units 2 load/store units across 16 lanes). Adding multiple lanes popular technique improve vector performance requires little increase control complexity require changes toexisting machine code. also allows designers trade die area, clock rate,voltage, energy without sacrificing peak performance. clock rate a4.2 Vector Architecture ■293vector processor halved, doubling number lanes retain peak performance. Vector-Length Registers: Handling Loops Equal 32 vector register processor natural vector length determined maximumvector length (mvl). length, 32 example above, unlikely(A)( B)Element group Figure 4.4 Using multiple functional units improve performance single vector add instruction, C5A+B. vector processor (A) left single add pipeline complete one addition per clock cycle. vector processor (B) right four add pipelines complete four additions per clock cycle. elements within single vector add instruction interleaved across four pipelines. set elements move pipelines together termed element group . Reproduced permission Asanovic, K., 1998. Vector Microprocessors (Ph.D. thesis). Computer Science Division, University California, Berkeley.294 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesto match real vector length program. Moreover, real program, length particular vector operation often unknown compile time. fact, single piece code may require different vector lengths. example, considerthis code: (i=0; <n; i=i+1) Y[i] = * X[i] + Y[i]; size vector operations depends n, may even known run time. value n might also parameter procedure containingthe preceding loop therefore subject change execution.Lane 1 Lane 2 Lane 3 Lane 0 FP add pipe 0 Vector registers: elements 0, 4, 8, . . . FP mul. pipe 0FP mul. pipe 1 Vector load-store unitFP mul. pipe 2FP mul. pipe 3Vector registers: elements 1, 5, 9, . . . Vector registers: elements 2, 6, 10, . . . Vector registers: elements 3, 7, 11, . . . FP add pipe 1FP add pipe 2FP add pipe 3 Figure 4.5 Structure vector unit containing four lanes. vector register mem- ory divided across lanes, lane holding every fourth element vector register. figure shows three vector functional units: FP add, FP multiply,and load-store unit. vector arithmetic units contains four execution pipe- lines, one per lane, act concert complete single vector instruction. Note section vector register file needs provide enough ports pipe-lines local lane. figure show path provide scalar operand vector-scalar instructions, scalar processor (or Control Processor) broadcasts scalar value lanes.4.2 Vector Architecture ■295The solution problems add vector-length register (vl). vl controls length vector operation, including vector load store. value vl, however, cannot greater maximum vector length (mvl). solves problem long real length less equal max-imum vector length ( mvl). parameter means length vector registers grow later computer generations without changing instruction set. willsee next section, multimedia SIMD extensions equivalent mvl,s expand instruction set every time increase vector length. value nis known compile time thus may greater mvl? tackle second problem vector longer maximum length, technique called strip mining traditionally used. Strip mining generation code vector operation done size less thanor equal mvl. One loop handles number iterations multiple themvl another loop handles remaining iterations must less mvl. RISC-V better solution separate loop strip mining. instruction setvl writes smaller mvl loop variable ninto vl(and another register). number iterations loop larger n, fastest loop compute mvl values time, setvl setsvlto mvl. n smaller mvl, compute last nelements final iteration loop, setvl setsvlton.setvl also writes another scalar register help later loop bookkeeping. RV64V code vectorDAXPY value n. vsetdcfg 2 DP FP # Enable 2 64b Fl.Pt. registers fld f0,a # Load scalar loop: setvl t0,a0 # vl = t0 = min(mvl,n) vld v0,x5 # Load vector Xslli t1,t0,3 # t1 = vl * 8 (in bytes)add x5,x5,t1 # Increment pointer X vl*8vmul v0,v0,f0 # Vector-scalar multvld v1,x6 # Load vector Yvadd v1,v0,v1 # Vector-vector addsub a0,a0,t0 # n /C0= vl (t0) vst v1,x6 # Store sum add x6,x6,t1 # Increment pointer vl*8bnez a0,loop # Repeat n != 0vdisable # Disable vector regs Predicate Registers: Handling Statements Vector Loops Amdahl ’s law, know speedup programs low moderate levels vectorization limited. presence conditionals (IF state-ments) inside loops use sparse matrices two main reasons lowerlevels vectorization. Programs contain statements loops cannot run296 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesin vector mode using techniques discussed statements introduce control dependences loop. Likewise, cannot imple-ment sparse matrices efficiently using capabilities seen far.We examine strategies dealing conditional execution here, leaving dis-cussion sparse matrices later. Consider following loop written C: (i = 0; <64; i=i+1) (X[i] != 0) X[i] = X[i] –Y[i]; loop cannot normally vectorized conditional execution body; however, inner loop could run iterations X[i] 6¼ 0, subtraction could vectorized. common extension capability vector-mask control . RV64V, predicate registers hold mask essentially provide conditional execution ofeach element operation vector instruction. registers use Boolean vector control execution vector instruction, conditionally executed instructions use Boolean condition determine whether execute scalarinstruction (see Chapter 3 ). predicate register p0is set, following vec- tor instructions operate vector elements whose corresponding entries inthe predicate register 1. entries destination vector register cor-respond 0 mask register unaffected vector operation. Likevector registers, predicate registers configured disabled. Enablinga predicate register initializes 1 s, meaning subsequent vector instruc- tions operate vector elements. use following code previous loop, assuming starting addresses XandYare x5and x6, respectively: vsetdcfg 2*FP64 # Enable 2 64b FP vector regs vsetpcfgi 1 # Enable 1 predicate registervld v0,x5 # Load vector X v0vld v1,x6 # Load vector v1fmv.d.x f0,x0 # Put (FP) zero f00 .. (m−1)m .. (m−1) +MVL(m+MVL) .. (m−1) +2×MVL(m+2×MVL) .. (m−1) +3×MVL. . . (n−MVL) .. (n−1)Range iValue j n/MVL 123 . . . 0 . . .. . . Figure 4.6 vector arbitrary length processed strip mining. blocks first length MVL, utilizing full power vector processor. figure, weuse variable mfor expression ( n % MVL ). (The C operator %is modulo.)4.2 Vector Architecture ■297vpne p0,v0,f0 # Set p0(i) 1 v0(i)!=f0 vsub v0,v0,v1 # Subtract vector mask vst v0,x5 # Store result Xvdisable # Disable vector registersvpdisable # Disable predicate registers Compiler writers use term IF-conversion transform statement straight-line code sequence using conditional execution. Using vector-mask register overhead, however. scalar archi- tectures, conditionally executed instructions still require execution time whenthe condition satisfied. Nonetheless, elimination branch asso-ciated control dependences make conditional instruction faster even itsometimes useless work. Similarly, vector instructions executed vectormask still take execution time, even elements mask zero. Likewise, despite significant number zeros mask, using vector-mask control may still significantly faster using scalar mode. see Section 4.4 , one difference vector processors GPUs way handle conditional statements. Vector processors make thepredicate registers part architectural state rely compilers manipulatemask registers explicitly. contrast, GPUs get effect using hardware tomanipulate internal mask registers invisible GPU software. cases,the hardware spends time execute vector element whether corresponding mask bit 0 1, GFLOPS rate drops masks used. Memory Banks: Supplying Bandwidth Vector Load/Store Units behavior load/store vector unit significantly complicated arithmetic functional units. start-up time load time toget first word memory register. rest vector sup-plied without stalling, vector initiation rate equal rate newwords fetched stored. Unlike simpler functional units, initiation rate maynot necessarily 1 clock cycle memory bank stalls reduce effective throughput. Typically, penalties start-ups load/store units higher arithmetic units —over 100 clock cycles many processors. R V 6 4 V ,w ea u eas r - u pt eo f1 2c l c kc c l e ,t h es ea st h eC r -1. (Recent vector computers use caches bring latency vector loadsand stores.) maintain initiation rate one word fetched stored per clock cycle, memory system must capable producing accepting much data. Spreading accesses across ultiple independent memor banks usually delivers desired rate. soon see, h aving significant numbers banks useful dealing vector loads stores access rows columnsof data.298 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesMost vector processors use memory banks, allow several independent accesses rather simple memory interleaving three reasons: 1.Many vector computers support many loads stores per clock cycle, memory bank cycle time usually several times larger processor cycletime. support simultaneous accesses multiple loads stores, mem-ory system needs multiple banks needs able control addresses tothe banks independently. 2.Most vector processors support ability load store data words sequential. cases, independent bank addressing, rather inter-leaving, required. 3.Most vector computers support multiple processors sharing memory system, processor generating separate stream ofaddresses. combination, features lead desire large number independent memory banks, following example shows. Example largest configuration Cray T90 (Cray T932) 32 processors, capa- ble generating 4 loads 2 stores per clock cycle. processor clock cycle is2.167 ns, cycle time SRAMs used memory system 15 ns.Calculate minimum number memory banks required allow processors run full memory bandwidth. Answer maximum number memory references cycle 192: 32 processors times 6 references per processor. SRAM bank busy 15/2.167 ¼6.92 clock cycles, round 7 processor clock cycles. Therefore requirea minimum 192 /C27¼1344 memory banks! Cray T932 actually 1024 memory banks, early models could sustain full bandwidth processors simultaneously. subsequent memory upgrade replaced 15 ns asynchronous SRAMs pipelined synchronous SRAMs halved memory cycle time, thereby providing sufficientbandwidth. Taking higher-level perspective, vector load/store units play similar role prefetch units scalar processors try deliver data bandwidth bysupplying processors streams data. Stride: Handling Multidimensional Arrays Vector Architectures position memory adjacent elements vector may sequential. Consider straightforward code matrix multiply C:4.2 Vector Architecture ■299for (i = 0; <100; i=i+1) (j = 0; j <100; j=j+1) { A[i][j] = 0.0;for (k = 0; k <100; k=k+1) A[i][j] = A[i][j] + B[i][k] * D[k][j]; } could vectorize multiplication row B column strip-mine inner loop k index variable. so, must consider address adjacent elements B adjacent elements D. array allocated memory, linearized must laidout either row-major order (as C) column-major order (as Fortran). Thislinearization means either elements row elements col-umn adjacent memory. example, preceding C code allocates inrow-major order, elements accessed iterations innerloop separated row size times 8 (the number bytes per entry) totalof 800 bytes. Chapter 2 , saw blocking could improve locality cache- based systems. vector processors without caches, need another technique fetch elements vector adjacent memory. distance separating elements gathered single vector register called stride . example, matrix stride 100 double words (800 bytes), matrix B would stride 1 double word (8 bytes). column-major order, used Fortran, strides would reversed. Matrix Dwould stride 1, 1 double word (8 bytes), separating successiveelements, matrix B would stride 100, 100 double words (800 bytes). Thus, without reordering loops, compiler ’t hide long distances successive elements B D. vector loaded vector register, acts logically adja- cent elements. Thus vector processor handle strides greater one, callednonunit strides , using vector load vector store operations stride capability. ability access nonsequential memory locations reshapethem dense structure one major advantages vectorarchitecture. Caches inherently deal unit-stride data; increasing block size help reduce miss rates large scientific datasets unit stride, increasing blocksize even negative effect data accessed nonunit strides.While blocking techniques solve problems (see Chapter 2 ), ability access noncontiguous data efficiently remains advantage vectorprocessors certain problems, see Section 4.7 . RV64V, addressable unit byte, stride example would 800. value must computed dynamically size matrix may known compile time —just like vector length —may change different executions statement. vector stride, likethe vector starting address, put general-purpose register. theRV64V instruction VLDS (load vector stride) fetches vector vector300 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesregister. Likewise, storing nonunit stride vector, use instruction VSTS (store vector stride). Supporting strides greater one complicates memory system. introduce nonunit strides, becomes possible request accesses samebank frequently. multiple accesses contend bank, memory bankconflict occurs, thereby stalling one access. bank conflict thus stall willoccur Number banks Least common multiple Stride, Number banks ðÞ<Bank busy time Example Suppose 8 memory banks bank busy time 6 clocks total memory latency 12 cycles. long take complete 64-elementvector load stride 1? stride 32? Answer number banks larger bank busy time, stride 1, load take 12+64 ¼76 clock cycles, 1.2 clock cycles per element. worst possible stride value multiple number memory banks, inthis case stride 32 8 memory banks. Every access memory (after first one) collide previous access wait 6-clock- cycle bank busy time. total time 12+1+6 * 63 ¼391 clock cycles, 6.1 clock cycles per element, slowing factor 5! Gather-Scatter: Handling Sparse Matrices Vector Architectures previously mentioned, sparse matrices commonplace, important techniques allow programs sparse matrices execute vector mode.In sparse matrix, elements vector usually stored compacted form accessed indirectly. Assuming simplified sparse structure, might see code looks like this: (i = 0; <n; i=i+1) A[K[i]] = A[K[i]] + C[M[i]]; code implements sparse vector sum arrays C, using index vec- tors K designate nonzero elements C. (A C must thesame number nonzero elements —nof —soKandMare size.) primary mechanism supporting sparse matrices gather-scatter oper- ations using index vectors. goal operations support moving compressed representation (i.e., zeros included) normalrepresentation (i.e., zeros included) sparse matrix. gather operation takes index vector fetches vector whose elements addresses given adding base address offsets given index vector. result isa dense vector vector register. elements operated dense4.2 Vector Architecture ■301form, sparse vector stored expanded form scatter store, using index vector. Hardware support operations called gather-scat- ter, appears nearly modern vector processors. RV64V instructions arevldi (load vector indexed gather) vsti (store vector indexed scatter). example, x5,x6,x7, x28 contain starting addresses vectors previous sequence, code inner loop vectorinstructions as: vsetdcfg 4*FP64 # 4 64b FP vector registers vld v0, x7 # Load K[]vldx v1, x5, v0) # Load A[K[]]vld v2, x28 # Load M[]vldi v3, x6, v2) # Load C[M[]]vadd v1, v1, v3 # Add themvstx v1, x5, v0) # Store A[K[]] vdisable # Disable vector registers technique allows code sparse matrices run vector mode. simple vectorizing compiler could automatically vectorize precedingsource code compiler would know elements K aredistinct values, thus dependences exist. Instead, programmer directive would tell compiler safe run loop vector mode. Although indexed loads stores (gather scatter) pipelined, typically run much slowly nonindexed loads stores, mem-ory banks known start instruction. register file must alsoprovide communication lanes vector unit support gather andscatter. element gather scatter individual address, ’tb e handled groups, conflicts many places throughout mem- ory system. Thus individual access incurs significant latency even cache- based systems. However, Section 4.7 shows, memory system deliver better performance designing case using hardware resourcesversus architects laissez-faire attitude toward unpredictableaccesses. see Section 4.4 , loads gathers stores scatters GPUs separate instructions restrict addresses sequential. turn thepotentially slow gathers scatters efficient unit-stride accesses memory, GPU hardware must recognize sequential addresses execution GPU programmer ensure addresses gatheror scatter adjacent locations. Programming Vector Architectures advantage vector architectures compilers tell programmers atcompile time whether section code vectorize not, often giving hints302 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesas vectorize code. straightforward execution model allows experts domains learn improve performance revisingtheir code giving hints compiler ’s okay assume indepen- dence operations, gather-scatter data transfers. thisdialogue compiler programmer, side giving hintsto improve performance, simplifies programming vectorcomputers. Today, main factor affects success program runs vector mode structure program itself: loops true datadependences (see Section 4.5 ), restructured dependences? factor influenced algorithms chosen and, someextent, coded. indication level vectorization achievable scientific programs, let’s look vectorization levels observed Perfect Club benchmarks. Figure 4.7 shows percentage operations executed vector mode two versions code running Cray Y-MP. first version obtained compiler optimization original code, second version usesextensive hints team Cray Research programmers. Several studies theperformance applications vector processors show wide variation levelof compiler vectorization.Benchmark nameOperations executed vector mode, compiler-optimizedOperations executed vector mode, programmer aidSpeedup hint optimization BDNA 96.1% 97.2% 1.52 MG3D 95.1% 94.5% 1.00FLO52 91.5% 88.7% N/AARC3D 91.1% 92.0% 1.01SPEC77 90.3% 90.4% 1.07MDG 87.7% 94.2% 1.49TRFD 69.8% 73.7% 1.67DYFESM 68.8% 65.6% N/AADM 42.9% 59.6% 3.60OCEAN 42.8% 91.2% 3.92TRACK 14.4% 54.6% 2.52SPICE 11.5% 79.9% 4.06QCD 4.2% 75.1% 2.15 Figure 4.7 Level vectorization among Perfect Club benchmarks exe- cuted Cray Y-MP ( Vajapeyam, 1991 ).The first column shows vectorization level obtained compiler without hints, second column shows theresults codes improved hints team Cray Research programmers.4.2 Vector Architecture ■303The hint-rich versions show significant gains vectorization level codes compiler could vectorize well itself, codes 50% vectorization. median vectorization improved 70% 90%. 4.3 SIMD Instruction Set Extensions Multimedia SIMD Multimedia Extensions started simple observation many mediaapplications operate narrower data types 32-bit processors opti- mized for. Graphics systems would use 8 bits represent three primary colors plus 8 bits transparency. Depending application, audio samples areusually represented 8 16 bits. partitioning carry chains within, say, a256-bit adder, processor could perform simultaneous operations short vectorsof thirty-two 8-bit operands, sixteen 16-bit operands, eight 32-bit operands, four64-bit operands. additional cost partitioned adders small. Figure 4.8 summarizes typical multimedia SIMD instructions. Like vector instructions, aSIMD instruction specifies operation vectors data. Unlike vector machines large register files RISC-V RV64V vector registers, hold, say, thirty-two 64-bit elements 32 vector registers, SIMDinstructions tend specify fewer operands thus use much smaller register files. contrast vector architectures, offer elegant instruction set intended target vectorizing compiler, SIMD extensions threemajor omissions: vector length register, strided gather/scatter data transferinstructions, mask registers. 1.Multimedia SIMD extensions fix number data operands opcode, led addition hundreds instructions MMX, SSE,and AVX extensions x86 architecture. Vector architectures avector-length register specifies number operands current oper-ation. variable-length vector registers easily accommodate programs thatnaturally shorter vectors maximum size architecture supports.Moreover, vector architectures implicit maximum vector length Instruction category Operands Unsigned add/subtract Thirty-two 8-bit, sixteen 16-bit, eight 32-bit, four 64-bit Maximum/minimum Thirty-two 8-bit, sixteen 16-bit, eight 32-bit, four 64-bitAverage Thirty-two 8-bit, sixteen 16-bit, eight 32-bit, four 64-bitShift right/left Thirty-two 8-bit, sixteen 16-bit, eight 32-bit, four 64-bitFloating point Sixteen 16-bit, eight 32-bit, four 64-bit, two 128-bit Figure 4.8 Summary typical SIMD multimedia support 256-bit-wide opera- tions. Note IEEE 754-2008 floating-point standard added half-precision (16- bit) quad-precision (128-bit) floating-point operations.304 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesarchitecture, combined vector length register avoids use many opcodes. 2.Up recently, multimedia SIMD offer sophisticated addressing modes vector architectures, namely strided accesses gather-scatter accesses. features increase number programs vector compiler successfully vectorize (see Section 4.7 ). 3.Although changing, multimedia SIMD usually offer mask reg- isters support conditional execution elements vector architectures. omissions make harder compiler generate SIMD code increase difficulty programming SIMD assembly language. x86 architecture, MMX instructions added 1996 repurposed 64-bit floating-point registers, basic instructions could perform eight 8-bitoperations four 16-bit operations simultaneously. joined parallel MAX MIN operations, wide variety masking conditional instructions, operations typically found digital signal processors, ad hoc instructions thatwere believed useful important media libraries. Note MMX reused thefloating-point data-transfer instructions access memory. Streaming SIMD Extensions (SSE) successor 1999 added 16 separate registers ( XMM registers) 128 bits wide, instructions could simul- taneously perform sixteen 8-bit operations, eight 16-bit operations, four 32-bitoperations. also performed parallel single-precision floating-point arithmetic. SSE separate registers, needed separate data transfer instructions. Intel soon added double-precision SIMD floating-point data types via SSE2 in2001, SSE3 2004, SSE4 2007. Instructions four single-precisionfloating-point operations two parallel double-precision operations increasedthe peak floating-point performance x86 computers, long programmersplaced operands side side. generation, also added ad hocinstructions whose aim accelerate specific multimedia functions perceivedto important. Advanced Vector Extensions (AVX), added 2010, doubled width registers 256 bits ( YMM registers) thereby offered instructions double number operations narrower data types. Figure 4.9 shows AVX instructions useful double-precision floating-point computations. AVX2 in2013 added 30 new instructions gather ( VGATHER ) vector shifts (VPSLL ,VPSRL ,VPSRA ). AVX-512 2017 doubled width 512 bits (ZMM registers), doubled number registers 32, added 250 new instructions including scatter ( VPSCATTER ) mask registers (OPMASK ). AVX includes preparations extend registers 1024 bits future editions architecture. general, goal extensions accelerate carefully written libraries rather compiler generate (see Appendix H), recentx86 compilers trying generate code, particularly floating-point-intensive applications. Since opcode determines width SIMD regis-ter, every time width doubles, must number SIMD instructions.4.3 SIMD Instruction Set Extensions Multimedia ■305Given weaknesses, multimedia SIMD extensions popular? First, initially cost little add th e standard arithmetic unit easy implement. Second, require scant extra processor state compared tovector architectures, always concern context switch times. Third,you need lot memory bandwidth su pport vector architecture, many computers ’t have. Fourth, SIMD deal problems virtual memory single instruction generate 32 memory accessesand cause page fault. original SIMD extensions usedseparate data transfers p er SIMD group operands aligned memory, cannot cross page boundaries. Another advantage short, fixed- length “vectors ”of SIMD easy introdu ce instructions help new media standards, instru ctions perform permutations instructions consume either fewer operands vectors pro-duce. Finally, concern well vector architectures workwith caches. recent vector architectures addressed prob-lems. overarching issue, however, due overiding importance ofbackwards binary compatability, n architecture gets started SIMD path ’sv e r yh r dt og e f fi . Example get idea multimedia instructions look like, assume added 256-bit SIMD multimedia instruction extension RISC-V, tentatively calledRVP “packed. ”We concentrate floating-point example. add suffix “4D”on instructions operate four double-precision operands once. Like vector architectures, think SIMD Processor havinglanes, four case. RV64P expands F registers full width, thiscase 256 bits. example shows RISC-V SIMD code DAXPY loop,AVX instruction Description VADDPD Add four packed double-precision operands VSUBPD Subtract four packed double-precision operands VMULPD Multiply four packed double-precision operands VDIVPD Divide four packed double-precision operands VFMADDPD Multiply add four packed double-precision operands VFMSUBPD Multiply subtract four packed double-precision operands VCMPxx Compare four packed double-precision operands EQ, NEQ, LT, LE, GT, GE, … VMOVAPD Move aligned four packed double-precision operands VBROADCASTSD Broadcast one double-precision operand four locations 256-bit register Figure 4.9 AVX instructions x86 architecture useful double-precision floating-point programs. Packed- double 256-bit AVX means four 64-bit operands executed SIMD mode. width increases AVX, itis increasingly important add data permutation instructions allow combinations narrow operands different parts wide registers. AVX includes instructions shuffle 32-bit, 64-bit, 128-bit operands within 256-bit register. example, BROADCAST replicates 64-bit operand four times AVX register. AVX alsoincludes large variety fused multiply-add/subtract instructions; show two here.306 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectureswith changes RISC-V code SIMD underlined. assume starting addresses XandYare x5andx6, respectively. Answer RISC-V SIMD code: fld f0,a #Load scalar splat.4D f0,f0 #Make 4 copies addi x28,x5,#256 #Last address load Loop: fld.4D f1,0(x5) #Load X[i] ... X[i+3] fmul.4D f1,f1,f0 #a /C2X[i] ... /C2X[i+3] fld.4D f2,0(x6) #Load Y[i] ... Y[i+3] fadd.4D f2,f2,f1 # /C2X[i]+Y[i]... #a/C2X[i+3]+Y[i+3] fsd.4D f2,0(x6) #Store Y[i]... Y[i+3] addi x5,x5,#32 #Increment index X addi x6,x6,#32 #Increment index Ybne x28,x5,Loop #Check done changes replacing every RISC-V double-precision instruction 4D equivalent, increasing increment 8 32, adding splat instruc- tion makes 4 copies 256 bits f0. dramatic 32/C2reduction dynamic instruction bandwidth RV64V, RISC-V SIMD get almost 4 /C2reduction: 67 versus 258 instructions executed RV64G. code knows number elements. number often determined run time, would require extra strip-mine loop handle case number isnot modulo 4. Programming Multimedia SIMD Architectures Given ad hoc nature SIMD multimedia extensions, easiest way usethese instructions libraries writing assembly language. Recent extensions become regular, giving compilers reason- able target. borrowing techniques vectorizing compilers, compilers arestarting produce SIMD instructions automatically. example, advanced com-pilers today generate SIMD floating-point instructions deliver much higherperformance scientific codes. However, programmers must sure align allthe data memory width SIMD unit code run preventthe compiler generating scalar instructions otherwise vectorizable code. Roofline Visual Performance Model One visual, intuitive way compare potential floating-point performance var-iations SIMD architectures Roofline model ( Williams et al., 2009 ). horizontal diagonal lines graphs produces give simple model itsname indicate value (see Figure 4.11 ). ties together floating-point perfor- mance, memory performance, arithmetic intensity two-dimensional graph.4.3 SIMD Instruction Set Extensions Multimedia ■307Arithmetic intensity ratio floating-point operations per byte memory accessed. calculated taking total number floating-point opera-tions program divided total number data bytes transferred mainmemory program execution. Figure 4.10 shows relative arithmetic intensity several example kernels. Peak floating-point performance found using hardware specifica- tions. Many kernels case study fit on-chip caches, peakmemory performance defined memory system behind caches. Note need peak memory bandwidth available processors, DRAM pins Figure 4.27 page 328. One way find (delivered) peak memory performance run Stream benchmark. Figure 4.11 shows Roofline model NEC SX-9 vector processor left Intel Core i7 920 multicore computer right. vertical Y- axis achievable floating-point performance 2 256 GFLOPS/s. hor-izontal X-axis arithmetic intensity, varying 1/8 FLOP/DRAM byte accessed 16 FLOP/DRAM byte accessed graphs. Note graph log-log scale, Rooflines done computer. given kernel, find point X-axis based arithmetic intensity. drew vertical line point, performance ker-nel computer must lie somewhere along line. plot horizontalline showing peak floating-point performance computer. Obviously, theactual floating-point performance higher horizontal line becausethat hardware limit. could plot peak memory performance? X-axis FLOP/byte Y-axis FLOP/s, bytes/s diagonal line 45-degree angle figure. Thus plot third line gives maximum floating-point performance memory system computer support givenArithmetic intensity O(N) O(log(N)) O(1) Sparse matrix(SpMV) Structuredgrids(Stencils,PDEs )Structuredgrids(Latticemethods )Spectralmethods(FFTs)Densematrix(BLAS3) N-body(Particlemethods) Figure 4.10 Arithmetic intensity, specified number floating-point opera- tions run program divided number bytes accessed main memory (Williams et al., 2009 ).Some kernels arithmetic intensity scales prob- lem size, dense matrix, many kernels arithmetic intensities independent problem size.308 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesarithmetic intensity. express limits formula plot lines graphs Figure 4.11 : Attainable GFLOPs =s¼Min Peak Memory BWð /C2Arithmetic Intensity, Peak Floating /C0Point Perf :Þ “Roofline ”sets upper bound performance kernel depending arithmetic intensity. think arithmetic intensity pole hits roof, either hits flat part roof, means performance computationallylimited, hits slanted part roof, means performance ulti-mately limited memory bandwidth. Figure 4.11 , vertical dashed line right (arithmetic intensity 4) example former verticaldashed line left (arithmetic intensity 1/4) example latter. Givena Roofline model computer, apply repeatedly, ’t vary kernel. Note “ridge point, ”where diagonal horizontal roofs meet, offers interesting insight computer. far right, kernelswith high arithmetic intensity achieve maximum performance thatcomputer. far left, almost kernel potentially hit max-imum performance. see, vector processor much higher1/8 1/2 Arithmetic intensit y1/4 1 2 4 8 16 1/8 1/2 Arithmetic intensit y1/4 1 2 4 8 16256 128 64 32 16 8 4 2256 128 64 32 16 8 4 2Double precision GLFOP/s Double precision GLFOP/sNEC SX-9 CPUIntel Core i7 920 (Nehalem) 102.4 GFLOP/s 42.66 GFLOP/s 162 GB/s(Stream) 16.4 GB/s(Stream) Figure 4.11 Roofline model one NEC SX-9 vector processor left Intel Core i7 920 multicore computer SIMD extensions right ( Williams et al., 2009 ).This Roofline unit-stride memory accesses double-precision floating-point performance. NEC SX-9 vector supercomputer announced 2008 thatcost millions dollars. peak DP FP performance 102.4 GFLOP/s peak memory bandwidth of162 GB/s Stream benchmark. Core i7 920 peak DP FP performance 42.66 GFLOP/s peak memory bandwidth 16.4 GB/s. dashed vertical lines arithmetic intensity 4 FLOP/byte show processors operate peak performance. case, SX-9 102.4 FLOP/s 2.4 /C2faster Core i7 42.66 GFLOP/s. arithmetic intensity 0.25 FLOP/byte, SX-9 10 /C2faster 40.5 GFLOP/s versus 4.1 GFLOP/s Core i7.4.3 SIMD Instruction Set Extensions Multimedia ■309memory bandwidth ridge point far left compared SIMD Processors. Figure 4.11 shows peak computational performance SX-9 2.4/C2faster Core i7, memory performance 10 /C2faster. programs arithmetic intensity 0.25, SX-9 10 /C2faster (40.5 versus 4.1 GFLOP/s). higher memory bandwidth moves ridge point 2.6in Core i7 0.6 SX-9, means many programs reachthe peak computational performance vector processor. 4.4 Graphics Processing Units People buy GPU chip thousands parallel floating-point units fewhundred dollars plug desk side PC. affordability conve-nience makes high performance computing available many. interest inGPU computing blossomed potential combined programminglanguage made GPUs easier program. Therefore many programmers sci- entific multimedia applications today pondering whether use GPUs CPUs. programmers interested machine learning, subject ofChapter 7 , GPUs currently preferred platform. GPUs CPUs go back computer architecture genealogy com- mon ancestor; “missing link ”that explains both. Section 4.10 describes, primary ancestors GPUs graphics accelerators, doinggraphics well reason GPUs exist. GPUs moving towardmainstream computing, ’t abandon responsibility continue excel graphics. Thus design GPUs may make sense architects ask, given hardware invested graphics well, supplement toimprove performance wider range applications? Note section concentrates using GPUs computing. see GPU computing combines traditional role graphics acceleration, see“Graphics Computing GPUs, ”by John Nickolls David Kirk ( Appendix 5th edition Computer Organization Design authors book). terminology hardware features quite different vector SIMD architectures, believe easier start withthe simplified programming model GPUs describe architecture. Programming GPU CUDA elegant solution problem representing parallelismin algorithms, algorithms, enough matter. seems res-onate way way think code, allowing easier,more natural expression parallelism beyond task level. Vincent Natol, “Kudos CUDA, ”HPC Wire (2010)310 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesThe challenge GPU programmer simply getting good perfor- mance GPU, also coordinating scheduling computation system processor GPU transfer data systemmemory GPU memory. Moreover, see see later section,GPUs virtually every type parallelism captured theprogramming environment: multith reading, MIMD, SIMD, even instruction-level. NVIDIA decided develop C-like language programming environment would improve productivity GPU programmers attacking challenges heterogeneous computing multifaceted parallelism. name system CUDA , Compute Unified Device Architecture. CUDA pro- duces C/C++ system processor ( host) C C++ dialect GPU (device , thus CUDA). similar programming language OpenCL , several companies developing offer vendor-independent language mul-tiple platforms. NVIDIA decided unifying theme forms parallelism CUDA Thread . Using lowest level parallelism program- ming primitive, compiler h ardware gang thousands CUDA Threads together utilize various styles parallelism within GPU: mul-tithreading, MIMD, SIMD, instruction-level parallelism. Therefore NVI-DIA classifies CUDA programming model single instruction, multiplethread ( SIMT ). reasons soon see, threads blocked together executed groups threads, called Thread Block . call hardware executes whole block threads multithreaded SIMD Processor . need details give example CUDA program: To distinguish functions GPU (device) functions system processor (host), CUDA uses __device__ or__global__ former __host__ latter. CUDA variables declared __device__ allocated GPU Memory (see below), accessible multithreaded SIMDProcessors. The extended function call syntax function name runs GPU name << < dimGrid ,dimBlock >> > (…parameter list …) dimGrid anddimBlock specify dimensions code (in Thread Blocks) dimensions block (in threads). In addition identifier blocks ( blockIdx ) identifier thread block ( threadIdx ), CUDA provides keyword number threads per block ( blockDim ), comes dimBlock parameter preceding bullet.4.4 Graphics Processing Units ■311Before seeing CUDA code, let ’s start conventional C code DAXPY loop Section 4.2 : // Invoke DAXPYdaxpy(n, 2.0, x, y);// DAXPY Cvoid daxpy(int n, double a, double *x, double *y){ (int = 0; <n; ++i) y[i] = a*x[i] + y[i]; } Following CUDA version. launch nthreads, one per vector element, 256 CUDA Threads per Thread Block multithreaded SIMD Processor. TheGPU function starts calculating corresponding element index ibased block ID, number threads per block, thread ID. long index within array ( i<n), performs multiply add. // Invoke DAXPY 256 threads per Thread Block __host__int nblocks = (n+ 255) / 256; daxpy <<< nblocks, 256 >>> (n, 2.0, x, y); // DAXPY CUDA __global__void daxpy(int n, double a, double *x, double *y){ int = blockIdx.x*blockDim.x + threadIdx.x;if (i <n) y[i] = a*x[i] + y[i]; } Comparing C CUDA codes, see common pattern parallelizing data-parallel CUDA code. C version loop iteration inde-pendent others, allowing loop transformed straightforwardly intoa parallel code loop iteration becomes separate thread. (As previouslymentioned described detail Section 4.5 , vectorizing compilers also rely lack dependences iterations loop, called loop-carried dependences .) programmer determines parallelism CUDA explicitly specifying grid dimensions number threads per SIMD Processor. Byassigning single thread element, need synchronize betweenthreads writing results memory. GPU hardware handles parallel execution thread management; done applications operating system. simplify scheduling thehardware, CUDA requires Thread Blocks able execute independently order. Different Thread Blocks cannot communicate directly, although coordinate using atomic memory operations global memory. soon see, many GPU hardware concepts obvious CUDA. Writing efficient GPU code requires programmers think terms SIMD312 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesoperations, even though CUDA programming model looks like MIMD. Per- formance programmers must keep GPU hardware mind writing CUDA. could hurt programmer productivity, programmersare using GPUs instead CPUs get performance. reasons explained shortly,they know need keep groups 32 threads together control flow toget best performance multithreaded SIMD Processors create manymore threads per multithreaded SIMD Processor hide latency DRAM. Theyalso need keep data addresses localized one blocks memory toget expected memory performance. Like many parallel systems, compromise productivity perfor- mance CUDA include intrinsics give programmers explicit control overthe hardware. struggle productivity one hand versus allowing theprogrammer able express anything hardware otherhand happens often parallel computing. interesting see lan-guage evolves classic productivity-performance battle well seewhether CUDA becomes popular GPUs even architectural styles. NVIDIA GPU Computational Structures uncommon heritage mentioned helps explain GPUs ownarchitectural style terminology independent CPUs. One obsta-cle understanding GPUs jargon, terms even havingmisleading names. obstacle surprisingly difficult overcome, asthe many rewrites chapter attest. try bridge twin goals making architecture GPUs understand- able learning many GPU terms nontraditional definitions, approach use CUDA terminology software initially use moredescriptive terms hardware, sometimes borrowing terms OpenCL.Once explain GPU architecture terms, ’ll map official jargon NVIDIA GPUs. left right, Figure 4.12 lists descriptive term used section, closest term mainstream computing, official NVIDIA GPU jargon case interested, short explanation term. rest section explains microarchitectural features GPUs using descriptive terms theleft figure. use NVIDIA systems example representative GPU archi- tectures. Specifically, follow terminology preceding CUDA parallel pro-gramminglanguageand use theNVIDIAPascal GPU example (see Section 4.7 ). Like vector architectures, GPUs work well data-level parallel problems. styles gather-scatter data transfers mask registers, GPU processors even registers vector processors. Sometimes, GPUs implement certain features hardware vector processors would imple-ment software. difference vector processors scalarprocessor execute software function. Unlike vector architectures,4.4 Graphics Processing Units ■313TypeDescriptive nameClosest old term outside GPUsOfficial CUDA/NVIDIAGPU term Short explanationProgram abstractionsVectorizableLoopVectorizable Loop Grid vectorizable loop, executed GPU, made one Thread Blocks (bodies vectorized loop) execute parallel Body Vectorized LoopBody (Strip- Mined) Vectorized LoopThread Block vectorized loop executed multithreaded SIMD Processor, made one threads SIMD instructions. communicate via localmemory Sequence SIMD Lane OperationsOne iteration Scalar LoopCUDA Thread vertical cut thread SIMD instructions corresponding one element executed one SIMD Lane. Result stored depending mask andpredicate registerMachine objectA Thread ofSIMD InstructionsThread Vector InstructionsWarp traditional thread, contains SIMD instructions executed multithreaded SIMD Processor. Results stored depending per- element mask SIMD InstructionVector Instruction PTX InstructionA single SIMD instruction executed across SIMD LanesProcessing hardwareMultithreadedSIMD Processor(Multithreaded) Vector ProcessorStreaming MultiprocessorA multithreaded SIMD Processor executes threads SIMD instructions, independent SIMD Processors Thread Block SchedulerScalar Processor Giga Thread EngineAssigns multiple Thread Blocks (bodies vectorized loop) multithreaded SIMD Processors SIMD Thread SchedulerThread Scheduler MultithreadedCPUWarp SchedulerHardware unit schedules issues threads SIMD instructions ready execute;includes scoreboard track SIMD Thread execution SIMD Lane Vector Lane Thread ProcessorA SIMD Lane executes operations thread SIMD instructions single element. Resultsstored depending maskMemory hardwareGPU Memory Main Memory Global MemoryDRAM memory accessible multithreaded SIMD Processors GPU Private Memory Stack Thread Local Storage (OS)Local Memory Portion DRAM memory private SIMD Lane Local Memory Local Memory Shared MemoryFast local SRAM one multithreaded SIMD Processor, unavailable SIMD Processors SIMD Lane RegistersVector Lane RegistersThread Processor RegistersRegisters single SIMD Lane allocated across full Thread Block (body vectorized loop) Figure 4.12 Quick guide GPU terms used chapter. use first column hardware terms. Four groups cluster 11 terms. top bottom: program abstractions, machine objects, processing hardware, memory hardware. Figure 4.21 page 312 associates vector terms closest terms here, Figure 4.24 page 317 Figure 4.25 page 318 reveal official CUDA/NVIDIA AMD terms definitions along terms used OpenCL.314 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesGPUs also rely multithreading within single multithreaded SIMD Processor hide memory latency (see Chapters 2and3). However, efficient code vec- tor architectures GPUs requires programmers think groups SIMDoperations. AGrid code runs GPU consists set Thread Blocks . Figure 4.12 draws analogy grid vectorized loop Thread Block body loop (after strip-mined, afull computation loop). give concrete example, let ’s suppose want mul- tiply two vectors together, 8192 elements long: A=B*C .W e ’ll return example throughout section. Figure 4.13 shows relationship example first two GPU terms. GPU code works whole8192 element multiply called Grid (or vectorized loop). break manageable sizes, Grid composed Thread Blocks (or body vectorized loop), 512 elements. Note SIMD instructionexecutes 32 elements time. 8192 elements vectors, examplethus 16 Thread Blocks 16 ¼8192/C4512. Grid Thread Block programming abstractions implemented GPU hardware help program- mers organize CUDA code. (The Thread Block analogous strip-mined vector loop vector length 32.) Thread Block assigned processor executes code, call multithreaded SIMD Processor , Thread Block Scheduler . programmer tells Thread Block Scheduler, implemented hardware,how many Thread Blocks run. example, would send 16 Thread Blocksto multithreaded SIMD Processors compute 8192 elements loop Figure 4.14 shows simplified block diagram multithreaded SIMD Processor. similar vector processor, many parallel functional units instead deeply pipelined, vector processor. pro-gramming example Figure 4.13 , multithreaded SIMD Processor assigned 512 elements vectors work on. SIMD Processors full processors withseparate PCs programmed using threads (see Chapter 3 ). GPU hardware contains collection multithreaded SIMD Proces- sors execute Grid Thread Blocks (bodies vectorized loop); is, aGPU multiprocessor composed multithreaded SIMD Processors. GPU one several dozen multithreaded SIMD Processors. example, Pascal P100 system 56, smaller chips may asone two. provide transparent scalability across models GPUs differingnumber multithreaded SIMD Processors, Thread Block Scheduler assignsThread Blocks (bodies vectorized loop) multithreaded SIMD Processors.Figure 4.15 shows floor plan P100 implementation ofthe Pascal architecture. Dropping one level detail, machine object hardware creates, manages, schedules, executes thread SIMD instructions .I ti sa traditional thread contains exclusively SIMD instructions. threads SIMD instructions PCs, run multithreaded SIMDProcessor. SIMD Thread Scheduler knows threads SIMD instruc- tions ready run sends dispatch unit run on4.4 Graphics Processing Units ■315Figure 4.13 mapping Grid (vectorizable loop), Thread Blocks (SIMD basic blocks), threads SIMD instructions vector-vector multiply, vector 8192 elements long. thread SIMD instruc- tions calculates 32 elements per instruction, example, Thread Block contains 16 threads SIMD instructions Grid contains 16 Thread Blocks. hardware Thread Block Scheduler assigns Thread Blocksto multithreaded SIMD Processors, hardware Thread Scheduler picks thread SIMD instructionsto run clock cycle within SIMD Processor. SIMD Threads Thread Block communicate via local memory. (The maximum number SIMD Threads execute simultaneously per Thread Block 32 Pascal GPUs.)316 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesthe multithreaded SIMD Processor. Thus GPU hardware two levels hard- ware schedulers: (1) Thread Block Scheduler assigns Thread Blocks (bod- ies vectorized loops) multithreaded SIMD Processors (2) SIMDThread Scheduler within SIMD Processor, schedules threads SIMD instructions run. SIMD instructions threads 32 wide, thread SIMD instructions example would compute 32 elements computation. example, Thread Blocks would contain 512/32 ¼16 SIMD Threads (see Figure 4.13 ). thread consists SIMD instructions, SIMD Processor must parallel functional units perform operation. call SIMD Lanes , quite similar Vector Lanes Section 4.2 .Instruction cache Instruction register Regi- sters 1K×32 Load store unitLoad store unitLoad store unitLoad store unit Address coalescing unit Interconnection network Local memory 64 KBTo global memoryLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitLoad store unitReg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Reg 1K×32Warp scheduler SIMD lanes (thread processors) Figure 4.14 Simplified block diagram multithreaded SIMD Processor. 16 SIMD Lanes. SIMD Thread Scheduler has, say, 64 independent threads SIMD instructions schedules table 64 program counters(PCs). Note lane 1024 32-bit registers.4.4 Graphics Processing Units ■317With Pascal GPU, 32-wide thread SIMD instructions mapped 16 physical SIMD Lanes, SIMD instruction thread SIMD instruc- tions takes 2 clock cycles complete. thread SIMD instructions executed lock step scheduled beginning. Staying anal- ogy SIMD Processor vector processor, could say 16 lanes, vector length 32, chime 2 clock cycles. (This wide shallow nature use accurate term SIMD Processor rather vector.) Note number lanes GPU SIMD Processor anything number threads Thread Block, number lanes vector processor vary 1 maximum vector length. example, across GPU gen- erations, number lanes per SIMD Processor fluctuated 8 32. definition threads SIMD instructions independent, SIMD Thread Scheduler pick whatever thread SIMD instructions ready, need stick next SIMD instruction sequence within thread. SIMD Thread Scheduler includes scoreboard (see Chapter 3 ) keep track 64 threads SIMD instructions see SIMD instruction ready go. latency memory instructions variable hits misses caches TLB, thus requirement scoreboard determine instructions complete. Figure 4.16 shows SIMD Thread Scheduler picking threads SIMD instructions different order time. assumption GPU architects GPU applications many threads SIMD instruc- tions multithreading hide latency DRAM increase utiliza- tion multithreaded SIMD Processors. Figure 4.15 Full-chip block diagram Pascal P100 GPU. 56 multithreaded SIMD Processors, L1 cache local memory, 32 L2 units, memory-bus width 4096 data wires. (It 60 blocks, four spares improve yield.) P100 4 HBM2 ports supporting 16 GB capacity. contains 15.4 billion transistors.318 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesContinuing vector multiply example, multithreaded SIMD Processor must load 32 elements two vectors memory registers, perform multiply reading writing registers, store product back reg- isters memory. hold memory elements, SIMD Processor impressive 32,768 –65,536 32-bit registers (1024 per lane Figure 4.14 ), depending model Pascal GPU. like vector pro- cessor, registers divided logically across Vector Lanes or, case, SIMD Lanes. SIMD Thread limited 256 registers, might think SIMD Thread 256 vector registers, vector register 32 elements element 32 bits wide. (Because double- precision floating-point operands use two adjacent 32-bit registers, alternative view SIMD Thread 128 vector registers 32 elements, 64 bits wide.) trade-off register use maximum number threads; fewer registers per thread means threads possible, registers SIMD thread 8 instruction 11 SIMD thread 1 instruction 42 SIMD thread 3 instruction 95 SIMD thread 8 instruction 12TimeSIMD thread scheduler SIMD thread 1 instruction 43SIMD thread 3 instruction 96 Photo: Jud Schoonmake r Figure 4.16 Scheduling threads SIMD instructions. scheduler selects ready thread SIMD instructions issues instruction synchronously SIMD Lanes executing SIMD Thread. threads SIMD instructions indepen- dent, scheduler may select different SIMD Thread time.4.4 Graphics Processing Units ■319mean fewer threads. is, SIMD Threads need maximum number registers. Pascal architects believe much precious silicon area would idle threads maximum number registers. able execute many threads SIMD instructions, dynamically allocated set physical registers SIMD Processor threads ofSIMD instructions created freed SIMD Thread exits. example,a programmer Thread Block uses 36 registers per thread with, say, 16SIMD Threads alongside another Thread Block 20 registers per thread with32 SIMD Threads. Subsequent Thread Blocks may show order, registers allocated demand. variability lead fragmen- tation make registers unavailable, practice Thread Blocks use thesame number registers given vectorizable loop ( “grid”). hardware must know registers Thread Block large register file, thisis recorded per Thread-Block basis. flexibility requires routing, arbitration,and banking hardware specific register given Thread Blockcould end location register file. Note CUDA Thread vertical cut thread SIMD instructions, corresponding one element executed one SIMD Lane. Beware CUDA Threads different POSIX Threads; ’t make arbitrary system calls CUDA Thread. We’re ready see GPU instructions look like. NVIDA GPU Instruction Set Architecture Unlike system processors, instruction set target NVIDIA compilers abstraction hardware instruction set. PTX (Parallel Thread Execution ) provides stable instruction set compilers well compatibility across gen-erations GPUs. hardware instruction set hidden programmer.PTX instructions describe operations single CUDA Thread usuallymap one-to-one hardware instructions, one PTX instruction expandto many machine instructions, vice versa. PTX uses unlimited number ofwrite-once registers compiler must run register allocation procedure tomap PTX registers fixed number read-write hardware registers available actual device. optimizer runs subsequently reduce register use even further. optimizer also eliminates dead code, folds instructions together,and calculates places branches might diverge places divergedpaths could converge. Although similarity x86 microarchitecture PTX, translate internal form (microinstructions x86), differenceis translation happens hardware runtime execution x86versus software load time GPU. format PTX instruction opcode.type d, a, b, c; dis destination operand; a,b, care source operands; operation type one following:320 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesSource operands 32-bit 64-bit registers constant value. Destinations registers, except store instructions. Figure 4.17 shows basic PTX instruction set. instructions pred- icated 1-bit predicate registers, set set predicate instruction(setp ). control flow instructions functions call andreturn , thread exit ,branch , barrier synchronization threads within Thread Block (bar.sync) . Placing predicate front branch instruction gives us con- ditional branches. compiler PTX programmer declares virtual registers as32-bit 64-bit typed untyped values. example, R0,R1,... 32-bit values RD0,RD1,... 64-bit registers. Recall assignment virtual registers physical registers occurs load time PTX. following sequence PTX instructions one iteration DAXPY loop page 292: shl.u32 R8, blockIdx, 8 ; Thread Block ID * Block size ;(256 2 8) add.u32 R8, R8, threadIdx ; R8 = = CUDA Thread ID shl.u32 R8, R8, 3 ; byte offset ld.global.f64 RD0, [X+R8]; RD0 = X[i]ld.global.f64 RD2, [Y+R8]; RD2 = Y[i]mul.f64 RD0, RD0, RD4 ; Product RD0 = RD0 * RD4 ; (scalar a) add.f64 RD0, RD0, RD2 ; Sum RD0 = RD0 + RD2 (Y[i])st.global.f64 [Y+R8], RD0; Y[i] = sum (X[i]*a + Y[i]) demonstrated above, CUDA programming model assigns one CUDA Thread loop iteration offers unique identifier number Thread Block(blockIdx ) one CUDA Thread within block ( threadIdx ). Thus creates 8192 CUDA Threads uses unique number address elementwithin array, incrementing branching code. first three PTXinstructions calculate unique element byte offset R8, added base arrays. following PTX instructions load two double-precisionfloating-point operands, multiply add them, store sum. (We ’ll describe PTX code corresponding CUDA code “if (i <n)”below.) Note unlike vector architectures, GPUs ’t separate instructions sequential data transfers, strided data transfers, gather-scatter data transfers.Type .type specifier Untyped bits 8, 16, 32, 64 bits .b8,.b16 ,.b32 ,.b64 Unsigned integer 8, 16, 32, 64 bits .u8,.u16 ,.u32 ,.u64 Signed integer 8, 16, 32, 64 bits .s8,.s16 ,.s32 ,.s64 Floating Point 16, 32, 64 bits .f16 ,.f32 ,.f644.4 Graphics Processing Units ■321Group Instruction Example Meaning Comments Arithmeticarithmetic .type = .s32, .u32, .f32, .s64, .u64, .f64 add.type add.f32 d, a, b d=a+b ; sub.type sub.f32 d, a, b = –b; mul.type mul.f32 d, a, b d=a*b ; mad.type mad.f32 d, a, b, c d=a*b+c ; multiply-add div.type div.f32 d, a, b d=a/b ; multiple microinstructions rem.type rem.u32 d, a, b d=a%b ; integer remainder abs.type abs.f32 d, = jaj; neg.type neg.f32 d, = 0 –a; min.type min.f32 d, a, b = (a <b)? a:b; floating selects non-NaN max.type max.f32 d, a, b = (a >b)? a:b; floating selects non-NaN setp.cmp.type setp.lt.f32 p, a, b p = (a <b); compare set predicate numeric .cmp = eq, ne, lt, le, gt, ge; unordered cmp = equ, neu, ltu, leu, gtu, geu, num, nanmov.type mov.b32 d, = a; move selp.type selp.f32 d, a, b, p = p? a: b; select predicate cvt.dtype.atype cvt.f32.s32 d, = convert(a); convert atype dtype Special functionspecial .type = .f32 (some .f64) rcp.type rcp.f32 d, = 1/a; reciprocal sqrt.type sqrt.f32 d, = sqrt(a); square root rsqrt.type rsqrt.f32 d, = 1/sqrt(a); reciprocal square root sin.type sin.f32 d, = sin(a); sine cos.type cos.f32 d, = cos(a); cosine lg2.type lg2.f32 d, = log(a)/log(2) binary logarithm ex2.type ex2.f32 d, d=2* *a ; binary exponential Logicallogic.type = .pred,.b32, .b64 and.type and.b32 d, a, b d=a&b ; or.type or.b32 d, a, b = jb; xor.type xor.b32 d, a, b = ^b; not.type not.b32 d, a, b = /C24a; one’s complement cnot.type cnot.b32 d, a, b = (a==0)? 1:0; C logical shl.type shl.b32 d, a, b = <<b; shift left shr.type shr.s32 d, a, b = >>b; shift right Memory accessmemory.space = .global, .shared, .local, .const; .type = .b8, .u8, .s8, .b16, .b32, .b64 ld.space.type ld.global.b32 d, [a+off] = *(a+off); load memory space st.space.type st.shared.b32 [d+off], *(d+off) = a; store memory space tex.nd.dtyp.btype tex.2d.v4.f32.f32 d, a, b = tex2d(a, b); texture lookup atom.spc.op.type atom.global.add.u32 d,[a], b atom.global.cas.b32 d,[a], b, catomic {d=* ; *a = op(*a, b); }atomic read-modify-writeoperation atom.op = and, or, xor, add, min, max, exch, cas; .spc = .global; .type = .b32 Control flowbranch @p bra target (p) goto target; conditional branch call call (ret), func, (params) ret = func(params); call function ret ret return; return function call bar.sync bar.sync wait threads barrier synchronization exit exit exit; terminate thread execution Figure 4.17 Basic PTX GPU thread instructions.322 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesAll data transfers gather-scatter! regain efficiency sequential (unit- stride) data transfers, GPUs include special Address Coalescing hardware rec- ognize SIMD Lanes within thread SIMD instructions collectivelyissuing sequential addresses. runtime hardware notifies MemoryInterface Unit request block transfer 32 sequential words. get impor-tant performance improvement, GPU programmer must ensure adjacentCUDA Threads access nearby addresses time coa-lesced one memory cache blocks, example does. Conditional Branching GPUs like case unit-stride data transfers, strong similaritiesbetween vector architectures GPUs handle statements, formerimplementing mechanism largely software limited hardware supportand latter making use even hardware. see, addition explicit predicate registers, GPU branch hardware uses internal masks, branch synchronization stack, instruction markers manage branch divergesinto multiple execution paths paths converge. PTX assembler level, control flow one CUDA Thread described PTX instructions branch, call, return, exit, plus individual per-thread-lanepredication instruction, specified programmer per-thread-lane1-bit predicate registers. PTX assembler analyzes PTX branch graph andoptimizes fastest GPU hardware instruction sequence. make decision branch need lock step. GPU hardware instruction level, control flow includes branch, jump, jump indexed, call, call indexed, return, exit, special instructions managethe branch synchronization stack. GPU hardware provides SIMD Thread withits stack; stack entry contains identifier token, target instruction address,and target thread-active mask. GPU special instructions push stackentries SIMD Thread special instructions instruction markers popa stack entry unwind stack specified entry branch target instruction address target thread-active mask. GPU hardware instructions also individual per-lane predication (enable/disable), specified 1-bitpredicate register lane. PTX assembler typically optimizes simple outer-level IF-THEN-ELSE statement coded PTX branch instructions solely predicated GPU instruc-tions, without GPU branch instructions. complex control flow oftenresults mixture predication GPU branch instructions specialinstructions markers use branch synchronization stack push stack entry lanes branch target address, others fall through. NVI- DIA says branch diverges happens. mixture also used SIMD Lane executes synchronization marker converges , pops stack entry branches stack-entry address stack-entry thread-active mask.4.4 Graphics Processing Units ■323The PTX assembler identifies loop branches generates GPU branch instructions branch top loop, along special stack instructions handle individual lanes breaking loop converging SIMD Laneswhen lanes completed loop. GPU indexed jump indexed callinstructions push entries stack lanes complete switchstatement function call, SIMD Thread converges. GPU set predicate instruction ( setp inFigure 4.17 ) evaluates condi- tional part statement. PTX branch instruction depends thatpredicate. PTX assembler generates predicated instructions GPU branch instructions, uses per-lane predicate register enable disable SIMD Lane instruction. SIMD instructions threads inside theTHEN part statement broadcast operations SIMD Lanes. Thoselanes predicate set 1 perform operation store result, theother SIMD Lanes ’t perform operation store result. ELSE statement, instructions use complement predicate (relative theTHEN statement), SIMD Lanes idle perform operationand store result formerly active siblings ’t. end ELSE statement, instructions unpredicated original computation proceed. Thus, equal length paths, IF-THEN-ELSE operates 50%efficiency less. statements nested, thus use stack, PTX assembler typically generates mix predicated instructions GPU branch specialsynchronization instructions complex control flow. Note deep nestingcan mean SIMD Lanes idle execution nested conditionalstatements. Thus, doubly nested statements equal-length paths run 25% efficiency, triply nested 12.5% efficiency, on. analogous case would vector processor operating mask bitsare ones. Dropping level detail, PTX assembler sets “branch synchro- nization ”marker appropriate conditional branch instructions pushes current active mask stack inside SIMD Thread. conditional branchdiverges (some lanes take branch fall through), pushes stack entryand sets current internal active mask based condition. branch synchro- nization marker pops diverged branch entry flips mask bits ELSE portion. end statement, PTX assembler adds anotherbranch synchronization marker pops prior active mask stack intothe current active mask. mask bits set 1, branch instruction end skips instructions ELSE part. similar optimizationfor part case mask bits 0 conditional branchjumps instructions. Parallel statements PTX branches often use branch conditions unanimous (all lanes agree follow path) SIMD Thread diverge different individual lane controlflow. PTX assembler optimizes branches skip blocks instruc-tions executed lane SIMD Thread. optimization is324 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesuseful conditional error checking, example, test must made rarely taken. code conditional statement similar one Section 4.2 (X[i] != 0) X[i] = X[i] –Y[i]; else X[i] = Z[i]; statement could compile following PTX instructions (assuming thatR8already scaled thread ID), *Push ,*Comp ,*Pop indicating branch synchronization markers inserted PTX assembler push old mask, complement current mask, pop restore old mask: ld.global.f64 RD0, [X+R8] ; RD0 = X[i] setp.neq.s32 P1, RD0, #0 ;P1 predicate reg 1@!P1, bra ELSE1, *Push ; Push old mask, set new ; mask bits P1 false, go ELSE1 ld.global.f64 RD2, [Y+R8] ; RD2 = Y[i] sub.f64 RD0, RD0, RD2 ; Difference RD0 st.global.f64 [X+R8], RD0 ; X[i] = RD0@P1, bra ENDIF1, *Comp ; complement mask bits ; P1 true, go ENDIF1 ELSE1: ld.global.f64 RD0, [Z+R8] ; RD0 = Z[i] st.global.f64 [X+R8], RD0 ; X[i] = RD0 ENDIF1: <next instruction >,*Pop ; pop restore old mask again, normally instructions IF-THEN-ELSE statement exe- cuted SIMD Processor. ’s SIMD Lanes enabled instructions lanes ELSE instructions. previouslymentioned, surprisingly common case individual lanes agree thepredicated branch —such branching parameter value lanes active mask bits 0s 1s —the branch skips instructions ELSE instructions. flexibility makes appear element program counter; however, slowest case, one SIMD Lane could store result every 2 clock cycles, rest idle. analogous slowest case vector architec-tures operating one mask bit set 1. flexibility lead naiveGPU programmers poor performance, helpful early stagesof program development. Keep mind, however, choice aSIMD Lane clock cycle perform operation specified PTXinstruction idle; two SIMD Lanes cannot simultaneously execute differentinstructions. flexibility also helps explain name CUDA Thread given ele- ment thread SIMD instructions, gives illusion acting inde-pendently. naive programmer may think thread abstraction means GPUshandle conditional branches gracefully. threads go one way, rest go4.4 Graphics Processing Units ■325another, seems true long ’re hurry. CUDA Thread either executing instruction every thread Thread Block idle. synchronization makes easier handle loops conditionalbranches mask capability turn SIMD Lanes detectsthe end loop automatically. resulting performance sometimes belies simple abstraction. Writing programs operate SIMD Lanes highly independent MIMD mode likewriting programs use lots virtual address space computer smallerphysical memory. correct, may run slowly program- mer pleased result. Conditional execution case GPUs runtime hardware vec- tor architectures compile time. Vector compilers double IF-conversion,generating four different masks. execution basically GPUs, butthere overhead instructions executed vectors. Vector architec-tures advantage integrated scalar processor, allowing themto avoid time 0 cases dominate calculation. Although willdepend speed scalar processor versus vector processor, cross- point ’s better use scalar might less 20% mask bits 1s. One optimization available runtime GPUs, compile timefor vector architectures, skip ELSE parts mask bits all0s 1s. Thus efficiency GPUs execute conditional statements comes frequently branches diverge. example, one calculationof eigenvalues deep conditional nesting, measurements code showthat around 82% clock cycle issues 29 32 32 mask bits set 1, GPUs execute code efficiently one might expect. Note mechanism handles strip-mining vector loops —when number elements ’t perfectly match hardware. example beginning section shows statement checks see SIMDLane element number (stored R8in preceding example) less limit (i<n), sets masks appropriately. NVIDIA GPU Memory Structures Figure 4.18 shows memory structures NVIDIA GPU. SIMD Lane multithreaded SIMD Processor given private section off-chip DRAM,which call private memory . used stack frame, spilling registers, private variables ’t fit registers. SIMD Lanes notshare private memories. GPUs cache private memory L1 L2 caches aid register spilling speed function calls. call on-chip memory local multithreaded SIMD Proces- sorlocal memory . small scratchpad memory low latency (a dozen clocks) high bandwidth (128 bytes/clock) programmer storedata needs reused, either thread another thread same326 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesThread Block. Local memory limited size, typically 48 KiB. carries state Thread Blocks executed processor. shared theSIMD Lanes within multithreaded SIMD Processor, memory notshared multithreaded SIMD Processors. multithreaded SIMDProcessor dynamically allocates portions local memory Thread Blockwhen creates Thread Block, frees memory threads theThread Block exit. portion local memory private Thread Block. Finally, call off-chip DRAM shared whole GPU Thread Blocks GPU Memory . vector multiply example used GPU Memory. system processor, called host, read write GPU Memory. Local memory unavailable host, private multithreaded SIMDProcessor. Private memories unavailable host well.CUDA thread Thread block Per-block local memory Grid 0 . . . Grid 1 . . . GPU memorySequence Inter-grid synchronizationPer-CUDA thread private memory Figure 4.18 GPU memory structures. GPU memory shared Grids (vectorized loops), local memory shared threads SIMD instructions within Thread Block(body vectorized loop), private memory private single CUDA Thread.Pascal allows preemption Grid, requires local private memory able saved restored global memory. completeness sake, GPU also access CPU memory via PCIe bus. path commonly usedfor final result address host memory. option eliminates final copy GPU memory host memory.4.4 Graphics Processing Units ■327Rather rely large caches contain whole working sets application,GPUs traditionally use smaller streaming cachesand, work- ing sets hundreds megabytes, rely extensive multithreading threads ofSIMD instructions hide long latency DRAM. Given use multithreadingto hide DRAM latency, chip area used large L2 L3 caches system pro-cessors spent instead computing resources large number ofregisters tohold state many threads SIMD instructions. contrast, mentioned, vectorloads stores amortize latency across many elements pay thelatency pipeline rest accesses. Although hiding memory latency behind many threads original philos- ophy GPUs vector processors, recent GPUs vector processors havecaches reduce latency. argument follows Little ’s Law queuing theory: longer latency, threads need run memory access, whichin turn requires registers. Thus GPU caches added lower averagelatency thereby mask potential shortages number registers. improve memory bandwidth reduce overhead, mentioned, PTX data transfer instructions cooperation memory controller coalesce individual parallel thread requests SIMD Thread together single memory block request addresses fall block. restrictions areplaced GPU program, somewhat analogous guidelines system pro-cessor programs engage hardware prefetching (see Chapter 2 ). GPU mem- ory controller also hold requests send ones together open pageto improve memory bandwidth (see Section 4.6 ).Chapter 2 describes DRAM sufficient detail readers understand potential benefits grouping relatedaddresses. Innovations Pascal GPU Architecture multithreaded SIMD Processor Pascal complicated simpli-fied version Figure 4.20 . increase hardware utilization, SIMD Processor two SIMD Thread Schedulers, multiple instruction dispatch units (some GPUs four thread schedulers). dual SIMD Thread Scheduler selects two threads SIMD instructions issues one instruction eachto two sets 16 SIMD Lanes, 16 load/store units, 8 special function units. Withmultiple execution units available, two threads SIMD instructions scheduledeach clock cycle, allowing 64 lanes active. threads indepen-dent, need check data dependences instruction stream. Thisinnovation would analogous multithreaded vector processor issuevector instructions two independent threads. Figure 4.19 shows Dual Scheduler issuing instructions, Figure 4.20 shows block diagram multithreaded SIMD Processor Pascal GP100 GPU. new generation GPU typically adds new features increase performance make easier programmers. four main innovationsof Pascal:328 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectures■Fast single-precision, double-precision, half-precision floating-point arithmetic —Pascal GP100 chip significant floating-point performance three sizes, part IEEE standard floating-point. single-precision floating-point GPU runs peak 10 TeraFLOP/s.Double-precision roughly half-speed 5 TeraFLOP/s, half-precisionis double-speed 20 TeraFLOP/s expressed 2-element vectors.The atomic memory operations include floating-point add three sizes.Pascal GP100 first GPU high performance half-precision. ■High-bandwidth memory —The next innovation Pascal GP100 GPU use stacked, high-bandwidth memory ( HBM2 ). memory wide bus 4096 data wires running 0.7 GHz offering peak bandwidth 732 GB/s, twice fast previous GPUs. ■High-speed chip-to-chip interconnect —Given coprocessor nature GPUs, PCI bus communications bottleneck trying usemultiple GPUs one CPU. Pascal GP100 introduces NVLink commu- nications channel supports data transfers 20 GB/s direc-tion. GP100 4 NVLink channels, providing peak aggregate chip-to-chip bandwidth 160 GB/s per chip. Systems 2, 4, 8 GPUs areavailable multi-GPU applications, GPU perform load, store,and atomic operations GPU connected NVLink. Additionally, NVLink channel communicate CPU cases. example, IBM Power9 CPU supports CPU-GPU communication. chip,NVLink provides coherent view memory GPUs CPUsconnected together. also provides cache-to-cache communication insteadof memory-to-memory communication.SIMD thread scheduler Instruction dispatch unit SIMD thread 8 instruction 11 SIMD thread 2 instruction 42 SIMD thread 14 instruction 95 SIMD thread 8 instruction 12Time SIMD thread 2 instruction 43SIMD thread 14 instruction 96SIMD thread scheduler Instruction dispatch unit SIMD thread 9 instruction 11 SIMD thread 3 instruction 33 SIMD thread 15 instruction 95 SIMD thread 9 instruction 12 SIMD thread 15 instruction 96SIMD thread 3 instruction 34 Figure 4.19 Block diagram Pascal ’s dual SIMD Thread scheduler. Compare design single SIMD Thread design Figure 4.16 .4.4 Graphics Processing Units ■329■Unified virtual memory paging support —The Pascal GP100 GPU adds page-fault capabilities within unified virtual address space. featureallows single virtual address every data structure identical acrossall GPUs CPUs single system. thread accesses address remote, page memory transferred local GPU subsequent use. Unified memory simplifies programming model providing demandpaging instead explicit memory copying CPU GPU orInstruction Cache Texture / L1 Cache 64KB Shared MemoryDispatch UnitsDispatch UnitsDispatch UnitsDispatch UnitsSIMD Thread Scheduler Register File (32,768 × 32-bit)SIMD Thread SchedulerInstruction Buffer Instruction Buffer SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFUDispatch UnitsDispatch UnitsDispatch UnitsDispatch Units Register File (32,768 × 32-bit) SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU SIMD LaneSIMD LaneDP UnitSIMD LaneSIMD LaneDP UnitLD/ST SFU Tex Tex Tex Tex Figure 4.20 Block diagram multithreaded SIMD Processor Pascal GPU. 64 SIMD Lanes (cores) pipelined floating-point unit, pipelined integer unit, logic dispatching instructions oper-ands units, queue holding results. 64 SIMD Lanes interact 32 double-precision ALUs (DP units) perform 64-bit floating-point arithmetic, 16 load-store units (LD/STs), 16 special function units (SFUs) calculate functions square roots, reciprocals, sines, cosines.330 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesbetween GPUs. also allows allocating far memory exists GPU solve problems large memory requirements. virtual memory system, care must taken avoid excessive page movement. Similarities Differences Vector Architectures GPUs seen, really many similarities vector architectures GPUs. Along quirky jargon GPUs, similarities contrib-uted confusion architecture circles novel GPUs really are. Nowthat ’ve seen covers vector computers GPUs, appreciate similarities differences. architectures aredesigned execute data-level parallel programs, take different paths, com- parison depth order provide better understanding needed DLP hardware. Figure 4.21 shows vector term first closest equiv- alent GPU. SIMD Processor like vector processor. multiple SIMD Processors GPUs act independent MIMD cores, many vector computers havemultiple vector processors. view consider NVIDIA Tesla P100 asa 56-core machine hardware support multithreading, corehas 64 lanes. biggest difference multithreading, fundamental GPUs missing vector processors. Looking registers two architectures, RV64V register file implementation holds entire vectors —that is, contiguous block elements. contrast, single vector GPU distributed across registers SIMDLanes. RV64V processor 32 vector registers perhaps 32 elements, or1024 elements total. GPU thread SIMD instructions 256 registers with32 elements each, 8192 elements. extra GPU registers supportmultithreading. Figure 4.22 block diagram execution units vector processor left multithreaded SIMD Processor GPU right. pedagogicpurposes, assume vector processor four lanes multithreadedSIMD Processor also four SIMD Lanes. figure shows four SIMDLanes act concert much like four-lane vector unit, SIMD Processoracts much like vector processor. reality, many lanes GPUs, GPU “chimes ”are shorter. vector processor might 2 8 lanes vector length of, say, 32 — making chime 4 16 clock cycles —a multithreaded SIMD Processor might 8 16 lanes. SIMD Thread 32 elements wide, GPU chime would 2or 4 clock cycles. difference use “SIMD Processor ”as descriptive term closer SIMD design traditional vec-tor processor design.4.4 Graphics Processing Units ■331Type Vector termClosest CUDA/NVIDIA GPU term CommentProgram abstractionsVectorizedLoopGrid Concepts similar, GPU using less descriptive term Chime — vector instruction (PTX instruction) takes 2 cycles Pascal complete, chime short GPUs. Pascal twoexecution units support common floating-point instructions used alternately, effective issue rate 1 instruction every clock cycleMachine objectsVectorInstructionPTX Instruction PTX instruction SIMD Thread broadcast SIMD Lanes, similar vector instruction Gather/ ScatterGlobal load/store (ld. global/st.global)All GPU loads stores gather scatter, SIMD Lane sends unique address. ’s GPU Coalescing Unit get unit-stride performance addresses SIMD Lanesallow Mask RegistersPredicate Registers Internal Mask RegistersVector mask registers explicitly part architectural state, GPU mask registers internal hardware. GPU conditional hardware adds new feature beyond predicate registers manage masks dynamicallyProcessing memory hardwareVectorProcessorMultithreaded SIMD ProcessorThese similar, SIMD Processors tend many lanes, taking clock cycles per lane complete vector, whilevector architectures lanes take many cycles complete vector. also multithreaded vectors usually Control ProcessorThread Block Scheduler closest Thread Block Scheduler assigns Thread Blocks multithreaded SIMD Processor. GPUs noscalar-vector operations unit-stride strided data transfer instructions, Control Processors often provide vector architectures Scalar ProcessorSystem Processor lack shared memory high latency communicate PCI bus (1000s clock cycles), system processor GPU rarely takes tasks scalar processor vector architecture Vector Lane SIMD Lane similar; essentially functional units registersVector RegistersSIMD Lane Registers equivalent vector register register 16 SIMD Lanes multithreaded SIMD Processor running threadof SIMD instructions. number registers per SIMD Thread flexible, maximum 256 Pascal, maximum number vector registers 256 Main MemoryGPU Memory Memory GPU versus system memory vector case Figure 4.21 GPU equivalent vector terms.332 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesThe closest GPU term vectorized loop Grid, PTX instruction closest vector instruction SIMD Thread broadcasts PTX instructionto SIMD Lanes. respect memory access instructions two architectures, GPU loads gather instructions GPU stores scatter instructions. dataaddresses CUDA Threads refer nearby addresses fall samecache/memory block time, Address Coalescing Unit GPUwill ensure high memory bandwidth. explicit unit-stride load store instruc- tions vector architectures versus implicit unit stride GPU programming writing efficient GPU code requires programmers think terms SIMDoperations, even though CUDA programming model looks like MIMD.Because CUDA Threads generate addresses, strided well asgather-scatter, addressing vectors found vector architectures GPUs.PC Mask FU0 0123 4 60 61 62 6356         7FU1 FU2 FU3Mask MaskInstruction cache Instruction register Vector load/store unit Memory interface unitControl processor Vector registersMask Mask Mask Mask Mask 0000 1 1023 1023 1023 102311         1Instruction cache Instruction register SIMD load/store unitRegisters Address coalescing unit Memory interface unitSIMD thread scheduler Dispatch unit PCPCPCPC FU0 FU1 FU2 FU3 Figure 4.22 vector processor four lanes left multithreaded SIMD Processor GPU four SIMD Lanes right. (GPUs typically 16 32 SIMD Lanes.) Control Processor supplies scalar operands scalar-vector operations, increments addressing unit nonunit stride accesses memory, performs accounting-type operations. Peak memory performance occurs GPU Address Coalescing Unit discover localized addressing. Similarly, peak computational performance occurs internal mask bitsare set identically. Note SIMD Processor one PC per SIMD Thread help multithreading.4.4 Graphics Processing Units ■333As mentioned several times, two architectures take different approaches hiding memory latency. Vector architectures amortize across elements vector deeply pipelined access, pay thelatency per vector load store. Therefore vector loads stores arelike block transfer memory vector registers. contrast, GPUshide memory latency using multithreading. (Some researchers investigatingadding multithreading vector architectures try capture best bothworlds.) respect conditional branch instructions, architectures implement using mask registers. conditional branch paths occupy time and/or space even store result. difference vector compilermanages mask registers explicitly software GPU hardware assem-bler manages implicitly using branch synchronization markers inter-nal stack save, complement, restore masks. Control Processor vector computer plays important role exe- cution vector instructions. broadcasts operations Vector Lanes andbroadcasts scalar register value vector-scalar operations. also implicit calculations explicit GPUs, automatically incrementing memory addresses unit-stride nonunit-stride loads stores. Control Processoris missing GPU. closest analogy Thread Block Scheduler, whichassigns Thread Blocks (bodies vector loop) multithreaded SIMD Processors.The runtime hardware mechanisms GPU generate addresses thendiscover adjacent, commonplace many DLP applications, arelikely less power-efficient using Control Processor. scalar processor vector computer executes scalar instructions vector program; is, performs operations would slow vector unit. Although system processor associated GPU theclosest analogy scalar processor vector architecture, separate addressspaces plus transferring PCIe bus means thousands clock cycles ofoverhead use together. scalar processor slower vectorprocessor floating-point computations vector computer, sameratio system processor versus multithreaded SIMD Processor (given theoverhead). Therefore “vector unit ”in GPU must computations would expect using scalar processor vector computer. is, rather thancalculate system processor communicate results, fasterto disable one SIMD Lane using predicate registers built-in masksand scalar work one SIMD Lane. relatively simple scalar pro-cessor vector computer likely faster power-efficient thanthe GPU solution. system processors GPUs become closely tiedtogether future, interesting see system processors play role scalar processors vector multimedia SIMD architectures.334 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesSimilarities Differences Multimedia SIMD Computers GPUs high level, multicore computers multimedia SIMD instruction extensions share similarities GPUs. Figure 4.23 summarizes similarities differences. multiprocessors whose processors use multiple SIMD Lanes, although GPUs processors many lanes. use hardware multithread-ing improve processor utilization, although GPUs hardware support formany threads. roughly 2:1 performance ratios peak perfor-mance single-precision double-precision floating-point arithmetic. use caches, although GPUs use smaller streaming caches, multicore computers use large multilevel caches try contain whole working sets completely. Bothuse 64-bit address space, although physical main memory much smaller inGPUs. support memory protection page level well demand paging,which allows address far memory board. addition large numerical differences processors, SIMD Lanes, hard- ware thread support, cache sizes, many architectural differences. Thescalar processor multimedia SIMD instructions tightly integrated tradi- tional computers; separated I/O bus GPUs, even separate main memories. multiple SIMD Processors GPU use singleaddress space support coherent view memory systemsgiven support CPU vendors (such IBM Power9). Unlike GPUs, mul-timedia SIMD instructions historically support gather-scatter memoryaccesses, Section 4.7 shows significant omission. Feature Multicore SIMD GPU SIMD Processors 4 –88 –32 SIMD Lanes/Processor 2 –4u p 6 4 Multithreading hardware support SIMD Threads 2 –4u p 6 4 Typical ratio single-precision double-precision performance 2:1 2:1Largest cache size 40 MB 4 MBSize memory address 64-bit 64-bitSize main memory 1024 GB 24 GBMemory protection level page Yes YesDemand paging Yes YesIntegrated scalar processor/SIMD Processor Yes NoCache coherent Yes Yes systems Figure 4.23 Similarities differences multicore multimedia SIMD extensions recent GPUs.4.4 Graphics Processing Units ■335Summary veil lifted, see GPUs really multi- threaded SIMD Processors, although processors, lanesper processor, multithreading hardware traditional multicorecomputers. example, Pascal P100 GPU 56 SIMD Processors with64 lanes per processor hardware support 64 SIMD Threads. Pascalembraces instruction-level parallelism issuing instructions two SIMDThreads two sets SIMD Lanes. GPUs also less cache memory —Pas- cal’s L2 cache 4 MiB —and coherent cooperative distant scalar processor distant GPUs. CUDA programming model wraps forms parallelism around single abstraction, CUDA Thread. Thus CUDA programmercan think programming thousands threads, although really executingeach block 32 threads many lanes many SIMD Processors. TheCUDA programmer wants good performance keeps mind threadsare organized blocks executed 32 time addresses need toadjacent addresses get good performance memory system. Although ’ve used CUDA NVIDIA GPU section, rest assured ideas found OpenCL programming language GPUsfrom companies. understand better GPUs work, reveal real jargon. Figures 4.24 and4.25 match descriptive terms definitions section official CUDA/NVIDIA AMD terms definitions. also includethe OpenCL terms. believe GPU learning curve steep part ofusing terms “streaming multiprocessor ”for SIMD Processor, “thread processor ”for SIMD Lane, “shared memory ”for local memory — especially local memory notshared SIMD Processors! hope two-step approach gets curve quicker, even ’sa bit indirect. 4.5 Detecting Enhancing Loop-Level Parallelism Loops programs fountainhead many types parallelism wepreviously discussed Chapter 5 . section, discuss compiler technology used discovering amount parallelism exploitin program well hardware support compiler techniques. defineprecisely loop parallel (or vectorizable), dependence prevent aloop parallel, techniques eliminating types depen-dences. Finding manipulating loop-level parallelism critical exploiting DLP TLP, well aggressive static ILP approaches (e.g., VLIW) examine Appendix H.336 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesLoop-level parallelism normally investigated source level close it, analysis ILP done instructions generated compiler. Loop-level analysis involves determining dependences exist among operands loop across iterations thatloop. now, consider data dependences, arise anoperand written point rea later point. Name dependences also exist may removed renaming techniques discussed inChapter 3 . analysis loop-level parallelism focuses determining whether data accesses later iterations dependent data values produced earlier itera- tions; dependence called loop-carried dependence . examplesTypeMore descriptivename used inthis bookOfficial CUDA/NVIDIAtermShort explanation AMD OpenCL terms Official CUDA/NVIDIA definitionProgram abstractionsVectorizable loopGrid vectorizable loop, executed GPU, made one “Thread Blocks ”(or bodies vectorized loop) execute parallel. OpenCLname “index range. ”AMD name “NDRange ”A Grid array Thread Blocks execute concurrently, sequentially, mixture Body VectorizedloopThread BlockA vectorized loop executed multithreaded SIMD Processor, madeup one threads SIMDinstructions. SIMD Threads communicate via local memory. AMD OpenCL name “work group ”A Thread Block array CUDA Threads execute concurrentlyand cooperate communicatevia shared memory barrier synchronization. Thread Block Thread Block ID within Grid Sequence SIMD LaneoperationsCUDA ThreadA vertical cut thread SIMD instructions corresponding oneelement executed one SIMD Lane. Result stored depending mask. AMD OpenCL call CUDA Threada“work item ”A CUDA Thread lightweight thread executes sequentialprogram cooperate CUDA Threads executing Thread Block. CUDAThread thread ID within Thread BlockMachine objectA thread SIMDinstructionsWarp traditional thread, contains SIMD instructions executed amultithreaded SIMD Processor. Resultsare stored depending per-element mask. AMD name “wavefront ”A warp set parallel CUDA Threads (e.g., 32) execute thesame instruction together amultithreaded SIMT/SIMD Processor SIMD instructionPTX instructionA single SIMD instruction executed across SIMD Lanes. AMD name is“AMDIL ”or“FSAIL ”instructionA PTX instruction specifies instruction executed CUDAThread Figure 4.24 Conversion terms used chapter official NVIDIA/CUDA AMD jargon. OpenCL names given book ’s definitions.4.5 Detecting Enhancing Loop-Level Parallelism ■337TypeMore descriptivename used inthis bookOfficial CUDA/NVIDIA termShort explanation AMD OpenCL terms Official CUDA/NVIDIA definitionProcessing hardwareMultithreadedSIMD processorStreaming multiprocessorMultithreaded SIMD Processor executes thread SIMD instructions, independent SIMD Processors. AMD andOpenCL call “compute unit. ” However, CUDA programmer writes program one lane rather “vector ”of multiple SIMD LanesA streaming multiprocessor (SM) multithreaded SIMT/SIMD Processor executes warps CUDA Threads. SIMT programspecifies execution oneCUDA Thread, rather vector multiple SIMD Lanes Thread Block SchedulerGiga Thread EngineAssigns multiple bodies vectorized loop multithreaded SIMD Processors. AMD name is“Ultra-Threaded Dispatch Engine ”Distributes schedules Thread Blocks grid streaming multiprocessors resourcesbecome available SIMD Thread schedulerWarp schedulerHardware unit schedules issues threads SIMD instructions ready execute; includes scoreboard track SIMDThread execution. AMD name is“Work Group Scheduler ”A warp scheduler streaming multiprocessor schedules warps execution next instruction ready execute SIMD Lane Thread processorHardware SIMD Lane executes operations thread SIMD instructions single element.Results stored depending onmask. OpenCL calls “processing element. ”AMD name also “SIMD Lane ”A thread processor datapath register file portion streaming multiprocessor executesoperations one lanes awarpMemory hardwareGPU Memory Global memoryDRAM memory accessible multithreaded SIMD Processors GPU. OpenCL calls “global memory ”Global memory accessible CUDA Threads Thread Block grid; implemented region DRAM, may cached Private memoryLocal memory Portion DRAM memory private SIMD Lane. AMD andOpenCL call “private memory ”Private “thread-local ”memory CUDA Thread; implemented acached region DRAM Local memory Shared memoryFast local SRAM one multithreaded SIMD Processor, unavailable SIMDProcessors. OpenCL calls “local memory. ”AMD calls “group memory ”Fast SRAM memory shared CUDA Threads composing Thread Block, private ThreadBlock. Used communicationamong CUDA Threads Thread Block barrier synchronization points SIMD Lane registersRegisters Registers single SIMD Lane allocated across body vectorizedloop. AMD also calls “registers ”Private registers CUDA Thread; implemented asmultithreaded register file certain lanes several warps thread processor Figure 4.25 Conversion terms used chapter official NVIDIA/CUDA AMD jargon. Note descriptive terms “local memory ”and “private memory ”use OpenCL terminology. NVIDIA uses SIMT (single- instruction multiple-thread) rather SIMD describe streaming multiprocessor. SIMT preferred SIMD per-thread branching control flow unlike SIMD machine.we considered Chapters 2and3had loop-carried dependences thus loop-level parallel. see loop parallel, let us first look source representation: (i=999; >=0; i=i-1) x[i] = x[i] + s; loop, two uses x[i] dependent, dependence within single iteration loop-carried. loop-carried dependence betweensuccessive uses different iterations, dependence involves induc-tion variable easily recognized eliminated. saw examples ofhow eliminate dependences involving induction variables loop unrollingin Section 2.2 Chapter 2 , look additional examples later section. finding loop-level parallelism involves recognizing structures loops, array references, induction variable computations, com-piler analysis easily near source level, contrast tothe machine-code level. Let ’s look complex example. Example Consider loop like one: (i=0; <100; i=i+1) { A[i+1] = A[i] + C[i]; /* S1 */B[i+1] = B[i] + A[i+1]; /* S2 */ } Assume A, B, C distinct, nonoverlapping arrays. (In practice, arrays may sometimes may overlap. arrays may passed asparameters procedure includes loop, determining whether arrays over- lap identical often requires sophisticated, interprocedural analysis pro- gram.) data dependences among statements S1andS2in loop? Answer two different dependences: 1.S1uses value computed S1in earlier iteration, iteration com- putes A[i+1], read iteration i+1. true S2for B[i] B[i+1]. 2.S2uses value A[i+1] computed S1in iteration. two dependences distinct different effects. see differ, let ’s assume one dependences exists time. dependence statement S1is earlier iteration S1, dependence loop-carried. dependence forces successive iterations loop executein series.4.5 Detecting Enhancing Loop-Level Parallelism ■339The second dependence ( S2depending S1) within iteration loop-carried. Thus, dependence, multiple iter-ations loop would execute parallel, long pair state-ments iteration kept order. saw type dependencein example Section 2.2, unrolling could expose parallelism.These intra-loop dependences common; example, sequence vector instructions uses chaining exhibits exactly sort dependence. also possible loop-carried dependence prevent parallelism, next example shows. Example Consider loop like one: (i=0; <100; i=i+1) { A[i] = A[i] + B[i]; /* S1 */B[i+1] = C[i] + D[i]; /* S2 */ } dependences S1andS2? loop parallel? not, show make parallel. Answer Statement S1 uses value assigned previous iteration statement S2,s loop-carried dependence S2andS1. Despite loop-carried dependence, loop made parallel. Unlike earlier loop, depen-dence circular; neither statement depends itself, although S1depends onS2,S2does depend S1. loop parallel written without cycle dependences absence cycle means depen- dences give partial ordering statements. Although circular dependences preceding loop, must transformed conform partial ordering expose parallelism. Twoobservations critical transformation: 1.There dependence S1toS2. were, would cycle dependences loop would parallel. otherdependence absent, interchanging two statements affect exe-cution S2. 2.On first iteration loop, statement S2depends value B[0] computed prior initiating loop. two observations allow us replace preceding loop following code sequence: A[0] = A[0] + B[0]; (i=0; <99; i=i+1) {340 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesB[i+1] = C[i] + D[i]; A[i+1] = A[i+1] + B[i+1]; }B[100] = C[99] + D[99]; dependence two statements longer loop-carried iterations loop may overlapped, provided statements iterationare kept order. analysis needs begin finding loop-carried dependences. depen- dence information inexact , sense tells us dependence may exist. Consider following example: (i=0;i <100;i=i+1) { A[i] = B[i] + C[i]D[i] = A[i] * E[i] } second reference example need translated load instruction know value computed stored previousstatement. Thus second reference simply reference registerinto computed. Performing optimization requires knowing thatthe two references always memory address intervening access location. Normally, data dependence analysis tells one reference may depend another; complex analysis required determine two references must exact address. preceding example, simple version analysis suffices thetwo references basic block. Often loop-carried dependences form recurrence . recurrence occurs variable defined based value variable earlieriteration, usually one immediately preceding, following code fragment: (i=1;i <100;i=i+1) { Y[i] = Y[i-1] + Y[i]; } Detecting recurrence important two reasons: architectures (especially vector computers) special support executing recurrences,and ILP context, may still possible exploit fair amount parallelism. Finding Dependences Clearly, finding dependences program important determinewhich loops might contain parallelism eliminate name dependences. Thecomplexity dependence analysis arises also presence arrays4.5 Detecting Enhancing Loop-Level Parallelism ■341and pointers languages C C++, pass-by-reference parameter pass- ing Fortran. scalar variable references explicitly refer name, usually analyzed quite easily aliasing pointers referenceparameters causing complications uncertainty analysis. compiler detect dependences general? Nearly dependence analysis algorithms work assumption array indices affine . sim- plest terms, one-dimensional array index affine written forma/C2i+b, aandbare constants iis loop index variable. index multidimensional array affine index dimension affine. Sparse array accesses, typically form x[y[i]], one major exam- ples nonaffine accesses. Determining whether dependence two references array loop thus equivalent determining whether two affine functions canhave identical value different indices bounds loop. Forexample, suppose stored array element index value a/C2i+band loaded array index value c/C2i+d, iis for-loop index variable runs mton. dependence exists two conditions hold: 1.There two iteration indices, jandk, within limits for- loop. is, m/C20j/C20n,m/C20k/C20n. 2.The loop stores array element indexed a/C2j+band later fetches thatsame array element indexed c/C2k+d,t h ti , a/C2j+b¼c/C2k+d. general, cannot determine whether dependence exists compile time. example, values a,b,c, may known (they could values arrays), making impossible tell dependence exists. cases, thedependence testing may expensive decidable compile time; forexample, accesses may depend iteration indices multiple nested loops. Many programs, however, contain primarily simple indices a,b,c,a n dare constants. cases, possible devise reasonable compile time testsfor dependence. example, simple sufficient test absence dependence thegreatest common divisor (GCD) test. based observation loop-carried dependence exists, GCD ( c,a) must divide ( d–b). (Recall integer, x,divides another integer, y, get integer quotient division y/xand remainder.) Example Use GCD test determine whether dependences exist following loop: (i=0; <100; i=i+1) { X[2*i+3] = X[2*i] * 5.0; } Answer Given values a¼2,b¼3,c¼2, d¼0, GCD( a,c)¼2, d/C0b¼/C03. 2 divide /C03, dependence possible.342 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesThe GCD test sufficient guarantee dependence exists; however, cases GCD test succeeds dependence exists. arise, forexample, GCD test consider loop bounds. general, determining whether dependence actually exists NP-complete. practice, however, many common cases analyzed precisely low cost. Recently, approaches using hierarchy exact tests increasing generality andcost shown accurate efficient. (A test exact pre- cisely determines whether dependence exists. Although general case isNP-complete, exist exact tests restricted situations much cheaper.) addition detecting presence dependence, compiler wants clas- sify type dependence. classification allows compiler recognizename dependences eliminate compile time renaming copying. Example following loop multiple types dependences. Find true depen- dences, output dependences, antidependences, eliminate outputdependences antidependences renaming. (i=0; <100; i=i+1) { Y[i] = X[i] / c; /* S1 */X[i] = X[i] + c; /* S2 */ Z[i] = Y[i] + c; /* S3 */ Y[i] = c - Y[i]; /* S4 */ } Answer following dependences exist among four statements: 1.There true dependences S1toS3and S1toS4because Y[i]. loop-carried, prevent loop consid-ered parallel. dependences force S3andS4to wait S1 complete. 2.There antidependence S1toS2, based X[i]. 3.There antidependence S3toS4for Y[i]. 4.There output dependence S1toS4, based Y[i]. following version loop eliminates false (or pseudo) dependences. (i=0; <100; i=i+1 { T[i] = X[i] / c; /* renamed remove output dependence */ X1[i] = X[i] + c;/* X renamed X1 remove antidependence */ Z[i] = T[i] + c;/* renamed remove antidependence */ Y[i] = c - T[i]; }4.5 Detecting Enhancing Loop-Level Parallelism ■343After loop, variable X renamed X1. code follows loop, compiler simply replace name X X1. case, renamingdoes require actual copy operation, done substituting namesor register allocation. cases, however, renaming require copying. Dependence analysis critical technology exploiting parallelism, well transformation-like blocking Chapter 2 covers. detecting loop-level parallelism, dependence analysis basic tool. Effectively compiling programsfor vector computers, SIMD computers, multiprocessors depends critically analysis. major drawback dependence analysis applies limited set circumstances, namely, among references within singleloop nest using affine index functions. Thus many situations wherearray-oriented dependence analysis cannot tell us want know; example, analyzing accesses done pointers, rather array indicescan much harder. (This one reason Fortran still preferred Cand C++ many scientific applications designed parallel computers.) Simi-larly, analyzing references across procedure calls extremely difficult. Thus, analysis code written sequential languages remains important, also need approaches OpenMP CUDA write explicitly parallel loops. Eliminating Dependent Computations previously mentioned, one important forms dependent compu-tations recurrence. dot product perfect example recurrence: (i=9999; >=0; i=i-1) sum = sum + x[i] * y[i]; loop parallel loop-carried dependence var- iable sum. can, however, transform set loops, one completely parallel partly parallel. first loop execute completely parallel portion loop. looks like this: (i=9999; >=0; i=i-1) sum[i] = x[i] * y[i]; Notice sum expanded scalar vector quantity (a trans- formation called scalar expansion ) transformation makes new loop completely parallel. done, however, need reduce step, sums elements vector. looks like this: (i=9999; >=0; i=i-1) finalsum = finalsum + sum[i]; Although loop parallel, specific structure called reduc- tion. Reductions common linear algebra, see Chapter 6 ,344 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesthey also key part primary parallelism primitive MapReduce used warehouse-scale computers. general, function used reduction operator, common cases include operators max min. Reductions sometimes handled special hardware vector SIMD architecture allows reduce step done much faster could donein scalar mode. work implementing technique similar bedone multiprocessor environment. general transformation workswith number processors, suppose simplicity 10 processors.In first step reducing sum, processor executes following (with pas processor number ranging 0 9): (i=999; >=0; i=i-1) finalsum[p] = finalsum[p] + sum[i+1000*p]; loop, sums 1000 elements 10 processors, completely parallel. simple scalar loop complete summation ofthe last 10 sums. Similar approaches used vector processors SIMDProcessors. important observe preceding transformation relies associa- tivity addition. Although arithmetic unlimited range precision asso-ciative, computer arithmetic associative, either integer arithmetic,because limited range, floating-point arithmetic, rangeand precision. Thus using restructuring techniques sometimes lead toerroneous behavior, although occurrences rare. reason, mostcompilers require optimizations rely associativity explicitly enabled. 4.6 Cross-Cutting Issues Energy DLP: Slow Wide Versus Fast Narrow fundamental power advantage data-level parallel architectures comes theenergy equation Chapter 1 . Assuming ample data-level parallelism, performance halve clock rate double execution resources: twice num-ber lanes vector computer, wider registers ALUs multimedia SIMD, andmore SIMD Lanes GPUs. lower voltage dropping clockrate, actually reduce energy well asthe power computation main- taining peak performance. Thus GPUs tend lower clock rates system processors, rely high clock rates performance (see Section 4.7 ). Compared out-of-order processors, DLP processors simpler control logic launch large number operations per clock cycle; example, con-trol identical lanes vector processors, logic decide onmultiple instruction issues speculative execution logic. also fetch anddecode far fewer instructions. Vector architectures also make easier turnoff unused portions chip. vector instruction explicitly describes resources needs number cycles instruction issues.4.6 Cross-Cutting Issues ■345Banked Memory Graphics Memory Section 4.2 noted importance substantial memory bandwidth vector architectures support unit stride, nonunit stride, gather-scatter accesses. achieve highest memory performance, stacked DRAMs used top-end GPUs AMD NVIDIA. Intel also uses stacked DRAM XeonPhi product. Also known high bandwidth memory (HBM ,HBM2 ), memory chips stacked placed package processing chip. exten-sive width (typically 1024 –4096 data wires) provides high bandwidth, plac- ing memory chips package processor chip reduces latency power consumption. capacity stacked DRAM typically 8 –32 GB. Given potential demands memory computation tasks graphics acceleration tasks, memory system could see largenumber uncorrelated requests. Unfortunately, diversity hurts memory per-formance. cope, GPU ’s memory controller maintains separate queues traffic bound different banks, waiting enough traffic justifyopening row transferring requested data once. delay improvesbandwidth stretches latency, controller must ensure processing units starve waiting data, otherwise neighboring processors could become idle. Section 4.7 shows gather-scatter techniques memory- bank-aware access techniques deliver substantial increases performanceversus conventional cache-based architectures. Strided Accesses TLB Misses One problem strided accesses interact translation looka-side buffer (TLB) virtual memory vector architectures GPUs. (GPUs alsouse TLBs memory mapping.) Depending TLB organized thesize array accessed memory, even possible get one TLB miss every access element array! type collision happen caches, performance impact probably less. 4.7 Putting Together: Embedded Versus Server GPUs Tesla Versus Core i7 Given popularity graphics applications, GPUs found mobile clients traditional servers heavy-duty desktop computers. Figure 4.26 lists key characteristics NVIDIA Tegra Parker system chip embedded clients, popular automobiles, Pascal GPU servers. GPUserver engineers hope able live animation within five years movieis released. GPU-embedded engineers turn want server gameconsole today hardware within five years. NVIDIA Tegra P1 six ARMv8 cores smaller Pascal GPU (capa- ble 750 GFLOPS) 50 GB/s memory bandwidth. key component346 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesof NVIDIA DRIVE PX2 computing platform used cars autono- mous driving. NVIDIA Tegra X1 previous generation used several high-end tablets, Google Pixel C NVIDIA Shield TV. Maxwell-class GPU capable 512 GFLOPS. NVIDIA Tesla P100 Pascal GPU discussed extensively chap- ter. (Tesla Nvidia ’s name products targeting general-purpose computing.) clock rate 1.4 GHz, includes 56 SIMD Processors. path toHBM2 memory 4096-bits wide, transfers data rising andfalling edge 0.715 GHz clock, means peak memory bandwidthof 732 GB/s. connects host system processor memory via PCI Express /C216 Gen 3 link, peak bidirectional rate 32 GB/s. physical characteristics P100 die impressively large: contains 15.3 billion transistors, die size 645 mm 2in 16-nm TSMC process, typical power 300 W. Comparison GPU MIMD Multimedia SIMD group Intel researchers published paper ( Lee et al., 2010 ) comparing quad- core Intel i7 multimedia SIMD extensions Tesla GTX 280. Althoughthe study compare latest versions CPUs GPUs, mostNVIDIA Tegra 2 NVIDIA Tesla P100 Market Automotive, Embedded, Console, TabletDesktop, server System processor Six-Core ARM (2 Denver2 +4 A57)Not applicable System interface applicable PCI Express /C216 Gen 3 System interface bandwidthNot applicable 16 GB/s (each direction), 32 GB/s (total) Clock rate 1.5 GHz 1.4 GHzSIMD multiprocessors 2 56SIMD Lanes/SIMD multiprocessor128 64 Memory interface 128-bit LP-DDR4 4096-bit HBM2 Memory bandwidth 50 GB/s 732 GB/sMemory capacity 16 GB 16 GBTransistors 7 billion 15.3 billionProcess TSMC 16 nm FinFET TSMC 16 nm FinFETDie area 147 mm 2645 mm2 Power 20 W 300 W Figure 4.26 Key features GPUs embedded clients servers.Comparison GPU MIMD Multimedia SIMD ■347in-depth comparison two styles explained reasons behind differences performance. Moreover, current versions architecturesshare many similarities ones study. Figure 4.27 lists characteristics two systems. products pur- chased fall 2009. Core i7 Intel ’s 45-nanometer semiconductor technology, GPU TSMC ’s 65-nanometer technology. Although might fairer comparison done neutral party bothinterested parties, purpose section notto determine much faster one product other, try understand relative value featuresof two contrasting architecture styles. rooflines Core i7 920 GTX 280 Figure 4.28 illustrate dif- ferences computers. 920 slower clock rate 960 (2.66 GHz vs. 3.2 GHz), rest system same. GTX 280 much higher memory bandwidth double-precision floating-point perfor-mance, also double-precision ridge point considerably left. pre-viously mentioned, much easier hit peak computational performance thefurther ridge point roofline left. double-precision ridge pointCore i7-960 GTX 280 Ratio 280/i7 Number processing elements (cores SMs) 4 30 7.5 Clock frequency (GHz) 3.2 1.3 0.41Die size 263 576 2.2Technology Intel 45 nm TSMC 65 nm 1.6Power (chip, module) 130 130 1.0Transistors 700 1400 2.0Memory bandwidth (GB/s) 32 141 4.4Single-precision SIMD width 4 8 2.0Double-precision SIMD width 2 1 0.5Peak single-precision scalar FLOPS (GFLOP/S) 26 117 4.6Peak single-precision SIMD FLOPS (GFLOP/S) 102 311 –933 3.0 –9.1 (SP 1 add multiply) N.A. (311) (3.0)(SP 1 instruction fused multiply-adds) N.A. (622) (6.1)(Rare SP dual issue fused multiply-add multiply) N.A. (933) (9.1)Peak double-precision SIMD FLOPS (GFLOP/S) 51 78 1.5 Figure 4.27 Intel Core i7-960 NVIDIA GTX 280. rightmost column shows ratios GTX 280 Core i7. single-precision SIMD FLOPS GTX 280, higher speed (933) comes rare case dual issuing fused multiply-add multiply. reasonable 622 single fused multiply-adds. Note memory bandwidths higher Figure 4.28 DRAM pin bandwidths Figure 4.28 processors measured benchmark program. Table 2 Lee, W.V., et al., 2010. Debunking the100/C2GPU vs. CPU myth: evaluation throughput computing CPU GPU. In: Proc. 37th Annual Int ’l. Sym- posium Computer Architecture (ISCA), June 19 –23, 2010, Saint-Malo, France.348 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectures128 64 3216 842 1128 64 3216 842 1Core i7 920 (Nehalem) 1024 512 256128 643216 8 12 Arithmetic intensit y48 1 6 1/8 1/4 1/212 Arithmetic intensity48 1 6 1/8 1/4 1/2 1 2 Arithmetic intensity48 1 6 1/8 1/4 1/2 12 Arithmetic intensit y48 1 6 1/8 1/4 1/2Core i7 920 (Nehalem)NVIDIA GTX280 1024 512 256128 643216 8NVIDIA GTX280Double-precision GFLOP/s Double-precision GFLOP/sSingle-precision GFLOP/s Single-precision GFLOP/sPeak =42.66 GFLOP/s Stream=16.4 GB/s Stream=127 GB/s78 GF/s 78 GF/s Stream=127 GB/s624 GF/s Stream=16.4 GB/s85.33 GF/s 42.66 GF/s Figure 4.28 Roofline model ( Williams et al. 2009 ).These rooflines show double-precision floating-point perfor- mance top row single-precision performance bottom row. (The DP FP performance ceiling also bottom row give perspective.) Core i7 920 left peak DP FP performance 42.66 GFLOP/s, SP FP peak 85.33 GFLOP/s, peak memory bandwidth 16.4 GB/s. NVIDIA GTX 280 DP FP peak of78 GFLOP/s, SP FP peak 624 GFLOP/s, 127 GB/s memory bandwidth. dashed vertical line left represents arithmetic intensity 0.5 FLOP/byte. limited memory bandwidth 8 DP GFLOP/s 8 SP GFLOP/s Core i7. dashed vertical line right arithmetic intensity of4 FLOP/byte. limited computationally 42.66 DP GFLOP/s 64 SP GFLOP/s Core i7 78 DP GFLOP/s 512 DP GFLOP/s GTX 280. hit highest computation rate Core i7, need use 4 cores SSE instructions equal number multiplies adds. GTX 280, needto use fused multiply-add instructions multithreaded SIMD Processors.Comparison GPU MIMD Multimedia SIMD ■349is 0.6 GTX 280 versus 2.6 Core i7. single-precision performance, ridge point moves far right, ’s considerably harder hit roof single-precision performance much higher. Note arithmeticintensity kernel based bytes go main memory, bytesthat go cache memory. Thus caching change arithmetic intensity akernel particular computer, presuming references really go thecache. Rooflines help explain relative performance case study. Notealso bandwidth unit-stride accesses architectures. Realgather-scatter addresses coalesced slower GTX 280 Core i7, see. researchers said selected benchmark programs analyzing computational memory characteristics four recently proposed bench-mark suites “formulated set throughput computing kernels cap- ture characteristics. ”Figure 4.29 describes 14 kernels, Figure 4.30 shows performance results, larger numbers meaning faster. Given raw performance specifications GTX 280 vary 2.5 /C2 slower (clock rate) 7.5 /C2faster (cores per chip) performance varies 2.0 /C2slower (Solv) 15.2 /C2faster (GJK), Intel researchers explored reasons differences: ■Memory bandwidth . GPU 4.4 /C2the memory bandwidth, helps explain LBM SAXPY run 5.0 5.3 /C2faster; working sets hundreds megabytes thus ’t fit Core i7 cache. (To access memory intensively, use cache blocking SAXPY.) Thus theslope rooflines explains performance. SpMV also large work- ing set, runs 1.9 /C2because double-precision floating point GTX 280 1.5 /C2faster Core i7. ■Compute bandwidth . Five remaining kernels compute bound: SGEMM, Conv, FFT, MC, Bilat. GTX faster 3.9, 2.8, 3.0, 1.8, 5.7, respectively. first three use single-precisionfloating-point arithmetic, GTX 280 single-precision 3 –6/C2faster. (The 9 /C2faster Core i7 shown Figure 4.27 occurs special case GTX 280 issue fused multiply-add multiplyper clock cycle.) MC uses double-precision, explains ’s 1.8 /C2 faster since DP performance 1.5 /C2faster. Bilat uses transcendental func- tions, GTX 280 supports directly (see Figure 4.17 ). Core i7 spends two-thirds time calculating transcendental functions, GTX 280 5.7 /C2faster. observation helps point value hardware support operations occur workload: double-precision floating-point perhaps even transcendentals. ■Cache benefits . Ray casting (RC) 1.6 /C2faster GTX cache blocking Core i7 caches prevents becoming memorybandwidth bound, GPUs. Cache blocking help Search, too. Ifthe index trees small fit cache, Core i7 twice350 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesas fast. Larger index trees make memory bandwidth bound. Overall, GTX 280 runs Search 1.8 /C2faster. Cache blocking also helps Sort. programmers ’t run Sort SIMD Processor, written 1-bit Sort primitive called split. However, split algorithm executes many instructions scalar sort does. result, GTX 280 runs only0.8/C2as fast Core i7. Note caches also help kernels Core i7 cache blocking allows SGEMM, FFT, SpMV becomeKernel Application SIMD TLP Characteristics SGEMM ( SGEMM ) Linear algebra Regular Across 2D tilesCompute bound tiling Monte Carlo ( MC) Computational financeRegular Across pathsCompute bound Convolution ( Conv ) Image analysis Regular Across pixelsCompute bound; BW bound small filters FFT ( FFT) Signal processingRegular Across smaller FFTsCompute bound BW bound depending size SAXPY ( SAXPY ) Dot product Regular Across vectorBW bound large vectors LBM ( LBM ) Time migration Regular Across cellsBW bound Constraint solver ( Solv) Rigid body physicsGather/ ScatterAcross constraintsSynchronization bound SpMV ( SpMV ) Sparse solver Gather Across nonzeroBW bound typical large matrices GJK ( GJK ) Collision detectionGather/ ScatterAcross objectsCompute bound Sort ( Sort) Database Gather/ ScatterAcross elementsCompute bound Ray casting ( RC) Volume renderingGather Across rays4–8 MB first level working set; 500 MB last level working set Search ( Search ) Database Gather/ ScatterAcross queriesCompute bound small tree, BW bound bottom tree large tree Histogram ( Hist) Image analysis Requires conflictdetectionAcross pixelsReduction/synchronization bound Bilateral ( Bilat ) Image analysis Regular Across pixelsCompute bound Figure 4.29 Throughput computing kernel characteristics. name parentheses identifies benchmark name section. authors suggest code machines equal optimization effort. Table 1 Lee, W.V., et al., 2010. Debunking 100 /C2GPU vs. CPU myth: evaluation throughput computing CPU GPU. In: Proc. 37th Annual Int ’l. Symposium Computer Architecture (ISCA), June 19 –23, 2010, Saint- Malo, France.Comparison GPU MIMD Multimedia SIMD ■351compute bound. observation reemphasizes importance cache block- ing optimizations Chapter 2 . ■Gather-Scatter . multimedia SIMD extensions little help data scattered throughout main memory; optimal performance comes whendata aligned 16-byte boundaries. Thus GJK gets little benefit fromSIMD Core i7. previously mentioned, GPUs offer gather-scatteraddressing found vector architecture omitted SIMD exten-sions. Address Coalescing Unit helps well combining accesses thesame DRAM line, thereby reducing number gathers scatters. Thememory controller also batches together accesses identical DRAM page. combination means GTX 280 runs GJK startling 15.2 /C2faster Core i7, larger single physical parameter Figure 4.27 . observation reinforces importance gather-scatter vector GPUarchitectures missing SIMD extensions. ■Synchronization . performance synchronization Hist limited atomic updates, responsible 28% total runtime Core i7despite hardware fetch-and-increment instruction. Thus Hist isKernel Units Core i7-960 GTX 280GTX 280/ i7-960 SGEMM GFLOP/s 94 364 3.9 MC Billion paths/s 0.8 1.4 1.8Conv Million pixels/s 1250 3500 2.8FFT GFLOP/s 71.4 213 3.0SAXPY GB/s 16.8 88.8 5.3LBM Million lookups/s 85 426 5.0Solv Frames/s 103 52 0.5SpMV GFLOP/s 4.9 9.1 1.9GJK Frames/s 67 1020 15.2Sort Million elements/s 250 198 0.8RC Frames/s 5 8.1 1.6 Search Million queries/s 50 90 1.8 Hist Million pixels/s 1517 2583 1.7Bilat Million pixels/s 83 475 5.7 Figure 4.30 Raw relative performance measured two platforms. study, SAXPY used measure memory bandwidth, right unit isGB/s GFLOP/s. Based Table 3 Lee, W.V., et al., 2010. Debunking 100/C2GPU vs. CPU myth: evaluation throughput computing CPU GPU. In: Proc. 37th Annual Int ’l. Symposium Computer Architecture (ISCA), June 19 –23, 2010, Saint-Malo, France.352 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesonly 1.7 /C2faster GTX 280. Solv solves batch independent con- straints small amount computation followed barrier synchronization. Core i7 benefits atomic instructions memory consistencymodel ensures right results even previous accesses memoryhierarchy completed. Without memory consistency model, GTX280 version launches batches system processor, leads tothe GTX 280 running 0.5 /C2as fast Core i7. observation points synchronization performance important data parallelproblems. interesting gather-scatter support vector architectures, pre- date SIMD instructions decades, important effective usefulnessof SIMD extensions, predicted comparison ( Gebis Patterson, 2007 ). Intel researchers noted 6 14 kernels would exploit SIMD better efficient gather-scatter support Core i7. Note important feature missing comparison describing level effort get results two systems. Ideally, future comparisons would release code used systems others could re-create experiments different hardware platforms possibly improve results. Comparison Update intervening years, weaknesses Core i7 Tesla GTX 280 havebeen addressed successors. Intel ’s ACV2 added gather instructions, AVX/512 added scatter instructions, found Intel Skylake series. Nvidia Pascal double-precision floating-point performance one- half instead one-eighth speed single precision, fast atomic operations, andcaches. Figure 4.31 lists characteristics two successors, Figure 4.32 com- pares performance using 3 14 benchmarks original paper (those werethe ones could find source code), Figure 4.33 shows two new roofline models. new GPU chip 15 50 times faster new CPU chipsis 50 times faster predecessors, new GPU 2 –5 times faster new CPU. 4.8 Fallacies Pitfalls data-level parallelism easiest form parallelism ILP programmer ’s perspective, plausibly simplest architect ’s perspec- tive, still many fallacies pitfalls. Fallacy GPUs suffer coprocessors . Although split main memory GPU memory disadvantages, advantages distance CPU.4.8 Fallacies Pitfalls ■353For example, PTX exists part I/O device nature GPUs. level indirection compiler hardware gives GPU architectsmuch flexibility system processor architects. ’s often hard know advance whether architecture innovation well supported compilersand libraries important applications. Sometimes new mechanism even prove useful one two generations fade importance world changes. PTX allows GPU architects try innovations speculativelyand drop subsequent generations disappoint fade impor-tance, encourages experimentation. justification inclusion under-standably considerably higher system processors —and thus much lessXeon Platinum 8180 P100 Ratio P100/Xeon Number processing elements (cores SMs) 28 56 2.0 Clock frequency (GHz) 2.5 1.3 0.52Die size N.A. 610 mm 2– Technology Intel 14 nm TSMC 16 nm 1.1Power (chip, module) 80 W 300 W 3.8Transistors N.A. 15.3 B – Memory bandwidth (GB/s) 199 732 3.7Single-precision SIMD width 16 8 0.5Double-precision SIMD width 8 4 0.5Peak single-precision SIMD FLOPS (GFLOP/s) 4480 10,608 2.4Peak double-precision SIMD FLOPS (GFLOP/s) 2240 5304 2.4 Figure 4.31 Intel Xeon ?? NVIDIA P100. rightmost column shows ratios P100 Xeon. Note memory bandwidths higher Figure 4.28 DRAM pin bandwidths Figure 4.28 processors measured benchmark program. Kernel Units Xeon Platinum 8180 P100 P100/XeonGTX 280/i7- 960 SGEMM GFLOP/s 3494 6827 2.0 3.9 DGEMM GFLOP/s 1693 3490 2.1 — FFT-S GFLOP/s 410 1820 4.4 3.0FFT-D GFLOP/s 190 811 4.2 — SAXPY GB/s 207 544 2.6 5.3DAXPY GB/s 212 556 2.6 — Figure 4.32 Raw relative performance measured modern versions two platforms compared relative performance original platforms. Like Figure 4.30 , SAXPY DAXPY used measure memory bandwidth, proper unit GB/s GFLOP/s.354 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesexperimentation occur —as distributing binary machine code normally implies new features must supported future generations architecture. demonstration value PTX different generation architec- ture radically changed hardware instruction set —from memory-oriented like x86 register-oriented like RISC-V well doubling address size 64 bits —without disrupting NVIDIA software stack. Pitfall Concentrating peak performance vector architectures ignoring start-up overhead . Early memory-memory vector processors TI ASC CDC STAR- 100 long start-up times. vector problems, vectors longer than100 vector code faster scalar code! CYBER 205 — derived STAR-100 —the start-up overhead DAXPY 158 clock cycles, substantially increases break-even point. clock rates Cray-1and CYBER 205 identical, Cray-1 would faster vector length greater 64. Cray-1 clock rate also higher (even though 205 newer), crossover point vector length 100.1024 512 256 128 6432 16 8 12 Arithmetic intensit y48 1 6 2 1 1/2 1/4 1/8 Arithmetic intensity48 1 6 32 32 1/8 1/4 1/2Core i7 920 (Nehalem)1024 512 256 128 6432 16 8GFLOP/s GFLOP/s Stream=16.4 GB/s43 GF/s16,384 81924096 2048Xeon Platinum 8180(Skylake) Stream=199 GB/s4480 GF/s16,384 81924096 2048NVIDIA P100 (Pascal) 3696 GF/s8944 GF/s Stream=558 GB/s NVIDIA GTX280 (Tesla)78 GF/s624 GF/s 85 GF/s Stream=127 GB/s Figure 4.33 Roofline models older newer CPUs versus older newer GPUs. higher roofline computer single-precision floating-point performance, lower one double-precision performance.4.8 Fallacies Pitfalls ■355Pitfall Increasing vector performance, without comparable increases scalar performance . imbalance problem many early vector processors, place Seymour Cray (the architect Cray computers) rewrote rules. Many theearly vector processors comparatively slow scalar units (as well large start- overheads). Even today, processor lower vector performance better scalar performance outperform processor higher peak vector perfor-mance. Good scalar performance keeps overhead costs (strip mining, forexample) reduces impact Amdahl ’s law. excellent example comes comparing fast scalar processor vector processor lower scalar performance. Livermore Fortran kernels acollection 24 scientific kernels varying degrees vectorization. Figure 4.34 shows performance two different processors benchmark. Despite vector processor ’s higher peak performance, low scalar performance makes slower fast scalar processor measured harmonic mean. flip danger today increasing vector performance —say, increasing number lanes —without increasing scalar performance. myopia another path unbalanced computer. next fallacy closely related. Fallacy get good vector performance without providing memory bandwidth . saw DAXPY loop Roofline model, memory bandwidth quite important SIMD architectures. DAXPY requires 1.5 memory referencesper floating-point operation, ratio typical many scientific codes. Evenif floating-point operations took time, Cray-1 could increase theperformance vector sequence used, memory-limited. TheCray-1 performance Linpack jumped compiler used blocking tochange computation values could kept vector registers. Thisapproach lowered number memory references per FLOP improved performance nearly factor two! Thus memory bandwidth Cray-1 became sufficient loop formerly required bandwidth. ProcessorMinimum rate loop (MFLOPS)Maximum rate loop (MFLOPS)Harmonic mean 24 loops (MFLOPS) MIPS M/ 120-50.80 3.89 1.85 Stardent- 15000.41 10.08 1.72 Figure 4.34 Performance measurements Livermore Fortran kernels two different processors. MIPS M/120-5 Stardent-1500 (formerly Ardent Titan-1) use 16.7 MHz MIPS R2000 chip main CPU. Stardent-1500 uses vector unit scalar FP half scalar performance (as mea- sured minimum rate) MIPS M/120-5, uses MIPS R2010 FP chip. vector processor factor 2.5 /C2faster highly vectorizable loop (maximum rate). However, lower scalar performance Stardent-1500 negates higher vector performance total performance measured harmonic mean 24 loops.356 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesFallacy GPUs, add threads ’t enough memory performance . GPUs use many CUDA Threads hide latency main memory. memory accesses scattered correlated among CUDA Threads, memory systemwill get progressively slower responding individual request. Eventually,even many threads cover latency. “more CUDA Threads ”strat- egy work, need lots CUDA Threads, CUDA Threads also must well behaved terms locality memory accesses. 4.9 Concluding Remarks Data-level parallelism increasing importance personal mobile devices,given popularity applications showing importance audio, video, games devices. combined model easier program task-level parallelism potentially better energy efficiency, ’s easy see renaissance data-level parallelism decade. seeing system processors take characteristics GPUs, vice versa. One biggest differences performance conven-tional processors GPUs gather-scatter addressing. Traditional vec-tor architectures show add addressing SIMD instructions, weexpect see ideas added well-proven vector architectures SIMD extensions time. said opening Section 4.4 , GPU question simply architecture best, given hardware investment graphics well, howcan enhanced support computation general? Although vectorarchitectures many advantages paper, remains proven whether vec-tor architectures good foundation graphics GPUs. RISC-V hasembraced vector SIMD. Thus, like architecture debates past, mar-ketplace help determine importance strengths weaknesses two styles data parallel architectures. 4.10 Historical Perspective References Section M.6 (available online) features discussion Illiac IV (a representative early SIMD architectures) Cray-1 (a representative vector architec-tures). also look multimedia SIMD extensions history GPUs. Case Study Exercises Jason D. Bakos Case Study: Implementing Vector Kernel Vector Processor GPU Concepts illustrated case study ■Programming Vector Processors ■Programming GPUs ■Performance EstimationCase Study Exercises Jason D. Bakos ■357MrBayes popular computational biology application inferring evolu- tionary histories among set input species based prealigned DNA sequence data length n. MrBayes works performing heuristic search space binary tree topologies inputs leaves. order toevaluate particular tree, application must compute n/C24 conditional like- lihood table (named clP) interior node. table function con-ditional likelihood tables node ’s two descendent nodes ( clL andclR, single precision floating point) 4 /C24 transition probability table ( tiPL tiPR , single precision floating point). One application ’s kernels com- putation conditional likelihood table shown follows: (k=0; k <seq_length; k++) { clP[h++] = (tiPL[AA] *clL[A] + tiPL[AC] *clL[C] + tiPL[AG] *clL[G] + tiPL[AT] *clL[T]) * (tiPR[AA] *clR[A] + tiPR[AC] *clR[C] + tiPR[AG] *clR[G] + tiPR[AT] *clR[T]); clP[h++] = (tiPL[CA] *clL[A] + tiPL[CC] *clL[C] + tiPL[CG] *clL[G] + tiPL[CT] *clL[T]) * (tiPR[CA] *clR[A] + tiPR[CC] *clR[C] + tiPR[CG] *clR[G] + tiPR[CT] *clR[T]); clP[h++] = (tiPL[GA] *clL[A] + tiPL[GC] *clL[C] + tiPL[GG] *clL[G] + tiPL[GT] *clL[T]) * (tiPR[GA] *clR[A] + tiPR[GC] *clR[C] + tiPR[GG] *clR[G] + tiPR[GT] *clR[T]); clP[h++] = (tiPL[TA] *clL[A] + tiPL[TC] *clL[C] + tiPL[TG] *clL[G] + tiPL[TT] * clL[T]) * (tiPR[TA] *clR[A] + tiPR[TC] *clR[C] + tiPR[TG] *clR[G] + tiPR[TT] *clR[T]); clL += 4;clR += 4; } 4.1 [25]<4.1, 4.2 >Assume constants shown follows. Constants Values AA,AC,AG,AT 0,1,2,3 CA,CC,CG,CT 4,5,6,7GA,GC,GG,GT 8,9,10,11 TA,TC,TG,TT 12,13,14,15 A,C,G,T 0,1,2,3358 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesWrite code RISC-V RV64V. Assume starting addresses tiPL , tiPR ,clL,clR, clP RtiPL ,RtiPR ,RclL ,RclR , RclP , respectively. unroll loop. facilitate vector addition reductions,assume add following instructions RV64V:Vector Summation Reduction Single Precision: vsum Fd, Vs instruction performs summation reduction vector register Vs, writ- ing sum scalar register Fd. 4.2 [5]<4.1, 4.2 >Assuming seq_length == 500 , dynamic instruction count implementations? 4.3 [25]<4.1, 4.2 >Assume vector reduction instruction executed vector functional unit, similar vector add instruction. Show codesequence lays convoys assuming single instance vector functional unit. many chimes code require? many cycles per FLOP needed, ignoring vector instruction issue overhead? 4.4 [15]<4.1, 4.2 >Consider possibility unrolling loop mapping mul- tiple iterations vector operations. Assume use scatter-gather loadsand stores ( vldi andvsti ). affect way write RV64Vcode kernel? 4.5 [25]<4.4>Now assume want implement MrBayes kernel GPU using single thread block. Rewrite C code kernel using CUDA.Assume pointers conditional likelihood transition probabilitytables specified parameters kernel. Invoke one thread iter-ation loop. Load reused values shared memory performingoperations it. 4.6 [15]<4.4>With CUDA use coarse-grain p arallelism block level compute conditional likelihood multiple nodes parallel. Assumethat want compute conditional likelihood bottom tree up. Assume seq_length ¼¼500 notes group tables 12 leaf nodes stored consecutive memory locations theorder node number (e.g., mth element clP node n clP[n*4*seq_length+ m*4]). Assume want compute conditional likelihood nodes 12 –17, shown Figure 4.35 . Change method compute array indices answer Exercise 4.5to include block number. 4.7 [15]<4.4>Convert code Exercise 4.6 PTX code. many instructions needed kernel? 4.8 [10]<4.4>How well expect code perform GPU? Explain answer.Case Study Exercises Jason D. Bakos ■359Exercises 4.9 [10/20/20/15/15] <4.2>Consider following code, multiplies two vec- tors contain single-precision complex values: (i=0;i <300;i++) { c_re[i] = a_re[i] *b_re[i] /C0a_im[i] *b_im[i]; c_im[i] = a_re[i] *b_im[i] + a_im[i] *b_re[i]; }Assume processor runs 700 MHz maximum vector length 64. load/store unit start-up overhead 15 cycles; multiply unit,8 cycles; add/subtract unit, 5 cycles. a.[10]<4.3>What arithmetic intensity kernel? Justify answer. b.[20]<4.2>Convert loop RV64V assembly code using strip mining. c.[20]<4.2>Assuming chaining single memory pipeline, many chimes required? many clock cycles required per complex resultvalue, including start-up overhead? d.[15]<4.2>If vector sequence chained, many clock cycles required per complex result value, including overhead? e.[15]<4.2>Now assume processor three memory pipelines chaining. bank conflicts loop ’s accesses, many clock cycles required per result?0 1 2 3 4 5 6 7 8 9 10 1112 1318 192122 20 14 15 16 17 Figure 4.35 Sample tree.360 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architectures4.10 [30]<4.2,4.3,4.4 >In problem, compare performance vector processor hybrid system contains scalar processor GPU-based coprocessor. hybrid system, host processor superior scalar perfor-mance GPU, case scalar code executed host processorwhile vector code executed GPU. refer first system thevector computer second system hybrid computer. Assume yourtarget application contains vector kernel arithmetic intensity 0.5FLOPs per DRAM byte accessed; however, application also scalar com-ponent must performed kernel order prepare input vectors output vectors, respectively. sample dataset, scalar por- tion code requires 400 ms execution time vector processor andthe host processor hybrid system. kernel reads input vectors consistingof 200 MB data output data consisting 100 MB data. vectorprocessor peak memory bandwidth 30 GB/s GPU peak mem-ory bandwidth 150 GB/s. hybrid system additional overhead thatrequires input vectors transferred host memory GPU localmemory kernel invoked. hybrid system direct memory access (DMA) bandwidth 10 GB/s average latency 10 ms. Assume vector processor GPU performance bound mem-ory bandwidth. Compute execution time required computers thisapplication. 4.11 [15/25/25] <4.4, 4.5 >Section 4.5 discussed reduction operation reduces vector scalar repeated application operation. reduction aspecial type loop recurrence. example shown follows: dot=0.0; (i=0;i <64;i++) dot = dot + a[i] *b[i]; vectorizing compiler might apply transformation called scalar expansion , expands dot vector splits loop multiply performed vector operation, leaving reduction separate scalaroperation: (i=0;i <64;i++) dot[i] = a[i] *b[i]; (i=1;i <64;i++) dot[0] = dot[0] + dot[i]; mentioned Section 4.5 , allow floating-point addition asso- ciative, several techniques available parallelizing reduction. a.[15]<4.4, 4.5 >One technique called recurrence doubling, adds sequences progressively shorter vectors (ie, two 32-element vectors, two 16-element vectors, on). Show C code would look exe- cuting second loop way. b.[25]<4.4, 4.5 >In vector processors, individual elements within vec- tor registers addressable. case, operands vector operation may betwo different parts vector register. allows another solution thereduction called partial sums . idea reduce vector msums misCase Study Exercises Jason D. Bakos ■361the total latency vector functional unit, including operand read write times. Assume VMIPS vector registers addressable (e.g., initiatea vectoroperation withtheoperand V1(16) , indicating input oper- begins element 16). Also, assume total latency adds, includingthe operand read result write, eight cycles. Write VMIPS code sequencethat reduces contents V1to eight partial sums. c.[25]<4.4, 4.5 >When performing reduction GPU, one thread associ- ated element input vector. first step thread towrite corresponding value shared memory. Next, thread enters aloop adds pair input values. reduces number elementsby half iteration, meaning number active threads alsoreduces half iteration. order maximize performance reduction, number fully populated warps maximized throughout course loop. words, active threads shouldbe contiguous. Also, thread index shared array away avoid bank conflicts shared memory. following loop vio-lates first guidelines also uses modulo operator, whichis expensive GPUs: unsigned int tid = threadIdx.x; for(unsigned int s=1; <blockDim.x; *¼2) { ((tid % (2 *s)) == 0) { sdata[tid] += sdata[tid + s];} __syncthreads(); } Rewrite loop meet guidelines eliminate use modulo operator. Assume 32 threads per warp bank conflict occurs whenever two threads warp reference index whose modulo 32 equal. 4.12 [10/10/10/10] <4.3>The following kernel performs portion finite- difference time-domain (FDTD) method computing Maxwell ’s equations three-dimensional space, part one SPEC06fp benchmarks: (int x=0; x <NX/C01; x++) { (int y=0; <NY/C01; y++) { (int z=0; z <NZ/C01; z++) { int index = x *NY*NZ + *NZ + z; (y >0& &x >0) { material = IDx[index];dH1 = (Hz[index] /C0Hz[index-incrementY])/dy[y]; dH2 = (Hy[index] /C0Hy[index-incrementZ])/dz[z]; Ex[index] = Ca[material] *Ex[index]+Cb[material] * (dH2 /C0dH1); }}}}362 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU ArchitecturesAssume dH1,dH2,Hy,Hz,dy,dz,Ca,Cb, Exare single- precision floating-point arrays. Assume IDx array unsigned int. a.[10]<4.3>What arithmetic intensity kernel? b.[10]<4.3>Is kernel amenable vector SIMD execution? not? c.[10]<4.3>Assume kernel executed processor 30 GB/s memory bandwidth. kernel memory bound orcompute bound? d.[10]<4.3>Develop roofline model processor, assuming peak computational throughput 85 GFLOP/s. 4.13 [10/15] <4.4>Assume GPU architecture contains 10 SIMD processors. SIMD instruction width 32 SIMD processor contains 8 lanesfor single-precision arithmetic load/store instructions, meaning nondi- verged SIMD instruction produce 32 results every 4 cycles. Assume kernel divergent branches causes, average, 80% threads active. Assumethat 70% SIMD instructions executed single-precision arithmetic 20%are load/store. memory latencies covered, assume averageSIMD instruction issue rate 0.85. Assume GPU clock speed of1.5 GHz. a.[10]<4.4>Compute throughput, GFLOP/s, kernel GPU. b.[15]<4.4>Assume following choices: (1)Increasing number single-precision lanes 16 (2)Increasing number SIMD processors 15 (assume change ’t affect performance metrics code scales additional processors) (3)Adding cache effectively reduce memory latency 40%, increase instruction issue rate 0.95 speedup throughput improvements? 4.14 [10/15/15] <4.5>In exercise, examine several loops analyze potential parallelization. a.[10]<4.5>Does following loop loop-carried dependency? (i=0;i <100;i++) { A[i] = B[2 *i+4]; B[4*i+5] = A[i]; } b.[15]<4.5>In following loop, find true dependences, output depen- dences, antidependences. Eliminate output dependences antidepen-dences renaming. (i=0;i <100;i++) { A[i] = A[i] *B[i]; / *S1*/Case Study Exercises Jason D. Bakos ■363B[i] = A[i] + c; / *S2*/ A[i] = C[i] *c; / *S3*/ C[i] = D[i] *A[i]; / *S4*/ c.[15]<4.5>Consider following loop: (i=0;i <100;i++) { A[i] = A[i] + B[i]; / *S1*/ B[i+1] = C[i] + D[i]; / *S2*/ }Are dependences S1andS2? loop parallel? not, show make parallel. 4.15 [10]<4.4>List describe least four factors influence performance GPU kernels. words, runtime behaviors caused ker-nel code cause reduction resource utilization kernel execution? 4.16 [10]<4.4>Assume hypothetical GPU following characteristics: ■Clock rate 1.5 GHz ■Contains 16 SIMD processors, containing 16 single-precision floating- point units ■Has 100 GB/s off-chip memory bandwidth Without considering memory bandwidth, peak single-precision floating-point throughput GPU GLFOP/s, assuming memory latencies hidden? throughput sustainable given memory band-width limitation? 4.17 [60]<4.4>For programming exercise, write characterize behavior CUDA kernel contains high amount data-level parallelismbut also contains conditional execution behavior. Use NVIDIA CUDA Toolkitalong GPU-SIM University British Columbia ( http://www.gpgpu- sim.org /) CUDA Profiler write compile CUDA kernel performs 100 iterations Conway ’s Game Life 256 /C2256 game board returns final state game board host. Assume board initialized host. Associate one thread cell. Make sure add barrier game iteration. Use following game rules: ■Any live cell fewer two live neighbors dies. ■Any live cell two three live neighbors lives next generation. ■Any live cell three live neighbors dies. ■Any dead cell exactly three live neighbors becomes live cell. finishing kernel answer following questions: a.[60]<4.4>Compile code using –ptx option inspect PTX rep- resentation kernel. many PTX instructions make PTX364 ■Chapter Four Data-Level Parallelism Vector, SIMD, GPU Architecturesimplementation kernel? conditional sections kernel include branch instructions predicated nonbranch instructions? b.[60]<4.4>After executing code simulator, dynamic instruction count? achieved instructions per cycle (IPC) instruc- tion issue rate? dynamic instruction breakdown terms control instructions, arithmetic-logical unit (ALU) instructions, memory instruc-tions? shared memory bank conflicts? effectiveoff-chip memory bandwidth? c.[60]<4.4>Implement improved version kernel off-chip memory references coalesced observe differences runtimeperformance.Case Study Exercises Jason D. Bakos ■3655.1 Introduction 368 5.2 Centralized Shared-Memory Architectures 377 5.3 Performance Symmetric Shared-Memory Multiprocessors 393 5.4 Distributed Shared-Memory Directory-Based Coherence 404 5.5 Synchronization: Basics 412 5.6 Models Memory Consistency: Introduction 417 5.7 Cross-Cutting Issues 422 5.8 Putting Together: Multicore Processors Performance 426 5.9 Fallacies Pitfalls 438 5.10 Future Multicore Scaling 442 5.11 Concluding Remarks 444 5.12 Historical Perspectives References 445 Case Studies Exercises Amr Zaky David A. Wood 4465 Thread-Level Parallelism turning away conventional organization came middle 1960s, law diminishing returns began take effect effort increase operational speed computer. . . . Electronic circuits ultimately limited speed operation speed light . . . many circuits already operating nanosecond range. W. Jack Bouknight et al., Illiac IV System (1972) dedicating future product development multicore designs. believe key inflection point theindustry. Intel President Paul Otellini, describing Intel ’s future direction Intel Developer Forum 2005 Since 2004 processor designers increased core counts exploit Moore ’s Law scaling, rather focusing single-core performance. failure Dennard scaling, shift multicore parts partially response, may soon limit multicorescaling single-core scaling curtailed. Hadi Esmaeilzadeh, et al., Power Limitations Dark Silicon Challenge Future Multicore (2012) Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00005-5 ©2019 Elsevier Inc. rights reserved.5.1 Introduction quotations open chapter show, view advances uniprocessor architecture nearing end held researchers many years.Clearly, views premature; fact, period 1986 –2003, uniprocessor performance growth, driven microprocessor, highestrate since first transistorized computers late 1950s early 1960s. Nonetheless, importance multiprocessors growing throughout 1990s designers sought way build servers supercomputers achieved higher performance single microprocessor, exploiting tremendouscost-performance advantages commodity microprocessors. discussedin Chapters 1and3, slowdown uniprocessor performance arising diminishing returns exploiting instruction-level parallelism (ILP) combinedwith growing concern power led new era computer architec-ture—an era multiprocessors play major role low end high end. second quotation captures clear inflection point. increased importance multiprocessing reflects several major factors: ■The dramatically lower efficiencies silicon energy use encoun- tered 2000 2005 designers attempted find exploit moreILP, turned inefficient, since power silicon costs grewfaster performance. ILP, scalable general-purposeway know increase performance faster basic technology allows(from switching perspective) multiprocessing. ■A growing interest high-end servers cloud computing software-as-a-service become important. ■A growth data-intensive applications driven availability massiveamounts data Internet. ■The insight increasing performance desktop less important(outside graphics, least), either current performance acceptableor highly compute- data-intensive applications done onthe cloud. ■An improved understanding use multiprocessors effectively,especially server environments significant inherent parallel-ism, arising large datasets (usually form data parallelism), “natural-world ”parallelism (which occurs scientific engineering codes), parallelism among large numbers independent requests (request-levelparallelism). ■The advantages leveraging design investment replication rather thanunique design; multiprocessor designs provide leverage. third quotation reminds us multicore may provide limited possi- bilities scaling performance. combination Amdahl ’s Law effects the368 ■Chapter Five Thread-Level Parallelismend Dennard scaling mean future multicore may limited, least method scaling performance single applications. return topic late chapter. chapter, focus exploiting thread-level parallelism (TLP). TLP implies existence multiple program counters thus exploited primarilythrough MIMDs. Although MIMDs around decades, movement ofthread-level parallelism forefront across range computing embed-ded applications high-end severs relatively recent. Likewise, extensive useof thread-level parallelism wide-range general-purpose applications, versus either transaction processing scientific applications, relatively new. focus chapter multiprocessors , define computers consisting tightly coupled processors whose coordination usage typi-cally controlled single operating system share memory ashared address space. systems exploit thread-level parallelism twodifferent software models. first execution tightly coupled set ofthreads collaborating single task, typically called parallel processing . second execution multiple, relatively independent processes may originate one users, form request-level parallelism , although much smaller scale explore next chapter.Request-level parallelism may exploited single application running onmultiple processors, database responding queries, multiple applica-tions running independently, often called multiprogramming . multiprocessors examine chapter typically range size dual processor dozens sometimes hundreds processors communicateand coordinate sharing memory. Although sharing memory implies shared address space, necessarily mean single physical memory. multiprocessors include single-chip systems withmultiple cores, known multicore , computers consisting multiple chips, typically multicore. Many companies make multiproces-sors, including HP, Dell, Cisco, IBM, SGI, Lenovo, Oracle, Fujitsu, manyothers. addition true multiprocessors, return topic multithread- ing, technique supports multiple threads executing interleaved fashion single multiple-issue processor. Many multicore processors also include support multithreading. next chapter, consider ultrascale computers built large numbers processors, connected networking technology (not necessarilythe networking technology used connect computers Internet) andoften called clusters ; large-scale systems used cloud computing primarily massive numbers independent tasks executed parallel.More recently, computationally intensive tasks easily made parallel, Search certain machine learning algorithms also made use clusters. clusters grow tens thousands servers beyond,we call warehouse-scale computers . Amazon, Google, Microsoft, Facebook make warehouse-scale computers.5.1 Introduction ■369In addition multiprocessors study warehouse-scaled systems next chapter, range special large-scale multiprocessor systems, sometimes called multicomputers , less tightly coupled multiprocessors examined chapter usually tightly coupled thewarehouse-scale systems next chapter. primary use multicom-puters high-end scientific computation, although sometimes used forcommercial applications filling niche multiprocessors warehouse-scale computers. Cray X series IBM BlueGene typical examples ofthese multicomputers. Many books, Culler et al. (1999) , cover systems detail. large changing nature field multiprocessing (the just-mentioned Culler et al. reference 1000 pages discusses multipro-cessing!), chosen focus attention believe themost important general-purpose portions computing space. AppendixI discusses issues arise building computers contextof large-scale scientific applications. focus multiprocessors roughly 4 –256 processor cores, might occupy anywhere 4 16 separate chips. designs vastly dominate terms units dollars. large-scale multiprocessors, theinterconnection networks critical part design; Appendix F focuseson topic. Multiprocessor Architecture: Issues Approach take advantage MIMD multiprocessor nprocessors, must usually least nthreads processes execute; multithreading, present multicore chips today, number 2 –4 times higher. inde- pendent threads within single process typically identified programmeror created operating system (from multiple independent requests). theother extreme, thread may consist tens iterations loop, generatedby parallel compiler exploiting data parallelism loop. Although amountof computation assigned thread, called grain size , important consid- ering exploit thread-level parallelism efficiently, important qualitative distinction instruction-level parallelism thread-level parallelism isidentified high level software system programmer threadsconsist hundreds millions instructions may executed parallel. Threads also used exploit data-level parallelism, although over- head usually higher would seen SIMD processor GPU(seeChapter 4 ). overhead means grain size must sufficiently large exploit parallelism efficiently. example, although vector processor GPU may able efficiently parallelize operations short vectors, resulting grain size parallelism split among many threads may small thatthe overhead makes exploitation parallelism prohibitively expensive inan MIMD.370 ■Chapter Five Thread-Level ParallelismExisting shared-memory multiprocessors fall two classes, depending number processors involved, turn dictates memory organization interconnect strategy. refer multiprocessors memory orga-nization constitutes small large number processors continuesto change time. first group, call symmetric (shared-memory) multiprocessors (SMPs), centralized shared-memory multiprocessors , features small moder- ate numbers cores, typically 32 fewer. multiprocessors smallprocessor counts, possible processors share single centralized mem- ory processors equal access to, thus term symmetric . multicore chips, memory often shared centralized fashion among cores; mostexisting multicores SMPs, all. (Note literature mistakenlyappears use SMP stand Shared Memory Processor, usage iserroneous.) multicores nonuniform access outermost cache, structure called NUCA forNonuniform Cache Access , thus truly SMPs, even single main memory. IBM Power8 distributed L3 caches nonuniform access time different addresses L3. multiprocessors consisting multiple multicore chips, often sep- arate memories multicore chip. Thus memory distributed ratherthan centralized. see later chapter, many designs distrib-uted memory fast access local memory much slower access toremote memory; often differences access time various remote memoriesare small comparison difference access times localmemory remote memory. designs, programmer software system need aware whether accesses local remote memory, may able ignore distribution accesses among remote memories.Because SMP approach becomes less attractive growing number pro-cessors, largest multiprocessors use form distributedmemory. SMP architectures also sometimes called uniform memory access (UMA) multiprocessors, arising fact processors uniformlatency memory, even memor organized multiple banks. Figure 5.1 shows multiprocessors look like. architecture SMPs topic Section 5.2 , explain approach context multicore. alternative design approach consis ts multiprocessors physically distributed memory, called distributed shared memory (DSM). Figure 5.2 shows multiprocessors look like. support larger processor counts,memory must distributed among processors rather centralized; oth-erwise, memory system would able support bandwidth demands larger number processors without incurring excessively long access latency. rapid increase processor performance associated increase processor ’s memory bandwidth requirements, size multiprocessor for5.1 Introduction ■371which distributed memory preferred continues shrink. introduction multicore processors meant even 2-chip multiprocessors, whichmight 16 –64 processor cores, use distributed memory. larger number processors also raises need high-bandwidth interconnect, whichwe see examples Appendix F. directed networks (i.e., switches)and indirect networks (typically multidimensional meshes) used. Distributing memory among nodes increases bandwidth reduces latency local memory. DSM multiprocessor also called NUMA (nonuniform memory access) access time depends location adata word memory. key disadvantages DSM communicatingdata among processors becomes somewhat complex DSM requiresmore effort software take advantage increased memory bandwidthProcessor Processor Processor Processor Main memory I/O systemOne levels cacheOne levels cacheOne levels cacheOne levels cache Shared cachePrivate caches Figure 5.1 Basic structure centralized shared-memory multiprocessor based multicore chip. Multiple processor-cache subsystems share physical mem- ory, typically one level shared cache multicore, one levels ofprivate per-core cache. key architectural property uniform access time memory processors. multichip design, interconnection net- work links processors memory, may one banks. single-chip multicore, interconnection network simply memory bus.372 ■Chapter Five Thread-Level Parallelismafforded distributed memories. multicore-based multiprocessors processor chips use distributed memory, explain theoperation distributed memory multiprocessors viewpoint. SMP DSM architectures, communication among threads occurs shared address space, meaning memory reference madeby processor memory location, assuming correct access rights. term shared memory associated SMP DSM refers fact theaddress space shared. contrast, clusters warehouse-scale computers next chapter look like individual computers connected network, memory one pro-cessor cannot accessed another processor without assistance softwareprotocols running processors. designs, message-passing protocolsare used communicate data among processors. Challenges Parallel Processing application multiprocessors ranges running independent tasks withessentially communication running parallel programs threads mustcommunicate complete task. Two important hurdles, explainable withAmdahl ’s Law, make parallel processing challenging. overcome hurdles typically requires comprehensive approach addresses choice ofMemory I/O Interconnection networkMemory I/O Memory I/OMulticore MPMulticore MPMulticore MPMulticore MP Memory I/O I/O Memory Memory I/O Memory I/O Memory I/O Multicore MPMulticore MPMulticore MPMulticore MP Figure 5.2 basic architecture distributed-memory multiprocessor 2017 typically consists multi- core multiprocessor chip memory possibly I/O attached interface interconnection networkthat connects nodes. processor core shares entire memory, although access time local memory attached core ’s chip much faster access time remote memories.5.1 Introduction ■373algorithm implementation, underlying programming language sys- tem, operating system support functions, architecture hard- ware implementation. Although many instances, one keybottleneck, scaling larger processor counts (approaching 100 more),often allaspects software hardware need attention. first hurdle limited parallelism available programs, second arises relatively high cost communications. Limitationsin available parallelism make difficult achieve good speedups parallelprocessor, first example shows. Example Suppose want achieve speedup 80 100 processors. fraction original computation sequential? Answer Recall Chapter 1 Amdahl ’s Law Speedup ¼1 Fraction enhanced Speedupenhanced+1/C0Fraction enhanced ðÞ simplicity example, assume program operates two modes: parallel processors fully used, enhanced mode, orserial one processor use. simplification, speedup inenhanced mode simply number processors, whereas fraction ofenhanced mode time spent parallel mode. Substituting previous equation: 80¼1 Fraction parallel 100+1/C0Fraction parallel/C0/C1 Simplifying equation yields: 0:8/C2Fraction parallel +8 0/C21/C0Fraction parallel/C0/C1 ¼1 80/C079:2/C2Fraction parallel¼1 Fraction parallel¼80/C01 79:2 Fraction parallel¼0:9975 Thus, achieve speedup 80 100 processors, 0.25% original computation sequential! course, achieve linear speedup (speedup nwith nprocessors), entire program must usually parallel serial portions. practice, programs operate fullyparallel sequential mode, often use less full complement ofthe processors running parallel mode. Amdahl ’s Law used analyze applications varying amounts speedup, nextexample shows.374 ■Chapter Five Thread-Level ParallelismExample Suppose application running 100-processor multiprocessor, assume application use 1, 50, 100 processors. assumethat 95% time use 100 processors, much remain-ing 5% execution time must employ 50 processors want speedupof 80? Answer use Amdahl ’s Law terms: Speedup ¼1 Fraction 100 Speedup100+Fraction 50 Speedup50+1/C0Fraction 100/C0Fraction 50 ðÞ Substituting in: 80¼1 0:95 100+Fraction 50 50+1/C00:95/C0Fraction 80 ðÞ Simplifying: 0:76 + 1 :6/C2Fraction 50+4:0/C080/C2Fraction 50¼1 4:76/C078:4/C2Fraction 50¼1 Fraction 50¼0:048 95% application use 100 processors perfectly, get speedup 80, 4.8% remaining time must spent using 50 processors 0.2% canbe serial! second major challenge parallel p rocessing involves large latency remote access parallel processor . existing shared-memory multipro- cessors, communication data separate cores may cost 35 –50 clock cycles among cores separate chips anywhere 100 clock cycles much 300 clock cycles (for lar ge-scale multiproce ssors), depending communication mechanism, type interconnection network, thescale multiprocessor. effect long communication delays clearly substantial. Let ’s consider simple example. Example Suppose application running 32-processor multiprocessor 100 ns delay handle reference remote memory. application,assume references except involving communication hit thelocal memory hierarchy, obviously optimistic. Processors stalled remote request, processor clock rate 4 GHz. base CPI (assum- ing references hit cache) 0.5, much faster multiprocessorif communication versus 0.2% instructions involve remotecommunication reference?5.1 Introduction ■375Answer simpler first calculate clock cycles per instruction. effective CPI multiprocessor 0.2% remote references CPI¼Base CPI + Remote request rate /C2Remote request cost ¼0:5+0:2%/C2Remote request cost remote request cost Remote access cost Cycle time¼100ns 0:25ns¼400 cycles Therefore compute CPI: CPI¼0:5+0:20%/C2400 ¼1:3 multiprocessor local references 1.3/0.5 ¼2.6 times faster. prac- tice, performance analysis much complex fraction thenoncommunication references miss local hierarchy remoteaccess time single constant value. example, cost remotereference could worse contention caused many references trying touse global interconnect lead increased delays, access time might better memory distributed access local memory. problem could also analyzed using Amdahl ’s Law, exercise leave reader. problems —insufficient parallelism long-latency remote communi- cation —are two biggest performance challenges using multiprocessors. problem inadequate application parallelism must attacked primarily insoftware new algorithms offer better parallel performance, well asby software systems maximize amount time spent executing thefull complement processors. Reducing impact long remote latency canbe attacked architecture programmer. example, reduce frequency remote accesses either hardware mechanisms, caching shared data, software mechanisms, restructuring datato make accesses local. try tolerate latency using multi-threading (discussed later chapter) using prefetching (a topic coverextensively Chapter 2 ). Much chapter focuses techniques reducing impact long remote communication latency. example, Sections 5.2 5.4 discuss caching used reduce remote access frequency, maintaining coherent view memory. Section 5.5 discusses synchronization, which, inherently involves interprocessor communication also limit parallelism,is major potential bottleneck. Section 5.6 covers latency-hiding techniques memory consistency models shared memory. Appendix I, focus primar-ily larger-scale multiprocessors used predominantly scientific work.376 ■Chapter Five Thread-Level ParallelismIn appendix, examine nature applications challenges achieving speedup dozens hundreds processors. 5.2 Centralized Shared-Memory Architectures observation use large, multilevel caches substantially reduce memory bandwidth demands processor key insight motivates central-ized memory multiprocessors. Originally, processors single-core andoften took entire board, memory located shared bus. recent, higher-performance processors, memory demands outstripped capability reasonable buses, recent microprocessors directly connectmemory single chip, sometimes called backside ormemory bus distinguish bus used connect I/O. Accessing chip ’s local memory whether I/O operation access another chip requires goingthrough chip “owns ”that memory. Thus access memory asymmetric: faster local memory slower remote memory. multicore thatmemory shared among cores single chip, asymmetric access memory one multicore memory another usually remains. Symmetric shared-memory machines usually support caching shared private data. Private data used single processor, shared data used multiple processors, essentially providing communication among processors reads writes shared data. private item iscached, location migrated cache, reducing average access time aswell memory bandwidth required. processor uses data,the program behavior identical uniprocessor. shared data cached, shared value may replicated multiple caches. addition reduction access latency required memory bandwidth, replication alsoprovides reduction contention may exist shared data items arebeing read multiple processors simultaneously. Caching shared data, how-ever, introduces new problem: cache coherence. Multiprocessor Cache Coherence? Unfortunately, caching shared data introduces new problem. view ofmemory held two different processors individual caches, theprocessors could end seeing different values memory location,asFigure 5.3 illustrates. difficulty generally referred cache coher- ence problem . Notice coherence problem exists global state, defined primarily main memory, local state, definedby individual caches, private processor core. Thus, multi- core level caching may shared (e.g., L3), although levels private (e.g., L1 L2), coherence problem still exists must solved. Informally, could say memory system coherent read data item returns recently written value data item. definition,5.2 Centralized Shared-Memory Architectures ■377although intuitively appealing, vague simplistic; reality much complex. simple definition contains two different aspects memory systembehavior, critical writing correct shared-memory programs.The first aspect, called coherence , defines values returned read. second aspect, called consistency , determines written value returned read. Let ’s look coherence first. memory system coherent 1.A read processor P location X follows write P X, writes X another processor occurring write read P, alwaysreturns value written P. 2.A read processor location X follows write another processor X returns written value read write sufficiently separated timeand writes X occur two accesses. 3.Writes location serialized ; is, two writes location two processors seen order processors. example, values 1 2 written location, processors never read value location 2 later read 1. first property simply preserves program order —we expect property true even uniprocessors. second property defines notion itmeans coherent view memory: processor could continuously readan old data value, would clearly say memory incoherent. need write serialization subtle, equally important. Suppose serialize writes, processor P1 writes location X followed P2 writing location X. Serializing writes ensures every processor seethe write done P2 point. serialize writes, mightbe case processors could see write P2 first see thewrite P1, maintaining value written P1 indefinitely. simplest wayTime EventCache contents processor ACache contents processor BMemory contents location X 0 1 1 Processor reads X 1 12 Processor B reads X 1 1 13 Processor stores 0 X010 Figure 5.3 cache coherence problem single memory location (X), read written two processors (A B). initially assume neither cache contains variable X value 1. also assume write- cache; write-back cache adds additional similar complications. value X written A, ’s cache memory contain new value, B ’s cache not, B reads value X receive 1!378 ■Chapter Five Thread-Level Parallelismto avoid difficulties ensure writes location seen order; property called write serialization . Although three properties described sufficient ensure coherence, question written value seen also important. see why,observe cannot require read X instantaneously see valuewritten X processor. If, example, write X oneprocessor precedes read X another processor small time, itmay impossible ensure read returns value data written, sincethe written data may even left processor point. issue exactly written value must seen reader defined memory consistency model —a topic discussed Section 5.6 . Coherence consistency complementary: Coherence defines behav- ior reads writes memory location, consistency defines thebehavior reads writes respect accesses memory locations . now, make following two assumptions. First, write complete(and allow next write occur) processors seen effect thatwrite. Second, processor change order write respect memory access. two conditions mean that, processor writes location followed location B, processor sees new value B mustalso see new value A. restrictions allow processor reorder reads,but forces processor finish write program order. rely thisassumption reach Section 5.6 , see exactly implications definition, well alternatives. Basic Schemes Enforcing Coherence coherence problem multiprocessors I/O, although similar origin, hasdifferent characteristics affect appropriate solution. Unlike I/O, wheremultiple data copies rare event —one avoided whenever possible —a program running multiple processors normally copies datain several caches. coherent multiprocessor, caches provide migration andreplication shared data items. Coherent caches provide migration data item moved local cache used transparent fashion. migration reduces latency access shared data item allocated remotely bandwidthdemand shared memory. caches make copy data item local cache, coherent caches also provide replication shared data read simultaneously.Replication reduces latency access contention read shared dataitem. Supporting migration replication critical performance acces-sing shared data. Thus, rather trying solve problem avoiding software, multiprocessors adopt hardware solution introducing protocol maintain coherent caches. protocols maintain coherence multiple processors called cache coherence protocols . Key implementing cache coherence protocol tracking5.2 Centralized Shared-Memory Architectures ■379the state sharing data block. state cache block kept using status bits associated block, similar valid dirty bits kept uniprocessor cache. two classes protocols use, usesdifferent techniques track sharing status: ■Directory based —The sharing status particular block physical memory kept one location, called directory . two different types directory-based cache coherence. SMP, use one centralizeddirectory, associated memory single serialization point, outermost cache multicore. DSM, makes sense single directory would create single point contention andmake difficult scale many multicore chips given memory demandsof multicores eight cores. Distributed directories com-plex single directory, designs subject Section 5.4 . ■Snooping —Rather keeping state sharing single directory, every cache copy data block physical memory could trackthe sharing status block. SMP, caches typically acces-sible via broadcast medium (e.g., bus connects per-core caches tothe shared cache memory), cache controllers monitor snoop medium determine whether copy block requested bus switch access. Snooping also used coherence protocol multichip multiprocessor, designs support snooping protocol ontop directory protocol within multicore. Snooping protocols became popular multiprocessors using microproces- sors (single-core) caches attached single shared memory bus. busprovided convenient broadcast medium implement snooping protocols.Multicore architectures changed picture significantly multicores share level cache chip. Thus designs switched using direc- tory protocols, since overhead small. allow reader become fami-liar types protocols, focus snooping protocol discussa directory protocol come DSM architectures. Snooping Coherence Protocols two ways maintain coherence requirement described priorsection. One method ensure processor exclusive access data itembefore writing item. style protocol called write invalidate protocol invalidates copies write. far common pro-tocol. Exclusive access ensures readable writable copies item exist write occurs: cached copies item invalidated. Figure 5.4 shows example invalidation protocol write-back caches action. see protocol ensures coherence, consider writefollowed read another processor: write requires exclusive380 ■Chapter Five Thread-Level Parallelismaccess, copy held reading processor must invalidated (thus protocol name). Therefore read occurs, misses cache isforced fetch new copy data. write, require writing processor exclusive access, preventing processor able write simultaneously. two processors attempt write data simul-taneously, one wins race (we ’ll see decide wins shortly), causing processor ’s copy invalidated. processor complete write, must obtain new copy data, must containthe updated value. Therefore protocol enforces write serialization. alternative invalidate protocol update cached copies data item item written. type protocol called write update write broadcast protocol. write update protocol must broadcast writes shared cache lines, consumes considerably bandwidth. thisreason, virtually recent multiprocessors opted implement writeinvalidate protocol, focus invalidate protocols restof chapter.Processor activity Bus activityContents processor A’s cacheContents processor B’s cacheContents memory location X 0 Processor reads X Cache miss X00 Processor B reads X Cache miss X000 Processor writes 1t oXInvalidation X10 Processor B reads X Cache miss X111 Figure 5.4 example invalidation protocol working snooping bus single cache block (X) write-back caches. assume neither cache initially holds X value X memory 0. pro- cessor memory contents show value processor bus activity completed. blank indi-cates activity copy cached. second miss B occurs, processor responds value canceling response memory. addition, contents B ’s cache memory contents X updated. update memory, occurs block becomes shared, simplifies protocol, itis possible track ownership force write-back block replaced. requires introduction additional status bit indicating ownership block. ownership bit indicates block may shared reads, owning processor write block, processor responsible updating otherprocessors memory changes block replaces it. multicore uses shared cache (e.g., L3), allmemory seen shared cache; L3 acts like memory example, coherency must handled private L1 L2 caches core. observation led designers opt directory protocol within multicore. make work, L3 cache must inclusive; recall Chapter 2 , cache inclusive location higher level cache (L1 L2 case) also L3. return topic inclusion page 423.5.2 Centralized Shared-Memory Architectures ■381Basic Implementation Techniques key implementing invalidate protocol multicore use bus, another broadcast medium, perform invalidates. older multiple-chip mul-tiprocessors, bus used coherence shared-memory access bus. asingle-chip multicore, bus connection private caches(L1 L2 Intel i7) shared outer cache (L3 i7). performan invalidate, processor simply acquires bus access broadcasts addressto invalidated bus. processors continuously snoop bus, watch-ing addresses. processors check whether address bus cache. so, corresponding data cache invalidated. write block shared occurs, writing processor must acquire bus access broadcast invalidation. two processors attempt write sharedblocks time, attempts broadcast invalidate operation willbe serialized arbitrate bus. first processor obtain bus accesswill cause copies block writing invalidated. proces-sors attempting write block, serialization enforced buswould also serialize writes. One implication scheme write shared data item cannot actually complete obtains bus access. coherence schemes require method serializing accesses cache block, eitherby serializing access communication medium another shared structure. addition invalidating outstanding copies cache block writ- ten into, also need locate data item cache miss occurs. write-through cache, easy find recent value data item writtendata always sent memory, recent value data itemcan always fetched. (Write buffers lead additional complexities must effectively treated additional cache entries.) write-back cache, problem finding recent data value harder recent value data item private cache ratherthan shared cache memory. Fortunately, write-back caches use samesnooping scheme cache misses writes: processor snoops everyaddress placed shared bus. processor finds dirty copy therequested cache block, provides cache block response read requestand causes memory (or L3) access aborted. additional complexity comes retrieve cache block another processor ’s private cache (L1 L2), often take longer retrieving L3. Becausewrite-back caches generate lower requirements memory bandwidth, cansupport larger numbers faster processors. result, multicore processorsuse write-back outermost levels cache, examine imple-mentation coherence write-back caches. normal cache tags used implement process snooping, valid bit block makes invalidation easy implement. Read misses, whether generated invalidation event, also straightfor- ward simply rely snooping capability. writes, want toknow whether copies block cached because, noother cached copies, write need placed bus a382 ■Chapter Five Thread-Level Parallelismwrite-back cache. sending write reduces time write required bandwidth. track whether cache block shared, add extra state bit associated cache block, valid bit dirty bit. Byadding bit indicating whether block shared, decide whether writemust generate invalidate. write block shared state occurs, thecache generates invalidation bus marks block exclusive .N invalidations sent core block. core solecopy cache block normally called owner cache block. invalidation sent, state owner ’s cache block changed shared unshared (or exclusive). another processor later requests thiscache block, state must made shared again. snooping cache alsosees misses, knows exclusive cache block requested byanother processor state made shared. Every bus transaction must check cache-address tags, could poten- tially interfere processor cache accesses. One way reduce interferenceis duplicate tags snoop accesses directed duplicate tags. Another approach use directory shared L3 cache; directory indicates whether given block shared possibly cores copies. thedirectory information, invalidates directed caches copiesof cache block. requires L3 must always copy data item inL1 L2, property called inclusion , return Section 5.7 . Example Protocol snooping coherence protocol usually implemented incorporating finite- state controller core. controller responds requests proces-sor core bus (or broadcast medium), changing state ofthe selected cache block, well using bus access data invalidate it.Logically, think separate controller associated eachblock; is, snooping operations cache requests different blocks pro-ceed independently. actual implementations, single controller allows multipleoperations distinct blocks proceed interleaved fashion (i.e., one operation may initiated another completed, even though one cache access one bus access allowed time). Also, remember that, although refer abus following description, interconnection network supports abroadcast coherence controllers associated private caches canbe used implement snooping. simple protocol consider three states: invalid, shared, modified. shared state indicates block private cache potentially shared,whereas modified state indicates block updated private cache; note modified state implies block exclusive. Figure 5.5 shows requests generated core (in top half table) well asthose coming bus (in bottom half table). protocol fora write-back cache easily changed work write-through cache rein-terpreting modified state exclusive state updating cache writes5.2 Centralized Shared-Memory Architectures ■383in normal fashion write-through cache. common extension basic protocol addition exclusive state, describes block unmodified held one private cache. describe otherextensions page 388. Request SourceState addressed cache blockType cache action Function explanation Read hit Processor Shared modifiedNormal hit Read data local cache. Read missProcessor Invalid Normal miss Place read miss bus. Read missProcessor Shared Replacement Address conflict miss: place read miss bus. Read missProcessor Modified Replacement Address conflict miss: write-back block; place read miss bus. Write hit Processor Modified Normal hit Write data local cache.Write hit Processor Shared Coherence Place invalidate bus. operations often called upgrade ownership misses, fetch data change state. Write missProcessor Invalid Normal miss Place write miss bus. Write missProcessor Shared Replacement Address conflict miss: place write miss bus. Write missProcessor Modified Replacement Address conflict miss: write-back block; place write miss bus. Read missBus Shared action Allow shared cache memory service read miss. Read missBus Modified Coherence Attempt read shared data: place cache block bus, write-back block, change state shared. Invalidate Bus Shared Coherence Attempt write shared block; invalidate block.Write missBus Shared Coherence Attempt write shared block; invalidate cache block. Write missBus Modified Coherence Attempt write block exclusive elsewhere; write- back cache block make state invalid local cache. Figure 5.5 cache coherence mechanism receives requests core ’s processor shared bus responds based type request, whether hits misses local cache, state local cache block specified request. fourth column describes type cache action normal hit miss (the uniprocessor cache would see), replacement (a uniprocessor cache replacement miss), coher-ence (required maintain cache coherence); normal replacement action may cause coherence action depend- ing state block caches. read, misses, write misses, invalidates snooped bus, action required read write addresses match block local cache block valid.384 ■Chapter Five Thread-Level ParallelismWhen invalidate write miss placed bus, cores whose pri- vate caches copies cache block invalidate it. write miss write- back cache, block exclusive one private cache, cache also writesback block; otherwise, data read shared cache memory. Figure 5.6 shows finite-state transition diagram single private cache block using write invalidation protocol write-back cache. simplicity,the three states protocol duplicated represent transitions based CPU read hit Shared (read only) CPU write miss Write-back cache block Place write miss bus CPU write hit CPU read hitExclusive (read/write)Exclusive (read/write)Cache state transitions based requests CPUPlace write miss busInvalid CPU read Place read miss bus CPU write CPU read miss Write-back block Place read miss bus Place invalidate bus CPU write CPU write miss Place write miss busPlace read miss busCPU read miss Cache state transitions based requests busWrite miss block Write-back block; abortmemory accessWrite-back block; abort memory access Read missfor block CPU read missShared (read only)InvalidInvalidate blockWrite miss block Figure 5.6 write invalidate, cache coherence protocol private write-back cache showing states state transitions block cache. cache states shown circles, access permitted local processor without state transition shown parentheses name state. stimulus causing astate change shown transition arcs regular type, bus actions generated part state tran- sition shown transition arc bold. stimulus actions apply block private cache, specific address cache. Thus read miss block shared state miss cache block butfor different address. left side diagram shows state transitions based actions processor asso-ciated cache; right side shows transitions based operations bus. read miss exclusive shared state write miss exclusive state occur address requested processor match address local cache block. miss standard cache replacement miss. attempt writea block shared state generates invalidate. Whenever bus transaction occurs, private caches contain cache block specified bus transaction take action dictated right half diagram. protocol assumes memory (or shared cache) provides data read miss block clean local caches. Inactual implementations, two sets state diagrams combined. practice, many subtle variations invalidate protocols, including introduction exclusive unmodified state, whether processor memory provides data miss. multicore chip, shared cache (usually L3, sometimes L2) acts theequivalent memory, bus bus private caches core shared cache, whichin turn interfaces memory.5.2 Centralized Shared-Memory Architectures ■385processor requests (on left, corresponds top half table Figure 5.5 ), opposed transitions based bus requests (on right, corresponds bottom half table Figure 5.5 ). Boldface type used distinguish bus actions, opposed conditions state tran-sition depends. state node represents state selected privatecache block specified processor bus request. states cache protocol would needed uniprocessor cache, would correspond invalid, valid (and clean), dirty states.Most state changes indicated arcs left half Figure 5.6 would needed write-back uniprocessor cache, exception inval- idate write hit shared block. state changes represented arcs inthe right half Figure 5.6 needed coherence would appear uniprocessor cache controller. mentioned earlier, one finite-state machine per cache, stimuli coming either attached processor bus. Figure 5.7 shows state transitions right half Figure 5.6 combined left half figure form single state diagram cache block. understand protocol works, observe valid cache block either shared state one private caches exclusive state inexactly one cache. transition exclusive state (which required aprocessor write block) requires invalidate write miss placedon bus, causing local caches make block invalid. addition, someother local cache block exclusive state, local cache generates write-back, supplies block containing desired address. Finally, read miss occurs bus block exclusive state, local cache exclusive copy changes state shared. actions gray Figure 5.7 , handle read write misses bus, essentially snooping component protocol. One propertythat preserved protocol, protocols, memoryblock shared state always date outer shared cache (L2 L3, ormemory shared cache), simplifies implementation. fact, itdoes matter whether level private caches shared cache memory; key accesses cores go level. Although simple cache protocol correct, omits number complica- tions make implementation much trickier. important isthat protocol assumes operations atomic —that is, operation done way intervening operation occur. example, theprotocol described assumes write misses detected, acquire thebus, receive response single atomic action. reality true.In fact, even read miss might atomic; detecting miss L2 multicore, core must arbitrate access bus connecting shared L3. Nonatomic actions introduce possibility protocol deadlock , meaning reaches state cannot continue. explore thesecomplications later section examine DSM designs.386 ■Chapter Five Thread-Level ParallelismWith multicore processors, coherence among processor cores implemented chip, using either snooping simple central directory protocol.Many multiprocessor chips, including Intel Xeon AMD Opteron, supportmultichip multiprocessors could built connecting high-speed interfacealready incorporated chip. next-level interconnects exten- sions shared bus, use different approach interconnecting multicores. multiprocessor built multiple multicore chips usually distributed memory architecture need interchip coherency mechanismabove beyond one within chip. cases, form directoryscheme used.CPU write hit CPU read hitWrite miss blockCPU writePlace write miss busRead miss blockCPU read missWrite-back block Place invalidate busCPU writePlace read miss busWrite miss block CPU read CPU write missInvalidInvalidate block Write-back data; place read miss busShared (read only)Write-back blockCPU write miss Place write miss busCPU readhit Write-back data Place write miss busCPU read miss Place read miss bu Exclusive (read/write) Figure 5.7 Cache coherence state diagram state transitions induced local processor shown black bus activities shown gray.As Figure 5.6 , activities transition shown bold.5.2 Centralized Shared-Memory Architectures ■387Extensions Basic Coherence Protocol coherence protocol described simple three-state protocol often referred first letter states, making MSI (Modified,Shared, Invalid) protocol. many extensions basic protocol, whichwe mention captions figures section. extensions createdby adding additional states transactions optimize certain behaviors, pos-sibly resulting improved performance. Two common extensions 1.MESI adds state Exclusive basic MSI protocol, yielding four states (Modified, Exclusive, Shared, Invalid). exclusive state indicates thata cache block resident single cache clean. block inthe E state, written without generating invalidates, optimizesthe case block read single cache written thatsame cache. course, read miss block E state occurs,the block must changed state maintain coherence. allsubsequent accesses snooped, possible maintain accuracy state. particular, another processor issues read miss, state changed exclusive shared. advantage adding state subsequentwrite block exclusive state core need acquire busaccess generate invalidate, since block known exclusivelyin local cache; processor merely changes state modified. stateis easily added using bit encodes coherent state exclusivestate using dirty bit indicate bock modified. Intel i7 usesa variant MESI protocol, called MESIF, adds state (Forward) designate sharing processor respond request. designed enhance performance distributed memory organizations. 2.MOESI adds state Owned MESI protocol indicate associ- ated block owned cache out-of-date memory. MSI MESIprotocols, attempt share block Modified state, thestate changed Shared (in original newly sharing cache), andthe block must written back memory. MOESI protocol, block canbe changed Modified Owned state original cache withoutwriting memory. caches, newly sharing block, keepthe block Shared state; state, original cache holds, indicates main memory copy date designated cache owner. owner block must supply miss, since memoryis date must write block back memory replaced.The AMD Opteron processor family uses MOESI protocol. next section examines performance protocols parallel multiprogrammed workloads; value extensions basic protocol clear examine performance. But, that, let ’s take brief look limitations use symmetric memory structure asnooping coherence scheme.388 ■Chapter Five Thread-Level ParallelismLimitations Symmetric Shared-Memory Multiprocessors Snooping Protocols number processors multiprocessor grows, memory demands processor grow, centralized resource system become abottleneck. multicores, single shared bus became bottleneck afew cores. result, multicore designs gone higher bandwidth intercon-nection schemes, well multiple, independent memories allow larger num-bers cores. multicore chips examine Section 5.8 use three different approaches: 1.The IBM Power8, 12 processors single multicore, uses 8 parallel buses connect distributed L3 caches 8 separatememory channels. 2.The Xeon E7 uses three rings connect 32 processors, distributed L3 cache, two four memory channels (depending configu-ration). 3.The Fujitsu SPARC64 X+ uses crossbar connect shared L2 16 cores multiple memory channels. SPARC64 X+ symmetric organization uniform access time. Power8 nonuniform access time L3 memory. Although theuncontended access time differences among memory addresses within singlePower8 multicore large, contention memory, access time differences become significant even within one chip. Xeon E7 operate access times uniform; practice, software systems usuallyorganize memory memory cha nnels associated subset cores. Snooping bandwidth caches also become problem every cache must examine every miss, additional interconnection bandwidthonly pushes problem cache. understand problem, consider thefollowing example. Example Consider 8-processor multicore processor L1 L2 caches, snooping performed shared bus among L2 caches. Assumethe average L2 request, whether coherence miss miss, 15 cycles.Assume clock rate 3.0 GHz, CPI 0.7, load/store frequency 40%.If goal 50% L2 bandwidth consumed coherencetraffic, maximum coherence miss rate per processor? Answer Start equation number cache cycles used (where CMR coherence miss rate):5.2 Centralized Shared-Memory Architectures ■389Cache cycles available ¼Clock rate Cycles per request /C22¼3:0Ghz 30¼0:1/C2109 Cache cycles available ¼Memory references =clock=processor /C2Clock rate /C2processor count /C2CMR ¼0:4 0:7/C23:0GHz /C28/C2CMR ¼13:7/C2109/C2CMR CMR ¼0:1 13:7¼0:0073¼0:73% means coherence miss rate must 0.73% less. next section, see several applications coherence miss rates excess 1%. Alter-natively, assume CMR 1%, could support 6processors. Clearly, even small multicores require method scaling snoop bandwidth. several techniques increasing snoop bandwidth: 1.As mentioned earlier, tags duplicated. doubles effective cache-level snoop bandwidth. assume half coherence requestsdo hit snoop request cost snoop request 10 cycles (versus 15), cut average cost CMR 12.5 cycles. reduction allows coherence miss rate 0.88, alternatively supportone additional processor (7 versus 6). 2.If outermost cache multicore (typically L3) shared, distribute cache processor portion memory handlessnoops portion address space. approach, used theIBM 12-core Power8, leads NUCA design, effectively scales snoopbandwidth L3 number processors. snoop hit L3, thenwe must still broadcast L2 caches, must turn snoop contents.Since L3 acting filter snoop requests, L3 must inclusive. 3.We place directory level outermost shared cache (say, L3). L3 acts filter snoop requests must inclusive. use directory L3 means need snoop broadcast L2 caches, directory indicates may copy block. L3may distributed, associated directory entries may also distributed. Thisapproach used Intel Xeon E7 series, supports 8 32 cores. Figure 5.8 shows multicore distributed cache system, used schemes 2 3, might look. additional multicore chips added toform larger multiprocessor, off-chip network would needed, well method extend coherence mechanisms (as see Section 5.8 ).390 ■Chapter Five Thread-Level ParallelismThe AMD Opteron represents another intermediate point spectrum snooping directory protocol. Memory directly connected eachmulticore chip, four multicore chips connected. system NUMA local memory somewhat faster. Opteron implements coherence protocol using point-to-point links broadcast three otherchips. interprocessor links shared, way processorcan know invalid operation completed explicit acknowledg-ment. Thus coherence protocol uses broadcast find potentially shared cop-ies, like snooping protocol, uses acknowledgments order operations,like directory protocol. local memory somewhat faster thanremote memory Opteron implementation, software treats Opteron multiprocessor uniform memory access. InSection 5.4 , examine directory-based protocols, eliminate need broadcast caches miss. multicore designs use directoriesProcessor Interconnection networkProcessor Processor Processor MemoryBank 3 shared cacheBank 2 shared cacheBank 1 shared cacheBank 0 shared cacheOne levels private cacheOne levels private cacheOne levels private cacheOne levels private cache I/O system Figure 5.8 single-chip multicore distributed cache. current designs, distributed shared cache usually L3, levels L1 L2 private. typ- ically multiple memory channels (2 –8 today ’s designs). design NUCA, since access time L3 portions varies faster access time directly attached core.Because NUCA, also NUMA.5.2 Centralized Shared-Memory Architectures ■391within multicore (Intel Xeon E7), others add directories scaling beyond multicore. Distributed directories eliminate need single point serialize accesses (typically single shared bus snooping scheme),and scheme removes single point serialization must deal manyof challenges distributed directory scheme. Implementing Snooping Cache Coherence devil details. Classic proverb wrote first edition book 1990, final “Putting Together ”was 30-processor, single-bus multiprocessor using snoop-based cohe- rence; bus capacity 50 MiB/s, would enough busbandwidth support even one core Intel i7 2017! wrote thesecond edition book 1995, first cache coherence multiprocessors withmore single bus recently appeared, added appendix descri- bing implementation snooping system multiple buses. 2017, every multicore multiprocessor system supports 8 cores uses inter- connect single bus, designers must face challenge imple-menting snooping (or directory scheme) without simplification bus toserialize events. observed page 386, major complication actually implement- ing snooping coherence protocol described write andupgrade misses atomic recent multiprocessor. steps detect- ing write upgrade miss, communicating processors memory, getting recent value write miss ensuring anyinvalidates processed, updating cache cannot done though theytook single cycle. multicore single bus, steps made effectively atomic arbitrating bus shared cache memory first (before changing thecache state) releasing bus actions complete. canthe processor know invalidates complete? early designs, single line used signal necessary invalidates received processed. Following signal, processor generated miss couldrelease bus, knowing required actions would completed anyactivity related next miss. holding bus exclusively steps,the processor effectively made individual steps atomic. system without single, central bus, must find method making steps miss atomic. particular, must ensure two processorsthat attempt write block time, situation called race, strictly ordered: one write processed precedes next begun. matter two writes race wins race, therebe single winner whose coherence actions completed first. multicore392 ■Chapter Five Thread-Level Parallelismusing multiple buses, races eliminated block memory associated single bus, ensuring two attempts access block must serialized common bus. property, together ability restart themiss handling loser race, keys implementing snooping cachecoherence without bus. explain details Appendix I. possible combine snooping directories, several designs use snooping within multicore directories among multiple chips combinationof directories one cache level snooping another level. 5.3 Performance Symmetric Shared-Memory Multiprocessors multicore using snooping coherence protocol, several different phenomena combine determine performance. particular, overall cache performance isa combination behavior uniprocessor cache miss traffic trafficcaused communication, results invalidations subsequent cachemisses. Changing processor count, cache size, block size affect thesetwo components miss rate different ways, leading overall system behav-ior combination two effects. Appendix B breaks uniprocessor miss rate three C ’s classification (capacity, compulsory, conflict) provides insight application behavior potential improvements cache design. Similarly, misses thatarise interprocessor communication, often called coherence misses , broken two separate sources. first source true sharing misses arise communication data cache coherence mechanism. invalidation-based protocol,the first write processor shared cache block causes invalidation toestablish ownership block. Additionally, another processor attempts read modified word cache block, miss occurs resultant block transferred. misses classified true sharing misses theydirectly arise sharing data among processors. second effect, called false sharing , arises use invalidation- based coherence algorithm single valid bit per cache block. False sharingoccurs block invalidated (and subsequent reference causes miss)because word block, one read, written into.If word written actually used processor received inval- idate, reference true sharing reference would caused miss independent block size. If, however, word written wordread different invalidation cause new value commu-nicated, causes extra cache miss, false sharing miss. falsesharing miss, block shared, word cache actually shared, andthe miss would occur block size single word. following exam-ple makes sharing patterns clear.5.3 Performance Symmetric Shared-Memory Multiprocessors ■393Example Assume words z1 z2 cache block, shared state caches P1 P2. Assuming following sequence events,identify miss true sharing miss, false sharing miss, hit. missthat would occur block size one word designated true sharing miss. Time P1 P2 1 Write z12 Read z23 Write z14 Write z25 Read z2 Answer classifications time step: 1.This event true sharing miss, since z1 shared state P2 needs invalidated P2. 2.This event false sharing miss, since z2 invalidated write z1 P1, value z1 used P2. 3.This event false sharing miss, since block containing z1 marked shared due read P2, P2 read z1. cache block containingz1 shared state read P2; write miss required obtain exclusive access block. protocols, handled upgrade request , generates bus invalidate, transfer cache block. 4.This event false sharing miss reason step 3. 5.This event true sharing miss, since value read written P2. Although see effects true false sharing misses commercial workloads, role coherence misses significant tightly coupled applications share significant amounts user data. examine effectsin detail Appendix consider performance parallel scientificworkload. Commercial Workload section, examine memory system behavior 4-processor shared-memory multiprocessor running online transaction processing workload.The study examine done 4-processor Alpha system 1998, itremains comprehensive insightful study performance mul-tiprocessor workloads. focus understanding multiprocessorcache activity, particularly behavior L3, much traffic iscoherence-related.394 ■Chapter Five Thread-Level ParallelismThe results collected either AlphaServer 4100 using configur- able simulator modeled AlphaServer 4100. processor Alpha- Server 4100 Alpha 21164, issues four instructions per clock runs 300 MHz. Although clock rate Alpha processor system isconsiderably slower processors systems designed 2017, basic struc-ture system, consisting four-issue processor three-level cache hier-archy, similar multicore Intel i7 processors, shown inFigure 5.9 . Rather focus performance details, consider data looks simulated L3 behavior L3 caches varying 2 8 MiB perprocessor. Although original study considered three different workloads, focus attention online transaction-processing (OLTP) workload modeled afterTPC-B (which memory behavior similar newer cousin TPC-C, describedinChapter 1 ) using Oracle 7.3.2 underlying database. workload con- sists set client processes generate requests set servers han-dle them. server processes consume 85% user time, remaininggoing clients. Although I/O latency hidden careful tuning andenough requests keep processor busy, server processes typically block I/O 25,000 instructions. Overall, 71% execution time spent user mode, 18% operating system, 11% idle, primarily waiting I/O.Of commercial applications studied, OLTP application stresses memorysystem hardest shows significant challenges even evaluated withmuch larger L3 caches. example, AlphaServer, processors stalledCache level Characteristic Alpha 21164 Intel i7 L1 Size 8 KB I/8 KB 32 KB I/32 KB Associativity Direct-mapped 8-way I/8-way DBlock size 32 B 64 BMiss penalty 7 10 L2 Size 96 KB 256 KB Associativity 3-way 8-wayBlock size 32 B 64 BMiss penalty 21 35 L3 Size 2 MiB (total 8 MiB unshared) 2 MiB per core (8 MiB total shared) Associativity Direct-mapped 16-wayBlock size 64 B 64 BMiss penalty 80 /C24100 Figure 5.9 characteristics cache hierarchy Alpha 21164 used study Intel i7. Although sizes larger associativity higher i7, miss penalties also higher, behav-ior may differ slightly. systems high penalty (125 cycles more) transfer required private cache. key difference L3 shared i7 versus four separate, unshared caches Alpha server.5.3 Performance Symmetric Shared-Memory Multiprocessors ■395for approximately 90% cycles memory accesses occupying almost half stall time L2 misses 25% stall time. start examining effect varying size L3 cache. studies, L3 cache varied 1 8 MiB per processor; 2 MiB per pro-cessor, total size L3 equal Intel i7 6700. case i7,however, cache shared, provides advantages disadvan-tages. unlikely shared 8 MiB cache outperform separate L3 caches total size 16 MiB. Figure 5.10 shows effect increasing cache size, using two-way set associative caches, reduces large number con-flict misses. execution time improved L3 cache grows thereduction L3 misses. Surprisingly, almost gain occurs going 1to 2 MiB (or 4 8 MiB total cache four processors). little addi-tional gain beyond that, despite fact cache misses still cause sig-nificant performance loss 2 MiB 4 MiB caches. question is, Why? better understand answer question, need determine factors contribute L3 miss rate change L3 cache grows. Figure 5.11 shows data, displaying number memory access cycles con- tributed per instruction five sources. two largest sources L3 memoryaccess cycles 1 MiB L3 instruction capacity/conflict misses. alarger L3, two sources shrink minor contributors. Unfortunately, the100 90 80 706050Normalized execution time 40302010 0 1 2 4 8 L3 cache size (MB)PAL code Memory access L2/L3 cache accessIdle Instruction execution Figure 5.10 relative performance OLTP workload size L3 cache, set two-way set associative, grows 1 8 MiB. idle time also grows cache size increased, reducing performance gains. growth occurs because, fewer memory system stalls, server processes needed cover I/O latency. workload could retuned increase com-putation/communication balance, holding idle time check. PAL code set sequences specialized OS-level instructions executed privileged mode; exam- ple TLB miss handler.396 ■Chapter Five Thread-Level Parallelismcompulsory, false sharing, true sharing misses unaffected larger L3. Thus, 4 8 MiB, true sharing misses generate dominant fraction themisses; lack change true sharing misses leads limited reductions inthe overall miss rate increasing L3 cache size beyond 2 MiB. Increasing cache size eliminates uniprocessor misses leaving multiprocessor misses untouched. increasing processorcount affect different types misses? Figure 5.12 shows data assuming base configuration 2 MiB, two-way set associative L3 cache (the effective per processor cache size i7 less associativity). might expect, increase true sharing miss rate, compensated byany decrease uniprocessor misses, leads overall increase memoryaccess cycles per instruction. final question examine whether increasing block size —which decrease instruction cold miss rate and, within limits, also reducethe capacity/conflict miss rate possibly true sharing miss rate —is helpful workload. Figure 5.13 shows number misses per 1000 instructions block size increased 32 256 bytes. Increasing block size 32 256 bytes affects four miss rate components: ■The true sharing miss rate decreases factor 2, indicating somelocality true sharing patterns.3.25 3 2.75 2.5 2.25 2 1.75 1.5 1.25 1 0.75 0.5 0.25Memory cycles per instruction 0 1 2 4 8 Cache size (MB)CompulsoryCapacity/conflict False sharingInstruction True sharing Figure 5.11 contributing causes memory access cycle shift cache size increased. L3 cache simulated two-way set associative.5.3 Performance Symmetric Shared-Memory Multiprocessors ■3973 2.5 2 1.5 1 0.5 0Memory cycles per instruction 1 2 4 6 8 Processor countCompulsoryCapacity/conflict False sharingInstruction True sharing Figure 5.12 contribution memory access cycles increases processor count increases primarily increased true sharing. compulsory misses slightly increase processor must handle compulsory misses. 16 151413 12 1110 9 87 6 5 4 3 2 1 0Misses per 1000 instructions 32 64 128 256 Block size (bytes)CompulsoryCapacity/conflict False sharingInstruction True sharing Figure 5.13 number misses per 1000 instructions drops steadily block size L3 cache increased, making good case L3 block size least128 bytes. L3 cache 2 MiB, two-way set associative.398 ■Chapter Five Thread-Level Parallelism■The compulsory miss rate significantly decreases, would expect. ■The conflict/capacity misses show small decrease (a factor 1.26 compared factor 8 increase block size), indicating spatial locality nothigh uniprocessor misses occur L3 caches larger 2 MiB. ■The false sharing miss rate, although small absolute terms, nearly doubles. lack significant effect instruction miss rate startling. instruction-only cache behavior, would conclude spa-tial locality poor. case mixed L2 L3 caches, effects suchas instruction-data conflicts may also contribute high instruction cache missrate larger blocks. studies documented low spatial locality theinstruction stream large database OLTP workloads, lots shortbasic blocks special-purpose code sequences. Based data, misspenalty larger block size L3 perform well 32-byte block size L3 expressed multiplier 32-byte block size penalty. Block sizeMiss penalty relative 32-byte block miss penalty 64 bytes 1.19128 bytes 1.36256 bytes 1.52 modern DDR SDRAMs make block access fast, numbers attainable, especially 64 byte (the i7 block size) 128 byte block size.Of course, must also worry effects increased traffic memoryand possible contention memory cores. latter effect mayeasily negate gains obtained improving performance single processor. Multiprogramming OS Workload next study multiprogrammed workload consisting user activity OS activity. workload used two independent copies compile phases ofthe Andrew benchmark, benchmark emulates software development envi-ronment. compile phase consists parallel version UNIX “make ” command executed using eight processors. workload runs 5.24 secondson eight processors, creating 203 processes performing 787 disk requestson three different file systems. workload run 128 MiB memory,and paging activity takes place. workload three distinct phases: compiling benchmarks, involves substantial compute activity; installing object files library; andremoving object files. last phase completely dominated I/O, onlytwo processes active (one runs). middle phase, I/O also5.3 Performance Symmetric Shared-Memory Multiprocessors ■399plays major role, processor largely idle. overall workload much system- I/O-intensive OLTP workload. workload measurements, assume following memory I/O systems: ■Level 1 instruction cache —32 KB, two-way set associative 64-byte block, 1 clock cycle hit time. ■Level 1 data cache —32 KB, two-way set associative 32-byte block, 1 clock cycle hit time. focus examining behavior Level 1 datacache, contrast OLTP study, focused L3 cache. ■Level 2 cache —1 MiB unified, two-way set associative 128-byte block, 10 clock cycle hit time. ■Main memory —Single memory bus access time 100 clock cycles. ■Disk system —Fixed-access latency 3 ms (less normal reduce idle time). Figure 5.14 shows execution time breaks eight processors using parameters listed. Execution time broken four components: 1.Idle—Execution kernel mode idle loop 2.User—Execution user code 3.Synchronization —Execution waiting synchronization variables 4.Kernel —Execution OS neither idle synchronization access multiprogramming workload significant instruction cache perfor- mance loss, least OS. instruction cache miss rate OS a64-byte block size, two-way set associative cache varies 1.7% 32 KB User executionKernel executionSynchronization waitProcessor idle (waiting I/O) Instructions executed27% 3% 1% 69% Execution time27% 7% 2% 64% Figure 5.14 distribution execution time multiprogrammed parallel “make ”workload. high fraction idle time due disk latency one eight processors active. data subsequent measurementsfor workload collected SimOS system ( Rosenblum et al., 1995 ). actual runs data collection done M. Rosenblum, S. Herrod, E. Bugnion Stanford University.400 ■Chapter Five Thread-Level Parallelismcache 0.2% 256 KB cache. User-level instruction cache misses roughly one-sixth OS rate, across variety cache sizes. partially accounts fact that, although user code executes nine times many instructions asthe kernel, instructions take four times long smaller num-ber instructions executed kernel. Performance Multiprogramming OS Workload section, examine cache performance multiprogrammed work-load cache size block size changed. differences betweenthe behavior kernel user processes, keep two com-ponents separate. Remember, though, user processes execute thaneight times many instructions, overall miss rate determined primarilyby miss rate user code, see, often one-fifth kernelmiss rate. Although user code executes instructions, behavior oper- ating system cause cache misses user processes two reasons beyond larger code size lack locality. First, kernel initializes pagesbefore allocating user, significantly increases compulsorycomponent kernel ’s miss rate. Second, kernel actually shares data thus nontrivial coherence miss rate. contrast, user processes cause coher-ence misses process scheduled different processor, thiscomponent miss rate small. major difference multi-programmed workload one like OLTP workload. Figure 5.15 shows data miss rate versus data cache size versus block size kernel user components. Increasing data cache size affects theuser miss rate affects kernel miss rate. Increasing block size hasbeneficial effects miss rates larger fraction misses arisefrom compulsory capacity, potentially improved withlarger block sizes. coherence misses relatively rarer, negativeeffects increasing block size small. understand kernel userprocesses behave differently, look kernel misses behave. Figure 5.16 shows variation kernel misses versus increases cache size block size. misses broken three classes: compulsory misses,coherence misses (from true false sharing), capacity/conflict misses(which include misses caused interference OS user processand multiple user processes). Figure 5.16 confirms that, kernel ref- erences, increasing cache size reduces uniprocessor capacity/conflictmiss rate. contrast, increasing block size causes reduction compulsorymiss rate. absence large increases coherence miss rate block size increased means false sharing effects probably insignificant, although misses may offsetting gains reducing true sharing misses. examine number bytes needed per data reference, Figure 5.17 , see kernel higher traffic ratio grows block size. easyto see occurs: going 16-byte block 128-byte block, the5.3 Performance Symmetric Shared-Memory Multiprocessors ■4014%5%6%7% 3% 2% 1%Miss rate Miss rate 0% Cache size (KB)32 64 128 256Kernel miss rate User miss rate6%7%8%9%10% 4%5% 3%1%2% 0% Block size (bytes)16 32 64 128Kernel miss rate User miss rate Figure 5.15 data miss rates user kernel components behave differently increases L1 data cache size (on left) versus increases L1 data cache block size (on right). Increasing L1 data cache 32 256 KB (with 32-byte block) causes user miss rate decrease proportionately thekernel miss rate: user-level miss rate drops almost factor 3, whereas kernel-level miss rate drops afactor 1.3. largest size, L1 closer size L2 modern multicore processors. Thus data indicates kernel miss rate still significant L2 cache. miss rate user kernel com- ponents drops steadily L1 block size increased (while keeping L1 cache 32 KB). contrast effectsof increasing cache size, increasing block size improves kernel miss rate significantly (just factor 4 kernel references going 16-byte 128-byte blocks versus factor 3 user references). Miss rate Miss rate 0%2%4%6% 5% 3% 1% 32 64 128 Cache size (KB)2567% 0%2%4%9% 8% 7% 6% 5% 3% 1% 16 32 64 Block size (bytes)12810% CompulsoryCoherenceCapacity/conflict Figure 5.16 components kernel data miss rate change L1 data cache size increased 32 256 KB, multiprogramming workload run eight processors. compulsory miss rate component stays constant unaffected cache size. capacity component drops factor 2, whereas coherence component nearly doubles. increase coherence misses occurs probability miss caused invalidation increases cache size, since fewer entries bumped due capacity.As would expect, increasing block size L1 data cache substantially reduces compulsory miss rate kernel references. also significant impact capacity miss rate, decreasing factor 2.4 range block sizes. increased block size small reduction coherence traffic, appears stabilize at64 bytes, change coherence miss rate going 128-byte lines. significant reduc- tions coherence miss rate block size increases, fraction miss rate caused coherence grows 7% 15%.miss rate drops 3.7, number bytes transferred per miss increases 8, total miss traffic increases factor 2. user programalso doubles block size goes 16 128 bytes, starts much lower level. multiprogrammed workload, OS much demanding user memory system. OS OS-like activity included workload, andthe behavior similar measured workload, become verydifficult build sufficiently capable memory system. One possible route toimproving performance make OS cache-aware either betterprogramming environments programmer assistance. example, theOS reuses memory requests arise different system calls. Despite fact reused memory completely overwritten, hardware, rec- ognizing this, attempt preserve coherency possibility por-tion cache block may read, even not. behavior analogous thereuse stack locations procedure invocations. IBM Power series sup-port allow compiler indicate type behavior procedure invoca-tions, newest AMD processors similar support. harder detectsuch behavior OS, may require programmer assistance, thepayoff potentially even greater. OS commercial workloads pose tough challenges multiprocessor mem- ory systems, unlike scientific applications, examine Appendix I,they less amenable algorithmic compiler restructuring. number ofcores increases, predicting behavior applications likely get moredifficult. Emulation simulation methodologies allow simulation tens3.5 2.02.53.0 1.5 1.00.5Memory traffic measured bytes per data reference 0.0 Block size (bytes)16 32 64 128Kernel traffic User traffic Figure 5.17 number bytes needed per data reference grows block size increased kernel user components. interesting compare chart data scientific programs shown Appendix I.5.3 Performance Symmetric Shared-Memory Multiprocessors ■403to hundreds cores large applications (including operating systems) crucial maintaining analytical quantitative approach design. 5.4 Distributed Shared-Memory Directory-Based Coherence saw Section 5.2 , snooping protocol requires communication caches every cache miss, including writes potentially shared data. absence centralized data structure tracks state caches fundamental advantage snooping-based scheme, since allows beinexpensive, well Achilles ’heel comes scalability. example, consider multiprocessor consisting four 4-core multicores capable sustaining one data reference per clock 4 GHz clock. thedata Section I.5 Appendix I, see applications may require4–170 GiB/s memory bus bandwidth. maximum memory bandwidth sup- ported i7 two DDR4 memory channels 34 GiB/s. several i7 multi- core processors shared memory system, would easily swamp it. last years, development multicore processors forced designers toshift form distributed memory support bandwidth demands theindividual processors. increase memory bandwidth interconnection bandwidth dis- tributing memory, shown Figure 5.2 page 373; immediately sep- arates local memory traffic remote memory traffic, reducing bandwidthdemands memory system interconnection network. Unless eliminate need coherence protocol broadcast every cache miss, distributing memory gain us little. mentioned earlier, alternative snooping-based coherence proto- col directory protocol . directory keeps state every block may cached. Information directory includes caches (or collections ofcaches) copies block, whether dirty, on. Within multicorewith shared outermost cache (say, L3), easy implement directory scheme:simply keep bit vector size equal number cores L3 block. bit vector indicates private L2 caches may copies block L3, invalidations sent caches. works perfectly singlemulticore L3 inclusive, scheme one used Intel i7. solution single directory used multicore scalable, even though avoids broadcast. directory must distributed, distributionmust done way coherence protocol knows find direc-tory information cached block memory. obvious solution dis-tribute directory along memory different coherence requests go different directories, different memory requests go different memories. information maintained outer cache, like L3, mul-tibanked, directory information distributed different cachebanks, effectively increasing bandwidth.404 ■Chapter Five Thread-Level ParallelismA distributed directory retains characteristic sharing status block always single known location. property, together main-tenance information says nodes may caching block, iswhat allows coherence protocol avoid broadcast. Figure 5.18 shows distributed-memory multiprocessor looks directories added node. simplest directory implementations associate entry directory memory block. implementations, amount information pro-portional product number memory blocks (where block isthe size L2 L3 cache block) times number nodes, anode single multicore processor small collection processors imple-ments coherence internally. overhead problem multiprocessors less hundred processors (each might multicore) directory overhead reasonable block size tolerable.For larger multiprocessors, need methods allow directory structure tobe efficiently scaled, supercomputer-sized systems need worryabout this.Interconnection networkMulticore processor + caches Memory I/O DirectoryMemory I/O DirectoryMemory I/O DirectoryMemory I/O MemoryDirectory Memory I/O Memory Memory I/O I/O Memory I/ODirectory Directory Directory DirectoryMulticore processor + cachesMulticore processor + cachesMulticore processor + caches Multicore processor + cachesMulticore processor + cachesMulticore processor + cachesMulticore processor + caches Figure 5.18 directory added node implement cache coherence distributed-memory multi- processor. case, node shown single multicore chip, directory information associated memory may reside either multicore. directory responsible tracking caches share thememory addresses portion memory node. coherence mechanism handle mainte- nance directory information coherence actions needed within multicore node.5.4 Distributed Shared-Memory Directory-Based Coherence ■405Directory-Based Cache Coherence Protocols: Basics snooping protocol, two primary operations directory protocol must implement: handling read miss handling write shared,clean cache block. (Handling write miss block currently shared asimple combination two.) implement operations, directory musttrack state cache block. simple protocol, states could thefollowing: ■Shared —One nodes block cached, value memory date (as well caches). ■Uncached —No node copy cache block. ■Modified —Exactly one node copy cache block, written block, memory copy date. processor called owner block. addition tracking state potentially shared memory block, must track nodes copies block copies need tobe invalidated write. simplest way keep bit vector eachmemory block. block shared, bit vector indicates whetherthe corresponding processor chip (which likely multicore) copy thatblock. also use bit vector keep track owner block whenthe block exclusive state. efficiency reasons, also track state cache block individual caches. states transitions state machine cache identical used snooping cache, although actions transition areslightly different. processes invalidating locating exclusive copyof data item different involve communication betweenthe requesting node directory directory one moreremote nodes. snooping protocol, two steps combined theuse broadcast nodes. see protocol state diagrams, useful examine catalog message types may sent processors directories forthe purpose handling misses maintaining coherence. Figure 5.19 shows types messages sent among nodes. local node node request originates. home node node memory location directory entry address reside. physical address space statically distributed, sothe node contains memory directory given physical address isknown. example, high-order bits may provide node number, whereas low-order bits provide offset within memory node. local node may also home node. directory must accessed homenode local node copies may exist yet third node, called remote node .406 ■Chapter Five Thread-Level ParallelismA remote node node copy cache block, whether exclusive (in case copy) shared. remote node may aseither local node home node. cases, basic protocol notchange, interprocessor messages may replaced intraprocessormessages. section, assume simple model memory consistency. mini- mize type messages complexity protocol, make assump- tion messages received acted upon order sent. assumption may true practice result additional compli-cations, address Section 5.6 discuss memory con- sistency models. section, use assumption ensure invalidatessent node honored new messages transmitted, weassumed discussion implementing snooping protocols. inthe snooping case, omit details necessary implement coherenceMessage type Source DestinationMessage contents Function message Read miss Local cache Home directoryP, Node P read miss address A; request data make P read sharer. Write missLocal cache Home directoryP, Node P write miss address A; request data make P exclusive owner. Invalidate Local cache Home directoryA Request send invalidates remote caches caching block address A. Invalidate Home directoryRemote cacheA Invalidate shared copy data address A. Fetch Home directoryRemote cacheA Fetch block address send home directory; change state remote cache shared. Fetch/ invalidateHome directoryRemote cacheA Fetch block address send home directory; invalidate block cache. Data value replyHome directoryLocal cache Return data value home memory. Data write-backRemote cacheHome directoryA, Write back data value address A. Figure 5.19 possible messages sent among nodes maintain coherence, along source des- tination node, contents (where P 5requesting node number, 5requested address, 5data contents), function message. first three messages requests sent local node home. fourth sixth messages messages sent remote node home home needs data satisfy read write miss request. Data value replies used send value home node back therequesting node. Data value write-backs occur two reasons: block replaced cache must written back home memory, also reply fetch fetch/invalidate messages home. Writing back data value whenever block becomes shared simplifies number states protocol becauseany dirty block must exclusive shared block always available home memory.5.4 Distributed Shared-Memory Directory-Based Coherence ■407protocol. particular, serialization writes knowing invalidates write completed simple broadcast-based snooping mechanism. Instead, explicit acknowledgments required response writemisses invalidate requests. discuss issues detail inAppendix I. Example Directory Protocol basic states cache block directory-based protocol exactly like thosein snooping protocol, states directory also analogous weshowed earlier. Thus start simple state diagrams show state tran-sitions individual cache block examine state diagram direc-tory entry corresponding block memory. snooping case, thesestate transition diagrams represent details coherence protocol; how-ever, actual controller highly dependent number details multi-processor (message delivery properties, buffering structures, on). section, present basic protocol state diagrams. knotty issues involved implementing state transition diagrams examined Appendix I. Figure 5.20 shows protocol actions individual cache responds. use notation last section, requests coming outsidethe node gray actions bold. state transitions individual cacheare caused read misses, write misses, invalidates, data fetch requests;Figure 5.20 shows operations. individual cache also generates read miss, write miss, invalidate messages sent home directory. Read write misses require data value replies, events wait replies changing state. Knowing invalidates complete separate problem andis handled separately. operation state transition diagram cache block Figure 5.20 essentially snooping case: states identical, thestimulus almost identical. write miss operation, broadcast thebus (or network) snooping scheme, replaced data fetch andinvalidate operations selectively sent directory controller. Like snooping protocol, cache block must exclusive state writ- ten, shared block must date memory. many multicore pro-cessors, outermost level processor cache shared among cores (as isthe L3 Intel i7, AMD Opteron, IBM Power7), hardware atthat level maintains coherence among private caches core samechip, using either internal directory snooping. Thus on-chip multicorecoherence mechanism used extend coherence among larger set pro-cessors simply interfacing outermost shared cache. interface L3, contention processor coherence requests less issue, duplicating tags could avoided. directory-based protocol, directory implements half coherence protocol. message sent directory causes two different types of408 ■Chapter Five Thread-Level Parallelismactions: updating directory state sending additional messages satisfy request. states directory represent three standard states block;unlike snooping scheme, however, directory state indicates state allthe cached copies memory block, rather single cache block. memory block may uncached node, cached multiple nodes readable (shared), cached exclusively writable exactly one node. addition state block, directory must track set nodes thathave copy block; use set called Sharers perform function. multiprocessors fewer 64 nodes (each may represent four toeight times many processors), set typically kept bit vector. DirectoryModified (read/write) CPU write hit CPU read hitFetch invalidateCPU writeSend write miss messageFetch CPU read missData write-back Send invalidate messageCPU write hitSend read miss message Read missCPU read CPU read hit CPU write miss Data write-back Write missCPU readmissInvalidate Data write-back; read missData write-backSend write miss message CPU write missInvalidShared (read only) Figure 5.20 State transition diagram individual cache block directory-based system. Requests local processor shown black , home directory shown gray.The states identical snooping case, transactions similar, explicit invalidate write-back requests replacing write misses formerly broadcast bus. snooping controller, assume attempt write shared cache block treated miss; practice, transaction treated anownership request upgrade request deliver ownership without requiring cache block fetched.5.4 Distributed Shared-Memory Directory-Based Coherence ■409requests need update set Sharers also read set perform invalidations. Figure 5.21 shows actions taken directory response messages received. directory receives three different requests: read miss, write miss,and data write-back. messages sent response directory shown bold, updating set Sharers shown bold italics. stimulus messages external, actions shown gray. simplifiedprotocol assumes actions atomic, requesting value andsending another node; realistic implementation cannot use assumption. understand directory operations, let ’s examine requests received actions taken state state. block uncached state, copy inmemory current value, possible requests block ■Read miss —The requesting node sent requested data memory, requester made sharing node. state block ismade shared.Sharers = {} Invalidate; Sharers = {P}; data value replyData value reply Sharers = Sharers + {P} Data value reply; Sharers = {P} Data value reply; Sharers = {P} Write missFetch; data value reply; Sharers = Sharers + {P} Read missWrite missUncached Fetch/invalidate Data value reply Sharers = {P}Read miss Data write-backWrite missRead missShared (read only) Exclusive (read/write) Figure 5.21 state transition diagram directory states structure transition diagram individual cache. actions gray externally caused. Bold indicates action taken directory response request.410 ■Chapter Five Thread-Level Parallelism■Write miss —The requesting node sent value becomes sharing node. block made exclusive indicate valid copy cached. Sharers indicates identity owner. block shared state, memory value date, two requests occur: ■Read miss —The requesting node sent requested data memory, requesting node added sharing set. ■Write miss —The requesting node sent value. nodes set Sharers sent invalidate messages, Sharers set contain identity therequesting node. state block made exclusive. block exclusive state, current value block held cache node identified set Sharers (the owner), three pos-sible directory requests: ■Read miss —The owner sent data fetch message, causes state block owner ’s cache transition shared causes owner send data directory, written memory sent back requesting processor. identity requesting node added set Sharers, still contains identity processor owner(since still readable copy). ■Data write-back —The owner replacing block therefore must write back. write-back makes memory copy date (the home directoryessentially becomes owner), block uncached, Sharers setis empty. ■Write miss —The block new owner. message sent old owner, causing cache invalidate block send value directory,from sent requesting node, becomes new owner.Sharers set identity new owner, state block remainsexclusive. state transition diagram Figure 5.21 simplification, snooping cache case. case directory, well snooping schemeimplemented network bus, protocols need deal withnonatomic memory transactions. Appendix explores issues depth. directory protocols used real multiprocessors contain additional optimi- zations. particular, protocol read write miss occurs blockthat exclusive, block first sent directory home node. stored home memory also sent original requesting node. Many protocols use commercial multiprocessors forward data fromthe owner node requesting node directly (as well performing write-back home). optimizations often add complexity increasing pos-sibility deadlock increasing types messages must handled.5.4 Distributed Shared-Memory Directory-Based Coherence ■411Implementing directory scheme requires solving challenges discussed snooping protocols. are, however, new additional prob- lems, describe Appendix I. Section 5.8 , briefly describe mod- ern multicores extend coherence beyond single chip. combinations ofmultichip coherence multicore coherence include four possibilities snoop-ing/snooping (AMD Opteron), snooping/directory, directory/snooping, direc-tory/directory! Many multiprocessors chosen form snooping withina single chip, attractive outermost cache shared inclusive,and directories across multiple chips. approach simplifies implementation processor chip, rather individual core, need tracked. 5.5 Synchronization: Basics Synchronization mechanisms typically built user-level software routinesthat rely hardware-supplied synchronization instructions. smaller multipro- cessors low-contention situations, key hardware capability uninterrup-tible instruction instruction sequence capable atomically retrieving andchanging value. Software synchronization mechanisms constructedusing capability. section, focus implementation lockand unlock synchronization operations. Lock unlock used straightfor-wardly create mutual exclusion, well implement complex synchro-nization mechanisms. high-contention situations, synchronization become performance bot- tleneck contention introduces additional delays latency ispotentially greater multiprocessor. discuss basic synchroni-zation mechanisms section extended large processor counts inAppendix I. Basic Hardware Primitives key ability require implement synchronization multiprocessor aset hardware primitives ability atomically read modify memory location. Without capability, cost building basic synchronization primitives high increase processor count increases. Thereare number alternative formulations basic hardware primitives, ofwhich provide ability atomically read modify location, together withsome way tell whether read write performed atomically. Thesehardware primitives basic building blocks used build wide vari-ety user-level synchronization operations, including things locks andbarriers. general, architects expect users employ basic hardware primitives, instead expect primitives used system program- mers build synchronization library, process often complex tricky.Let’s start one hardware primitive show used build basic synchronization operations.412 ■Chapter Five Thread-Level ParallelismOne typical operation building synchronization operations atomic exchange , interchanges value register value memory. see use build basic synchronization operation, assume want tobuild simple lock value 0 used indicate lock free 1is used indicate lock unavailable. processor tries set lock doingan exchange 1, register, memory address corresponding tothe lock. value returned exchange instruction 1 otherprocessor otherwise already claimed access 0. latter case, valueis also changed 1, preventing competing exchange also retrieving 0. example, consider two processors try exchange simul- taneously: race broken exactly one processors performthe exchange first, returning 0, second processor return 1 doesthe exchange. key using exchange (or swap) primitive implement syn-chronization operation atomic: exchange indivisible, twosimultaneous exchanges ordered write serialization mechanisms.It impossible two processors trying set synchronization variable thismanner determine simultaneously set variable. number atomic primitives used implement synchronization. key property read update memoryvalue manner tell whether two operations executed atom-ically. One operation, present many older multiprocessors, test-and-set ,w h c h tests value sets value passes test. example, could define anoperation tested 0 set value 1, used fashion sim-ilar used atomic exchange. Another atomic synchronization primitive isfetch-and-increment : returns value memory location atomically incre- ments it. using value 0 indicate synchronization variable unclaimed, use fetch-and-increment, used exchange. areother uses operations like fetch-and-increment, see shortly. Implementing single atomic memory operation introduces challenges requires memory read write single, uninterruptibleinstruction. requirement complicates implementation coherencebecause hardware cannot allow operations read andthe write, yet must deadlock. alternative pair instructions second instruction returns value deduced whether pair instructionswas executed though instructions atomic. pair instructions iseffectively atomic appears though operations executed pro-cessor occurred pair. Thus, instruction pair effec-tively atomic, processor change value instructionpair. approach used MIPS processors RISC V. RISC V, pair instructions includes special load called load reserved (also called load linked load locked) special store called store conditional . Load reserved loads contents memory given rs1 rd creates reservation memory address. Store conditional stores value inrs2 memory address given rs1. reservation load broken by5.5 Synchronization: Basics ■413a write memory location, store conditional fails writes non- zero rd; succeeds, store conditional writes 0. processor con- text switch two instructions, store conditional always fails. instructions used sequence, load reserved returns initial value store conditional returns 0 succeeds, followingsequence implements atomic exchange memory location specified thecontents x1 value x4: try: mov x3,x4 ;mov exchange value lr x2,x1 ;load reserved fromsc x3,0(x1) ;store conditionalbnez x3,try ;branch store failsmov x4,x2 ;put load value x4 end sequence, contents x4and memory location specified byx1have atomically exchanged. Anytime processor intervenes modifies value memory lrandscinstructions, screturns 0i nx3, causing code sequence try again. advantage load reserved/store conditional mechanism used build synchronization primitives. example, atomicfetch-and-increment: try: lr x2,x1 ;load reserved 0(x1) addi x3,x2,1 ;increment sc x3,0(x1) ;store conditionalbnez x3,try ;branch store fails instructions typically implemented keeping track address specified lrinstruction register, often called reserved register .I f interrupt occurs, cache block matching address link registeris invalidated (e.g., another sc), link register cleared. scinstruction simply checks address matches reserved register. so, scsuc- ceeds; otherwise, fails. store conditional fail either anotherattempted store load reserved address exception, care must taken inchoosing instructions inserted two instructions. particular, register-register instructions safely permitted; otherwise, possible create deadlock situations processor never complete sc.I na - tion, number instructions load reserved store conditionalshould small minimize probability either unrelated event com-peting processor causes store conditional fail frequently. Implementing Locks Using Coherence atomic operation, use coherence mechanisms amultiprocessor implement spin locks —locks processor continuously tries acquire, spinning around loop succeeds. Spin locks used pro-grammers expect lock held short amount time want process locking low latency lock available. Because414 ■Chapter Five Thread-Level Parallelismspin locks tie processor waiting loop lock become free, inappropriate circumstances. simplest implementation, would use cache coherence, would keep lock variables memory. processor could con-tinually try acquire lock using atomic operation, say, atomic exchange,and test whether exchange returned lock free. release lock, theprocessor simply stores value 0 lock. code sequence locka spin lock whose address x1. uses EXCH macro atomic exchange sequence page 414: addi x2,R0,#1 lockit: EXCH x2,0(x1) ;atomic exchange bnez x2,lockit ;already locked? multiprocessor supports cache coherence, cache locks using coherence mechanism maintain lock value coherently. Caching locks hastwo advantages. First, allows implementation process “spinning ” (trying test acquire lock tight loop) could done local cached copy rather requiring global memory access attempt acquire lock. second advantage comes observation often localityin lock accesses; is, processor used lock last use thenear future. cases, lock value may reside cache processor,greatly reducing time acquire lock. Obtaining first advantage —being able spin local cached copy rather generating memory request attempt acquire lock —requires change simple spin procedure. attempt exchange preceding loop requires write operation. multiple processors attempting get lock, generate write. writes lead write missesbecause processor trying obtain lock variable exclusive state. Thus modify spin lock procedure spins reads local copy lock successfully sees lock available. Thenit attempts acquire lock swap operation. processor first reads thelock variable test state. processor keeps reading testing valueof read indicates lock unlocked. processor races processes similarly “spin waiting ”to see lock var- iable first. processes use swap instruction reads old value stores a1 lock variable. single winner see 0, losers see 1that placed winner. (The losers continue set variable tothe locked value, ’t matter.) winning processor executes code lock and, finished, stores 0 lock variable release thelock, starts race again. code perform spin lock(remember 0 unlocked 1 locked): lockit: ld x2,0(x1) ;load lock bnez x2,lockit ;not available-spinaddi x2,R0,#1 ;load locked valueEXCH x2,0(x1) ;swap bnez x2,lockit ;branch lock ’t05.5 Synchronization: Basics ■415Let’s examine “spin lock ”scheme uses cache coherence mecha- nisms. Figure 5.22 shows processor bus directory operations multiple processes trying lock variable using atomic swap. processor withthe lock stores 0 lock, caches invalidated must fetch thenew value update copy lock. One cache gets copy theunlocked value (0) first performs swap. cache miss pro-cessors satisfied, find variable already locked, must returnto testing spinning. example shows another advantage load reserved/store conditional primitives: read write operations explicitly separated. load reserved need cause bus traffic. fact allows following simple codesequence, characteristics optimized version usingexchange ( x1has address lock, lrhas replaced LD, schas replaced EXCH ):Step P0 P1 P2Coherence state lock atend step Bus/directory activity 1 lock Begins spin, testing lock ¼0Begins spin, testing lock ¼0Shared Cache misses P1 P2 satisfied either order. Lock state becomes shared. 2 Set lock 0(Invalidate received)(Invalidate received)Exclusive (P0) Write invalidate lock variable P0. 3 Cache miss Cache miss Shared Bus/directory services P2 cache miss; write-back P0; stateshared. 4 (Waits bus/ directory busy)Lock¼0 test succeedsShared Cache miss P2 satisfied. 5 Lock ¼0 Executes swap, gets cache missShared Cache miss P1 satisfied. 6 Executes swap, gets cache missCompletes swap: returns 0 setslock¼1Exclusive (P2) Bus/directory services P2 cache miss; generates invalidate; lock isexclusive. 7 Swap completes returns 1, sets lock ¼1Enter critical sectionExclusive (P1) Bus/directory services P1 cache miss; sends invalidate generates write-back P2. 8 Spins, testing lock¼0None Figure 5.22 Cache coherence steps bus traffic three processors, P0, P1, P2. figure assumes write invalidate coherence. P0 starts lock (step 1), value lock 1 (i.e., locked); initially exclu- sive owned P0 step 1 begins. P0 exits unlocks lock (step 2). P1 P2 race see reads unlocked value swap (steps 3 –5). P2 wins enters critical section (steps 6 7), P1 ’s attempt fails, starts spin waiting (steps 7 8). real system, events take many 8 clock ticks acquiring bus replying misses take much longer. step 8 reached, process repeat P2, eventually getting exclusive access setting lock 0.416 ■Chapter Five Thread-Level Parallelismlockit: lr x2,0(x1) ;load reserved bnez x2,lockit ;not available-spin addi x2,R0,#1 ;locked valuesc x2,0(x1) ;storebnez x2,lockit ;branch store fails first branch forms spinning loop; second branch resolves races two processors see lock available simultaneously. 5.6 Models Memory Consistency: Introduction Cache coherence ensures multiple processors see consistent view mem-ory. answer question howconsistent view memory must be. By“how consistent, ”we really asking processor must see value updated another processor. processors communicate throughshared variables (used data values synchronization), questionboils this: order must processor observe data writes ofanother processor? way “observe writes another proces- sor”is reads, question becomes properties must enforced among reads writes different locations different processors? Although question consistent memory must seems simple, remarkably complicated, see simple example. two code segments processes P1andP2, shown side side: P1: = 0; P2: B = 0; ..... ..... A=1 ; B=1 ; L1: (B == 0)... L2: (A == 0)... Assume processes running different processors, locations andBare originally cached processors initial value 0. writes always take immediate effect immediately seen processors, willbe impossible statements (labeled L1andL2) evaluate condi- tions true, since reaching statement means either AorBmust assigned value 1. suppose write invalidate delayed, theprocessor allowed continue delay. possible bothP1andP2have seen invalidations BandA(respectively) attempt read values. question behavior allowed, and, so, conditions? straightforward model memory consistency called sequential consistency . Sequential consistency requires result execution though memory accesses executed processor kept inorder accesses among different processors arbitrarily interleaved.Sequential consistency eliminates possibility nonobvious executionin previous example assignments must completed theIF statements initiated.5.6 Models Memory Consistency: Introduction ■417The simplest way implement sequential consistency require processor delay completion memory access invalidations caused access completed. course, equally effective delay next mem-ory access previous one completed. Remember memory consis-tency involves operations among different variables: two accesses mustbe ordered actually different memory locations. example, mustdelay read AorB(A= =0 orB= =0 ) previous write completed (B=1 orA=1 ). sequential consistency, cannot, example, simply place write write buffer continue read. Although sequential consistency presents simple programming paradigm, reduces potential performance, especially multiprocessor large numberof processors long interconnect delays, see following example. Example Suppose processor write miss takes 50 cycles establish own- ership, 10 cycles issue invalidate ownership established, 80cycles invalidate complete acknowledged issued. Assum-ing four processors share cache block, long write miss stallthe writing processor processor sequentially consistent? Assume invalidates must explicitly acknowledged coherence controller knows completed. Suppose could continue executing obtainingownership write miss without waiting invalidates; long wouldthe write take? Answer wait invalidates, write takes sum ownership time plus time complete invalidates. invalidates overlap, needonly worry last one, starts 10+10+10+10 ¼40 cycles own- ership established. Therefore total time write 50+40+80 ¼170 cycles. comparison, ownership time 50 cycles. appropriate writebuffer implementations, even possible continue ownership isestablished. provide better performance, researchers architects explored two dif- ferent routes. First, developed ambitious implementations preservesequential consistency use latency-hiding techniques reduce penalty;we discuss Section 5.7 . Second, developed less restrictive memory consistency models allow faster hardware. models affect howthe programmer sees multiprocessor, discuss less restrictive models, let ’s look programmer expects. Programmer ’s View Although sequential consistency model performance disadvantage, viewpoint programmer, advantage simplicity. challenge418 ■Chapter Five Thread-Level Parallelismis develop programming model simple explain yet allows high- performance implementation. One programming model allows us efficient imple- mentation assume programs synchronized . program synchronized accesses shared data ordered synchronization operations. data ref-erence ordered synchronization operation if, every possible execution, awrite variable one processor access (either read write) thatvariable another processor separated pair synchronization opera-tions, one executed write writing processor one executed access second processor. Cases variables may updated without ordering synchronization called data races execution outcome depends relative speed processors, like races hardware design,the outcome unpredictable, leads another name synchronized pro-grams: data-race-free . simple example, consider variable read updated two dif- ferent processors. processor surrounds read update lock anunlock, ensure mutual exclusion update ensure read consistent. Clearly, every write separated read processor pair synchronization operations: one unlock (after write) one lock(before read). course, two processors writing variable inter-vening reads, writes must also separated synchronization operations. broadly accepted observation programs synchronized. observation true primarily because, accesses unsynchronized, thebehavior program would likely unpredictable speed exe-cution would determine processor data race thus affect results program. Even sequential consistency, reasoning programs difficult. Programmers could attempt guarantee ordering constructing synchronization mechanisms, extremely tricky, lead buggy pro-grams, may supported architecturally, meaning may workin future generations multiprocessor. Instead, almost programmers willchoose use synchronization libraries correct optimized mul-tiprocessor type synchronization. Finally, use standard synchronization primitives ensures even architecture implements relaxed consistency model sequential consis-tency, synchronized program behave though hardware implementedsequential consistency. Relaxed Consistency Models: Basics Release Consistency key idea relaxed consistency models allow reads writes complete order, use synchronization operations enforce ordering asynchronized program behaves though processor sequentially5.6 Models Memory Consistency: Introduction ■419consistent. variety relaxed models classified according read write orderings relax. specify orderings set rules form X !Y, meaning operation X must complete operation done. Sequential consistency requires maintaining four possible orderings:R!W, R!R, W!R, W !W. relaxed models defined subset four orderings relax: 1.Relaxing W !R ordering yields model known total store ordering orprocessor consistency . model retains ordering among writes, many programs operate sequential consistency operate model, without additional synchronization. 2.Relaxing W !R ordering W !W ordering yields model known partial store order . 3.Relaxing four orderings yields variety models including weak ordering , PowerPC consistency model, release consistency , RISC V consistency model. relaxing orderings, processor may obtain significant performance advantages, reason RISC V, ARMv8, well C++ andC language standards chose release consistency model. Release consistency distinguishes synchronization operations used acquire access shared variable (denoted A) release object allow another processor acquire access (denoted R). Release consistency based observation synchronized programs,an acquire operation must precede use shared data, release operationmust follow updates shared data also precede time nextacquire. property allows us slightly relax ordering observing thata read write precedes acquire need complete acquire,and also read write follows release need wait release.Thus orderings preserved involve Aand R,a ss h w ni n Figure 5.23 ;a st h ee x p l ei n Figure 5.24 shows, model imposes few- est orders five models. Release consistency provides one least restrictive models easily checkable ensures synchronized programs see sequentially consis-tent execution. Although synchronization operations either acquire arelease (an acquire normally reads synchronization variable atomicallyupdates it, release usually writes it), operations, barrier,act acquire release cause ordering equivalent weak ordering. Although synchronization operations always ensure previous writes completed, may want guarantee writes completed without anidentified synchronization operation. cases, explicit instruction, calledFENCE RISC V, used ensure previous instructions threadhave completed, including completion memory writes associated inval-idates. information complexities, implementation issues, and420 ■Chapter Five Thread-Level Parallelism = B = acquire (S); C = = release (S); E = F =Sequential consistency = B = acquire (S); C = = release (S); E = F =TSO (total store order) processor consistency = B = acquire (S); C = = release (S); E = F =PSO (partial store order) = B = acquire (S); C = = release (S); E = F =Weak ordering Release consistency = B = acquire (S); C = = release (S); E = F = Figure 5.24 examples five consistency models discussed section show reduction number orders imposed models become relaxed. minimum orders shown arrows. Orders implied transitivity, write C release sequential consistency model theacquire release weak ordering release consistency, shown.Model Used inOrdinary orderings Synchronization orderings Sequential consistency machines optional modeR!R, R!W, W!R, W!WS!W, S!R, R!S, W!S, S!S Total store order processor consistencyIBMS/370, DEC VAX, SPARCR!R, R!W, W!WS!W, S!R, R!S, W!S, S!S Partial store order SPARC R !R, R!WS !W, S!R, R!S, W!S, S!S Weak ordering PowerPC !W, S!R, R!S, W!S, S!S Release consistency MIPS, RISC V, Armv8, C, C++ specificationsSA!W, A!R, R!SR,W!SR, SA!SA,SA!SR,SR!SA,SR!SR Figure 5.23 orderings imposed various consistency models shown ordinary accesses synchronization accesses. models grow restrictive (sequential consistency) least restrictive (release consistency), allowing increased flexibility implementation. weaker models rely fences created syn- chronization operations, opposed implicit fence every memory operation. Aand Rstand acquire release operations, respectively, needed define release consistency. use notation Aand SRfor consistently, ordering one would become two orderings (e.g., !W becomes A!W, SR!W), !S would become four orderings shown last line bottom-right table entry.5.6 Models Memory Consistency: Introduction ■421performance potential relaxed models, highly recommend excellent tutorial Adve Gharachorloo (1996) . 5.7 Cross-Cutting Issues multiprocessors redefine many system characteristics (e.g., performance assessment, memory latency, importance scalability), introduceinteresting design problems cut across spectrum, affecting hardwareand software. section, give several examples related issue mem-ory consistency. examine performance gained multithreading isadded multiprocessing. Compiler Optimization Consistency Model Another reason defining model memory consistency specify rangeof legal compiler optimizations performed shared data. explicitlyparallel programs, unless synchronization points clearly defined pro-grams synchronized, compiler cannot interchange read write twodifferent shared data items transformations might affect semanticsof program. restriction prevents even relatively simple optimizations, suchas register allocation shared data, process usually interchanges reads writes. implicitly parallelized programs —for example, written High Performance Fortran (HPF) —programs must synchronized synchronization points known, issue arise. Whether compilerscan get significant advantage relaxed consistency models remains anopen question, research viewpoint practical viewpoint,where lack uniform models likely retard progress deploying compilers. Using Speculation Hide Latency Strict Consistency Models saw Chapter 3 , speculation used hide memory latency. also used hide latency arising strict consistency model, giving much benefit relaxed memory model. key idea processor usedynamic scheduling reorder memory references, letting possibly executeout order. Executing memory references order may generate violationsof sequential consistency, might affect execution program. Thispossibility avoided using delayed commit feature speculative proces-sor. Assume coherency protocol based invalidation. processorreceives invalidation memory reference memory reference committed, processor uses speculation recovery back computation restart memory reference whose address invalidated. reordering memory requests processor yields execution order could result outcome differs would seen under422 ■Chapter Five Thread-Level Parallelismsequential consistency, processor redo execution. key using approach processor need guarantee result would accesses completed order, achieve detecting whenthe results might differ. approach attractive speculative restartwill rarely triggered. triggered unsynchronizedaccesses actually cause race ( Gharachorloo et al., 1992 ). Hill (1998) advocated combination sequential processor consistency together speculative execution consistency model choice. argu-ment three parts. First, aggressive implementation either sequential con- sistency processor consistency gain advantage relaxed model. Second, implementation adds little implementation costof speculative processor. Third, approach allows programmer rea-son using simpler programming models either sequential processor con-sistency. MIPS R10000 design team insight mid-1990s andused R10000 ’s out-of-order capability support type aggressive imple- mentation sequential consistency. One open question successful compiler technology optimizing memory references shared variables. state optimization technology fact shared data often accessed via pointers array indexing lim-ited use optimizations. technology become available andlead significant performance advantages, compiler writers would want ableto take advantage relaxed programming model. possibility thedesire keep future flexible possible led RISC V designers opt forrelease consistency, long series debates. Inclusion Implementation multiprocessors use multilevel cache hierarchies reduce demand onthe global interconnect latency cache misses. cache also providesmultilevel inclusion —every level cache hierarchy subset level farther away processor —then use multilevel structure reduce contention coherence traffic processor traffic occurs snoops processor cache accesses must contend cache. Many multiprocessors multilevel caches enforce inclusion property, although recent multiproces-sors smaller L1 caches different block sizes sometimes chosen toenforce inclusion. restriction also called subset property cache subset cache hierarchy. first glance, preserving multilevel inclusion property seems trivial. Con- sider two-level example: miss L1 either hits L2 generates miss inL2, causing brought L1 L2. Likewise, invalidate hits L2 must sent L1, cause block invalidated exists. catch happens block sizes L1 L2 different. Choosing different block sizes quite reasonable, since L2 much largerand much longer latency component miss penalty, thus want5.7 Cross-Cutting Issues ■423to use larger block size. happens “automatic ”enforcement inclu- sion block sizes differ? block L2 represents multiple blocks L1, miss L2 causes replacement data equivalent multiple L1blocks. example, block size L2 four times L1, miss inL2 replace equivalent four L1 blocks. Let ’s consider detailed example. Example Assume L2 block size four times L1. Show miss address causes replacement L1 L2 lead violation inclu-sion property. Answer Assume L1 L2 direct-mapped block size L1 bbytes block size L2 4 bbytes. Suppose L1 contains two blocks starting addresses xandx+band xmod 4 b¼0, meaning xalso starting address block L2; single block L2 contains L1 blocks x, x+b,x+2b, x+3b. Suppose processor generates reference block ythat maps block containing xin caches thus misses. L2 missed, fetches 4 bbytes replaces block containing x,x+b,x+2b,a n x+3b, L1 takes bbytes replaces block containing x. L1 still con- tains x+b, L2 not, inclusion property longer holds. maintain inclusion multiple block sizes, must probe higher levels hierarchy replacement done lower level ensure words replaced lower level invalidated higher-level caches;different levels associativity create sort problems. Baer Wang (1988) described advantages challenges inclusion detail, 2017 designers opted implement inclusion, often settlingon one block size levels cache. example, Intel i7 uses inclusionfor L3, meaning L3 always includes contents L2 L1. Thisdecision allows i7 implement straightforward directory scheme L3 minimize interference snooping L1 L2 circum- stances directory indicates L1 L2 cached copy. TheAMD Opteron, contrast, makes L2 inclusive L1 restrictionfor L3. uses snooping protocol, needs snoop L2 unless ahit, case snoop sent L1. Performance Gains Multiprocessing Multithreading section, briefly look study effectiveness using multithread-ing multicore processor, IBM Power5; return topic thenext section, examine performance Intel i7. IBM Power5 isa dual-core supports simultaneous multithreading (SMT); basic architectureis similar recent Power8 (which examine next section),but two cores per processor.424 ■Chapter Five Thread-Level ParallelismTo examine performance multithreading multiprocessor, measure- ments made IBM system eight Power5 processors, using one core processor. Figure 5.25 shows speedup 8-processor Power5 multiprocessor, without SMT, SPECRate2000 benchmarks, asdescribed caption. average, SPECintRate 1.23 times faster, andthe SPECfpRate 1.16 times faster. Note floating-point benchmarksexperience slight decrease performance SMT mode, maximumwupwise swim mgrid applu mesa galgelSPECfpRate SPECintRateart equake facerec ammp lucas fma3d sixtrack apptu gzip vpr gcc mcf crafty parser eon perlbmk gap vortex bzip2 twolf 0.9 1.0 1.1 1.2 Speedup1.3 1.4 1.5 Figure 5.25 comparison SMT single-thread (ST) performance 8-processor IBM eServer p5 575 using SPECfpRate (top half) SPECintRate (bottom half) benchmarks. Note x-axis starts speedup 0.9, performance loss. one processor Power5 core active, slightly improve resultsfrom SMT decreasing destructive interference memory system. SMT results obtained creating 16user threads, whereas ST results use eight threads; one thread per processor, Power5 switched single-threaded mode OS. results collected John McCalpin IBM. see data, standard deviation results SPECfpRate higher SPECintRate (0.13 versus0.07), indicating SMT improvement FP programs likely vary widely.5.7 Cross-Cutting Issues ■425reduction speedup 0.93. Although one might expect SMT would better job hiding higher miss rates SPECFP benchmarks, appears limits memory system encountered running SMT modeon benchmarks. 5.8 Putting Together: Multicore Processors Performance roughly 10 years, multicore primary focus scaling perfor- mance, although implementations vary widely, support largermultichip multiprocessors. section, examine design three differentmulticores, support provide larger multiprocessors, perfor-mance characteristics, broader evaluation small large multi-processor Xeon systems, concluding detailed evaluation themulticore i7 920, predecessor i7 6700. Performance Multicore-Based Multiprocessors Multiprogrammed Workload Figure 5.26 shows key characteristics three multicore processors designed server applications available 2015 2017. Intel Xeon E7 based basic design i7, cores, slightly slowerclock rate (power limitation), larger L3 cache. Power8 new-est IBM Power series features cores bigger caches. FujitsuSPARC64 X+ newest SPARC server chip; unlike T-series mentioned inChapter 3 , uses SMT. processors configured multicore multiprocessor servers, available family, varying processor count,cache size, on, figure shows. three systems show range techniques connecting on- chip cores connecting multiple processor chips. First, let ’s look cores connected within chip. SPARC64 X+ simplest: sharesa single L2 cache, 24-way set associative, among 16 cores. arefour separate DIMM channels attach memory accessible 16 /C24 switch cores channels. Figure 5.27 shows Power8 Xeon E7 chips organized. core Power8 8 MiB bank L3 directly connected; banks accessed via interconnection network, 8 separate buses. Thus Power8 true NUCA ( Nonuniform Cache Architecture ), access time attached bank L3 much faster accessing another L3. EachPower8 chip set links used build large multiprocessor usingan organization see shortly. memory links connected specialmemory controller includes L4 interfaces directly DIMMs. Part B Figure 5.27 , shows Xeon E7 processor chip organized 18 cores (20 cores shown figure). Three rings426 ■Chapter Five Thread-Level Parallelismconnect cores L3 cache banks, core bank L3 connected two rings. Thus cache bank core accessible anyother core choosing right ring. Therefore, within chip, E7 uni-form access time. practice, however, E7 normally operated NUMAarchitecture logically associating half cores memory channel; thisFeature IBM Power8 Intel Xeon E7 Fujitsu SPARC64 X+ Cores/chip 4, 6, 8, 10, 12 4, 8, 10, 12, 22, 24 16 Multithreading SMT SMT SMTThreads/core 8 2 2Clock rate 3.1 –3.8 GHz 2.1 –3.2 GHz 3.5 GHz L1 cache 32 KB per core 32 KB per core 64 KB per coreL1 cache 64 KB per core 32 KB per core 64 KB per coreL2 cache 512 KB per core 256 KB per core 24 MiB sharedL3 cache L3: 32 –96 MiB: 8 MiB per core (using eDRAM); shared nonuniform access time10–60 MiB@2.5 MiB per core; shared, larger core countsNone Inclusion Yes, L3 superset Yes, L3 superset Yes Multicore coherenceprotocolExtended MESI behavioral locality hints(13-states)MESIF: extended form MESI allowing direct transfers cleanblocksMOESI Multichip coherence implementationHybrid strategy snooping directoryHybrid strategy snooping directoryHybrid strategy snooping directory Multiprocessor interconnectsupportCan connect 16 processor chips 1 2hops reach processorUp 8 processor chips directly via Quickpath; larger system anddirectory support additional logicCrossbar interconnect chip, supports 64 processors;includes directory support Processor chip range1–16 2 –32 1 –64 Core count range4–192 12 –576 8 –1024 Figure 5.26 Summary characteristics three recent high-end multicore processors (2015 –2017 releases) designed servers. table shows range processor counts, clock rates, cache sizes within pro- cessor family. Power8 L3 NUCA (Nonuniform Cache Access) design, also supports off-chip L4 128 MiB using EDRAM. 32-core Xeon recently announced, system shipments occurred. TheFujitsu SPARC64 also available 8-core design, normally configured single processor system. Thelast row shows range configured systems published performance data (such SPECintRate) processor chip counts total core counts. Xeon systems include multiprocessors extend basic inter- connect additional logic; example, using standard Quickpath interconnect limits processor count to8 largest system 8 /C224¼192 cores, SGI extends interconnect (and coherence mechanisms) extra logic offer 32 processor system using 18-core processor chips total size 576 cores. Newer releases processors increased clock rates (significantly Power8 case, less others) core counts (signifi-cantly case Xeon).5.8 Putting Together: Multicore Processors Performance ■427(A) Power8 chip organizationCore 512 KB L2 8 MB L3 RegionCore 512 KB L2 8 MB L3 RegionCore 512 KB L2 8 MB L3 RegionCore 512 KB L2 8 MB L3 RegionUp 12 Cores . . . . . . On-chip coherence data interconnect Off-chip Intra-group SMP CtrlOff-chip Intergroup SMP CtrlInterrupt ControlPHBMemory Control x8Memory Control x8Memory Control x8Memory Control x8Memory Control x8Memory Control x8Memory Control x8Memory ControlPHBPHB MuxAccelerator Interface RNG Accel Crypto AccelCmp AccelCAPP Intragroup links 8 B @ 4.8 GHz / direction x 3 single-ended linksIntergroup links 2 B @ 6.4 GHz / direction x 3 differential linksPCI gen 3 I/O 8 GHz / direction differential (16x+16x) (16x+8x+8x)2 B @ 9.6 GHz read, 1B+cmd @ 9.6 GHz write x 8 differential channels (B)Core >=2.5 GHz Core BoCore >=2.5 GHzCore BoCore >=2.5 GHzCore Bo Core >=2.5 GHzCore Bo Core >=2.5 GHzCore Bo Core >=2.5 GHzCore Bo Core >=2.5 GHzCore BoCore >=2.5 GHzCore Bo Core >=2.5 GHzCore Bo Core >=2.5 GHzCore Bo Core >=2.5 GHzCore BoCacheBoL3 2.5 MBCacheBoL3 2.5 MBCacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MBCacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBo Xeon E7 organizationL3 2.5 MBCacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MB CacheBoL3 2.5 MBCore >=2.5 GHz Core Bo Home Agent DD (4 channels)Core >=2.5 GHz Core Bo Core >=2.5 GHz Core Bo Core >=2.5 GHz Core Bo DD DD DD DDHome Agent DDR (2 channels) DD DDLink0 Link1 QPI Agent 2x20, 9.6 GT/s Link2 QPI Agent 1x20, 9.6 GT/sPCle x10 PCle x8 PCle x16 x4 DMI Port0 Port1 Port2 DMI DMA IOAPIC IIO UBox PCUGlobally routed power management links Msg Ch Figure 5.27 on-chip organizations Power8 Xeon E7 shown. Power8 uses 8 separate buses L3 CPU cores. Power8 also two sets links connecting larger multiprocessors. Xeon uses three rings connect processors L3 cache banks, well QPI interchip links. Software usedto logically associate half cores memory channel.increases probability desired memory page open given access. E7 provides 3 QuickPath Interconnect (QPI) links connecting multiple E7s. Multiprocessors consisting multicores use variety different inter- connection strategies, Figure 5.28 shows. Power8 design provides support connecting 16 Power8 chips total 192 cores. intragroup links pro-vide higher bandwidth interconnect among completely connected module 4processor chips. intergroup links used connect processor chip tothe 3 modules. Thus processor two hops other, mem-ory access time determined whether address resides local memory, clus- ter memory, intercluster memory (actually latter two different values, difference swamped intercluster time). Xeon E7 uses QPI interconnect multiple multicore chips. 4-chip, multiprocessor, latest announced Xeon could 128 cores,the three QPI links processor connected three neighbors, yieldinga 4-chip fully connected multiprocessor. memory directly connectedto E7 multicore, even 4-chip arrangement nonuniform memory accesstime (local versus remote). Figure 5.28 shows 8 E7 processors con- nected; like Power8, leads situation every processor one two hops every processor. number Xeon-based mul-tiprocessor servers 8 processor chips. designs, typ-ical organization connect 4 processor chips together square, module,with processor connecting two neighbors. third QPI chip isconnected crossbar switch. large systems created fashion.Memory accesses occur four locations different timings: local tothe processor, immediate neighbor, neighbor cluster two hops away, crossbar. organizations possible require less full crossbar return hops get remote memory. SPARC64 X+ also uses 4-processor module, processor three connections immediate neighbors plus two (or three largest con-figuration) connections crossbar. largest configuration, 64 processorchips connected two crossbar switches, total 1024 cores. Memoryaccess NUMA (local, within module, crossbar), coherencyis directory-based. Performance Multicore-Based Multiprocessors Multiprogrammed Workload First, compare performance scalability three multicore processors using SPECintRate, considering configurations 64 cores. Figure 5.29 shows performance scales relative performance smallest configura-tion, varies 4 16 cores. plot, smallest configuration isassumed perfect speedup (i.e., 8 8 cores, 12 12 cores, etc.). Thisfigure notshow performance among different processors. Indeed performance varies significantly: 4-core configuration, IBM Power8 is5.8 Putting Together: Multicore Processors Performance ■4294 chip group 12-core Power 812-core Power 812-core Power 812-core Power 8 12-core Power 812-core Power 812-core Power 812-core Power 8 12-core Power 812-core Power 812-core Power 812-core Power 8 12-core Power 812-core Power 812-core Power 812-core Power 825.6 GB/s Inter-group Cable 78.4 GB/s Intra-group Bus Power8 system 16 chips. (A) (B) Xeon E7 system showing 8 chips.E7 E7 E7 E7E7 E7 E7 E7 (C) SPARC64 X+ 4-chip building block.CPU CPU XB XB CPU CPU Figure 5.28 system architecture three multiprocessors built multicore chips.430 ■Chapter Five Thread-Level Parallelism1.5 times fast SPARC64 X+ per core basis! Instead Figure 5.29 shows performance scales processor family additional coresare added. Two three processors show diminishing returns scale 64 cores. Xeon systems appear show degradation 56 64 cores. Thismay largely due cores share smaller L3. example, 40-core system uses 4 chips, 60 MiB L3, yielding 6 MiB L3 per core.The 56-core 64-core systems also use 4 chips 35 45 MiB L3 per chip, 2.5 –2.8 MiB per core. likely resulting larger L3 miss rates lead reduction speedup 56-core 64-core systems. IBM Power8 results also unusual, appearing show significant super- linear speedup. effect, however, due largely differences clock rates,which much larger across Power8 processors processors44 8 1 21 62 02 42 83 2 Number coresSpeedup relative smallest configuration 36 40 44 48 52 56 60 64812162024283236404448525660646872768084 Intel Xeon E7IBM Power8 Fujitsu SPARC64 X+ Figure 5.29 performance scaling SPECintRate benchmarks four multicore processors number cores increased 64. Performance processor plotted relative smallest configuration assuming configuration perfect speedup. Although chart shows given multiprocessorscales additional cores, supply data performance among processors. differences clock rates, even within given processor family. generally swamped core scaling effects, except Power8 shows clock range spread 1.5 /C2from smallest configuration 64 core configuration.5.8 Putting Together: Multicore Processors Performance ■431in figure. particular, 64-core configuration highest clock rate (4.4 GHz), whereas 4-core configuration 3.0 GHz clock. normalize relative speedup 64-core system based clock rate differential withthe 4-core system, effective speedup 57 rather 84. Therefore, thePower8 system scales well, perhaps best among processors, notmiraculous. Figure 5.30 shows scaling three systems configurations 64 processors. again, clock rate differential explains Power8 results; theclock-rate equivalent scaled speedup 192 processors 167, versus 223, accounting clock rate differences. Even 167, Power8 scaling some- better SPARC64 X+ Xeon systems. Surprisingly, althoughthere effects speedup going smallest system 64 cores,they seem get dramatically worse larger configurations. Thenature workload, highly parallel user-CPU-intensive, andthe overheads paid going 64 cores probably lead result.020406080100120140160180200220240260280300320340360380400420440460480500520540560580600Speedup relative smallest configuration Number cores020406080100120140160180200220240260280300320340360380400420440460480500520540560580600Intel Xeon E7IBM Power8 Fujitu SPARC64 X+ Figure 5.30 scaling relative performance multiprocessor multicore. before, performance shown relative smallest available system. Xeon result 80 cores L3 effect showed smaller configurations. systems larger 80 cores 2.5 3.8 MiB L3 per core, 80-core, orsmaller, systems 6 MiB per core.432 ■Chapter Five Thread-Level ParallelismScalability Xeon MP Different Workloads section, focus scalability Xeon E7 multiprocessors three different workloads: Java-based commercially oriented workload, virtualmachine workload, scientific parallel processing workload, theSPEC benchmarking organization, described next. ■SPECjbb2015: Models supermarket system handles mix ofpoint-of-sale requests, online purcha ses, data-mining operations. performance metric throughput-or iented, use maximum per- formance measurement server side running multiple Java virtual machines. ■SPECVirt2013: Models collection virtual machines running independentmixes SPEC benchmarks, including CPU benchmarks, web servers,and mail servers. system must meet quality service guarantee eachvirtual machine. ■SPECOMP2012: collection 14 scientific engineering programs writ-ten OpenMP standard shared-memory parallel processing. Thecodes written Fortran, C, C++ range fluid dynamics tomolecular modeling image manipulation. previous results, Figure 5.31 shows performance assuming linear speedup smallest configuration, benchmarks varies 48 cores 72 cores, plotting performance relative smallest configu- ration. SPECjbb2015 SPECVirt2013 include significant systems soft-ware, including Java VM software VM hypervisor. thesystem software, interaction among processes small. contrast,SPECOMP2012 true parallel code multiple user processes sharing dataand collaborating computation. Let’s begin examining SPECjbb2015. obtains speedup efficiency (speedup/processor ratio) 78% 95%, showing good speedup, even largest configuration. SPECVirt2013 even better (for range sys- tem measured), obtaining almost linear speedup 192 cores. SPECjbb2015and SPECVirt2013 benchmarks scale application size (as TPCbenchmarks discussed Chapter 1 ) larger systems effects Amdahl ’s Law interprocess communication minor. Finally, let ’s turn SPECOMP2012, compute-intensive benchmarks one truly involves parallel processing. major trendvisible steady loss efficiency scale 30 576 cores 576 cores, system exhibits half efficiency showed 30 cores. reduction leads relative speedup 284, assuming 30-corespeedup 30. probably Amdahl ’s Law effects resulting limited parallelism well synchronization communication overheads. Unlike5.8 Putting Together: Multicore Processors Performance ■433the SPECjbb2015 SPECVirt2013, benchmarks scaled larger systems. Performance Energy Efficiency Intel i7 920 Multicore section, closely examine performance i7 920, predecessor 6700, two groups benchmarks considered Chapter 3 :t h e parallel Java benchmarks parallel PARSEC benchmarks (described detail inFigure 3.32 page 247). Although study uses older i7 920, remains, far, comprehensive study energy efficiency multicore processors andthe effects multicore combined SMT. fact i7 920 6700 aresimilar indicates basic insights also apply 6700.406080100120140160180200220240260280300320340Performance relative smallest configuration 406080100120140160180200220240260280300320340360380400420440460480500520540560580SPECVirtSC2013 SPECOMP2012SPECJBB2015 Figure 5.31 Scaling performance range Xeon E7 systems showing performance relative smallest benchmark configuration, assuming configuration gets perfect speedup (e.g., smallest SPEWCOMP configuration 30 cores assume performance 30 system). relative performance assessed data, comparisons across benchmarks relevance. Note difference scale vertical horizontal axes.434 ■Chapter Five Thread-Level ParallelismFirst, look multicore performance scaling versus single-core without use SMT. combine multicore SMT capabil- ity. data section, like earlier i7 SMT evaluation(Chapter 3 )c ef r Esmaeilzadeh et al. (2011) .T h ed e ti st h es ea used earlier (see Figure 3.32 page 247), except Java benchmarks tradebeans pjbb2005 removed (leaving five scalable Java bench-marks); tradebeans pjbb2005 never achieve speedup 1.55 even withfour cores total eight threads, thus appropriate evaluatingmore cores. Figure 5.32 plots speedup energy efficiency Java PARSEC benchmarks without use SMT. Energy efficiency computed ratio: energy consumed single-core run divided energy con-sumed two- four-core run (i.e., efficiency inverse energy con-sumed). Higher energy efficiency means processor consumes lessenergy computation, value 1.0 break-even 13.5 1.064 1.10 1.04 1.021.00 0.98 i7 2P 4P ener gy efficienc yi7 2P 4P speedup0.96 0.94 0.92 0.90 0.88 0.863 2.5 2 1.51.08 Java speedupPARSEC speedup Java energy efficiencyPARSEC energy efficiency 2P 4P Cores Figure 5.32 chart shows speedup energy efficiency two- four-core executions par- allel Java PARSEC workloads without SMT. data collected Esmaeilzadeh et al. (2011) using setup described Chapter 3 . Turbo Boost turned off. speedup energy efficiency summarized using harmonic mean, implying workload total time spent running benchmark 2 cores isequivalent.5.8 Putting Together: Multicore Processors Performance ■435point. unused cores cases deep sleep mode, minimized power consumption essentially turning off. comparing data single-core multicore benchmarks, important remember thatthe full energy cost L3 cache memory interface paid single-core (as well multicore) case. fact increases likelihood thatenergy consumption improve app lications scale reasonably well. H r n cm e ni su e dt os u r z er e u lts implication described caption. figure shows, PARSEC benchmarks get better speedup Java benchmarks, achieving 76% speedup efficiency (i.e., actual speedup divided processor count) four cores, whereas Java benchmarksachieve 67% speedup efficiency four cores. Although observation isclear data, analyzing di fference exists difficult. quite possible Amdahl ’s Law effects reduced speedup Java workload, includes typically serial parts, garbagecollector. addition, interaction processor architecture theapplication, affects issues cost synchronization commu- nication, may also play role. particular, well-parallelized applications, PARSEC, sometimes benefit advantageous ratio betweencomputation communication, reduces dependence communi-cations costs (see Appendix I). differences speedup translate differences energy efficiency. example, PARSEC benchmarks actually slightly improve energy effi-ciency single-core version; result may significantly affected bythe fact L3 cache effectively used multicore runs single-core case energy cost identical cases. Thus, PARSEC benchmarks, multicore approach achieves designers hopedfor switched ILP-focused design multicore design; namely,it scales performance fast faster scaling power, resulting constant oreven improved energy efficiency. Java case, see neither two- norfour-core runs break even energy efficiency lower speedup levelsof Java workload (although Java energy efficiency 2p run sameas PARSEC). energy efficiency four-core Java case reasonably high (0.94). likely ILP-centric processor would need even power achieve comparable speedup either PARSEC Java workload. Thusthe TLP-centric approach also certainly better ILP-centric approach forimproving performance applications. see Section 5.10 , reasons pessimistic simple, efficient, long-term scaling ofmulticore. Putting Multicore SMT Together Finally, consider combination multicore multithreading measur-ing two benchmark sets two four processors one two threads(a total four data points eight threads). Figure 5.33 shows speedup436 ■Chapter Five Thread-Level Parallelismand energy efficiency obtained Intel i7 processor count two four SMT employed, using harmonic mean summarize twobenchmarks sets. Clearly, SMT add performance sufficientthread-level parallelism available even multicore situation. example, four-core, no-SMT case, speedup efficiencies 67% 76% Java PARSEC, respectively. SMT four cores, ratios anastonishing 83% 97%. Energy efficiency presents slightly different picture. case PARSEC, speedup essentially linear four-core SMT case (eight threads), powerscales slowly, resulting energy efficiency 1.1 case. Javasituation complex; energy efficiency peaks two-core SMT (four-thread) run 0.97 drops 0.89 four-core SMT (eight-thread) run. seems highly likely Java benchmarks encountering Amdahl ’s Law effects four threads deployed. architects haveobserved, multicore shift responsibility performance (and thusenergy efficiency) programmer, results Java workloadcertainly bear out.12Px1T 2Px2T 4Px1T 4Px2T1.54 1.20 1.15 1.10 1.05i7 2Px1T, 2Px2T, 4Px1T, 4Px2T speedup i7 2Px1T, 2Px2T, 4Px1T, 4Px2T ener gy efficienc 1.00 0.95 0.90 0.85 0.803.5 3 2.5 2Java speedupPARSEC speedup Java energy efficiencyPARSEC energy efficiency Figure 5.33 chart shows speedup two- four-core executions parallel Java PARSEC workloads without SMT. Remember preceding results vary number threads two eight reflect archi-tectural effects application characteristics. Harmonic mean used summarize results, discussed Figure 5.32 caption.5.8 Putting Together: Multicore Processors Performance ■4375.9 Fallacies Pitfalls Given lack maturity understanding parallel computing, many hidden pitfalls uncovered either careful designers unfor-tunate ones. Given large amount hype surrounded multiprocessorsover years, common fallacies abound. included selection them. Pitfall Measuring performance multiprocessors linear speedup versus execution time. Graphs like Figures 5.32 and5.33, plot performance versus number processors, showing linear speedup, plateau, falling off, longbeen used judge success parallel processors. Although speedup onefacet parallel program, direct measure performance. first issueis power processors scaled: program linearly improves per-formance equal 100 Intel Atom processors (the low-end processor used net-books) may slower version run 8-core Xeon. especially careful floating-point-intensive programs; processing elements without hardware assist may scale wonderfully poor collective performance. Comparing execution times fair comparing best algo- rithms computer. Comparing identical code two computers mayseem fair, not; parallel program may slower uniprocessor thanon sequential version. Developing parallel program sometimes lead toalgorithmic improvements, comparing previously best-known sequentialprogram parallel code —which seems fair —will compare equivalent algorithms. reflect issue, terms relative speedup (same program) true speedup (best program) sometimes used. Results suggest superlinear performance, program nprocessors ntimes faster equivalent uniprocessor, may indicate comparison unfair, although instances “real”superlinear speedups encountered. example, scientific applications regu-larly achieve superlinear speedup small increases processor count (2 4 to8 16). results usually arise critical data structures fit aggregate caches multiprocessor 2 4 processors fit aggregate cache multiprocessor 8 16 processors. saw pre-vious section, differences (such high clock rate) may appear yield super-linear speedups comparing slightly different systems. summary, comparing performance comparing speedups best tricky worst misleading. Comparing speedups two different multiproces-sors necessarily tell us anything relative performance mul-tiprocessors, also saw previous section. Even comparing two different algorithms multiprocessor tricky must use true speedup, rather relative speedup, obtain valid comparison. Fallacy Amdahl ’s Law ’t apply parallel computers . 1987 head research organization claimed Amdahl ’s Law (see Section 1.9) broken MIMD multiprocessor. statement hardly438 ■Chapter Five Thread-Level Parallelismmeant, however, law overturned parallel computers; neglected portion program still limit performance. understand basis media reports, let ’s see Amdahl (1967) originally said: fairly obvious conclusion drawn point effort expended achieving high parallel processing rates wasted unless accompanied achievements sequential processing rates nearly magnitude. [p. 483] One interpretation law that, portions every program must sequential, limit useful economic number processors —say, 100. showing linear speedup 1000 processors, interpretation Amdahl ’s Law disproved. basis statement Amdahl ’s Law “overcome ”was use scaled speedup , also called weak scaling . researchers scaled bench- mark dataset size 1000 times larger compared unipro-cessor parallel execution times scaled benchmark. particularalgorithm, sequential portion program constant independent ofthe size input, rest fully parallel —thus, linear speedup 1000 processors. running time grew faster linear, programactually ran longer scaling, even 1000 processors. Speedup assumes scaling input true speedup, reporting misleading. parallel benchmarks often runon different-sized multiprocessors, important specify type applica-tion scaling permissible scaling done. Although simplyscaling data size processor count rarely appropriate, assuming fixedproblem size much larger processor count (called strong scaling ) often inappropriate, well, likely users given much larger multipro-cessor would opt run larger detailed version application. See Appendix discussion important topic. Fallacy Linear speedups needed make multiprocessors cost-effective . widely recognized one major benefits parallel computing offer a“shorter time solution ”than fastest uniprocessor. Many people, however, also hold view parallel processors cannot cost-effective uniproces-sors unless achieve perfect linear speedup. argument says that, becausethe cost multiprocessor linear function number processors, any- thing less linear speedup means performance/cost ratio decreases, mak- ing parallel processor less cost-effective using uniprocessor. problem argument cost function processor count also depends memory, I/O, overhead system (box,power supply, interconnect, etc.). also makes less sense multicore era,when multiple processors per chip. effect including memory system cost pointed Wood Hill (1995) . use example based recent data using TPC-C SPECRate benchmarks, argument could also made parallel sci- entific application workload, would likely make case even stronger.5.9 Fallacies Pitfalls ■439Figure 5.34 shows speedup TPC-C, SPECintRate, SPECfpRate IBM eServer p5 multiprocessor configured 4 –64 processors. figure shows TPC-C achieves better linear speedup. SPECintRateand SPECfpRate, speedup less linear, cost, unlikeTPC-C, amount main memory disk required scale less linearly. AsFigure 5.35 shows, larger processor counts actually cost- effective 4-processor configuration. comparing cost-performance two computers, must sure include accurate assessments totalsystem cost performance achievable. many applications largermemory demands, comparison dramatically increase attractivenessof using multiprocessor. Pitfall developing software take advantage of, optimize for, multiproces- sor architecture . long history software lagging behind multiprocessors, probably software problems much harder. give one example showthe subtlety issues, many examples could choose from. One frequently encountered problem occurs software designed uni- processor adapted multiprocessor environment. example, SGISpeedup72 64 56 48 40 32 2416 8 0Linear speedup Speedup TPM Speedup SPECintRate Speedup SPECfpRate 0 Processor count8 1 62 43 24 04 85 66 4 Figure 5.34 Speedup three benchmarks IBM eServer p5 multiprocessor configured 4, 8, 16, 32, 64 processors. dashed line shows linear speedup.440 ■Chapter Five Thread-Level Parallelismoperating system 2000 originally protected page table data structure single lock, assuming page allocation infrequent. uniprocessor, thisdoes represent performance problem. multiprocessor, become amajor performance bottleneck programs. Consider program uses large number pages initialized startup, UNIX statically allocated pages. Suppose program parallelized multiple processes allocate pages. page allocation requires use page table data structure, locked whenever inuse, even OS kernel allows multiple threads OS serialized ifthe processes try allocate pages (which exactly mightexpect initialization time).Performance/cost relative four-processor system1.15 1.101.05 01.00 0.95 0.90 0.85 Processor count8 1 62 43 24 04 85 66 4TPM performance/cost SPECint performance/cost SPECfp performance/cost Figure 5.35 performance/cost IBM eServer p5 multiprocessors 4 –64 processors shown relative 4-processor configuration. measurement 1.0 indicates configuration cost-effective 4-processor system. 8-processor configurations show advantage three benchmarks, whereas two thethree benchmarks show cost-performance advantage 16- 32-processor configurations. TPC-C, con- figurations used official runs, means disk memory scale nearly linearly processor count, 64-processor machine approximately twice expensive 32-processor version. contrast, diskand memory scaled slowly (although still faster necessary achieve best SPECRate 64 processors). particular, disk configurations go one drive 4-processor version four drives (140 GB) 64- processor version. Memory scaled 8 GiB 4-processor system 20 GiB 64-processor system.5.9 Fallacies Pitfalls ■441This page table serialization eliminates parallelism initialization sig- nificant impact overall parallel performance. performance bottleneck per- sists even multiprogramming. example, suppose split parallelprogram apart separate processes run them, one process per processor,so sharing processes. (This exactly one userdid, reasonably believed performance problem due unin-tended sharing interference application.) Unfortunately, lock still seri-alizes processes, even multiprogramming performance poor. Thispitfall indicates kind subtle significant performance bugs arise software runs multiprocessors. Like many key software compo- nents, OS algorithms data structures must rethought multiprocessorcontext. Placing locks smaller portions page table effectively eliminatesthe problem. Similar problems exist memory structures, increases thecoherence traffic cases sharing actually occurring. multicore became dominant theme everything desktops servers, lack adequate investment parallel software became apparent.Given lack focus, likely many years software systems use adequately exploit growing numbers cores. 5.10 Future Multicore Scaling 30 years, researchers designers predicted end uni- processors dominance multiprocessors. early years thiscentury, prediction constantly proven wrong. saw Chapter 3 , costs trying find exploit ILP became prohibitive efficiency(both silicon area power). course, multicore magically solvethe power problem clearly increases transistor count theactive number transistors switching, two dominant contributions power. see section, energy issues likely limit multicore scaling severely previously thought. ILP scaling failed limitations ILP available effi- ciency exploiting ILP. Similarly, combination two factors means thatsimply scaling performance adding cores unlikely broadly successful.This combination arises challenges posed Amdahl ’s Law, assesses efficiency exploiting parallelism, end Dennard ’s Scaling, dictates energy required multicore processor. understand factors, take simple model technology scaling (based extensive highly detailed analysis Esmaeilzadeh et al. (2012)).Let’s start reviewing energy consumption power CMOS. Recall Chapter 1 energy switch transistor given Energy /Capacitive load /C2Voltage2 CMOS scaling limited primarily thermal power, combination static leakage power dynamic power, tends dominate. Power given by442 ■Chapter Five Thread-Level ParallelismPower ¼Energy per Transistor /C2Frequency /C2Transistors switched ¼Capacitive load /C2Voltage2/C2Frequency /C2Transistors switched understand implications energy power scale, let ’s compare today ’s 22 nm technology technology projected available 2021 – 24 (depending rate Moore ’s Law continues slow down). Figure 5.36 shows comparison based technology projections resulting effects energy power scaling. Notice power scaling >1.0 means future device consumes power; case, 1.79 /C2as much. Consider implications one latest Intel Xeon processors, E7-8890, 24 cores, 7.2 billion transistors (including almost 70 MiB ofcache), operates 2.2 GHz, thermal power rating 165 watts, die size 456 mm 2. clock frequency already limited power dissipation: 4-core version clock 3.2 GHz, 10-core version 2.8 GHz clock. Withthe 11 nm technology, size die would accommodate 96 cores almost280 MiB cache operate clock rate (assuming perfect frequency scaling)of 4.9 GHz. Unfortunately, cores operating efficiency improve-ments, would consume 165 /C21.79¼295 watts. assume 165-W heat dissipation limit remains, 54 cores active. limit yields max-imum performance speedup 54/24 ¼2.25 5 –6 year period, less one- half performance scaling seen late 1990s. Furthermore, may Amdahl ’s Law effects, next example shows. Example Suppose 96-core future generation processor, average 54 cores busy. Suppose 90% time, use available cores;9% time, use 50 cores; 1% time strictly serial. muchspeedup might expect? Assume cores turned use anddraw power assume use different number cores distributedso need worry average power consumption. would theDevice count scaling (since transistor 1/4 size) 4 Frequency scaling (based projections device speed) 1.75Voltage scaling projected 0.81 Capacitance scaling projected 0.39Energy per switched transistor scaling (CV 2) 0.26 Power scaling assuming fraction transistors switching chip exhibits full frequency scaling1.79 Figure 5.36 comparison 22 nm technology 2016 future 11 nm technology, likely available sometime 2022 2024. characteristics 11 nm technology based International Tech- nology Roadmap Semiconductors, recently discontinued uncertainty con- tinuation Moore ’s Law scaling characteristics seen.5.10 Future Multicore Scaling ■443multicore speedup compare 24-processor count version use processor 99% time? Answer find many cores used 90% time 54 usable, follows: Average Processor Usage ¼0:09/C250 + 0 :01/C21+0:90/C2Max processor 54¼4:51 + 0 :90/C2Max processor Max processor ¼55 Now, find speedup: Speedup ¼1 Fraction 55 55+Fraction 50 50+1/C0Fraction 55/C0Fraction 50 ðÞ Speedup ¼1 0:90 55+0:09 50+0:01¼35:5 compute speedup 24 processors: Speedup ¼1 Fraction 24 24+1/C0Fraction 24 ðÞ Speedup ¼1 0:99 24+0:01¼19:5 considering power constraints Amdahl ’s Law effects, 96- processor version achieves less factor 2 speedup 24-processorversion. fact, speedup clock rate increase nearly matches speedupfrom 4 /C2processor count increase. comment issues concluding remarks. 5.11 Concluding Remarks saw previous section, multicore magically solve powerproblem clearly increases transistor count active numberof transistors switching, two dominant contributions power. Thefailure Dennard scaling merely makes extreme. multicore alter game. allowing idle cores placed power-saving mode, improvement power efficiency achieved, results chapter shown. example, shutting cores theIntel i7 allows cores operate Turbo mode. capability allows atrade-off higher clock rates fewer processors processorswith lower clock rates. importantly, multicore shifts burden keeping processor busy relying TLP, application programmer responsible for444 ■Chapter Five Thread-Level Parallelismidentifying, rather ILP, hardware responsible. Multipro- grammed highly parallel workloads avoid Amdahl ’s Law effects ben- efit easily. Although multicore provides help energy efficiency challenge shifts much burden software system, remain difficult chal-lenges unresolved questions. example, attempts exploit thread-level ver-sions aggressive speculation far met fate ILPcounterparts. is, performance gains modest likely lessthan increase energy consumption, ideas speculative threads hardware run-ahead successfully incorporated processors. speculation ILP, unless speculation almost always right, costsexceed benefits. Thus, present, seems unlikely form simple multicore scal- ing provide cost-effective path growing performance. fundamentalproblem must overcome: finding exploiting significant amounts paral-lelism energy- silicon-efficient manner. previous chapter, weexamined exploitation data parallelism via SIMD approach. many appli- cations, data parallelism occurs large amounts, SIMD energy- efficient method exploiting data parallelism. next chapter, explorelarge-scale cloud computing. environments, massive amounts parallel-ism available millions independent tasks generated individual users.Amdahl ’s Law plays little role limiting scale systems tasks (e.g., millions Google search requests) independent. Finally, inChapter 7 , explore rise domain-specific architectures (DSAs). domain-specific architectures exploit parallelism targeted domain, often data parallelism, GPUs, DSAs achieve much higher effi- ciency measured energy consumption silicon utilization. last edition, published 2012, raised question whether would worthwhile consider heterogeneous processors. time, suchmulticore delivered announced, heterogeneous multiprocessors hadseen limited success special-purpose computers embedded systems.While programming models software systems remain challenging, itappears inevitable multiprocessors heterogeneous processors play important role. Combining domain-specific processors, like discussed Chapters 4and7, general-purpose processors perhaps best road for- ward achieve increased performance energy efficiency maintainingsome flexibility general-purpose processors offer. 5.12 Historical Perspectives References Section M.7 (available online) looks history multiprocessors parallelprocessing. Divided time period architecture, section features dis-cussions early experimental multiprocessors great debates inparallel processing. Recent advances also covered. References read-ing included.5.12 Historical Perspectives References ■445Case Studies Exercises Amr Zaky David A. Wood Case Study 1: Single Chip Multicore Multiprocessor Concepts illustrated case study ■Snooping Coherence Protocol Transitions ■Coherence Protocol Performance ■Coherence Protocol Optimizations ■Synchronization multicore SMT multiprocessor illustrated Figure 5.37 . cache con- tents shown. core single, private cache coherence maintained using snooping coherence protocol Figure 5.7 . cache direct-mapped, four lines, holding 2 bytes (to simplify diagram). simplifica-tion, whole line addresses memory shown address fields thecaches, tag would normally exist. coherence states denotedM, S, Modified, Shared, Invalid. 5.1. [10/10/10/10/10/10/10] <5.2>For part exercise, initial cache memory state assumed initially contents shown Figure 5.37 . part exercise specifies sequence one CPU operations ofthe form Line numberCoherency stateAddress Data 0 AC00 0010 1 AC08 0008 2 AC10 00303 AC18 0010Cache lineCoherency stateAddress Data 0 AC00 0010 1 AC28 0068 2 AC10 00103 AC18 0018Cache lineCoherency stateAddress Data 0 AC20 20 1 AC08 0008 2 AC10 00103 AC18 0010 Address Data …… AC00 0010 AC08 0008 AC10 0010 AC18 0018 AC20 0020 AC28 0028 AC30 0030 …. …..Core 0 Core 1 Core3 Memor Figure 5.37 Multicore (point-to-point) multiprocessor.446 ■Chapter Five Thread-Level ParallelismCcore#: R, <address >for reads Ccore#: W, <address ><--<value written >for writes. example, C3: R, AC10 & C0: W, AC18 <- - 0018 Read write operations 1 byte time. Show resulting state (i.e., coherence state, tags, data) caches memory actions given below. Show cache lines experience state change; example: C0.L0: (I, AC20, 0001) indicates line 0 core 0 assumes “invalid ”coherence state ( I), stores AC20 memory, data con- tents 0001 . Furthermore, represent changes memory state M: <address ><–value . Different parts (a) (g) depend one another: assume actions parts applied initial cache memory states. a.[10]<5.2>C0: R, AC20 b.[10]<5.2>C0: W, AC20 <-- 8 0 c.[10]<5.2>C3: W, AC20 <-- 8 0 d.[10]<5.2>C1: R, AC10 e.[10]<5.2>C0: W, AC08 <-- 4 8 f.[10]<5.2>C0: W, AC30 <-- 7 8 g.[10]<5.2>C3: W, AC30 <-- 7 8 5.2. [20/20/20/20] <5.3>The performance snooping cache-coherent multiprocessor depends many detailed implementation issues determine quickly cacheresponds data exclusive state block. implementations, pro- cessor read miss cache block exclusive another processor ’sc c h ei faster miss block memory. caches smaller, thusfaster, main memory. Conversely, implementations, misses satisfied bymemory faster satisfied caches. caches generallyoptimized “front side ”or CPU references, rather “back side ”or snooping accesses. multiprocessor illustrated Figure 5.37 , consider execution sequence operations single processor core ■read write hits generate stall cycles; ■read write misses generate N memory N cache stall cycles satisfied memory cache, respectively; ■write hits generate invalidate incur N invalidate stall cycles; ■a write-back block, either due conflict another processor ’s request exclusive block, incurs additional N writeback stall cycles. Consider two implementations different performance characteristics summa- rized Figure 5.38 .Case Studies Exercises Amr Zaky David A. Wood ■447To observe cycle values used, illustrate following sequence operations, assuming initial caches ’states Figure 5.37 , behave implementation 1. C1: R, AC10 C3: R, AC10 simplicity, assume second operation begins first com- pletes, even though different processor cores. Implementation 1, ■the first read generates 50 stall cycles read satisfied C0’s cache: C1stalls 40 cycles waits block, C0stalls 10 cycles writes block back memory response C1’s request; ■the second read C3generates 100 stall cycles miss satisfied memory. Therefore sequence generates total 150 stall cycles. following sequences operations, many stall cycles generated implementation? a.[20]<5.3>C0: R, AC20 C0: R, AC28C0: R, AC30 b.[20]<5.3>C0: R, AC00 C0: W, AC08 <-- 4 8 C0: W, AC30 <-- 7 8 c.[20]<5.3>C1: R, AC20 C1: R, AC28C1: R, AC30 d.[20]<5.3>C1: R, AC00 C1: W, AC08 <-- 4 8 C1: W, AC30 <-- 7 8 5.3. [20]<5.2>Someapplicationsreadalargedatasetfirstandthenmodifymostorallofit. base MSI coherence protocol first fetch cache blocks Sharedstate forced perform invalidate operation upgrade theParameterImplementation 1 CyclesImplementation 2 Cycles Nmemory 100 100 Ncache 40 130 Ninvalidate 15 15 Nwriteback 10 10 Figure 5.38 Snooping coherence latencies.448 ■Chapter Five Thread-Level ParallelismModified state. additional delay significant impact workloads. MESI addition standard protocol (see Section 5.2 ) provides relief cases. Draw new protocol diagrams MESI protocol adds Exclusive stateand transitions base MSI protocol ’s Modified, Shared, Invalidate states. 5.4. [20/20/20/20/20] <5.2>Assume cache contents Figure 5.37 timing Implementation 1 Figure 5.38 . total stall cycles following code sequences base protocol new MESI protocol Exercise5.3? Assume state transitions require zero interconnect transactions incur noadditional stall cycles. a.[20]<5.2>C0: R, AC00 C0: W, AC00 <-- 4 0 b.[20]<5.2>C0: R, AC20 C0: W, AC20 <-- 6 0 c.[20]<5.2>C0: R, AC00 C0: R, AC20 d.[20]<5.2>C0: R, AC00 C1: W, AC00 <-- 6 0 e.[20]<5.2>C0: R, AC00 C0: W, AC00 <-- 6 0 C1: W, AC00 <-- 4 0 5.5. Code running single core sharing variables cores suffer performance degradation snooping coherence protocol. Consider two following iterative loops functionally equivalent seem similar complexity. One could led conclude would spend acomparably close number cycles executed processor core. Loop 1 Loop 2 Repeat i: 1 .. n Repeat i:1 .. nA[i] <- - A[i-1] +B[i]; A[i] <- - A[i] +B[i]; Assume ■every cache line hold exactly one element B; ■arrays B interfere cache; ■all elements B cache either loop executed. Compare performance run core whose cache uses MESI coherence protocol. Use stall cycles data Implementation 1 Figure 5.38 . Assume cache line hold multiple elements B (A B go separate cache lines). affect relative performances Loop1and Loop2?Case Studies Exercises Amr Zaky David A. Wood ■449Suggest hardware and/or software mechanisms would improve perfor- mance Loop1 single core. 5.6. [20]<5.2>Many snooping coherence protocols additional states, state tran- sitions, bus transactions reduce overhead maintaining cache coherency. Implementation 1 Exercise 5.2, misses incurring fewer stall cycles supplied cache supplied memory. MOESIprotocol extension (see Section 5.2 ) addresses need. Draw new protocol diagrams additional state transitions. 5.7. [20/20/20/20] <5.2>For following code sequences timing parameters two implementations Figure 5.36 , compute total stall cycles base MSI protocol optimized MESI protocol Exercise 5.3. Assume statetransitions require bus transactions incur additional stall cycles. a.[20]<5.2>C1: R, AC10 C3: R, AC10C0: R, AC10 b.[20]<5.2>C1: R, AC20 C3: R, AC20C0: R, AC20 c.[20]<5.2>C0: W, AC20 <-- 8 0 C3: R, AC20C0: R, AC20 d.[20]<5.2>C0: W, AC08 <-- 8 8 C3: R, AC08 C0: W, AC08 <-- 9 8 5.8. [20/20/20/20] <5.5>The spin lock simplest synchronization mechanism possible commercial shared-memory machines. spin lock relies onthe exchange primitive atomically load old value store new value.The lock routine performs exchange operation repeatedly finds lockunlocked (i.e., returned value 0). addi x2, x0, #1 lockit: EXCH x2, 0(x1) bnez x2, lockit lock released simply storing 0intox2. discussed Section 5.5 , optimized spin lock employs cache coherence uses load check lock, allowing spin shared var-iable cache. lockit: ld x2, 0(x1) bnez x2, lockitaddi x2, x0, #1EXCH x2,0(x1) bnez x2, lockit450 ■Chapter Five Thread-Level ParallelismAssume processor cores C0, C1, C3 trying acquire lock address 0xAC00 (i.e., register R1 holds value 0xAC00). Assume cache con- tents Figure 5.37 timing parameters Implementation 1 Figure 5.38 . simplicity, assume critical sections 1000 cycles long. a.[20]<5.5>Using simple spin lock, determine approximately many memory stall cycles processor incurs acquiring lock. b.[20]<5.5>Using optimized spin lock, determine approximately many memory stall cycles processor incurs acquiring lock. c.[20]<5.5>Using simple spin lock, approximately many memory accesses occur? d.[20]<5.5>Using optimized spin lock, approximately many memory accesses occur? Case Study 2: Simple Directory-Based Coherence Concepts illustrated case study ■Directory Coherence Protocol Transitions ■Coherence Protocol Performance ■Coherence Protocol Optimizations Consider distributed shared-memory system illustrated Figure 5.39 . con- sists 8 nodes processor cores organized three-dimensional hypercube withpoint-to-point interconnections, shown figure. simplification, weassume following scaled-down configuration: ■Every node single processor core direct-mapped L1 data cache dedicated cache controller. ■The L1 data cache capacity two cache lines line size B bytes. ■The L1 cache states denoted M,S, andIfor Modified, Shared, Invalid. example cache entry would like 1: S, M3, 0xabcd - - > Cache line 1 “Shared ”state; contains memory block M3and data value block 0xabcd. ■The system memory comprises 8 memory blocks (i.e., one memory block per node) distributed among eight nodes, every node owning amemory block. Node Ciowns memory block Mi. ■Each memory block B-bytes wide tracked coherency directory entry stored memory block.Case Studies Exercises Amr Zaky David A. Wood ■451■The state memory directory entry denoted DM,DS,a n dDIfor Direc- tory Modified, Directory Shared, Directory Invalid. Additionally, thedirectory entry lists block sharers using bit vector 1 bit everynode. example memory block associated directory entry: M3: 0XABCD, DS, 00000011 - - > Memory block M3(in node C3) contains value 0xABCD shared nodes 0and1(corresponding 1s bit vector). Read/Write Notation describe read/write transactions, use notation Ci#: R, <Mi>for reads andCi#: W, <Mi><--<value written >for writes.Node 0 (000) Node 2 (010) Node 6 (110)Node 4 (100)Node 1 (001) Node 5 (101) Node 7 (111)Node 3 (011) Figure 5.39 Multicore multiprocessor DSM.452 ■Chapter Five Thread-Level ParallelismFor example, C3: R, M2 describes core node 3 issuing read transaction address memory block M2(the address may possibly cached C3already). C0: W, M3 <- - 0018 describes core node 0 issuing write transaction (data 0X0018 ) address memory block M3(the address may possibly cached C0already). Messages directory coherency schemes depend exchange command and/or data messages described directory protocol described Figure 5.20 .A n example command message read request. example data messageis read response (with data included). ■Messages originating ending node cross inter-node links. ■Message distinct source/destination nodes travel inter-node links.These messages may destined one cache controller another, acache controller directory controller, directory controller cache controller. ■Messages traveling source node distinct destination node stat-ically routed. /C14The static routing algorithm selects shorted path source destination nodes. /C14The short path determined considering binary representations source destination indices (e.g., 001 node C1and100 node C4), moving one node neighboring node already crossed message. –For example, go node 6to node 0(110 - - >000), path 110-- >100-- >000. –Because one shorted path may exist ( 110- - >010- - >000is another path preceding example), assume path isselected inverting first least significant bit different corresponding bit destination index. example, travel node 1 node 6 ( 001- - >110), path 001- - >000- - > 010- - >110. /C14The longest possible path traveled message 3 links (equal number bits binary representation node index). ■A node simultaneously process three messages from/to distinctneighboring nodes ’links two competing link resource clarified following examples messages sent/received to/from/through node 000.Case Studies Exercises Amr Zaky David A. Wood ■453Messages: 001 - - >010; 010 - - >000 (to cache/directory controller); 100 - - >001.OK(distinct destinations). Message: 001 - - >010; 000 - - >001 (from cache/directory control- ler);100 -- >001. OK two messages destined node 001 case destination contention, ties broken assigning priority a.message destined node ( 000 example) cache directory controller; b.messages forwarded one another (through 000 example); c.messages originating node ( 000 example) cache directory controller. ■Assume transmission service delays following table. Message type Cache controller Directory controller Link data 2 cycles 5 cycles 10 cyclesWith data (3 + B=4de ) cycles (6 + 10 /C3B) cycles (4 + B) ■If message forwarded node, first completely received node sent next node path. ■Assume cache controller; directory controller unlimited capacity toenqueue messages service FCFS order. 5.9. [10/10/10] <5.4>For part exercise, assume initially caches lines invalid, data memory Miis byte i(0X00 <=i<= 0x07 ) repeated many times block size. Assume successive requests completely serialized. is, core issue coherency request theprevious request (by different core) completed. following parts, ■show final state (i.e., coherence state, sharers/owners, tags, data) thecaches directory controller (including data values) given transac-tion sequence completed; ■show messages transferred (choose suitable format message types).a.[10]<5.4>C3: R, M4 C3: R, M2C7: W, M4 <- -0xaaaa C1: W, M4 <- -0xbbbb b.[10]<5.4>C3: R, M0 C3: R, M2C6: W, M4 <- -0xaaaa C3: W, M4 <- -0xbbbb454 ■Chapter Five Thread-Level Parallelismc.[10]<5.4>C0: R, M7 C3: R, M4 C6: W, M2 <- -0xaaaa C2: W, M2 <- -0xbbbb 5.10. [10/10/10] <5.4>The directory protocol used 5.9 (based Figure 5.20 ) assumes directory controller receives requests, sends invalidates, receivesmodified data, sends modified data requester block dirty, on.Assume directory controller delegate work cores.For example, notify exclusive owner modified block someother core needs block owner send block newsharer. Specifically, consider following optimizations indicate theirbenefits (if any) are. Also, specify messages modified (in compar- ison Figure 5.20 protocol) support new change. Hint: Benefits might reduction number messages, faster response time, on. a.[10]<5.4>On write miss shared memory block, directory controller sends data requester instructs sharers send invalidate acknowledgements directly requester. b.[10]<5.4>On read miss block modified core, directory controller instructs owner modified copy directly forward data requester. c.[10]<5.4>On read miss block shared (S) state cores, directory controller instructs one sharers (say, one closest requester) directly forward data requester. 5.11. [15/15/15] <5.4>In problem 5.9, assumed transactions sys- tem serially executed, unrealistic inefficient DSM mul-ticore. relax condition. require transactionsoriginating one core serialized. However, different cores independentlyissue read/write transactions even compete memory block.The transactions problem 5.9 represented next reflect new, relaxedconstraints. Redo problem 5.9 new, relaxed constraints. a.[15]<5.4> C1: W, M4 <- -0xbbbb C3: R, M4 C7: R, M2 C3: W, M4 <- -0xaaaa b.[15]<5.4> C3: R, M0 C6: W, M4 <--0xaaaa C3: R, M2C3: W, M4 <- -0xbbbb c.[15]<5.4> C0: R, M7 C2: W, M2 <- -0xbbbb C3:R, M4 C6: W, M2 <- -0xaaaa 5.12. [10/10] <5.4>Use routing delay information described earlier trace following groups transactions progress system (assume accesses misses).Case Studies Exercises Amr Zaky David A. Wood ■455a.C0:R, M7 C2: W, M2 <- -0xbbbb C3: R, M4 C6: W, M2 <- -0xaaaa b.C0: R, M7 C3: R, M7 C2: W, M7 <--0xbbbb 5.13. [20]<5.4>What extra complexities may arise messages adaptively rerouted links? example, coherency message core M1directory controller C2 (expressed binary M001-->C010)w l lb er u e de h e rt h r u g h inter-node path C001-->C000-->C010or inter-node path C001-->C011-- >C010, depending link availability. 5.14. [20]<5.4>In read miss, cache might overwrite line shared (S) state without notifying directory owns corresponding memory block. Alter-natively, notify directory deletes cache list ofsharers. Show following transaction groups (performed one time series) proceed approaches. C3: R, M4 C3: R, M2 C2: W, M4 <- -0xabcd Case Study 3: Memory Consistency Concepts Illustrated Case Study ■Legitimate Program Behavior Sequential Consistency (SC) Models ■Hardware Optimization Allowed SC Models ■Using Synchronization Primitives Make Consistency Model Emulate Restrictive Model 5.15. [10/10] <5.6>Consider following code segments running two processors P1 P2. Assume AandBare initially 0. P1: (B == 0);A=1;P2: (A==0);B=1 ; a.If processors adhere sequential consistency (SC) consistency model. possible values B end segments? Showthe statement interleaving supporting answer(s). b.Repeat (a) processors adhere total store order (TSO) consistency model. 5.16. [5]<5.6>Consider following code segments running two processors P1 P2. Assume A, B, initially 0. Explain optimizing compiler might make impossible Bto ever set 2in sequentially consistent execution model.456 ■Chapter Five Thread-Level ParallelismP1: A=1;A=2;While (B == 0);P2: B=1;While (A <>1); B= 2; 5.17. [10]<5.4>. processor implementing SC consistency model, data cache augmented data prefetch unit. alter SC implementation exe-cution results? not? 5.18. [10/10] <5.6>Assume following code segment executed processor implements partial store order (PSO), A=1; B=2;If (C== 3)D=B; a.Augment code synchronization primitives make emulate behavior total store order (TSO) implementation. b.Augment code synchronization primitives make emulate behavior sequential consistency (SC) implementation. 5.19. [20/20/20] <5.6>Sequential consistency (SC) requires reads writes appear executed total order. may require processor stallin certain cases committing read write instruction. Consider codesequence write read B write Aresults cache miss read Bresults cache hit. SC, processor must stall read Buntil order (and thus per- form) write A. Simple implementations SC stall processor cache receives data perform write. Release consistency (RC) consistency mode (see Section 5.6 ) relaxes constraints: ordering —when desired —is enforced judicious use synchroni- zation operations. allows, among optimizations, processors imple-ment write buffers, hold committed writes yet orderedwith respect processors ’writes. Reads pass (and potentially bypass) write buffer RC (which could SC). Assume one memory operation performed per cycle oper- ations hit cache satisfied write buffer introduce stall cycles. Operations miss incur latencies listed Figure 5.38 . many stall cycles occur prior operation SC RC consistency models? (Write buffer hold one write.) a.[20]<5.6>P0: write 110 <- - 80 //assume miss (no cache line) P0: read 108 //assume miss (no cache line)Case Studies Exercises Amr Zaky David A. Wood ■457b.[20] <5.6>P0: read 110 //assume miss (no cache line) P0: write 100 <- - 90 //assume hit c.[20]<5.6>P0: write 100 <- - 80 //assume miss P0: write 110 <- - 90 //assume hit 5.20. [20]<5.6>Repeat part (a) problem 5.19 SC model processor read prefetch unit. Assume read prefetch triggered 20 cycles advance write operation. Exercises 5.21. [15]<5.1>Assume function application form F(i, p), gives fraction time exactly processors usable given atotal p processors available. means Xp i¼1Fi,pðÞ ¼ 1 Assume iprocessors use, applications run itimes faster. a.Rewrite Amdahl ’s Law gives speedup function p application. b.An application runs single processor time seconds. Different por- tions running time improve larger number processors used.Figure 5.40 provides details. much speedup achieve 8 processors? c.Repeat 32 processors infinite number processors. 5.22. [15/20/10] <5.1>In exercise, examine effect interconnection network topology CPI programs running 64-processordistributed-memory multiprocessor. processor clock rate 2.0 GHz, andthe base CPI application references hitting cache 0.75. Assume 0.2% instructions involve remote communication reference. cost remote communication reference (100+10 h) ns, h num-ber communication network hops remote reference make theremote processor memory back. Assume communication links arebidirectional. a.[15]<5.1>Calculate worst-case remote communication cost 64 processors arranged ring, 8 /C28 processor grid, hypercube (hint: longest communication path 2 nhypercube nlinks). Fraction 20% 20% 10% 5% 15% 20% 10% Processors (P) 1 2 4 6 8 16 128 Figure 5.40 Percentage application ’s time use P processors.458 ■Chapter Five Thread-Level Parallelismb.[20]<5.1>Compare base CPI application remote commu- nication CPI achieved three topologies part (a). 5.23. [15]<5.2>Show basic snooping protocol Figure 5.6 changed write-through cache. major hardware functionality needed write-through cache compared write-back cache? 5.24. [20/20] <5.2>Please answer following problems: a.[20]<5.2>Add clean exclusive state basic snooping cache coherence protocol ( Figure 5.6 ). Show protocol finite state machine format used figure. b.[20]<5.2>Add “owned ”state protocol part (a) describe using finite state machine format used Figure 5.6 . 5.25. [15]<5.2>One proposed solution problem false sharing add valid bit per word. would allow protocol invalidate word without removing entire block, letting processor keep portion block cache another processor writes different portion block. extra complicationsare introduced basic snooping cache coherence protocol ( Figure 5.6 )b addition? Consider possible protocol actions. 5.26. [15/20] <5.3>This exercise studies impact aggressive techniques exploit instruction-level parallelism processor used design shared-memory multiprocessor systems. Consider two systems identical except theprocessor. System uses processor simple single-issue, in-order pipeline,and system B uses processor four-way issue, out-of-order execution areorder buffer 64 entries. a.[15]<5.3>Following convention Figure 5.11 , let us divide execution time instruction execution, cache access, memory access, stalls. would expect components differ system system B? b.[10]<5.3>Based discussion behavior OLTP workload Section 5.3 , important difference OLTP workload benchmarks limit benefit aggressive processordesign? 5.27. [15]<5.3>How would change code application avoid false shar- ing? might done compiler might require programmerdirectives? 5.28. [15]<5.3>An application calculating number occurrences certain word large number documents. large number processorsdivided work, searching different documents. created hugearray—word_count —of 32-bit integers, every element number times word occurred document. second phase, computationis moved small SMP server four processors. processor sums approximately ¼of array elements. Later, one processor calculates total sum.Case Studies Exercises Amr Zaky David A. Wood ■459for (int p= 0; p <=3; p++) // iteration executed separate processor. { sum [p] = 0;for (int i= 0; <n/4; i++) // n size word_count divisible 4 sum[p] = sum[p] + word_count[p+4*i]; }total_sum = sum[0] +sum[1]+sum[2]+sum[3] //executed processor. a.Assuming processor 32-byte L1 data cache. Identify cache line sharing (true false) code exhibits. b.Rewrite code reduce number misses elements array word_count. c.Identify manual fix make code rid false sharing. 5.29. [15]<5.4>Assume directory-based cache coherence protocol. directory currently information indicates processor P1 data “exclu- sive”mode. directory gets request cache block pro- cessor P1, could mean? directory controller do? (Suchcases called “race conditions ”and reason coherence protocols hard design verify.) 5.30. [20]<5.4>A directory controller send invalidates lines replaced local cache controller. avoid messages, keep thedirectory consistent, replacement hints used. messages tell controller block replaced. Modify directory coherence protocol Section 5.4 use replacement hints. 5.31. [20/15/20/15] <5.4>One downside straightforward implementation direc- tories using fully populated bit vectors total size directory infor- mation scales product: processor count /C2memory blocks. memory grows linearly processor count, total size directory grows quadratically inthe processor count. practice, directory needs 1 bit per memoryblock (which typically 32 –128 bytes), problem serious small-to- moderate processor counts. example, assuming 128-byte block, P pro-cessors, amount directory storage compared main memory P/(128*8) ¼ P/1024, 12.5% overhead P ¼128 processors. avoid prob- lem observing need keep amount information pro- portional cache size processor. explore solutions theseexercises. a.[20]<5.4>One method obtain scalable directory protocol organize multiprocessor logical hierarchy processors leaves thehierarchy directories positioned root subtree. directory subtree records descendants cache memory blocks. also460 ■Chapter Five Thread-Level Parallelismrecords memory blocks —with home subtree —that cached out- side subtree. Compute amount storage needed record proces- sor information directories, assuming directory fully associative. answer incorporate number nodes eachlevel hierarchy well total number nodes. b.[15]<5.4>Another approach reducing directory size allow limited number directory ’s memory blocks shared given time. Implement directory four-way set-associative cache storing full bit vec-tors. directory cache miss occurs, choose directory entry invalidate theentry. Describe organization work elaborating happen asa block read, written replaced written back memory. Modify pro-tocol Figure 5.20 reflect new transitions required directory organization. c.[20]<5.4>Rather reducing number directory entries, imple- ment bit vectors dense. example, set every directory entry 9 bits. block cached one node outside home, fieldcontains node number. block cached one node outsideits home, field bit vector bit indicating group eight pro-cessors, least one caches block. Illustrate scheme wouldwork 64-processor DSM machine consists eight 8-processorsgroups. d.[15] extreme approach reducing directory size implement “empty ”directory; is, directory every processor store memory states. receives requests forwards appropriate . benefit directory directory DSM system? 5.32. [10]<5.5>Implement classical compare-and-swap instruction using load linked/store conditional instruction pair. 5.33. [15]<5.5>One performance optimization commonly used pad synchroniza- tion variables useful data cache line. Con- struct example demonstrating optimization extremely useful insome situations. Assume snoopy write invalidate protocol. 5.34. [30]<5.5>One possible implementation load linked/store conditional pair multicore processors constrain instructions using uncached mem-ory operations. monitor unit intercepts reads writes core thememory. keeps track source load linked instructions whether intervening stores occur load linked corresponding store conditional instruction. monitor prevent failing store conditional writing data use interconnect signals inform processor store failed. Design monitor memory system supporting four-core SMP. Take account that, generally, read write requests different data sizes (4/8/16/32 bytes). memory location target load linked/storeCase Studies Exercises Amr Zaky David A. Wood ■461conditional pair, memory monitor assume load linked/store conditional references location can, possibly, interleaved regular accesses location. monitor complexity independentof memory size. 5.35. [25]<5.5>Prove that, two-level cache hierarchy L1 closer processor, inclusion maintained extra action L2 least muchassociativity L1, caches use LRU replacement, caches thesame block sizes. 5.36. [Discussion] <5>When trying perform detailed performance evaluation multiprocessor system, system designers use one three tools: analytical models,trace-driven simulation, execution-driven simulation. Analytical models usemathematical expressions model behavior programs. Trace-driven simu-lations run applications real machine generate trace, typically ofmemory operations. traces replayed cache simulator simulator simple processor model predict performance system various parameters changed. Execution-driven simulators simulate theentire execution maintaining equivalent structure processor state andso on. a.What accuracy/speed trade-offs approaches? b.CPU traces, carefully collected, exhibit artifacts system collected on. Discuss issue using branch-prediction spin-waitsynchronization examples. (Hint: program available pureCPU trace; trace available.) 5.37. [40]<5.7, 5.9 >Multiprocessors clusters usually show performance increases increase number processors, ideal n times speedupfor n processors. goal biased benchmark make program getsworse performance add processors. example, means one pro-cessor multiprocessor cluster runs program fastest, two slower,four slower two, on. key performance characteristics organization give inverse linear speedup?462 ■Chapter Five Thread-Level ParallelismThis page intentionally left blank6.1 Introduction 466 6.2 Programming Models Workloads Warehouse-Scale Computers 471 6.3 Computer Architecture Warehouse-Scale Computers 477 6.4 Efficiency Cost Warehouse-Scale Computers 482 6.5 Cloud Computing: Return Utility Computing 490 6.6 Cross-Cutting Issues 501 6.7 Putting Together: Google Warehouse-Scale Computer 503 6.8 Fallacies Pitfalls 514 6.9 Concluding Remarks 518 6.10 Historical Perspectives References 519 Case Studies Exercises Parthasarathy Ranganathan 5196 Warehouse-Scale Computers Exploit Request-Level andData-Level Parallelism datacenter computer. Luiz Andr /C19e Barroso, Google (2007) hundred years ago, companies stopped generating power steam engines dynamos plugged newly built electric grid. cheap power pumped electricutilities ’t change businesses operate. set chain reaction economic social transformations brought modern world existence. Today, similar revolution way. Hooked Internet ’s global computing grid, massive information-processing plants begun pumping data software code homes businesses. time, ’s computing ’s turning utility. Nicholas Carr, Big Switch: Rewiring World, Edison Google (2008) Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00006-7 ©2019 Elsevier Inc. rights reserved.6.1 Introduction Anyone build fast CPU. trick build fast system. Seymour Cray, Considered father supercomputer warehouse-scale computer (WSC)1is foundation Internet services billions people use every day: search, social networking, online maps, videosharing, online shopping, email services, on. tremendous popularityof Internet services necessitated creation WSCs could keep withthe rapid demands public. Although WSCs may appear large data centers, architecture operation quite different, see. Today ’s WSCs act one giant machine costs hundreds million dollars build-ing, electrical cooling infrastructure, servers, networking equip-ment connects houses 50,000 –100,000 servers. Moreover, rapid growth commercial cloud computing (see Section 6.5 ) makes WSCs accessible anyone credit card. Computer architecture extends naturally designing WSCs. example, Luiz Barroso Google (quoted earlier) dissertation research computer architecture. believes architect ’s skills designing scale, designing dependability, knack debugging hardware helpful cre-ation operation WSCs. leading-edge scale, requires innovation power distribution, cooling, monitoring, operations, WSC modern descendant thesupercomputer —making Seymour Cray godfather today ’s WSC architects. extreme computers handled computations could done nowhere else,but expensive companies could afford them. time target providing information technology world instead high- performance computing (HPC) scientists engineers; thus WSCs arguablyplay important role society today Cray ’s supercomputers past. Unquestionably, WSCs many orders magnitude users high- performance computing, represent much greater share market.Whether measured number users revenue, Google 1000 times largerthan Cray Research ever was. 1This chapter based material book Datacenter Computer: Introduction Design Warehouse-Scale Machines, Second Edition ,b Luiz Andr /C19e Barroso, Jimmy Clidaras, Urs H €olzle Google (2013) ; blog Perspectives mvdirona.com talks “Cloud-Computing Economies Scale ”and“Data Center Networks Way, ”by James Hamilton Amazon Web Services (2009, 2010) ; paper Clouds: View Cloud Computi ng, Michael Armbrust et al. (2010) .466 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismWSC architects share many goals requirements server architects: ■Cost-performance —Work done per dollar critical part scale. Reducing costs collection WSCs percent could savemillions dollars. ■Energy efficiency —Except photons leave WSCs, essen- tially closed systems, almost energy consumed turned heat thatmust removed. Thus, peak power consumed power drive costof power distribution cost cooling systems. majority theinfrastructure costs building WSC goes toward power cooling.Moreover, energy efficiency important part environmental steward-ship. Therefore, work done per joule critical WSCs servers high cost building power mechanical infrastructure warehouse computers resulting monthly utility bills. ■Dependability via redundancy —The long-running nature Internet services means hardware software WSC must collectively provide atleast 99.99% (called “four nines ”) availability; is, services must less 1 h per year. Redundancy key dependability WSCsand servers. Although server architects often utilize hardware highercosts reach high availability, WSC architects rely instead numerous cost-effective servers connected network redundancy managed soft-ware. addition local redundancy inside WSC, organization needs redundant WSCs mask events take whole WSCs. Indeed, although every cloud service needs available least 99.99% thetime, dependability full Internet company like Amazon, Google, orMicrosoft needs even higher. one companies completelyoffline 1 h per year —that is, 99.99% availability —that would front page news. Multiple WSCs added benefit reducing latency servicesthat widely deployed ( Figures 6.18 –6.20). ■Network I/O —Server architects must provide good network interface external world, WSC architects must also. Networking needed keepdata consistent multiple WSCs well interface public. ■Both interactive batch processing workloads —Although one expects highly interactive workloads services like search social networking billions users, WSCs, like servers, also run massively parallel batch programs calculate metadata useful services. example, MapRe-duce jobs run convert pages returned crawling web intosearch indices (see Section 6.2 ). surprisingly, also characteristics notshared server architecture: ■Ample parallelism —A concern server architect whether applications targeted marketplace enough concurrency justify amount par-allel hardware whether cost high sufficient communication6.1 Introduction ■467hardware exploit parallelism. WSC architect concern. First, batch applications benefit large number distinct datasets require independent processing, billions web pages web crawl. pro-cessing data-level parallelism , saw Chapter 4 , time applied data storage instead data memory. Second, interactive Internet serviceapplications, also known software service (SaaS ), benefit mil- lions independent users interactive Internet services. Reads writes areseldom dependent SaaS, SaaS rarely needs synchronize. example,search uses read-only index email normally reads writes independent information. call type easy parallelism request-level parallelism ,a many independent efforts proceed parallel naturally little need forcommunication synchronization; example journal-based updatingcan reduce throughput demands. Even read-/write-dependent features some-times dropped offer storage scale size modern WSCs. anycase, WSC applications choice find algorithms scale acrosshundreds thousands servers, customers expect whatthe WSC technology provides. ■Operational costs count —Server architects typically ignore operational costs server, assuming pale comparison purchase costs. WSCs lon- ger lifetimes —the building electrical cooling infrastructure often amortized 10 –15 years —so operational costs add up: energy, power distribu- tion, cooling represent 30% costs WSC 10 years. ■Location counts —To build WSC, first step building warehouse. One question where? Real estate agents emphasize location, location aWSC means access water, inexpensive electricity, proximity Internet back-bone optical fibers, people nearby work WSC, low risk envi-ronmental disasters, earthquakes, floods, hurricanes. obviousconcern cost land, including enough space grow WSC. companies many WSCs, another concern finding place geographically near current future population Internet users, reduce latency theInternet. factors include taxes, property costs, social issues (people some-times want facility country), political issues (some jurisdictions requirelocal hosting), cost networking, reliability networking, cost power, sourceof power (e.g., hydroelectric versus coal), weather (cooler cheaper, asSection 6.4 shows), overall Internet connectivity (Australia close Singa- pore geographically, network link bandwidth great). ■Computing efficiently low utilization —Server architects usually design systems peak performance within cost budget worry power make sure ’t exceed cooling capacity enclosure. see ( Figure 6.3 ), WSC servers rarely fully utilized, part ensure low response time part offer redundancy needed deliver dependablecomputing. Given operational costs count, servers need compute effi-ciently utilization levels. ■Scale opportunities/problems associated scale —Often extreme computers extremely expensive require custom hardware,468 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismand yet cost customization cannot effectively amortized since extreme computers made. However, purchasing thousands servers time, aregreat volume discounts. WSCs massive internally economy scale even many WSCs.As see Sections 6.5 and6.10, economies scale led com- mercial cloud computing lower per-unit costs WSC meantthat companies could rent servers profit costs outsidersto themselves. flip side 100,000 servers failures.Figure 6.1 shows outages anomalies 2400 servers. Even server mean time failure (MTTF) amazing 25 years (200,000 h), WSC architect would need design five server failures day.Figure 6.1 lists annualized disk failure rate 2% –10%. Given two disks per server annual failure rate 4%, 100,000 servers WSCarchitect expect see one disk fail per hour. However, software failures vastly outnumber hardware failures, Figure 6.1 shows, system design must resilient server crashes caused software bugs,which would happen even frequently disk failures. thou- sands servers large facilities, WSC operators become good changing disks, cost disk failure much lower aWSC small data center. applies DRAMs. Plausibly, WSCscould use even less reliable components cheaper ones available. Approx. number events 1st year Cause Consequence 1o r2Power utility failuresLose power whole WSC; ’t bring WSC UPS generators work (generators work 99% time). 4Cluster upgradesPlanned outage upgrade infrastructure, many times evolving networking needs recabling, switch firmware upgrades, soon. nine planned cluster outages every unplannedoutage. 1000sHard-drive failures2%–10% annual disk failure rate ( Pinheiro et al., 2007 ) Slow disks Still operate, run 10 /C2to 20/C2more slowly Bad memories One uncorrectable DRAM error per year ( Schroeder et al., 2009 ) Misconfigured machinesConfiguration led /C2430% service disruptions ( Barroso H €Olzle, 2009 ) Flaky machines 1% servers reboot week ( Barroso H €Olzle, 2009 ) 5000Individual server crashesMachine reboot; typically takes 5 min (caused problems software hardware). Figure 6.1 List outages anomalies approximate frequencies occurrences first year new cluster 2400 servers. label Google calls cluster array ; see Figure 6.5 . Based Barroso, L.A., 2010. Warehouse Scale Computing [keynote address]. In: Proceedings ACM SIGMOD, June 8 –10, 2010, Indianapolis, IN.6.1 Introduction ■469Example Calculate availability service running 2400 servers Figure 6.1 . Unlike service real WSC, example service cannot toleratehardware software failures. Assume time reboot software 5 minand time repair hardware 1 h. Answer estimate service availability calculating time outages failures component. ’ll conservatively take lowest number category Figure 6.1 split 1000 outages evenly four components. ignore slow disks —the fifth component 1000 outages —because hurt performance availability, power utility failures, becausethe uninterruptible power supply (UPS) system hides 99% them. Hours Outageservice¼4 + 250 + 250 + 250ðÞ /C2 1 h + 250 + 5000ðÞ /C2 5 min ¼754 + 438 ¼1192 h Since 365 /C224 8760 h year, availability Availabilitysystem¼8760/C01192 8760/C18/C19 ¼7568 8760¼86% Without software redundancy mask many outages, service 2400 servers would average one day week —zero“nines ”—which far 99.99% availability goal WSCs. AsSection 6.10 explains, forerunners WSCs computer clusters . Clusters collections independent computers connected together usinglocal area networks (LANs) switches. workloads requireintensive communication, clusters offered much cost-effective computing shared-memory multiprocessors. (Shared-memory multiprocessors forerunners multicore computers discussed Chapter 5 .) Clusters became popular late 1990s scientific computing later Internet services.One view WSCs logical evolution clusters ofhundreds servers tens thousands servers. natural question whether WSCs similar modern clusters high- performance computing. Although similar scale cost —there HPC designs million processors cost hundreds millions dollars —they historically powerful processors much lower- latency networks nodes found WSCs HPCapplications interdependent communicate frequently (seeSection 6.3 ). programming environment also emphasizes thread-level paral- lelism data-level parallelism (see Chapters 4and5), typically emphasizing latency complete single task contrast bandwidth complete many inde-pendent tasks via request-level parallelism. HPC clusters also tend havelong-running jobs keep servers fully utilized, even weeks time,470 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismwhereas utilization servers WSCs ranges 10% 50% (see Figure 6.3 page 441) varies every day. Unlike supercomputer environ- ments, thousands developers work WSC code base deploy significantsoftware releases every week ( Barroso et al., 2017 ). WSCs compare conventional data centers? operators traditional data center generally collect machines third-party software frommany parts organization run centrally others. main focustends consolidation many services onto fewer machines, areisolated protect sensitive information. Thus, virtual machines increasingly important data centers. Virtual machines important WSCs well, play different role. used offer isolationbetween different customers slice hardware resources different-sizedshares rent several price points (see Section 6.5 ). Unlike WSCs, conventional data centers tend great deal hardware software heterogeneity toserve varied customers inside organization. WSC programmers customizethird-party software build own, WSCs much homogeneoushardware; WSC goal make hardware/software warehouse act like single computer typically runs variety applications. Often biggest cost conventional data center people maintain it, whereas, willsee Section 6.4 , well-designed WSC, server hardware greatest cost, people costs shift topmost bottommost. Conventional datacenters also ’t scale WSC, ’t get economic benefits scale previously mentioned. Thus, although WSC might considered extreme data center computers housed separately space special electrical cooling infrastructure, traditional data centers share little challenges opportu- nities WSC, either architecturally operationally. start introduction WSCs workload programming model. 6.2 Programming Models Workloads Warehouse-Scale Computers problem solution, may problem, fact —not solved, coped time. Shimon Peres addition public-facing Internet services search, video sharing,and social networking make famous, WSCs also run batch applications,such converting videos new formats creating search indexes web crawls. popular framework batch processing WSC MapReduce ( Dean Ghemawat, 2008 ) open-source twin Hadoop. Figure 6.2 shows increas- ing popularity MapReduce Google time. Inspired Lisp functions6.2 Programming Models Workloads Warehouse-Scale Computers ■471of name, Map first applies programmer-supplied function logical input record. Map runs hundreds computers produce interme-diate result key-value pairs. Reduce collects output distributed tasksand collapses using another programmer-defined function. Assuming Reduce function commutative associative, run log N time. appropriate software support, functions fast yet easy understandand use. Within 30 min, novice programmer run MapReduce task thou-sands computers. Figure 6.2 shows average job uses hundreds servers. highly tuned applications high-performance computing, MapReducejobs parallel applications today, whether measured total CPU timeor number servers utilized. MapReduce program calculates number occurrences every English word large collection documents. Following simplifiedversion program, shows inner loop assumesonly one occurrence English words found document ( Dean Ghemawat, 2008 ):MonthNumber MapReduceJobsAverage completiontime (s)Average no. servers perjobAvg. no. cores perserverCPU coreyearsInput data(PB)Intermediate data (PB)Output data(PB) Sep-16 95,775,891 331 130 2.4 311,691 11,553 4095 6982 Sep-15 115,375,750 231 120 2.7 272,322 8307 3980 5801Sep-14 55,913,646 412 142 1.9 200,778 5989 2530 3951Sep-13 28,328,775 469 137 1.4 81,992 2579 1193 1684Sep-12 15,662,118 480 142 1.8 60,987 2171 818 874Sep-11 7,961,481 499 147 2.2 40,993 1162 276 333Sep-10 5,207,069 714 164 1.6 30,262 573 139 37Sep-09 4,114,919 515 156 3.2 33,582 548 118 99Sep-07 2,217,000 395 394 1.0 11,081 394 34 14 Mar-06 171,000 874 268 1.6 2002 51 7 3 Aug-04 29,000 634 157 1.9 217 3.2 0.7 0.2 Figure 6.2 Monthly MapReduce usage Google 2004 2016. 12 years number MapReduce jobs increased factor 3300. Figure 6.17 page 461 estimates running September 2016 workload Ama- zon’s cloud computing service EC2 would cost $114 million. Updated Dean, J., 2009. Designs, lessons advice building large distributed systems [keynote address]. In: Proceedings 3rd ACM SIGOPS International Workshop Large-Scale Distributed Systems Middleware, Co-located 22nd ACM Symposium Oper- ating Systems Principles, October 11 –14, 2009, Big Sky, Mont.472 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismmap(String key, String value): // key: document name // value: document contentsfor word w value:EmitIntermediate(w, “1”); // Produce list words reduce(String key, Iterator values): // key: word// values: list countsint result = 0;for v values: result += ParseInt(v); // get integer key- value pairEmit(AsString(result)); function EmitIntermediate used Map function emits word document value one. Reduce function sums values perword document using ParseInt() get number occurrences perword documents. MapReduce runtime environment schedules map tasksand reduce tasks nodes WSC. (The complete version program found Dean Ghemawat (2008) .) MapReduce thought generalization single instruction stream, multiple data streams (SIMD) operation ( Chapter 4 )—except func- tion applied passed data —that followed function used reduction output Map task. reductions commonplaceeven SIMD programs, SIMD hardware often offers special operations thereductions. example, Intel ’s AVX SIMD instructions include “horizontal ” instructions add pairs operands adjacent registers. accommodate variability performance hundreds computers, MapReduce scheduler assigns new tasks based quickly nodes completeprior tasks. Obviously, single slow task hold completion largeMapReduce job. Dean Barroso (2013) label situation tail latency .I n WSC, solution slow tasks provide software mechanisms cope withsuch variability inherent scale. approach sharp contrast tothe solution server conventional data center, traditionally slowtasks mean hardware broken needs replaced server software needs tuning rewriting. Performance heterogeneity norm 50,000 – 100,000 servers WSC. example, toward end MapReduce program,the system start backup executions nodes tasks ’t completed yet take result whichever finishes first. return forincreasing resource usage percentage points, Dean Ghemawat (2008) found large tasks completed 30% faster. Dependability built MapReduce start. example, node MapReduce job required report back master node periodically6.2 Programming Models Workloads Warehouse-Scale Computers ■473with list completed tasks updated status. node report back deadline, master node deems node dead reassigns node’s work nodes. Given amount equipment WSC, ’s surprising failures commonplace, prior example attests. deliveron 99.99% availability, systems software must cope reality WSC. Toreduce operational costs, WSCs use automated monitoring software allowingone operator responsible 1000 servers. Programming frameworks MapReduce batch processing exter- nally facing SaaS Search rely upon internal software services success. example, MapReduce relies Google File System (GFS) (Ghemawat et al., 2003 ) Colossus ( Fikes, 2010 ) supply files com- puter, MapReduce tasks scheduled anywhere. addition GFS Colossus, examples scalable storage systems include Amazon ’s key value storage system Dynamo ( DeCandia et al., 2007 ) Google record storage system BigTable ( Chang et al., 2006 ). Note systems often build upon other. example, BigTable stores logs anddata GFS Colossus, much relational database may use file system provided kernel operating system. internal services usually make different decisions similar software running single servers. example, rather assuming storage reliable, suchas using RAID storage servers, systems often make complete replicas thedata. Replicas help read performance well availability; withproper placement, replicas overcome many system failures, like thoseinFigure 6.1 . Systems like Colossus use error-correcting codes rather full rep- licas reduce storage costs, constant cross-server redundancy rather within-a-server within-a-storage array redundancy. Thus, failure entire server storage device ’t negatively affect availability data. Another example different approach WSC storage software often uses relaxed consistency rather following ACID (atomicity, consistency,isolation, durability) requirements conventional database systems. Theinsight ’s important multiple replicas data agree time , but, applications, need agreement times. example,eventual consistency fine video sharing. Eventual consistency makes storage systems much easier scale, absolute requirement WSCs. workload demands public interactive services vary consider- ably; even prominent global service Google Search varies factor oftwo depending time day. factoring weekends, holidays, andpopular times year applications —such photograph-sharing services New Year ’s Day online shopping Christmas —a much greater variation server utilization becomes apparent. Figure 6.3 shows average utilization 5000 Google servers 6-month period. Note less 0.5% servers averaged 100% utilization, servers operated 10% 50% utilization. Stated alternatively, 10% servers utilizedmore 50%. Thus, ’s much important servers WSC perform474 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismwell little perform efficiently peak, rarely operate peak. summary, WSC hardware software must cope variability load based user demand performance dependability vagaries hardware scale. Example result measurements like Figure 6.3 , SPECpower benchmark measures power performance 0% load 100% 10% increments (seeChapter 1 ). overall single metric summarizes benchmark sum performance measures (server-side Java operations per second) divided bythe sum power measurements watts. Thus, level assumed equally likely. would numbers summary metric change levels weighted utilization frequencies Figure 6.3 ?000.0050.010.0150.020.0250.03 0.1 0.2 0.3 0.4 0.5 CPU utilizationFraction time 0.6 0.7 0.8 0.9 1.0 Figure 6.3 Average CPU utilization 5000 servers 6-month period Google. Servers rarely completely idle fully utilized, instead operat-ing time 10% 50% maximum utilization. third column right Figure 6.4 calculates percentages plus minus 5% come weightings; thus 1.2% 90% row means 1.2% servers werebetween 85% 95% utilized. Figure 1 Barroso, L.A., H €olzle, U., 2007. case energy-proportional computing. IEEE Comput. 40 (12), 33 –37.6.2 Programming Models Workloads Warehouse-Scale Computers ■475Answer Figure 6.4 shows original weightings new weighting match Figure 6.3 . weightings reduce performance summary 30% 3210 ssj_ops/watt 2454. Given scale, software must handle failures, means little rea- son buy “gold-plated ”hardware reduces frequency failures. pri- mary impact would increase cost. Barroso H €olzle (2009) found factor 20 difference price-performance high-end Hewlett Packard shared-memory multiprocessor commodity Hewlett Packard server running TPC-C database benchmark. surprisingly, Google companies WSCs use low-end commodity servers. fact, Open Compute Project(http://opencompute.org ) organization companies collaborate open designs servers racks data centers. WSC services also tend develop software rather buy third-party commercial software, part cope huge scale partto save money. example, even best price-performance platform forTPC-C 2017, adding cost SAP SQL Anywhere database Windows operating system increases cost Dell PowerEdge T620 server 40%. contrast, Google runs BigTable Linux operating system itsservers, pays licensing fees. Given review applications systems software WSC, ready look computer architecture WSC.Load Performance WattsSPEC weightingsWeighted performanceWeighted wattsFigure 6.3 weightingsWeighted performanceWeighted watts 100% 2,889,020 662 9.09% 262,638 60 0.80% 22,206 5 90% 2,611,130 617 9.09% 237,375 56 1.20% 31,756 880% 2,319,900 576 9.09% 210,900 52 1.50% 35,889 970% 2,031,260 533 9.09% 184,660 48 2.10% 42,491 1160% 1,740,980 490 9.09% 158,271 45 5.10% 88,082 2550% 1,448,810 451 9.09% 131,710 41 11.50% 166,335 5240% 1,159,760 416 9.09% 105,433 38 19.10% 221,165 7930% 869,077 382 9.09% 79,007 35 24.60% 213,929 9420% 581,126 351 9.09% 52,830 32 15.30% 88,769 5410% 290,762 308 9.09% 26,433 28 8.00% 23,198 25 0% 0 181 9.09% 0 16 10.90% 0 20 Total 15,941,825 4967 1,449,257 452 933,820 380 ssj_ops/W 3210 ssj_ops/W 2454 Figure 6.4 SPECpower result using weightings Figure 6.3 instead even weightings.476 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelism6.3 Computer Architecture Warehouse-Scale Computers Networks connective tissue binds 50,000 –100,000 servers together. Analogous memory hierarchy Chapter 2 , WSCs use hierarchy networks. Figure 6.5 shows one example. Ideally, combined network would provide nearly performance custom high-end switch 100,000 serversat cost per port commodity switch designed 50 servers. willsee Section 6.6 , networks WSCs area active innovation. structure holds servers rack. Although width racks varies per WSC —some classic 19-in. wide rack; others two three times wider —the height tends higher 6 –7 ft since people must service them. rack roughly 40 –80 servers. often convenient connect network cables top rack, switch commonly called Top Rack (ToR) switch. (Some WSCs racks multiple ToR switches.) Typically, bandwidth within rack much higher racks, Rack switch RackArray switch Figure 6.5 Hierarchy switches WSC. Based Figure 1.1 Barroso, L.A., Clidaras, J., H €olzle, U., 2013. datacenter computer: introduction design warehouse-scale machines. Synth. Lect. Comput. Architect. 8 (3), 1 –154.6.3 Computer Architecture Warehouse-Scale Computers ■477so matters less software places sender receiver within rack. flexibility ideal software perspective. switches often offer 4 –16 uplinks, leave rack go next higher switch network hierarchy. Thus, bandwidth leaving rackis 6–24 times smaller bandwidth within rack. ratio called over- subscription . However, large oversubscription means programmers must aware performance consequences placing senders receivers differentracks. increased software-scheduling burden another argument networkswitches designed specifically data center. switch connects array racks considerably expensive ToR switch. cost due part higher connectivity inpart bandwidth switch must much greater reduce theoversubscription problem. Barroso et al. (2013) reported switch 10 times bisection bandwidth —basically, worst-case internal bandwidth —of rack switch costs 100 times much. One reason cost switchbandwidth nports grow n 2.Sections 6.6 and6.7describe networking ToR switch great detail. Storage natural design fill rack servers, minus whatever space needed theswitches. design leaves open question storage placed. ahardware construction perspective, simplest solution would include disksinside rack rely Ethernet connectivity access information thedisks remote servers. expensive alternative would use network-attached storage (NAS), perhaps storage network like InfiniBand. past, WSCs generally relied local disks provided storage software handled connec-tivity dependability. example, GFS used local disks maintained replicasto overcome dependability problems. redundancy covered local diskfailures also power failures racks whole clusters. flexibility ofGFS’s eventual consistency lowers cost keeping replicas consistent, also reduces network bandwidth requirements storage system. Today storage options considerably varied. Although racks balanced terms servers disks, past, may also racks deployed without local disks racks loaded disks. System softwaretoday often uses RAID-like error correction codes lower storage cost ofdependability. aware confusion term cluster talking architecture WSC. Using definition Section 6.1 , WSC extremely large cluster. contrast, Barroso et al. (2013) used term cluster mean next-sized grouping computers, containing many racks. chap- ter, avoid confusion, use term array mean large collection racks organized rows, preserving original definition word clusterto represent anything collection networked computers within rack anentire warehouse full networked computers.478 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismWSC Memory Hierarchy Figure 6.6 shows latency, bandwidth, capacity memory hierarchy inside WSC, Figure 6.7 shows data visually. figures based following assumptions ( Barroso et al., 2013 ): Local Rack Array DRAM latency ( μs) 0.1 300 500 Flash latency ( μs) 100 400 600 Disk latency ( μs) 10,000 11,000 12,000 DRAM bandwidth (MB/s) 20,000 100 10Flash bandwidth (MB/s) 1000 100 10Disk bandwidth (MB/s) 200 100 10DRAM capacity (GB) 16 1024 31,200Flash capacity (GB) 128 20,000 600,000Disk capacity (GB) 2000 160,000 4,800,000 Figure 6.6 Latency, bandwidth, capacity memory hierarchy WSC (Barroso et al., 2013 ).Figure 6.7 plots information. 0.1 Local DRAM Local Disk Rack DRAM Rack Disk Datacenter DRAMDatacenter Disk1.010.0100.01,000.010,000.0100,000.01,000,000.010,000,000.0 Latenc (us) Bandwidth (MB/s ) Capacit (GB) Figure 6.7 Graph latency, bandwidth, capacity memory hierarchy WSC data Figure 6.6 (Barroso et al., 2013 ).6.3 Computer Architecture Warehouse-Scale Computers ■479■Each server contains 16 GiB memory 100-ns access time trans- fers 20 GB/s, 128 GiB Flash 100- μs latency transfers 1 GB/s, 2 TB disk offer 10-ms access time transfer 200 MB/s. Thereare two sockets per board, share one 1 Gbit/s Ethernet port. ■In example, every pair racks includes one rack switch holds 80servers. Networking software plus switch overhead increases latency toDRAM 100 μs disk access latency 11 ms. Thus, total storage capacity rack roughly 1 TB DRAM, 20 TB Flash, 160 TB ofdisk storage. 1 Gbit/s Ethernet limits remote bandwidth DRAM,Flash, disk within rack 100 MB/s. ■The array 30 racks, storage capacity array goes factor 30:30 TB DRAM, 600 TB Flash, 4.8 PB disk. array switch hard-ware software increases latency DRAM within array 500 μs, 600μs Flash, disk latency 12 ms. bandwidth array switch limits remote bandwidth either array DRAM, array Flash, array disk 10 MB/s. Figures 6.6 and6.7show network overhead dramatically increases latency local DRAM Flash, rack DRAM Flash, array DRAM andFlash, still 10 times better latency accessing localdisk. network collapses difference bandwidth rack DRAM, Flash, disk array DRAM, Flash, disk. WSC needs 40 arrays reach 100,000 servers, one level networking hierarchy. Figure 6.8 shows conventional Layer 3 routers connect arrays together Internet. applications fit single array within WSC. need one array use sharding orpartitioning , meaning dataset split independent pieces distributed different arrays. analogy, ’s like picking registration packets conference one person handling names another N Z. Operations whole dataset sent servers hosting pieces, results coalesced client computer. Example average memory latency assuming 90% accesses local server, 9% outside server within rack, 1% outside rackbut within array? Answer average memory access time 90%/C20:1 ðÞ +9%/C2100 ðÞ +1%/C2300 ðÞ ¼ 0:09 + 27 + 5 ¼32:09μs factor 300 slowdown versus 100% local accesses. Clearly, locality access within server vital WSC performance.480 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismExample long take transfer 1000 MB disks within server, servers rack, servers different racks array?How much faster transfer 1000 MB DRAM three cases? Answer 1000 MB transfer disks takes Within server ¼1000=200¼5s Within rack ¼1000=100¼10 Within array ¼1000=10¼100 memory-to-memory block transfer takes Within server ¼1000=20000 ¼0:05 Within rack ¼1000=100¼10 Within array ¼1000=10¼100 Thus, block transfers outside single server, ’t even matter whether data memory disk rack switch array switchare bottlenecks. performance limits affect design WSC softwareand inspire need higher-performance switches (see Section 6.6 ). Internet LB LBCR CRInternet SS SAR AR AR AR Key:  CR =L3 core router  AR =L3 access router  =Array switch  LB =Load balancer  R =Rack 80 servers top rack switchR R R R .. ........ R RDatacenter Layer 3 Layer 2 Figure 6.8 Layer 3 network used link arrays together Internet ( Greenberg et al., 2009 ).A load balancer monitors busy set servers directs traffic less loaded ones try keep servers approximately equally utilized. Another option use separate border router connect Internet data center Layer 3 switches. see Section 6.6 , many modern WSCs abandoned conventional layered networking stack traditional switches.6.3 Computer Architecture Warehouse-Scale Computers ■481Although examples educational, note computers networking equipment much larger faster examples 2013 (seeSection 6.7 ). Servers deployed 2017 256 –1024 GiB DRAM, recent switches reduced delays 300 ns per hop. Given architecture equipment, ready see house, power, cool discuss cost build operate wholeWSC, compared equipment within it. 6.4 Efficiency Cost Warehouse-Scale Computers Infrastructure costs power distribution cooling majority theconstruction costs WSC, concentrate them. ( Section 6.7 describes power cooling infrastructure WSC detail.) Acomputer room air-conditioning (CRAC ) unit cools air server room using chilled water, similar refrigerator removes heat releasing out-side refrigerator. liquid absorbs heat, evaporates. Conversely, aliquid releases heat, condenses. Air conditioners pump liquid coils underlow pressure evaporate absorb heat, sent external con- denser released. Thus, CRAC unit, fans push warm air past set coils filled cold water, pump moves warmed water chillers tobe cooled down. Figure 6.9 shows large collection fans water pumps move air water throughout system. addition chillers, data centers leverage colder outside air water temperature cool water sent chillers. However, dependingon location, chillers may still needed warmer times year. Surprisingly, ’s obvious figure many servers WSC support subtracting overhead power distribution cooling. nameplate power rating server manufacturer always conservative: it’s maximum power server draw. first step measure single server variety workloads deployed WSC. (Networking istypically 5% power consumption, ignored first.) determine number servers WSC, available power equipment could divided measured server power; however, thiswould conservative according Fan et al. (2007) . found significant gap thousands servers could theoretically do, worst case, practice, since real workloadswill keep thousands servers simultaneously peaks. found thatthey could safely oversubscribe number servers much 40% basedon power single server. recommended WSC architects doso increase average utilization power within WSC; however, alsosuggested using extensive monitoring software along safety mechanism thatde-schedules lower priority tasks case workload shifts. power usage inside equipment Google WSC deployed 2012 ( Barroso et al., 2013 ):482 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelism■42% power processors ■12% DRAM ■14% disks ■5% networking ■15% cooling overhead ■8% power overhead ■4% miscellaneous Measuring Efficiency WSC widely used, simple metric evaluate efficiency data center WSC called power utilization effectiveness (orPUE ): PUE¼Total facility powerðÞ =IT equipment powerðÞ Thus, PUE must greater equal 1, bigger PUE, less efficient WSC. Computer room air handlerCooling tower CWS pumpHeat exchanger (Water-side economizer) A/C condenserPrimary pump A/C evaporator Leakage Cold HotDiluted hot/cold mixColdFans Air impellerServer fans 6 9 W eachA/C compressor Blow & evaporative loss 8 MW facility: ~200,000 gal/day Figure 6.9 Mechanical design cooling systems. CWS stands circulating water system. Hamilton, J., 2010. Cloud computing economies scale. In: Paper Presented AWS Workshop Genomics Cloud Computing, June 8, 2010, Seattle, WA. http://mvdirona.com/jrh/TalksAndPapers/JamesHamilton_ GenomicsCloud20100608.pdf .6.4 Efficiency Cost Warehouse-Scale Computers ■483Greenberg et al. (2009) reported PUE 19 data centers portion overhead went cooling infrastructure. Figure 6.10 shows found, sorted PUE least efficient. median PUE 1.69,with cooling infrastructure using half much power theservers —on average, 0.55 1.69 cooling. Note average PUEs, vary daily depending workload even external air temper- ature, see ( Figure 6.11 ). attention paid PUE past decade, data centers much efficient today. However, Section 6.8 explains, universally accepted definition included PUE: batteries preserve operation apower failure separate building, included not? measurefrom output power substation, power first enters WSC?Figure 6.10 shows improvement average PUE Google data centers time, Google measures inclusively. Since performance per dollar ultimate metric, still need measure performance. Figure 6.7 shows, bandwidth drops latency increases depend- ing distance data. WSC, DRAM bandwidth within server is200 times greater within rack, turn 10 times greater withinan array. Thus, another kind locality consider placement dataand programs within WSC.Power usage effectiveness (PUE) 0 0.5 1 1.5 2 2.5 3 3.5IT AC Other1.33 1.351.431.47 1.49 1.52 1.59 1.671.691.69 1.69 1.82 2.04 2.042.132.33 2.38 2.63 3.03 Figure 6.10 Power utilization efficiency 19 data centers 2006 ( Greenberg et al., 2009 ).The power air conditioning (AC) uses (such power distribution) normalized power equipment calculating PUE. Thus, power equipment must 1.0, AC varies 0.30 1.40 times power equipment. Power “other ”varies 0.05 0.60 equipment.484 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismAlthough designers WSC often focus bandwidth, programmers devel- oping applications WSC also concerned latency latency isvisible users. Users ’satisfaction productivity tied response time service. Several studies timesharing days report user productivity isinversely proportional time interaction, typically broken human entry time, system response time, time person think response hitting next entry ( Doherty Thadhani, 1982 ). results experiments showed cutting system response time 30% shavedthe time interaction 70% ( Brady, 1986 ). implausible result explained human nature: people need less time think given fasterresponse, less likely get distracted remain “on roll. ” Figure 6.12 shows results recent experiment Bing search engine, delays 50 –2000 ms inserted search server ( Schurman Brutlag, 2009 ). expected previous studies, time next click roughly doubled delay; is, 200 ms delay server led 500 ms increase intime next click. Revenue dropped linearly increasing delay, user sat-isfaction. separate study Google search engine found effectslingered long 4-week experiment ended. Five weeks later, were0.1% fewer searchers per day users experienced 200 ms delays, therewere 0.2% fewer searches users experienced 400 ms delays. Given theamount money made search, even small changes disconcerting. fact, results negative ended experiment prematurely. extreme concern satisfaction users Internet service, performance goals typically specified high percentage ofrequests latency threshold, rather offer target averagelatency. threshold goals called service level objectives (SLOs ). SLO might 99% requests must 100 ms. Thus, designers of2013 2008 2009 2010 Trailin g twelve-month (TTM) PUE2011 2012 2014 2015 2016 20171.101.141.18PUE1.221.26 1.12 1.11 Quarterl PUEContinuous PUE improvement Average PUE data centers Figure 6.11 Average power utilization efficiency (PUE) 15 Google WSCs 2008 2017. spiking line quarterly average PUE, straighter line trailing 12-month average PUE. Q4 2016, averages 1.11 1.12, respectively.6.4 Efficiency Cost Warehouse-Scale Computers ■485Amazon ’s Dynamo key-value storage system decided services offer good latency top Dynamo, storage system deliver latency goal 99.9% time ( DeCandia et al., 2007 ). example, one improvement Dynamo helped 99.9th percentile much average case, whichreflects priorities. Dean Barroso (2013) proposed term tail tolerant describe systems designed meet goals: fault-tolerant computing aims create reliable whole less- reliable parts, large online services need create predictably responsivewhole less-predictable parts. causes unpredictability include contention shared resources (processors networks, etc.), queuing, variable microprocessor performance optimi-zations like Turbo mode energy-saving techniques like DVFS, software garbage collection, many more. Google concluded instead trying prevent variability WSC, made sense develop tail-tolerant techniques tomask work around temporary latency spikes. example, fine-grained loadbalancing quickly move small amounts work servers reducequeuing delays. Cost WSC mentioned introduction, unlike architects, designers WSCs worryabout cost operate well cost build WSC. Accounting labels theformer costs operational expenditures (OPEX ) latter costs capital expenditures (CAPEX ). put cost energy perspective, Hamilton (2010) case study estimate costs WSC. determined CAPEX 8-MW facilitywas $88 million roughly 46,000 servers corresponding networking equipment added another $79 million CAPEX WSC. Figure 6.13 shows rest assumptions case study.Server delay (ms)Increased time next click (ms)Queries/ userAny clicks/ userUser satisfactionRevenue/ user 50 –– – – – 200 500 – /C00.3% /C00.4% – 500 1200 – /C01.0% /C00.9% /C01.2% 1000 1900 /C00.7% /C01.9% /C01.6% /C02.8% 2000 3100 /C01.8% /C04.4% /C03.8% /C04.3% Figure 6.12 Negative impact delays Bing search server user behavior (Schurman Brutlag, 2009 ).486 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismHamilton ’s study works $11/watt building, power, cooling. Barroso et al. (2013) reported consistent results several cases, cost $9 to$13/watt. Thus, 16-MW facility costs $144 million $208 million, notinclud- ing computing, storage, networking equipment. convert CAPEX OPEX cost capital conversion, assuming 5% borrowing cost, standard convention US accounting rules. is,we amortize CAPEX fixed amount month effective life ofSize facility (critical load watts) 8,000,000 Average power usage (%) 80%Power usage effectiveness 1.45Cost power ( $/kWh) $0.07 % Power cooling infrastructure (% total facility cost) 82%CAPEX facility (not including equipment) $88,000,000 Number servers 45,978Cost/server $1450 CAPEX servers $66,700,000 Number rack switches 1150Cost/rack switch $4800 Number array switches 22Cost/array switch $300,000 Number layer 3 switches 2Cost/layer 3 switch $500,000 Number border routers 2Cost/border router $144,800 CAPEX networking gear $12,810,000 Total CAPEX WSC $167,510,000 Server amortization time 3 yearsNetworking amortization time 4 yearsFacilities amortization time 10 yearsAnnual cost money 5% Figure 6.13 Case study WSC, rounded nearest $5000. Internet bandwidth costs vary application, included here. remaining 18% CAPEX facility includes buying property cost construction building.We added people costs security facilities management Figure 6.14 , part case study. Note Hamilton ’s estimates done joined Amazon, based WSC particular company. Based onHamilton, J., 2010. Cloud computing economies scale. In: Paper Presented AWS Workshop Genomics Cloud Computing, June 8, 2010, Seattle, WA. http:// mvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf .6.4 Efficiency Cost Warehouse-Scale Computers ■487the equipment. Figure 6.14 breaks monthly OPEX Hamilton ’s case study. Note amortization rates differ significantly case study, 10 years facility 4 years networking equipment 3 years theservers. Thus, WSC facility lasts decade, servers replaced every3 years networking equipment every 4 years. amortizing CAPEX,Hamilton came monthly OPEX, including accounting cost ofborrowing money (5% annually) pay WSC. $3.8 million, monthly OPEX 2% CAPEX (or 24% annually). figure allows us calculate handy guideline keep mind making decisions components use concerned energy. fully burdened cost watt per year WSC, including costof amortizing power cooling infrastructure, Monthly cost infrastructure + monthly cost power Facility size watts/C212¼$765K + $475K 8M/C212¼$1:86 cost roughly $2 per watt-year. Thus, reducing costs saving energy result spending $2 per watt-year (see Section 6.8 ). Note Figure 6.14 , third OPEX related power, category trending server costs trending time. networking equipment significant 8% total OPEX 19% server CAPEX, networking equipment trending quickly serversare, perhaps continuing demand higher network bandwidth(seeFigure 6.22 page 467). difference especially true switches networking hierarchy rack, represent network-ing costs (see Section 6.6 ). People costs security facilities management 2% OPEX. Dividing OPEX Figure 6.14 number servers hours per month, cost $0.11 per server per hour. Expense (% total) Category Monthly cost Percent monthly cost Amortized CAPEX (85%) Servers $2,000,000 53% Networking equipment $290,000 8% Power cooling infrastructure $765,000 20% infrastructure $170,000 4% OPEX (15%) Monthly power use $475,000 13% Monthly people salaries benefits $85,000 2% Total OPEX $3,800,000 100% Figure 6.14 Monthly OPEX Figure 6.13 , rounded nearest $5000. Note 3-year amortization servers means purchasing new servers every 3 years, whereas facility amortized 10 years. Thus, amor-tized capital costs servers three times facility. People costs include three securityguard positions continuously 24 h day, 365 days year, $20 per hour per person, one facilities person 24 h day, 365 days year, $30 per hour. Benefits 30% salaries. calculation include cost network bandwidth Internet varies application vendor maintenance fees varyby equipment negotiations.488 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismBarroso et al. (2013) evaluated CAPEX OPEX terms cost per watt per month. Thus, 12-MW WSC depreciated 12 years, depreciation cost is$0.08 per watt per month. assumed company got capital WSC taking loan 8% annually —corporate loans typically 7% 12% —and interest payments added another $0.05, giving total $0.13 per watt per month. factored cost servers similarly. 500 watt server cost $4000 $8 per watt, 4-year depreciation $0.17 per watt per month. 8% interest loan servers added $0.02. estimated networking $0.03 per watt per month. reported typical OPEX cost multiple MW WSCs varied $0.02 $0.08 per watt per month. grand total $0.37 $0.43 per watt per month. 8-MW WSC, monthly cost minus cost electricity $3.0 million $3.5 million. subtract monthly power use Hamilton ’s calculation, estimate monthly rate $3.3 million. Given different approaches predicting costs, estimates remarkably consistent. Example cost electricity varies region United States $0.03 $0.15 per kilowatt-hour. impact hourly server costs twoextreme rates? Answer multiply critical load 8 MW average PUE Figure 6.13 (sec- ond row) calculate average power usage: 8/C21:45/C280%¼9:28 Megawatts monthly cost power goes $475,000 Figure 6.14 to$205,000 $0.03 per kilowatt-hour $1,015,000 $0.15 per kilowatt-hour. changes electricity cost alter hourly server costs $0.11 $0.10 $0.13, respectively. Example would happen monthly costs amortization times made —say, 5 years? would change hourly cost per server? Answer spreadsheet available online http://mvdirona.com/jrh/TalksAndPapers/ PerspectivesDataCenterCostAndPower.xls .Changing amortization time 5 years changes first four rows Figure 6.14 Servers $1,260,000 37% Networking equipment $242,000 7% Power cooling infrastructure $1,115,000 33% infrastructure $245,000 7% total monthly OPEX $3,422,000. replaced everything every 5 years, cost would $0.103 per server hour, amortized costs facility rather servers, Figure 6.14 .6.4 Efficiency Cost Warehouse-Scale Computers ■489The rate $0.10 per server per hour much less cost many companies operate (smaller) conventional data cen-ters. cost advantage WSCs led large Internet companies offer computingas utility where, like electricity, pay use. Today, utility computing better known cloud computing . 6.5 Cloud Computing: Return Utility Computing computers kind advocated become computers future, computing may someday organized public utility tele-phone system public utility …. computer utility could become basis new important industry. John McCarthy, MIT centennial celebration (1961) Driven demand increasing number users, Internet companies Amazon, Google, Microsoft built increasingly larger warehouse-scalecomputers commodity components, making McCarthy ’s prediction eventu- ally come true, thought popularity timesharing.This demand led innovations systems software support operating thisscale, including BigTable, Colossus, Dynamo, GFS, MapReduce. alsodemanded improvement operational techniques deliver service available least 99.99% time despite component failures security attacks. Examples techniques include failover, firewalls, virtual machines, andprotection distributed denial-of-service attacks. software andexpertise providing ability scale increasing customer demand jus-tified investment, WSCs 50,000 –100,000 servers become common- place 2017. increasing scale came increasing economies scale. Based study 2006 compared WSC data center 1000 servers, Hamilton (2010) reported following advantages: ■5.7 times reduction storage costs —It cost WSC $4.6 per GB per year disk storage versus $26 per GB data center. ■7.1 times reduction administrative costs —The ratio servers per adminis- trator 1000 WSC versus 140 data center. ■7.3 times reduction networking costs —Internet bandwidth cost WSC $13 per Mbit/s/month versus $95 data center. surprisingly, one negotiate much better price per Mbit/s ordering 1000 Mbit/s thanby ordering 10 Mbit/s. Another economy scale comes purchasing. high level pur- chasing leads volume discount prices virtually everything WSC.490 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismEconomies scale also apply operational costs. prior section, saw many data centers operate PUE 2.0. Large firms justify hiring mechanical power engineers develop WSCs lower PUEs, rangeof 1.1 –1.2 (see Section 6.7 ). Internet services need distributed multiple WSCs dependabil- ity reduce latency, especially international markets. large firms usemultiple WSCs reason. ’s much expensive individual firms create multiple, small data centers around world single data center intheir corporate headquarters. Finally, reasons presented Section 6.1 , servers data centers tend utilized 10% –20% time. making WSCs available public, uncorrelated peaks different customers raise average utilizationabove 50%. Thus, economies scale WSC offer factors 5 –7 several compo- nents WSC plus factors 1.5 –2 entire WSC. Since last edition book, concerns security flipped cloud. 2011 skepticism placing critical data cloud could make easier hackers break data kept premises ( “on prem ”) locked local data center. 2017 data break-ins data centers routine barely make news. example, insecurity even led rapid growth ransomware — criminals break in, encrypt data organization, ’t release key paid ransom —costing firms $1 billion 2015. contrast, WSCs continuously attack, operators respond quickly haltthem thus build better defenses. result, ransomware unheard inside WSCs. WSCs clearly secure vast majority local data centers today, many CIOs believe critical data safer cloud than“on prem. ” Although several cloud computing providers, feature Amazon Web Services (AWS) since one oldest currently largest commer-cial cloud provider. Amazon Web Services Utility computing goes back commercial timesharing systems even batchprocessing systems 1960s 1970s, companies paid aterminal phone line billed based much computing theyused. Many efforts since end timesharing tried offer pay-as-you-go services, often met failure. Amazon started offering utility computing via Amazon Simple Storage Service (Amazon S3) Amazon Elastic Computer Cloud (Amazon EC2) 2006, made novel technical business decisions: ■Virtual machines . Building WSC using x86-commodity computers run- ning Linux operating system Xen virtual machine solved several6.5 Cloud Computing: Return Utility Computing ■491problems. First, allowed Amazon protect users other. Second, simplified software distribution within WSC, customers needed install image AWS automatically distributed theinstances used. Third, ability kill virtual machine reliably madeit easy Amazon customers control resource usage. Fourth, virtualmachines could limit rate used physical processors, disks,and network well amount main memory, gave AWSmultiple price points: lowest price option packing many virtual coreson single server, highest price option exclusive access machine resources, well several intermediary points. Fifth, virtual machines hid identity hardware, allowing AWS continue sell timeon older machines might otherwise unattractive customers theyknew age machines. Finally, virtual machines allowed AWS intro-duce new faster hardware either packing even virtual cores perserver simply offering instances higher performance per virtualcore; virtualization meant offered performance need integermultiple performance hardware. ■Very low cost . AWS announced rate $0.10 per hour per instance 2006, startlingly low amount. instance one virtual machine, andat$0.10 per hour, AWS allocated two instances per core multicore server. Thus, one EC2 computer unit equivalent 1.0 –1.2 GHz AMD Opteron Intel Xeon era. ■(Initial) reliance open source software . availability good-quality software licensing problems costs associated running onhundreds thousands servers made utility computing much econom-ical Amazon customers. AWS later started offering instancesincluding commercial third-party software higher prices. ■No (initial) guarantee service . Amazon originally promised best effort. low cost attractive many could live without service guarantee.Today AWS provides availability SLOs 99.95% services asAmazon EC2 Amazon S3. Additionally, Amazon S3 designed fordurability saving multiple replicas object across multiple locations. (According AWS, chances permanently losing object one 100 billion.) AWS also provides Service Health Dashboard showsthe current operational status AWS services real time thatAWS uptime performance fully transparent. ■No contract required . part costs low, necessary start using EC2 credit card. Figures 6.15 and6.16 show hourly price many types EC2 instances 2017. Expanding 10 instance types 2006, nowmore 50. fastest instance 100 times quicker slowest, and492 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismInstance Per hourRatio m4.largeVirtual coresCompute unitsMemory (GiB) Storage (GB)General-purposet2.nano $0.006 0.05 1 Variable 0.5 EBS t2.micro $0.012 0.11 1 Variable 1.0 EBS t2.small $0.023 0.21 1 Variable 2.0 EBS t2.medium $0.047 0.4 2 Variable 4.0 EBS t2.large $0.094 0.9 2 Variable 8.0 EBS t2.xlarge $0.188 1.7 4 Variable 16.0 EBS t2.2xlarge $0.376 3.5 8 Variable 32.0 EBS m4.large $0.108 1.0 2 6.5 8.0 EBS m4.xlarge $0.215 2.0 4 13 16.0 EBS m4.2xlarge $0.431 4.0 8 26 32.0 EBS m4.4xlarge $0.862 8.0 16 54 64.0 EBS m4.10xlarge $2.155 20.0 40 125 160.0 EBS m4.16xlarge $3.447 31.9 64 188 256.0 EBS m3.medium $0.067 0.6 1 3 3.8 1 /C24 SSD m3.large $0.133 1.2 2 6.5 7.5 1 /C232 SSD m3.xlarge $0.266 2.5 4 13 15.0 2 /C240 SSD m3.2xlarge $0.532 4.9 8 26 30.0 2 /C280 SSDCompute-optimizedc4.large $0.100 0.9 2 8 3.8 EBS c4.xlarge $0.199 1.8 4 16 7.5 EBS c4.2xlarge $0.398 3.7 8 31 15.0 EBS c4.4xlarge $0.796 7.4 16 62 30.0 EBS c4.8xlarge $1.591 14.7 36 132 60.0 EBS c3.large $0.105 1.0 2 7 3.8 2 /C216 SSD c3.xlarge $0.210 1.9 4 14 7.5 2 /C240 SSD c3.2xlarge $0.420 3.9 8 28 15.0 2 /C280 SSD c3.4xlarge $0.840 7.8 16 55 30.0 2 /C2160 SSD c3.8xlarge $1.680 15.6 32 108 60.0 2 /C2320 SSD Figure 6.15 Price characteristics on-demand general-purpose compute-optimized EC2 instances Virginia region United States February 2017. AWS started, one EC2 computer unit equiv- alent 1.0 –1.2 GHz AMD Opteron Intel Xeon 2006. Variable instances newest cheapest category. offer full performance high-frequency Intel CPU core workload utilizes less 5% core onaverage 24 h, serving web pages. AWS also offers Spot Instances much lower cost (about 25%).With Spot Instances, customers set price willing pay number instances willing run, AWS runs bids spot price drops level. AWS also offers Reserved Instances cases customers know use instance year. pay yearly fee per instance thenan hourly rate 30% column 1 use service. Reserved Instance used 100% whole year, average cost per hour including amortization annual fee 65% rate first column. EBS Elastic Block Storage, raw block-level storage system found elsewhere network, rather ina local disk local solid stage disk (SSD) within server VM.6.5 Cloud Computing: Return Utility Computing ■493the largest offers 2000 times memory smallest. Rent cheapest instance whole year $50. addition computation, EC2 charges long-term storage Internet traffic. (There cost network traffic inside AWS regions.) Elastic BlockStorage (EBS) costs $0.10 per GB per month using SSDs $0.045 per GB monthly hard disk drives. Internet traffic costs $0.01 per GB going EC2 $0.09 per GB coming EC2.Instance Per hourRatio m4.large Virtual cores Compute units Memory (GiB) Storage (GB)GPUp2.xlarge $0.900 8.3 4 12 61.0 EBS p2.8xlarge $7.200 66.7 32 94 488.0 EBS p2.16xlarge $14.400 133.3 64 188 732.0 EBS g2.2xlarge $0.650 6.0 8 26 15.0 60 SSD g2.8xlarge $2.600 24.1 32 104 60.0 2 /C2120 SSDFPGAf1.2xlarge $1.650 15.3 8 (1 FPGA) 26 122.0 1 /C2470 SSD f1.16xlarge $13.200 122.2 64 (8 FPGA) 188 976.0 4 /C2940 SSDMemory-optimizedx1.16xlarge $6.669 61.8 64 175 976.0 1 /C21920 SSD x1.32xlarge $13.338 123.5 128 349 1,952.0 2 /C21920 SSD r3.large $0.166 1.5 2 6.5 15.0 1 /C232 SSD r3.xlarge $0.333 3.1 4 13 30.5 1 /C280 SSD r3.2xlarge $0.665 6.2 8 26 61.0 1 /C2160 SSD r3.4xlarge $1.330 12.3 16 52 122.0 1 /C2320 SSD r3.8xlarge $2.660 24.6 32 104 244.0 2 /C2320 SSD r4.large $0.133 1.2 2 7 15.3 EBS r4.xlarge $0.266 2.5 4 14 30.5 EBS r4.2xlarge $0.532 4.9 8 27 61.0 EBS r4.4xlarge $1.064 9.9 16 53 122.0 EBS r4.8xlarge $2.128 19.7 32 99 244.0 EBS r4.16xlarge $4.256 39.4 64 195 488.0 EBS onlyStorage-optimizedi2.xlarge $0.853 7.9 4 14 30.5 1 /C2800 SSD i2.2xlarge $1.705 15.8 8 27 61.0 2 /C2800 SSD i2.4xlarge $3.410 31.6 16 53 122.0 4 /C2800 SSD i2.8xlarge $6.820 63.1 32 104 244.0 8 /C2800 SSD d2.xlarge $0.690 6.4 4 14 30.5 3 /C22000 HDD d2.2xlarge $1.380 12.8 8 28 61.0 6 /C22000 HDD d2.4xlarge $2.760 25.6 16 56 122.0 12 /C22000 HDD d2.8xlarge $5.520 51.1 36 116 244.0 24 /C22000 HDD Figure 6.16 Price characteristics on-demand GPUs, FPGAs, memory-optimized, storage-optimized EC2 instances Virginia region United States February 2017.494 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismExample Calculate cost running average MapReduce job Figure 6.2 page 438 EC2 several months years. Assume plenty jobs, thereis significant extra cost round get integer number hours. Nextcalculate cost per month run MapReduce jobs. Answer first question is, right size instance match typical server Google? Let ’s assume closest match Figure 6.15 c4.large 2 virtual cores 3.6 GiB memory, costs $0.100 per hour. Figure 6.17 calculates average total cost per year running Google MapReduce workload onEC2. average September 2016 MapReduce job would cost little $1o n EC2, total workload month would cost $114 million AWS. Example Given cost MapReduce jobs, imagine boss wants investigate ways lower costs. much might save using AWS Spot Instances? Answer MapReduce jobs could disrupted kicked spot instance, MapReduce designed tolerate restart failed jobs. AWS Spot price forc4.large $0.0242 versus $0.100, meant savings $87 million Sep- tember 2016, guarantees response times! addition low-cost pay-for-use model utility computing, another strong attractor cloud computing users cloud-computing pro-viders take risks over-provisioning under-provisioning. either mistake could fatal, risk avoidance godsend startup companies. much precious investment spent servers product readyfor heavy use, company could run money. service suddenly becamepopular ’t enough servers match demand, company couldAug-04 Sep-09 Sep-12 Sep-16 Average completion time (h) 0.15 0.14 0.13 0.11 Average number servers per job 157 156 142 130Cost per hour EC2 c4.large instance $0.100 $0.100 $0.100 $0.100 Average EC2 cost per MapReduce job $2.76 $2.23 $1.89 $1.20 Monthly number MapReduce jobs 29,000 4,114,919 15,662,118 95,775,891 Total cost MapReduce jobs EC2/EBS $80,183 $9,183,128 $29,653,610 $114,478,794 Figure 6.17 Estimated cost run Google MapReduce workload select months 2004 2016 (Figure 6.2 ) using 2017 prices AWS EC2. using 2017 prices, underestimates actual AWS costs.6.5 Cloud Computing: Return Utility Computing ■495make bad impression potential new customers desperately needs order grow. poster child scenario FarmVille Zynga, social networking game Facebook. FarmVille announced, largest social game wasabout five million daily players. FarmVille one million players 4 days afterlaunching 10 million players 60 days. 270 days, 28 milliondaily players 75 million monthly players. FarmVille deployed onAWS, able grow seamlessly number users. Moreover, able toshed load based customer demand time day. FarmVille successful Zynga decided open data centers 2012. 2015, Zynga returned AWS, deciding better let AWS run itsdata centers ( Hamilton, 2015 ). FarmVille dropped popular Facebook application 110th 2016, Zynga able downsize gracefullywith AWS, much grew AWS beginning. 2014, AWS offered new service hearkened back timesharing days 1960s John McCarthy referring opening quote ofthis section. Instead managing virtual machines cloud, Lambda lets users supply function source code (such Python) lets AWS automatically manage resources required code scale input size makeit highly available. Google Cloud Compute Functions Microsoft Azure Func-tions equivalent capabilities competing cloud providers. Section 6.10 explains, Google App Engine originally offered quite similar service 2008. trend referred Serverless Computing , users ’t manage servers (but functions arein fact run servers). tasks provided include operating system maintenance, capacity provisioning automatic scal- ing, code security patch deployment, code monitoring logging. runs code response events, http request database update. One way tothink Serverless Computing set processes running parallel across theentire WSC share data disaggregated storage service asAWS S3. cost Serverless Computing program idle. AWS accounting six orders magnitude finer EC2, recording usage per 100 msinstead per hour. Cost varies depending amount memory needed, program used 1 GiB memory, cost $0.000001667 per 100 ms $6 per hour. Serverless Computing thought next evolutionary step toward realizing cloud computing ideals data center computer, pay-as-you-go pricing, means automatic dynamic scaling. Cloud computing made benefits WSC available everyone. Cloud computing offers cost associativity illusion infinite scalability noextra cost user: 1000 servers 1 h cost 1 server 1000 h. cloud computing provider ensure enough servers, storage, Internet bandwidth available meet demand. previ-ously mentioned optimized supply chain, drops time-to-delivery weekfor new computers, considerable aid providing illusion without496 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismbankrupting provider. transfer risks, cost associativity, pay-as-you-go pricing, greater security powerful argument companies varying sizes use cloud computing. Big AWS Cloud? AWS started 2006 grew large Amazon.com, rather use aseparate computing infrastructure, became one AWS ’s customers 2010. Figure 6.18 shows AWS facilities 16 locations around world 2017, two way. point interest, Figures 6.19 and6.20 show similar maps Google Microsoft. AWS location consists two three nearby facilities (one two kilometers apart) called availability zones . named safe software running two ensure dependabilityas unlikely would fail simultaneously power outagesor natural disaster ( Hamilton, 2014 ). 16 locations contain 42 availability zones, zones one WSCs. 2014 WSC atleast 50,000 servers, 80,000. 33 533 222 22 2 23 332 Figure 6.18 2017 AWS 16 sites ( “regions ”), two opening soon. sites two three availability zones , located nearby unlikely affected natural disaster power outage, one occur. (The number availability zones listed inside circle map.) 16sites regions collectively 42 availability zones. availability zone one WSCs. https://aws.ama zon.com/about-aws/global-infrastructure/ .6.5 Cloud Computing: Return Utility Computing ■497Hamilton (2017) says best least three WSCs per region. reason simply one WSC fails, region needs take load ofthe failed WSC. one WSC, would reserve halfof capacity failover. three, could used two-thirds capacityand still handle quick failover. data centers have, less reservedexcess capacity; AWS regions 10 WSCs. found two published estimates total number servers AWS 2014. One estimate 2 million servers, AWS 11 regions 28 availability zones ( Clark, 2014 ). Another estimate 2.8 5.6 million servers (Morgan 2014). extrapolate 2014 2017 based increasednumber availability zones, estimates grow 3.0 million servers thelow end 8.4 million high end. total number WSCs (data centers)is 84 –126. Figure 6.21 shows growth time, using extrapolations two projections offer high low estimates number seversand WSCs time. AWS understandably mum actual number. said AWS 1 million customers 2014 “every dayAWS adds enough physical server capacity equivalent needed support Amazon.com in2004”when $7 billion annual revenue company ( Hamilton, 2014 ). One way check validity estimates look investments. Amazon spent $24 billion capital investments property equipmentFigure 6.19 2017 Google 15 sites. Americas: Berkeley County, South Carolina; Council Bluffs, Iowa; Douglas County, Georgia; Jackson County, Alabama; Lenoir, North Carolina; Mayes County, Oklahoma; MontgomeryCounty, Tennessee; Quilicura, Chile; Dalles, Oregon. Asia: Changhua County, Taiwan; Singapore. Europe: Dublin, Ireland; Eemshaven, Netherlands; Hamina, Finland; St. Ghislain, Belgium. https://www.google.com/about/ datacenters/inside/locations/ .498 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismAustralia EastSoutheast AsiaEast AsiaGermany CentralGermany Northeast North Central US US DoD CentralUS DoD East Brazil SouthEast US East US 2 US Gov VirginiaWest Europe Japan WestJapan East China NorthNorth Europe UK West US Gov Arizona West Central US US Gov Texas South Central USWest USWest US 2France Central France SouthUK South Canada East US Gov lowa Central USCanada Central China East West India Central India South IndiaKorea SouthKorea Central Australia SoutheastGenerally available Coming soon Figure 6.20 2017 Microsoft 34 sites, four opening soon. https://azure.microsoft.com/en-us/regions/ .6.5 Cloud Computing: Return Utility Computing ■499between 2013 2015, one estimate two-thirds investment AWS (Gonzalez Day 2016; Morgan 2016). Assume takes year toconstruct new WSC. estimate Figure 6.21 2014 2016 34 51 WSCs. cost per AWS WSC $310 million $470 million. Hamilton states “even medium sized datacenter (WSC) likely exceed $200M. ”(Hamilton, 2017 ). goes say today cloud providers currently “O(10 2)”WSCs; Figure 6.21 estimate 84 –126 AWS WSCs. Despite fuzziness estimates, appear surprisingly consistent. goeson predict meet future demands, largest cloud providers even-tually rise “O(10 5)”WSCs, 1000 times WSCs today!0 2006 2008 2010 2012 2014 201625005000Servers (1000s) Regions, availability zones, WSCs750010000 RegionsAvailability ZonesWSCs (low estimate)WSCs (high estimate)Servers (low estimate)Servers (high estimate) Figure 6.21 Growth AWS regions availability zones (right vertical axis) time. regions two three availability zones. availability zone one WSCs, largest 10 WSCs. WSC least 50,000 servers, biggest 80,000 servers ( Hamilton, 2014 ). Based two published estimates number AWS servers 2014 ( Clark, 2014 ; Morgan 2014), project number servers per year (left vertical axis) WSCs (right vertical access) function actual number availability zones.500 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismNo matter many servers WSCs cloud, two cross-cutting issues shape cost-performance WSCs thus cloud computing WSC network efficiency server hardware software. 6.6 Cross-Cutting Issues Net gear SUV datacenter. James Hamilton (2009) Preventing WSC Network Bottleneck Figure 6.22 shows network demands doubling every 12 –15 months Google, resulting 50 /C2growth traffic servers Google ’s fleet WSCs 7 years. Clearly, without great care, WSC network could easily become performance cost bottleneck. previous edition, pointed data center switch could cost almost $1 million, 50 times much Top Rack switch. switch expensive, resulting oversubscription affected design software placement services data within WSC. WSC network bottlenecks constrained data placement, turn complicated WSC software. software one valuable assets WSC company, cost added complexity significant. ideal WSC network would black box whose topology band- width uninteresting restrictions: workload could Jul ‘08 Jun ‘09 ‘10 Apr ‘11 Mar ‘12 Feb ‘13 Dec ‘13TimeAggregate traffic Nov ‘1450x 1xTraffic generated servers datacenters Time Aggregate traffic e g 50x 1x gy Figure 6.22 Network traffic servers Google ’s WSCs 7 years (Singh et al., 2015).6.6 Cross-Cutting Issues ■501be placed anywhere optimized server utilization rather network traffic locality. Vahdat et al. (2010) proposed borrowing networking technology supercomputing overcome price performance problems. lat-ter proposed networking infrastructure could scale 100,000 ports and1 Pbit/s bisection bandwidth. major benefit novel data centerswitches simplify software challenges oversubscription. Since time, many companies WSC designed switches overcome challenges ( Hamilton, 2014 ).Singh et al. (2015) reported several generations custom networks used inside Google WSCs, Figure 6.23 lists. keep costs down, built switches standard commodity switch chips. found features traditional data center switches usedin part justify high costs —such decentralized network routing pro- tocols manage support arbitrary deployment scenarios —were unnecessary WSC network topology could planned advance deployment Data center generation switchFirst deployedMerchant siliconTop rack (ToR) switch configEdge aggregation blockSpine blockFabric speed Host speedBisection BW Four-Post CRs2004 Vendor 48 /C21 Gbps –– 10 Gbps 1 Gbps 2 Tbps Firehose 1.0 2005 8 /C210 Gbps 4/C210 Gbps (ToR)2/C210 Gbps 24/C21 Gbps down2/C232/C210 Gbps32/C210 Gbps10 Gbps 1 Gbps 10 Tbps Firehose 1.1 2006 8 /C210 Gbps 4 /C210 Gbps 48/C21 Gbps down64/C210 Gbps 32 /C210 Gbps10 Gbps 1 Gbps 10 Tbps Watchtower 2008 16 /C210 Gbps 4 /C210 Gbps 48/C21 Gbps down4/C2128/C210 Gbps128/C210 Gbps10 Gbps n /C21 Gbps 82 Tbps Saturn 2009 24 /C210 Gbps 24 /C210 Gbps 4 /C2288/C210 Gbps288/C210 Gbps10 Gbps n /C210 Gbps 207 Tbps Jupiter 2012 16 /C240 Gbps 16 /C240 Gbps 8 /C2128/C240 Gbps128/C240 Gbps10/40 Gbpsn/C210 Gbps/ n/C240 Gbps1300 Tbps Figure 6.23 Six generations network switches deployed Google WSCs ( Singh et al., 2015 ).The Four-Post CRs used commercial 512 port, 1 Gbit/s Ethernet switches, 48-port, 1 Gbit/s Ethernet Top Rack (ToR) switches, allowed 20,000 servers array. goal Firehose 1.0 deliver 1 Gbps nonblocking bisectionbandwidth 10,000 servers, ran problems low connectivity ToR switch caused problems links failed. Firehose 1.1 first custom-designed switch better connectivity ToR switch. Watchtower Saturn followed footsteps, used new, faster merchant switch chips. Jupiteruses 40 Gbps links switches deliver 1 Pbit/s bisection bandwidth. Section 6.7 describes Jupi- ter switch Edge Aggregation Spine Blocks Clos networks detail.502 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismand network single operator. Google instead used centralized control relied common configuration copied data center switches. modular hardware design robust software control allowed theseswitches used inside WSC wide area networks betweenWSCs. Google scaled bandwidth WSCs networks 100X decade,and offered 1 Pbit/s bisection bandwidth 2015. Using Energy Efficiently Inside Server Although PUE measures efficiency WSC, nothing say whatgoes inside equipment. Thus, another source electrical inefficiency isthe power supply inside server, converts input high voltage voltages chips disks use. 2007 many power supplies 60% – 80% efficient, meant greater losses inside server therewere going many steps voltage changes high-voltagelines utility tower supply low-voltage lines server. One reasonwas power supply often oversized watts themotherboard. Moreover, power supplies typically worst effi-ciency 25% load less, even though, Figure 6.3 page 441 shows, many WSC servers operate range. Computer motherboards also voltage reg- ulator modules (VRMs), relatively low efficiency well. Barroso H €olzle (2007) said goal whole server energy proportionality ; is, servers consume energy proportion amount work performed. decade later, ’ve gotten close hit ideal goal. example, best-rated SPECpower servers Chapter 1 still use 20% full power idle almost 50% full power 20% load.That represents huge progress since 2007 idle computer used 60% fullpower 70% 20% load, still room improve. Systems software designed use available resource potentially improves performance, without concern energy implications. example,operating systems use memory program data file caches, althoughmuch data likely never used. Software architects need considerenergy well performance future designs ( Carter Rajamani, 2010 ). Given background six sections, ready appreciate work Google WSC architects. 6.7 Putting Together: Google Warehouse-Scale Computer many companies WSCs competing vigorously market- place, reluctant share latest innovations public (and other). Fortunately, Google continued tradition providingdetails recent WSCs new editions book, making this6.7 Putting Together: Google Warehouse-Scale Computer ■503edition likely up-to-date public description Google WSC, representative current state-of-the-art. Power Distribution Google WSC start power distribution. Although many variations deployed, North America electric power typically goes multiple voltage changes way server, starting high-voltage lines utility tower 110,000 V. large-scale sites multiple WSCs, power delivered on-site substa- tions ( Figure 6.24 ). substations sized hundreds megawatts power. voltage reduced 10,000 35,000 V distribution WSCs site. Near buildings WSC, voltage reduced around 400 V (Figure 6.25 ) distribution rows servers data center floor. (480 V common North America, 400 V rest world; Google uses 415 V.) prevent whole WSC going offline power lost, WSCs version uninterruptible power supply (UPS), servers conventional data centers. Diesel generators connected power distribu- tion system level provide power event issue utility power. Although outages less minutes, WSCs store thousands gallons diesel site extended event. operators even make pro- visions local fuel companies continuous delivery diesel site need operate generators days weeks. Inside WSC, power delivered racks via copper bus ducts run row racks, Figure 6.26 shows. last step splits three-phase power three separate single-phase powers 240 –277 V delivered power Figure 6.24 on-site substation.504 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismcables rack. Near top rack, power converters turn 240 V AC current 48 V DC bring voltage boards use. summary, power distributed hierarchy WSC, level hierarchy corresponding distinct failure maintenance unit: whole WSC, arrays, rows, racks. Software aware hierarchy, spreads work storage topographically increase dependability. WSCs around world different distribution voltages frequencies, overall design similar. primary places improvement power Figure 6.25 image shows transformers, switch gear, generators close proximity WSC. Figure 6.26 Row servers copper bus ducts distribute 400 V servers. Although hard see, shelf right side photo. also shows cold aisle operators use service equipment.6.7 Putting Together: Google Warehouse-Scale Computer ■505efficiency voltage transformers step, highly optimized components, little opportunity left. Cooling Google WSC deliver power utility poles floor WSC, need remove heat generated using it. considerably opportunities improvement cooling infrastructure. One easiest ways improve energy efficiency simply run equipment higher temperatures air need cooled much. Google runs equipment 80+ °F (27+ °C), considerably higher traditional data centers cold need wear jacket. Airflow carefully planned equipment, even using Computational Fluid Dynamics simulation design facility. Efficient designs preserve temperature cool air reducing chances mixing hot air. example, WSCs today alternating aisles hot air cold air orienting servers opposite directions alternating rows racks hot exhaust blows alternating directions. referred hot aisles andcold aisles.Figure 6.26 shows cold aisle people use service servers, Figure 6.27 shows hot aisle. hot air hot aisle rises ducts ceiling. Figure 6.27 Hot aisle Google data center, clearly designed accommodate people.506 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismIn conventional data centers, server relies internal fans ensure suf- ficient flow cool air hot chips maintain temperature. mechanical fans one weakest components servers; example, MTBF fans 150,000 h versus 1,200,000 h disks. Google WSC, server fans work synergistically dozens giant fans room ensure airflow whole room ( Figure 6.28 ). division labor means small server fans use little power possible delivering maximum performance worst-case power ambient conditions. large fans controlled using air pressure control variable. fan speeds adjusted maintain min- imum pressure difference hot cold aisles. cool hot air, add large-scale fan-coils either end rows racks. Hot air racks delivered fan-coils via horizontal plenum inside hot aisle. (Two rows share pair cooling coils, placed cold aisle two rows.) cooled air sent via plenum ceiling wall big fans Figure 6.28 , return cooled air room containing racks. We’ll describe remove heat water cooling coils shortly, let ’s reflect architecture far. separates racks cooling capacity provided fan-coils, allows sharing cooling across two rows racks WSC. Thus, efficiently provides cooling high-power racks less low-power racks. thousands racks WSC, unlikely identical, power variability racks common, design accommodates. Cool water supplied individual fan-coils via network pipes cooling plant. Heat transferred water via forced convection cooling coils, warm water returns cooling plant. Figure 6.28 cool air blows room containing aisles servers. hot air goes large vents ceilings cooled returning fans.6.7 Putting Together: Google Warehouse-Scale Computer ■507To improve efficiency WSCs, architects try use local environment remove heat whenever possible. Evaporative cooling towers common WSCs leverage colder outside air cool water instead chilled mechanically. temperature matters called wet-bulb temperature , lowest temperature achieved evaporating water air. temperature parcel air would cooled saturation (100% relative humidity) evaporation water it, latent heat supplied parcel. Wet-bulb temperature measured blowing air bulb end thermometer water it. Warm water sprayed inside cooling tower collected pools bottom, transferring heat outside air via evaporation thereby cooling water. technique called water-side economization .Figure 6.29 shows steam rising cooling towers. alternative use cold water instead crisp air. Google ’s WSC Finland uses water-to-water heat exchanger takes frigid water Gulf Finland chill warm water inside WSC. cooling tower system uses water caused evaporation cooling towers. example, 8-MW facility might need 70,000 –200,000 gallons water per day, thus desire WSC located near ample sources water. Although cooling plant designed heat removed without artificial cooling time, mechanical chillers aid rejecting heat regions weather warm. Figure 6.29 Steam rising cooling towers transfer heat air water used cool equipment.508 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismRacks Google WSC saw Google gets power rack cools hot air exhausts rack. ’re ready explore rack itself. Figure 6.30 shows typical rack found inside Google WSC. put rack context, WSC consists multiple arrays (which Google calls clusters). Although arrays vary size, one two dozen rows row holding two three dozen racks. 20 slots shown middle rack Figure 6.30 hold servers. Depending width, four servers placed single tray. power converters near top rack turn 240 V AC current 48 V DC, run copper bus bars back rack power servers. diesel generators provide backup power whole WSC take tens seconds offer power. Instead populating large room Battery backupPower conversionNetwork switches Configurable payload bay Figure 6.30 Google rack WSC. dimensions 7 ft high, 4 ft wide, 2 ft deep (2 ×1.2 ×0.5 m). Top Rack switches indeed top rack. Next comes power converter converts 240 V AC 48 V DC servers rack using bus bar back rack. Next 20 slots (depending height server) configured various types servers placed rack. four servers placed per tray. bottom rack high-efficiency distributed modular DC uninterruptible power supply (UPS) batteries.6.7 Putting Together: Google Warehouse-Scale Computer ■509enough batteries power whole WSC several minutes —which com- mon practice early WSCs —Google puts small batteries bottom rack. UPS distributed rack, cost incurred racks aredeployed, instead paying upfront UPS capacity full WSC. Thesebatteries also better traditional batteries DCside voltage conversions, use efficient charging scheme. Inaddition, replacing 94%-efficient lead batteries 99.99%-efficient localUPS helps lower PUE. ’s efficient UPS system. comforting top rack Figure 6.30 indeed contain Top Rack switch, describe next. Networking Google WSC Google WSC network uses topology called Clos, named telecommunications expert invented ( Clos, 1953 ).Figure 6.31 shows structure Google Clos network. multistage network uses lowport-count ( “low radix ”) switches, offers fault tolerance, increases network scale bisection bandwidth. Google increases scale simply byadding stages multistage network. fault tolerance provided itsinherent redundancy, means failure link small impact overall network capacity. AsSection 6.6 describes, Google builds customer switches standard commodity switch chips uses centralized control network routing man-agement. Every switch given consistent copy current topology thenetwork, simplifies complex routing Clos network. Spine Block 1 Edge Aggregation Block 1Edge Aggregation Block 2Edge Aggregation Block NSpine Block 2Spine Block 3Spine Block 4Spine Block Server rackswith ToRswitches Figure 6.31 Clos network three logical stages containing crossbar switches: ingress, middle, egress. input ingress stage go middle stages routed output egress stage. figure, middle stages MSpine Blocks, ingress egress stages NEdge Activation Blocks. Figure 6.22 shows changes Spine Blocks Edge Aggregation Blocks many generations Clos networks Google WSCs.510 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismThe latest Google switch Jupiter, switch ’s sixth generation. Figure 6.32 shows building blocks switch, Figure 6.33 shows wiring middle blocks housed racks. cables use bundles opticalfibers. commodity switch chip Jupiter 16 /C216 crossbar using 40 Gbps links. Top Rack switch four chips, configured 4840-Gbps links servers 16 40-Gbps links network fabric, yieldingan oversubscription 3:1, better earlier generations. Moreover,this generation first time servers offered 40-Gbps links. middle blocks Figures 6.32 and6.33 consist 16 switch chips. use two stages, 256 10-Gbps links Top Rack connectivity and64 40-Gbps links connect rest network fabric spine.Each chips Top Rack switch connects eight middle blocks usingdual redundant 10-Gbps links. aggregation block connected spine block 512 40-Gbps links. spine block uses 24 switch chips offer 128 40-Gbps ports aggre-gation blocks. largest scale, use 64 aggregation blocks provide dual redundant links. maximum size, bisection bandwidth impressive 1.3 Pbit (10 15) per second. Note whole Internet might bisection bandwidth 0.2 Pbit/s; one reason Jupiter built high bisection bandwidth,but Internet not.Merchant silicon Middle block (MB)Spine block Centauri 16 ×40G32 ×40G 128 ×40G 64 aggregation blocks Aggregation block (512 ×40G 256 spine blocks) 256 ×10G down32 ×40G 64 ×40G 1×40G1×40G 2×10G ×32MB 1MB 2MB 3MB 4MB 5MB 6MB 7MB 8 Figure 6.32 Building blocks Jupiter Clos network.6.7 Putting Together: Google Warehouse-Scale Computer ■511Servers Google WSC seen power, cool, communicate, finally ready see computers actual work WSC. example server Figure 6.34 two sockets, containing 18-core Intel Haswell processor running 2.3 GHz (see Section 5.8). photo shows 16 DIMMs, servers typically deployed 256 GB total DDR3-1600 DRAM. Haswell memory hierarchy two 32 KiB L1 caches, 256 KiB L2 cache, 2.5 MiB L3 cache per core, resulting 45 MiB L3 cache. local memory bandwidth 44 GB/s latency 70 ns, intersocket bandwidth 31 GB/s latency 140 ns remote memory. Kanev et al. (2015) highlighted differences SPEC benchmark suite WSC workload. L3 cache barely needed SPEC, useful real WSC workload. baseline design single network interface card (NIC) 10 Gbit/s Ethernet link, although 40 Gbit/s NICs available. (Other cloud providers MiddleMiddle BlockBlockMiddle Block Figure 6.33 Middle blocks Jupiter switches housed racks. Four packed rack. rack hold two spine blocks.512 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismmoved 25 Gbit/s multiples thereof.) photo Figure 6.34 shows two SATA disk drives, contain 8 TB, server also configured SSD flash drives 1 TB storage. peak power baseline 150 watts. Four servers fit slot rack inFigure 6.30 . baseline node supplemented offer storage (or “diskfull ”) node. second unit contains 12 SATA disks connected server PCIe. Peak power storage node 300 watts. Conclusion previous edition, Google WSC described PUE 1.23 2011. 2017, average PUE whole Google fleet 16 sites dropped 1.12, Belgium WSC leading way 1.09 PUE. energy-saving techniques include ■Operating servers higher temperatures means air chilled 80+°F (27 °C) instead traditional 64 –71°F (18 –22°C). ■A higher target cold air temperature helps put facility often within range sustained cooling towers, energy- efficient traditional chillers. Figure 6.34 example server Google WSC. Haswell CPUs (2 sockets /C218 cores/C22 threads ¼72“virtual cores ”per machine) 2.5 MiB last level cache per core 45 MiB using DDR3-1600. use Wellsburg Platform Controller Hub TFP 150 W.6.7 Putting Together: Google Warehouse-Scale Computer ■513■Deploying WSCs temperate climates allow use evaporative cooling exclusively large portions year. ■Adding large fans entire rooms work concert small fans servers reduce energy satisfying worst-case scenarios. ■Averaging cooling per server whole racks servers deploying thecooling coils per row accommodate warmer cooler racks. ■Deploying extensive monitoring hardware software measure actual PUEversus designed PUE improves operational efficiency. ■Operating servers worst-case scenario power distributionsystem would suggest. safe since ’s statistically improbable thou- sands servers would highly busy simultaneously long monitoring system off-load work unlikely case ( Fan et al., 2007; Ranganathan et al., 2006 ). PUE improves facility operating closer fully designed capacity, efficientbecause servers cooling systems energy-proportional. Suchincreased utilization reduces demand new servers new WSCs. interesting see innovations remain improve WSC efficiency good guardians environment. hard imagine engineers might halve power cooling overhead WSC prior tothe next edition book, previous edition one. 6.8 Fallacies Pitfalls Despite WSC 15 years old, WSC architects like Google havealready uncovered many pitfalls fallacies WSCs, often hard way.As said introduction, WSC architects today ’s Seymour Crays. Fallacy Cloud computing providers losing money. AWS announced, popular question cloud computing whether profitable low prices time. Amazon Web Services grown large must recorded separately Amazon ’s quarterly reports. surprise some, AWS proved profitable portionof company. AWS $12.2 billion revenue 2016, operating margin 25%, whereas Amazon ’s retail operations operating margin less 3%. AWS consistently responsible three-fourths Amazon ’s profits. Pitfall Focusing average performance instead 99th percentile performance. AsDean Barroso (2013) observed, developers WSC services worry tail care mean. customers get terrible perfor-mance, experience drive away competitor, ’ll never return.514 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismPitfall Using wimpy processor trying improve WSC cost-performance. Amdahl's Law still applies WSC. serial work request increase request latency work runs slow server ( H€olzle, 2010; Lim et al., 2008 ). serial work increases latency, cost using wimpy processor must include software development costs optimize code return lower latency. larger number threads many slow servers also difficult schedule load balance, thus var-iability thread performance lead longer latencies. required waitfor longest task, 1-in-1000 chance bad scheduling probably issuewith 10 tasks, problematic 1000 tasks. Many smaller servers also lead lower utilization ’s clearly easier schedule fewer things. Finally, even parallel algorithms getless efficient problem partitioned finely. Google rule thumb use low-end range server class computers ( Barroso H€olzle, 2009 ). concrete example, Reddi et al. (2010) compared embedded microproces- sors (Atom) server microprocessors (Nehalem Xeon) running Bing searchengine. found latency query three times longer onAtom Xeon. Moreover, Xeon robust. load increaseson Xeon, quality service degrades gradually modestly. Atom designquickly violates quality-of-service target tries absorb additional load. Although Atom design energy-efficient, response time affects rev- enue, revenue loss likely much greater cost savings lessenergy. Energy-efficient designs cannot match response-time goals areunlikely deployed; ’ll see another version pitfall lesson next chapter (Section 7.9). behavior translates directly search quality. Given importance latency user, Figure 6.12 suggests, Bing search engine uses multiple strategies refine search results query latency yet exceeded cutoff latency. lower latency larger Xeon nodes means spend time refining search results. Thus, even Atom almost load, gaveworse answers 1% queries Xeon. normal loads, 2% answerswere worse. Kanev et al. (2015) recent, yet consistent, results. Pitfall Inconsistent measure PUE different companies. Google ’s PUE measurements start power reaches substa- tion. measure entrance WSC, skips voltage step downsthat represent 6% loss. also different results depending theseason year WSC relies atmosphere help cool system. Finally, report design goal WSC instead measuring result- ing system. conservative best PUE measurement running average past 12 months easured PUE, starting feed utility.6.8 Fallacies Pitfalls ■515Fallacy Capital costs WSC facility higher servers houses. Although quick look Figure 6.13 page 453 might lead one conclu- sion, quick glimpse ignores length amortization part fullWSC. However, facility lasts 10 –15 years, whereas servers need repurchased every 3 4 years. Using amortization times Figure 6.13 10 years 3 years, respectively, capital expenditures decade $72 million facility 3.3 /C2$67 million, $221 million, servers. Thus, capital costs servers WSC decade factor threehigher WSC facility. Pitfall Trying save power inactive low power modes versus active low power modes. Figure 6.3 page 441 shows average utilization servers 10% 50%. Given concern operational costs WSC Section 6.4 , one would think low power modes would huge help. AsChapter 1 mentions, DRAMs disks cannot accessed inactive low power modes , must returned fully active mode read write, matter low rate. pitfall time energy required return tofully active mode make inactive low power modes less attractive. Figure 6.3 shows almost servers average least 10% utilization, long periods lowactivity might expected, long periods inactivity (Lo et al., 2014). contrast, processors still run lower power modes small multiple regular rate, active low power modes much easier use. Note time move fully active mode processors also measured microsec-onds, active low power modes also address latency concerns lowpower modes. Fallacy Given improvements DRAM dependability fault tolerance WSC systems software, need spend extra ECC memory WSC. ECC adds 8 bits every 64 bits DRAM, potentially ninth DRAM costs could saved eliminating error-correcting code (ECC), especially sincemeasurements DRAM claimed failure rates 1000 –5000 FIT (failures per billion hours operation) per megabit ( Tezzaron Semiconductor, 2004 ). Schroeder et al. (2009) studied measurements DRAMs ECC pro- tection majority Google ’s WSCs, surely many hundreds thousands servers, 2.5-year period. found 15 –25 times higher FIT rates published, 25,000 –70,000 failures per megabit. Failures affected 8% DIMMs, average DIMM 4000 correctableerrors 0.2 uncorrectable errors per year. Measured server, thirdexperienced DRAM errors year, average 22,000 correctable errors 1 uncorrectable error per year. is, one-third servers, one memory error corrected every 2.5 h. Note systems used powerfulChipkill codes rather simpler SECDED codes. easier scheme hadbeen used, uncorrectable error rates would 4 –10 times higher.516 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismIn WSC parity error protection, servers would reboot memory parity error. reboot time 5 min, one-third machines would spend 20% time rebooting! behavior would lowerthe performance expensive facility 6%. Moreover, systemswould suffer many uncorrectable errors without operators notified theyoccurred. early years, Google used DRAM even parity protection. 2000, testing shipping next release search index, itstarted suggesting random documents response test queries ( Barroso H€olzle, 2009 ). reason stuck-at-zero fault DRAMs, cor- rupted new index. Google added consistency checks detect errors thefuture. WSC grew size ECC DIMMs became affordable, ECCbecame standard Google WSCs. ECC added benefit making itmuch easier find broken DIMMs repair. data suggest Fermi GPU ( Chapter 4 ) adds ECC memory predecessors ’t even parity protection. Moreover, FIT rates cast doubts efforts use Intel Atom processor WSC —because improved power efficiency —since chip set support ECC DRAM. Pitfall Coping effectively microsecond delays opposed nanosecond millisecond delays. Barroso et al. (2017) point modern computer systems make easy pro- grammers mitigate latencies nanosecond millisecond timescales (suchas cache DRAM accesses tens nanoseconds disk accesses fewmilliseconds) systems significantly lack support microsecond- scale events. Programmers get synchronous interface memory hierarchy, hardware heroic work accesses appear consistent andcoherent ( Chapter 2 ). Operating systems offer programmers similar synchronous interface disk read, many lines OS code enabling safe switching toanother process waiting disk returning originalprocess data ready. need new mechanisms cope micro-second delays memory technologies like Flash fast network interfaces like100 Gbit/s Ethernet. Fallacy Turning hardware periods low activity improves cost-performance WSC. Figure 6.14 page 454 shows cost amortizing power distribution cooling infrastructure 50% higher entire monthly power bill. Thus,although certainly would save money compact workloads turn offidle machines, even half power saved, monthly operational billwould reduced 7%. would also practical problems overcome extensive WSC monitoring infrastructure depends able poke equipment see respond. Another advantage energy proportionalityand active low power modes compatible WSC monitoringinfrastructure, allows single operator responsible 10006.8 Fallacies Pitfalls ■517servers. Note also preventive maintenance one important tasks take place idle time. conventional WSC wisdom run valuable tasks periods little activity recoup investment power distribution cooling. primeexample batch MapReduce jobs create indices search. Another exam-ple getting value meager utilization spot pricing AWS, theexample Figure 6.17 page 461 illustrates. AWS users flexible tasks run save factor four computation lettingAWS schedule tasks flexibly using spot instances, WSC would otherwise low utilization. 6.9 Concluding Remarks Inheriting title building world ’s biggest computers, computer architects WSCs designing large part future supports mobile clientand IoT devices. Many us use WSCs many times day, number times per day number people using WSCs surely increase next decade. Already six billion seven billion people planet havecell phone subscriptions. devices become Internet-ready, many morepeople around world able benefit WSCs. Moreover, economies scale uncovered WSC realized long- dreamed-of goal computing utility. Cloud computing means anyone any-where good ideas business models tap thousands servers delivertheir vision almost instantly. course, important obstacles could limit growth cloud computing around standards, privacy, rate growth Internet bandwidth, pitfalls mention Section 6.8 , foresee addressed cloud computing continue flourish. Among many attractive features cloud computing offers eco- nomic incentives conservation. Whereas hard convince cloud computingproviders turn unused equipment save energy given cost infra- structure investment, easy convince cloud computing users give idle instances since paying them, whether anything useful. Similarly, charging use encourages programmers use computation, communication, storage efficiently, difficult encouragewithout understandable pricing scheme. explicit pricing also makes itpossible researchers evaluate innovations cost-performance instead justperformance, costs easily measured believable. Finally, cloudcomputing means researchers evaluate ideas scale thousandsof computers, past large companies could afford. believe WSCs changing goals principles server design, needs mobile clients IoT changing goals principles microprocessor design. revolutionizing software industry, well.Performance per dollar performance per joule drive mobile clienthardware WSC hardware, parallelism domain-specific acceleratorsare key delivering sets goals. Architects play vital role bothhalves exciting future world.518 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level ParallelismLooking forward, end Moore ’s Law Dennard scaling ( Chapter 1 ) means single-thread performance newest processors much faster predecessors, likely stretch lifetimes serversin WSCs. Thus, money formerly spent replacing older servers insteadbe used expand cloud, could mean cloud even moreeconomically attractive next decade today. Moore ’s Law era combined innovations design operation WSCs caused theperformance-cost-energy curve WSCs improve continuously. endof glorious era, plus removal largest causes inefficiency WSCs, field likely need look innovations computer architecture chips populate WSC sustained improvement, topic thenext chapter. 6.10 Historical Perspectives References Section M.8 (available online) covers development clusters thefoundation WSC utility computing. (Readers interested learning moreshould start Barroso et al. (2013) blog postings James Hamilton http://perspectives.mvdirona.com plus talks annual Amazon Re-Invent conference.) Case Studies Exercises Parthasarathy Ranganathan Case Study 1: Total Cost Ownership Influencing Warehouse-Scale Computer Design Decisions Concepts illustrated case study ■Total Cost Ownership (TCO) ■Influence Server Cost Power Entire WSC ■Benefits Drawbacks Low-Power Servers Total cost ownership important metric measuring effectiveness warehouse-scale computer (WSC). TCO includes CAPEX OPEXdescribed Section 6.4, reflects ownership cost entire datacenterto achieve certain level performance. considering different servers,networks, storage architectures, TCO often important comparison metric used datacenter owners decide options best; however, TCO multidimensional computation takes account many different factors.The goal case study take detailed look WSCs, see differentarchitectures influence TCO, understand TCO drives operator deci-sions. case study use numbers Figures 6.13 6.14 Section6.4, assumes described WSC achieves operator ’s target level ofCase Studies Exercises Parthasarathy Ranganathan ■519performance. TCO often used compare different server options mul- tiple dimensions. exercises case study examine comparisons made context WSCs complexity involved making thedecisions. 6.1 [5/5/10] <6.2, 6.4 >In chapter, data-level parallelism discussed way WSCs achieve high performance large problems. Conceivably, evengreater performance obtained using high-end servers; however, higherperformance servers often come nonlinear price increase. a.[5]<6.4>Assuming servers 10% faster utilization, 20% expensive, CAPEX WSC? b.[5]<6.4>If servers also use 15% power, OPEX warehouse-scale computer? c.[10]<6.2, 6.4 >Given speed improvement power increase, must cost new servers comparable original cluster? ( Hint: Based TCO model, may change critical load facility.) 6.2 [5/10] <6.4, 6.6, 6.8 >To achieve lower OPEX, one appealing alternative use low-power versions servers reduce total electricity required run servers; however, similar high-end servers, low-power versions high-endcomponents also nonlinear trade-offs. a.[5]<6.4, 6.6, 6.8 >If low-power server options offered 15% lower power performance 20% expensive, good trade-off? b.[10]<6.4, 6.6, 6.8 >At cost servers become comparable original cluster? price electricity doubles? 6.3 [5/10/15] <6.4, 6.6 >Servers different operating modes offer opportuni- ties dynamically running different configurations cluster match work-load usage. Use data Figure 6.35 power/performance modes given low-power server. a.[5]<6.4, 6.6 >If server operator decided save power costs running servers medium performance, many servers would needed achievethe level performance? b.[10]<6.4, 6.6 >What CAPEX OPEX configuration? Mode Performance Power High 100% 100% Medium 75% 60% Low 59% 38% Figure 6.35 Power –performance modes low-power servers.520 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismc.[15]<6.4, 6.6 >If alternative could purchase server 20% cheaper x% slower uses y% less power, find perfor- mance –power curve provides TCO comparable baseline server. 6.4 [Discussion] <6.4>Discuss trade-offs benefits two options Exer- cise 6.3, assuming constant workload run servers. 6.5 [Discussion] <6.2, 6.4 >Unlike high-performance computing (HPC) clusters, WSCs often experience significant workload fluctuation throughout day. Dis- cuss trade-offs benefits two options Exercise 6.3, time assum-ing workload varies. 6.6 [Discussion] <6.4, 6.7 >The TCO model presented far abstracts away signif- icant amount lower level details. Discuss impact abstractions theoverall accuracy TCO model. abstractions safe make? Inwhat cases would greater detail provide significantly different answers? Case Study 2: Resource Allocation WSCs TCO Concepts illustrated case study ■Server Power Provisioning within WSC ■Time Variance Workloads ■Effects Variance TCO key challenges deploying efficient WSCs provisioning resources properly utilizing fullest capacity. problem com-plex due size WSCs well potential variance workloadsbeing run. exercises case study show different uses resourcescan affect TCO. Assume data Figures 6.13 6.14 appropriate. 6.7 [5/5/10] <6.4>One challenges provisioning WSC determining proper power load, given facility size. described chapter, nameplate power often peak value rarely encountered. a.[5]<6.4>Estimate per-server TCO changes nameplate server power 200 W cost $3000. b.[5]<6.4>Also consider higher power, cheaper server option whose power 300 W costs $2000. c.[10]<6.4>How per-server TCO change actual average power usage servers 70% nameplate power? 6.8 [15/10] <6.2, 6.4 >One assumption TCO model critical load facility fixed, amount servers fits critical load. reality, due variations server power based load, critical power used facilitycan vary given time. Operators must initially provision datacenter basedon critical power resources estimate much power used thedatacenter components.Case Studies Exercises Parthasarathy Ranganathan ■521a.[15]<6.2, 6.4 >Extend TCO model initially provision WSC based server nameplate power 300 W, also calculate actual monthly critical power used TCO assuming server averages 40% utilization andso consumes 225 W. much capacity left unused? b.[10]<6.2, 6.4 >Repeat exercise 500-W server averages 20% utilization consumes 300 W. 6.9 [10]<6.4, 6.5 >WSCs often used interactive manner end users, mentioned Section 6.5. interactive usage often leads time-of-day fluctu-ations, peaks correlating specific time periods. example, Netflixrentals peak evening periods 8 –10 p.m.; entirety time-of-day effects significant. Compare per-server TCO datacenter witha capacity match utilization 4 a.m. compared 9 p.m. 6.10 [Discussion/15] <6.4, 6.5 >Discuss options better utilize excess servers off-peak hours find ways save costs. Given interactivenature WSCs, challenges aggressively reducing powerusage? 6.11 [Discussion/25] <6.4, 6.6, 6.8 >Propose one possible way improve TCO focusing reducing server power. challenges evaluating pro- posal? Estimate TCO improvements based proposal. advantages drawbacks? Exercises 6.12 [10/10/10] <6.1, 6.2 >One important enablers WSC ample request- level parallelism, contrast instruction- thread-level parallelism. ques- tion explores implication different types parallelism computer archi-tecture system design. a.[10]<6.1>Discuss scenarios improving instruction- thread-level parallelism would provide greater benefits achievable throughrequest-level parallelism. b.[10]<6.1, 6.2 >What software design implications increasing request-level parallelism? c.[10]<6.1, 6.2 >What potential drawbacks increasing request-level parallelism? 6.13 [Discussion/15/15] <6.2, 6.3 >When cloud computing service provider receives jobs consisting multiple Virtual Machines (VMs) (e.g., MapReduce job),many scheduling options exist. VMs scheduled round-robin man-ner spread across available processors servers, consoli-dated use processors possible. Using scheduling options, ajob 24 VMs submitted 30 processors available cloud(each able run 3 VMs), round-robin would use 24 processors, con-solidated scheduling would use 8 processors. scheduler also find available processor cores different scopes: socket, server, rack, array racks.522 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelisma.[Discussion] <6.2, 6.3 >Assuming submitted jobs compute- heavy workloads, possibly different memory bandwidth requirements, pros cons round-robin versus consolidated scheduling interms power cooling costs, performance, reliability? b.[15]<6.2, 6.3 >Assuming submitted jobs I/O-heavy workloads, pros cons round-robin versus consolidated scheduling, atdifferent scopes? c.[15]<6.2, 6.3 >Assuming submitted jobs network-heavy work- loads, pros cons round-robin versus consolidated schedul-ing, different scopes? 6.14 [15/15/10/10] <6.2, 6.3 >MapReduce enables large amounts parallelism data-independent tasks run multiple nodes, often using commodity hard-ware; however, limits level parallelism. example, redun-dancy MapReduce write data blocks multiple nodes, consuming disk and,potentially, network bandwidth. Assume total dataset size 300 GB, networkbandwidth 1 Gb/s, 10 s/GB map rate, 20 s/GB reduce rate. Also assume 30% data must read remote nodes, output file written two nodes redundancy. Use Figure 6.6 parameters. a.[15]<6.2, 6.3 >Assume nodes rack. expected runtime 5 nodes? 10 nodes? 100 nodes? 1000 nodes? Discussthe bottlenecks node size. b.[15]<6.2, 6.3 >Assume 40 nodes per rack remote read/write equal chance going node. expected run-time 100 nodes? 1000 nodes? c.[10]<6.2, 6.3 >An important consideration minimizing data movement much possible. Given significant slowdown going local rackto array accesses, software must strongly optimized maximize locality.Assume 40 nodes per rack, 1000 nodes used MapRe-duce job. runtime remote accesses within rack 20% ofthe time? 50% time? 80% time? d.[10]<6.2, 6.3 >Given simple MapReduce program Section 6.2, discuss possible optimizations maximize locality workload. 6.15 [20/20/10/20/20/20] <6.2, 6.3 >WSC programmers often use data replication overcome failures software. Hadoop HDFS, example, employs three-wayreplication (one local copy, one remote copy rack, one remote copy aseparate rack), ’s worth examining replication needed. a.[20]<6.2>Let us assume Hadoop clusters relatively small, 10 nodes less, dataset sizes 10 TB less. Using failure fre- quency data Figure 6.1, kind availability 10-node Hadoop cluster one-, two-, three-way replications? b.[20]<6.2>Assuming failure data Figure 6.1 1000-node Hadoop cluster, kind availability one-, two-, three-wayreplications? reason benefits replication, scale?Case Studies Exercises Parthasarathy Ranganathan ■523c.[10]<6.2, 6.3 >The relative overhead replication varies amount data written per local compute hour. Calculate amount extra I/O traffic network traffic (within across rack) 1000-node Hadoop job thatsorts 1 PB data, intermediate results data shuffling writtento HDFS. d.[20]<6.2, 6.3 >Using Figure 6.6, calculate time overhead two- three- wayreplications.Usingthe failureratesshowninFigure6.1,comparetheexpectedexecution times replication versus two- three-way replications. e.[20]<6.2, 6.3 >Now consider database system applying replication logs, assuming transaction average accesses hard disk generates1 KB log data. Calculate time overhead two- three-way replica-tions. transaction executed in-memory takes 10 μs? f.[20]<6.2, 6.3 >Now consider database system ACID consistency requires two network round trips two-phase commitment. timeoverhead maintaining consistency well replications? 6.16 [15/15/20/Discussion] <6.1, 6.2, 6.8 >Although request-level parallelism allows many machines work single problem parallel, thereby achieving greateroverall performance, one challenges avoid dividing problem finely. look problem context service level agreements (SLAs), using smaller problem sizes greater partitioning requireincreased effort achieve target SLA. Assume SLA 95% queriesrespond 0.5 faster, parallel architecture similar MapReduce thatcan launch multiple redundant jobs achieve result. followingquestions, assume query –response time curve shown Figure 6.36 . curve shows latency response, based number queries per second, abaseline server well “small ”server uses slower processor model.Latency (s) 13 2.5 1.5 0.52 1 023456789 1 0 Queries per second, one serverBaseline Small Figure 6.36 Query –response time curve.524 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelisma.[15]<6.1, 6.2, 6.8 >How many servers required achieve SLA, assuming query-response time curve shown Figure 6.36 WSC receiving 30,000 queries per second? many “small ”servers required achieve SLA, given response-time probability curve? Looking onlyat server costs, much cheaper must “small ”servers normal servers achieve cost advantage target SLA? b.[15]<6.1, 6.2, 6.8 >Often, “small ”servers also less reliable due cheaper components. Using numbers Figure 6.1, assume number ofevents due flaky machines bad memories increases 30%. many“small ”servers required now? much cheaper must servers standard servers? c.[20]<6.1, 6.2, 6.8 >Now assume batch processing environment. “small ” servers provide 30% overall performance regular servers. Stillassuming reliability numbers Exercise 6.15 part (b), many “small ” nodes required provide expected throughput 2400-node array standard servers, assuming perfect linear scaling performance tonode size average task length 10 min per node? scalingis 85%? 60%? d.[Discussion] <6.1, 6.2, 6.8 >Often scaling linear function, instead logarithmic function. natural response may instead purchaselarger nodes computational power per node minimize thearray size. Discuss trade-offs architecture. 6.17 [10/10/15/Discussion] <6.3, 6.8 >One trend high-end servers toward inclusion nonvolatile flash memory memory hierarchy, either throughsolid-state disks (SSDs) PCI Express-attached cards. Typical SSDs band-width 250 MB/s latency 75 μs, whereas PCIe cards bandwidth 600 MB/s latency 35 μs. a.[10] Take Figure 6.7 include points local server hierarchy. Assuming identical performance scaling factors like DRAM accessed different hierarchy levels, flash memory devices compare accessed across rack? Across array? b.[10] Discuss software-based optimizations utilize new level memory hierarchy. c.[15] discussed “Fallacies Pitfalls ”(Section 6.8), replacing disks SSDs necessarily cost-effective strategy. Consider WSC operatorthat uses provide cloud services. Discuss scenarios using SSDsor flash memory would make sense. d.[Discussion] Recently, vendors discussed new memory tech- nologies much faster flash. example, look spec-ifications Intel 3D X-point memory discuss would factor inFigure 6.7.Case Studies Exercises Parthasarathy Ranganathan ■5256.18 [20/20/Discussion] <6.3>Memory Hierarchy : Caching heavily used WSC designs reduce latency, multiple caching options satisfy varying access patterns requirements. a.[20] Let ’s consider design options streaming rich media Web (e.g., Netflix). First need estimate number videos, number encode formats per video, concurrent viewing users. Assume streaming video pro-vider 12,000 titles online streaming, title least fourencode formats (at 500, 1000, 1600, 2200 kbps). Let ’s also assume 100,000 concurrent viewers entire site, average video 75 minlong (accounting 30-min shows 2-h videos). Estimate totalstorage capacity, I/O network bandwidths, video-streaming-relatedcomputation requirements. b.[20] access patterns reference locality characteristics per user, per video, across videos? ( Hint: Random versus sequential, good versus poor temporal spatial locality, relatively small versus large working set size.) c.[Discussion] movie storage options exist using DRAM, SSD, hard drives? Compare performance TCO. Would new memory technol-ogies like Problem 6.17(d) useful? 6.19 [Discussion/20/Discussion/Discussion] <6.3>Consider social networking web- site 100 million active users posting updates (in text andpictures) well browsing interacting updates social networks.To provide low latency, Facebook many websites use memcached acaching layer backend storage/database tiers. Assume giventime average user browsing megabytes content, given day theaverage user uploads megabytes content. a.[20] social networking website discussed here, much DRAM needed host working set? Using servers 96 GB DRAM, esti-mate many local versus remote memory accesses needed generate user’s home page? b.[Discussion] consider two candidate memcached server designs, one using conventional Xeon processors using smaller cores, Atom processors. Given memcached requires large physical memorybut low CPU utilization, pros cons two designs? c.[Discussion] Today ’s tight coupling memory modules processors often requires increase CPU socket count order provide large memorysupport. List designs provide large physical memory without propor-tionally increasing number sockets server. Compare basedon performance, power, costs, reliability. d.[Discussion] user ’s information stored memcached storage servers, servers physically hosted different ways.Discuss pros cons following server layout WSC: (1)526 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismmemcached collocated storage server, (2) memcached storage servers separate nodes rack, (3) memcached servers racks storage servers collocated separate racks. 6.20 [5/5/10/10/Discussion/Discussion/Discussion] <6.3, 6.5, 6.6 >Datacenter Net- working : MapReduce WSC powerful combination tackle large-scale data processing. problem, assume sort one petabyte (1 PB) ofrecords 6 h using 4000 servers 48,000 hard drives (Google discussed doingthis 2008). a.[5] Derivedisk bandwidth Figure 6.6 associatedtext. manyseconds take read data main memory write sorted results back? b.[5] Assuming server two 1 Gb/s Ethernet network interface cards (NICs) WSC switch infrastructure oversubscribed factor 4,how many seconds take shuffle entire dataset across 4000 servers? c.[10] Assuming network transfer performance bottleneck petabyte sort, estimate oversubscription ratio Google datacenter? d.[10] let ’s examine benefits 10 Gb/s Ethernet without oversubscription —for example, using 48-port 10 Gb/s Ethernet (this used 2010 Indy sort benchmark winner TritonSort). long take toshuffle 1 PB data? e.[Discussion] Compare two approaches here: (1) massive scale-out approach high network oversubscription ratio, (2) relatively small-scale system high-bandwidth network. potential bottle-necks? advantages disadvantages, terms scalability TCO? f.[Discussion] Sort many important scientific computing workloads communication-heavy, many workloads not. List three example workloads benefit high-speed networking. EC2 instanceswould recommend use two classes workloads? g.[Discussion] Look various benchmarks www.sortbenchmark.org recent winners category. results match insights fromthe discussion part (e) above? cloud instance used mostrecent winner CloudSort compare answer part (f) above? 6.21 [10/25/Discussion] <6.4, 6.6 >Because massive scale WSCs, important properly allocate network resources based workloads areexpected run. Different allocations significant impacts theperformance total cost ownership. a.[10] Using numbers spreadsheet detailed Figure 6.13, oversubscription ratio access-layer switch? impact TCOif oversubscription ratio cut half? doubled? b.[25] Reducing oversubscription ratio potentially improve performance workload network-limited. Assume MapReduce job uses 120Case Studies Exercises Parthasarathy Ranganathan ■527servers reads 5 TB data. Assume ratio read/intermediate/out- put data Figure 6.2, Sep-09, use Figure 6.6 define bandwidths memory hierarchy. reading data, assume 50% data read fromremote disks; that, 80% read within rack 20% read fromwithin array. intermediate data output data, assume 30% thedata uses remote disks; that, 90% within rack 10% within thearray. overall performance improvement reducing over-subscription ratio half? performance oversubscription ratiois doubled? Calculate TCO case. c.[Discussion] seeing trend cores per system. also see- ing increasing adoption optical communication (with potentially higherbandwidth improved energy efficiency). think emerging technology trends affect design future WSCs? 6.22 [5/15/15/20/25/Discussion] <6.5>Realizing Capability Cloud : Imagine site operation infrastructure manager Alexa.com top site considering using Amazon Web Services (AWS). factors need toconsider determining whether migrate AWS? services instancetypes could use, much cost could save? use Alexa andsite traffic information (e.g., Wikipedia provides page view stats) estimate theamount traffic received top site, take concrete examples theWeb, following example: http://2bits.com/sites/2bits.com/files/drupal- single-server-2.8-million-page-views-a-day.pdf .The slides describe Alexa #3400 site receives 2.8 million page views per day, using single server. server two quad-core Xeon 2.5 GHz processors 8 GB DRAM andthree 15 K RPM SAS hard drives RAID1 configuration, costs about$400 per month. site uses caching heavily, CPU utilization rangesfrom 50% 250% (roughly 0.5 –2.5 cores busy). a.[5] Looking available EC2 instances (http://aws.amazon.com/ec2/ instance-types/ ), instance types match exceed current server configuration? b.[15] Looking EC2 pricing information (http://aws.amazon.com/ec2/ pricing/ ), select cost-efficient EC2 instances (combinations allowed) host site AWS. monthly cost EC2? c.[15] add costs IP address network traffic equation, suppose site transfers 100 GB/day Internet. themonthly cost site now? d.[20] AWS also offers micro instance free 1 year new customers 15 GB bandwidth traffic going across AWS. Based yourestimation peak average traffic department Web server, host free AWS?528 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelisme.[25] Based service characteristics, much larger site like Netflix.com migrates streaming encoding infrastructure AWS, AWS services could used Netflix purposes? f.[Discussion] Look similar offerings cloud providers (Google, Microsoft, Alibaba, etc.). answers parts (a) –(e) change? g.[Discussion] “Serverless computing ”allows build run higher-level applications services without thinking specific servers. Examples include AWS Lambda, Google Cloud Functions, Microsoft Azure Functions,etc. Continuing wear site operation infrastructure manager hat,when would consider serverless computing? 6.23 [Discussion/Discussion/20/20/Discussion] <6.4, 6.8 >Figure 6.12 shows impact user perceived response time revenue, motivates need toachieve high-throughput maintaining low latency. a.[Discussion] Taking Web search example, possible ways reducing query latency? b.[Discussion] monitoring statistics collect help understand time spent? plan implement monitoring tool? c.[20] Assuming number disk accesses per query follows normal dis- tribution, average 2 standard deviation 3, kind diskaccess latency needed satisfy latency SLA 0.1 95% queries? d.[20] In-memory caching reduce frequencies long-latency events (e.g., accessing hard drives). Assuming steady-state hit rate 40%, hit latency of0.05 s, miss latency 0.2 s, caching help meet latency SLA 0.1 sfor 95% queries? e.[Discussion] cached content become stale even inconsistent? often happen? detect invalidate content? 6.24 [15/15/20/Discussion] <6.4, 6.6 >The efficiency typical power supply units (PSUs) varies load changes; example, PSU efficiency about80% 40% load (e.g., output 40 W 100-W PSU), 75% loadis 20% 40%, 65% load 20%. a.[15] Assume power-proportional server whose actual power proportional CPU utilization, utilization curve shown Figure 6.3. average PSU efficiency? b.[15] Suppose server employs 2 Nredundancy PSUs (i.e., doubles number PSUs) ensure stable power one PSU fails. aver- age PSU efficiency? c.[20] Blade server vendors use shared pool PSUs provide redun- dancy also dynamically match number PSUs server ’s actual power consumption. HP c7000 enclosure uses six PSUs total of16 servers. case, average PSU efficiency enclosure ofserver utilization curve?Case Studies Exercises Parthasarathy Ranganathan ■529d.[Discussion] Consider impact different efficiency numbers con- text broader TCO discussions Figures 6.13 6.14: dif- ferent design impact total TCO? Given these, would optimizedesigns future warehouse-scale computers? 6.25 [5/Discussion/10/15/Discussion/Discussion/Discussion] <6.4, 6.8 >Power stranding term used refer power capacity provisioned used datacenter. Consider data presented Figure 6.37 [Fan, Weber, Barroso, 2007] different groups machines. (Note paper callsa“cluster ”is referred “array”in chapter.) a.[5] stranded power (1) rack level, (2) power distribution unit level, (3) array (cluster) level? trends oversub-scription power capacity larger groups machines? b.[Discussion] think causes differences power stranding different groups machines? c.[10] Consider array-level collection machines total machines never use 72% aggregate power (this sometimes also referredto ratio peak-of-sum sum-of-peaks usage). Using costmodel case study, compute cost savings comparing datacenter provisioned peak capacity one provisioned actual use. d.[15] Assume datacenter designer chose include additional servers array level take advantage stranded power. Using example config- uration assumptions part (a), compute many servers included warehouse-scale computer total power provisioning. e.[Discussion] needed make optimization part (d) work real- world deployment? ( Hint: Think needs happen cap power rare case servers array used peak power.) 1 1 0.99 0.98 0.97 0.96 0.950.8 0.6 0.4 0.2 0 0.4 0.5 0.6 7 .08 .0 0.9 1 59.0 58.09 .0 8.057.07.056.0 Normalized power Normalized power (a) Full distribution (b) Zoomed viewCDF CDF 1Rack PDU ClusterRack PDU Cluster Figure 6.37 Cumulative distribution function (CDF) real datacenter.530 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismf.[Discussion] Two kinds policies envisioned manage power caps [Ranganathan et al., 2006]: (1) preemptive policies power budgets predetermined ( “don’t assume use power; ask do!”) (2) reactive policies power budgets throttled event power budget violation ( “use much power needed told can’t!”). Discuss trade-offs approaches would use type. g.[Discussion] happens total stranded power systems become energy-proportional (assume workloads similar Figure 6.4)? 6.26 [5/20/Discussion] <6.4, 6.7 >Section 6.7 discussed use per-server battery sources Google design. Let us examine consequences design. a.[5] Assume use battery mini-server-level UPS 99.99% effi- cient eliminates need facility-wide UPS 92% efficient.Assume substation switching 99.7% efficient efficiency forthe PDU, step-down stages, electrical breakers 98%, 98%, and99%, respectively. Calculate overall power infrastructure efficiency improvements using per-server battery backup. b.[20] Assume UPS 10% cost equipment. Using rest assumptions cost model case study, break-even point costs battery (as fraction cost single server) atwhich total cost ownership battery-based solution better thatfor facility-wide UPS? c.[Discussion] trade-offs two approaches? particular, think manageability failure model changeacross two different designs? 6.27 [5/5/Discussion] <6.4>For exercise, consider simplified equation total operational power WSC follows: Total operational power ¼1 + Cooling inefficiency multiplierðÞ *IT equipment power : a.[5] Assume 8 MW datacenter 80% power usage, electricity costs $0.10 per kilowatt-hour, cooling-inefficiency multiplier 0.8. Compare cost savings (1) optimization improves cooling efficiency by20%, (2) optimization improves energy efficiency ITequipment 20%. b.[5] percentage improvement equipment energy efficiency needed match cost savings 20% improvement cooling efficiency? c.[Discussion/10] conclusions draw relative importance optimizations focus server energy efficiency cooling energyefficiency? 6.28 [5/5/Discussion] <6.4>As discussed chapter, cooling equipment WSCs consume lot energy. Cooling costs loweredCase Studies Exercises Parthasarathy Ranganathan ■531by proactively managing temperature. Temperature-aware workload placement one optimization proposed manage temperature reduce cooling costs. idea identify cooling profile given room map hottersystems cooler spots, WSC level requirements overallcooling reduced. a.[5] coefficient performance (COP) computer room air conditioning unit (CRAC) measure efficiency, defined ratio heatremoved (Q) amount work necessary (W) remove heat. TheCOP CRAC unit increases temperature air CRAC unitpushes plenum. air returns CRAC unit 20 °C remove 10 KW heat COP 1.9, much energy expend CRACunit? takes COP 3.1 cool volume air, air returned 25 °C, much energy expend CRAC unit? b.[5] Assume workload distribution algorithm able match hot work- loads well cool spots allow computer room air-conditioning (CRAC) unit run higher temperature improve cooling efficiencieslike exercise above. power savings two casesdescribed above? c.[Discussion] Given scale WSC systems, power management com- plex, multifaceted problem. Optimizations improve energy efficiency beimplemented hardware software, system level, clusterlevel equipment cooling equipment, etc. important con-sider interactions designing overall energy-efficiency solution forthe WSC. Consider consolidation algorithm looks server utilization consolidates different workload classes server increase server uti- lization (this potentially server operating higher energy effi-ciency system energy-proportional). would optimizationinteract concurrent algorithm tried use different power states (seeACPI, Advanced Configuration Power Interface, examples)? Whatother examples think multiple optimizations potentiallyconflict one another WSC? would solve problem? 6.29 [5/10/15/20] <6.2, 6.6 >Energy proportionality (sometimes also referred energy scale-down) attribute system consume power idle,but importantly gradually consume power proportion activity level work done. exercise, examine sensitivity energy consumption different energy proportionality models. exercises below,unless otherwise mentioned, use data Figure 6.4 default. a.[5] simple way reason energy proportionality assume linearity activity power usage. Using peak power idle powerdata Figure 6.4 linear interpolation, plot energy-efficiency trendsacross varying utilizations. (Energy efficiency expressed performance perwatt.) happens idle power (at 0% activity) half assumed inFigure 6.4? happens idle power zero?532 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismb.[10] Plot energy-efficiency trends across varying activities, use data column 3 Figure 6.4 power variation. Plot energy efficiency assuming idle power (alone) half assumed Figure6.4. Compare plots linear model previous exercise. Whatconclusions draw consequences focusing purely idlepower alone? c.[15] Assume system utilization mix column 7 Figure 6.4. simplic- ity, assume discrete distribution across 1000 servers, 109 servers at0% utilization, 80 servers 10% utilization, etc. Compute total performanceand total energy workload mix using assumptions part (a) part(b). d.[20] One could potentially design system sublinear power versus load relationship region load levels 0% 50%. wouldhave energy-efficiency curve peaks lower utilizations (at expense higher utilizations). Create new version column 3 Figure 6.4 shows energy-efficiency curve. Assume system utilization mix incolumn 7 Figure 6.4. simplicity, assume discrete distribution across1000 servers, 109 servers 0% utilization, 80 servers 10% utilizations,etc. Compute total performance total energy workload mix. 6.30 [15/20/20] <6.2, 6.6 >This exercise illustrates interactions energy propor- tionality models optimizations server consolidation energy-efficient server designs. Consider scenarios shown Figures 6.38 and6.39. a.[15] Consider two servers power distributions shown Figure 6.38: case (the server considered Figure 6.4) case B (a less energy-proportional butmore energy-efficient server case A). Assume system utilization mix incolumn 7 Figure 6.4. simplicity, assume discrete distribution across 1000 servers, 109 servers 0% utilization, 80 servers 10% utilizations, etc., Activity (%) 0 10 20 30 40 50 60 70 80 90 100 Power, case (W) 181 308 351 382 416 451 490 533 576 617 662 Power, case B (W) 250 275 325 340 395 405 415 425 440 445 450 Figure 6.38 Power distribution two servers. Activity (%) 0 10 20 30 40 50 60 70 80 90 100 No. servers, case B 109 80 153 246 191 115 51 21 15 12 8 No. servers, case C 504 6 8 11 26 57 95 123 76 40 54 Figure 6.39 Utilization distributions across cluster, without consolidation.Case Studies Exercises Parthasarathy Ranganathan ■533shown row 1 Figure 6.39. Assume performance variation based column 2 Figure 6.4. Compare total performance total energy workload mix two server types. b.[20] Consider cluster 1000 servers data similar data shown Figure 6.4 (and summarized first rows Figures 6.38 6.39). total performance total energy workload mix assump-tions? assume able consolidate workloads model thedistribution shown case C (second row Figure 6.39). totalperformance total energy now? total energy compare witha system linear energy-proportional model idle power zerowatts peak power 662 W? c.[20] Repeat part (b), power model server B, compare results part (a). 6.31 [10/Discussion] <6.2, 6.4, 6.6 >System-Level Energy Proportionality Trends : Consider following breakdowns power consumption server: CPU, 50%; memory, 23%; disks, 11%; networking/other, 16% CPU, 33%; memory, 30%; disks, 10%; networking/other, 27% a.[10] Assume dynamic power range 3.0 /C2for CPU (i.e., power con- sumption CPU idle one-third power consumption peak).Assume dynamic range memory systems, disks, network-ing/other categories are, respectively, 2.0 /C2, 1.3/C2, 1.2 /C2. overall dynamic range total system two cases? b.[Discussion/10] learn results part (a)? would achieve better energy proportionality system level? ( Hint: Energy propor- tionality system level cannot achieved CPU optimizations alone, instead requires improvement across components.) 6.32 [30]<6.4>Pitt Turner IV et al. [2008] presented good overview datacenter tier classifications. Tier classifications define site infrastructure performance. simplicity, consider key differences shown Figure 6.40 (adapted Pitt Turner IV et al. [2008]). Using TCO model case study guiding frame-work, compare cost implications different tiers shown. 6.33 [Discussion] <6.4>Based observations Figures 6.12 6.13, say qualitatively trade-offs revenue loss downtime andcosts incurred uptime? 6.34 [15/Discussion] <6.4>Some recent studies defined metric called TPUE, stands “true PUE ”or“total PUE. ”TPUE defined PUE *SPUE. PUE, power utilization effectiveness, defined Section 6.4 ratio ofthe total facility power total equipment power. SPUE, server PUE,is new metric analogous PUE, instead applied computing equipment. SPUE defined ratio total server input power useful power, useful power defined power consumed electronic components534 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismdirectly involved computation: motherboard, disks, CPUs, DRAM, I/O cards, on. words, SPUE metric captures inefficiencies associated withthe power supplies, voltage regulators, fans housed server. a.[15]<6.4>Consider design uses higher supply temperature computer room air conditioning (CRAC) units. efficiency CRACunit approximately quadratic function temperature, designtherefore improves overall PUE, let ’s assume 7%. (Assume baseline PUE 1.7.) However, higher temperature server level triggers theon-board fan controller operate fan much higher speeds. fan poweris cubic function speed, increased fan speed leads degradation SPUE. Assume fan power model: Fan power ¼284*ns*ns*ns/C075*ns*ns, nsis normalized fan speed=fan speed rpm/18,000 baseline server power 350 W. Compute SPUE fan speed increases (1)10,000 –12,500 rpm (2) 10,000 –18,000 rpm. Compare PUE TPUE cases. (For simplicity, ignore inefficiencies power deliv- ery SPUE model.) b.[Discussion] Part (a) illustrates that, PUE excellent metric capture overhead facility, capture inefficiencies within equip- ment itself. identify another design changes TPUE arepotentially lower changes traditional PUE? ( Hint: See Exercise 6.26.) 6.35 [Discussion/30/Discussion] <6.2>Two benchmarks provide good starting point energy-efficiency accounting servers —the SPECpower_ssj2008 benchmark (available http://www.spec.org/power_ssj2008/ ) JouleSort metric (available http://sortbenchmark.org/ ). a.[Discussion] <6.2>Look descriptions two benchmarks. similar? different? would improve thesebenchmarks better address goal improving WSC energy efficiency?Tier 1 Single path power cooling distributions, without redundant components99.0% Tier 2 (N + 1) redundancy = two power cooling distribution paths99.7% Tier 3 (N + 2) redundancy = three power cooling distribution paths uptime even maintenance99.98% Tier 4 Two active power cooling distribution paths, redundant components path, tolerate single equipment failure without impacting load99.995% Figure 6.40 Overview data center tier classifications. (Adapted Pitt Turner IV et al. [2008].).Case Studies Exercises Parthasarathy Ranganathan ■535b.[30]<6.2>JouleSort measures total system energy perform out-of-core sort attempts derive metric enables comparison systems rang- ing embedded devices supercomputers. Look description theJouleSort metric http://sortbenchmark.org .Download publicly available version sort algorithm run different classes machines —al p p , aP C ,am b l ep h n e ,e c . —or different configurations. learn JouleSort ratings different setups? c.[Discussion] <6.2>Consider system best JouleSort rating experiments above. would improve energy efficiency? Forexample, try rewriting sort code improve JouleSort rating. doesrunning sort cloud energy efficiency? 6.36 [10/10/15] <6.1, 6.2 >Figure 6.1 listing outages array servers. dealing large scale WSCs, important balance clusterdesign software architectures achieve required uptime without incurring significant costs. question explores implications achieving availability hardware only. a.[10]<6.1, 6.2 >Assuming operator wishes achieve 95% availability server hardware improvements alone, many events typewould reduced? now, assume individual server crashesare completely handled redundant machines. b.[10]<6.1, 6.2 >How answer part (a) change individual server crashes handled redundancy 50% time? 20% time? None ofthe time? c.[15]<6.1, 6.2 >Discuss importance software redundancy achieving high level availability. WSC operator considered buying machines thatwere cheaper 10% less reliable, implications would soft-ware architecture? challenges associated software redundancy? d.[Discussion] <6.1>Discuss importance eventual consistency warehouse-scale computers scale. 6.37 [15]<6.1, 6.8 >Look current prices standard DDR4 DRAM versus DDR4 DRAM error-correcting code (ECC). increase priceper bit achieving higher reliability ECC provides? Using DRAMprices alone, data provided Section 6.8, uptime per dollar WSC non-ECC versus ECC DRAM? 6.38 [5/Discussion] <6.1, 6.8 >WSC Reliability Manageability Concerns : a.[5] Consider cluster servers costing $2000 each. Assuming annual fail- ure rate 5%, average hour service time per repair, replacementparts requiring 10% system cost per failure, annual mainte- nance cost per server? Assume hourly rate $100 per hour service technician.536 ■Chapter Six Warehouse-Scale Computers Exploit Request-Level Data-Level Parallelismb.[Discussion] Comment differences manageability model versus traditional enterprise datacenter large number small- medium-sized applications running dedicated hardwareinfrastructure. c.[Discussion] Discuss trade-offs heterogeneous machines warehouse-scale computer. 6.39 [Discussion] <6.4, 6.7, 6.8 >The OpenCompute project www.opencompute. orgprovides community design share efficient designs warehouse- scale computers. Look recently proposed designs. theycompare design trade-offs discussed chapter? designsdiffer Google case study discussed Section 6.7? 6.40 [15/15] <6.3, 6.4, 6.5 >Assume MapReduce job Page #438 Sec- tion 6.2 executing task 2 ^40 bytes input data, 2 ^37 bytes intermediate data, 2 ^30 bytes output data. job entirely memory/storage bound, performance quantified DRAM/Disk bandwidth Figure 6.6. a.How much job cost run m4.16xlarge m4.large Figure 6.15? EC2 instance provides better performance? EC2 instance pro-vides better cost? b.How much would job cost SSD added system, m3. medium? performance cost m3.medium compare withthe best instance part (a) above? 6.41 <6.1, 6.4 >[5/5/10/Discussion] Imagine created web service runs well (responds within 100 ms latency) 99% time, performance issues 1% time (maybe CPU went lower power state response took 1000 ms, etc.). a.[5] service grows popular, 100 servers com- putation touch servers handle user request. thepercentage time query likely slow response time, across100 servers? b.[5] Instead “two nines ”(99%) single server latency SLA, many “nines ” need single server latency SLA cluster latencySLA bad latencies 10% time lower? c.[10] answers parts (a) (b) change 2000 servers? d.[Discussion] Section 6.4 (page 452) discusses “tail-tolerant ”designs. kind design optimizations would need make web service(Hint: Look “Tail Scale ”paper Dean Barroso [2013]).Case Studies Exercises Parthasarathy Ranganathan ■5377.1 Introduction 540 7.2 Guidelines DSAs 543 7.3 Example Domain: Deep Neural Networks 544 7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator 557 7.5 Microsoft Catapult, Flexible Data Center Accelerator 567 7.6 Intel Crest, Data Center Accelerator Training 579 7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit 579 7.8 Cross-Cutting Issues 592 7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators 595 7.10 Fallacies Pitfalls 602 7.11 Concluding Remarks 604 7.12 Historical Perspectives References 606 Case Studies Exercises Cliff Young 6067 Domain-Specific Architectures Moore ’s Law ’t continue forever …We another 10 20 years reach fundamental limit Gordon Moore, Intel Co-Founder (2005) Computer Architecture. https://doi.org/10.1016/B978-0-12-811905-1.00007-9 ©2019 Elsevier Inc. rights reserved.7.1 Introduction Gordon Moore predicted amazing growth transistors per chip 1965, opening chapter quote shows also predicted demise50 years later. evidence, Figure 7.1 shows even company founded —which decades proudly used Moore ’s Law guideline capital investment —is slowing development new semiconductor processes. semiconductor boom time, architects rode Moore ’s Law create novel mechanisms could turn cornucopia transistors higher perfor- mance. resources five-stage pipeline, 32-bit RISC processor —which needed little 25,000 transistors 1980s —grew factor 100,000 enable features accelerated general-purpose code general-purposeprocessors, earlier chapters document: ■1st-level, 2nd-level, 3rd-level, even 4th-level caches ■512-bit SIMD floating-point units ■15+ stage pipelines ■Branch prediction ■Out-of-order execution ■Speculative prefetching ■Multithreading ■Multiprocessing sophisticated architectures targeted million-line programs written effi- cient languages like C++. Architects treated code black boxes, generally 2000 2002 2004 2006 2008Intel process (nm) 2010 2012 2014 2016142232456590130180 10100 Figure 7.1 Time new Intel semiconductor process technology measured nm. y-axis log scale. Note time stretched previously 24 months per new process step 30 months since 2010.540 ■Chapter Seven Domain-Specific Architectureswithout understanding either internal structure programs even trying do. Benchmark programs like SPEC2017 artifacts measure accelerate. Compiler writers people thehardware-software interface, dates back RISC revolution the1980s, limited understanding high-level application behavior;that’s compilers cannot even bridge semantic gap C C++ architecture GPUs. AsChapter 1 described, Dennard scaling ended much earlier Moore ’s Law. Thus transistors switching means power. energy budget increasing, ’ve already replaced single inefficient processor multiple efficient cores. Hence, nothing left sleeves continuemajor improvements cost-performance energy efficiency general-purpose architectures. energy budget limited (because electromi-gration, mechanical thermal limits chips), want higher performance(higher operations/second), need lower energy per operation. Figure 7.2 another take relative energy costs memory logic mentioned Chapter 1 , time calculated overhead arithmetic instruc- tion. Given overhead, minor twists existing cores may get us 10% improve- ments, want order-of-magnitude improvements offeringprogrammability, need increase number arithmetic operations perinstruction one hundreds. achieve level efficiency, need adrastic change computer architecture general-purpose cores domain- specific architectures (DSAs). Thus, field switched uniprocessors multiprocessors past decade necessity, desperation reason architects working DSAs. new normal computer consist standard processors run conventional large programs operating systems along domain-specific processors narrow range tasks, themextremely well. Thus computers much heterogeneous thanthe homogeneous multicore chips past. RISC instruction Overhead 125 pJ ALU 32-bit addition 7 pJ + 8-bit addition 0.2–0.5 pJ +SP floating point 15–20 pJ +D-$ Load/Store Overhead 150 pJ ALU Figure 7.2 Energy costs picoJoules 90 nm process fetch instructions access data cache compared energy cost arithmetic operations(Qadeer et al., 2015 ).7.1 Introduction ■541Part argument preceding architecture innovations past decades leveraged Moore ’s Law (caches, out-of-order execution, etc.) may good match domains —especially terms energy usage —so resources recycled make chip better match domain. example, caches excellent general-purpose architectures,but necessarily DSAs; applications easily predictable memoryaccess patterns huge data sets like video little data reuse, multilevelcaches overkill, hording area energy could put better use. There-fore promise DSAs improved silicon efficiency better energy efficiency, latter typically important attribute today. Architects probably ’t create DSA large C++ program like com- piler found SPEC2017 benchmark. Domain-specific algorithms arealmost always small compute-intensive kernels larger systems, asfor object recognition speech understanding. DSAs focus subsetand plan run entire program. addition, changing code bench-mark longer breaking rules; perfectly valid source speedup forDSAs. Consequently, going make useful contributions, architects interested DSA must shed blinders learn application domains algorithms. addition needing expand areas expertise, challenge domain-specific architects find target whose demand large enough jus-tify allocating dedicated silicon SOC even custom chip. nonrecur- ring engineering (NRE) costs custom chip supporting software amortized number chips manufactured, unlikely make eco-nomic sense need 1000 chips. One way accommodate smaller volume applications use reconfigurable chips FPGAs lower NRE custom chips andbecause several different applications may able reuse reconfigurable hard-ware amortize costs (see Section 7.5 ). However, since hardware less efficient custom chips, gains FPGAs modest. Another DSA challenge port software it. Familiar programming environments like C++ programming language compiler rarely theright vehicles DSA. rest chapter provides five guidelines design DSAs tutorial example domain, deep neural networks (DNNs ). chose DNNs revolutionizing many areas computing today.Unlike hardware targets, DNNs applicable wide range problems,so reuse DNN-specific architecture solutions speech, vision, lan-guage, translation, search ranking, many areas. follow four examples DSAs: two custom chips data center accelerate DNNs, FPGA data center accelerates many domains, image-processing unit designed personal mobile devices (PMDs ). compare cost-performance DSAs along CPUs GPUs usingDNN benchmarks, conclude prediction upcoming renaissance forcomputer architecture.542 ■Chapter Seven Domain-Specific Architectures7.2 Guidelines DSAs five principles generally guided designs four DSAs ’ll see Sections 7.4 –7.7. five guidelines lead increased area energy efficiency, also provide two valuable bonus effects. First, lead tosimpler designs, reduce cost NRE DSAs (see fallacy inSection 7.10 ). Second, user-facing applications commonplace DSAs, accelerators follow principles better match 99th- percentile response-time deadlines time-varying performance optimiza- tions traditional processors, see Section 7.9 .Figure 7.3 shows four DSAs followed guidelines. 1.Use dedicated memories minimize distance data moved . many levels caches general-purpose microprocessors use great dealof area energy trying move data optimally program. example, atwo-way set associative cache uses 2.5 times much energy equivalent software-controlled scratchpad memory. definition, compiler writers programmers DSAs understand domain, need forthe hardware try move data them. Instead, data movement reducedwith software-controlled memories dedicated tailored specificfunctions within domain. 2.Invest resources saved dropping advanced microarchitectural optimi- zations arithmetic units bigger memories . AsSection 7.1 describes, architects turned bounty Moore ’s Law resource-intensive optimizations CPUs GPUs (out-of-order execu-tion, multithreading, multiprocessing, prefetching, address coalescing, etc.). Guideline TPU Catapult Crest Pixel Visual Core Design target Data center ASIC Data center FPGA Data center ASIC PMD ASIC/SOC IP 1. Dedicated memories24 MiB Unified Buffer, 4 MiB AccumulatorsVaries N.A. Per core: 128 KiB line buffer, 64 KiB P.E.memory 2. Larger arithmetic unit65,536 Multiply- accumulatorsVaries N.A. Per core: 256 Multiply- accumulators (512 ALUs) 3. Easy parallelismSingle-threaded, SIMD, in-orderSIMD, MISD N.A. MPMD, SIMD, VLIW 4. Smaller data size8-Bit, 16-bit integer 8-Bit, 16-bit integer 32-bit Fl. Pt.21-bit Fl. Pt. 8-bit, 16-bit, 32-bit integer 5. Domain- specific lang.TensorFlow Verilog TensorFlow Halide/TensorFlow Figure 7.3 four DSAs chapter closely followed five guidelines. Pixel Visual Core typ- ically 2 –16 cores. first implementation Pixel Visual Core support 8-bit arithmetic.7.2 Guidelines DSAs ■543Given superior understanding execution programs nar- rower domains, resources much better spent processing units larger on-chip memory. 3.Use easiest form parallelism matches domain . Target domains DSAs almost always inherent parallelism. key deci- sions DSA take advantage parallelism expose itto software. Design DSA around natural granularity parallelismof domain expose parallelism simply programming model. Forexample, respect data-level parallelism, SIMD works domain, ’s certainly easier programmer compiler writer MIMD. Simi-larly, VLIW express instruction-level parallelism domain, thedesign smaller energy-efficient out-of-order execution. 4.Reduce data size type simplest needed domain . see, applications many domains typically memory-bound, increase effective memory bandwidth on-chip memory utiliza- tion using narrower data types. Narrower simpler data also let ’s pack arithmetic units chip area. 5.Use domain-specific programming language port code DSA . AsSection 7.1 mentions, classic challenge DSAs getting applications run novel architecture. long-standing fallacy assuming yournew computer attractive programmers rewrite code foryour hardware. Fortunately, domain-specific programming languages werebecoming popular even architects forced switch attentionto DSAs. Examples Halide vision processing TensorFlow DNNs (Ragan-Kelley et al., 2013; Abadi et al., 2016 ). languages make porting applications DSA much feasible. previously mentioned, asmall, compute-intensive portion application needs run DSA insome domains, also simplifies porting. DSAs introduce many new terms, mostly new domains also novel architecture mechanisms seen conventional processors. didinChapter 4 ,Figure 7.4 lists new acronyms, terms, short explanations aid reader. 7.3 Example Domain: Deep Neural Networks Artificial intelligence (AI) next big wave computing —it’s next major turning point human history …the Intelligence Revolution driven data, neural networks computing power. Intel committed toAI [thus]…we’ve added set leading-edge accelerants required growth widespread adoption AI. Brian Krzanich , Intel CEO (2016)544 ■Chapter Seven Domain-Specific ArchitecturesArea Term Acronym Short explanationGeneralDomain-specific architecturesDSA special-purpose processor designed particular domain. relies processors handle processing outside domain Intellectual property blockIP portable design block integrated SOC. enable marketplace organizations offer IP blocks others compose theminto SOCs System chip SOC chip integrates components computer; commonly found PMDsDeep neural networksActivation — Result “activating ”the artificial neuron; output nonlinear functions Batch — collection datasets processed together lower cost fetching weights Convolutional neural networkCNN DNN takes inputs set nonlinear functions spatially nearby regions outputs prior layer, multiplied weights Deep neural networkDNN sequence layers collections artificial neurons, consist nonlinear function applied products weights times outputs prior layer Inference — production phase DNNs; also called prediction Long short-term memoryLSTM RNN well suited classify, process, predict time series. hierarchical design consisting modules called cells MultiLayer perceptronMLP DNN takes inputs set nonlinear functions outputs prior layer multiplied weights. layers called fully connected Rectified linear unitReLU nonlinear function performs f(x) ¼max(x,0) . popular nonlinear functions sigmoid hyperbolic tangent (tanh) Recurrent neural networkRNN DNN whose inputs prior layer andthe previous state Training — development phase DNNs; also called learning Weights — values learned training applied inputs; also called parametersTPUAccumulators — 4096 256 /C232-bit registers (4 MiB) collect output MMU input Activation Unit Activation unit — Performs nonlinear functions (ReLU, sigmoid, hyperbolic tangent, max pool, average pool). input comes Accumulators output goes Unified Buffer Matrix multiply unitMMU systolic array 256 /C2256 8-bit arithmetic units perform multiply-add. inputs Weight Memory Unified Buffer, output theAccumulators Systolic array — array processing units lockstep input data upstream neighbors, compute partial results, pass inputs results downstream neighbors Unified buffer UB 24 MiB on-chip memory holds activations. sized try avoid spilling activations DRAM running DNN Weight memory — 8 MiB external DRAM chip containing weights MMU. Weights transferred Weight FIFO entering MMU Figure 7.4 handy guide DSA terms used Sections 7.3 –7.6.Figure 7.29 page 472 guide Section 7.7 .Artificial intelligence (AI) made dramatic comeback since turn cen- tury. Instead building artificial intelligence large set logical rules, focus switched machine learning example data path artificial intel- ligence. amount data needed learn much greater thought. Thewarehouse scale computers (WSCs) century, harvest store peta-bytes information found Internet billions users theirsmartphones, supply ample data. also underestimated amount com-putation needed learn massive data, GPUs —which excellent single-precision floating-point cost-performance —embedded thousands servers WSCs deliver sufficient computing. One part machine learning, called DNNs, AI star past five years. Example DNN breakthroughs language translation, DNNsimproved single leap advances prior decade ( Tung, 2016; Lewis-Kraus, 2016 ); switch DNNs past five years reduced error rate image recognition competition 26% 3.5% ( Krizhevsky et al., 2012; Szegedy et al., 2015; et al., 2016 ); 2016, DNNs enabled computer program first time beat human champion Go ( Silver et al., 2016 ). Although many run cloud, also enabled Goo- gle Translate smartphones, described Chapter 1 . 2017 new, sig- nificant DNN results appear nearly every week. Readers interested learning DNNs found section download try tutorials TensorFlow ( TensorFlow Tutorials, 2016 ), less adventurous, consult free online textbook DNNs (Nielsen, 2016 ). Neurons DNNs DNNs inspired neuron brain. artificial neuron used neu- ral networks simply computes sum set products weights orparam- eters data values put nonlinear function determine output. see, artificial neuron large fan-in large fan-out. image-processing DNN, input data would pixels photo, pixel values multiplied weights. Although many nonlinear func-tions tried, popular one today simply f(x) ¼max(x,0) , returns 0 xis negative original value positive zero. (This simple function goes complicated name rectified linear unit orReLU .) output nonlinear function called activation , output artificial neuron “activated. ” cluster artificial neurons might process different portions input, output cluster becomes input next layer artificial neurons. Thelayers input layer output layer called hidden layers .F r image processing, think layer looking different types fea- tures, going lower-level ones like edges angles higher-level ones like eyes ears. image-processing application trying decide image546 ■Chapter Seven Domain-Specific Architecturescontained dog, output last layer could probability number 0 1 perhaps list probabilities corresponding list dog breeds. number layers gave DNNs name. original lack data computing horsepower kept neural networks relatively shallow.Figure 7.5 shows number layers variety recent DNNs, number weights, number operations per weight fetched. 2017 DNNshave 150 layers. Training Versus Inference preceding discussion concerns DNNs production. DNN develop-ment starts defining neural network architecture, picking number andtype layers, dimensions layer, size data. Althoughexperts may develop new neural network architectures, practitioners willchoose among many existing designs (e.g., Figure 7.5 ) shown perform well problems similar theirs. neural architecture selected, next step learn weights associated edge neural network graph. weights deter- mine behavior model. Depending choice neural architecture,there anywhere thousands hundreds millions weights sin-gle model (see Figure 7.5 ). Training costly process tuning weights DNN approximates complex function (e.g., mapping pictures tothe objects picture) described training data. development phase universally called training orlearning ,w h e r e production phase many names: inference ,prediction ,scoring ,implemen- tation ,evaluation ,running ,o rtesting . DNNs use supervised learning given training set learn data preprocessedin order correct labels. Thus, ImageNet DNN competition(Russakovsky et al., 2015 ), training set consists 1.2 million photos, photo labeled one 1000 categories. Several categoriesName DNN layers Weights Operations/Weight MLP0 5 20M 200 MLP1 4 5M 168LSTM0 58 52M 64LSTM1 56 34M 96CNN0 16 8M 2888CNN1 89 100M 1750 Figure 7.5 Six DNN applications represent 95% DNN workloads inference Google 2016, use Section 7.9 .The columns DNN name, number layers DNN, number weights, operations per weight (oper- ational intensity). Figure 7.41 page 595 goes detail DNNs.7.3 Example Domain: Deep Neural Networks ■547are quite detailed, specific breeds dogs cats. winner deter- mined evaluating separate secret set 50,000 photos see DNN lowest error rate. Setting weights iterative process goes backward neu- ral network using training set. process called backpropagation .F r example, know breed dog image training set, yousee DNN says image, adjust weights toimprove answer. Amazingly, weights start training processshould set random data, keep iterating ’re satisfied DNN accuracy using training set. mathematically inclined, goal learning find function maps inputs correct outputs multilayer neural network architec-ture. Backpropagation stands “back propagation errors. ”It calculates gra- dient weights input optimization algorithm tries tominimize errors updating weights. popular optimizationalgorithm DNNs stochastic gradient descent . adjusts weights propor- tionally maximize descent gradient obtained backpropagation. Readers interested learning see Nielsen (2016) orTensorFlow Tutorials (2016) . Training take weeks computation, Figure 7.6 shows. inference phase often 100 ms per data sample, million times less.Although training takes much longer single inference, total compute timefor inference product number customers DNN howfrequently invoke it. training, deploy DNN, hoping training set representative real world, DNN popular users spend much time employing ’ve put devel- oping it! Type data Problem areaSize benchmark ’s training setDNN architecture HardwareTraining time text [1] Word prediction (word2vec)100 billion words (Wikipedia)2-layer skip gram1 NVIDIA Titan X GPU6.2 hours audio [2] Speech recognition 2000 hours (Fisher Corpus)11-layer RNN 1 NVIDIA K1200 GPU3.5 days images [3] Image classification1 million images (ImageNet)22-layer CNN 1 NVIDIA K20 GPU3 weeks video [4] activity recognition 1 million videos (Sports-1M)8-layer CNN 10 NVIDIA GPUs 1 month Figure 7.6 Training set sizes training time several DNNs ( Iandola, 2016 ).548 ■Chapter Seven Domain-Specific ArchitecturesThere tasks ’t training datasets, trying predict future real-world event. Although ’t cover here, reinforce- ment learning (RL) popular algorithm learning 2017. Instead training set learn from, RL acts real world gets signal areward function, depending whether action made situation betteror worse. Although ’s hard imagine faster changing field, three types DNNs reign popular 2017: MultiLayer Perceptrons (MLPs ), Convolutional Neural Networks (CNNs ), Recurrent Neural Networks (RNNs ). examples supervised learning, rely training sets. Multilayer Perceptron MLPs original DNNs. new layer set nonlinear functions Fof weighted sum outputs prior one yn¼F(W/C2yn/C01). weighted sum consists vector-matrix multiply outputs weights (seeFigure 7.7 ). layer called fully connected output neuron result depends allinput neurons prior layer. calculate number neurons, operations, weights per layer DNN types. easiest MLP vector-matrix Dim[i] Dim[i] Vector matrix multiply Nonlinear functionDim[i-1] Dim[i-1]Layer[i-1] Layer[i] nlfVMX Output WeightsInput nlf VMX Figure 7.7 MLP showing input Layer[ i21] left output Layer[ i]o n right. ReLU popular nonlinear function MLPs. dimensions input output layers often different. layer called fully connected depends inputs prior layer, even many zeros. One studysuggested 44% zeros, presumably part ReLU turns neg- ative numbers zeros.7.3 Example Domain: Deep Neural Networks ■549multiply input vector times weights array. parameters equations determine weights operations inference (we count multiply add two operations): ■Dim[ i]: Dimension output vector, number neurons ■Dim[ i/C01]: Dimension input vector ■Number weights: Dim[ i/C01]/C2Dim[ i] ■Operations: 2 /C2Number weights ■Operations/Weight: 2 final term operational intensity Roofline model discussed Chapter 4 . use operations per weight millions weights, usually ’t fit chip. example, dimensions one stage MLP Section 7.9 Dim[ i/C01]¼4096 Dim[ i]¼2048, layer, number neurons 2048, number weights is8,388,608, number operations 16,777,216, operational intensityis 2. recall Roofline model, low operational intensity makes itharder deliver high performance. Convolutional Neural Network CNNs widely used computer vision applications. images two-dimensional structure, neighboring pixels natural place look find rela- tionships. CNNs take inputs set nonlinear functions spatially nearby regions outputs prior layer multiplies weights, whichreuses weights many times. idea behind CNNs layer raises level abstraction image. example, first layer might identify horizontal lines verticallines. second layer might combine identify corners. next stepmight rectangles circles. following layer could use input detectportions dog, like eyes ears. higher layers would trying identify characteristics different breeds dogs. neural layer produces set two-dimensional feature maps , cell two-dimensional feature map trying identify one feature cor-responding area input. Figure 7.8 shows starting point 2 /C22 stencil computation input image creates elements first feature map. stencil computation uses neighboring cells fixed pattern update elements array.The number output feature maps depend many different features trying capture image stride used apply stencil. process actually complicated image usually single, flat two-dimensional layer. Typically, color image three levelsfor red, green, blue. example, 2 /C22 stencil access 12 elements: 2 /C22550 ■Chapter Seven Domain-Specific Architecturesof red pixels, 2 /C22 green pixels, 2 /C22 blue pixels. case, need 12 weights per output feature map 2 /C22 stencil three input levels image. Figure 7.9 shows general case arbitrary number input output feature maps, occurs first layer. calculation three-dimensional stencil input feature maps set weights produceone output feature map. mathematically oriented, number input feature maps output feature maps equal 1 stride 1, single layer two-dimensionalCNN calculation two-dimensional discrete convolution. see Figure 7.9 , CNNs complicated MLPs. parameter equations calculate weights operations: ■DimFM[ i/C01]: Dimension (square) input Feature Map ■DimFM[ i]: Dimension (square) output Feature Map ■DimSten[ i]: Dimension (square) stencil ■NumFM[ i/C01]: Number input Feature Maps ■NumFM[ i]: Number output Feature Maps ■Number neurons: NumFM[ i]/C2DimFM[ i]2Output feature map WeightsInput image nlf VMX Vector matrix multiply Nonlinear function nlfVMX Figure 7.8 Simplified first step CNN. example, every group four pixels input image multiplied four weights create cells outputfeature map. pattern depicted shows stride two groups inputpixels, strides possible. relate figure MLP, think 2/C22 convolution tiny fully connected operation produce one point output feature map. Figure 7.9 shows multiple feature maps turn points vector third dimension.7.3 Example Domain: Deep Neural Networks ■551■Number weights per output Feature Map: NumFM[ i/C01]/C2DimSten[ i]2 ■Total number weights per layer: NumFM[ i]/C2Number weights per output Feature Map ■Number operations per output Feature Map: 2 /C2DimFM[ i]2/C2Number weights per output Feature Map ■Total number operations per layer: NumFM[ i]/C2Number operations per output Feature Map ¼2/C2DimFM[ i]2/C2NumFM[ i]/C2Number weights per output Feature Map ¼2/C2DimFM[ i]2/C2Total number weights per layer ■Operations/Weight: 2 /C2DimFM[ i]2 CNN Section 7.9 layer DimFM[ i/C01]¼28, DimFM[ i]¼14, Dim- Sten[ i]¼3, NumFM[ i/C01]¼64 (number input feature maps), NumFM[ i]¼ 128 (number output feature maps). layer 25,088 neurons, 73,728weights, 28,901,376 operations, operational intensity 392.As example indicates, CNN layers generally fewer weights greateroperational intensity fully connected layers found MLPs.Vector matrix multiply Nonlinear function nlfVMX WeightsNumFM[ i-1] NumFM[ i]DimFM[i]DimFM[ i] DimFM[i-1]DimFM[ i-1] nlf VMX DimSten[ i]DimSten[ i]NumFM[ i-1]Layer[i-1] (input feature maps)Layer[i] (output feature maps) Figure 7.9 CNN general step showing input feature maps Layer[ i21] left, output feature maps Layer[ i] right, three-dimensional stencil input feature maps produce single output feature map. output feature map unique set weights, vector-matrix multiply happens every one. dotted lines show future output feature maps figure. figure illustrates, dimensions number input output feature maps often different. Aswith MLPs, ReLU popular nonlinear function CNNs.552 ■Chapter Seven Domain-Specific ArchitecturesRecurrent Neural Network third type DNN RNNs, popular speech recognition lan- guage translation. RNNs add ability explicitly model sequential inputs byadding state DNN model RNNs remember facts. ’s analogous difference hardware combinational logic state machine. Forexample, might learn gender person, would want passalong remember later translating words. layer RNN col-lection weighted sums inputs prior layer previous state. Theweights reused across time steps. Long short-term memory (LSTM ) far popular RNN today. LSTMs mitigate problem previous RNNs inability remem-ber important long-term information. Unlike two DNNs, LSTM hierarchical design. LSTM consists modules called cells. think cells templates macros linked together create full DNN model, similar layers MLP line toform complete DNN model. Figure 7.10 shows LSTM cells linked together. hooked left right, connecting output one cell input next. also unrolled time, runs top Figure 7.10 . Thus sentence input word time per iteration unrolled loop. long-term short-term memory information gives LSTM name also passed top-downfrom one iteration next. Figure 7.11 shows contents LSTM cell. would expect Figure 7.10 , input left, output right, two memory inputs top, two memory outputs bottom. cell five vector-matrix multiplies using five unique sets weights. matrix multiply input like MLP Figure 7.7 . Three others called gates gate limit much information one source passed along standard output memory output. amount infor-mation sent per gate set weights. weights mostly zeros smallvalues, little gets through; conversely, mostly large, gatelets information flow. three gates called input gate , output gate, forget gate . first two filter input output, last one determines forget along long-term memory path. short-term memory output vector-matrix multiply using Short Term Weights output cell. short-term label applied becauseit directly use inputs cell. LSTM cell inputs outputs connected together, size three input-output pairs must same. Looking inside cell, areenough dependencies inputs outputs often size.Let’s assume size, called Dim. Even so, vector-matrix multiplies size. vectors three gate multiplies 3 /C2Dim, LSTM concatenates three inputs. vector input multiply 2 /C2Dim, LSTM7.3 Example Domain: Deep Neural Networks ■553concatenates input short-term memory input vector. vector last multiply 1 /C2Dim, output. finally calculate weights operations: ■Number weights per cell: 3 /C2(3/C2Dim/C2Dim) +(2 /C2Dim/C2Dim) +(1/C2Dim/C2Dim) ¼12/C2Dim2 ■Number operations 5 vector-matrix multiplies per cell: 2 /C2Number weights per cell ¼24/C2Dim2 ■Number operations 3 element-wise multiplies 1 addition (vectors size output): 4 /C2Dim ■Total number operations per cell (5 vector-matrix multiplies 4 element-wise operations): 24 /C2Dim2+4/C2Dim ■Operations/Weight: /C242LSTM 0 LSTM 1 . . . LSTM n “now” LSTM 0 LSTM 1 . . . LSTM n “is” LSTM 0 LSTM 1 . . . LSTM n “the”Time LSTM 0 LSTM 1 . . . LSTM n “time” LSTM 0 LSTM 1 . . . LSTM n <end_input> LSTM 0 LSTM 1 . . . LSTM n “momento” LSTM 0 LSTM 1 . . . LSTM n “el” LSTM 0 LSTM 1 . . . LSTM n “es” LSTM 0 LSTM 1 . . .. . . . . . . . . . . . . . . . . . . . .LSTM n “ahora”“momento” “el”“es”“ahora”<end_output> Figure 7.10 LSTM cells connected together. inputs left (English words), outputs right (the translated Spanish words). cells thought unrolled time, top bottom. Thus short-term long-term memory LSTM implemented passing information top-down unrolledcells. unrolled enough translate whole sentences even paragraphs. sequence-to-sequence translation models delay output get end input ( Wu et al., 2016 ). produce translation reverse order ,u n gt h em recent translated word input next step, “now time ”becomes “ahora es el momento. ”(This figure next often shown turned 90 degrees LSTM litera- ture, ’ve rotated consistent Figures 7.7 7.8.)554 ■Chapter Seven Domain-Specific ArchitecturesDim 1024 one six cells LSTM Section 7.9 . number weights 12,582,912, number operations 25,169,920, operationalintensity 2.0003. Thus LSTMs like MLPs typically moreweights lower operational intensity CNNs.Output InputLTMemoryin STMemoryin LTMemoryout STMemoryoutnlf VMX nlfnlf : : : ++ VMXVMX nlf VMX nlfOutput gate weights Forget gate weights Input gate weightsVMX Short term weightsnlf VMX Input weightsVector matrix multiply Element-wise multiply Element-wise addition Nonlinear function Concatenation Figure 7.11 LSTM cell contains 5 vector-matrix multiplies, 3 element-wise multiplies, 1 element-wise add, 6 nonlinear functions. standard input short-term memory input concatenated form vector operand input vector-matrix multiply. standard input, long-term memory input, short-term memory input concatenated form vector used three four vector-matrix multiplies. non-linear functions three gates Sigmoids f(x)¼1/(1 + exp( /C0x)); others hyperbolic tangents. (This figure previous one often shown turned 90 degrees LSTM literature, ’ve rotated consistent Figures 7.7 7.8.)7.3 Example Domain: Deep Neural Networks ■555Batches DNNs many weights, performance optimization reuse weights fetch ed memory across set inputs, thereby increasing effective operational intensity. example, image-processing DNN might work set 32 images time reduce theeffective cost fetching weights factor 32. datasets calledbatches orminibatches . addition improving performance inference, backpropagation needs batch examples instead one time order totrain well. Looking MLP Figure 7.7 , batch seen sequence input row vectors, think matrix height dimension thatmatches batch size. sequence row vector inputs five matrix multi-plies LSTMs Figure 7.11 also considered matrix. cases, com- puting matrices instead sequentially independent vectors improvescomputing efficiency. Quantization Numerical precision less important DNNs many applications. Forexample, need double-precision floating-point arithmetic, standard bearer high-performance computing. ’s even unclear need full accuracy IEEE 754 floating-point standard, aimsto accurate within one-half unit last place floating-pointsignificand. take advantage flexibility numerical precision, devel- opers use fixed point instead floating point inference phase. (Train-ing almost always done floating-point arithmetic.) conversion iscalled quantization , transformed application said quantized (Vanhoucke et al., 2011 ). fixed-point data width usually 8 16 bits, standard multiply-add operation accumulating twice widthof multiplies. transformation typically occurs training, andit reduce DNN accuracy percentage points ( Bhattacharya Lane, 2016 ). Summary DNNs Even quick overview suggests DSAs DNNs need perform least matrix-oriented operations well: vector-matrix multiply, matrix-matrix multiply, stencil computations. also need support nonlinear functions, include minimum ReLU, Sigmoid, tanh. modestrequirements still leave open large design space, next foursections explore.556 ■Chapter Seven Domain-Specific Architectures7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator Tensor Processing Unit ( TPU)1is Google ’s first custom ASIC DSA WSCs. domain inference phase DNNs, programmed using Tensor-Flow framework, designed DNNs. first TPU deployedin Google data centers 2015. heart TPU 65,536 (256 /C2256) 8-bit ALU Matrix Multiply Unit large software-managed on-chip memory. TPU ’s single-threaded, deter- ministic execution model good match 99th-percentile response-time requirement typical DNN inference application. TPU Origin Starting far back 2006, Google engineers discussions deploying GPUs, FPGAs, custom ASICs data centers. concluded thefew applications could run special hardware could done virtually freeusing excess capacity large data centers, ’s hard improve free. conversation changed 2013 projected people used voicesearch three minutes day using speech recognition DNNs, would required Google ’s data centers double order meet computation demands. would expensive satisfy conventional CPUs. Google thenstarted high-priority project quickly produce custom ASIC inference(and bought off-the-shelf GPUs training). goal improve cost-performance 10 /C2over GPUs. Given mandate, TPU designed, ver- ified ( Steinberg, 2015 ), built, deployed data centers 15 months. TPU Architecture reduce chances delaying deployment, TPU designed coprocessor PCIe I/O bus, allows plugged existing servers. Moreover, simplify hardware design debugging, host serversends instructions PCIe bus directly TPU execute, ratherthan TPU fetch instructions. Thus TPU closer spirit toan FPU (floating-point unit) coprocessor GPU, fetches instruc-tions memory. Figure 7.12 shows block diagram TPU. host CPU sends TPU instructions PCIe bus instruction buffer. internal blocks typically connected together 256-byte-wide (2048-bits) paths. Starting upper-right corner, Matrix Multiply Unit heart TPU. contains 1This section based paper “In-Datacenter Performance Analysis Tensor Processing Unit ”Jouppi et al., 2017 , one book authors coauthor.7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator ■557256/C2256 ALUs perform 8-bit multiply-and-adds signed unsigned integers. 16-bit products collected 4 MiB 32-bit Accumulators matrix unit. using mix 8-bit weights 16-bit activations(or vice versa), Matrix Unit computes half-speed, computes aquarter-speed 16 bits. reads writes 256 values per clock cycleand perform either matrix multiply convolution. nonlinear functionsare calculated Activation hardware. weights matrix unit staged on-chip Weight FIFO reads off-chip 8 GiB DRAM called Weight Memory (for inference, weights read-only; 8 GiB supports many simultaneously active models).The intermediate results held 24 MiB on-chip Unified Buffer , serve inputs Matrix Multiply Unit. programmable DMA controllertransfers data CPU Host memory Unified Buffer.Control ControlControl Off-chip I/O Data buffer Computation Control PCIe Gen3 x16 interface Host interfaceControl Control Unified buffer (local activation storage)Systolic data setupDDR3-2133 interfacesWeight FIFO (weight fetcher) Accumulators Activation Normalize / Pool10 GiB/s 167 GiB/s167 GiB/s14 GiB/s 30 GiB/s 30 GiB/s30 GiB/sDDR3 DRAM chips 14 GiB/s14 GiB/s InstrMatrix multiply unit (64K per cycle) Figure 7.12 TPU Block Diagram. PCIe bus Gen3 /C216. main computation part light-shaded Matrix Multiply Unit upper-right corner. inputs medium-shaded Weight FIFO medium-shaded Uni- fied Buffer output medium-shaded Accumulators. light-shaded Activation Unit performs non- linear functions Accumulators, go Unified Buffer.558 ■Chapter Seven Domain-Specific ArchitecturesTPU Instruction Set Architecture instructions sent relatively slow PCIe bus, TPU instructions follow CISC tradition, including repeat field. TPU programcounter, branch instructions; instructions sent hostCPU. clock cycles per instruction (CPI) CISC instructions typi-cally 10 –20. dozen instructions overall, five key ones: 1.Read_Host_Memory reads data CPU host memory Unified Buffer. 2.Read_Weights reads weights Weight Memory Weight FIFO input Matrix Unit. 3.MatrixMultiply/Convolve causes Matrix Multiply Unit perform matrix-matrix multiply, vector-matrix multiply, element-wise matrix multi- ply, element-wise vector multiply, convolution Unified Buffer Accumulators. matrix operation takes variable-sized B*256 input,multiplies 256 /C2256 constant input, produces B*256 output, taking B pipelined cycles complete. example, input 4 vectors 256elements, B would 4, would take 4 clock cycles complete. 4.Activate performs nonlinear function artificial neuron, options ReLU, Sigmoid, tanh, on. inputs Accumulators,and output Unified Buffer. 5.Write_Host_Memory writes data Unified Buffer CPU host memory. instructions alternate host memory read/write, set configuration, two versions synchronization, interrupt host, debug-tag, nop, halt. CISC MatrixMultiply instruction 12 bytes, 3 Unified Buffer address; 2are accumulator address; 4 length (sometimes 2 dimensions convolutions);and rest opcode flags. goal run whole inference models TPU reduce interactions host CPU flexible enough match DNN needs 2015and beyond, instead required 2013 DNNs. TPU Microarchitecture microarchitecture philosophy TPU keep Matrix Multiply Unit busy. plan hide execution instructions overlapping execution MatrixMultiply instruction. Thus preceding four general categories instructions separate execution hardware (with readand write host memory combined unit). increase instruction par-allelism further, Read_Weights instruction follows decoupled access/7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator ■559execute philosophy ( Smith, 1982b ) complete sending addresses weights fetched Weight Memory. matrix unit not-ready signals Unified Buffer Weight FIFO causethe matrix unit stall data yet available. Note TPU instruction execute many clock cycles, unlike traditional RISC pipeline one clock cycle per stage. reading large SRAM much expensive arithmetic, Matrix Multiply Unit uses systolic execution save energy reducingreads writes Unified Buffer ( Kung Leiserson, 1980; Ramacher et al., 1991; Ovtcharov et al., 2015b ). systolic array two- dimensional collection arithmetic units independently compute apartial result function inputs arithmetic units con-sidered upstream unit. relies data different directions arriv- ing cells array regular intervals combined. Becausethe data flows array advancing wave front, similar toblood pumped human circulatory system heart, whichis origin systolic name. Figure 7.13 demonstrates systolic array works. six circles bot- tom multiply-accumulate units initialized weights wi. staggered input data xiare shown coming array above. 10 steps figure represent 10 clock cycles moving top bottom page.The systolic array passes inputs products sums right.The desired sum products emerges data completes path thesystolic array. Note systolic array, input data read frommemory, output data written memory. TPU, systolic array rotated. Figure 7.14 shows weights loaded top input data flows array left. given256-element multiply-accumulate operation moves matrix diag-onal wave front. weights preloaded take effect advancingwave alongside first data new block. Control data pipelined givethe illusion 256 inputs read once, feed delay, updateone location 256 accumulator memories. correctness perspec-tive, software unaware systolic nature matrix unit, perfor- mance, worry latency unit. TPU Implementation TPU chip fabricated using 28-nm process. clock rate 700 MHz. Figure 7.15 shows floor plan TPU. Although exact die size revealed, less half size Intel Haswell server microprocessor,which 662 mm 2. 24 MiB Unified Buffer almost third die, Matrix Multiply Unit quarter, datapath nearly two-thirds die. 24 MiB size picked part match pitch Matrix Unit die and, given the560 ■Chapter Seven Domain-Specific ArchitecturesX3 X2 X1 W11 W21W12 W22 (A)W13 W23W12 X2W11X3 (E)W13 W23 W22W21 X1 (B)X3 X2 X1 W11 W21W12 W22W13 W23* (C)X3 X2 W11 X1 W21W12 W22W13 W23W12 X2W11X3 (F)W13 W23 W22W21 X1*++ (G)W13 X3W11 W12 W23 W21W22 X2 (H)W13 X3W11 W12 W23 W21W22 X2 +y1 = w11x1 + w12x2 + w13x3 (I)W11 W12 W13 W23 X3W21 W22y1 = w11x1 + w12x2 + w13x3 (D)X3 X2 W11 X1 W21W12 W22W13 W23+* (J)W11 W12 W13 W23 X3W21 W22y1 = w11x1 + w12x2 + w13x3 y2 = w21x1 + w22x2 + w23x3 Figure 7.13 Example systolic array action, top bottom page. example, six weights already inside multiply-accumulate units, norm TPU. three inputs staggered time get desired effect, example shown coming top. (In TPU, data actually comes left.) array passes data next element result computation right tothe next element. end process, sum products found right. Drawings courtesy Yaz Sato.Local Unified Buffer activations (96Kx256x8b = 24 MiB) 29% chipMatrix multiply unit (256x256x8b = 64K MAC) 24% Host Interf. 2%Accumulators (4Kx256x32b = 4 MiB) 6%D R port ddr3 3%DR port ddr3 3%Activation pipeline 6% Control 2% PCle Interface 3% Misc. I/O 1% Figure 7.15 Floor plan TPU die. shading follows Figure 7.14 . light data buffers 37%, light computation units 30%, medium I/O 10%, dark control 2% die. Control much larger (and much difficultto design) CPU GPU. unused white space consequence emphasison time tape-out TPU.Data Partial sums ++ + Done . . .. . .Control + Figure 7.14 Systolic data flow Matrix Multiply Unit.562 ■Chapter Seven Domain-Specific Architecturesshort development schedule, part simplify compiler. Control 2%. Figure 7.16 shows TPU printed circuit card, inserts existing servers SATA disk slot. TPU Software TPU software stack compatible developed CPUs GPUs applications could ported quickly. portion application run TPU typically written using TensorFlow compiled API run GPUs TPUs ( Larabel, 2016 ).Figure 7.17 shows TensorFlow code portion MLP. Like GPUs, TPU stack split User Space Driver Kernel Driver. Kernel Driver lightweight handles memory management interrupts. designed long-term stability. User Space driver changes frequently. sets controls TPU execution, reformats data TPU order, translates API calls TPU instructions turns application binary. User Space driver compiles model first time evaluated, caching program image writing weight image TPU Weight Memory; second following evaluations run full speed. TPU runs models completely inputs outputs, maximizing ratio TPU compute time I/O time. Computation often done one layer time, overlapped execution allowing matrix unit hide noncritical path operations. Figure 7.16 TPU printed circuit board. inserted slot SATA disk server, card uses PCIe bus.7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator ■563Improving TPU TPU architects looked variations microarchitecture see whether could improved TPU. Like FPU, TPU coprocessor relatively easy microarchitecture evaluate, TPU architects created performance model estimated performance memory bandwidth, matrix unit size, clock rateand number accumulators varied. Measurements using TPU hardware coun-ters found modeled performance average within 8% thehardware.# Network Parameters n_hidden_1 = 256 # 1st layer number features n_hidden_2 = 256 # 2nd layer number features n_input = 784 # MNIST data input (img shape: 28*28)n_classes = 10 # MNIST total classes (0-9 digits) # tf Graph input x = tf.placeholder("float", [None, n_input])y = tf.placeholder("float", [None, n_classes]) # Create model def multilayer_perceptron(x, weights, biases): # Hidden layer ReLU activation layer_1 = tf.add(tf.matmul(x, weights[ ’h1’]), biases[ ’b1’]) layer_1 = tf.nn.relu(layer_1)# Hidden layer ReLU activationlayer_2 = tf.add(tf.matmul(layer_1, weights[ ’h2’]), biases[ ’b2’]) layer_2 = tf.nn.relu(layer_2) # Output layer linear activationout_layer = tf.matmul(layer_2, weights[ ’out’]) + biases[ ’out’] return out_layer # Store layers weight & bias weights = { ’h1’: tf.Variable(tf.random_normal([n_input, n_hidden_1])), ’h2’: tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])), ’out’: tf.Variable(tf.random_normal([n_hidden_2, n_classes])) } biases = { ’b1’: tf.Variable(tf.random_normal([n_hidden_1])), ’b2’: tf.Variable(tf.random_normal([n_hidden_2])), ’out’: tf.Variable(tf.random_normal([n_classes])) } Figure 7.17 Portion TensorFlow program MNIST MLP. two hidden 256 /C2256 layers, layer using ReLU nonlinear function.564 ■Chapter Seven Domain-Specific ArchitecturesFigure 7.18 shows performance sensitivity TPU parameters scale range 0.25 /C2to 4/C2.(Section 7.9 lists benchmarks used.) addition evaluating impact raising clock rates ( clock inFigure 7.18 ), Figure 7.18 also plots design ( clock +) increases clock rate scales number accumulators correspondingly compiler keep mem-ory references flight. Likewise, Figure 7.18 plots matrix unit expansion number accumulators increase square rise one dimension(matrix +), matrix grows dimensions, well increasing matrix unit ( matrix ). First, increasing memory bandwidth ( memory ) biggest impact: perfor- mance improves 3 /C2on average memory bandwidth increases 4 /C2, reduces time waiting weight memory. Second, clock rate little benefit onaverage without accumulators. Third, average performance inFigure 7.18 slightly degrades matrix unit expands 256 /C2256 512/C2512 applications, whether get accumulators. issue analogous internal fragmentation large pages, worse becauseit’s two dimensions. Consider 600 /C2600 matrix used LSTM1. 256 /C2256 matrix unit, takes nine steps tile 600 /C2600, total 18 μs time. larger 512/C2512 unit requires four steps, step takes four times longer, 32 μs time. TPU ’s CISC instructions long, decode insignificant hide overhead loading DRAM.0.00.0 0.5 1.0 1.5 2.0 Scale relative original MPUPerformance relative original MPU 2.5 3.0 3.5 4.0memory matrix+ matrixclock+ clock 0.51.01.52.02.53.03.5 Figure 7.18 Performance metrics scale 0.25 ×to 4×: memory bandwidth, clock rate+accumulators, clock rate, matrix unit dimension+accumulators, andone dimension square matrix unit average performance calculated six DNN applications Section 7.9 . CNNs tend computation-bound, MLPs LSTMs memory-bound. applications benefit faster mem-ory, faster clock makes little difference, bigger matrix unit actually hurts per- formance. performance model code running inside TPU factor CPU host overhead.7.4 Google ’s Tensor Processing Unit, Inference Data Center Accelerator ■565Given insights performance model, TPU architects next evaluated alternative hypothetical TPU might designed process technology ’d 15 months so. aggressive logic synthesis block design might increased clock rateby 50%. architects found designing interface circuit GDDR5memory, used K80, would improve Weight Memory bandwidth bymore factor five. Figure 7.18 shows, increasing clock rate 1050 MHz, helping memory, made almost change performance.If clock left 700 MHz, uses GDDR5 instead Weight Memory, performance increased 3.2 /C2, even accounting host CPU overhead invoking DNN revised TPU. improve averageperformance further. Summary: TPU Follows Guidelines Despite living I/O bus relatively little memory bandwidth thatlimits full utilization TPU, small fraction big number can, nonetheless,be relatively large. see Section 7.9 , TPU delivered goal tenfold improvement cost-performance GPU running DNN infer-ence applications. Moreover, redesigned TPU change switch memory technology GPU would three times faster. One way explain TPU ’s success see followed guidelines inSection 7.2 . 1.Use dedicated memories minimize distance data moved . TPU 24 MiB Unified Buffer holds intermediate matrices andvectors MLPs LSTMs feature maps CNNs. optimized foraccesses 256 bytes time. also 4 MiB Accumulators, 32-bits wide, collect output Matrix Unit act input hardware calculates nonlinear functions. 8-bit weights stored separateoff-chip weight memory DRAM accessed via on-chip weight FIFO. Incontrast, types sizes data would exist redundant copies sev-eral levels inclusive memory hierarchy general-purpose CPU. 2.Invest resources saved dropping advanced microarchitectural optimi- zations arithmetic units bigger memories . TPU offers 28 MiB dedicated memory 65,536 8-bit ALUs, whichmeans 60% memory 250 times many ALUs aserver-class CPU, despite half size power (see Section 7.9 ). Com- pared server-class GPU, TPU 3.5 times on-chip memory 25 times many ALUs. 3.Use easiest form parallelism matches domain . TPU delivers performance via two-dimensional SIMD parallelismwith 256 /C2256 Matrix Multiply Unit, internally pipelined systolic organization, plus simple overlapped execution pipeline its566 ■Chapter Seven Domain-Specific Architecturesinstructions. GPUs rely instead multiprocessing, multithreading, one- dimensional SIMD, CPUs rely multiprocessing, out-of-order execution, one-dimensional SIMD. 4.Reduce data size type simplest needed domain . TPU computes primarily 8-bit integers, although supports 16-bit integers accumulates 32-bit integers. CPUs GPUs also support64-bit integers 32-bit 64-bit floating point. 5.Use domain-specific programming language port code DSA . TPU programmed using TensorFlow programming framework,whereas GPUs rely CUDA OpenCL CPUs must run virtuallyeverything. 7.5 Microsoft Catapult, Flexible Data Center Accelerator time Google thinking deploying custom ASIC itsdata centers, Microsoft considering accelerators theirs. Microsoftperspective solution follow guidelines: ■It preserve homogeneity servers enable rapid redeployment ofmachines avoid making maintenance scheduling even com-plicated, even notion bit odds concept DSAs. ■It scale applications might need resources could fit intoa single accelerator without burdening applications multipleaccelerators. ■It needed power-efficient. ■It ’t become dependability problem single point failure. ■It fit within available spare space power existing servers. ■It could hurt data center network performance reliability. ■The accelerator improve cost-performance server. first rule prevented deploying ASIC helped applications servers, decision Google made. Microsoft started project called Catapult placed FPGA PCIe bus board data center servers. boards dedicated network appli-cations need one FPGA. plan use flexibility ofthe FPGA tailor use varying applications different servers andto reprogram server accelerate distinct applications time. planincreased return investment accelerator. Another advantage ofFPGAs lower NRE ASICs, could improve return investment. discuss two generations Catapult, showing design evolved meet needs WSCs.7.5 Microsoft Catapult, Flexible Data Center Accelerator ■567One interesting upside FPGAs application —or even phase application —can thought DSA, section, get see many examples novel architectures one hardware platform. Catapult Implementation Architecture Figure 7.19 shows PCIe board Microsoft designed fit within servers, limited power cooling 25 W. constraint led selection 28-nm Altera Stratix V D5 FPGA first implemen- tation Catapult. board also 32 MiB flash memory includes two banks DDR3-1600 DRAM total capacity 8 GiB. FPGA 3926 18-bit ALUs, 5 MiB on-chip memory, 11 GB/s bandwidth DDR3 DRAMs. Figure 7.19 Catapult board design. (A) shows block diagram, (B) pho- tograph sides board, 10 cm /C29c /C216 mm. PCIe inter- FPGA network wired connector bottom board plugs directly motherboard. (C) photograph server, 1U (4.45 cm) high half standard rack wide. server two 12-core Intel Sandy Bridge Xeon CPUs, 64 GiB DRAM, 2 solid-state drives, 4 hard-disk drives, 10-Gbit Ethernet network card. highlighted rectangle right (C) shows location Catapult FPGA board server. cool air sucked left (C), hot air exhausts right, passes Catapult board. hot spot amount power connector could deliver mean Catapult board limited 25 watts. Forty-eight servers share Ethernet switch connects data center network, occupy half data center rack.568 ■Chapter Seven Domain-Specific ArchitecturesEach 48 servers half data center rack contains Catapult board. Catapult follows preceding guidelines supporting applications need single FPGA without affecting performance data center net-work. adds separate low-latency 20 Gbit/s network connects 48 FPGAs.The network topology two-dimensional 6 /C28 torus network. follow guideline single point failure, network reconfigured operate even one FPGAs fails. board also hasSECDED protection memories outside FPGA, required forlarge-scale deployment data center. FPGAs use great deal memory chip deliver programma- bility, vulnerable ASICs single-event upsets (SEUs) radiation process geometries shrink. Altera FPGA Catapult boardsincludes mechanisms detect correct SEUs inside FPGA reduces thechances SEUs periodically scrubbing FPGA configuration state. separate network added benefit reducing variability com- munication performance compared data center network. Network unpredict-ability increases tail latency —which especially detrimental applications face end users —so separate network makes easier successfully offload work CPU accelerator. FPGA network run much simpler pro-tocol data center error rates considerably lower thenetwork topology well defined. Note resiliency requires care reconfiguring FPGAs nei- ther appear failed nodes crash host server corrupt neighbors.Microsoft developed high-level protocol ensuring safety reconfiguringone FPGAs. Catapult Software Possibly biggest difference Catapult TPU programin hardware-description language Verilog VHDL. Catapultauthors write ( Putnam et al., 2016 ): Going forward, biggest obstacle widespread adoption FPGAs datacenter likely programmability. FPGA development still requiresextensive hand-coding Register Transfer Level manual tuning. reduce burden programming Catapult FPGAs, Register Transfer Level (RTL) code divided shell role,a sFigure 7.20 shows. shell code like system library embedded CPU. contains RTL code thatwill reused across applications FPGA board, data marshaling,CPU-to-FPGA communication, FPGA-to-FPGA communication, data movement,reconfiguration, health monitoring. shell RTL code 23% AlteraFPGA. role code application logic, Catapult programmer writesusing remaining 77% FPGA resources. shell addedbenefit offering standard API standard behavior across applications.7.5 Microsoft Catapult, Flexible Data Center Accelerator ■569CNNs Catapult Microsoft developed configurable CNN accelerator application Catapult. Configuration parameters include number neural networklayers, dimension layers, even numerical precision beused. Figure 7.21 shows block diagram CNN accelerator. key features are: ■Run-time configurable design, without requiring recompilation using FPGA tools. ■To minimize memory accesses, offers efficient buffering CNN data struc-tures (see Figure 7.21 ). ■A two-dimensional array Processing Elements (PEs) scale thousands units.Shell RoleDDR3 Core 04 GB DDR3-1333 ECC SO-DIMM DDR3 Core 1724 GB DDR3-1333 ECC SO-DIMM 256 Mb QSPI Config FlashConfig Flash (RSU) JTAG LEDs Temp Sensors xcvr reconfigx8 PCIe CoreHost CPU DMA Engine SEUI2C72 4 8 2 2 2 2Inter-FPGA Router North SLIIISouth SLIIIEast SLIIIWest SLIIIApplication Figure 7.20 Components Catapult shell role split RTL code.570 ■Chapter Seven Domain-Specific ArchitecturesImages sent DRAM input multibank buffer FPGA. inputs sent multiple PEs perform stencil computations produce theoutput feature maps. controller (upper left Figure 7.21 ) orchestrates flow data PE. final results recirculated input buffers compute next layer CNN. Like TPU, PEs designed used systolic array. Figure 7.22 shows details PE design.Top controller Layer controller Address generation Scan chain Broad-castInput volume Segment 0 Input volume Segment 1 Input volume Segment N-2Input volume Input volume Segment N-1Layer config.Data re-distributionOutput volume z zx xy yOutput feature map Input kernel weight 0 PE PE PE PEOutput feature map Input kernel weight 1 PE PE PE PEOutput feature map Input kernel weight M-2 PE PE PE PEOutput feature map Input kernel weight M-1 PE PE PE PE       Figure 7.21 CNN Accelerator Catapult. Input Volume left correspond Layer[ i/C01] left Figure 7.20 , NumFM[ i/C01] corresponding DimFM[ i/C01] corresponding z. Output Volume top maps Layer[ i], z mapping NumFM[ i] DimFM[ i] mapping x. next figure shows inside Processing Element (PE).7.5 Microsoft Catapult, Flexible Data Center Accelerator ■571IBW0 FU0,0 Max pool controlMax pool commandBias data loadInput double Buffer arrayUmi Dram FetcherDRAM PCle Umi command Shallow FIFO ArrayControlAddress Ring ArbitorFunctional unit arrayIBD0 OB0FU1,0 FU2,0 FUn,0 +b0 MPE0 0IBW1 FU0,0 OB1FU1,1 FU2,1 FUn,1 +b1 MPE1 1IBW2 FU0,0 OB2FU1,2 FU2,2 FUn,2 +b2 MPE2   2IBWn FU0,n OBnFU1,n FU2,n FUn,n +bn MPEn n       IBD0 IBD0 IBD0Single Layer ControlRegister interface Multi layer controlKernel weights Buffer array Output buffer Array Bias buffer Array Activation Function array Max pooling arrayUmi command Figure 7.22 Processing Element (PE) CNN Accelerator Catapult Figure 7.21 .The two-dimension Functional Units (FU) consist ALU registers.572 ■Chapter Seven Domain-Specific ArchitecturesSearch Acceleration Catapult primary application totestthe return oninvestmentof Catapultwas criticalfunc- tionof Microsoft Bingsearchenginecalled ranking . ranks order ofthe results search. output document score, determines position thedocument onthe web page presented user. algorithm three stages: 1.Feature Extraction extracts thousands interesting features document based search query, frequency query phrase appearsin document. 2.Free-Form Expressions calculates thousands combinations features prior stage. 3.Machine-Learned Scoring uses machine-learning algorithms evaluate features first two stages calculate floating-point score docu-ment returned host search software. Catapult implementation ranking produces identical results equivalent Bing software, even reproducing known bugs! Taking advantage one preceding guidelines, ranking function fit within single FPGA. ranking stages split acrosseight FPGAs: ■One FPGA Feature Extraction. ■Two FPGAs Free-Form Expressions. ■One FPGA compression stage increases scoring engine efficiency. ■Three FPGA Machine-Learned Scoring. remaining FPGA spare used tolerate faults. Using multiple FPGAs one application works well dedicated FPGA network. Figure 7.23 shows Feature Extraction stage organization. uses 43 feature- extraction state machines compute parallel 4500 features per document- query pair. Next following Free-Form Expressions stage. Rather implement functions directly gates state machines, Microsoft developed 60-core pro-cessor overcomes long-latency operations multithreading. Unlike GPU,Microsoft ’s processor require SIMD execution. three features let match latency target: 1.Each core supports four simultaneous threads one stall long operation others continue. functional units pipelined, theycan accept new operation every clock cycle. 2.Threads statically prioritized using priority encoder. Expressions longest latency use thread slot 0 cores, next slowest slot 1 onall cores, on.7.5 Microsoft Catapult, Flexible Data Center Accelerator ■5733.Expressions large fit time allocated single FPGA split across two FPGAs used free-form expressions. One cost reprogrammability FPGA slower clock rate custom chips. Machine-Learned Scoring uses two forms parallelism try overcomethat disadvantage. first pipeline matches available pipelineparallelism application. ranking, limit 8 μs per stage. second version parallelism rarely seen multiple instruction streams ,single data stream (MISD) parallelism, large number independent instruction streams operate parallel single document. Figure 7.24 shows performance ranking function Catapult. see Section 7.9 , user-facing applications often rigid response times; ’t matter high throughput application misses deadline. Thex-axis shows response-time limit, 1.0 cutoff. maximum latency, Catapult 1.95 times fast host Intel server. Catapult Version 1 Deployment populating whole warehouse-scale computer tens thousands servers, Microsoft test deployment 17 full racks, contained 17/C248/C22 1632 Intel servers. Catapult cards network links tested manufacture system integration, deployment, seven 1632 cardsfailed (0.43%), one 3264 FPGA network links (0.03%) defective.After several months deployment, nothing else failed.Feature extraction FSMsFeature- gathering networkHit vector preprocessing FSM Figure 7.23 architecture FPGA implementation Feature Extraction stage. hit vector, describes locations query words document, streamed hit vector preprocessing state machine split control data tokens. tokens issued parallel 43 unique feature state machines. feature-gathering network collects generated feature value pairsand forwards following Free-Form Expressions stage.574 ■Chapter Seven Domain-Specific ArchitecturesCatapult Version 2 Although test deployment successful, Microsoft changed architecture real deployment enable Bing Azure Networking use sameboards architecture ( Caulfield et al., 2016 ). main problem V1 architecture independent FPGA network enable FPGAto see process standard Ethernet/IP packets, prevented beingused accelerate data center network infrastructure. addition, cablingwas expensive complicated, limited 48 FPGAs, rerouting traffic certain failure patterns reduced performance could isolate nodes. solution place FPGA logically CPU NIC, network traffic goes FPGA. “bump-on-a-wire ”placement removes many weaknesses FPGA network Catapult V1. Moreover, itenables FPGAs run low-latency network protocol allows themto treated global pool FPGAs data center even acrossdata centers. Three changes occurred V1 V2 overcome original concerns Catapult applications interfering data center network traffic. First, datacenter network upgraded 10 Gbit/s 40 Gbit/s, increasing headroom.Second, Catapult V2 added rate limiter FPGA logic, ensuring FPGAapplication could overwhelm network. final perhaps most00 0.5 1 Latenc (normalized 95th percentile tar get)95% more95th percentile latency versus throughputThroughput (normalized) 1.5 212345 SoftwareFPGA 29% lower latencyThroughput Figure 7.24 Performance ranking function Catapult given latency bound. x-axis shows response time Bing ranking function. maximum response time 95th percentile Bing application thex-axis 1.0, data points right may higher throughput arrive late useful. y-axis shows 95% throughputs Catapult pure software given response time. normalized response time 1.0, Catapult 1.95 throughput Intel server running pure software mode. Stated alternatively, Cat-apult matches throughput Intel server 1.0 normalized response time, Catapult ’s response time 29% less.7.5 Microsoft Catapult, Flexible Data Center Accelerator ■575important change networking engineers would use cases FPGA, given bump-in-the-wire placement. placement trans- formed former interested bystanders enthusiastic collaborators. deploying Catapult V2 majority new servers, Microsoft essen- tially second supercomputer composed distributed FPGAs shares thesame network wires CPU servers scale, oneFPGA per server. Figures 7.25 and7.26 show block diagram board Catapult V2. Catapult V2 follows shell role split RTL simplify pro- gramming, time publication, shell uses almost half FPGA resources (44%) complicated network protocol shares thedata center network wires. Catapult V2 used Ranking acceleration function network accel- eration. Ranking acceleration, rather perform nearly ranking func-tion inside FPGA, Microsoft implemented compute-intensiveportions left rest host CPU: ■Thefeature functional unit (FFU) collection finite state machines measure standard features search, counting frequency par-ticular search term. similar concept Feature Extraction stage ofCatapult V1. 2-socket server blade TOR40Gb/s 40Gb/sDRAMCPU FPGA NICDRAM CPUDRAM Gen3 2x8Accelerator cardGen3 x8QPI QSFP QSFP QSFP Figure 7.25 Catapult V2 block diagram. network traffic routed FPGA NIC. also PCIe connector CPUs, allows FPGA beused local compute accelerator, Catapult V1.576 ■Chapter Seven Domain-Specific Architectures■Thedynamic programming feature (DPF) creates Microsoft proprietary set features using dynamic programming bears similarity Free- Form Expressions stage Catapult V1. designed use non-local FPGAs tasks, simplifies scheduling. Figure 7.27 shows performance Catapult V2 compared software format similar Figure 7.24 . throughput increased 2.25 /C2 without endangering latency, whereas speedup previously 1.95 /C2. ranking deployed measured production, Catapult V2 bet- ter tail latencies software; is, FPGA latencies never exceeded software latencies given demand despite able absorb twice workload. Summary: Catapult Follows Guidelines Microsoft reported adding Catapult V1 servers pilot phase increased total cost ownership (TCO) less 30%. Thus, appli- cation, net gain cost-performance Ranking least 1.95/1.30, return investment 1.5. Although comment made TCO concerning Catapult V2, board similar number type chips, might guess TCO higher. so, cost-performance Cat- apult V2 2.25/1.30, 1.75 Ranking. Catapult followed guidelines Section 7.2 . 40G QSFP Ports (NIC TOR) Stratix V D5 FPGA 4GB DDR3 Figure 7.26 Catapult V2 board uses PCIe slot. uses FPGA Catapult V1 TDP 32 W. 256-MB Flash chip holds golden image FPGA loaded power on, well one application image.7.5 Microsoft Catapult, Flexible Data Center Accelerator ■5771.Use dedicated memories minimize distance data moved . Altera V FPGA 5 MiB memory on-chip, application customize use. example, CNNs, used input output feature maps Figure 7.21 . 2.Invest resources saved dropping advanced microarchitectural optimi- zations arithmetic units bigger memories . Altera V FPGA also 3926 18-bit ALUs tailored appli-cation. CNNs, used create systolic array drives Pro-cessing Elements Figure 7.22 , form datapaths 60-core multiprocessor used Free Form Expression stage ranking. 3.Use easiest form parallelism matches domain . Catapult picks form parallelism matches application. exam-ple, Catapult uses two-dimensional SIMD parallelism CNN applicationand MISD parallelism Machine Scoring phase stream Ranking. 4.Reduce data size type simplest needed domain . Catapult use whatever size type data application wants, froman 8-bit integer 64-bit floating point. 5.Use domain-specific programming language port code DSA . case, programming done hardware register-transfer language (RTL) Verilog, even less productive language C C++. Micro- soft (and possibly could not) follow guideline given use FPGAs. Although guideline concerns one-time porting application software FPGA, applications frozen time. Almost definition, software evolves add features fix bugs, especially something important web search.001234 0.5 1 Latenc (normalized 99th percentile tar get)Throughput (normalized) 1.5 2FPGA Software Figure 7.27 Performance ranking function Catapult V2 format asFigure 7.24 .Note version measures 99th percentile earlier figure plots 95th percentile.578 ■Chapter Seven Domain-Specific ArchitecturesMaintenance successful programs software ’s development costs. Moreover, programming RTL, software maintenance even burden- some. Microsoft developers, like others use FPGAs accelerators, hopethat future advances domain-specific languages systems hardware-softwareco-design reduce difficulty programming FPGAs. 7.6 Intel Crest, Data Center Accelerator Training quotation Intel CEO opens Section 7.3 came press release announcing Intel going start shipping DSAs ( “accelerants ”) DNN. first example Crest, announced writing edition.Despite limited details, include significance traditionalmicroprocessor manufacturer like Intel taking bold step embracing DSAs. Crest aimed DNN training. Intel CEO said goal accelerate DNN training hundredfold next three years. Figure 7.6 shows train- ing take month. likely demand decrease DNN training eight hours, would 100 times quicker CEO predicted. DNNs surely become even complex next 3 years, willrequire much greater training effort. Thus seems little danger a100/C2improvement training overkill. Crest instructions operate blocks 32 /C232 matrices. Crest uses number format called flex point , scaled fixed-point representation: 32 /C232 matri- ces 16-bit data share single 5-bit exponent provided part theinstruction set. Figure 7.28 shows block diagram Lake Crest chip. compute matrices, Crest uses the12 processing clusters Figure 7.28 . cluster includes large SRAM, big linear algebra processing unit, small amount logic on-and off-chip routing. four 8 GiB HBM2 DRAM modules offer 1 TB/s memorybandwidth, lead attractive Roofline model Crest chip. Inaddition high-bandwidth paths main memory, Lake Crest supports high band-width interconnects directly compute cores inside processing clusters,which facilitates quick core-to-core communication without passing shared memory. Lake Crest ’s goal factor 10 improvement training GPUs. Figure 7.28 shows 12 Inter-Chip Links (ICLs) 2 Inter-Chip Controllers (ICCs), Crest clearly designed allow many Crest chips collaborate, sim-ilar spirit dedicated network connecting 48 FPGAs Catapult. ’s likely 100 /C2improvement training require ganging together several Crest chips. 7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit Pixel Visual Core programmable, scalable DSA intended image processing computer vision Google, initially cell phones tablets running the7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■579Android operating system, potentially Internet Things (IoT) devices. multicore design, supporting 2 16 cores deliver adesired cost-performance. designed either chip partof asystem chip (SOC). much smaller area energy budget TPU cousin. Figure 7.29 lists terms acronyms found section. Pixel Visual Core example new class domain specific architectures vision processing call image processing units (IPUs ). IPUs solve inverse problem GPUs: analyze modify input image contrastto generating output image. call IPUs signal that, DSA, theydo need everything well also CPUs (and GPUs) inthe system perform non-input-vision tasks. IPUs rely stencil computationsmentioned CNNs. innovations Pixel Visual Core include replacing one-dimensional SIMD unit CPUs two-dimensional array processing elements(PEs). provide two-dimensional shifting network PEs awareof two-dimensional spatial relationship elements, two-dimensional version buffers reduces accesses off-chip memory. Thisnovel hardware makes easy perform stencil computations central toboth vision processing CNN algorithms. ISPs, Hardwired Predecessors IPUs portable mobile devices (PMDs) multiple cameras input, hasled hardwired accelerators called image signal processors (ISPs) enhancing8GB HBM2Interposer 8GB HBM28GB HBM2 8GB HBM2HBM PHYMem Ctrlr HBM PHYMem Ctrlr PCl Express ×16PCle Controller & DMASPI, IC2, GPIOMem CtrlrHBM PHY Mem CtrlrHBM PHYMGMT CPU ICCICC ICLICL ICL ICL ICL ICL ICL ICL ICLICL ICL ICLProcessing Cluster Processing Cluster Processing Cluster Processing ClusterProcessing Cluster Processing Cluster Processing Cluster Processing ClusterProcessing Cluster Processing Cluster Processing Cluster Processing Cluster Figure 7.28 Block diagram Intel Lake Crest processor. acquired Intel, Crest said chip almost full reticle TSMC 28 nm, would make die size 600 –700 mm2. chip available 2017. Intel also building Knights Crest, hybrid chip containing Xeon x86 cores Crest accelerators.580 ■Chapter Seven Domain-Specific Architecturesinput images. ISP usually fixed function ASIC. Virtually every PMD today includes ISP. Figure 7.30 shows typical organization image-processing system, including lens, sensor, ISP, CPU, DRAM, display. ISP receivesimages, removes artifacts images lens sensor, interpolates miss-ing colors, significantly improves overall visual quality image.PMDs tend small lens thus tiny noisy pixels, step critical producing high-quality photos videos. ISP processes input image raster scan order calculating series cascading algorithms via software configurable hardware building blocks, typi-cally organized pipeline minimize memory traffic. stage pipe-line clock cycle, pixels input, output.Computation typically performed small neighborhoods pixels(stencils ). Stages connected buffers called line buffers . line buffers helpTerm Acronym Short explanation Core – processor. Pixel Visual Core 2 –16 cores. first implementation 8; also called stencil processor (STP) Halide – domain-specific programming language image processing separates algorithm execution schedule Halo – extended region around 16 /C216 computation array handle stencil computation near borders array. holds values, ’t compute Image signal processorsISP fixed function ASIC improves visual quality image; found virtually PMDs cameras Image processing unitIPU DSA solves inverse problem GPU: analyzes modifies input image contrast generating output image Line buffer pool LB line buffer designed capture sufficient number full lines intermediate image keep next stage busy. Pixel Visual Core uses two-dimensional line buffers, eachChange 64 128 KiB. Line Buffer Pool contains one LB per core plus one LB DMA Network chip NOC network connects cores Pixel Visual CorePhysical ISA pISA Pixel Visual Core instruction set architecture (ISA) executed hardwareProcessing element array– 16 /C216 array Processing Elements plus halo performs 16-bit multiply-add operations. Processing Element includes Vector Lane local memory. shift data en mass neighbors four directions Sheet generator SHG memory accesses blocks 1 /C21t o3 1 /C231 pixels, called sheets . different sizes allow option including space halo Scalar lane SCL operations Vector Lane except adds instructions handle jumps, branches, interrupts, controls instruction flow vector array, schedules allthe loads stores sheet generator. also small instruction memory. plays role scalar processor vector architecture Vector lane VL Portion Processing Element performs computer arithmeticVirtual ISA vISA Pixel Visual Core ISA generated compiler. mapped pISA execution Figure 7.29 handy guide Pixel Visual Core terms Section 7.7 .Figure 7.4 page 437 guide Sections 7.3 –7.6.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■581keep processing stages utilized via spatial locality capturing enough full lines intermediate image facilitate computation required next stage. enhanced image either sent display DRAM storage later processing. ISP also sends statistics image (e.g., color lumahistograms, sharpness, on) CPU, turn processes sendsinformation help system adapt. Although efficient, ISPs two major downsides. Given increasing demand improved image quality handheld devices, first inflexibil-ity ISP, especially takes years design manufacture new ISP within SOC. second computing resources used image-enhancing function, matter needed time thePMD. Current generation ISPs handle workloads 2 Tera-operations persecond PMD power budget, DSA replacement achieve similar per-formance efficiency. Pixel Visual Core Software Pixel Visual Core generalized typical hardwired pipeline organization ker-nels ISP directed acyclic graph (DAG ) kernels. Pixel Visual Core image-processing programs typically written Halide, domain- specific functional programming language image processing. Figure 7.31 Halide example blurs image. Halide functional section expressthe function programmed separate schedule section specify howto optimize function underlying hardware.Output image (Display)Image Img & Stats AWB AE AFSensor (CCD CMOS)Lens DRAM CPUISP BUS Figure 7.30 Diagram showing interconnection Image Signal Processor (ISP), CPU, DRAM, lens, sensor. ISP sends statistics CPU well improved image either display DRAM storage later processing. CPU processes image statistics sends information let system adapt:Auto White Balance (AWB) ISP, Auto Exposure (AE) sensor, Auto Focus (AF) lens, known 3As.582 ■Chapter Seven Domain-Specific ArchitecturesPixel Visual Core Architecture Philosophy power budget PMDs 6 –8 W bursts 10 –20 seconds, dropping tens milliwatts screen off. Given challenging energy goals PMD chip, Pixel Visual Core architecture strongly shaped relative energy costs primitive operations mentioned Chapter 1 made explicit inFigure 7.32 . Strikingly, single 8-bit DRAM access takes much energy 12,500 8-bit additions 7 –100 8-bit SRAM accesses, depending organi- zation SRAM. 22 /C2to 150 /C2higher cost IEEE 754 floating-point operations 8-bit integer operations, plus die size energy benefits ofstoring narrower data, strongly favor using narrow integers whenever algorithmscan accommodate them. addition guidelines Section 7.2 , observations led themes guided Pixel Visual Core design: ■Two-dimensional better one-dimensional : Two-dimensional organiza- tions beneficial processing images minimizes communicationdistance two- three-dimensional nature image data canutilize organizations. ■Closer better farther : Moving data expensive. Moreover, relative cost moving data ALU operation increasing. course DRAMtime energy costs far exceed local data storage movement. primary goal going ISP IPU get reuse hardware via programmability. three main features Pixel Visual Core:Func buildBlur(Func input) { // Functional portion (independent target processor) Func blur_x("blur_x"), blur_y("blur_y"); blur_x(x,y) = (input(x /C01,y) + input(x,y)*2 + input(x+1,y)) / 4; blur_y(x,y) = (blur_x(x,y /C01) + blur_x(x,y)*2 + blur_x(x,y+1)) / 4; (has_ipu) { // Schedule portion (directs optimize target processor)blur_x.ipu(x,y);blur_y.ipu(x,y); } return blur_y; } Figure 7.31 Portion Halide example blur image. Theipu(x,y) suffix schedules function Pixel Visual Core. blur effect looking image translucent screen, reduces noise detail.A Gaussian function often used blur image.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■5831.Following theme two-dimensional better one-dimensional, Pixel Visual Core uses two-dimensional SIMD architecture instead one-dimensional SIMD architecture. Thus two-dimensional array indepen-dent processing element s(PEs), contains 2 16-bit ALUs, 1 16-bit MAC unit, 10 16-bit registers, 10 1-bit predicate registers. 16-bit arith-metic follows guideline providing precision needed domain. 2.Pixel Visual Core needs temporary storage PE. Following guideline Section 7.2 avoiding caches, PE memory compiler-managed scratchpad memory. logical size PE memory 128 entries of16 bits, 256 bytes. would inefficient implement sep- arate small SRAM PE, Pixel Visual Core instead groups PE memory 8 PEs together single wide SRAM block. PEs operate inSIMD fashion, Pixel Visual Core bind individual reads writestogether form “squarer ”SRAM, efficient narrow deep wide shallow SRAMs. Figure 7.33 shows four PEs. 3.To able perform simultaneous stencil computations PEs, Pixel Visual Core needs collect inputs nearest neighbors. communication pat-tern requires “NSEW ”(North, South, East, West) shift network: shift data en masse PEs compass direction. ’t lose pixels along edges shifts images, Pixel Visual Core connects end- points network together form torus. Note shift network contrast systolic array processing element arrays TPU Catapult. case, software explicitly moves data thedesired direction across array, whereas systolic approach hardware-controlled, two-dimensional pipeline tha moves data wavefront invisible software. Pixel Visual Core Halo A3/C23, 5/C25, 7 /C27 stencil going get inputs 1, 2, 3 external pixels edges two-dimensional subset computed (half dimension ofthe stencil minus one-half). leaves two choices. Either Pixel Visual CoreOperation Energy (pJ) Operation Energy (pJ) Operation Energy (pJ) 8b DRAM LPDDR3 125.00 8b SRAM 1.2 –17.1 16b SRAM 2.4 –34.2 32b Fl. Pt. muladd 2.70 8b int muladd 0.12 16b int muladd 0.43 32b Fl. Pt. add 1.50 8b int add 0.01 16b int add 0.02 Figure 7.32 Relative energy costs per operation picoJoules assuming TSMC 28-nm HPM process, process Pixel Visual Core used [17][18][19][20]. absolute energy cost less Figure 7.2 using 28 nm instead 90 nm, relative energy costs similarly high.584 ■Chapter Seven Domain-Specific Architecturesunder utilizes hardware elements near border, pass input values, Pixel Visual Core slightly extends two-dimensional PEs withsimplified PEs leave ALUs. difference size astandard PE simplified PE 2.2 /C2, Pixel Visual Core extended array. extended region called halo.Figure 7.34 shows two rows halo surrounding 8x8 PE array illustrates example 5 /C25 stencil compu- tation upper-left corner relies halo. Processor Pixel Visual Core collection 16 /C216 PEs 4 halo lanes dimension, called PE array orvector array , main computation unit Pixel Visual Core. also load-store unit called Sheet Generator (SHG ). SHG refers memory accesses blocks 1 /C21 256 /C2256 pixels, called sheets . hap- pens downsampling, typical values 16 /C216 20 /C220. implementation Pixel Visual Core even number 2 cores, depending resources available. Thus needs network con-nect together, every core also interface Network Chip(NOC). typical NOC implementation Pixel Visual Core expen-sive cross switch, however, require data travel long distance,which expensive. Leveraging pipeline nature application, NOCtypically needs communicate neighboring cores. implemented asa two-dimensional mesh, allows power gating pairs cores soft- ware control.Mem MemS NMem MemS NE E SW W Figure 7.33 two-dimensional SIMD includes two-dimensional shifting “N,”“S,”“E,”“W,”show direction shift (North, South, East, West). PE software-controlled scratchpad memory.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■585Finally, Pixel Visual Core also includes scalar processor called scalar lane (SCL). identical vector lane, except adds instructions handle jumps, branches, interrupts, controls instruction flow vector array, schedules loads stores sheet generator. also hasa small instruction memory. Note Pixel Visual Core single instructionstream controls scalar vector units, similar CPU core hasa single instruction stream scalar SIMD units.pe pe pe pe pe pe pe pepe pe pe pepepe pe pe pe pe pe pepe pe pe pepe pe pepe pe pe pe pepe pe pepepepepe pe pepe pe pepe pe pe pepe pepepe pe pepe pepe pe pe pepepepepe pe pe pepe pe pe pe pepe pepepe pe pe pe pe pe pe pepepe pepepe pe pepe pepe pe pe pepepepepe pe pe pepe pe pepe pepepe pepe pe pe pe pe pepe pe pepepepepe pe pepe pepe pe pe pepepe pepe pe pe pepe pe pepe pe5 x 5 stencil Figure 7.34 two-dimensional array full processing elements (shown unshaded circles) surrounded two layers simplified processing elements (shaded diamonds) called halo.In figure, 8 /C28 64 full PEs 80 simplified PEs halo. (Pixel Visual Core actually 16 /C216 256 full PEs two layers halo thus 144 simplified PEs.) edges halo connected (shown gray lines) form torus. Pixel Visual Core series two-dimensional shifts across processing elements move neighbor portions stencil com- putation center PE stencil. example 5 /C25 stencil shown upper- left corner. Note 16 25 pieces data 5 /C25 stencil location come halo processing elements.586 ■Chapter Seven Domain-Specific ArchitecturesIn addition cores, also DMA engine transfer data DRAM line buffers efficiently converting image memory layout formats (e.g., packing/unpacking). well sequential DRAM accesses,the DMA engines perform vector-like gather reads DRAM well sequentialand strided reads writes. Pixel Visual Core Instruction Set Architecture Like GPUs, Pixel Visual Core uses two-step compilation process. first step iscompiling programs target language (e.g., Halide) vISA instruc- tions. Pixel Visual Core vISA (virtual Instruction Set Architecture ) inspired part RISC-V instruction set, uses image-specific memory modeland extends instruction set handle image processing, particular, thetwo-dimensional notion images. vISA, two-dimensional array core isinfinite, number register unbounded, memory size similarly unlim-ited. vISA instructions contain pure functions ’t directly access DRAM (seeFigure 7.36 ), greatly simplifies mapping onto hardware. next step compile vISA program pISA (physical Instruction Set Architecture ) program. Using vISA target compilers allows proces- sor software-compatible past programs yet accept changes pISAinstruction set, vISA plays role PTX GPUs (see Chapter 4 ). Lowering vISA pISA takes two steps: compilation mapping early-bound parameters, patching code late-bound parameters.The parameters must bound include STP size, halo size, number STPs, mapping line buffers, mapping kernels processors, well register local memory allocations. Figure 7.35 shows pISA long instruction word (VLIW) instruc- tion set 119-bit-wide instructions. first 43-bit field ScalarLane, next 38-bit field specifies computation two-dimensionalPE array, third 12-bit field specifies memory accesses two-dimensional PE array. last two fields immediates computation oraddressing. operations VLIW fields ’d expect: two ’s complement integer arithmetic, saturating integer arithmetic, logical operations, shifts, data transfers, special ones like divide iteration count lead-ing zeros. Scalar Lane supports superset operations two-dimensional PE array, plus adds instructions control-flow sheet-generator control. 1-bit Predicate registers mentioned enables condi-tional moves registers (e.g., A¼BifC). Field Scalar Math Memory Imm MemImm # Bits 43 38 12 16 10 Figure 7.35 VLIW format 119-bit pISA instruction.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■587Although pISA VLIW instruction wide, Halide kernels short, often 200 –600 instructions. Recall IPU, needs execute compute-intensive portion application, leaving rest function-ality CPUs GPUs. Thus instruction memory Pixel Visual Core holdsjust 2048 pISA instructions (28.5 KiB). Scalar Lane issues sheet generator instructions access line buffers. Unlike memory accesses within Pixel Visual Core, latency morethan 1 clock cycle, DMA-like interface. lane first sets theaddresses transfer size special registers. Pixel Visual Core Example Figure 7.36 shows vISA code output Halide compiler blur example Figure 7.31 , comments added clarity. calculates blur first xdirection direction using 16-bit arithmetic. vISA code matches functional part Halide program. code thoughtof executing across pixels image. Pixel Visual Core Processing Element One architectural decisions big build halo. Pixel Visual Coreuses 16 /C216 PEs, adds halo 2 extra elements, support 5 /C25 // vISA inner loop blur x dimension input.b16 t1 <- _input[x*1+( /C01)][y*1+0][0]; // t1 = input[x /C01,y] input.b16 t2 <- _input[x*1+0][y*1+0][0]; // t2 = input[x,y] mov.b16 st3 <-2 ; mul.b16 t4 <- t2, st3; //t4 = input[x,y] * 2 add.b16 t5 <- t1, t4; //t5 = input[x /C01,y] + input[x,y]*2 input.b16 t6 <- _input[x*1+1][y*1+0][0]; // t6 = input[x+1,y] add.b16 t7 <- t5, t6; //t7 = input[x+1,y]+input[x,y]+input[x /C01,y]*2 mov.b16 st8 <-4 ; div.b16 t9 <- t7, st8; //t9 = t7/4 output.b16 _blur_x[x*1+0][y*1+0][0] <- t9; // blur_x[x,y] = t7/4 // vISA inner loop blur dimension input.b16 t1 <- _blur_x[x*1+0][y*1+( /C01)][0]; // t1 = blur_x[x,y /C01] input.b16 t2 <- _blur_x[x*1+0][y*1+0][0]; // t2 = blur_x[x,y] mov.b16 st3 <-2 ; mul.b16 t4 <- t2, st3; //t4 = blur_x[x,y] * 2 add.b16 t5 <- t1, t4; //t5 = blur_x[x,y /C01] + blur_x[x,y]*2 input.b16 t6 <- _blur_x[x*1+0][y*1+1][0]; // t6 = blur_x[x,y+1] add.b16 t7 <- t5, t6; //t7 = blurx[x,y+1]+blurx[x,y /C01]+blurx[x,y]*2 mov.b16 st8 <-4 ; div.b16 t9 <- t7, st8; //t9 = t7/4 output.b16 _blur_y[x*1+0][y*1+0][0] <- t9; // blur_y[x,y] = t7/4 Figure 7.36 Portion vISA instructions compiled Halide Blur code Figure 7.31 .This vISA code corresponds functional part Halide code.588 ■Chapter Seven Domain-Specific Architecturesstencils directly. Note bigger array PEs, less halo overhead support given stencil size. Pixel Visual Core, smaller size halo PEs 16 /C216 arrays means costs 20% area halo. 5 /C25 stencil, Pixel Visual Core calculate 1.8 times many results per clock cycle (162/122), ratio 1.3 3 /C23 stencil (162/142). design arithmetic unit PE driven multiply-accumulate (MAC), primitive stencil computation. Pixel Visual Core native MACs 16-bits wide multiplies, accumulate 32-bit width. Pipelining MAC would use energy unnecessarily reading writing added pipeline register. Thus multiply-add hardware deter- mines clock cycle. operations, previously mentioned, tradi- tional logical arithmetic operations along saturating versions arithmetic operations specialized instructions. PE two 16-bit ALUs operate variety ways within single clock cycle: ■Independently, producing two 16-bit results: op B, C op D. ■Fused, producing one 16-bit result: op (C op D). ■Joined, producing one 32-bit result: A:C op B:D. Two-Dimensional Line Buffers Controller DRAM accesses use much energy (see Figure 7.32 ), Pixel Visual Core memory system carefully designed minimize number DRAM accesses. key innovation two-dimensional line buffer . Kernels logically running separate cores, connected DAG input sensor DRAM output DRAM. line buffers hold portions image calculated kernels. Figure 7.37 shows logical use line buffers Pixel Visual Core. 2D stencil processor 2D stencil processorDRAM DRAMLens 2D stencil processor2D stencil processorLineBuffer LineBufferLineBuffer LineBufferLineBuffer LineBufferLineBuffer Figure 7.37 Programmer view Pixel Visual Core: directed-acyclic graph kernels.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■589Here four features two-dimensional line buffer must support: 1.It must support two-dimensional stencil computations various sizes, unknown design time. 2.Because halo, 16 /C216 PE array Pixel Visual Core, STPs want read 20 /C220 blocks pixels line buffer write 16 /C216 blocks pixels line buffer. (As previously mentioned, call theseblocks pixels sheets .) 3.Because DAG programmable, need line buffers allocated software two cores. 4.Several cores may need read data line buffer. Thus line buffer support multiple consumers, although needs one producer. Line buffers Pixel Visual Core really multi-reader, two-dimensional FIFO abstraction top relatively large amount SRAM: 128 KiB per instance. Itcontains temporary “images ”that used once, small, dedicated local FIFO much efficient cache data distant memory. accommodate size mismatch reading 20 /C220 blocks pixels writing 16 /C216 blocks, fundamental unit allocation FIFO group 4 /C24 pixels. Per stencil processor, one Line Buffer Pool (LBP) eight logical line buffers ( LB), plus one LBP DMA I/O. LBP three levels abstraction: 1.At top, LBP controller supports eight LBs logical instances. LB one FIFO producer eight FIFO consumers per LB. 2.The controller keeps track set head tail pointers FIFO. Note sizes line buffers inside LBP flexible controller. 3.At bottom many physical memory banks support bandwidth requirements. Pixel Visual Core eight physical memory banks, 128-bit interface 16 KiB capacity. controller LBP challenging must fulfill bandwidth demands STPs I/O DMAs well schedule reads writesto banks physical SRAM memory. LBP controller one mostcomplicated pieces Pixel Visual Core. Pixel Visual Core Implementation first implementation Pixel Visual Core separate chip. Figure 7.38 shows floorplan chip, 8 cores. fabricated TSMC28 nm technology 2016. chip dimensions 6 /C27.2 mm, runs 426 MHz, stacked 512 MB DRAM Silicon Package, consumes590 ■Chapter Seven Domain-Specific Architectures(including DRAM) 187 –4500 mW depending workload. 30% power chip ARMv7 A53 core control, MIPI, PCIe, PCIe, LPDDR interfaces, interface half die 23 mm2. Power Pixel Visual Core running worst case “power virus ”can go high 3200 mW. Figure 7.39 shows floor plan core. Summary: Pixel Visual Core Follows Guidelines Pixel Visual Core multicore DSA image vision processing intended stand-alone chip IP block mobile device SOCs. see Section 7.9 , performance per watt CNNs factors 25 –100 better CPUs GPUs. Pixel Visual core followed guidelines Section 7.2 . 1.Use dedicated memories minimize distance data moved . Perhaps distinguishing architecture feature Pixel Visual Core software-controlled, two-dimensional line buffers. 128 KiB per core, significant fraction area. core also 64 KiB software- controlled PE memory temporary storage. 2.Invest resources saved dropping advanced microarchitectural optimi- zations arithmetic units bigger memories . Two key features Pixel Visual Core 16 /C216 two-dimensional array processing elements per core two-dimensional shifting network processing elements. offers halo region acts buffer allow full utilization 256 arithmetic units. 3 x MIPI-In 2 x MIPI-Out A53 LPDDR4 Pixel Visual Core PCIE 4 x Gen3 Figure 7.38 Floor plan 8-core Pixel Visual Core chip. A53 ARMv7 core. LPDDR4 DRAM controller. PCIE MIPI I/O buses.7.7 Pixel Visual Core, Personal Mobile Device Image Processing Unit ■5913.Use easiest form parallelism matches domain . Pixel Visual Core relies two-dimensional SIMD parallelism using PE array, VLIW express instruction-level parallelism, multiple program multiple data (MPMD) parallelism utilize multiple cores. 4.Reduce data size type simplest needed domain . Pixel Visual Core relies primarily 8-bit 16-bit integers, also works 32-bit integers, albeit slowly. 5.Use domain-specific programming language port code DSA . Pixel Visual Core programmed domain-specific language Halide image processing TensorFlow CNNs. 7.8 Cross-Cutting Issues Heterogeneity System Chip (SOC) easy way incorporate DSAs system I/O bus, approach data center accelerators chapter. avoid fetching memory operands slow I/O bus, accelerators local DRAM. Figure 7.39 Floor plan Pixel Visual Core. left right, top down: sca- lar lane (SCL) 4% core area, NOC 2%, line buffer pool (LBP) 15%, sheet generator (SHG) 5%, halo 11%, processing element array 62%. torus connection halo makes four edges array logical neighbors. area-efficient collapse halo two sides, preserves topology.592 ■Chapter Seven Domain-Specific ArchitecturesAmdahl ’s Law reminds us performance accelerator limited frequency shipping data host memory accelerator mem- ory. surely applications would benefit host CPU theaccelerators integrated system chip (SOC), one goals Pixel Visual Core eventually Intel Crest. design called IP block , standing Intellectual Property , descriptive name might portabl e design block. IP blocks typically specified hardware description language like Verilog VHDL inte-grated SOC. IP blocks enable marketplace many companies make IP blocks companies buy build SOCs appli- cations without desi gn everything themselves. Figure 7.40 indicates importance IP blocks plotting number IP blocks across genera-tions Apple PMD SOCs; tripled four years. Another indication ofthe importance IP blocks CPU GPU get one-third thearea Apple SOCs, IP blocks occupying remainder ( Shao Brooks, 2015 ). Designing SOC like city planning, independent groups lobby limited resources, finding right compromise difficult. CPUs, GPUs, caches, video encoders, adjustable designs shrink orexpand use less area energy deliver less performance.Budgets differ depending whether SOC tablets IoT. Thus anIP block must scalable area, energy, performance. Moreover, espe-cially important new IP block offer small resource version maynot already well-established foothold SOC ecosystem; adoption ismuch easier initial resource request modest. Pixel Visual Core approach multicore design, allowing SOC engineer choose 2 16 cores match area power budget desired performance. A8 (2014 )A7 (2013 )A6 (2012 )# specialized IP blocks A5 (2011 )A4 (2010 )0102030 Figure 7.40 Number IP blocks Apple SOCs iPhone iPad 2010 2014 ( Shao Brooks, 2015 ).7.8 Cross-Cutting Issues ■593It interesting see whether attractiveness integration leads data center processors coming traditional CPU companies IP accel- erators integrated CPU die, whether systems companies continuedesigning accelerators include IP CPUs ASICs. Open Instruction Set One challenge designers DSAs determining collaborate CPU run rest application. ’s going SOC, major decision CPU instruction set choose, recently virtuallyevery instruction set belonged single company. Previously, practical firststep SOC sign contract company lock instruction set. alternative design custom RISC processor port compiler libraries it. cost hassle licensing IP cores led sur-prisingly large number do-it-yourself simple RISC processors SOCs. OneAMD engineer estimated 12 instruction sets modern microprocessor! RISC-V offers third choice: viable free open instruction set plenty opcode space reserved adding instructions domain-specific coprocessors,which enables previously mentioned tighter integration CPUs andDSAs. SOC designers select standard instruction set comes witha large base support software without sign contract. still pick instruction set early design, ’t pick one company sign contract. design RISC-V core them- selves, buy one several companies sell RISC-V IP blocks, download one free open-source RISC-V IP blocks developed byothers. last case analogous open-source software, offers webbrowsers, compilers, operating systems, volunteers maintain forusers download use free. bonus, open nature instruction set improves business case small companies offering RISC-V technology customers ’t worry long-term viability company unique instruction set. Another attraction RISC-V DSAs instruction set important general-purpose processors. DSAs programmed athigher levels using abstractions like DAGs parallel patterns, casefor Halide TensorFlow, less instruction set level.Moreover, world performance-cost energy-cost advances comefrom adding DSAs, binary compatibility may play important role asin past. time writing, future open RISC-V instruction set appears promising. (We wish could peer future learn statusof RISC-V next edition book!)594 ■Chapter Seven Domain-Specific Architectures7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators use DNN domain compare cost-performance accelerators chapter.2We start thorough comparison TPU standard CPUs GPUs add brief comparisons Catapult Pixel Visual Core. Figure 7.41 shows six benchmarks use comparison. consist two examples three types DNNs Section 7.3 . six benchmarks represent 95% TPU inference workload Google data centersin 2016. Typically written TensorFlow, surprisingly short: 100 – 1500 lines code. small pieces larger applications run host server, thousands millions lines C++ code. applicationsare typically user-facing, leads rigid response-time limits, see. Figures 7.42 and7.43 show chips servers compared. server-class computers deployed Google data centers time TPUswere deployed. deployed Google data centers, must least check forinternal memory errors, excluded choices, Nvidia MaxwellGPU. Google purchase deploy them, machines sensibly configured, awkward artifacts assembled solely win benchmarks. traditional CPU server represented 18-core, dual-socket Haswell processor Intel. platform also host server GPUs TPUs. 2This section also largely based upon paper “In-Datacenter Performance Analysis Tensor Processing Unit ” Jouppi et al., 2017 , one book authors coauthor.Name LOCDNN layers Weights TPU Ops/Weight% deployed TPUs 2016 FC Conv Element Pool Total MLP0 100 5 5 20M 20061% MLP1 1000 4 4 5M 168 LSTM0 1000 24 34 58 52M 6429% LSTM1 1500 37 19 56 34M 96 CNN0 1000 16 16 8M 28885% CNN1 1000 4 72 13 89 100M 1750 Figure 7.41 Six DNN applications (two per DNN type) represent 95% TPU ’s workload. 10 columns DNN name; number lines code; types number layers DNN (FC fully connected; Conv convolution; Element element-wise operation LSTM, see Section 7.3 ; Pool pooling, downsizing stage replaces group elements average maximum); number weights; TPU oper-ational intensity; TPU application popularity 2016. operational intensity varies TPU, CPU, GPU batch sizes vary. TPU larger batch sizes still staying response time limit. One DNN RankBrain ( Clark, 2015 ), one LSTM GNM Translate ( Wu et al., 2016 ), one CNN DeepMind AlphaGo (Silver et al., 2016; Jouppi, 2016 ).7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators ■595Haswell fabricated Intel 22-nm process. CPU GPU large dies: 600 mm2! GPU accelerator Nvidia K80. K80 card contains two dies offers SECDED internal memory DRAM. Nvidia states ( Nvidia, 2016 ) K80 Accelerator dramatically lowers datacenter cost delivering applica- tion performance fewer, powerful servers. DNN researchers frequently used K80s 2015, deployed Google. Note K80s also chosen new cloud-based GPUs AmazonWeb Services Microsoft Azure late 2016. number dies per benchmarked server varies 2 8, following figures show results normalized per die, except Figure 7.50 , compares performance/watt whole servers. Performance: Rooflines, Response Time, Throughput illustrate performance six benchmarks three processors, weadapt Roofline performance model Chapter 4 . use Roofline model TPU, DNN applications quantized, first replace floating-pointoperations integer multiply-accumulate operations. weights notnormally fit on-chip memory DNN applications, second change toredefine operational intensity integer operations per byte weights read(Figure 7.41 ).Chip model mm2nm MHz TDPMeasured TOPS/s GB/s On-chip memory Idle Busy 8b FP Intel Haswell 662 22 2300 145W 41W 145W 2.6 1.3 51 51 MiB NVIDIA K80 561 28 560 150W 25W 98W – 2.8 160 8 MiB TPU <331* 28 700 75W 28W 40W 92 – 34 28 MiB *The TPU die size less half Haswell die size. Figure 7.42 chips used benchmarked servers Haswell CPUs, K80 GPUs, TPUs. Haswell 18 cores, K80 13 SMX processors. Server Dies/Server DRAM TDPMeasured power Idle Busy Intel Haswell 2 256 GiB 504W 159W 455W NVIDIA K80 (2 dies/card) 8 256 GiB (host)+12 GiB /C28 1838W 357W 991W TPU 4 256 GiB (host)+8 GiB /C24 861W 290W 384W Figure 7.43 Benchmarked servers use chips Figure 7.42 .The low-power TPU allows better rack-level density high-power GPU. 8 GiB DRAM per TPU Weight Memory.596 ■Chapter Seven Domain-Specific ArchitecturesFigure 7.44 shows Roofline model single TPU log-log scales. TPU long “slanted ”part Roofline, operational intensity means performance limited memory bandwidth rather peak compute. Five six applications happily bumping heads ceiling: MLPs LSTMs memory-bound, CNNs computation-bound.The single DNN bumping head ceiling CNN1. DespiteCNNs high operational intensity, CNN1 running 14.1 TeraOperations Per Second (TOPS), CNN0 runs satisfying 86 TOPS. readers interested deep dive happened CNN1, Figure 7.45 uses performance counters give partial visibility utilization TPU. TPU spends less half cycles performing matrix oper- ations CNN1 (column 7, row 1). active cycles, half 65,536 MACs hold useful weights layers CNN1 shal-low feature depths. 35% cycles spent waiting weights load frommemory matrix unit, occurs four fully connected layersthat run operational intensity 32. leaves roughly 19% cycles not100 1000 10.1 100.5151050RooflineTPU log-log TeraOps/sec (log scale)LSTM0 LSTM1 MLP1MLP0 CNN0 CNN114.186.0 12.3 9.7 2.83.7 Operational intensit y: MAC Ops/wei ght byte (log scale ) Figure 7.44 TPU Roofline. ridge point far right 1350 multiply- accumulate operations per byte weight memory. CNN1 much Roofline DNNs spends third time waiting weights loaded matrix unit shallow depth somelayers CNN results half elements within matrix unit holding use-ful values ( Jouppi et al., 2017 ).7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators ■597explained matrix-related counters. overlapped execution TPU, exact accounting cycles, see 23% ofcycles stalls RAW dependences pipeline 1% spent stalled input PCIe bus. Figures 7.46 and7.47 show Rooflines Haswell K80. six NN applications generally ceilings TPU Figure 7.44 . Response-time limits reason. Many DNN applications parts ofservices part end-user-facing services. Researchers demonstratedthat small increases response time cause customers use service less (seeChapter 6 ). Thus, although training may hard response-time deadlines, inference usually does. is, inference cares throughput maintaining latency bound. Figure 7.48 illustrates impact 99th percentile response-time limit 7 ms MLP0 Haswell K80, required applicationdeveloper. (The inferences per second 7-ms latency include server hosttime well accelerator time.) operate 42% 37%, respec-tively, highest throughput achievable MLP0, response-time limitis relaxed. Thus, although CPUs GPUs potentially much higher through-put, ’s wasted ’t meet response-time limit. bounds affect TPU well, 80% Figure 7.48 , operating much closer highest MLP0 throughput. compared CPUs GPUs, single-threaded TPUhas none sophisticated microarchitectural features discussed Section 7.1 consume transistors energy improve average case 99th-percentile case.Application MLP0 MLP1 LSTM0 LSTM1 CNN0 CNN1 Mean Row Array active cycles 12.7% 10.6% 8.2% 10.5% 78.2% 46.2% 28% 1 Useful MACs 64K matrix (% peak) 12.5% 9.4% 8.2% 6.3% 78.2% 22.5% 23% 2Unused MACs 0.3% 1.2% 0.0% 4.2% 0.0% 23.7% 5% 3Weight stall cycles 53.9% 44.2% 58.1% 62.1% 0.0% 28.1% 43% 4Weight shift cycles 15.9% 13.4% 15.8% 17.1% 0.0% 7.0% 12% 5Non-matrix cycles 17.5% 31.9% 17.9% 10.3% 21.8% 18.7% 20% 6RAW stalls 3.3% 8.4% 14.6% 10.6% 3.5% 22.8% 11% 7Input data stalls 6.1% 8.8% 5.1% 2.4% 3.4% 0.6% 4% 8TeraOp/s (92 Peak) 12.3 9.7 3.7 2.8 86.0 14.1 21.4 9 Figure 7.45 Factors limiting TPU performance NN workload based hardware performance counters. Rows 1, 4, 5, 6 total 100% based measurements activity matrix unit. Rows 2 3 furtherbreak fraction 64K weights matrix unit hold useful weights active cycles. counters cannot exactly explain time matrix unit idle row 6; rows 7 8 show counters two possible reasons, including RAW pipeline hazards PCIe input stalls. Row 9 (TOPS) based measurements productioncode rows based performance-counter measurements, perfectly consistent. Host server overhead excluded here. MLPs LSTMs memory-bandwidth limited, CNNs not. CNN1 results explained text.598 ■Chapter Seven Domain-Specific ArchitecturesFigure 7.49 gives bottom line relative inference performance per die, including host server overhead two accelerators. Recall architectsuse geometric mean ’t know actual mix programs run. comparison, however, doknow mix ( Figure 7.41 ). TheHaswell log-log TeraOps/sec (log scale)Roofline LSTM0 LSTM1 MLP1MLP0 CNN0 CNN1 100 1000 10.1 100.60.6 0.51.1 0.20.3 Operational intensit y: MAC Ops/wei ght byte (log scale )0.20.40.60.812 Figure 7.46 Intel Haswell CPU Roofline ridge point 13 multiply-accumulate operations/byte, much left Figure 7.44 . K80 log-log TeraOps/sec (log scale)Roofline LSTM0 LSTM1 MLP1MLP0 CNN0 CNN10.9 0.70.7 0.5 0.21.0 100 1000 10.2 10 Operational intensit y: MAC Ops/wei ght byte (log scale )0.40.60.812 Figure 7.47 NVIDIA K80 GPU die Roofline. much higher memory bandwidth moves ridge point 9 multiply-accumulate operations per weight byte, even left Figure 7.46 .7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators ■599weighted mean last column Figure 7.49 using actual mix makes GPU 1.9 times, TPU 29.2 times fast CPU, TPU is15.3 times fast GPU. Cost-Performance, TCO, Performance/Watt buying computers thousands, cost-performance trumps general per- formance. best cost metric data center total cost ownership (TCO). actual price Google pays thousands chips depends negotiationsbetween companies involved. business reasons, Google ’t publish price information data might let deduced. However, power cor-related TCO, Google publish watts per server, use performance/watt proxy performance/TCO. section, compare servers(Figure 7.43 ) rather single dies ( Figure 7.42 ). Figure 7.50 shows weighted mean performance/watt K80 GPU TPU relative Haswell CPU. present two different calculations perfor- mance/watt. first ( “total”) includes power consumed host CPU server calculating performance/watt GPU TPU. second(“incremental ”) subtracts host CPU server power total GPU TPU beforehand.Type Batch 99th% response Inf/s (IPS) % max IPS CPU 16 7.2 ms 5482 42% CPU 64 21.3 ms 13,194 100%GPU 16 6.7 ms 13,461 37%GPU 64 8.3 ms 36,465 100%TPU 200 7.0 ms 225,000 80%TPU 250 10.0 ms 280,000 100% Figure 7.48 99th% response time per die throughput (IPS) MLP0 batch size varies. longest allowable latency 7 ms. GPU TPU, maximum MLP0 throughput limited host server overhead. Type MLP0 MLP1 LSTM0 LSTM1 CNN0 CNN1 Mean GPU 2.5 0.3 0.4 1.2 1.6 2.7 1.9 TPU 41.0 18.5 3.5 1.2 40.3 71.0 29.2Ratio 16.7 60.0 8.0 1.0 25.4 26.3 15.3 Figure 7.49 K80 GPU TPU performance relative CPU DNN workload. mean uses actual mix six applications Figure 7.41 . Relative performance GPU TPU includes host server overhead. Figure 7.48 corresponds second column table (MLP0), showing relative IPS meet 7-ms latency threshold.600 ■Chapter Seven Domain-Specific ArchitecturesFor total-performance/watt, K80 server 2.1 /C2Haswell. incremental- performance/watt, Haswell power omitted, K80 server 2.9 /C2. TPU server 34 times better total-performance/watt Haswell, makes TPU server 16 times performance/watt K80 server.The relative incremental-performance/watt —which Google ’s justification custom ASIC —is 83 TPU, lifts TPU 29 times per- formance/watt GPU. Evaluating Catapult Pixel Visual Core Catapult V1 runs CNNs 2.3 /C2as fast 2.1 GHz, 16-core, dual-socket server (Ovtcharov et al., 2015a ). Using next generation FPGAs (14-nm Arria 10), performance goes 7 /C2, perhaps even 17 /C2with careful floorplan- ning scaling Processing Elements ( Ovtcharov et al., 2015b ). cases, Catapult increases power less 1.2 /C2. Although ’s apples versus oranges, TPU runs CNNs 40 /C2to 70 /C2versus somewhat faster server (seeFigures 7.42, 7.43 , 7.49). Pixel Visual Core TPU made Google, good news directly compare performance CNN1, common DNN, although translated TensorFlow. runs batch size 1 instead 32 TPU. TPU runs CNN1 50 times fast PixelVisual Core, makes Pixel Visual Core half fast GPU alittle faster Haswell. Incremental performance/watt CNN1 raises PixelVisual Core half TPU, 25 times GPU, 100 times CPU. Performance/Watt relative CPU GPU Incremental performance/WattTPU/GPU Total performance/Watt0255075100 2.983 29 2.134 16TPU/CPU GPU/CPU Figure 7.50 Relative performance/watt GPU TPU servers CPU GPU servers. Total performance/watt includes host server power, incremental ’t. widely quoted metric, use proxy performance/TCO data center.7.9 Putting Together: CPUs Versus GPUs Versus DNN Accelerators ■601Because Intel Crest designed training rather inference, ’tb e fair include section, even available measure. 7.10 Fallacies Pitfalls early days DSAs DNNs, fallacies abound. Fallacy costs $100 million design custom chip . Figure 7.51 shows graph article debunks widely quoted $100- million myth “only”$50 million, cost salaries (Olofsson, 2011 ). Note author ’s estimate sophisticated processors include features DSAs definition omit, even noimprovement development process, would expect cost DSAdesign less. optimistic six years later, when, anything, mask costs even higher smaller process technologies? First, software largest category, almost third cost. avail- ability applications written domain-specific languages allows compilers todo work porting applications DSA, saw TPUand Pixel Visual Core. open RISC-V instruction set also help reduce thecost getting system software well cut large IP costs. Mask fabrication costs saved multiple projects share single reticle. long small chip, amazingly enough, $30,000 anyone get 100 untested parts 28-nm TSMC technology ( Patterson Nikoli /C19c, 2015 ). SOFTWARE, $15,750,000 HARDWARE, $13,500,000 EDA TOOLS, $9,000,000 FABRICATION, $5,000,000IP, $5,000,000Sales+ Management, $4,500,000 Figure 7.51 breakdown $50 million cost custom ASIC came surveying others ( Olofsson, 2011 ).The author wrote company spent $2 million ASIC.602 ■Chapter Seven Domain-Specific ArchitecturesPerhaps biggest change hardware engineering, quarter cost. Hardware engineers begun follow software col- leagues use agile development. traditional hardware process hasseparate phases design requirements, architecture, logical design, layout, ver-ification, on, also uses different job titles people performeach phases. process heavy planning, documentation, sched-uling part change personnel phase. Software used follow “waterfall ”model well, projects com- monly late, budget, even canceled led radically different approach. Agile Manifesto 2001 basically said much likely small team iterated incomplete working prototype shown reg-ularly customers would produce useful software schedule budget morethan traditional plan-and-document approach waterfall process would. Small hardware teams agile iterations ( Lee et al., 2016 ). ameliorate long latency chip fabrication, engineers iterations using FPGAsbecause modern design systems produce EDIF FPGAs chiplayout single design. FPGA prototypes run 10 –20 times slower chips, still much faster simulators. also “tape-in ”iterations, work tape-out working incomplete prototype, butyou ’t pay costs fabricating chip. addition improved development process, modern hardware design languages support ( Bachrach et al., 2012 ), advances automatic gen- eration hardware high-level domain-specific languages ( Canis et al., 2013; Huang et al., 2016; Prabhakar et al., 2016 ). Open source cores download free modify also lower cost hardware design. Pitfall Performance counters added afterthought DSA hardware . TPU 106 performance counters, designers wanted even (see Figure 7.45 ). raison ’^etreforDSAs performance, way early evolution good idea going on. Fallacy Architects tackling right DNN tasks . architecture community paying attention deep learning: 15% papers ISCA 2016 hardware accelerators DNNs! Alas, nine papers looked CNNs, two mentioned DNNs. CNNs complexthan MLPs prominent DNN competitions ( Russakovsky et al., 2015 ), might explain allure, 5% Google datacenter NN workload. seems wise try accelerate MLPs LSTMs leastas much gusto. Fallacy DNN hardware, inferences per second (IPS) fair summary performance metric . IPS appropriate single, overall performance summary DNN hardware ’s simply inverse complexity typical inference application (e.g., number, size, type NN layers). example, the7.10 Fallacies Pitfalls ■603TPU runs 4-layer MLP1 360,000 IPS 89-layer CNN1 4700 IPS; thus TPU IPS varies 75X! Therefore using IPS single-speed sum- mary much misleading NN accelerators MIPS FLOPS fortraditional processors, IPS even disparaged. compareDNN machines better, need benchmark suite written high level portit wide variety DNN architectures. Fathom promising new attemptat benchmark suite ( Adolf et al., 2016 ). Pitfall ignorant architecture history designing DSA . Ideas ’t fly general-purpose computing may ideal DSAs, thus history-aware architects could competitive edge. TPU, three impor-tant architectural features date back early 1980s: systolic arrays ( Kung Leiserson, 1980 ), decoupled-access/execute ( Smith, 1982b ), CISC instruc- tions ( Patterson Ditzel, 1980 ). first reduced area power large Matrix Multiply Unit, second fetched weights concurrently operation ofthe Matrix Multiply Unit, third better utilized limited bandwidth thePCIe bus delivering instructions. advise mining historical perspectives sections end every chapter book discover jewels could embellish DSAs design. 7.11 Concluding Remarks chapter, ’ve seen several commercial examples recent shift traditional goal improving general-purpose computers programs benefit accelerating subset programs DSAs. versions Catapult preserved data-center homogeneity designing small, low-power FPGA board could fit inside server. hope thatthe flexibility FPGAs allow Catapult useful many current applica-tions new ones appeared deployment. Catapult runs search rankand CNNs faster GPUs, offering 1.5 –1.75 gain performance/TCO ranking CPUs. TPU project actually began FPGAs abandoned designers concluded FPGAs time competitive perfor- mance compared GPUs. also believed TPU would use much lesspower GPUs, fast faster, potentially making TPU muchbetter FPGAs GPUs. Finally, TPU device broke datacenter homogeneity Google servers data centers already hadGPUs. TPU basically followed footsteps GPU justanother type accelerator. nonrecurring engineering costs likely much higher TPU Catapult, rewards also greater: performance performance/ watt much higher ASIC FPGA. risk TPUwas appropriate DNN inference, mentioned, DNNs attrac-tive target potentially used many applications. 2013604 ■Chapter Seven Domain-Specific ArchitecturesGoogle ’s management took leap faith trusting DNN requirements 2015 beyond would justify investment TPU. deterministic execution model Catapult TPU better match response-time deadline user-facing applications thetime-varying optimizations CPUs GPUs (caches, out-of-order execution,multithreading, multiprocessing, prefetching, etc.) help average throughputmore latency. lack features helps explain why, despite havingmyriad ALUs big memory, TPU relatively small low powered.This achievement suggests “Cornucopia Corollary ”to Amdahl ’s Law: low uti- lization huge, cheap resource still deliver high, cost-effective performance . summary, TPU succeeded DNNs large matrix unit; substantial software-controlled on-chip memory; ability run whole inferencemodels reduce dependence host CPU; single-threaded, deterministic exe-cution model proved good match 99th-percentile response-timelimits; enough flexibility match DNNs 2017 well 2013; omis-sion general-purpose features enabled small low-power die despite larger datapath memory; use 8-bit integers quantized applications; fact applications written using TensorFlow, made easy toport DSA high-performance rather rewrite inorder run well different hardware. Pixel Visual Core demonstrated constraints designing DSA PMD terms die size power. Unlike TPU, separate processor thehost fetches instructions. Despite aimed primarily computervision, Pixel Visual Core run CNNs one two orders magnitude better performance/watt K80 GPU Haswell CPU. It’s early render judgment Intel Crest, although enthusiastic announcement Intel CEO signals shift computing landscape. Architecture Renaissance least past decade, architecture researchers publishing innova-tions based simulations using limited benchmarks claiming improvements forgeneral-purpose processors 10% less companies reporting gains DSA hardware products 10 times . think sign field undergoing transformation, expect see renaissance architecture innovation next decade ■the historic end Dennard scaling Moore ’s Law, means improving cost-energy-performance require innovation computerarchitecture; ■the productivity advances building hardware Agile hardwaredevelopment new hardware design languages leverage advances inmodern programming languages;7.11 Concluding Remarks ■605■the reduced cost hardware development free open instruction sets, open-source IP blocks, commercial IP blocks (which far DSAs found); ■the improvements mentioned productivity cost development means researchers afford demonstrate ideas building FPGAs even custom chips, instead trying convince skeptics withsimulators; ■the potential upside DSAs synergy domain-specific program-ming languages. believe many architecture researchers build DSAs raise bar still higher discussed chapter. ’t wait see computer architecture world look like next edition book! 7.12 Historical Perspectives References Section M.9 (available online) covers development DSAs. Case Studies Exercises Cliff Young Case Study: Google ’s Tensor Processing Unit Acceleration Deep Neural Networks Concepts illustrated case study ■Structure matrix multiplication operations ■Capacities memories rates computations ( “speeds feeds ”) simple neural network model ■Construction special-purpose ISA ■Inefficiencies mapping convolutions TPU hardware ■Fixed-point arithmetic ■Function approximation 7.1 [10/20/10/25/25] <7.3,7.4 >Matrix multiplication key operation supported hardware TPU. going details TPU hardware, ’s worth analyzing matrix multiplication calculation itself. One common way depictmatrix multiplication following triply nested loop: float a[M][K], b[K][N], c[M][N]; // M, N, K constants. (int = 0; <M; ++i) (int j = 0; j <N; ++j) (int k = 0; k <K; ++k) c[i][j] += a[i][k] * b[k][j];606 ■Chapter Seven Domain-Specific Architecturesa.[10] Suppose M, N, K equal. asymptotic complexity time algorithm? asymptotic complexity space arguments? mean operational intensity matrix multi-plication M, N, K grow large? b.[20] Suppose M=3, N=4, K=5, dimensions relatively prime. Write order accesses memory locations eachof three matrices A, B, C (you might start two-dimensional indices,then translate memory addresses offsets start eachmatrix). matrices elements accessed sequentially? Whichare not? Assume row-major (C-language) memory ordering. c.[10] Suppose transpose matrix B, swapping indices B[N][K] instead. So, innermost statement loop looks like: c[i][j] += a[i][k] * b[j][k]; Now, matrices elements accessed sequentially? d.[25] innermost (k-indexed) loop original routine performs dot-product operation. Suppose given hardware unit perform 8-element dot-product efficiently raw C code, behav-ing effectively like C function: void hardware_dot(float *accumulator, const float *a_slice, const float *b_slice) { float total = 0.;for (int k = 0; k <8; ++k) { total += a_slice[k] * b_slice[k]; }*accumulator += total; } would rewrite routine transposed B matrix part (c) use function? e.[25] Suppose instead, given hardware unit performs 8-element “saxpy ”operation, behaves like C function: void hardware_saxpy(float *accumulator, float a, const float *input) { (int k = 0; k <8; ++k) { accumulator[k] += * input[k]; } } Write another routine uses saxpy primitive deliver equivalent results original loop, without transposed memory ordering B matrix. 7.2 [15/10/10/20/15/15/20/20] <7.3,7.4 >Consider neural network model MLP0 Figure 7.5 . model 20 weights five fully connected layers (neural network researchers count input layer layer stack, noCase Studies Exercises Cliff Young ■607weights associated it). simplicity, let ’s assume layers size, layer holds 4 weights. assume layer identical geometry, group 4 weights represents 2 K *2 K matrix. TPU typically uses 8-bit numerical values, 20 weights take 20 MB. a.[15] batch sizes 128, 256, 512, 1024, 2048, big input activations layer model (which, except input layer, arealso output activations previous layer)? considering wholemode (i.e., ’s input first layer output last layer), batch size, transfer time input output overPCIe Gen3 x16, transfer speed 100 Gibit/s? b.[10] Given memory system speed 30 GiB/s, give lower bound time TPU takes read weights MLP0 memory. much timedoes take TPU read 256 /C2256“tile”of weights memory? c.[10] Show calculate TPU ’s 92 operations/second, given know systolic array matrix multiplier 256 /C2256 elements, performs 8-bit multiply-accumulate operation (MAC) cycle. high-performance-computing marketing terms, MAC counts two operations. d.[20] weight tile loaded matrix unit TPU, reused multiply 256-element input vector 256 /C2256 weight matrix represented tile produce 256-element output vector every cycle.How many cycles pass time takes load weight tile? isthe“break-even ”batch size, compute memory-load times equal, also known “ridge ”of roofline. e.[15] compute peak Intel Haswell x86 server 1 FLOPS, compute peak NVIDIA K80 GPU 3 FLOPS. Sup-posing hit peak numbers, calculate best-case compute timefor batch size 128. times compare time TPU takes toload 20 weights memory? f.[15] Assuming TPU program overlap computation I/O PCIe, calculate time elapsed CPU starts send firstbyte data TPU time last byte output returned. fraction PCIe bandwidth used? g.[20] Suppose deployed configuration one CPU connected five TPUs across single PCIe Gen3 x16 bus (with appropriate PCIe switches). Assume parallelize placing one layer MLP0 eachTPU, TPUs communicate directly PCIe.At batch=128, best-case latency calculating single inference,and throughput, terms inferences per second, would config-uration deliver? compare single TPU? h.[20] Suppose example batch inferences requires 50 core-micro- seconds processing time CPU. many cores host CPU willbe required drive single-TPU configuration batch=128? 7.3 [20/25/25/25/Discussion] <7.3,7.4 >Consider pseudo-assembly language TPU, consider program handles batch size 2048 tiny fully608 ■Chapter Seven Domain-Specific Architecturesconnected layer 256 /C2256 weight matrix. constraints sizes alignments computations instruction, entire program layer might look like following: read_host u#0, 256*2048 read_weights w#0, 256*256 // matmul weights implicitly read FIFO. activate u#256*2048, a#0, 256*2048write_host, u#256*2048, 256*2048 pseudo-assembly language, prefix “u#”refers memory address unified buffer; prefix “w#”refers memory address off-chip weight DRAM, prefix “a#”refers accumulator address. last argument assembly instruction describes number bytes beoperated upon. Let’s walk program instruction instruction: ■The read_host instruction reads 512 KB data host memory, storing beginning unified buffer (u#0). ■The read_weights instruction tells weight fetching unit read 64 KB ofweights, loading on-chip weight FIFO. 64 KB weightsrepresent single, 256 /C2256 matrix weights, call “weight tile.” ■The matmul instruction reads 512 KB input data address 0 unified buffer, performs matrix multiplication tile weights, andstores resulting 256 *2048=524,288, 32-bit activations accumulator address 0 (a#0). intentionally glossed details ordering weights; exercise expand details. ■The activate instruction takes 524,288 32-bit accumulators a#0, applies activation function them, stores resulting 524,288, 8-bit output values next free location unified buffer, u#524288. ■The write_host instruction writes 512 KB output activations, starting atu#524288, back host CPU. progressively add realistic details pseudo-assembly language explore aspects TPU design. a.[20] written pseudo-code terms bytes byte addresses (or case accumulators, terms addresses 32-bit values), TPU operates natural vector length 256. means unified buffer typically addressed 256-byte boundaries, accumula-tors addressed groups 256 32-bit values (or 1 KB boundaries), andweights loaded groups 65,536 8-bit values. Rewrite program ’s addresses transfer sizes take vector weight-tile lengths intoaccount. many 256-element vectors input activations readby program? many bytes accumulated values used whilecomputing results? many 256-element vectors output activations written back host?Case Studies Exercises Cliff Young ■609b.[25] Suppose application requirements change, instead multiplication 256 /C2256 weight matrix, shape weight matrix becomes 1024 /C2256. Think matmul instruction putting weights right argument matrix multiplication operator, 1024corresponds K, dimension matrix multiplication adds upvalues. Suppose two variants accumulate instruction,one overwrites accumulators results, ofwhich adds matrix multiplication results specified accumulator.How would change program handle 1024 /C2256 matrix? need accumulators? size matrix unit remains 256 /C2256; many 256 /C2256 weight tiles program need? c.[25] write program handle multiplication weight matrix size 256 /C2512. program need accumulators? rewrite program uses 2048, 256-entry accumulators? manyweight tiles program need? order stored inthe weight DRAM? d.[25] Next, write program handle multiplication weight matrix size 1024 /C2768. many weight tiles program need? Write program uses 2048, 256-entry accumulators. ordershould weight tiles stored weight DRAM? calculation,how many times input activation get read? e.[Discussion] would take build architecture reads 256-element set input activations once? many accumulators wouldthat require? way, big would accumulator memory be? Contrast approach TPU, uses 4096 accumula- tors, one set 2048 accumulators written matrix unitwhile another used activations. 7.4 [15/15/15] <7.3,7.4 >Consider first convolutional layer AlexNet, uses 7 /C27 convolutional kernel, input feature depth 3 output feature depth 48. original image width 220 /C2220. a.[15] Ignore 7 /C27 convolutional kernel moment, consider center element kernel. 1 /C21 convolutional kernel mathemati- cally equivalent matrix multiplication, using weight matrix isinput_depth /C2output_depth dimensions. depths, using stan- dard matrix multiplication, fraction TPU ’s 65,536 ALUs used? b.[15] convolutional neural networks, spatial dimensions also sources weight reuse, since convolutional kernel gets applied many different(x,y) coordinate positions. Suppose TPU reaches balanced compute memory batch size 1400 (as might computed exercise 1d). smallest square image size TPU process efficiently abatch size 1? c.[15] first convolutional layer AlexNet implements kernel stride 4, means rather moving one X pixel application,610 ■Chapter Seven Domain-Specific Architecturesthe 7 /C27 kernel moves 4 pixels time. striding means permute input data 220 /C2220/C23t ob e5 5 /C255/C248 (dividing X dimensions 4 multiplying input depth 16), simulta-neously restack 7 /C27/C23/C248 convolutional weights 2/C22/C248/C248 (just input data gets restacked 4 X Y, 7 /C27 elements convolutional kernel, ending ceiling(7/4) =2 elements X dimensions). thekernel 2 /C22, need perform four matrix multiplication oper- ations, using weight matrices size 48 /C248. fraction 65,536 ALUs used now? 7.5 [15/10/20/20/20/25] <7.3>The TPU uses fixed-point arithmetic (sometimes also called quantized arithmetic , overlapping conflicting definitions), integers used represent values real number line. number different schemes fixed-point arithmetic, share common theme thatthere affine projection integer used hardware real numberthat integer represents. affine projection form r=i *s+b, integer, r represented real value, b scale bias. canof course write projection either direction, integers reals vice versa(although need round converting reals integers). a.[15] simplest activation function supported TPU “ReLUX, ”which rectified linear unit maximum X. example, ReLU6 defined byRelu6(x)={ 0, x <0; x, 0 <=x<=6; 6, x >6} .S o0 . 0a n 6.0 real number line minimum maximum values Relu6 might produce. Assume use 8-bit unsigned integer hardware,and want make 0 map 0.0 255 map 6.0. Solve b. b.[10] many values real number line exactly representable 8-bit quantized representation ReLU6 output? real-numberspacing them? c.[20] difference representable values sometimes called “unit least place, ”orulp, performing numerical analysis. map real number fixed-point representation, map back, rarely getback original real number. difference original numberand representation called quantization error . mapping real number range [0.0,6.0] 8-bit integer, show worst-case quantization error one-half ulp (make sure round nearest rep- resentable value). might consider graphing errors function theoriginal real number. d.[20] Keep real-number range [0.0,6.0] 8-bit integer last step. 8-bit unsigned integer represents 1.0? quantization error for1.0? Suppose ask TPU add 1.0 1.0. answer getback, error result? e.[20] pick random number uniformly range [0.0, 6.0], quantize 8-bit unsigned integer, distribution would expectto see 256 integer values?Case Studies Exercises Cliff Young ■611f.[25] hyperbolic tangent function, tanh, another commonly used activation function deep learning: tanh xðÞ¼1/C0e/C02x 1+e/C02x Tanh also bounded range, mapping entire real number line interval /C01:0,1:0 ðÞ . Solve b range, using 8-bit unsigned representation. solve b using 8-bit two ’s complement representation. cases, real number integer 0 represent? integer represents thereal number 0.0? imagine issues might result quantiza-tion error incurred representing 0.0? 7.6 [20/25/15/15/30/30/30/40/40/25/20/Discussion] <7.3>In addition tanh, another s-shaped smooth function, logistic sigmoid function y=1 / (1+exp( /C0x)), logistic _sigmoid x ðÞ ¼1 1+e/C0x commonly used activation function neural networks. common way implement fixed-point arithmetic uses piecewise quadratic approxima-tion, significant bits input value select table entry touse. least significant bits input value sent degree-2 polyno-mial describes parabola fit subrange approximated function. a.[20] Using graphing tool (we like www.desmos.com/calculator ), draw graphs logistic sigmoid tanh functions. b.[25] draw graph y=tanh( x/2)/2. Compare graph logis- tic sigmoid function. much differ by? Build equation shows transform one other. Prove equation correct. c.[15] Given algebraic identity, need use two different sets coef- ficients approximate logistic sigmoid tanh? d.[15] Tanh odd function, meaning f( /C0x)=/C0f(x). exploit fact save table space? e.[30] Let ’s focus attention approximating tanh interval x20:0, 6:4 ½/C138 number line. Using floating-point arithmetic, write pro- gram divides interval 64 subintervals (each length 0.1), approximates value tanh subinterval using single constant floating-point value (so ’ll need pick 64 different floating-point values, one subinterval). spot-check 100 different values (randomlychosen fine) within subinterval, worst-case approximationerror see subintervals? choose constant minimizethe approximation error subinterval? f.[30] consider building floating-point linear approximation sub- interval. case, want pick pair floating-point values mandb, traditional line equation y¼mx+b, approximate 64 sub- intervals. Come strategy think reasonable build lin- ear interpolation 64 subintervals tanh. Measure worst-caseapproximation error 64 intervals. approximation monotonicwhen reaches boundary subintervals?612 ■Chapter Seven Domain-Specific Architecturesg.[40] Next, build quadratic approximation, using standard formula y¼ax2+bx+c:Experiment number different ways fit formula. Try fitting parabola endpoints midpoint bucket, using aTaylor approximation around single point bucket. worst-caseerror get? h.[40] (extra credit) Let ’s combine numerical approximations exercise fixed-point arithmetic previous exercise. Suppose inputx20:0, 6:4 ½/C138 represented 15-bit unsigned value, 0x0000 represent- ing 0.0 0x7FFF representing 6.4. output, similarly use 15-bitunsigned value, 0x0000 representing 0.0 0x7FFF representing 1.0.For constant, linear, quadratic approximations, calculate thecombined effect approximation quantization errors. Since input values, write program check exhaustively. i.[25] quadratic, quantized approximation, approximation mono- tonic within subinterval? j.[20] difference one ulp output scale correspond error 1.0 / 32767. many ulps error seeing case? k.[Discussion] choosing approximate interval [0.0, 6.4], effectively clipped “tail”of hyperbolic tangent function, values x>6:4. It’s unreasonable approximation set output value tail to1.0. ’s worst-case error, terms real numbers ulps, treating tail way? better place might clipped tailto improve accuracy? Exercises 7.7 [10/20/10/15] <7.2,7.5 >One popular family FPGAs, Virtex-7 series, built Xilinx. Virtex-7 XC7VX690T FPGA contains 3,600 25x18-bitinteger multiply-add “DSP slices. ”Consider building TPU-style design FPGA. a.[10] Using one 25 /C218 integer multiplier per systolic array cell, ’s larg- est matrix multiplication unit one could construct? Assume matrix mul- tiplication unit must square. b.[20] Suppose could build rectangular, nonsquare matrix multiplica- tion unit. implications would design hardware soft-ware? (Hint: think vector length software must handle.) c.[10] Many FPGA designs lucky reach 500 MHz operation. speed, calculate peak 8-bit operations per second device might achieve.How compare 3 FLOPS K80 GPU? d.[15] Assume make difference 3600 4096 DSP slices using LUTs, reduce clock rate 350 MHz. Isthis worthwhile trade-off make?Case Studies Exercises Cliff Young ■6137.8 [15/15/15] <7.9>Amazon Web Services (AWS) offers wide variety “com- puting instances, ”which machines configured target different applications scales. AWS prices tell us useful data Total Cost Ownership(TCO) various computing devices, particularly computer equipment oftendepreciated 1on 3-year schedule. July 2017, dedicated, compute-oriented “c4”computing instance includes two x86 chips 20 physical cores total. rents on-demand $1.75/hour, $17,962 3 years. contrast, dedicated“p2”computing instance also two x86 chips 36 cores total, adds 16 NVIDIA K80 GPUs. p2 rents on-demand $15.84/hour, $184,780 3 years. a.[15] c4 instance uses Intel Xeon E5-2666 v3 (Haswell) processors. p2 instance uses Intel Xeon E5-2686 v4 (Broadwell) processors. Neither part num- ber listed officially Intel ’s product website, suggests parts specially built Amazon Intel. E5-2660 v3 part similar corecount E5-2666 v3 street price around $1500. E5-2697 v4part similar core count E5-2686 v4 street price around$3000. Assume non-GPU portion p2 instance would priceproportional ratio street prices. TCO, 3 years, asingle K80 GPU? b.[15] Suppose compute- throughput-dominated workload runs rate 1 c4 instance rate GPU-acceleratedp2 instance. large must GPU-based solution cost-effective? Suppose general-purpose CPU core compute rate 30G single-precision FLOPS. Ignoring CPUs p2instance, fraction peak K80 FLOPs would required reach samerate computation c4 instance? c.[15] AWS also offers “f1”instances include 8 Xilinx Ultrascale +VU9P FPGAs. rent $13.20/hour, $165,758 3 years. VU9P deviceincludes 6840 DSP slices, perform 27 /C218-bit integer multiply- accumulate operations (recall one multiply-accumulate counts two“operations ”). 500 MHz, peak multiply-accumulate opera- tions/cycle f1-based system might achieve, counting 8 FPGAs toward computation total? Assuming integer operations FPGAs substitute floating-point operations, compareto peak single-precision multiply-accumulate operations/cycle GPUsof p2 instance? compare terms cost-effectiveness? 7.9 [20/20/25] <7.7>As shown Figure 7.34 (but simplified fewer PEs), Pixel Visual Core includes 16 /C216 set full processing elements, surrounded 1Capital expenses accounted lifetime asset, using “depreciation schedule. ”Rather taking one-time charge point asset acquired, standard accounting practice spreads capital cost overthe lifetime asset. one might account $30,000 device useful life 3 years assigning$10,000 depreciation year.614 ■Chapter Seven Domain-Specific Architecturesby additional two layers “simplified ”processing elements. Simplified PEs store communicate data omit computation hardware full PEs. Simplified PEs store copies data might “home data ”of neighboring core, (16+2+2)2=400 PEs total, 256 full 144 simplified. a.[20] Suppose wanted process 64 /C232 grayscale image 5 /C25 stencil using 8 Pixel Visual Cores. now, assume image laid inraster-scan order (pixels adjacent X adjacent memory, whilepixels adjacent 64 memory locations apart). the8 cores, describe memory region core import handleits part image. Make sure include halo region. parts ofthe halo region zeroed software ensure correct operation? may find convenient refer subregions image using 2D slice notation, example image[2:5][6:13] refers set pixels whose xcomponent 2 <=x<5 whose component 6 <=y<13 (the slices half-open following Python slicing practice). b.[20] change 3 /C23 stencil, regions imported memory change? many halo-simplified PEs go unused? c.[25] consider support 7 /C27 stencil. case, ’t many hardware-supported simplified PEs need cover three pixelsworth halo data “belong ”neighboring cores. handle this, use outermost ring full PEs simplified PEs. many pixelscan handle single core using strategy? many “tiles”are required handle 64 /C232 input image? utilization full PEs complete processing time 7 /C27 stencil 64 /C232 image? 7.10 [20/20/20/25/25] <7.7>Consider case eight cores Pixel Visual Core device connected four-port switch 2D SRAM,forming core+memory unit. remaining two ports switch link theseunits ring, core able access eight SRAMs. However,this ring-based network-on-chip topology makes data access patterns moreefficient others. Core Switch SRAMCore Switch SRAMCore Switch SRAMCore Switch SRAM···Case Studies Exercises Cliff Young ■615a.[20] Suppose link NOC bandwidth B, link full-duplex, simultaneously transfer bandwidth B direction. Links connect core switch, switch SRAM, pairsof switches ring. Assume local memory least B band-width, saturate link. Consider memory access pattern eachof eight PEs access closest memory (the one connected via theswitch core+memory unit). maximum memory bandwidththat core able achieve? b.[20] consider off-by-one access pattern, core iaccesses memory i+1, going three links reach memory (core 7 access mem- ory 0, ring topology). maximum memory bandwidththat core able achieve case? achieve bandwidth, need make assumptions capabilities 4-port switch? switch move data rate B? c.[20] Consider off-by-two access pattern, core iaccess memory i+2. again, maximum memory bandwidth core ableto achieve case? bottleneck links network-on-chip? d.[25] Consider uniform random memory access pattern, core uses SRAMs ⅛of memory requests. Assuming traffic pattern, much traffic traverses switch-to-switch link, compared amount oftraffic core associated switch SRAM itsassociated switch? e.[25] (advanced) conceive case (workload) network deadlock? standpoint software-only solutions, thecompiler avoid scenario? make changes hardware,what changes routing topology (and routing scheme) would guarantee deadlocks? 7.11 <7.2>The first Anton molecular dynamics supercomputer typically simulated box water 64 Å side. computer might approximated box 1 side length. single simulation step represented 2.5 fs simula- tion time, took 10 μs wall-clock time. physics models used molecular dynamics act every particle system exerts force everyother particle system ( “outer ”) time step, requiring amounts global synchronization across entire computer. a.Calculate spatial expansion factor simulation space hardware real space. b.Calculate temporal slowdown factor simulated time wall-clock time. c.These two numbers come surprisingly close. coincidence, limit constrains way? (Hint: speed oflight applies simulated chemical system hardware doesthe simulation.)616 ■Chapter Seven Domain-Specific Architecturesd.Given limits, would take use warehouse-scale supercomputer perform molecular dynamics simulations Anton rates? is, ’s fastest simulation step time might achieved machine 102or 103m side? simulating world-spanning Cloud service? 7.12 <7.2>The Anton communication network 3D, 8 /C28/C28 torus, node system six links neighboring nodes. Latency packet tran-sit single link 50 ns. Ignore on-chip switching time links thisexercise. a.What diameter (maximum number hops pair nodes) communication network? Given diameter, shortest latencyrequired broadcast single value one node machine 512nodes machine? b.Assuming adding two values takes zero time, shortest latency add sum 512 values single node, valuestarts different node machine? c.Once assume want perform sum 512 values, want 512 nodes system end copy sum. Ofcourse could perform global reduction followed broadcast. combined operation less time? pattern called all-reduce . Compare times all-reduce pattern time broadcast froma single node global sum single node. Compare bandwidth usedby all-reduce pattern patterns.Case Studies Exercises Cliff Young ■617A.1 Introduction A-2 A.2 Classifying Instruction Set Architectures A-3 A.3 Memory Addressing A-7 A.4 Type Size Operands A-13 A.5 Operations Instruction Set A-15 A.6 Instructions Control Flow A-16 A.7 Encoding Instruction Set A-21 A.8 Cross-Cutting Issues: Role Compilers A-24 A.9 Putting Together: RISC-V Architecture A-33 A.10 Fallacies Pitfalls A-42 A.11 Concluding Remarks A-46 A.12 Historical Perspective References A-47 Exercises Gregory D. Peterson A-47A Instruction Set Principles Add number storage location ninto accumulator. En number accumulator greater equal zero execute next order stands storage location n; otherwise proceed serially. Z Stop machine ring warning bell. Wilkes Renwick, Selection List 18 Machine Instructions EDSAC (1949)A.1 Introduction appendix concentrate instruction set architecture —the portion computer visible programmer compiler writer. material shouldbe review readers book; include background. appendixintroduces wide variety design alternatives available instruction setarchitect. particular, focus four topics. First, present taxonomy ofinstruction set alternatives give qualitative assessment advantages disadvantages various approaches. Second, present analyze instruction set measurements largely independent specific instructionset. Third, address issue languages compilers bearing oninstruction set architecture. Finally, “Putting Together ”section shows ideas reflected RISC-V instruction set, typical RISCarchitectures. conclude fallacies pitfalls instruction set design. illustrate principles provide comparison RISC-V, Appendix K also gives four examples general-purpose RISC architectures (MIPS, Power ISA, SPARC, Armv8), four embedded RISC processors (ARM Thumb2, RISC-V Compressed, microMIPS), three older architectures (80x86,IBM 360/370, VAX). discuss classify architectures, needto say something instruction set measurement. Throughout appendix, examine wide variety architectural measure- ments. Clearly, measurements depend programs measured thecompilers used making measurements. results interpretedas absolute, might see different data measurement different compiler different set programs. believe measurements appendix reasonably indicative class typical applications. Manyof measurements presented using small set benchmarks, datacan reasonably displayed differences among programs seen. Anarchitect new computer would want analyze much larger collection ofprograms making architectural decisions. measurements shown areusually dynamic —that is, frequency measured event weighed number times event occurs execution measured program. starting general principles, let ’s review three application areas Chapter 1 .Desktop computing emphasizes performance pro- grams integer floating-point data types, little regard programsize. example, code size never reported five generations ofSPEC benchmarks. Servers today used primarily database, file server, Web applications, plus time-sharing applications many users. Hence,floating-point performance much less important performance integersand character strings, yet virtually every server processor still includes floating- point instructions. Personal mobile devices andembedded applications value cost energy, code size important less memory cheaper andlower energy, classes instructions (such floating point) may beoptional reduce chip costs, compressed version instructions setdesigned save memory space may used.A-2 ■Appendix Instruction Set PrinciplesThus, instruction sets three applications similar. fact, archi- tectures similar RISC-V, focus here, used successfully desktops, servers, embedded applications. One successful architecture different RISC 80x86 (see Appen- dix K). Surprisingly, success necessarily belie advantages RISCinstruction set. commercial importance binary compatibility PC soft-ware combined abundance transistors provided Moore ’s Law led Intel use RISC instruction set internally supporting 80x86 instructionset externally. Recent 80x86 microprocessors, including Intel Core micropro- cessors built past decade, use hardware translate 80x86 instructions RISC-like instructions execute translated operations inside chip.They maintain illusion 80x86 architecture programmer allowingthe computer designer implement RISC-style processor performance. Thereremain, however, serious disadvantages complex instruction set like the80x86, discuss conclusions. background set, begin exploring instruction set architectures classified. A.2 Classifying Instruction Set Architectures type internal storage processor basic differentiation, thissection focus alternatives portion architecture. Themajor choices stack, accumulator, set registers. Operands may benamed explicitly implicitly: operands stack architecture implicitly top stack, accumulator architecture one operand implicitly accumulator. general-purpose register architectures explicit operands —either registers memory locations. Figure A.1 shows block diagram architectures, Figure A.2 shows code sequence C¼A+Bwould typically appear three classes instruction sets. explicit operands maybe accessed directly memory may need first loaded temporarystorage, depending class architecture choice specific instruction. figures show, really two classes register computers. One class access memory part instruction, called register-memory archi- tecture, access memory load store instructions,called load-store architecture. third class, found computers shipping today, keeps operands memory called memory-memory architecture. instruction set architectures registers single accumulatorbut place restrictions uses special registers. architecture issometimes called extended accumulator orspecial-purpose register computer. Although early computers used stack accumulator-style architectures, virtually every new architecture designed 1980 uses load-store register archi- tecture. major reasons emergence general-purpose register (GPR)computers twofold. First, registers —like forms storage internal processor —are faster memory. Second, registers efficient aA.2 Classifying Instruction Set Architectures ■A-3Stack Accumulator Register-memoryTOS ALU . . . . . .. . . ALU . . . . . .ALU . . .. . .. . .. . . Register-register/ load-storeALU . . . . . .. . .. . . MemoryProcessor (A) (B) (C) (D) Figure A.1 Operand locations four instruction set architecture classes. arrows indicate whether oper- input result arithmetic-logical unit (ALU) operation, input result. Lighter shadesindicate inputs, dark shade indicates result. (A), top stack (TOS) register points top input operand, combined operand below. first operand removed stack, result takes place second operand, TOS updated point result. operands implicit. (B), accu-mulator implicit input operand result. (C), one input operand register, one memory, result goes register. operands registers (D) and, like stack architecture, transferred memory via separate instructions: push pop (A) load store (D). Stack AccumulatorRegister (register-memory)Register (load-store) Push Load Load R1,A Load R1,A Push B Add B Add R3,R1,B Load R2,BAdd Store C Store R3,C Add R3,R1,R2Pop C Store R3,C Figure A.2 code sequence C5A+Bfor four classes instruction sets. Note Add instruction implicit operands stack accumulator architectures explicit operands register architectures. assumed A, B, C belong memory values B cannot destroyed. Figure A.1 shows Add operation class architecture.A-4 ■Appendix Instruction Set Principlescompiler use forms internal storage. example, register com- puter expression (A * B)+(B * C) –(A * D) may evaluated multiplications order, may efficient location ofthe operands pipelining concerns (see Chapter 3 ). Nevertheless, stack computer hardware must evaluate expression one order, becauseoperands hidden stack, may load operand multiple times. importantly, registers used hold variables. variables allocated registers, memory traffic reduces, program speeds (becauseregisters faster memory), code density improves (because reg- ister named fewer bits memory location). explained Section A.8 , compiler writers would prefer registers equivalent unreserved. Older computers compromise desire dedicatingregisters special uses, effectively decreasing number general-purpose reg-isters. number truly general-purpose registers small, trying allo-cate variables registers profitable. Instead, compiler reserveall uncommitted registers use expression evaluation. many registers sufficient? answer, course, depends effec- tiveness compiler. compilers reserve registers expression eval- uation, use somefor parameter passing,and allow theremainder tobeallocated toholdvariables. Modern compiler technology ability effectively use larger num-bers registers led increase register counts recent architectures. Two major instruction set characteristics divide GPR architectures. char- acteristics concern nature operands typical arithmetic logical instruc-tion (ALU instruction). first concerns whether ALU instruction two orthree operands. three-operand format, instruction contains one result operand two source operands. two-operand format, one operands source result operation. second distinction among GPRarchitectures concerns many operands may memory addresses inALU instructions. number memory operands supported typicalALU instruction may vary none three. Figure A.3 shows combinations two attributes examples computers. Although seven Number memory addressesMaximum number operands allowed Type architecture Examples 0 3 Load-store ARM, MIPS, PowerPC, SPARC, RISC-V 1 2 Register-memory IBM 360/370, Intel 80x86, Motorola 68000, TI TMS320C54x 2 2 Memory-memory VAX (also three-operand formats)3 3 Memory-memory VAX (also two-operand formats) Figure A.3 Typical combinations memory operands total operands per typical ALU instruction examples computers. Computers memory reference per ALU instruction called load-store register-register computers. Instructions multiple memory operands per typical ALU instruction called register-memory memory-memory, according whether one one memory operand.A.2 Classifying Instruction Set Architectures ■A-5possible combinations, three serve classify nearly existing computers. mentioned earlier, three load-store (also called register-register), register- memory, memory-memory. Figure A.4 shows advantages disadvantages alter- natives. course, advantages disadvantages absolutes: qualitative actual impact de pends compiler implemen- tation strategy. GPR computer mory-memory operations could eas- ily ignored compiler used load-store computer. One themost pervasive architectural impacts instruction encoding num- ber instructions needed perform task. see impact archi- tectural alternatives implementation approaches Appendix C Chapter 3 . Summary: Classifying Instruction Set Architectures end Sections A.3 –A.8we summarize characteristics would expect find new instruction set architecture, building foundation RISC-V architecture introduced Section A.9 . section clearly expect use general-purpose registers. Figure A.4 , combined Appendix C pipelining, leads expectation load-store version general-purpose register architecture. class architecture covered, next topic addressing operands.Type Advantages Disadvantages Register-register (0, 3)Simple, fixed-length instruction encoding. Simple code generation model. Instructions take similar numbers clocks execute(seeAppendix C )Higher instruction count architectures memory references instructions. instructions lower instruction density lead larger programs,which may instruction cache effects Register-memory (1, 2)Data accessed without separate load instruction first. Instruction format tends easy encode yields good densityOperands equivalent source operand binary operation destroyed. Encoding register number memory address instruction may restrict number registers.Clocks per instruction vary operand location Memory- memory (2, 2) (3, 3)Most compact. ’t waste registers temporariesLarge variation instruction size, especially three-operand instructions. addition, large variation work per instruction. Memory accessescreate memory bottleneck. (Not used today.) Figure A.4 Advantages disadvantages three common types general-purpose register com- puters. notation ( m,n) means mmemory operands ntotal operands. general, computers fewer alter- natives simplify compiler ’s task fewer decisions compiler make (see Section A.8 ). Computers wide variety flexible instruction formats reduce number bits required encode pro-gram. number registers also affects instruction size need log 2(number registers) register specifier instruction. Thus, doubling number registers takes three extra bits register-register architecture, 10% 32-bit instruction.A-6 ■Appendix Instruction Set PrinciplesA.3 Memory Addressing Independent whether architecture load-store allows operand memory reference, must define memory addresses interpretedand specified. measurements presented largely,but completely, computer independent. cases measurementsare significantly affected com piler technology. measurements made using optimizing compiler, compiler technology plays critical role. Interpreting Memory Addresses memory address interpreted? is, object accessed func- tion address length? instruction sets discussed bookare byte addressed provide access bytes (8 bits), half words (16 bits), andwords (32 bits). computers also provide access double words(64 bits). two different conventions ordering bytes within larger object. Little Endian byte order puts byte whose address “x…x000”at least-significant position double word (the little end). bytes numbered: 76543210 Big Endian byte order puts byte whose address “x…x000”at most- significant position double word (the big end). bytes numbered: 01234567 operating within one computer, byte order often unnoticeable — programs access locations both, say, words bytes, cannotice difference. Byte order problem exchanging data among com-puters different orderings, however. Little Endian ordering also fails matchthe normal ordering words strings compared. Strings appear“SDRAWKCAB ”(backwards) registers. second memory issue many computers, accesses objects larger byte must aligned . access object size sbytes byte address aligned Amod s¼0.Figure A.5 shows addresses access aligned misaligned. would someone design computer alignment restrictions? Misalign- ment causes hardware complications, memory typically aligned amultiple word double-word boundary. misaligned memory access may,therefore, take multiple aligned memory references. Thus, even computers thatallow misaligned access, programs aligned accesses run faster.A.3 Memory Addressing ■A-7Even data aligned, supporting byte, half-word, word accesses requires alignment network align bytes, half words, words 64-bit registers. Forexample, Figure A.5 , suppose read byte address 3 low-order bitshavingthevalue4.Wewill needtoshiftright3bytestoalignthebytetotheproper place 64-bitregister. Dependingon theinstruction,the computer mayalso need sign-extend quantity. Stores easy: addressed bytes memory may bealtered. computers byte, half-word, word operation affect theupper portion register. Although computers discussed book permitbyte, half-word, word accesses memory, IBM 360/370, Intel 80x86,and VAX support ALU operations register operands narrower full width. discussed alternative interpretations memory addresses, discuss ways addresses specified instructions, called addressing modes . Addressing Modes Given address, know bytes access memory. sub- section look addressing modes —how architectures specify addressValue three low-order bits byte address Width object 0 1 2 3 4 5 6 7 1 byte (byte) Aligned Aligned Aligned Aligned Aligned Aligned Aligned Aligned 2 bytes (half word) Aligned Aligned Aligned Aligned2 bytes (half word) Misaligned Misaligned Misaligned Misaligned4 bytes (word) Aligned Aligned4 bytes (word) Misaligned Misaligned4 bytes (word) Misaligned Misaligned4 bytes (word) Misaligned Misaligned8 bytes (double word) Aligned8 bytes (double word) Misaligned8 bytes (double word) Misaligned 8 bytes (double word) Misaligned 8 bytes (double word) Misaligned8 bytes (double word) Misaligned8 bytes (double word) Misaligned8 bytes (double word) Misaligned Figure A.5 Aligned misaligned addresses byte, half-word, word, double-word objects byte- addressed computers. misaligned example objects require two memory accesses complete. Every aligned object always complete one memory access, long memory wide object. figure shows memory organized 8 bytes wide. byte offsets label columns specify low-order three bits address.A-8 ■Appendix Instruction Set Principlesof object access. Addressing modes specify constants registers addition locations memory. memory location used, actual memory address specified addressing mode called effective address . Figure A.6 shows data addressing modes used recent computers. Immediates literals usually considered memory addressingmodes (even though value access instruction stream), althoughregisters often separated ’t usually memory addresses. kept addressing modes depend program counter, calledPC-relative addressing , separate. PC-relative addressing used primarily specifying code addresses control transfer instructions, discussed Section A.6 . Addressing mode Example instruction Meaning used Register Add R4,R3 Regs[R4] Regs[R4] +Regs[R3]When value register Immediate Add R4,3 Regs[R4] Regs[R4] +3 constants Displacement Add R4,100(R1) Regs[R4] Regs[R4] +Mem[100 +Regs[R1]]Accessing local variables (+simulates register indirect, direct addressing modes) Register indirectAdd R4,(R1) Regs[R4] Regs[R4] +Mem[Regs[R1]]Accessing using pointer computed address Indexed Add R3,(R1 +R2) Regs[R3] Regs[R3] +Mem[Regs[R1] +Regs [R2]]Sometimes useful array addressing: R1¼base array; R2¼index amount Direct absoluteAdd R1,(1001) Regs[R1] Regs[R1] +Mem[1001]Sometimes useful accessing static data; address constant mayneed large Memory indirectAdd R1, @(R3) Regs[R1] Regs[R1] +Mem[Mem[Regs[R3]]]IfR3is address pointer p, mode yields * p Autoincrement Add R1,(R2)+ Regs[R1] Regs[R1] +Mem[Regs[R2]] Regs[R2] Regs[R2] +dUseful stepping arrays within loop. R2points start array; reference increments R2 size element, Autodecrement Add R1, –(R2) Regs[R2] Regs[R2] –d Regs[R1] Regs[R1] +Mem[Regs[R2]]Same use autoincrement. Autodecrement/-increment also act push/pop implement astack. Scaled Add R1,100(R2)[R3] Regs[R1] Regs[R1] +Mem[100 +Regs[R2] +Regs[R3] * d]Used index arrays. May applied indexed addressing mode computers Figure A.6 Selection addressing modes examples, meaning, usage. autoincrement/-decrement scaled addressing modes, variable ddesignates size data item accessed (i.e., whether instruc- tion accessing 1, 2, 4, 8 bytes). addressing modes useful elements accessed areadjacent memory. RISC computers use displacement addressing simulate register indirect 0 address simulate direct addressing using 0 base register. measurements, use first name shown mode. extensions C used hardware descriptions defined page A.38.A.3 Memory Addressing ■A-9Figure A.6 shows common names addressing modes, though names differ among architectures. figure throughout book, use extension C programming language hardware description notation. Inthis figure, one non-C feature used: left arrow ( ) used assignment. also use array Mem name main memory array Regs registers. Thus, Mem[Regs[R1]] refers contents memory location whose address given contents register 1 ( R1). Later, introduce extensions accessing transferring data smaller word. Addressing modes ability significantly reduce instruction counts; also add complexity building computer may increase aver- age clock cycles per instruction (CPI) computers implement modes.Thus, usage various addressing modes quite important helping thearchitect choose include. Figure A.7 shows results measuring ddressing mode usage patterns three programs VAX architect ure. use old VAX architecture measurements appendix richest set addressing 0% 10% 20% 30% 40% 50% 60%24% 11% 39% 32% 40%3% 43% 17% 55%0% 6%16% Scaled Register indirect Immediate DisplacementTeX spice gcc TeX spice gcc TeX spice gcc TeX spice gcc1% 6% Memory indirectTeX spice gcc1% Frequenc addressin g mode Figure A.7 Summary use memory addressing modes (including immediates). major addressing modes account percent (0% –3%) mem- ory accesses. Register modes, counted, account one-half oper-and references, memory addressing modes (including immediate) account half. course, compiler affects addressing modes used; see Section A.8 . memory indirect mode VAX use displacement, autoincre- ment, autodecrement form initial memory address; programs, almost memory indirect references use displacement mode base. Displacement mode includes displacement lengths (8, 16, 32 bits). PC-relative addressingmodes, used almost exclusively branches, included. addressingmodes average frequency 1% shown.A-10 ■Appendix Instruction Set Principlesmodes fewest restrictions memory addressing. example, Figure A.6 page A.9 shows modes VAX supports. measurements appendix, however, use recent register-register architectures showhow programs use instruction sets current computers. AsFigure A.7 shows, displacement immediate addressing dominate addressing mode usage. Let ’s look properties two heavily used modes. Displacement Addressing Mode major question arises displacement-style addressing mode ofthe range displacements used. Based use various displacement sizes, decision sizes support made. Choosing displacement field sizes important directly affect instruction length. Figure A.8 30%35%40% 25% 20% 15% 10% 5%0% 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14Percentage displacement Number bits displacementFloating-point averageInteger average 15 Figure A.8 Displacement values widely distributed. large num- ber small values fair number large values. wide distribution displace- ment values due multiple storage areas variables different displacements toaccess (see Section A.8 ) well overall addressing scheme compiler uses. x-axis log 2of displacement, is, size field needed represent magnitude displacement. Zero x-axis shows percentage dis- placements value 0. graph include sign bit, heavily affectedby storage layout. displacements positive, majority largest dis- placements (14+ bits) negative. data collected computer 16-bit displacements, cannot tell us longer displacements. datawere taken Alpha architecture full optimization (see Section A.8 ) SPEC CPU2000, showing average integer programs (CINT2000) average floating-point programs (CFP2000).A.3 Memory Addressing ■A-11shows measurements taken data access load-store architecture using benchmark programs. look branch offsets Section A.6 —data acces- sing patterns branches different; little gained combining them,although practice immediate sizes made simplicity. Immediate Literal Addressing Mode Immediates used arithmetic operations, comparisons (primarily forbranches), moves constant wanted register. last caseoccurs constants written code —which tend small —and address constants, tend large. use immediates important knowwhether need supported operations subset. Figure A.9 shows frequency immediates general classes integer floating- point operations instruction set. Another important instruction set measurement range values imme- diates. Like displacement values, size immediate values affects instructionlength. Figure A.10 shows, small immediate values heavily used. Large immediates sometimes used, however, likely addressing calculations. Summary: Memory Addressing First, popularity, would expect new architecture support least following addressing modes: displacement, immediate, register indi- rect. Figure A.7 shows represent 75% –99% addressing modes used 0% 5% 10% 15% 20% 25%Loads ALU operations instructions21%16%25%19%23%22% 30%Floating-point average Integer average Figure A.9 one-quarter data transfers ALU operations imme- diate operand. bottom bars show integer programs use immediates one-fifth instructions, floating-point programs use immediates one-sixth instructions. loads, load immediate instruction loads 16 bits intoeither half 32-bit register. Load immediates loads strict sense becausethey access memory. Occasionally pair load immediates used load 32- bit constant, rare. (For ALU operations, shifts constant amount included operations immediate operands.) programs computer usedto collect statistics Figure A.8 .A-12 ■Appendix Instruction Set Principlesin measurements. Second, would expect size address displace- ment mode least 12 –16 bits, caption Figure A.8 suggests sizes would capture 75% –99% displacements. Third, would expect size immediate field least 8 –16 bits. claim substan- tiated caption figure refers. covered instruction set classes decided register-register archi- tectures, plus previous recommendations data addressing modes, nextcover sizes meanings data. A.4 Type Size Operands type operand designated? Usually, encoding opcode designates type operand —this method used often. Alterna- tively, data annotated tags interpreted hardware.These tags specify type operand, operation chosen accordingly.Computers tagged data, however, found computer museums. Let’s start desktop server architectures. Usually type oper- and—integer, single-precision floating point, character, —effectively30%35%40%45% 25% 20% 15% 10% 5% 0% 0123456789 1 0 1 1 1 2 1 3 1 4Percentage immediates Number bits needed immediateFloating-point average Integer average 15 Figure A.10 distribution immediate values. x-axis shows number bits needed represent magnitude immediate value —0 means imme- diate field value 0. majority immediate values positive. 20%were negative CINT2000, 30% negative CFP2000. measure- ments taken Alpha, maximum immediate 16 bits, programs Figure A.8 . similar measurement VAX, supported 32-bit immediates, showed 20% –25% immediates longer 16 bits. Thus, 16 bits would capture 80% 8 bits 50%.A.4 Type Size Operands ■A-13gives size. Common operand types include character (8 bits), half word (16 bits), word (32 bits), single-precision floating point (also 1 word), double- precision floating point (2 words). Integers almost universally representedas two ’s complement binary numbers. Characters usually ASCII, 16-bit Unicode (used Java) gaining popularity internationalizationof computers. early 1980s, computer manufacturers chose theirown floating-point representation. Almost computers since time followthe standard floating point, IEEE standard 754, although levelof accuracy recently abandoned application-specific processors. IEEE floating-point standard discussed detail Appendix J. architectures provide operations character strings, although oper- ations usually quite limited treat byte string single character.Typical operations supported character strings comparisons moves. business applications, architectures support decimal format, usually called packed decimal orbinary-coded decimal —4 bits used encode values 0 –9, 2 decimal digits packed byte. Numeric character strings sometimes called unpacked decimal , operations —called packing andunpacking —are usually provided converting back forth them. One reason use decimal operands get results exactly match decimal numbers, decimal fractions exact representation binary.For example, 0.10 10is simple fraction decimal, binary requires infinite set repeating digits: 0 :000110011 0011…2. Thus, calculations exact decimal close inexact binary, problem forfinancial transactions. (See Appendix J learn precise arithmetic.) SPEC benchmarks use byte character, half-word (short integer), word (integer single precision floating point), double-word (long integer), floating-point data types. Figure A.11 shows dynamic distribution sizes objects referenced memory programs. frequency access 0% 20% 40% 60% 80%Byte (8 bits)Half word (16 bits)Word (32 bits)Double word (64 bits) 10%1%5%0%26%29%59%70% Floating-point average Integer average Figure A.11 Distribution data accesses size benchmark programs. double-word data type used double-precision floating point floating-point pro-grams addresses, computer uses 64-bit addresses. 32-bit address computer 64-bit addresses would replaced 32-bit addresses, almost double-word accesses integer programs would become single-word accesses.A-14 ■Appendix Instruction Set Principlesdifferent data types helps deciding types important support effi- ciently. computer 64-bit access path, would taking two cycles access double word satisfactory? saw earlier, byte accesses require analignment network: important support bytes primitives? Figure A.11 uses memory references examine types data accessed. architectures, objects registers may accessed bytes half words. However, access infrequent —on VAX, accounts 12% register references, roughly 6% operand accessesin programs. A.5 Operations Instruction Set operators supported instruction set architectures categorized asinFigure A.12 . One rule thumb across architectures widely executed instructions simple operations instruction set. example, Figure A.13 shows 10 simple instructions account 96% instructions exe- cuted collection integer programs running popular Intel 80x86.Hence, implementor instructions sure make fast,as common case. Operator type Examples Arithmetic logicalInteger arithmetic logical operations: add, subtract, and, or, multiply, divide Data transfer Loads-stores (move instructions computers memory addressing) Control Branch, jump, procedure call return, trapsSystem Operating system call, virtual memory management instructionsFloating point Floating-point operations: add, multiply, divide, compareDecimal Decimal add, decimal multiply, decimal-to-character conversionsString String move, string compare, string search Graphics Pixel vertex operations, compression/decompression operations Figure A.12 Categories instruction operators examples each. computers generally provide full set operations first three categories. support forsystem functions instruction set varies widely among architectures, com- puters must instruction support basic system functions. amount support instruction set last four categories may vary none anextensive set special instructions. Floating-point instructions provided inany computer intended use application makes much use floating point. instructions sometimes part optional instruction set. Decimal string instructions sometimes primitives, VAX IBM 360, may besynthesized compiler simpler instructions. Graphics instructions typically operate many smaller data items parallel —for example, performing eight 8-bit additions two 64-bit operands.A.5 Operations Instruction Set ■A-15As mentioned before, instructions Figure A.13 found every computer every application ––desktop, server, embedded ––with variations operations inFigure A.12 largely depending data types instruction set includes. A.6 Instructions Control Flow measurements branch jump behavior fairly independent measurements applications, wenow examine use ofcontrol flowinstruc-tions, little common operations previous sections. consistent terminology instructions change flow con- trol. 1950s typically called transfers . Beginning 1960 name branch began used. Later, computers introduced additional names. Through- book use jump change control unconditional branch change conditional. distinguish four different types control flow change: ■Conditional branches ■Jumps ■Procedure calls ■Procedure returns want know relative frequency events, event different, may use different instructions, may different behavior. Figure A.14 shows frequencies control flow instructions load-store computerrunning benchmarks.Rank 80x86 instructionInteger average % total executed) 1 Load 22% 2 Conditional branch 20%3 Compare 16%4 Store 12%5 Add 8%6 6%7 Sub 5%8 Move register-register 4%9 Call 1%10 Return 1% Total 96% Figure A.13 top 10 instructions 80x86. Simple instructions dominate list responsible 96% instructions executed. percentages theaverage five SPECint92 programs.A-16 ■Appendix Instruction Set PrinciplesAddressing Modes Control Flow Instructions destination address control flow instruction must always specified. destination specified explicitly instruction vast majority cases — procedure return major exception, return target notknown compile time. common way specify destination supplya displacement added program counter (PC). Control flow instructions sort called PC-relative . PC-relative branches jumps advantageous target often near current instruction, specifying positionrelative current PC requires fewer bits. Using PC-relative addressing also per-mits code run independently loaded. property, called posi- tion independence , eliminate work program linked also useful programs linked dynamically execution. implement returns indirect jumps target known com- pile time, method PC-relative addressing required. Here, mustbe way specify target dynamically, change runtime. Thisdynamic address may simple naming register contains targetaddress; alternatively, jump may permit addressing mode used sup-ply target address. register indirect jumps also useful four important features: ■Case orswitch statements, found programming languages (which select among one several alternatives). ■Virtual functions ormethods object-oriented languages like C++ Java (which allow different routines called depending type theargument).0% 25% 50% 75%Call/return Jump Conditional branch75%82%6%10%19%8% 100% Frequenc branch instructionsFloating-point average Integer average Figure A.14 Breakdown control flow instructions three classes: calls returns, jumps, conditional branches. Conditional branches clearly dominate. type counted one three bars. programs computer used collect statistics Figure A.8 .A.6 Instructions Control Flow ■A-17■High-order functions orfunction pointers languages like C C++ (which allow functions passed arguments, giving flavor object- oriented programming). ■Dynamically shared libraries (which allow library loaded linked runtime actually invoked program rather loaded linked statically program run). four cases target address known compile time, hence usu- ally loaded memory register register indirect jump. branches generally use PC-relative addressing specify targets, important question concerns far branch targets branches. Knowingthe distribution displacements help choosing branch offsets support, thus affect instruction length encoding. Figure A.15 shows distribution displacements PC-relative branches instructions.About 75% branches forward direction. Conditional Branch Options changes control flow branches, deciding specify thebranch condition important. Figure A.16 shows three primary techniques use today advantages disadvantages. 30% 25% 20%15% 10% 5% 0% 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14Percentage distance Bits branch dis placementFloating-point averageInteger average 15 16 17 18 19 20 Figure A.15 Branch distances terms number instructions target branch instruction. frequent branches integer programs targets encoded 4 –8 bits. result tells us short displacement fields often suffice branches designer gain encoding density byhaving shorter instruction smaller branch displacement. measurements taken load-storecomputer (Alpha architecture) instructions aligned word boundaries. architecture requires fewer instructions program, VAX, would shorter branch distances. However, number bits needed displacement may increase computer variable-length instructions aligned byteboundary. programs computer used collect statistics Figure A.8 .A-18 ■Appendix Instruction Set PrinciplesOne noticeable properties branches large number comparisons simple tests, large number comparisons zero. Thus,some architectures choose treat comparisons special cases, especially ifacompare branch instruction used. Figure A.17 shows frequency different comparisons used conditional branching. Procedure Invocation Options Procedure calls returns include control transfer possibly state saving;at minimum return address must saved somewhere, sometimes speciallink register GPR. older architectures provide mechanism savemany registers, newer architectures require compiler generate storesand loads register saved restored. two basic conventions use save registers: either call site inside procedure called. Caller saving means calling procedure must save registers wants preserved access call, thus called procedure need worry registers. Callee saving opposite: called procedure must save registers wants use, leaving caller unre-strained. times caller save must used access patternsto globally visible variables two different procedures. example, suppose wehave procedure P1 calls procedure P2, procedures manipulate theName Examples condition tested Advantages Disadvantages Condition code (CC)80x86, ARM, PowerPC, SPARC, SuperHTests special bits set ALU operations, possibly programcontrolSometimes condition set free.CC extra state. Condition codes constrain ordering instructionsbecause passinformation one instruction branch Condition register/ limitedcomparisonAlpha, MIPS Tests arbitrary register result simple comparison (equality orzero tests)Simple Limited compare may affect critical path require extra comparison generalcondition Compare branchPA-RISC, VAX, RISC-VCompare part branch. Fairly general compares allowed(greater then, less then)One instruction rather two branchMay set critical path branch instructions Figure A.16 major methods evaluating branch conditions, advantages, disadvantages. Although condition codes set ALU operations needed purposes, measurements pro- grams show rarely happens. major implementation problems condition codes arise con- dition code set large haphazardly chosen subset instructions, rather controlled bit theinstruction. Computers compare branch often limit set compares use separate operation register complex compares. Often, different techniques used branches based floating-point com- parison versus based integer comparison. dichotomy reasonable number branches thatdepend floating-point comparisons much smaller number depending integer comparisons.A.6 Instructions Control Flow ■A-19global variable x. P1 allocated xto register, must sure save xto location known P2 call P2. compiler ’s ability discover called procedure may access register-allocated quantities complicated possibility separate compilation. Suppose P2 may touch xbut call another procedure, P3, may access x, yet P2 P3 compiled separately. complications, compilers conservatively caller save variable may accessed call. cases either convention could used, programs optimal callee save optimal caller save. result,most real systems today use combination two mechanisms. convention specified application binary interface (ABI) sets basic rules registers caller saved callee saved. Later inthis appendix examine mismatch sophisticated instructions forautomatically saving registers needs compiler. Summary: Instructions Control Flow Control flow instructions frequently executed instructions.Although many options conditional branches, would expectGreater thanGreater equalEqualNot equal Less equal Less than35%34%33%44%0%0%0% 11%18%16%2%5% 0% 10% 20% 30% 40% 50% Frequenc comparison ypes branchesFloating-point average Integer average Figure A.17 Frequency different types compares conditional branches. Less (or equal) branches dominate combination compiler architecture. Thesemeasurements include integer floating-point compares branches. programs computer used collect statistics Figure A.8 .A-20 ■Appendix Instruction Set Principlesbranch addressing new architecture able jump hundreds instruc- tions either branch. requirement suggests PC-relative branch displacement least 8 bits. would also expect see register indirectand PC-relative addressing jump instructions support returns well manyother features current systems. completed instruction architecture tour level seen assembly language programmer compiler writer. leaning toward aload-store architecture displacement, immediate, register indirect addres-sing modes. data 8-, 16-, 32-, 64-bit integers 32- 64-bit floating-point data. instructions include simple operations, PC-relative condi- tional branches, jump link instructions procedure call, register indirectjumps procedure return (plus uses). need select represent architecture form makes easy hardware execute. A.7 Encoding Instruction Set Clearly, choices mentioned herein affect instructions encodedinto binary representation execution processor. representationaffects size compiled program also implementationof processor, must decode representation quickly find oper-ation operands. operation typically specified one field, called theopcode . shall see, important decision encode addressing modes operations. decision depends range addressing modes degree independence opcodes modes. older computers oneto five operands 10 addressing modes operand (see Figure A.6 ). large number combinations, typically separate address specifier needed operand: address specifier tells addres- sing mode used access operand. extreme load-storecomputers one memory op erand one two addressing modes; obviously, case, addressing mode encoded part opcode. encoding instructions, number registers number addressing modes significant impact size instructions, asthe register field addressing mode field may appear many times singleinstruction. fact, instructions many bits consumed inencoding addressing modes register fields specifying opcode.The architect must balance several competing forces encoding theinstruction set: 1.The desire many registers addressing modes possible. 2.The impact size register addressing mode fields average instruction size hence average program size.A.7 Encoding Instruction Set ■A-213.A desire instructions encoded lengths easy han- dle pipelined implementation. (The value easily decoded instructions discussed Appendix C andChapter 3 .) minimum, architect wants instructions multiples bytes, rather arbitrary bitlength. Many desktop server architects chosen use fixed-length instruction gain implementation benefits sacrificing averagecode size. Figure A.18 shows three popular choices encoding instruction set. first call variable , allows virtually addressing modes operations. style best many addressing modes opera-tions. second choice call fixed , combines operation addressing mode opcode. Often fixed encoding single Operation no. operandsAddress specifier 1Address field 1 Address field 1Operation Address field 2Address field 3 Address specifierOperation Address field Address specifier 1Operation Address specifier 2Address field Address specifierOperation Address field 1Address field 2 Address specifier nAddress field n (A) Variable (e.g., Intel 80x86, VAX) (B) Fixed (e.g., RISC V, ARM, MIPS, PowerPC, SPARC) (C) Hybrid (e.g., RISC V Compressed (RV32IC), IBM 360/370, microMIPS, Arm Thumb2) Figure A.18 Three basic variations instruction encoding: variable length, fixed length, hybrid. variable format support number operands, address specifier determining addressing mode length spec-ifier operand. generally enables smallest code representation, unused fields need included. fixed format always number operands, addressing modes (if options exist) specified part theopcode. generally results largest code size. Although fields tend vary location, used different purposes different instruc- tions. hybrid approach multiple formats specified opcode, adding oneor two fields specify addressing mode one two fields specify theoperand address.A-22 ■Appendix Instruction Set Principlessize instructions; works best addressing modes operations. trade-off variable encoding fixed encoding size programs versus ease decoding processor. Variable tries use asfew bits possible represent program, individual instructions varywidely size amount work performed. Let’s look 80x86 instruction see example variable encoding: add EAX,1000(EBX) name add means 32-bit integer add instruction two operands, opcode takes 1 byte. 80x86 address specifier 1 2 bytes, specifying source/destination register ( EAX) addressing mode (displacement case) base register ( EBX) second operand. combination takes 1 byte specify operands. 32-bit mode (see Appendix K), size ofthe address field either 1 byte 4 bytes. Because1000 bigger 2 8, total length instruction 1+1+4¼6 bytes length 80x86 instructions varies 1 17 bytes. 80x86 programs generally smaller RISC architectures, use fixed formats (seeAppendix K). Given two poles instruction set design variable fixed, third alternative immediately springs mind: reduce variability size work ofthe variable architecture provide multiple instruction lengths reduce codesize. hybrid approach third encoding alternative, ’ll see examples shortly. Reduced Code Size RISCs RISC computers started used embedded applications, 32-bit fixed format became liability cost, hence smaller code, important. response, several manufacturers offered new hybrid version RISC instruc- tion sets, 16-bit 32-bit instructions. narrow instructions supportfewer operations, smaller address immediate fields, fewer registers, thetwo-address format rather classic three-address format RISC computers.RISC-V offers extension, called RV32IC, C standing compressed.Common instruction occurrences, intermediates small values com-mon ALU operations source destination register identical, areencoded 16-bit formats. Appendix K gives two examples, ARM Thumb microMIPS, claim code size reduction 40%. contrast instruction set extensions, IBM simply compresses stan- dard instruction set adds hardware decompress instructions arefetched memory instruction cache miss. Thus, instruction cachecontains full 32-bit instructions, compressed code kept main memory,ROMs, disk. advantage compressed format, RV32IC,A.7 Encoding Instruction Set ■A-23microMIPS Thumb2 instruction caches act 25% larger, IBM ’s CodePack means compilers need changed handle different instruction sets instruction decoding remain simple. CodePack starts run-length encoding compression PowerPC pro- gram loads resulting compression tables 2 KB table chip.Hence, every program unique encoding. handle branches, whichare longer aligned word boundary, PowerPC creates hash table inmemory maps compressed uncompressed addresses. Like aTLB (see Chapter 2 ), caches recently used address maps reduce number memory accesses. IBM claims overall performance cost 10%, resulting code size reduction 35% –40%. Summary: Encoding Instruction Set Decisions made components instruction set design discussed previous sections determine whether architect choice variable fixed instruction encodings. Given choice, architect interested code size performance pick variable encoding, one interested per-formance code size pick fixed encoding. RISC-V, MIPS, ARM allhave instruction set extension uses 16-bit instruction, well 32-bit;applications serious code size constraints opt use 16-bit variantto decrease code size. Appendix E gives 13 examples results architects ’ choices. Appendix C andChapter 3 , impact variability performance processor discussed further. almost finished laying groundwork RISC-V instruction set architecture introduced Section A.9 . that, however, helpful take brief look compiler technology effect programproperties. A.8 Cross-Cutting Issues: Role Compilers Today almost programming done high-level languages desktop server applications. development means instructions exe- cuted output compiler, instruction set architecture essentiallya compiler target. earlier times applications, architectural decisionswere often made ease assembly language programming specific kernel.Because compiler significantly affect performance computer,understanding compiler technology today critical designing efficientlyimplementing instruction set. popular try isolate compiler technology effect hardware performance architecture performance, popular try separate architecture implementation. separationis essentially impossible today ’s desktop compilers computers. Architec- tural choices affect quality code generated computer andthe complexity building good compiler it, better worse.A-24 ■Appendix Instruction Set PrinciplesIn section, discuss critical goals instruction set primarily compiler viewpoint. starts review anatomy current compilers. Next discuss compiler technology affects decisions architect,and architect make hard easy compiler produce goodcode. conclude review compilers multimedia operations, whichunfortunately bad example cooperation compiler writers andarchitects. Structure Recent Compilers begin, let ’s look optimizing compilers like today. Figure A.19 shows structure recent compilers. compiler writer ’s first goal correctness —all valid programs must compiled correctly. second goal usually speed compiled code. Typ-ically, whole set goals follows two, including fast compilation,debugging support, interoperability among languages. Normally, passes Language dependent; machine independentDependencies Transform language common intermediate formFunction Front end per language High-level optimizations Global optimizer Code generatorIntermediate representation example, loop transformations procedure inlining(also called procedure integration) Including global local optimizations +register allocation Detailed instruction selection machine-dependent optimizations; may includeor followed b assemblerSomewhat language dependent; largely machineindependent Small language dependencies; machine dependencies slight (e.g., register counts/types) Highly machine dependent; language independent Figure A.19 Compilers typically consist two four passes, highly opti- mizing compilers passes. structure maximizes probability program compiled various levels optimization produce output given input. optimizing passes designed optional may skipped faster compilation goal lower-quality code acceptable. pass simply one phase compiler reads transforms entire program.(The term phase often used interchangeably pass.) optimizing passes separated, multiple languages use optimizing code generation passes. new front end required new language.A.8 Cross-Cutting Issues: Role Compilers ■A-25in compiler transform higher-level, abstract representations progres- sively lower-level representations. Eventually reaches instruction set. structure helps manage complexity transformations makes writinga bug-free compiler easier. complexity writing correct compiler major limitation amount optimization done. Although multiple-pass structurehelps reduce compiler complexity, also means compiler must orderand perform transformations others. diagram optimizingcompiler Figure A.19 , see certain high-level optimizations per- formed long known resulting code look like. transformation made, compiler ’t afford go back revisit steps, possibly undoing transformations. iteration would prohibitive, incompilation time complexity. Thus, compilers make assumptions aboutthe ability later steps deal certain problems. example, compilers usu-ally choose procedure calls expand inline know theexact size procedure called. Compiler writers call problem thephase-ordering problem . ordering transformations interact instruction set architecture? good example occurs optimization called global common subexpression elimination . optimization finds two instances expression compute value saves value first computation tem-porary. uses temporary value, eliminating second computation thecommon expression. optimization significant, temporary must allocated register. Otherwise, cost storing temporary memory later reloading may negate savings gained recomputing expression. are, fact, cases optimization actually slows code temporaryis register allocated. Phase ordering complicates problem registerallocation typically done near end global optimization pass, beforecode generation. Thus, optimizer performs optimization must assume register allocator allocate temporary register. Optimizations performed modern compilers classified style transformation, follows: ■High-level optimizations often done source output fed later optimization passes. ■Local optimizations optimize code within straight-line code fragment (called basic block compiler people). ■Global optimizations extend local optimizations across branches intro- duce set transformations aimed optimizing loops. ■Register allocation associates registers operands. ■Processor-dependent optimizations attempt take advantage specific archi- tectural knowledge.A-26 ■Appendix Instruction Set PrinciplesRegister Allocation central role register allocation plays, speeding code making optimizations useful, one important — important —of optimizations. Register allocation algorithms today based technique called graph coloring . basic idea behind graph coloring construct graph representing possible candidatesfor allocation register use graph allocate registers.Roughly speaking, problem use limited set colors thatno two adjacent nodes dependency graph color. empha- sis approach achieve 100% regi ster allocation active variables. problem coloring graph general take exponential time afunction size graph (NP-compl ete). heuristic algorithms, however, work well practice, yiel ding close allocations run near- linear time. Graph coloring works best least 16 (and preferably more) general-purpose registers available global allocation integer variablesand additional registers floating point. Unfortunately, graph coloring work well number registers small heuristic algorithms coloring graph likely fail. Impact Optimizations Performance sometimes difficult separate simpler optimizations —local processor-dependent optimizations —from transformations done code gen- erator. Examples typical optimizations given Figure A.20 . last col- umn Figure A.20 indicates frequency listed optimizing transforms applied source program. Figure A.21 shows effect various optimizations instructions executed two programs. case, optimized programs executed roughly 25% –90% fewer instructions unoptimized programs. figure illustrates impor-tance looking optimized code suggesting new instruction set features,because compiler might completely remove instructions architect try-ing improve. Impact Compiler Technology Architect ’s Decisions interaction compilers high-level languages significantly affects programs use instruction set architecture. two important questions:how variables allocated addressed? many registers needed allo-cate variables appropriately? address questions, must look threeseparate areas current high-level languages allocate data:A.8 Cross-Cutting Issues: Role Compilers ■A-27■Thestack used allocate local variables. stack grown shrunk procedure call return, respectively. Objects stack addressed rel-ative stack pointer primarily scalars (single variables) ratherthan arrays. stack used activation records, notas stack eval- uating expressions. Hence, values almost never pushed popped stack.Optimization name ExplanationPercentage total number optimizing transforms High-level near source level; processor-independent Procedure integration Replace procedure call procedure body N.M.Local Within straight-line codeCommon subexpression eliminationReplace two instances computation single copy18% Constant propagation Replace instances variable assigned constant constant22% Stack height reduction Rearrange expression tree minimize resources needed expression evaluationN.M. Global Across branch Global common subexpression eliminationSame local, version crosses branches 13% Copy propagation Replace instances variable Athat assigned X(i.e., A¼X) X11% Code motion Remove code loop computes value iteration loop16% Induction variable eliminationSimplify/eliminate array addressing calculations within loops2% Processor-dependent Depends processor knowledge Strength reduction Many examples, replace multiply constant adds shiftsN.M. Pipeline scheduling Reorder instructions improve pipeline performanceN.M. Branch offset optimizationChoose shortest branch displacement reaches targetN.M. Figure A.20 Major types optimizations examples class. data tell us relative fre- quency occurrence various optimizations. third column lists static frequency common optimizations applied set 12 small Fortran Pascal programs. nine local global optimizations done compiler included measurement. Six optimizations coveredin figure, remaining three account 18% total static occurrences. abbreviation N.M. means number occurrences optimization measured. Processor-dependent optimizations usu- ally done code generator, none measured experiment. percentage portion static optimizations specified type. Data Chow, F.C., 1983. Portable Machine-IndependentGlobal Optimizer —Design Measurements (Ph.D. thesis). Stanford University, Palo Alto, CA (collected using Stanford UCODE compiler).A-28 ■Appendix Instruction Set Principles■The global data area used allocate statically declared objects, global variables constants. large percentage objects arraysor aggregate data structures. ■Theheap used allocate dynamic objects adhere stack disci- pline. Objects heap accessed pointers typically scalars. Register allocation much effective stack-allocated objects global variables, register allocation essentially impossible heap-allocated objects accessed pointers. Global variables stack variables impossible allocate aliased —there multiple ways refer address variable, making illegal put register.(Most heap variables effectively aliased today ’s compiler technology.) example, consider following code sequence, &returns address variable *dereferences pointer: p¼&a–gets address p a¼... –assigns directly *p¼... –uses p assign ...a... –accesses variable acould register allocated across assignment * pwith- generating incorrect code. Aliasing causes substantial problem ismcf, level 0Program, compiler optimization level 100% 0% 20% 40% 60% 80%Branches/calls Floating-point ALU ops Loads-stores Integer ALU ops Percenta ge unoptimized instructions executedmcf, level 1mcf, level 2mcf, level 3lucas, level 0lucas, level 1lucas, level 2lucas, level 3 11% 12% 21% 100% 76% 76% 84% 100% Figure A.21 Change instruction count programs lucas mcf SPEC2000 compiler optimization levels vary. Level 0 unoptimized code. Level 1 includes local optimizations, code scheduling, local register allocation.Level 2 includes global optimizations, loop transformations (software pipelining), global register allocation. Level 3 adds procedure integration. experiments performed Alpha compilers.A.8 Cross-Cutting Issues: Role Compilers ■A-29often difficult impossible decide objects pointer may refer to. com- piler must conservative; compilers allocate anylocal variables procedure register pointer may refer oneof local variables. Architect Help Compiler Writer Today, complexity compiler come translating simple state-ments like A¼B+C. programs locally simple , simple translations work fine. Rather, complexity arises programs large globally com-plex interactions, structure compilers means decisionsare made one step time code sequence best. Compiler writers often working corollary basic prin- ciple architecture: make frequent cases fast rare case correct . is, know cases frequent rare, generating code forboth straightforward, quality code rare case may bevery important —but must correct! instruction set properties help compiler writer. properties thought hard-and-fast rules, rather guidelinesthat make easier write compiler generate efficient cor- rect code. ■Provide regularity —Whenever makes sense, three primary components instruction set —the operations, data types, addressing modes —should orthogonal . Two aspects architecture said orthogonal independent. example, operations andaddressing modes orthogonal if, every operation one addres-sing mode applied, addressing modes applicable. regular- ity helps simplify code generation particularly important decision code generate split two passes compiler.A good counterexample property restricting registers beused certain class instructions. Compilers special-purpose registerarchitectures typically get stuck dilemma. restriction resultin compiler finding lots available registers, none ofthe right kind! ■Provide primitives, solutions —Special features “match ”a language construct kernel function often unusable. Attempts support high-level languages may work one language less required correct efficient implementation language. example attempts failed given Section A.10 . ■Simplify trade-offs among alternatives —One toughest jobs compiler writer figuring instruction sequence best every seg-ment code arises. earlier days, instruction counts total code sizemight good metrics, —as saw Chapter 1 —this longerA-30 ■Appendix Instruction Set Principlestrue. caches pipelining, trade-offs become complex. Anything designer help compiler writer understand costs alternative code sequences would help improve code. One mostdifficult instances complex trade-offs occurs register-memory architec-ture deciding many times variable referenced ischeaper load register. threshold hard compute and, fact,may vary among models architecture. ■Provide instructions bind quantities known compile time con-stants —A compiler writer hates thought processor interpreting runtime value known compile time. Good counterexamples thisprinciple include instructions interpret values fixed compiletime. instance, VAX procedure call instruction ( calls ) dynamically interprets mask saying registers save call, mask fixed compile time (see Section A.10 ). Compiler Support (or Lack Thereof) Multimedia Instructions Alas, designers SIMD instructions (see Section 4.3 Chapter 4 ) basi- cally ignored previous subsection. instructions tend solutions, notprimitives; short registers; data types match existing pro-gramming languages. Architects hoped find inexpensive solution wouldhelp users, often low-level graphics library routines use them. SIMD instructions really abbreviated version elegant architec- ture style compiler technology. explained Section 4.2, vector architectures operate vectors data. Invented originally scientific codes, multimedia kernels often vectorizable well, albeit often shorter vectors.As Section 4.3 suggests, think Intel ’s MMX SSE PowerPC ’s Alti- Vec, RISC-V P extension, simply short vector computers: MMX withvectors eight 8-bit elements, four 16-bit elements, two 32-bit elements,and AltiVec vectors twice length. implemented simply adja- cent, narrow elements wide registers. microprocessor architectures build vector register size archi- tecture: sum sizes elements limited 64 bits MMX 128bits AltiVec. Intel decided expand 128-bit vectors, added wholenew set instructions, called streaming SIMD extension (SSE). major advantage vector computers hiding latency memory access loading many elements overlapping execution data transfer.The goal vector addressing modes collect data scattered memory, place compact form operated efficiently, place results back belong. Vector computers include strided addressing andgather/scatter addressing (see Section 4.2) increase number programs vectorized. Stridedaddressing skips fixed number words access, sequentialaddressing often called unit stride addressing . Gather scatter find theirA.8 Cross-Cutting Issues: Role Compilers ■A-31addresses another vector register: think register indirect addressing vector computers. vector perspective, contrast, short-vector SIMD computers support unit strided accesses: memory accesses load store allelements single wide memory location. data mul-timedia applications often streams start end memory, strided andgather/scatter addressing modes essential successful vectorization (seeSection 4.7). Example example, compare vector computer MMX color representation conversion pixels RGB (red, green, blue) YUV (luminosity chromi-nance), pixel represented 3 bytes. conversion three linesof C code placed loop: Y¼(9798*R +19235*G +3736*B) / 32768; U¼(-4784*R 9437*G +4221*B) / 32768 +128; V¼(20218*R 16941*G 3277*B) / 32768 +128; 64-bit-wide vector computer calculate 8 pixels simultaneously. One vec- tor computer media strided addresses takes ■3 vector loads (to get RGB) ■3 vector multiplies (to convert R) ■6 vector multiply adds (to convert G B) ■3 vector shifts (to divide 32,768) ■2 vector adds (to add 128) ■3 vector stores (to store YUV) total 20 instructions perform 20 operations previous C code convert 8 pixels ( Kozyrakis, 2000 ). (Because vector might 32 64-bit ele- ments, code actually converts 32 /C28 256 pixels.) contrast, Intel ’s website shows library routine perform calculation 8 pixels takes 116 MMX instructions plus 6 80x86 instructions(Intel, 2001 ). six-fold increase instructions due large number instructions load unpack RGB pixels pack store YUV pixels, strided memory accesses. short, architecture-limited vectors registers simple memory addressing modes makes difficult use vectorizing compiler technology.Hence, SIMD instructions likely found hand-coded librariesthan compiled code. Summary: Role Compilers section leads several recommendations. First, expect new instructionset architecture least 16 general-purpose registers —not countingA-32 ■Appendix Instruction Set Principlesseparate registers floating-point numbers —to simplify allocation registers using graph coloring. advice orthogonality suggests supported addressing modes apply instructions transfer data. Finally, last threepieces advice —provide primitives instead solutions, simplify trade-offs alternatives, ’t bind constants runtime —all suggest better err side simplicity. words, understand less thedesign instruction set. Alas, SIMD extensions example goodmarketing outstanding achievement hardware –software co-design. A.9 Putting Together: RISC-V Architecture section describe load-store architecture called RISC-V. RISC-V freely licensed open standard, similar many RISC architectures, basedon observations similar covered last sections. (In Section M.3 wediscuss architectures became popular.) RISC-V builds on30 years experience RISC architectures “cleans ”most short-term inclusions omissions, leading architecture easierand efficient implement. RISC-V provides 32-bit 64-bit instruction set, well variety extensions features like floating point; extensions added either 32-bit 64-bit base instruction set.We discuss 64-bit version RISC-V, RV64, superset 32-bitversion RV32. Reviewing expectations section, desktop server applications: ■Section A.2 —Use general-purpose registers load-store architecture. ■Section A.3 —Support addressing modes: displacement (with address offset size 12 –16 bits), immediate (size 8 –16 bits), register indirect. ■Section A.4 —Support data sizes types: 8-, 16-, 32-, 64-bit inte- gers 64-bit IEEE 754 floating-point numbers. ■Section A.5 —Support simple instructions, dominate number instructions executed: load, store, add, subtract, moveregister-register, shift. ■Section A.6 —Compare equal, compare equal, compare less, branch (with PC-relative address least 8 bits long), jump, call, return. ■Section A.7 —Use fixed instruction encoding interested performance, use variable instruction encoding interested code size. somelow-end, embedded applications, small one-level caches, larger code size may significant performance implications. ISAs provide compressed instruction set extension provide way addressing thisdifference. ■Section A.8 —Provide least 16, preferably 32, general-purpose registers, sure addressing modes apply data transfer instructions, aim forA.9 Putting Together: RISC-V Architecture ■A-33a minimalist instruction set. section ’t cover floating-point programs, often use separate floating-point registers. justification increase total number registers without raising problems instruc-tion format speed general-purpose register file. compro-mise, however, orthogonal. introduce RISC-V showing follows recommendations. Like RISC predecessors, RISC-V emphasizes ■A simple load-store instruction set. ■Design pipelining efficiency (discussed Appendix C ), including fixed instruction set encoding. ■Efficiency compiler target. RISC-V provides good architectural model study, pop- ularity type processor, also easy architecture under-stand. use architecture Appendix C Chapter 3 , forms basis number exercises programming projects. RISC-V Instruction Set Organization RISC-V instruction set organized three base instruction sets support32-bit 64-bit integers, variety optional extensions one base instruc-tion sets. allows RISC-V implemented wide range potential appli- cationsfroma small embeddedprocessor witha minimal budgetfor logicandmemory likely costs $1 less, high-end processor configurations full support floating point, vectors, multiprocessor configurations. Figure A.22 summarizes three base instruction sets instruction set extensions basic func-tionality. purposes text, use RV64IMAFD (also known RV64G, forshort) examples. RV32G 32-bit subset 64-bit architecture RV64G. Registers RISC-V RV64G 32 64-bit general-purpose registers (GPRs), named x0, x1, …, x31. GPRs also sometimes known integer registers . Additionally, F extensions floating point part RV64G, come set 32 floating- point registers (FPRs), named f0, f1, …, f31, hold 32 single-precision (32-bit) values 32 double-precision (64-bit) values. (When holding one single-precision number, half FPR unused.) single- double-precision floating-point operations (32-bit 64-bit) provided. value x0 always 0. shall see later use register synthesize variety useful operations simple instruction set. special registers transferred general-purpose reg- isters. example floating-point status register, used hold informationA-34 ■Appendix Instruction Set Principlesabout results floating-point operations. also instructions mov- ing FPR GPR. Data Types RISC-V data types 8-bit bytes, 16-bit half words, 32-bit words, 64-bit double-words integer data 32-bit single precision 64-bit double precision floating point. Half words added found languages like CName base extension Functionality RV32I Base 32-bit integer instruction set 32 registers RV32E Base 32-bit instruction set 16 registers; intended low-end embedded applications RV64I Base 64-bit instruction set; registers 64-bits, instructions move 64-bit from/to registers (LD SD) added Adds integer multiply divide instructionsA Adds atomic instructions needed concurrent processing; see Chapter 5 F Adds single precision (32-bit) IEEE floating point, includes 32 32- bit floating point registers, instructions load store registers operate Extends floating point double precision, 64-bit, making registers 64-bits, adding instructions load, store, operate onthe registers Q extends floating point add support quad precision, adding 128-bit operations L Adds support 64- 128-bit decimal floating point IEEE standard C Defines compressed version instruction set intended small-memory-sized embedded applications. Defines 16-bitversions common RV32I instructions V future extension support vector operations (see Chapter 4 ) B future extension support operations bit fieldsT future extension support transactional memoryP extension support packed SIMD instructions: see Chapter 4 RV128I future base instruction set providing 128-bit address space Figure A.22 RISC-V three base instructions sets (and reserved spot future fourth); extensions extend one base instruction sets. instruction set thus named base name followed extensions. example, RISC-V64IMAFD refers base 64-bit instruction set extensions M, A, F, D. consistency naming software, combination given abbreviated name: RV64G, weuse RV64G text.A.9 Putting Together: RISC-V Architecture ■A-35and popular programs, operating systems, concerned size data structures. also become popular Unicode becomes widely used. RV64G operations work 64-bit integers 32- 64-bit floating point. Bytes, half words, words loaded general-purpose registers witheither zeros sign bit replicated fill 64 bits GPRs. loaded,they operated 64-bit integer operations. Addressing Modes RISC-V Data Transfers data addressing modes immediate displacement, 12-bitfields. Register indirect accomplished simply placing 0 12-bit displace-ment field, limited absolute addressing 12-bit field accomplished byusing register 0 base register. Embracing zero gives us four effective modes,although two supported architecture. RV64G memory byte addressable 64-bit address uses Little End- ian byte numbering. load-store architecture, references mem- ory either GPRs FPRs loads stores. Supporting data typesmentioned herein, memory accesses involving GPRs byte, half word,word, double word. FPRs may loaded stored single-precision ordouble-precision numbers. Memory accesses need aligned; however, maybe unaligned accesses run extremely slow. practice, programmers com-pilers would stupid use unaligned accesses. RISC-V Instruction Format RISC-V two addressing modes, encoded theopcode. Following advice making processor easy pipeline anddecode, instructions 32 bits 7-bit primary opcode. Figure A.23 shows instruction layout four major instruction types. formats simplewhile providing 12-bit fields displacement addressing, immediate constants, PC-relative branch addresses. R-type funct731 25 24 20 19 15 14 12 11 7 6 0 funct3 rs2 rs1 rd opcode I-type funct3 rs1 rd opcode S-type imm[11:5]imm[11:0] funct3 rs2 rs1 imm[4:0] opcode U-type imm[31:12] rd opcode Figure A.23 RISC-V instruction layout. two variations formata, called SB UJ formats; deal slightly different treatment immediate fields.A-36 ■Appendix Instruction Set PrinciplesThe instruction formats use instruction fields described Figure A.24 . opcode specifies general instruction type (ALU instruction, ALU immediate, load, store, branch, jump), funct fields used specific operations. example, ALU instruction encoded singleopcode funct field dictating exact operation: add, subtract, and, etc.Notice several formats encode multiple types instructions, including theuse I-format ALU immediates loads, use S-formatfor stores conditional branches. RISC-V Operations RISC-V (or properly RV64G) supports list simple operationsrecommended herein plus others. four broad classes instructions: loads stores, ALU operations, branches jumps, floating-point operations. general-purpose floating-point registers may loaded stored, except loading x0 effect. Figure A.25 gives examples load store instructions. Single-precision floating-point numbers occupy half floating-point register. Conversions single double precision must doneexplicitly. floating-point format IEEE 754 (see Appendix J). list allthe RV64G instructions appears Figure A.28 (page A.42).Instruction format Primary use rd rs1 rs2 Immediate R-type Register-register ALU instructionsDestination First source Second source I-type ALU immediates LoadDestination First source base registerValue displacement S-type Store Compare branchBase register first sourceData source store second sourceDisplacement offset U-type Jump link Jump linkregisterRegister destination forreturn PCTarget address jump linkregisterTarget address jump link Figure A.24 use instruction fields instruction type. Primary use shows major instructions use format. blank indicates corresponding field present instruction type. I-format isused loads ALU immediates, 12-bit immediate holding either value immediate displacement load. Similarly, S-format encodes store instructions (where first source register base register second contains register source value store) compare branch instructions(where register fields contain sources compare immediate field specifies offset branch target). actually two formats: SB UJ follow basic organization J, slightly modify interpretation immediate fields.A.9 Putting Together: RISC-V Architecture ■A-37To understand figures need introduce additional extensions C description language used initially page A-9: ■A subscript appended symbol whenever length datum transferred might clear. Thus, nmeans transfer n-bit quantity. use x,y zto indicate zshould transferred xandy. ■A subscript used indicate selection bit field. Bits labeled most-significant bit starting 0. subscript may single digit(e.g., Regs[x4 ] 0yields sign bit x4) subrange (e.g., Regs [x3] 56..63 yields least-significant byte x3). ■The variable Mem, used array stands main memory, indexed byte address may transfer number bytes. ■A superscript used replicate field (e.g., 048yields field zeros length 48 bits). ■The symbol ##is used concatenate two fields may appear either side data transfer, symbols ≪and≫shift first operand left right amount second operand.Example instruction Instruction name Meaning ld x1,80(x2) Load doubleword Regs[x1] Mem[80+Regs[x2]] lw x1,60(x2) Load word Regs[x1] 64Mem[60+Regs[x2]] 0)32## Mem[60+Regs[x2]] lwu x1,60(x2) Load word unsigned Regs[x1] 64032## Mem[60+Regs[x2]] lb x1,40(x3) Load byte Regs[x1] 64(Mem[40+Regs[x3]] 0)56## Mem[40+Regs[x3]] lbu x1,40(x3) Load byte unsigned Regs[x1] 64056## Mem[40+Regs[x3]] lh x1,40(x3) Load half word Regs[x1] 64(Mem[40+Regs[x3]] 0)48## Mem[40+Regs[x3]] flw f0,50(x3) Load FP single Regs[f0] 64Mem[50+Regs[x3]] ## 032 fld f0,50(x2) Load FP double Regs[f0] 64Mem[50+Regs[x2]] sd x2,400(x3) Store double Mem[400+Regs[x3]] 64Regs[x2] sw x3,500(x4) Store word Mem[500+Regs[x4]] 32Regs[x3] 32..63 fsw f0,40(x3) Store FP single Mem[40+Regs[x3]] 32Regs[f0] 0..31 fsd f0,40(x3) Store FP double Mem[40+Regs[x3]] 64Regs[f0] sh x3,502(x2) Store half Mem[502+Regs[x2]] 16Regs[x3] 48..63 sb x2,41(x3) Store byte Mem[41+Regs[x3]] 8Regs[x2] 56..63 Figure A.25 load store instructions RISC-V. Loads shorter 64 bits available sign- extended zero-extended forms. memory references use single addressing mode. course, loads stores available data types shown. RV64G supports double precision floating point, single precision floating point loads must aligned FP register, 64-bits wide.A-38 ■Appendix Instruction Set PrinciplesAs example, assuming x8 x10 32-bit registers: Regs[x10] 64(Mem[Regs[x8]] 0)32## Mem[Regs[R8]] means word memory location addressed contents register x8 sign-extended form 64-bit quantity stored register x10. ALU instructions register-register instructions. Figure A.26 gives examples arithmetic/logical instructions. operations include simplearithmetic logical operations: add, subtract, AND,OR,XOR, shifts. Immediate forms instructions provided using 12-bit sign-extended immedi-ate. operation LUI (load upper immediate) loads bits 12 –31 register, sign- extends immediate field upper 32-bits, sets low-order 12-bits register 0. LUI allows 32-bit constant built two instructions, data transfer using constant 32-bit address one extra instruction. mentioned herein, x0 used synthesize popular operations. Loading constant simply add immediate source operand x0, aregister-register move simply add (or or) one sources isx0. (We sometimes use mnemonic li, standing load immediate, representthe former, mnemonic mv latter.) RISC-V Control Flow Instructions Control handled set jumps set branches, Figure A.27 gives typical branch jump instructions. two jump instructions (jumpand link jump link register) unconditional transfers always storethe“link,”which address instruction sequentially following jump instruction, register specified rd field. event link addressis needed, rd field simply set x0, results typical uncon-ditional jump. two jump instructions differentiated whether address computed adding immediate field PC adding immediateExample instrucmtion Instruction name Meaning add x1,x2,x3 Add Regs[x1] Regs[x2]+Regs[x3] addi x1,x2,3 Add immediate unsignedRegs[x1] Regs[x2]+3 lui x1,42 Load upper immediateRegs[x1] 032##42##012 sll x1,x2,5 Shift left logical Regs[x1] Regs[x2] <<5 slt x1,x2,x3 Set less (Regs[x2] <Regs[x3]) Regs[x1] 1 else Regs[x1] 0 Figure A.26 basic ALU instructions RISC-V available register- register operands one immediate operand. LUI uses U-format employs rs1 field part immediate, yielding 20-bit immediate.A.9 Putting Together: RISC-V Architecture ■A-39field contents register. offset interpreted half word offset compatibility compressed instruction set, R64C, includes 16-bit instructions. branches conditional. branch condition specified instruc- tion, arithmetic comparison (equal, greater than, less than, theirinverses) permitted. branch-target address specified 12-bit signedoffset shifted left one place (to get 16-bit alignment) added thecurrent program counter. Branches based contents floating pointregisters implemented executing floating point comparison (e.g., feq.dor fle.d), sets integer register 0 1 based comparison, executing beq bne x0 operand. observant reader noticed 64-bit instructions RV64G. Primarily, 64-bit loads stores versionsof 32-bit, 16-bit, 8-bit loads sign extend (the default sign-extend).To support 32-bit modular arithmetic without additional instructions, ver-sions instructions ignore upper 32 bits 64-bit register, addand subtract word (addw, subw). Amazingly, everything else works. RISC-V Floating-Point Operations Floating-point instructions manipulate floating-point registers indicate whether operation performed single double precision. floating-point operations add, subtract, multiply, divide, square root, wellas fused multiply-add multiply-subtract. floating point instructions beginwith letter fand use suffix dfor double precision sfor single precision (e.g., f add.d ,fadd.s ,fmul.d ,fmul.s ,fmadd.d fmadd.s ). Floating- point compares set integer register based comparison, similarly theinteger instruction set-less-than set-great-than. addition floating-point loads stores ( flw, fsw, fld, fsd ), instruc- tionsareprovidedforconvertingbetweendifferent FPprecisions,for movingbetween integer FP registers (fmv), converting floating point integer(fcvt, uses integer registers source destination appropriate). Figure A.28 contains list nearly RV64G instructions summary meaning.Example instruction Instruction name Meaning jal x1,offset Jump link Regs[x1] PC+4; PC PC + (offset <<1) jalr x1,x2,offset Jump link register Regs[x1] PC+4; PC Regs[x2]+offset beq x3,x4,offset Branch equal zero (Regs[x3]==Regs[x4]) PC PC + (offset <<1) bgt x3,x4,name Branch equal zero (Regs[x3] >Regs[x4]) PC PC + (offset <<1) Figure A.27 Typical control flow instructions RISC-V. control instructions, except jumps address register, PC-relative.A-40 ■Appendix Instruction Set PrinciplesInstruction type/opcode Instruction meaning Data transfers Move data registers memory, integer FP; memory address mode 12-bit displacement +contents GPR lb, lbu, sb Load byte, load byte unsigned, store byte (to/from integer registers) lh, lhu, sh Load half word, load half word unsigned, store half word (to/from integer registers) lw, lwu, sw Load word, store word (to/from integer registers) ld, sd Load doubleword, store doubleword Arithmetic/logical Operations data GPRs. Word versions ignore upper 32 bitsadd, addi, addw, addiw, sub, subi, subw, subiwAdd subtract, word immediate versions slt, sltu, slti, sltiu set-less-than signed unsigned, immediate and, or, xor, andi, ori, xori and, or, xor, register-register register-immediate lui Load upper immediate: loads bits 31..12 register immediate value. Upper 32 bits set 0 auipc Sums immediate upper 20-bits PC register; used building branch 32-bit address sll, srl, sra, slli, srli, srai, sllw,slliw, srli,srliw, srai, sraiwShifts: logical shift left right arithmetic shift right, immediate word versions (word versions leave upper 32 bit untouched) mul, mulw, mulh, mulhsu, mulhu, div,divw, divu, rem, remu, remw, remuwInteger multiply, divide, remainder, signed unsigned support 64-bit products two instructions. Also word versions Control Conditional branches jumps; PC-relative register beq, bne, blt, bge, bltu, bgeu Branch based compare two registers, equal, equal, less than, greater equal, signed unsigned jal,jalr Jump link address relative register PC Floating point FP operation appear double precision (.d) single (.s)flw, fld, fsw, fsd Load, store, word (single precision), doubleword (double precision) fadd, fsub, fmult, fiv, fsqrt, fmadd, fmsub, fnmadd, fnmsub,fmin, fmax, fsgn, fsgnj, fsjnxAdd, subtract, multiply, divide, square root, multiply-add, multiply-subtract, negate multiply-add, negate multiply-subtract, maximum, minimum, andinstructions replace sign bit. single precision, opcode followed by: .s, double precision: .d. Thus fadd.s, fadd.d feq, flt, fle Compare two floating point registers; result 0 1 stored GPR fmv.x.*, fmv.*.x Move FP register abd GPR, “*”is fcvt.*.l, fcvt.l.*, fcvt.*. lu, fcvt.lu.*, fcvt. *.w, fcvt. w.*,fcvt. *.wu, fcvt.wu. *Converts FP register integer register, “*”is single double precision. Signed unsigned versions word,doubleword versions Figure A.28 list vast majority instructions RV64G. list also found back inside cover. table omits system instructions, synchronization atomic instructions, configuration instructions, instructions reset access performance counters, 10 instructions total.A.9 Putting Together: RISC-V Architecture ■A-41RISC-V Instruction Set Usage give idea instructions popular, Figure A.29 shows frequency instructions instruction classes SPECint2006 programs, usingRV32G. A.10 Fallacies Pitfalls Architects repeatedly tripped common, erroneous, beliefs. sec-tion look them. Pitfall Designing “high-level ”instruction set feature specifically oriented supporting high-level language structure . Attempts incorporate high-level language features instruction set led architects provide powerful instructions wide range flexibility. How-ever, often instructions work required frequent case, orthey ’t exactly match requirements languages. Many efforts aimed eliminating 1970s called semantic gap . Although idea supplement instruction set additions bringProgram Loads Stores Branches Jumps ALU operations astar 28% 6% 18% 2% 46% bzip 20% 7% 11% 1% 54%gcc 17% 23% 20% 4% 36%gobmk 21% 12% 14% 2% 50%h264ref 33% 14% 5% 2% 45%hmmer 28% 9% 17% 0% 46%libquantum 16% 6% 29% 0% 48%mcf 35% 11% 24% 1% 29%omnetpp 23% 15% 17% 7% 31%perlbench 25% 14% 15% 7% 39%sjeng 19% 7% 15% 3% 56%xalancbmk 30% 8% 27% 3% 31% Figure A.29 RISC-V dynamic instruction mix SPECint2006 programs. Omnetpp includes 7% instruc- tions floating point loads, stores, operations, compares; program includes even 1% otherinstruction types. change gcc SPECint2006, creates anomaly behavior. Typical integer programs load frequencies 1/5 3x store frequency. gcc, store frequency actually higher load frequency! arises large fraction execution time spent loop clears memory storingx0 (not compiler like gcc would usually spend execution time!). store instruction stores register pair, RISC ISAs included, would address issue.A-42 ■Appendix Instruction Set Principlesthe hardware level language, additions generate Wulf et al. (1981) called semantic clash : …by giving much semantic content instruction, computer designer made possible use instruction limited contexts. [p. 43] often instructions simply overkill —they general frequent case, resulting unneeded work slower instruction. Again, VAX CALLS good example. CALLS uses callee save strategy (the registers saved specified callee), butthe saving done call instruction caller. CALLS instruction begins arguments pushed stack, takes following steps: 1.Align stack needed. 2.Push argument count stack. 3.Save registers indicated procedure call mask stack (as men- tioned Section A.8 ). mask kept called procedure ’s code —this permits callee specify registers saved caller even withseparate compilation. 4.Push return address stack, push top base stack pointers (for activation record). 5.Clear condition codes, sets trap enable known state. 6.Push word status information zero word stack. 7.Update two stack pointers. 8.Branch first instruction procedure. vast majority calls real programs require amount over- head. procedures know argument counts, much faster linkage convention established using registers pass arguments rather stack memory. Furthermore, CALLS instruction forces two registers used linkage, many languages require one linkage register.Many attempts support procedure call activation stack management havefailed useful, either match language needs orbecause general hence expensive use. VAX designers provided simpler instruction, JSB, much faster pushes return PC stack jumps procedure. How- ever, VAX compilers use costly CALLS instructions. call instructions included architecture standardize procedure linkageconvention. computers standardized calling convention agree-ment among compiler writers without requiring overhead complex,very general procedure call instruction. Fallacy thing typical program .A.10 Fallacies Pitfalls ■A-43Many people would like believe single “typical ”program could used design optimal instruction set. example, see synthetic benchmarks discussed Chapter 1 . data appendix clearly show programs vary significantly use instruction set. example,Figure A.30 shows mix data transfer sizes four SPEC2000 pro- grams: would hard say typical four programs. var-iations even larger instruction set supports class applications,such decimal instructions, unused applications. Pitfall Innovating instruction set architecture reduce code size without account- ing compiler . Figure A.31 shows relative code sizes four compilers MIPS instruction set. Whereas architects struggle reduce code size 30% –40%, different compiler strategies change code size much larger factors. Similar performance opti-mization techniques, architect start tightest code compilerscan produce proposing hardware innovations save space. Fallacy architecture flaws cannot successful . 80x86 provides dramatic example: instruction set architecture one creators could love (see Appendix K). Succeeding generations Intel engineershave tried correct unpopular architectural decisions made designing the80x86. example, 80x86 supports segmentation, whereas others picked paging; uses extended accumulators integer data, processors use general-purpose registers; uses stack floating-point data, every-one else abandoned execution stacks long before.0% 20% 40% 60% 80% 100%Byte (8 bits)Half word (16 bits)Word (32 bits)Double word (64 bits) 18%22%0%0%3%19%0%0%18%28%6%40%62%31%94%60% applu equake gzip perl Figure A.30 Data reference size four programs SPEC2000. Although calculate average size, would hard claim average typical programs.A-44 ■Appendix Instruction Set PrinciplesDespite major difficulties, 80x86 architecture enormously successful. reasons threefold: first, selection microprocessor inthe initial IBM PC makes 80x86 binary compatibility extremely valuable. Second,Moore ’s Law provided sufficient resources 80x86 microprocessors translate internal RISC instruction set execute RISC-like instructions. Thismix enables binary compatibility valuable PC software base perfor- mance par RISC processors. Third, high volumes PC micro- processors mean Intel easily pay increased design cost hardwaretranslation. addition, high volumes allow manufacturer go thelearning curve, lowers cost product. larger die size increased power translation may liability embedded applications, makes tremendous economic sense desktop.And cost-performance desktop also makes attractive servers, itsmain weakness servers 32-bit addresses, resolved 64- bit address extension. Fallacy design flawless architecture . architecture design involves trade-offs made context set hardware software technologies. time technologies likely change, decisions may correct time made look like mistakes.For example, 1975 VAX designers overemphasized importance codesize efficiency, underestimating important ease decoding pipeliningwould five years later. example RISC camp delayed branch (seeAppendix K). simple matter control pipeline hazards five-stagepipelines, challenge processors longer pipelines issue multipleinstructions per clock cycle. addition, almost architectures eventually suc- cumb lack sufficient address space. one reason RISC-VCompilerApogee software version 4.1Green Hills Multi2000 Version 2.0Algorithmics SDE4.0B IDT/c 7.2.1 Architecture MIPS IV MIPS IV MIPS 32 MIPS 32 Processor NEC VR5432 NEC VR5000 IDT 32334 IDT 79RC32364Autocorrelation kernel 1.0 2.1 1.1 2.7Convolutional encoder kernel 1.0 1.9 1.2 2.4Fixed-point bit allocation kernel 1.0 2.0 1.2 2.3Fixed-point complex FFT kernel 1.0 1.1 2.7 1.8Viterbi GSM decoder kernel 1.0 1.7 0.8 1.1Geometric mean five kernels 1.0 1.7 1.4 2.0 Figure A.31 Code size relative Apogee Software Version 4.1 C compiler Telecom application EEMBC benchmarks. instruction set architectures virtually identical, yet code sizes vary factors 2. results reported February –June 2000.A.10 Fallacies Pitfalls ■A-45has planned possibility 128-bit addresses, although may decades capability needed. general, avoiding flaws long run would probably mean compromising efficiency architecture short run, danger-ous, since new instruction set architecture must struggle survive firstfew years. A.11 Concluding Remarks earliest architectures limited instruction sets hardware technology time. soon hardware technology permitted, computerarchitects began looking ways support high-level languages. search ledto three distinct periods thought support programs efficiently. Inthe 1960s, stack architectures became popular. viewed goodmatch high-level languages —and probably were, given compiler tech- nology day. 1970s, main concern architects reduce software costs. concern met primarily replacing software hard- ware, providing high-level architectures could simplify task soft-ware designers. result high-level language computer architecturemovement powerful architectures like VAX, large number ofaddressing modes, multiple data types, highly orthogonal architecture. the1980s, sophisticated compiler technology renewed emphasis pro-cessor performance saw return simpler architectures, based mainly theload-store style computer. following instruction set architecture changes occurred 1990s: ■Address size doubles —The 32-bit address instruction sets desktop server processors extended 64-bit addresses, expanding width ofthe registers (among things) 64 bits. Appendix K gives three examplesof architectures gone 32 bits 64 bits. ■Optimization conditional branches via conditional execution —InChapter 3 , see conditional branches limit performance aggressive com-puter designs. Hence, interest replacing conditional branches withconditional completion operations, conditional move (see Appendix H), added instruction sets. ■Optimization cache performance via prefetch —Chapter 2 explains increasing role memory hierarchy performance computers, cache miss computers taking many instruction times page faults took earlier computers. Hence, prefetch instructions added try tohide cost cache misses prefetching (see Chapter 2 ). ■Support multimedia —Most desktop embedded instruction sets extended support multimedia applications.A-46 ■Appendix Instruction Set Principles■Faster floating-point operations —Appendix J describes operations added enhance floating-point performance, operations perform multiply add paired single execution, part RISC-V. 1970 1985 many thought primary job computer architect design instruction sets. result, textbooks era emphasize instruction set design, much computer architecture textbooks 1950s 1960s empha- sized computer arithmetic. educated architect expected strong opin-ions strengths especially weaknesses popular computers.The importance binary compatibility quashing innovations instruction setdesign unappreciated many researchers textbook writers, giving theimpression many architects would get chance design instruction set. definition computer architecture today expanded include design evaluation full computer system —not definition instruction set processor —and hence plenty topics architect study. fact, material appendix central point ofthe book first edition 1990, included appendix primarily asreference material! Appendix K may satisfy readers interested instruction set architecture; describes variety instruction sets, either important marketplacetoday historically important, compares nine popular load-store computerswith RISC-V. A.12 Historical Perspective References Section M.4 (available online) features discussion evolution instruction sets includes references reading exploration related topics. Exercises Gregory D. Peterson A.1 [10]<A.9>Compute effective CPI implementation embedded RISC-V CPU using Figure A.29. Assume made following measure- ments average CPI instruction types: Instruction Clock cycles ALU operations 1.0 Loads 5.0Stores 3.0Branches Taken 5.0Not taken 3.0 Jumps 3.0 Average instruction frequencies astar gcc obtain instruction mix.Exercises Gregory D. Peterson ■A-47A.2 [10]<A.9>Compute effective CPI RISC-V using Figure A.29 table above. Average instruction frequencies bzip hmmer obtain instruction mix. may assume instructions (for instructionsnot accounted types Table A.29) require 3.0 clock cycles each. A.3 [10]<A.9>Compute effective CPI implementation RISC-V CPU using Figure A.29. Assume made following measurements averageCPI instruction types: A.4 [10]<A.9>Compute effective CPI RISC-V using Figure A.29 table above. Average instruction frequencies perlbench sjeng obtain instruction mix. A.5 [10]<A.8>Consider high-level code sequence three statements: A=B+C; B=A+C; D=A–B; Use technique copy propagation (see Figure A.20) transform code sequence point operand computed value. Note theinstances transformation reduced computational work astatement cases work increased. suggest technical challenge faced trying satisfy desire optimizing compilers? A.6 [30]<A.8>Compiler optimizations may result improvements code size and/or performance. Consider one benchmark programs fromthe SPEC CPU2017 EEMBC benchmark suites. Use RISC-V processoror processor available along GNU C compiler optimize thebenchmark program(s) using optimization, –O1, –O2, –O3. Compare performance size resulting programs. Also compare resultsto Figure A.21.Instruction Clock cycles ALU operations 1.0 Loads 3.5Stores 2.8Branches Taken 4.0Not taken 2.0 Jumps 2.4 Average instruction frequencies gobmk mcf obtain instruction mix. may assume instructions (for instructions accounted types Table A.29) require 3.0 clock cycles each.A-48 ■Appendix Instruction Set PrinciplesA.7 [20/20/20/25/10] <A.2, A.9 >Consider following fragment C code: (i =0; i<100; i++) { A[i] =B[i] +C; } Assume AandBare arrays 64-bit integers, Candiare 64-bit inte- gers. Assume data values addresses kept memory (ataddresses 1000, 3000, 5000, 7000 A,B,C, andi, respectively) except operated on. Assume values registers lost iterations loop. Assume addresses words 64 bits. a.[20]<A.2, A.9 >Write code RISC-V. many instructions required dynamically? many memory-data references executed? code size bytes? b.[20]<A.2>Write code x86. many instructions required dynamically? many memory-data references executed? code size bytes? c.[20]<A.2>Write code stack machine. Assume operations occur top stack. Push pop instructions access memory;all others remove operands stack replace result.The implementation uses hardwired stack top two stack entries,which keeps processor circuit small low cost. Additional stackpositions kept memory locations, accesses stack positionsrequire memory references. many instructions required dynamically?How many memory-data references executed? d.[25]<A.2, A.9 >Instead code fragment above, write routine computing matrix multiplication dense, single precision matrices, alsoknown SGEMM. input matrices size 100 /C2100, many instruc- tions required dynamically? many memory-data references executed? e.[10]<A.2, A.9 >As matrix size increases, affect number instructions executed dynamically number memory-datareferences? A.8 [25/25] <A.2, A.8, A.9 >Consider following fragment C code: for(p =0; p <8; p++) { Y[p] =(9798*R[p] +19235*G[p] +3736*B[p])/32768; U[p] =(-4784*R[p]/C09437*G[p] +4221*B[p])/32768 +128; V[p] =(20218*R[p]/C016941*G[p]/C03277*B[p])/32768 +128; } Assume R, G, B, Y, U, andVare arrays 64-bit integers. Assume data values addresses kept memory (at addresses 1000, 2000,3000, 4000, 5000, 6000 R, G ,B,Y, U , V, respectively) except operated on. Assume values registers lost iterations ofthe loop. Assume addresses words 64 bits.Exercises Gregory D. Peterson ■A-49a.[25]<A.2, A.9 >Write code RISC-V. many instructions required dynamically? many memory-data references executed? code size bytes? b.[25]<A.2>Write code x86. many instructions required dynamically? many memory-data references executed? code size bytes? Compare results multimedia instructions(MMX) vector implementations discussed A.8. A.9 [10/10/10/10] <A.2, A.7 >For following, consider instruction encoding instruction set architectures. a.[10]<A.2, A.7 >Consider case processor instruction length 14 bits 64 general-purpose registers size address fields is6 bits. possible instruction encodings following? ■3 two-address instructions ■63 one-address instructions ■45 zero-address instructions b.[10]<A.2, A.7 >Assuming instruction length address field sizes above, determine possible ■3 two-address instructions ■65 one-address instructions ■35 zero-address instructions Explain answer. c.[10]<A.2, A.7 >Assume instruction length address field sizes above. assume already 3 two-address 24 zero-addressinstructions. maximum number one-address instructions encoded processor? d.[10]<A.2, A.7 >Assume instruction length address field sizes above. assume already 3 two-address 65 zero-address instructions. maximum number one-address instructions encoded processor? A.10 [10/15] <A.2>For following assume integer values A,B,C,D,E, F reside memory. Also assume instruction operation codes represented in8 bits, memory addresses 64 bits, register addresses 6 bits. a.[10]<A.2>For instruction set architecture shown Figure A.2, many addresses, names, appear instruction code computeC=A+B, total code size? b.[15]<A.2>Some instruction set architectures Figure A.2 destroy operands course computation. loss data values processorinternal storage performance consequences. architecture FigureA.2, write code sequence compute: C=A+B D=A–E F=C+DA-50 ■Appendix Instruction Set PrinciplesIn code, mark operand destroyed execution mark each“overhead ”instruction included overcome loss data processor internal storage. total code size, number bytes ofinstructions data moved memory, number overhead instruc-tions, number overhead data bytes code sequences? A.11 [15]<A.2, A.7, A.9 >The design RISC-V provides 32 general-purpose reg- isters 32 floating-point registers. registers good, registers bet-ter? List discuss many trade-offs considered byinstruction set architecture designers examining whether to, much to,increase number RISC-V registers. A.12 [5]<A.3>Consider C struct includes following members: struct foo { char a;bool b;int c;double d;short e; float f; double g;char *cptr; float *fptr; int x; }; Note C, compiler must keep elements struct order given struct definition. 32-bit machine, size foo struct? minimum size required struct, assuming mayarrange order struct members wish? 64-bit machine? A.13 [30]<A.7>Many computer manufacturers include tools simulators allow measure instruction set usage user program. Among themethods use machine simulation, hardware-supported trapping, tech-niques instrument object code module inserting counters softwareor using built-in hardware counters. Pick processor tools instrument userprograms. (The open source RISC-V architecture supports collection tools.Tools Performance API (PAPI) work x86 processors.) Use theprocessor tools measure instruction set mix one SPEC CPU2017 benchmarks. Compare results shown chapter. A.14 [30]<A.8>Newer processors Intel's i7 Kaby Lake include support AVX2 vector/multimedia instructions. Write dense matrix multiply function using single-precision values compile different compilers optimi-zation flags. Linear algebra codes using Basic Linear Algebra Subroutine (BLAS)routines SGEMM include optimized versions dense matrix multiply.Compare code size performance code BLAS SGEMM.Explore happens using double-precision values DGEMM.Exercises Gregory D. Peterson ■A-51A.15 [30]<A.8>For SGEMM code developed i7 processor, include use AVX2 intrinsics improve performance. particular, try vec- torize code better utilize AVX hardware. Compare code size andperformance original code. Compare results Intel's Math KernelLibrary (MKL) implementation SGEMM. A.16 [30]<A.7, A.9 >The RISC-V processor open source boasts impressive collection implementations, simulators, compilers, tools. See riscv.org overview tools, including spike, simulator RISC-V processors. Usespike another simulator measure instruction set mix SPECCPU2017 benchmark programs. A.17 [35/35/35/35] <A.2–A.8>gcc targets modern instruction set architectures (seewww.gnu.org/software/gcc/ ). Create version gcc several architectures access to, x86, RISC-V, PowerPC, ARM. a.[35]<A.2–A.8>Compile subset SPEC CPU2017 integer benchmarks create table code sizes. architecture best program? b.[35]<A.2–A.8>Compile subset SPEC CPU2017 floating-point benchmarks create table code sizes. architecture best eachprogram? c.[35]<A.2–A.8>Compile subset EEMBC AutoBench benchmarks (see www.eembc.org/home.php ) create table code sizes. architecture best program? d.[35]<A.2–A.8>Compile subset EEMBC FPBench floating-point bench- marks create table code sizes. architecture best program? A.18 [40]<A.2–A.8>Power efficiency become important modern proces- sors, particularly embedded systems. Create version gcc two architec- tures access to, x86, RISC-V, PowerPC, Atom, ARM.(Note different versions RISC-V also explored compared.)Compile subset EEMBC benchmarks using EnergyBench measureenergy usage execution. Compare code size, performance, energyusage processors. best program? A.19 [20/15/15/20] task compare memory efficiency four different styles instruction set architectures. architecture styles are: ■Accumulator —All operations occur single register memory location. ■Memory-memory —All instruction addresses reference memory locations. ■Stack —All operations occur top stack. Push pop instructions access memory; others remove operands thestack replace result. implementation uses hardwiredstack top two stack entries, keeps processor circuit verysmall low cost. Additional stack positions kept memory loca-tions, accesses stack positions require memory references.A-52 ■Appendix Instruction Set Principles■Load-store —All operations occur registers, register-to-register instruc- tions three register names per instruction. measure memory efficiency, make following assumptions four instruction sets: ■All instructions integral number bytes length. ■The opcode always one byte (8 bits). ■Memory accesses use direct, absolute, addressing. ■The variables A,B,C, Dare initially memory. a.[20]<A.2, A.3 >Invent assembly language mnemonics (Figure A.2 provides useful sample generalize), architecture writethe best equivalent assembly language code high-level language codesequence: A=B+C; B=A+C; D=A–B; b.[15]<A.3>Label instance assembly codes part (a) value loaded memory loaded once. Also label eachinstance code result one instruction passed another instruction operand, classify events involving storage within processor storage memory. c.[15]<A.7>Assume given code sequence small, embedded computer application uses 16-bit memory address data operands. Ifa load-store architecture used, assume 16 general-purpose registers.For architecture answer following questions: many instruction bytesare fetched? many bytes data transferred from/to memory? Whicharchitecture efficient measured total memory traffic (code+data)? d.[20]<A.7>Now assume processor 64-bit memory addresses data operands. architecture answer questions part (c). therelative merits architectures changed chosen metrics? A.20 [30]<A.2, A.3 >Use four different instruction set architecture styles above, assume memory operations supported include register indirectas well direct addressing. Invent assembly language mnemonics (Fig-ure A.2 provides useful sample generalize), architecture, write best equivalent assembly language code fragment C code: (i =0; <= 100; i++) { A[i] =B[i]* C+ ; } Assume AandBare arrays 64-bit integers, C, D, andiare 64-bit integers.Exercises Gregory D. Peterson ■A-53A.21 [20/20] <A.3, A.6, A.9 >The size displacement values needed displace- ment addressing mode PC-relative addressing extracted com- piled applications. Use disassembler one SPEC CPU2017or EEMBC benchmarks compiled RISC-V processor. a.[20]<A.3, A.9 >For instruction using displacement addressing, record displacement value used. Create histogram displacement values. Com-pare results shown appendix Figure A.8. b.[20]<A.6, A.9 >For branch instruction using PC-relative addressing, record offset value used. Create histogram offset values. Comparethe results shown chapter Figure A.15. A.22 [15/15/10/10/10/10] <A.3>The value represented hexadecimal number 5249 5343 5643 5055 stored aligned 64-bit double word. a.[15]<A.3>Using physical arrangement first row Figure A.5, write value stored using Big Endian byte order. Next, interpret eachbyte ASCII character byte write corresponding char-acter, forming character string would stored Big Endian order. b.[15]<A.3>Using physical arrangement part (a), write value stored using Little Endian byte order, byte write cor-responding ASCII character. c.[10]<A.3>What hexadecimal values misaligned 2-byte words read given 64-bit double word stored Big Endianbyte order? d.[10]<A.3>What hexadecimal values misaligned 2-byte words read given 64-bit double word stored Big Endianbyte order? e.[10]<A.3>What hexadecimal values misaligned 2-byte words read given 64-bit double word stored Little Endianbyte order? f.[10]<A.3>What hexadecimal values misaligned 4-byte words read given 64-bit double word stored Little Endian byte order? A.23 [25,25] <A.3, A.9 >The relative frequency different addressing modes impacts choices addressing modes support instruction set architecture. Figure A.7 illustrates relative frequency addressing modes three applications theVAX. a.[25]<A.3>Compile one programs SPEC CPU2017 EEMBC benchmark suites target x86 architecture. Using disassembler,inspect instructions relative frequency various addressing modes.Create histogram illustrate relative frequency addressing modes.How results compare Figure A.7?A-54 ■Appendix Instruction Set Principlesb.[25]<A.3, A.9 >Compile one programs SPEC CPU2017 EEMBC benchmark suites target RISC-V architecture. Using disassem- bler, inspect instructions relative frequency various addressingmodes. Create histogram illustrate relative frequency addressingmodes. results compare Figure A.7? A.24 [Discussion] <A.2–A.12>Consider typical applications desktop, server, cloud, embedded computing. would instruction set architecture beimpacted machines targeting markets?Exercises Gregory D. Peterson ■A-55B.1 Introduction B-2 B.2 Cache Performance B-15 B.3 Six Basic Cache Optimizations B-22 B.4 Virtual Memory B-40 B.5 Protection Examples Virtual Memory B-49 B.6 Fallacies Pitfalls B-57 B.7 Concluding Remarks B-59 B.8 Historical Perspective References B-59 Exercises Amr Zaky B-60B Review Memory Hierarchy Cache: safe place hiding storing things. Webster ’s New World Dictionary American Language, Second College Edition (1976)B.1 Introduction appendix quick refresher memory hierarchy, including basics cache virtual memory, performance equations, simple optimizations. Thisfirst section reviews following 36 terms: cache fully associative write allocate virtual memory dirty bit unified cachememory stall cycles block offset misses per instructiondirect mapped write back blockvalid bit data cache localityblock address hit time address tracewrite cache miss setinstruction cache page fault random replacementaverage memory access time miss rate index fieldcache hit n-way set associative no-write allocatepage least recently used write buffermiss penalty tag field write stall review goes quickly, might want look Chapter 7 inComputer Organization Design , wrote readers less experience. Cache name given highest first level memory hierarchy encountered address leaves processor. principle local-ity applies many levels, taking advantage locality improve performanceis popular, term cache applied whenever buffering employed reuse commonly occurring items. Examples include file caches ,name caches , on. processor finds requested data item cache, called cache hit. processor find data item needs cache, cache miss occurs. fixed-size collection data containing requested word, called ablock line run, retrieved main memory placed cache. Temporal locality tells us likely need word near future, useful place cache accessed quickly.Because spatial locality , high probability data block needed soon. time required cache miss depends latency band- width memory. Latency determines time retrieve first word block, bandwidth determines time retrieve rest block.A cache miss handled hardware causes processors using in-order execu-tion pause, stall, data available. out-of-order execution, aninstruction using result must still wait, instructions may proceed dur-ing miss. Similarly, objects referenced program need reside main mem- ory.Virtual memory means objects may reside disk. address space isB-2 ■Appendix B Review Memory Hierarchyusually broken fixed-size blocks, called pages . time, page resides either main memory disk. processor references item within page present cache main memory, paltoccurs, entire page moved disk main memory. page faults take long,they handled software processor stalled. processor usuallyswitches task disk access occurs. high-level per-spective, reliance locality references relative relationships sizeand relative cost per bit cache versus main memory similar mainmemory versus disk. Figure B.1 shows range sizes access times level mem- ory hierarchy computers ranging high-end desktops low-end servers. Cache Performance Review locality higher speed smaller memories, memory hierarchy substantially improve performance. One method evaluate cache perfor-mance expand processor execution time equation Chapter 1 .W e account number cycles processor stalled waitingfor memory access, call memory stall cycles . performance product clock cycle time sum processor cycles memory stall cycles: CPU execution time ¼CPU clock cycles + Memory stall cyclesðÞ /C2 Clock cycle timeLevel 1 2 3 4 Name Registers Cache Main memory Disk storage Typical size <4 KiB 32 KiB 8 MiB <1T B >1T B Implementation technology Custom memory multiple ports, CMOSOn-chip CMOS SRAMCMOS DRAM Magnetic disk FLASH Access time (ns) 0.1 –0.2 0.5 –10 30 –150 5,000,000 Bandwidth (MiB/sec) 1,000,000 –10,000,000 20,000 –50,000 10,000 –30,000 100 –1000 Managed Compiler Hardware Operating system Operating system Backed Cache Main memory Disk FLASH disks DVD Figure B.1 typical levels hierarchy slow get larger move away processor large workstation small server. Embedded computers might disk storage much smaller memories caches. Increasingly, FLASH replacing magnetic disks, least first level file storage. access times increase move lower levels hierarchy, makes feasible manage transfer less responsively.The implementation technology shows typical technology used functions. access time given innanoseconds typical values 2017; times decrease time. Bandwidth given megabytes per second levels memory hierarchy. Bandwidth disk/FLASH storage includes media buffered interfaces.B.1 Introduction ■B-3This equation assumes CPU clock cycles include time handle cache hit processor stalled cache miss. Section B.2 reexamines simplifying assumption. number memory stall cycles depends number misses cost per miss, called miss penalty: Memory stall cycles ¼Number misses /C2Miss penalty ¼IC/C2Misses Instruction/C2Miss penalty ¼IC/C2Memory accesses Instruction/C2Miss rate /C2Miss penalty advantage last form components easily measured. already know measure instruction count (IC). (For speculative processors,we count instructions commit.) Measuring number memory refer-ences per instruction done fashion; every instruction requires aninstruction access, easy decide also requires data access. Note calculated miss penalty average, use herein constant. memory behind cache may busy time themiss prior memory requests memory refresh. number clockcycles also varies interfaces different clocks processor, bus, andmemory. Thus, please remember using single number miss penalty asimplification. component miss rate simply fraction cache accesses result miss (i.e., number accesses miss divided number accesses). Miss rates measured cache simulators take address trace instruction data references, simulate cache behavior determine references hitand miss, report hit miss totals. Many microprocessorstoday provide hardware count number misses memory references,which much easier faster way measure miss rate. preceding formula approximation miss rates miss penalties often different reads writes. Memory stall clock cycles couldthen defined terms number memory accesses per instruction, miss penalty (in clock cycles) reads writes, miss rate reads writes: Memory stall clock cycles ¼IC/C2Reads per instruction /C2Read miss rate /C2Read miss penalty +I C/C2Writes per instruction /C2Write miss rate /C2Write miss penalty usually simplify complete formula combining reads writes finding average miss rates miss penalty reads andwrites: Memory stall clock cycles ¼IC/C2Memory accesses Instruction/C2Miss rate /C2Miss penalty miss rate one important measures cache design, but, see later sections, measure.B-4 ■Appendix B Review Memory HierarchyExample Assume computer cycles per instruction (CPI) 1.0 memory accesses hit cache. data accesses loads stores, andthese total 50% instructions. miss penalty 50 clock cycles themiss rate 1%, much faster would computer instructions werecache hits? Answer First compute performance computer always hits: CPU execution time ¼CPU clock cycles + Memory stall cyclesðÞ /C2 Clock cycle ¼IC/C2CPI + 0 ðÞ /C2 Clock cycle ¼IC/C21:0/C2Clock cycle computer real cache, first compute memory stall cycles: Memory stall cycles ¼IC/C2Memory accesses Instruction/C2Miss rate /C2Miss penalty ¼IC/C21+0:5 ðÞ /C2 0:01/C250 ¼IC/C20:75 middle term (1+0.5) represents one instruction access 0.5 data accesses per instruction. total performance thus CPU execution time cache¼IC/C21:0+I C /C20:75 ðÞ /C2 Clock cycle ¼1:75/C2IC/C2Clock cycle performance ratio inverse execution times: CPU execution time cache CPU execution time¼1:75/C2IC/C2Clock cycle 1:0/C2IC/C2Clock cycle ¼1:75 computer cache misses 1.75 times faster. designers prefer measuring miss rate misses per instruction rather misses per memory reference. two related: Misses Instruction¼Miss rate /C2Memory accesses Instruction count¼Miss rate /C2Memory accesses Instruction latter formula useful know average number memory accesses per instruction allows convert miss rate missesper instruction, vice versa. example, turn miss rate per memoryreference previous example misses per instruction: Misses Instruction¼Miss rate /C2Memory accesses Instruction¼0:02/C21:5ðÞ ¼ 0:030B.1 Introduction ■B-5By way, misses per instruction often reported misses per 1000 instructions show integers instead fractions. Thus, preceding answer couldalso expressed 30 misses per 1000 instructions. advantage misses per instruction independent hardware implementation. example, speculative processors fetch twice manyinstructions actually committed, artificially reduce miss rateif measured misses per memory reference rather per instruction. draw-back misses per instruction architecture dependent; example, aver- age number memory accesses per instruction may different 80x86 versus RISC V. Thus, misses per instruction popular architects work-ing single computer family, although similarity RISC architecturesallows one give insights others. Example show equivalency two miss rate equations, let ’s redo preceding example, time assuming miss rate per 1000 instructions 30. ismemory stall time terms instruction count? Answer Recomputing memory stall cycles: Memory stall cycles ¼Number misses /C2Miss penalty ¼IC/C2Misses Instruction/C2Miss penalty ¼IC=1000 /C2Misses Intruction /C21000/C2Miss penalty ¼IC=1000 /C230/C225 ¼IC=1000 /C2750 ¼IC/C20:75 get answer page B-5, showing equivalence two equations. Four Memory Hierarchy Questions continue introduction caches answering four common questions first level memory hierarchy: Q1: block placed upper level? ( block placement ) Q2: block found upper level? ( block identification ) Q3: block replaced miss? ( block replacement ) Q4: happens write? ( write strategy ) answers questions help us understand different trade-offs mem- ories different levels hierarchy; hence, ask four questions every example.B-6 ■Appendix B Review Memory HierarchyQ1: Block Placed Cache? Figure B.2 shows restrictions block placed create three cat- egories cache organization: ■If block one place appear cache, cache said tobedirect mapped . mapping usually (Block address ) MOD (Number blocks cache ) ■If block placed anywhere cache, cache said fully associative . Fully associative: block 12 goanywhereDirect mapped: block 12 goonly block 4(12 MOD 8)Set associative: block 12 goanywhere set 0(12 MOD 4) 01234567 01234567 01234567 Block no.Block no.Block no. Set 0Set 1Set 2Set 3 1111111111222222 2 22233 012345678901234 56789012345678901Block Block frame address no.Cache Memory Figure B.2 example cache eight block frames memory 32 blocks. three options caches shown left right. fully associative, block 12 fromthe lower level go eight block frames cache. direct mapped, block 12 placed block frame 4 (12 modulo 8). Set associative, features, allows block placed anywhere set 0 (12modulo 4). two blocks per set, means block 12 placed either block 0 block 1 cache. Real caches contain thousands block frames, real memories contain millions blocks. set associative organization four sets withtwo blocks per set, called two-way set associative . Assume nothing cache block address question identifies lower-level block 12.B.1 Introduction ■B-7■If block placed restricted set places cache, cache set associative .Asetis group blocks cache. block first mapped onto set, block placed anywhere within set. set usu-ally chosen bit selection ; is, (Block address ) MOD (Number sets cache ) nblocks set, cache placement called n-way set associative . range caches direct mapped fully associative really continuum levels set associativity. Direct mapped simply one-way set associative, anda fully associative cache mblocks could called “m-way set associative. ” Equivalently, direct mapped thought msets, fully associa- tive one set. vast majority processor caches today direct mapped, two-way set associative, four-way set associative, reasons see shortly. Q2: Block Found Cache? Caches address tag block frame gives block address. Thetag every cache block might contain desired information checked tosee matches block address processor. rule, possible tagsare searched parallel speed critical. must way know cache block valid informa- tion. common procedure add valid bit tag say whether entry contains valid address. bit set, cannot matchon address. proceeding next question, let ’s explore relationship pro- cessor address cache. Figure B.3 shows address divided. first division block address block offset. block frame address divided tag field index field. block offset field selects desired data block, index field selects set, tag field compared hit. Although comparison could made onmore address tag, need following: ■The offset used comparison, entire block ispresent not, hence block offsets result match definition. Tag IndexBlock offsetBlock address Figure B.3 three portions address set associative direct- mapped cache. tag used check blocks set, index used select set. block offset address desired data within block. Fully associative caches index field.B-8 ■Appendix B Review Memory Hierarchy■Checking index redundant, used select set checked. address stored set 0, example, must 0 index field ’t stored set 0; set 1 must index value 1; on. optimization saves hardware power reducing width memorysize cache tag. total cache size kept same, increasing associativity increases num- ber blocks per set, thereby decreasing size index increasing thesize tag. is, tag-index boundary Figure B.3 moves right increasing associativity, end point fully associative caches index field. Q3: Block Replaced Cache Miss? miss occurs, cache controller must select block replaced desired data. benefit direct-mapped placement hardware decisions simplified —in fact, simple choice: one block frame checked hit, block replaced. fully associative orset associative placement, many blocks choose miss. Thereare three primary strategies employed selecting block replace: ■Random —To spread allocation uniformly, candidate blocks randomly selected. systems generate pseudorandom block numbers get repro- ducible behavior, particularly useful debugging hardware. ■Least recently used (LRU) —To reduce chance throwing information needed soon, accesses blocks recorded. Relying past predict future, block replaced one unused thelongest time. LRU relies corollary locality: recently used blocksare likely used again, good candidate disposal leastrecently used block. ■First in, first (FIFO) —Because LRU complicated calculate, approximates LRU determining oldest block rather LRU. virtue random replacement simple build hardware. number blocks keep track increases, LRU becomes increasingly expensiveand usually approximated. common approximation (often called pseudo- LRU) set bits set cache bit corresponding single way (a way bank set associative cache; four ways four-way set associative cache) cache. set accessed, bit corre-sponding way containing desired block turned on; bits asso-ciated set turned on, reset exception recentlyturned bit. block must replaced, processor chooses blockfrom way whose bit turned off, often randomly one choice isavailable. approximates LRU, block replaced haveB.1 Introduction ■B-9been accessed since last time blocks set accessed. Figure B.4 shows difference miss rates LRU, random, FIFO replacement. Q4: Happens Write? Reads dominate processor cache accesses. instruction accesses reads, andmost instructions ’t write memory. Figures A.32 A.33 Appendix suggest mix 10% stores 26% loads RISC V programs, making writes10%/(100%+26%+10%) 7% overall memory traffic. data cache traffic, writes 10%/(26%+10%) 28%. Making common case fast means optimizing caches reads, especially processors traditionally wait reads complete need wait writes. Amdahl ’s Law (Section 1.9) reminds us, however, high-performance designs cannot neglect speed ofwrites. Fortunately, common case also easy case make fast. block read cache time tag read compared, theblock read begins soon block address available. read hit, therequested part block passed processor immediately. miss,there benefit —but also harm except power desktop server computers; ignore value read. optimism allowed writes. Modifying block cannot begin tag checked see address hit. tag checking cannot occur inparallel, writes usually take longer reads. Another complexity pro-cessor also specifies size write, usually 1 8 bytes; thatportion block changed. contrast, reads access bytes thannecessary without fear. write policies often distinguish cache designs. two basic options writing cache:Associativity Two-way Four-way Eight-way Size LRU Random FIFO LRU Random FIFO LRU Random FIFO 16 KiB 114.1 117.3 115.5 111.7 115.1 113.3 109.0 111.8 110.4 64 KiB 103.4 104.3 103.9 102.4 102.3 103.1 99.7 100.5 100.3 256 KiB 92.2 92.1 92.5 92.1 92.1 92.5 92.1 92.1 92.5 Figure B.4 Data cache misses per 1000 instructions comparing least recently used, random, first in, first replacement several sizes associativities. little difference LRU random largest size cache, LRU outperforming others smaller caches. FIFO generally outperforms random smaller cache sizes. data collected block size 64 bytes Alpha architecture using 10 SPEC2000benchmarks. Five SPECint2000 (gap, gcc, gzip, mcf, perl) five SPECfp2000 (applu, art, equake, lucas, swim). use computer benchmarks figures appendix.B-10 ■Appendix B Review Memory Hierarchy■Write —The information written block cache andto block lower-level memory. ■Write back —The information written block cache. modified cache block written main memory replaced. reduce frequency writing back blocks replacement, feature called thedirty bit commonly used. status bit indicates whether block dirty (modified cache) clean (not modified). clean, block written back miss, identical information cache found lowerlevels. write back write advantages. write back, writes occur speed cache memory, multiple writes within blockrequire one write lower-level memory. writes ’tg ot memory, write back uses less memory bandwidth, making write back attractive multiprocessors. Since write back uses rest memory hierarchy mem- ory interconnect less write through, also saves power, making attractivefor embedded applications. Write easier implement write back. cache always clean, unlike write back read misses never result writes lower level.Write also advantage next lower level currentcopy data, simplifies data coherency. Data coherency important formultiprocessors I/O, examine Chapter 4 Appendix D. Multilevel caches make write viable upper-level caches, writes need propagate next lower level rather way tomain memory. see, I/O multiprocessors fickle: want write back processor caches reduce memory traffic write keep cacheconsistent lower levels memory hierarchy. processor must wait writes complete write through, processor said write stall. common optimization reduce write stalls write buffer, allows processor continue soon data written buffer, thereby overlapping processor execution memory updating. Aswe see shortly, write stalls occur even write buffers. data needed write, two options write miss: ■Write allocate —The block allocated write miss, followed pre- ceding write hit actions. natural option, write misses act like read misses. ■No-write allocate —This apparently unusual alternative write misses affect cache. Instead, block modified lower-level memory. Thus, blocks stay cache no-write allocate program tries read blocks, even blocks written still cache withwrite allocate. Let ’s look example.B.1 Introduction ■B-11Example Assume fully associative write-back cache many cache entries starts empty. Following sequence five memory operations (the address squarebrackets): Write Mem[100]; Write Mem[100];Read Mem[200];Write Mem[200];Write Mem[100]. number hits misses using no-write allocate versus write allocate? Answer no-write allocate, address 100 cache, allocation write, first two writes result misses. Address 200 also cache, read also miss. subsequent write address 200 hit. last write 100 still miss. result no-write allocate four misses andone hit. write allocate, first accesses 100 200 misses, rest hits 100 200 found cache. Thus, result writeallocate two misses three hits. Either write miss policy could used write write back. Usually, write-back caches use write allocate, hoping subsequent writes thatblock captured cache. Write-through caches often use no-write allo-cate. reasoning even subsequent writes block, writes must still go lower-level memory, ’s gained? Example: Opteron Data Cache give substance ideas, Figure B.5 shows organization data cache AMD Opteron microprocessor. cache contains 65,536 (64 K)bytes data 64-byte blocks two-way set associative placement, least-recently used replacement, write back, write allocate write miss. Let’s trace cache hit steps hit labeled Figure B.5 . (The four steps shown circled numbers.) described Section B.5 , Opteron presents 48-bit virtual address cache tag comparison, simul- taneously translated 40-bit physical address. reason Opteron ’t use 64 bits virtual address designers don’t think anyone needs much virtual address space yet, smaller size simplifies Opteron virtual address mapping. designers plan grow thevirtual address future microprocessors. physical address coming cache divided two fields: 34-bit block address 6-bit block offset (64 ¼2 6and 34+6 ¼40). blockB-12 ■Appendix B Review Memory Hierarchyaddress divided address tag cache index. Step 1 shows division. cache index selects tag tested see desired block cache. size index depends cache size, block size, set associativity.For Opteron cache set associativity set two, calculate indexas follows: 2Index¼Cache size Block size /C2Set associativity¼65,536 64/C22¼512¼29<25> Tag (512 blocks)(512 blocks)Index<9>Block offset <6>Block address Valid <1>Data <64>CPU address Victim bufferData inData Tag <25> =? 2:1 mux Lower-level memory=?2 21 3 3 4 Figure B.5 organization data cache Opteron microprocessor. 64 KiB cache two-way set associative 64-byte blocks. 9-bit index selects among 512 sets. four steps read hit, shown circled numbers order occurrence, label organization. Three bits block offset join index supply theRAM address select proper 8 bytes. Thus, cache holds two groups 4096 64-bit words, group containing half 512 sets. Although exercised example, line lower-level memory cache used miss load cache. size address leaving processor 40 bits physicaladdress virtual address. Figure B.24 page B-47 explains Opteron maps virtual physical cache access.B.1 Introduction ■B-13Hence, index 9 bits wide, tag 34 /C09 25 bits wide. Although index needed select proper block, 64 bytes much pro- cessor wants consume once. Hence, makes sense organize dataportion cache memory 8 bytes wide, natural data word 64-bit Opteron processor. Thus, addition 9 bits index proper cache block, 3more bits block offset used index proper 8 bytes. Index selectionis step 2 Figure B.5 . reading two tags cache, compared tag por- tion block address processor. comparison step 3 figure. sure tag contains valid information, valid bit must set else results comparison ignored. Assuming one tag match, final step signal processor load proper data cache using winning input 2:1 mul-tiplexor. Opteron allows 2 clock cycles four steps, instruc-tions following 2 clock cycles would wait tried use result ofthe load. Handling writes complicated handling reads Opteron, cache. word written cache, first three steps same. Opteron executes order, signals theinstruction committed cache tag comparison indicates hit datawritten cache. far assumed common case cache hit. happens miss? read miss, cache sends signal processor telling dataare yet available, 64 bytes read next level hierarchy.The latency 7 clock cycles first 8 bytes block, 2 clock cycles per 8 bytes rest block. data cache set associa- tive, choice block replace. Opteron uses LRU, selectsthe block referenced longest ago, every access must update LRUbit. Replacing block means updating data, address tag, valid bit, andthe LRU bit. Opteron uses write back, old data block could mod- ified, hence cannot simply discarded. Opteron keeps 1 dirty bit perblock record block written. “victim ”was modified, data address sent victim buffer. (This structure similar write buffer computers.) Opteron space eight victim blocks. parallel withother cache actions, writes victim blocks next level hierarchy. thevictim buffer full, cache must wait. write miss similar read miss, Opteron allocates block read write miss. seen works, data cache cannot supply memory needs processor: processor also needs instructions. Although single cache could try supply both, bottleneck. example, load store instruction executed, pipelined processor simultaneously requestboth data word andan instruction word. Hence, single cache would present structural hazard loads stores, leading stalls. One simple way conquerB-14 ■Appendix B Review Memory Hierarchythis problem divide it: one cache dedicated instructions another data. Separate caches found recent processors, including Opteron.Hence, 64 KiB instruction cache well 64 KiB data cache. processor knows whether issuing instruction address data address, separate ports both, thereby doubling bandwidth memory hierarchy processor. Separate caches also offer opportunity optimizing cache separately: different capacities, block sizes,and associativities may lead better performance. (In contrast instructioncaches data caches Opteron, terms unified ormixed applied caches contain either instructions data.) Figure B.6 shows instruction caches lower miss rates data caches. Separating instructions data removes misses due conflicts betweeninstruction blocks data blocks, split also fixes cache space devoted type. important miss rates? fair comparison separate instruction data caches unified caches requires total cache size thesame. example, separate 16 KiB instruction cache 16 KiB data cacheshould compared 32 KiB unified cache. Calculating average miss ratewith separate instruction data caches necessitates knowing percentage ofmemory references cache. data Appendix find split 100%/(100%+26%+10%) 74% instruction references (26%+10%)/(100%+26%+10%) 26% data references. Splitting affects performance beyond indicated change miss rates, see shortly. B.2 Cache Performance instruction count independent hardware, tempting evaluate processor performance using number. indirect performance measureshave waylaid many computer designer. corresponding temptation eval- uating memory hierarchy performance concentrate miss rate it,Size (KiB)Instruction cache Data cacheUnified cache 8 8.16 44.0 63.0 16 3.82 40.9 51.032 1.36 38.4 43.364 0.61 36.9 39.4128 0.30 35.3 36.2256 0.02 32.6 32.9 Figure B.6 Miss per 1000 instructions instruction, data, unified caches different sizes. percentage instruction references 74%. data two-way associative caches 64-byte blocks computer bench-marks Figure B.4 .B.2 Cache Performance ■B-15too, independent speed hardware. see, miss rate misleading instruction count. better measure memory hierarchy per- formance average memory access time : Average memory access time ¼Hit time + Miss rate /C2Miss penalty hit time time hit cache; seen two terms before. components average access time measured either absolutetime—say, 0.25 –1.0 ns hit —or number clock cycles proces- sor waits memory —such miss penalty 150 –200 clock cycles. Remember average memory access time still indirect measure perfor- mance; although better measure miss rate, substitute execution time. formula help us decide split caches unified cache. Example lower miss rate: 16 KiB instruction cache 16 KiB data cache 32 KiB unified cache? Use miss rates Figure B.6 help calculate correct answer, assuming 36% instructions data transfer instructions.Assume hit takes 1 clock cycle miss penalty 100 clock cycles. loador store hit takes 1 extra clock cycle unified cache one cache port satisfy two simultaneous requests. Using pipelining terminology Chapter 3 , unified cache leads structural hazard. average mem- ory access time case? Assume write-through caches write buffer andignore stalls due write buffer. Answer First let ’s convert misses per 1000 instructions miss rates. Solving preced- ing general formula, miss rate Miss rate ¼Misses 1000 Instructions=1000 Memory accesses Instruction every instruction access exactly one memory access fetch instruction, instruction miss rate Miss rate 16KB instruction ¼3:82=1000 1:00¼0:004 36% instructions data transfers, data miss rate Miss rate 16KB data ¼40:9=1000 0:36¼0:114 unified miss rate needs account instruction data accesses: Miss rate 32KB unified ¼43:3=1000 1:00 + 0 :36¼0:0318B-16 ■Appendix B Review Memory HierarchyAs stated herein, 74% memory accesses instruction references. Thus, overall miss rate split caches 74%/C20:004 ðÞ +2 6 %/C20:114 ðÞ ¼ 0:0326 Thus, 32 KiB unified cache slightly lower effective miss rate two 16 KiB caches. average memory access time formula divided instruction data accesses: Average memory access time ¼%instructions /C2Hit time + Instruction miss rate /C2Miss penalty ðÞ +%data/C2Hit time + Data miss rate /C2Miss penalty ðÞ Therefore, time organization Average memory access time split ¼74%/C21+0:004/C2200 ðÞ +2 6%/C21+0:114/C2200 ðÞ ¼74%/C21:80 ðÞ +2 6 %/C223:80 ðÞ ¼ 1:332 + 6 :188¼7:52 Average memory access time unified ¼74%/C21+0:0318 /C2200 ðÞ +2 6%/C21+1+0 :0318 /C2200 ðÞ ¼74%/C27:36 ðÞ +2 6 %/C28:36 ðÞ ¼ 5:446 + 2 :174¼7:62 Hence, split caches example —which offer two memory ports per clock cycle, thereby avoiding structural hazard —have better average memory access time single-ported unified cache despite worse effective miss rate. Average Memory Access Time Processor Performance obvious question whether average memory access time due cache misses predicts processor performance. First, reasons stalls, contention due I/O devices using memory. Designers often assume memory stalls due cache mis-ses, memory hierarchy typically dominates reasons stalls. Weuse simplifying assumption here, sure account allmemory stalls calculating final performance. Second, answer also depends processor. in-order exe- cution processor (see Chapter 3 ), answer basically yes. processor stalls misses, memory stall time strongly correlated averagememory access time. Let ’s make assumption now, ’ll return out-of-order processors next subsection.B.2 Cache Performance ■B-17As stated previous section, model CPU time as: CPU time ¼CPU execution clock cycles + Memory stall clock cyclesðÞ /C2Clock cycle time formula raises question whether clock cycles cache hit considered part CPU execution clock cycles part memory stall clock cycles. Although either convention defensible, widely accepted include hit clock cycles CPU execution clock cycles. explore impact caches performance. Example Let’s use in-order execution computer first example. Assume cache miss penalty 200 clock cycles, instructions usually take 1.0 clockcycles (ignoring memory stalls). Assume average miss rate 2%, isan average 1.5 memory references per instruction, average number cache misses per 1000 instructions 30. impact performance behavior cache included? Calculate impact using misses perinstruction miss rate. Answer CPU time ¼IC/C2CPI execution +Memory stall clock cycles Instruction/C18/C19 /C2Clock cycle time performance, including cache misses, CPU time cache ¼IC/C21:0+ 3 0 =1000 /C2200 ðÞ ½/C138 /C2 Clock cycle time ¼IC/C27:00/C2Clock cycle time calculating performance using miss rate: CPU time ¼IC/C2CPI execution + Miss rate /C2Memory accesses Instruction/C2Miss penalty/C18/C19 /C2Clock cycle time CPU time cache ¼IC/C21:0+ 1 :5/C22%/C2200 ðÞ ½/C138 /C2 Clock cycle time ¼IC/C27:00/C2Clock cycle time clock cycle time instruction count same, without cache. Thus, CPU time increases sevenfold, CPI 1.00 “perfect cache ”to 7.00 cache miss. Without memory hierarchy CPI would increase 1.0+200 /C21.5 301 —a factor 40 times longer system cache! example illustrates, cache behavior enormous impact per- formance. Furthermore, cache misses double-barreled impact proces-sor low CPI fast clock: 1.The lower CPI execution , higher relative impact fixed number cache miss clock cycles. 2.When calculating CPI, cache miss penalty measured processor clock cycles miss. Therefore, even memory hierarchies two computers areB-18 ■Appendix B Review Memory Hierarchyidentical, processor higher clock rate larger number clock cycles per miss hence higher memory portion CPI. importance cache processors low CPI high clock rates thus greater, and, consequently, greater danger neglecting cache behaviorin assessing performance computers. Amdahl ’s Law strikes again! Although minimizing average memory access time reasonable goal —and use much appendix —keep mind final goal reduce processor execution time. next example shows two differ. Example impact two different cache organizations performance processor? Assume CPI perfect cache 1.0, clock cycle timeis 0.35 ns, 1.4 memory references per instruction, size cachesis 128 KiB, block size 64 bytes. One cache direct mapped andthe two-way set associative. Figure B.5 shows set associative caches must add multiplexor select blocks set dependingon tag match. speed processor tied directly speed cache hit, assume processor clock cycle time must stretched 1.35 times accommodate selection multiplexor set associative cache.To first approximation, cache miss penalty 65 ns either cache orga-nization. (In practice, normally rounded integer number ofclock cycles.) First, calculate average memory access time processorperformance. Assume hit time 1 clock cycle, miss rate direct-mapped128 KiB cache 2.1%, miss rate two-way set associative cache thesame size 1.9%. Answer Average memory access time Average memory access time ¼Hit time + Miss rate /C2Miss penalty Thus, time organization Average memory access time 1-way ¼0:35 + :021/C265 ðÞ ¼1:72ns Average memory access time 2-way ¼0:35/C21:35 + :019/C265 ðÞ ¼ 1:71ns average memory access time better two-way set-associative cache. processor performance CPU time ¼IC/C2CPI execution +Misses Instruction/C2Miss penalty/C18/C19 /C2Clock cycle time ¼IC/C2CPI execution /C2Clock cycle time ðÞ½ + Miss rate /C2Memory accesses Instruction/C2Miss penalty /C2Clock cycle time/C18/C19 /C21B.2 Cache Performance ■B-19Substituting 65 ns (Miss penalty /C2Clock cycle time), performance cache organization CPU time 1-way ¼IC/C21:0/C20:35 + 0 :021/C21:4/C265 ðÞ ½/C138 ¼ 2:26/C2IC CPU time 2-way ¼IC/C21:0/C20:35/C21:35 + 0 :019/C21:4/C265 ðÞ ½/C138 ¼ 2:20/C2IC relative performance CPU time 2-way CPU time 1-way¼2:26/C2Instruction count 2:20/C2Instruction count¼1:03 contrast results average memory access time comparison, direct- mapped cache leads slightly better average performance clock cycleis stretched allinstructions two-way set associative case, even fewer misses. CPU time bottom-line evaluation becausedirect mapped simpler build, preferred cache direct mapped thisexample. Miss Penalty Out-of-Order Execution Processors out-of-order execution processor, define “miss penalty ”?I si full latency miss memory, “exposed ”or nonoverlapped latency processor must stall? question arise processorsthat stall data miss completes. Let’s redefine memory stalls lead new definition miss penalty non- overlapped latency: Memory stall cycles Instruction¼Misses Instruction/C2Total miss latency /C0Overlapped miss latency ðÞ Similarly, out-of-order processors stretch hit time, portion performance equation could divided total hit latency less overlapped hitlatency. equation could expanded account contention formemory resources out-of-order processor dividing total miss latency intolatency without contention latency due contention. Let ’s concentrate miss latency. decide following: ■Length memory latency —What consider start end mem- ory operation out-of-order processor. ■Length latency overlap —What start overlap processor (or, equivalently, say memory operation stalling theprocessor)?B-20 ■Appendix B Review Memory HierarchyGiven complexity out-of-order execution processors, single cor- rect definition. committed operations seen retirement pipeline stage, say processor stalled clock cycle retire maximum pos-sible number instructions cycle. attribute stall first instruc-tion could retired. definition means foolproof. Forexample, applying optimization improve certain stall time may alwaysimprove execution time another type stall —hidden behind targeted stall—may exposed. latency, could start measuring time memory instruction queued instruction window, address generated, theinstruction actually sent memory system. option works long isused consistent fashion. Example Let’s redo preceding example, time assume processor longer clock cycle time supports out-of-order execution yet still direct-mapped cache. Assume 30% 65 ns miss penalty overlapped; is, average CPU memory stall time 45.5 ns. Answer Average memory access time out-of-order (OOO) computer Average memory access time 1-way ,OOO¼0:35/C21:35 + 0 :021/C245:5 ðÞ ¼ 1:43ns performance OOO cache CUP time 1-way ,OOO¼IC/C21:6/C20:35/C21:35 + 0 :021/C21:4/C245:5 ðÞ ½/C138 ¼ 2:09/C2IC Hence, despite much slower clock cycle time higher miss rate direct- mapped cache, out-of-order computer slightly faster hide 30% ofthe miss penalty. summary, although state art defining measuring memory stalls out-of-order processors complex, aware issues signif-icantly affect performance. complexity arises out-of-order processorstolerate latency due cache misses without hurting performance. Conse- quently, designers usually use simulators out-of-order processor mem- ory evaluating trade-offs memory hierarchy sure animprovement helps average memory latency actually helps programperformance. help summarize section act handy reference, Figure B.7 lists cache equations appendix.B.2 Cache Performance ■B-21B.3 Six Basic Cache Optimizations average memory access time formula gave us framework present cache optimizations improving cache performance: Average memory access time ¼Hit time + Miss rate /C2Miss penalty Hence, organize six cache optimizations three categories: ■Reducing miss rate —larger block size, larger cache size, higher associativity ■Reducing miss penalty —multilevel caches giving reads priority writes ■Reducing time hit cache —avoiding address translation indexing cache Figure B.18 page B-40 concludes section summary implemen- tation complexity performance benefits six techniques.2index¼Cache size Block size /C2Set associativity CPU execution time ¼CPU clock cycles + Memory stall cyclesðÞ /C2 Clock cycle time Memory stall cycles ¼Number misses /C2Miss penalty Memory stall cycles ¼IC/C2Misses Instruction/C2Miss penalty Misses Instruction¼Miss rate /C2Memory accesses Instruction Average memory access time ¼Hit time + Miss rate /C2Miss penalty CPU execution time ¼IC/C2CPI execution +Memory stall clock cycles Instruction/C18/C19 /C2Clock cycle time CPU execution time ¼IC/C2CPI execution +Misses Instruction/C2Miss penalty/C18/C19 /C2Clock cycle time CPU execution time ¼IC/C2CPI execution + Miss rate /C2Memory accesses Instruction/C2Miss penalty/C18/C19 /C2Clock cycle time Memory stall cycles Instruction¼Misses Instruction/C2Total miss latency /C0Overlapped miss latency ðÞ Average memory access time ¼Hit time L1+ Miss rate L1/C2Hit time L2+ Miss rate L2/C2Miss penaltyL2 ðÞ Memory stall cycles Instruction¼Misses L1 Instruction/C2Hit time L2+Misses L2 Instruction/C2Miss penaltyL2 Figure B.7 Summary performance equations appendix. first equation calculates cache index size, rest help evaluate performance. final two equations deal multilevel caches, explainedearly next section. included help make figure useful reference.B-22 ■Appendix B Review Memory HierarchyThe classical approach improving cache behavior reduce miss rates, present three techniques so. gain better insights causes mis- ses, first start model sorts misses three simple categories: ■Compulsory —The first access block cannot cache, block must brought cache. also called cold-start misses orfirst-reference misses. ■Capacity —If cache cannot contain blocks needed execution program, capacity misses (in addition compulsory misses) occur blocks discarded later retrieved. ■Conflict —If block placement strategy set associative direct mapped, conflict misses (in addition compulsory capacity misses) occurbecause block may discarded later retrieved many blocksmap set. misses also called collision misses . idea hits fully associative cache become misses n-way set-associative cache due nrequests popular sets. (Chapter 5 adds fourth C, coherency misses due cache flushes keep mul- tiple caches coherent multiprocessor; ’t consider here.) Figure B.8 shows relative frequency cache misses, broken three C ’s. Compulsory misses occur infinite cache. Capacity misses occur fully associative cache. Conflict misses thosethat occur going fully associative eight-way associative, four-way associa- tive, on. Figure B.9 presents data graphically. top graph shows absolute miss rates; bottom graph plots percentage misses typeof miss function cache size. show benefit associativity, conflict misses divided misses caused decrease associativity. four divisions conflictmisses calculated: ■Eight-way —Conflict misses due going fully associative (no conflicts) eight-way associative ■Four-way —Conflict misses due going eight-way associative four- way associative ■Two-way —Conflict misses due going four-way associative two-way associative ■One-way —Conflict misses due going two-way associative one-way associative (direct mapped) see figures, compulsory miss rate SPEC2000 programs small, many long-running programs. identified three C ’s, computer designer them? Conceptually, conflicts easiest: Fully associative placement avoids allB.3 Six Basic Cache Optimizations ■B-23Cache size (KiB)Degree associativeTotal miss rateMiss rate components (relative percent) (sum5100% total miss rate) Compulsory Capacity Conflict 4 1-way 0.098 0.0001 0.1% 0.070 72% 0.027 28% 4 2-way 0.076 0.0001 0.1% 0.070 93% 0.005 7%4 4-way 0.071 0.0001 0.1% 0.070 99% 0.001 1%4 8-way 0.071 0.0001 0.1% 0.070 100% 0.000 0%8 1-way 0.068 0.0001 0.1% 0.044 65% 0.024 35%8 2-way 0.049 0.0001 0.1% 0.044 90% 0.005 10%8 4-way 0.044 0.0001 0.1% 0.044 99% 0.000 1%8 8-way 0.044 0.0001 0.1% 0.044 100% 0.000 0% 16 1-way 0.049 0.0001 0.1% 0.040 82% 0.009 17%16 2-way 0.041 0.0001 0.2% 0.040 98% 0.001 2%16 4-way 0.041 0.0001 0.2% 0.040 99% 0.000 0%16 8-way 0.041 0.0001 0.2% 0.040 100% 0.000 0%32 1-way 0.042 0.0001 0.2% 0.037 89% 0.005 11%32 2-way 0.038 0.0001 0.2% 0.037 99% 0.000 0%32 4-way 0.037 0.0001 0.2% 0.037 100% 0.000 0%32 8-way 0.037 0.0001 0.2% 0.037 100% 0.000 0%64 1-way 0.037 0.0001 0.2% 0.028 77% 0.008 23%64 2-way 0.031 0.0001 0.2% 0.028 91% 0.003 9%64 4-way 0.030 0.0001 0.2% 0.028 95% 0.001 4%64 8-way 0.029 0.0001 0.2% 0.028 97% 0.001 2% 128 1-way 0.021 0.0001 0.3% 0.019 91% 0.002 8%128 2-way 0.019 0.0001 0.3% 0.019 100% 0.000 0%128 4-way 0.019 0.0001 0.3% 0.019 100% 0.000 0%128 8-way 0.019 0.0001 0.3% 0.019 100% 0.000 0% 256 1-way 0.013 0.0001 0.5% 0.012 94% 0.001 6% 256 2-way 0.012 0.0001 0.5% 0.012 99% 0.000 0%256 4-way 0.012 0.0001 0.5% 0.012 99% 0.000 0%256 8-way 0.012 0.0001 0.5% 0.012 99% 0.000 0%512 1-way 0.008 0.0001 0.8% 0.005 66% 0.003 33%512 2-way 0.007 0.0001 0.9% 0.005 71% 0.002 28%512 4-way 0.006 0.0001 1.1% 0.005 91% 0.000 8%512 8-way 0.006 0.0001 1.1% 0.005 95% 0.000 4% Figure B.8 Total miss rate size cache percentage according three C ’s.Compulsory misses independent cache size, capacity misses decrease capacity increases, conflict missesdecrease associativity increases. Figure B.9 shows information graphically. Note direct-mapped cache size Nhas miss rate two-way set-associative cache size N/2 128 K. Caches larger 128 KiB prove rule. Note Capacity column also fully associative miss rate. Datawere collected Figure B.4 using LRU replacement.B-24 ■Appendix B Review Memory Hierarchyconflict misses. Full associativity expensive hardware, however, may slow processor clock rate (see example page B-29), leading loweroverall performance. little done capacity except enlarge cache. upper-level memory much smaller needed program, sig- nificant percentage time spent moving data two levels the0.000.010.020.030.040.050.060.070.080.090.10epytrepetarssiM epytrepetarssiM1024 4 8 16 32 64 128 256 5121-way 2-way 4-way 8-way Capacity Compulsory Cache size (KB) 0%100% 80% 60% 40% 20% Cache size (KB)4 8 16 32 64 128 256 512 10241-way 2-way 4-way8-wayCapacity Compulsory Figure B.9 Total miss rate (top) distribution miss rate (bottom) size cache according three C ’s data Figure B.8 .The top diagram shows actual data cache miss rates, bottom diagram shows percentage eachcategory. ( Space allows graphs show one extra cache size fit Figure B.8 .)B.3 Six Basic Cache Optimizations ■B-25hierarchy, memory hierarchy said thrash . many replacements required, thrashing means computer runs close speed lower- level memory, maybe even slower miss overhead. Another approach improving three C ’s make blocks larger reduce number compulsory misses, but, see shortly, large blocks canincrease kinds misses. three C ’s give insight cause misses, simple model limits; gives insight average behavior may explain individualmiss. example, changing cache size changes conflict misses well capacity misses, larger cache spreads references blocks. Thus, miss might move capacity miss conflict miss cache size changes. Simi-larly, changing block size sometimes reduce capacity misses (in addition tothe expected reduction compusolory misses), Gupta et al. (2013) show. Note also three C ’s also ignore replacement policy, dif- ficult model because, general, less significant. specific circum-stances replacement policy actually lead anomalous behavior, aspoorer miss rates larger associativity, contradicts three C ’s model. (Some proposed using address trace determine optimal placement memory avoid placement misses three C ’s model; ’ve followed advice here.) Alas, many techniques reduce miss rates also increase hit time miss penalty. desirability reducing miss rates using three optimizationsmust balanced goal making whole system fast. first exam-ple shows importance balanced perspective. First Optimization: Larger Block Size Reduce Miss Rate simplest way reduce miss rate increase block size. Figure B.10 shows trade-off block size versus miss rate set programs cache sizes. Larger block sizes reduce also compulsory misses. reduction occurs principle locality two components: temporal locality spatiallocality. Larger blocks take advantage spatial locality. time, larger blocks increase miss penalty. reduce number blocks cache, larger blocks may increase conflict misses andeven capacity misses cache small. Clearly, little reason increasethe block size size increases miss rate. also benefit reducing miss rate increases average memory access time. increase miss penalty may outweigh decrease miss rate. Example Figure B.11 shows actual miss rates plotted Figure B.10 . Assume mem- ory system takes 80 clock cycles overhead delivers 16 bytes every 2clock cycles. Thus, supply 16 bytes 82 clock cycles, 32 bytes 84 clockcycles, on. block size smallest average memory access timefor cache size Figure B.11 ?B-26 ■Appendix B Review Memory HierarchyAnswer Average memory access time Average memory access time ¼Hit time + Miss rate /C2Miss penalty assume hit time 1 clock cycle independent block size, access time 16-byte block 4 KiB cache Average memory access time ¼1+ 8 :57%/C282 ðÞ ¼ 8:027 clock cycles 256-byte block 256 KiB cache average memory access time Average memory access time ¼1+ 0 :49%/C2112 ðÞ ¼ 1:549 clock cyclesBlock size16 32 64 128 256Miss rate5%10% 0%64K16K4K 256K Figure B.10 Miss rate versus block size five different-sized caches. Note miss rate actually goes block size large relative cache size. line represents cache different size. Figure B.11 shows data used plot lines. Unfortunately, SPEC2000 traces would take long block size included, data based SPEC92 DECstation 5000 ( Gee et al. 1993 ). Cache size Block size 4K 16K 64K 256K 16 8.57% 3.94% 2.04% 1.09% 32 7.24% 2.87% 1.35% 0.70%64 7.00% 2.64% 1.06% 0.51% 128 7.78% 2.77% 1.02% 0.49%256 9.51% 3.29% 1.15% 0.49% Figure B.11 Actual miss rate versus block size five different-sized caches Figure B.10 .Note 4 KiB cache, 256-byte blocks higher miss rate 32- byte blocks. example, cache would 256 KiB order 256-byteblock decrease misses.B.3 Six Basic Cache Optimizations ■B-27Figure B.12 shows average memory access time block cache sizes two extremes. boldfaced entries show fastest block size agiven cache size: 32 bytes 4 KiB 64 bytes larger caches. sizesare, fact, popular block sizes processor caches today. techniques, cache designer trying minimize miss rate miss penalty. selection block size depends thelatency bandwidth lower-level memory. High latency high bandwidth encourage large block size cache gets many bytes per miss small increase miss penalty. Conversely, low latency low bandwidth encour-age smaller block sizes little time saved larger block. Forexample, twice miss penalty small block may close penalty ablock twice size. larger number small blocks may also reduce conflict mis-ses. Note Figures B.10 andB.12 show difference selecting block size based minimizing miss rate versus minimizing average memory access time. seeing positive negative impact larger block size compul- sory capacity misses, next two subsections look potential higher capacity higher associativity. Second Optimization: Larger Caches Reduce Miss Rate obvious way reduce capacity misses Figures B.8 andB.9is increase capacity cache. obvious drawback potentially longer hit time highercost power. technique especially popular off-chip caches. Third Optimization: Higher Associativity Reduce Miss Rate Figures B.8 andB.9show miss rates improve higher associativity. two general rules thumb gleaned figures. first isCache size Block size Miss penalty 4K 16K 64K 256K 16 82 8.027 4.231 2.673 1.894 32 84 7.082 3.411 2.134 1.588 64 88 7.160 3.323 1.933 1.449 128 96 8.469 3.659 1.979 1.470256 112 11.651 4.685 2.288 1.549 Figure B.12 Average memory access time versus block size five different-sized caches Figure B.10 .Block sizes 32 64 bytes dominate. smallest average time per cache size boldfaced.B-28 ■Appendix B Review Memory Hierarchythat eight-way set associative practical purposes effective reducing mis- ses sized caches fully associative. see difference com- paring eight-way entries capacity miss column Figure B.8 , capacity misses calculated using fully associative caches. second observation, called 2:1 cache rule thumb , direct- mapped cache size Nhas miss rate two-way set associative cache size N/2. held three C ’s figures cache sizes less 128 KiB. Like many examples, improving one aspect average memory access time comes expense another. Increasing block size reduces miss rate increasing miss penalty, greater associativity come cost increased hit time. Hence, pressure fast processor clock cycle encour-ages simple cache designs, increasing miss penalty rewards associativity, asthe following example suggests. Example Assume higher associativity would increase clock cycle time listed follows: Clock cycle time 2-way ¼1:36/C2Clock cycle time 1-way Clock cycle time 4-way ¼1:44/C2Clock cycle time 1-way Clock cycle time 8-way ¼1:52/C2Clock cycle time 1-way Assume hit time 1 clock cycle, miss penalty direct- mapped case 25 clock cycles level 2 cache (see next subsection) never misses, miss penalty need rounded integral number clock cycles. Using Figure B.8 miss rates, cache sizes three statements true? Average memory access time 8-way<Average memory access time 4-way Average memory access time 4-way<Average memory access time 2-way Average memory access time 2-way<Average memory access time 1-way Answer Average memory access time associativity Average memory access time 8-way ¼Hit time 8-way + Miss rate 8-way/C2Miss penalty8-way ¼1:52 + Miss rate 8-way/C225 Average memory access time 4-way ¼1:44 + Miss rate 4-way/C225 Average memory access time 2-way ¼1:36 + Miss rate 2-way/C225 Average memory access time 1-way ¼1:00 + Miss rate 1-way/C225 miss penalty time case, leave 25 clock cycles. example, average memory access time 4 KiB direct-mapped cache Average memory access time 1-way ¼1:00 + 0 :098/C225 ðÞ ¼ 3:44 time 512 KiB, eight-way set associative cache Average memory access time 8-way ¼1:52 + 0 :006/C225 ðÞ ¼ 1:66 Using formulas miss rates Figure B.8, Figure B.13 shows average memory access time cache associativity. figure showsB.3 Six Basic Cache Optimizations ■B-29that formulas example hold caches less equal 8 KiB four-way associativity. Starting 16 KiB, greater hit time larger asso- ciativity outweighs time saved due reduction misses. Note account slower clock rate rest program example, thereby understating advantage direct-mapped cache. Fourth Optimization: Multilevel Caches Reduce Miss Penalty Reducing cache misses traditional focus cache research, cache performance formula assures us improvements miss penalty canbe beneficial improvements miss rate. Moreover, Figure 2.2 page 80 shows technology trends improved speed processors faster DRAMs, making relative cost miss penalties increase time. performance gap processors memory leads architect question: make cache faster keep pace speed pro-cessors, make cache larger overcome widening gap pro-cessor main memory? One answer is, both. Adding another level cache original cache memory simplifies decision. first-level cache small enough match clock cycle time fast processor. Yet, second-level cache large enough capture many accesses would go main mem-ory, thereby lessening effective miss penalty. Although concept adding another level hierarchy straightfor- ward, complicates performance analysis. Definitions second level cacheare always straightforward. Let ’s start definition average memory access time two-level cache. Using subscripts L1 L2 refer, respec- tively, first-level second-level cache, original formula isAssociativity Cache size (KiB) 1-way 2-way 4-way 8-way 4 3.44 3.25 3.22 3.28 8 2.69 2.58 2.55 2.62 16 2.23 2.40 2.46 2.53 32 2.06 2.30 2.37 2.45 64 1.92 2.14 2.18 2.25 128 1.52 1.84 1.92 2.00 256 1.32 1.66 1.74 1.82 512 1.20 1.55 1.59 1.66 Figure B.13 Average memory access time using miss rates Figure B.8 param- eters example. Boldface type means time higher number left, is, higher associativity increases average memory access time.B-30 ■Appendix B Review Memory HierarchyAverage memory access time ¼Hit time L1+ Miss rate L1/C2Miss penaltyL1 Miss penaltyL1¼Hit time L2+ Miss rate L2/C2Miss penaltyL2 Average memory access time ¼Hit time L1+ Miss rate L1 /C2Hit time L2+ Miss rate L2/C2Miss penaltyL2 ðÞ formula, second-level miss rate measured leftovers first-level cache. avoid ambiguity, terms adopted two-levelcache system: ■Local miss rate —This rate simply number misses cache divided total number memory accesses cache. would expect, forthe first-level cache equal Miss rate L1, second-level cache Miss rate L2. ■Global miss rate —The number misses cache divided total num- ber memory accesses generated processor. Using terms above, theglobal miss rate first-level cache still Miss rate L1, second-level cache Miss rate L1/C2Miss rate L2. local miss rate large second-level caches first-level cache skims cream memory accesses. global miss rate useful measure: indicates fraction memory accesses thatleave processor go way memory. place misses per instruction metric shines. Instead con- fusion local global miss rates, expand memory stalls per instruc-tion add impact second-level cache. Average memory stalls per instruction ¼Misses per instruction L1/C2Hit time L2 + Misses per instruction L2/C2Miss penaltyL2 Example Suppose 1000 memory references 40 misses first-level cache 20 misses second-level cache. various miss rates? Assume miss penalty L2 cache memory 200 clock cycles, hit time L2 cache 10 clock cycles, hit time L1 1 clock cycle, 1.5memory references per instruction. average memory access time andaverage stall cycles per instruction? Ignore impact writes. Answer miss rate (either local global) first-level cache 40/1000 4%. local miss rate second-level cache 20/40 50%. global miss rate ofthe second-level cache 20/1000 2%. Average memory access time ¼Hit time L1+ Miss rate L1/C2Hit time L2+ Miss rate L2/C2Miss penaltyL2 ðÞ ¼1+4%/C210 + 50 %/C2200 ðÞ ¼ 1+4%/C2110¼5:4 clock cyclesB.3 Six Basic Cache Optimizations ■B-31To see many misses get per instruction, divide 1000 memory refer- ences 1.5 memory references per instruction, yields 667 instructions.Thus, need multiply misses 1.5 get number misses per1000 instructions. 40 /C21.5 60 L1 misses, 20 /C21.5 30 L2 mis- ses, per 1000 instructions. average memory stalls per instruction, assuming themisses distributed uniformly instructions data: Average memory stalls per instruction ¼Misses per instruction L1/C2Hit time L2+ Misses per instruction L2 /C2Miss penaltyL2 ¼60=1000ðÞ /C2 10 + 30 =1000ðÞ /C2 200 ¼0:060/C210 + 0 :030/C2200¼6:6 clock cycles subtract L1 hit time average memory access time (AMAT) multiply average number memory references per instruction, getthe average memory stalls per instruction: 5:4/C01:0 ðÞ /C2 1:5¼4:4/C21:5¼6:6 clock cycles example shows, may less confusion multilevel caches calculating using misses per instruction versus miss rates. Note formulas combined reads writes, assuming write- back first-level cache. Obviously, write-through first-level cache send writes second level, misses, write buffer might used. Figures B.14 andB.15 show miss rates relative execution time change size second-level cache one design. figures gaintwo insights. first global cache miss rate similar singlecache miss rate second-level cache, provided second-level cache ismuch larger first-level cache. Hence, intuition knowledge aboutthe first-level caches apply. second insight local cache miss rate isnota good measure secondary caches; function miss rate first- level cache, hence vary changing first-level cache. Thus, global cache miss rate used evaluating second-level caches. definitions place, consider parameters second-level caches. foremost difference two levels speed first-level cache affects clock rate processor, speed second-level cache affects miss penalty first-level cache. Thus, con-sider many alternatives second-level cache would ill chosen thefirst-level cache. two major questions design second-levelcache: lower average memory access time portion CPI, much cost? initial decision size second-level cache. Since everything first-level cache likely second-level cache, second-level cacheshould much bigger first. second-level caches little bigger,the local miss rate high. observation inspires design hugesecond-level caches —the size main memory older computers!B-32 ■Appendix B Review Memory HierarchyOne question whether set associativity makes sense second-level caches. Example Given following data, impact second-level cache associativity miss penalty? ■Hit time L2for direct mapped ¼10 clock cycles. ■Two-way set associativity increases hit time 0.1 clock cycle 10.1 clock cycles. ■Local miss rate L2for direct mapped ¼25%. ■Local miss rate L2for two-way set associative ¼20%. ■Miss penalty L2¼200 clock cycles. Answer direct-mapped second-level cache, first-level cache miss penalty Miss penalty1-way L2 ¼10 + 25 %/C2200¼60:0 clock cycles100% 0%10%20%30%40%50%60%70%80%90%Miss rate99% 99% 98% 4 8 16 32 64 128 256 512 1024 2048 4096 Cache size (KB)96% 55% 6% 5%4% 4% 4% 3% 3% 3%2% 2% 2% 1% 1% 4% 4%46% 39% 34%51%88% 67%Local miss rate Global miss rate Single cache miss rate Figure B.14 Miss rates versus cache size multilevel caches. Second-level caches smaller sum two 64 KiB first-level caches make little sense, reflected high miss rates. 256 KiB single cache within 10% global miss rates. miss rate single-level cache versus size plotted local miss rate global miss rate second-level cache using 32 KiB first-level cache. L2caches (unified) two-way set associative replacement. split L1instruction data caches 64 KiB two-way set associative LRU replace- ment. block size L1 L2 caches 64 bytes. Data collected Figure B.4 .B.3 Six Basic Cache Optimizations ■B-33Adding cost associativity increases hit cost 0.1 clock cycle, making new first-level cache miss penalty: Miss penalty2-way L2 ¼10:1+2 0 %/C2200¼50:1 clock cycles reality, second-level caches almost always synchronized first-level cache processor. Accordingly, second-level hit time must integralnumber clock cycles. lucky, shave second-level hit time to10 cycles; not, round 11 cycles. Either choice improvement overthe direct-mapped second-level cache: Miss penalty2-way L2 ¼10 + 20 %/C2200¼50:0 clock cycles Miss penalty2-way L2 ¼11 + 20 %/C2200¼51:0 clock cycles reduce miss penalty reducing miss rate second-level caches. Another consideration concerns whether data first-level cache second-level cache. Multilevel inclusion natural policy memory hierar- chies: L1 data always present L2. Inclusion desirable consistencybetween I/O caches (or among caches multiprocessor) determined checking second-level cache.8192Second-level cache size (KB)4096 2048 1024 512 256 1.00 1.25 1.50 1.75 2.00 2.25 2.50 Relative execution time1.60 1.651.10 1.141.02 1.06 2.34 2.391.94 1.991.76 1.82L2 hit = 8 clock cycles L2 hit = 16 clock cycles Figure B.15 Relative execution time second-level cache size. two bars different clock cycles L2 cache hit. reference execution time 1.00 an8192 KiB second-level cache 1-clock-cycle latency second-level hit. data collected way Figure B.14 , using simulator imitate Alpha 21264.B-34 ■Appendix B Review Memory HierarchyOne drawback inclusion measurements suggest smaller blocks smaller first-level cache larger blocks larger second-level cache. example, Pentium 4 64-byte blocks L1 caches 128-byte blocks inits L2 cache. Inclusion still maintained work second-levelmiss. second-level cache must invalidate first-level blocks map ontothe second-level block replaced, causing slightly higher first-level miss rate.To avoid problems, many cache designers keep block size alllevels caches. However, designer afford L2 cache slightly big- ger L1 cache? significant portion space used redun- dant copy L1 cache? cases sensible opposite policy multilevel exclusion: L1 data never found L2 cache. Typically, exclusion cache miss L1 results swap blocks L1 L2 instead areplacement L1 block L2 block. policy prevents wasting spacein L2 cache. example, AMD Opteron chip obeys exclusion propertyusing two 64 KiB L1 caches 1 MiB L2 cache. issues illustrate, although novice might design first- second- level caches independently, designer first-level cache simpler job given compatible second-level cache. less gamble use write through,for example, write-back cache next level act backstop forrepeated writes uses multilevel inclusion. essence cache designs balancing fast hits misses. second-level caches, far fewer hits first-level cache, theemphasis shifts fewer misses. insight leads much larger caches tech-niques lower miss rate, higher associativity larger blocks. Fifth Optimization: Giving Priority Read Misses Writes Reduce Miss Penalty optimization serves reads writes completed. start looking complexities write buffer. write-through cache important improvement write buffer proper size. Write buffers, however, complicate memory accesses might hold updated value location needed read miss. Example Look code sequence: sd x3, 512(x0);M[512] ¬R3 (cache index 0) ld x1, 1024(x0);x1 ¬M[1024](cache index 0) ld x2, 512(x0);x2 ¬M[512] (cache index 0) Assume direct-mapped, write-through cache maps 512 1024 block, four-word write buffer checked read miss. thevalue x2 always equal value x3?B.3 Six Basic Cache Optimizations ■B-35Answer Using terminology Chapter 2 , read-after-write data hazard memory. Let ’s follow cache access see danger. data x3 placed write buffer store. following load uses cache indexand therefore miss. second load instruction tries put value location512 register x2; also results miss. write buffer ’t completed writing location 512 memory, read location 512 put old, wrong value cache block, x2. Without proper precautions, x3x1 would equal x2! simplest way dilemma read miss wait write buffer empty. alternative check contents write buffer readmiss, conflicts memory system available, let readmiss continue. Virtually desktop server processors use latter approach,giving reads priority writes. cost writes processor write-back cache also reduced. Suppose read miss replace dirty memory block. Instead writing dirtyblock memory, reading memory, could copy dirty block buffer, read memory, write memory. way processor read, processor probably waiting, finish sooner. Similar pre-vious situation, read miss occurs, processor either stall buffer isempty check addresses words buffer conflicts. five optimizations reduce cache miss penalties miss rates, time look reducing final component average memory accesstime. Hit time critical affect clock rate processor; inmany processors today cache access time limits clock cycle rate, even processors take multiple clock cycles access cache. Hence, fast hit time multiplied importance beyond average memory access time for-mula helps everything. Sixth Optimization: Avoiding Address Translation Indexing Cache Reduce Hit Time Even small simple cache must cope translation virtual address processor physical address access memory. described inSection B.4 , processors treat main memory another level memory hierarchy, thus address virtual memory exists disk mustbe mapped onto main memory. guideline making common case fast suggests use virtual addresses cache, hits much common misses. caches termed virtual caches , physical cache used identify tradi- tional cache uses physical addresses. shortly see, importantto distinguish two tasks: indexing cache comparing addresses. Thus, theissues whether virtual physical address used index cache andB-36 ■Appendix B Review Memory Hierarchywhether virtual physical address used tag comparison. Full virtual addressing indices tags eliminates address translation time cache hit. ’t everyone build virtually addressed caches? One reason protection. Page-level protection checked part virtual physical address translation, must enforced matter what. One solu-tion copy protection information TLB miss, add field tohold it, check every access virtually addressed cache. Another reason every time process switched, virtual addresses refer different physical addresses, requiring cache flushed. Figure B.16 shows impact miss rates flushing. One solution increase width cache address tag process-identifier tag (PID). operating system assigns tags processes, need flush cache PID Miss rate20% 18%16%14% 12%10% 8%6% 4% 2% 0%0.6% 0.4% 18.8%1.1% 0.5% 13.0%1.8% 0.6% 8.7%2.7% 0.6% 3.9%3.4% 0.4% 2.7%3.9% 0.4% 0.9%4.1% 0.3% 0.4%4.3% 0.3% 0.3%4.3% 0.3% 0.3%4.3% 0.3% 0.3% Cache size2K 4K 8K 16K 32K 64K 128K 256K 512K 1024KPurge PIDs Uniprocess Figure B.16 Miss rate versus virtually addressed cache size program measured three ways: without process switches (uniprocess), process switches using process-identifier tag (PID), process switches without PIDs (purge). PIDs increase uniprocess absolute miss rate 0.3% –0.6% save 0.6% –4.3% purging. Agarwal (1987) collected statistics Ultrix operating system run- ning VAX, assuming direct-mapped caches block size 16 bytes. Note miss rate goes 128 256 K. nonintuitive behavior occur cachesbecause changing size changes mapping memory blocks onto cache blocks, change conflict miss rate.B.3 Six Basic Cache Optimizations ■B-37recycled; is, PID distinguishes whether data cache program. Figure B.16 shows improvement miss rates using PIDs avoid cache flushes. third reason virtual caches popular operating sys- tems user programs may use two different virtual addresses phys-ical address. duplicate addresses, called synonyms oraliases , could result two copies data virtual cache; one modified, havethe wrong value. physical cache ’t happen, accesses would first translated physical cache block. Hardware solutions synonym problem, called antialiasing , guarantee every cache block unique physical address. example, AMD Opteron usesa 64 KiB instruction cache 4 KiB page two-way set associativity; hence,the hardware must handle aliases involved three virtual address bits inthe set index. avoids aliases simply checking eight possible locationson miss —two blocks four sets —to sure none matches phys- ical address data fetched. one found, invalidated, whenthe new data loaded cache physical address guaranteed unique. Software make problem much easier forcing aliases share address bits. older version UNIX Sun Microsystems, example,required aliases identical last 18 bits addresses; restric-tion called page coloring . Note page coloring simply set associative map- ping applied virtual memory: 4 KiB (2 12) pages mapped using 64 (26) sets ensure physical virtual addresses match last 18 bits. Thisrestriction means direct-mapped cache 2 18(256 K) bytes smaller never duplicate physical addresses blocks. perspective cache, page coloring effectively increases page offset, software guaranteesthat last bits virtual physical page address identical. final area concern virtual addresses I/O. I/O typically uses phys- ical addresses thus would require mapping virtual addresses interact witha virtual cache. (The impact I/O caches discussed Appendix D.) One alternative get best virtual physical caches use part page offset —the part identical virtual physical addresses —to index cache. time cache read using index, virtual part address translated, tag match uses phys-ical addresses. alternative allows cache read begin immediately, yet tag comparison still physical addresses. limitation virtually indexed, physically tagged alternative direct-mapped cache bigger page size. example, data cache Figure B.5 page B-13, index 9 bits cache block offset 6 bits. use trick, virtual page size would least 2 (9+6)bytes 32 KiB. not, portion index must translated virtual physical address. Figure B.17 shows organization caches, translation lookaside buffers (TLBs), virtual memory thistechnique used.B-38 ■Appendix B Review Memory HierarchyAssociativity keep index physical part address yet still support large cache. Recall size index controlled formula: 2Index¼Cache size Block size /C2Set associativity example, doubling associativity doubling cache size change size index. IBM 3033 cache, extreme example, 16-way set associative, even though studies show little benefit miss rates aboveL1 tag compare address <26> L2 cache tag <21> L2 data <512>=? =? =?TLB tag compare address <43>TLB index <7>Virtual address <64> Physical address <40>Virtual page number <50> L2 tag compare address <21>L2 cache index <14>Block offset <6>Page offset <14> L1 cache tag <26> L1 data <512> CPUTo CPU CPU L1 cache CPUL1 cache index <8>Block offset <6> TLB tag <43> TLB data <26> Figure B.17 overall picture hypothetical memory hierarchy going virtual address L2 cache access. page size 16 KiB. TLB two-way set associative 256 entries. L1 cache direct-mapped 16 KiB, L2 cache four-way set associative total 4 MiB. use 64-byte blocks. virtual addressis 64 bits physical address 40 bits.B.3 Six Basic Cache Optimizations ■B-398-way set associativity. high associativity allows 64 KiB cache addressed physical index, despite handicap 4 KiB pages theIBM architecture. Summary Basic Cache Optimization techniques section improve miss rate, miss penalty, hit time gen-erally impact components average memory access equation wellas complexity memory hierarchy. Figure B.18 summarizes tech- niques estimates impact complexity, + meaning techniqueimproves factor, –meaning hurts factor, blank meaning impact. optimization figure helps one category. B.4 Virtual Memory …a system devised make core drum combination appear programmer single level store, requisite transfers taking placeautomatically. Kilburn et al. (1962) instant time computers running multiple processes, ownaddress space. (Processes described next section.) would expen-sive dedicate full address space worth memory process, especiallybecause many processes use small part address space. Hence, theremust means sharing smaller amount physical memory among many processes.TechniqueHit timeMiss penaltyMiss rateHardware complexity Comment Larger block size – + 0 Trivial; Pentium 4L2 uses 128 bytes Larger cache size – + 1 Widely used, especially L2 caches Higher associativity – + 1 Widely used Multilevel caches + 2 Costly hardware; harder L1 block size6¼L2 block size; widely used Read priority writes + 1 Widely usedAvoiding address translation cache indexing+ 1 Widely used Figure B.18 Summary basic cache optimizations showing impact cache performance complexity techniques appendix. Generally technique helps one factor. + means technique improves factor, –means hurts factor, blank means impact. complexity measure subjective, 0 easiest 3 challenge.B-40 ■Appendix B Review Memory HierarchyOne way this, virtual memory , divides physical memory blocks allocates different processes. Inherent approach must pro- tection scheme restricts process blocks belonging process. forms virtual memory also reduce time start program, notall code data need physical memory program begin. Although protection provided virtual memory essential current com- puters, sharing reason virtual memory invented. programbecame large physical memory, programmer ’s job make fit. Programmers divided programs pieces, identified pieces mutually exclusive, loaded unloaded overlays user program control execution. programmer ensured program never triedto access physical main memory computer, theproper overlay loaded proper time. well imagine, respon-sibility eroded programmer productivity. Virtual memory invented relieve programmers burden; auto- matically manages two levels memory hierarchy represented mainmemory secondary storage. Figure B.19 shows mapping virtual memory physical memory program four pages. addition sharing protected memory space automatically managing memory hierarchy, virtual memory also simplifies loading program execu-tion. Called relocation , mechanism allows program run location physical memory. program Figure B.19 placed anywhere 0 4K8K 12K16K20K24K 28KPhysical address Physical main memory Disk D0 4K 8K 12KVirtual address Virtual memoryA B C DC B Figure B.19 logical program contiguous virtual address space shown left. consists four pages, A, B, C, D. actual location three blocks physical main memory located disk.B.4 Virtual Memory ■B-41in physical memory disk changing mapping them. (Prior popularity virtual memory, processors would include relocation register purpose.) alternative hardware solution would software thatchanged addresses program time run. Several general memory hierarchy ideas Chapter 1 caches anal- ogous virtual memory, although many terms different. Page orseg- ment used block, page fault oraddress fault used miss. virtual memory, processor produces virtual addresses translated combi- nation hardware software physical addresses , access main mem- ory. process called memory mapping oraddress translation . Today, two memory hierarchy levels controlled virtual memory DRAMs magneticdisks. Figure B.20 shows typical range memory hierarchy parameters vir- tual memory. differences caches virtual memory beyond quantitative ones mentioned Figure B.20 : ■Replacement cache misses primarily controlled hardware, vir- tual memory replacement primarily controlled operating system. longer miss penalty means ’s important make good decision, operating system involved take time deciding replace. ■The size processor address determines size virtual memory, thecache size independent processor address size. ■In addition acting lower-level backing store main memory inthe hierarchy, secondary storage also used file system. fact, thefile system occupies secondary storage. usually theaddress space. Parameter First-level cache Virtual memory Block (page) size 16 –128 bytes 4096 –65,536 bytes Hit time 1 –3 clock cycles 100 –200 clock cycles Miss penalty 8 –200 clock cycles 1,000,000 –10,000,000 clock cycles (access time) (6 –160 clock cycles) (800,000 –8,000,000 clock cycles) (transfer time) (2 –40 clock cycles) (200,000 –2,000,000 clock cycles) Miss rate 0.1% –10% 0.00001% –0.001% Address mapping 25 –45-bit physical address 14–20-bit cache address32–64-bit virtual address 25 –45-bit physical address Figure B.20 Typical ranges parameters caches virtual memory. Virtual memory parameters represent increases 10 –1,000,000 times cache parame- ters. Usually, first-level caches contain 1 MiB data, whereas physical memory contains 256 MiB 1 TB.B-42 ■Appendix B Review Memory HierarchyVirtual memory also encompasses several related techniques. Virtual memory systems categorized two classes: fixed-size blocks, called pages , variable-size blocks, called segments . Pages typically fixed 4096 –8192 bytes, segment size varies. largest segment sup- ported processor ranges 216bytes 232bytes; smallest segment 1 byte. Figure B.21 shows two approaches might divide code data. decision use paged virtual memory versus segmented virtual memory affects processor. Paged addressing single fixed-size address dividedinto page number offset within page, analogous cache addressing. single address work segmented addresses; variable size segments requires 1 word segment number 1 word offset within segment,for total 2 words. unsegmented address space simpler compiler. pros cons two approaches well documented oper- ating systems textbooks; Figure B.22 summarizes arguments. ataD edoC Paging Segmentation Figure B.21 Example paging segmentation divide program. Page Segment Words per address One Two (segment offset) Programmer visible? Invisible application programmerMay visible application programmer Replacing block Trivial (all blocks size)Difficult (must find contiguous, variable-size, unused portion ofmain memory) Memory use inefficiencyInternal fragmentation (unused portion page)External fragmentation (unused pieces main memory) Efficient disk traffic Yes (adjust page size balance access time andtransfer time)Not always (small segments may transfer bytes) Figure B.22 Paging versus segmentation. waste memory, depending block size well segments fit together main memory. Programming lan- guages unrestricted pointers require segment address passed. hybrid approach, called paged segments , shoots best worlds: segments composed pages, replacing block easy, yet segment may betreated logical unit.B.4 Virtual Memory ■B-43replacement problem (the third line figure), computers today use pure segmentation. computers use hybrid approach, called paged segments ,i n segment integral number pages. simplifies replacementbecause memory need contiguous, full segments need mainmemory. recent hybrid computer offer multiple page sizes,with larger sizes powers 2 times smallest page size. IBM405CR embedded processor, example, allows 1 KiB, 4 KiB (2 2/C21 KiB), 16 KiB (24/C21 KiB), 64 KiB (26/C21 KiB), 256 KiB (28/C21 KiB), 1024 KiB (210/C21 KiB), 4096 KiB (212/C21 KiB) act single page. Four Memory Hierarchy Questions Revisited ready answer four memory hierarchy questions virtual memory. Q1: Block Placed Main Memory? miss penalty virtual memory involves access rotating magnetic storagedevice therefore quite high. Given choice lower miss rates simplerplacement algorithm, operating systems designers usually pick lower miss ratesbecause exorbitant miss penalty. Thus, operating systems allow blocks tobe placed anywhere main memory. According terminology inFigure B.2 page B-8, strategy would labeled fully associative. Q2: Block Found Main Memory? paging segmentation rely data structure indexed page segment number. data structure contains physical address block. segmentation, offset added segment ’s physical address obtain final physical address. paging, offset simply concatenated physicalpage address (see Figure B.23 ). data structure, containing physical page addresses, usually takes form page table. Indexed virtual page number, size table number pages virtual address space. Given 32-bit virtual address,4 KiB pages, 4 bytes per page table entry (PTE), size page table would (2 32/212)/C222¼222or 4 MiB. reduce size data structure, computers apply hashing func- tion virtual address. hash allows data structure length thenumber physical pages main memory. number could much smaller number virtual pages. structure called inverted page table . Using previous example, 512 MiB physical memory would need 1 MiB(8/C2512 MiB/4 KiB) inverted page table; extra 4 bytes per page table entry virtual address. HP/Intel IA-64 covers bases offeringB-44 ■Appendix B Review Memory Hierarchyboth traditional pages tables andinverted page tables, leaving choice mech- anism operating system programmer. reduce address translation time, computers use cache dedicated address translations, called translation lookaside buffer , simply translation buffer , described detail shortly. Q3: Block Replaced Virtual Memory Miss? mentioned earlier, overriding operating system guideline minimizing page faults. Consistent guideline, almost operating systems try toreplace least recently used (LRU) block past predicts future, one less likely needed. help operating system estimate LRU, many processors provide use bit orreference bit , logically set whenever page accessed. (To reduce work, actually set translation buffer miss, described shortly.)The operating system periodically clears use bits later records itcan determine pages touched particular time period. keep-ing track way, operating system select page among leastrecently referenced. Q4: Happens Write? level main memory contains rotating magnetic disks take millionsof clock cycles access. great discrepancy access time, onehas yet built virtual memory operating system writes main memoryto disk every store processor. (This remark interpreted anopportunity become famous first build one!) Thus, write strat- egy always write-back.Main memory Page tableVirtual address Virtual page number Page offset Physical address Figure B.23 mapping virtual address physical address via page table.B.4 Virtual Memory ■B-45Because cost unnecessary access next-lower level high, virtual memory systems usually include dirty bit. allows blocks written disk altered since read disk. Techniques Fast Address Translation Page tables usually large stored main memory some-times paged themselves. Paging means every memory access logically takes atleast twice long, one memory access obtain physical address asecond access get data. mentioned Chapter 2 , use locality avoid extra memory access. keeping address translations special cache, amemory access rarely requires second access translate data. specialaddress translation cache referred translation look aside buffer (TLB), also called translation buffer (TB). TLB entry like cache entry tag holds portions virtual address data portion holds physical page frame number, protection field,valid bit, usually use bit dirty bit. change physical page framenumber protection entry page table, operating system must makesure old entry TLB; otherwise, system ’t behave properly. Note dirty bit means corresponding page dirty, address translation TLB dirty particular block data cache dirty. operating system resets bits changing value page table invalidates corresponding TLB entry. entry reloaded thepage table, TLB gets accurate copy bits. Figure B.24 shows Opteron data TLB organization, step translation labeled. TLB uses fully associative placement; thus, translationbegins (steps 1 2) sending virtual address tags. course, tagmust marked valid allow match. time, type memoryaccess checked violation (also step 2) protection information TLB. reasons similar cache case, need include 12 bits page offset TLB. matching tag sends corresponding phys-ical address effectively 40:1 multiplexor (step 3). page offset thencombined physical page frame form full physical address (step 4). Theaddress size 40 bits. Address translation easily critical path determining clock cycle processor, Opteron uses virtually addressed, physically tagged L1 caches. Selecting Page Size obvious architectural parameter page size. Choosing page question balancing forces favor larger page size versus favoring asmaller size. following favor larger size:B-46 ■Appendix B Review Memory Hierarchy■The size page table inversely proportional page size; memory (or resources used memory map) therefore saved making pages bigger. ■As mentioned Section B.3 , larger page size allow larger caches fast cache hit times. ■Transferring larger pages secondary storage, possibly net-work, efficient transferring smaller pages. ■The number TLB entries restricted, larger page size means morememory mapped efficiently, thereby reducing number TLB misses. final reason recent microprocessors decided support mul- tiple page sizes; programs, TLB misses significant CPI thecache misses. main motivation smaller page size conserving storage. small page size result less wasted storage contiguous region virtual memory equal size multiple page size. term unused memory page internal fragmentation . Assuming process three primary segments (text, heap, stack), average wasted storage per processwill 1.5 times page size. amount negligible computers hun-dreds megabytes memory page sizes 4 –8 KiB. course, page sizes become large (more 32 KiB), storage (both main second-ary) could wasted, well I/O bandwidth. final concern process start-uptime; many processes small, large page size would lengthen time invoke process.<28>Virtual page number <36>Page offset <12> <1> V<1> D<1> A<36> Tag<28> Physical address (Low-order 12 bits address) (High-order 28 bits address)40-bit physical addressR/W U/S 40:1 mux2 1 4<12> 3 Figure B.24 Operation Opteron data TLB address translation. four steps TLB hit shown circled numbers . TLB 40 entries. Section B.5 describes various protection access fields Opteron page table entry.B.4 Virtual Memory ■B-47Summary Virtual Memory Caches virtual memory, TLBs, first-level caches, second-level caches map- ping portions virtual physical address space, get confusing whatbits go where. Figure B.25 gives hypothetical example going 64-bit virtual address 41-bit physical address two levels cache. L1 cache vir-tually indexed, physically tagged cache size page sizeare 8 KiB. L2 cache 4 MiB. block size 64 bytes. L1 tag compare address <28> L2 cache tag <19> L2 data <512>=? =? =?TLB tag compare address <43>TLB index <8>Virtual address <64> Physical address <41>Virtual page number <51> L2 tag compare address <19>L2 cache index <16>Block offset <6>Page offset <13> L1 cache tag <43> L1 data <512>TLB tag <43> TLB data <28> CPUTo CPU CPU L1 cache CPUL1 cache index <7>Block offset <6> Figure B.25 overall picture hypothetical memory hierarchy going virtual address L2 cache access. page size 8 KiB. TLB direct mapped 256 entries. L1 cache direct-mapped 8 KiB, L2 cache direct-mapped 4 MiB. use 64-byte blocks. virtual address 64 bits physical address 41 bits. primary difference simple figure real cache replication pieces figure.B-48 ■Appendix B Review Memory HierarchyFirst, 64-bit virtual address logically divided virtual page number page offset. former sent TLB translated physical address, high bit latter sent L1 cache act index.If TLB match hit, physical page number sent L1 cachetag check match. matches, ’s L1 cache hit. block offset selects word processor. L1 cache check results miss, physical address used try L2 cache. middle portion physical address used index 4MiB L2 cache. resulting L2 cache tag compared upper part physical address check match. matches, L2 cache hit, data sent processor, uses block offset select desiredword. L2 miss, physical address used get block frommemory. Although simple example, major difference drawing real cache replication. First, one L1 cache. aretwo L1 caches, top half diagram duplicated. Note wouldlead two TLBs, typical. Hence, one cache TLB instructions, driven PC, one cache TLB data, driven effective address. second simplification caches TLBs direct mapped. n-way set associative, would replicate set tag memory, comparators, data memory ntimes connect data memories n:1 multiplexor select hit. course, total cache size remained same,the cache index would also shrink log 2 nbits according formula Figure B.7 page B-22. B.5 Protection Examples Virtual Memory invention multiprogramming, computer would shared several programs running concurrently, led new demands protection andsharing among programs. demands closely tied virtual memory incomputers today, cover topic along two examples virtual memory. Multiprogramming leads concept process . Metaphorically, process program ’s breathing air living space —that is, running program plus state needed continue running it. Time-sharing variation multiprogram-ming shares processor memory several interactive users thesame time, giving illusion users computers. Thus, atany instant must possible switch one process another. Thisexchange called process switch orcontext switch . process must operate correctly whether executes continuously start finish, interrupted repeatedly switched processes. Theresponsibility maintaining correct process behavior shared designers ofthe computer operating system. computer designer must ensure thatB.5 Protection Examples Virtual Memory ■B-49the processor portion process state saved restored. operating system designer must guarantee processes interfere others ’ computations. safest way protect state one process another would copy current information disk. However, process switch would takeseconds —far long time-sharing environment. problem solved operating systems partitioning main memory several different processes state memory time. divi-sion means operating system designer needs help computer designer provide protection one process cannot modify another. Besides protection, computers also provide sharing code data pro-cesses, allow communication processes save memory reducingthe number copies identical information. Protecting Processes Processes protected one another page tables, eachpointing distinct pages memory. Obviously, user programs must preventedfrom modifying page tables protection would circumvented. Protection escalated, depending apprehension computer designer purchaser. Rings added processor protection structure expand memory access protection two levels (user kernel) many more. Like amilitary classification system top secret, secret, confidential, unclassified,concentric rings security levels allow trusted access anything, thesecond trusted access everything except innermost level, on.The“civilian ”programs least trusted and, hence, limited range accesses. may also restrictions pieces memory cancontain code —execute protection —and even entrance point levels. Intel 80x86 protection structure, uses rings, described later section. clear whether rings improvement practice overthe simple system user kernel modes. designer ’s apprehension escalates trepidation, simple rings may suffice. Restricting freedom given program inner sanctum requires anew classification system. Instead military model, analogy system isto keys locks: program ’t unlock access data unless key. keys, capabilities , useful, hardware operating system must able explicitly pass one program another without allowing program forge them. checking requires great deal hardwaresupport time checking keys kept low. 80x86 architecture tried several alternatives years. backward compatibility one guidelines architecture, themost recent versions architecture include experiments virtualmemory. ’ll go two options here: first older segmented address space newer flat, 64-bit address space.B-50 ■Appendix B Review Memory HierarchyA Segmented Virtual Memory Example: Protection Intel Pentium second system dangerous system man ever designs …. general tendency over-design second system, using ideas andfrills cautiously sidetracked first one. F. P. Brooks, Jr. Mythical Man-Month (1975) original 8086 used segments addressing, yet provided nothing virtual memory protection. Segments base registers bound registers andno access checks, segment register could loaded correspondingsegment physical memory. Intel ’s dedication virtual memory protection evident successors 8086, fields extended support larger addresses. protection scheme elaborate, many details carefully designed try avoid security loopholes. ’ll refer IA-32. next pages highlight Intel safeguards; find readingdifficult, imagine difficulty implementing them! first enhancement double traditional two-level protection model: IA-32 four levels protection. innermost level (0) corresponds thetraditional kernel mode, outermost level (3) least privileged mode.The IA-32 separate stacks level avoid security breaches levels. also data structures analogous traditional page tables con- tain physical addresses segments, well list checks made ontranslated addresses. Intel designers stop there. IA-32 divides address space, allowing operating system user access full space. TheIA-32 user call operating system routine space even pass param-eters retaining full protection. safe call trivial action,because stack operating system different user ’s stack. More- over, IA-32 allows operating system maintain protection level called routine parameters passed it. potential loophole protection prevented allowing user process ask operating systemto access something indirectly would able access itself.(Such security loopholes called Trojan horses .) Intel designers guided principle trusting operating system little possible, supporting sharing protection. exam-ple use protected sharing, suppose payroll program writes checks also updates year-to-date information total salary benefits pay- ments. Thus, want give program ability read salary andyear-to-date information modify year-to-date information sal-ary. see mechanism support features shortly. rest ofthis subsection, look big picture IA-32 protection exam-ine motivation.B.5 Protection Examples Virtual Memory ■B-51Adding Bounds Checking Memory Mapping first step enhancing Intel processor getting segmented addres- sing check bounds well supply base. Rather base address, seg- ment registers IA-32 contain index virtual memory data structure called descriptor table . Descriptor tables play role traditional page tables. IA-32 equivalent page table entry segment descriptor . con- tains fields found PTEs: ■Present bit —Equivalent PTE valid bit, used indicate valid translation ■Base field —Equivalent page frame address, containing physical address first byte segment ■Access bit —Like reference bit use bit architectures helpful replacement algorithms ■Attributes field —Specifies valid operations protection levels oper- ations use segment also limit field, found paged systems, establishes upper bound valid offsets segment. Figure B.26 shows examples IA-32 seg- ment descriptors. IA-32 provides optional paging system addition segmented addres- sing. upper portion 32-bit address selects segment descriptor, themiddle portion index page table selected descriptor. fol-lowing section describes protection system rely paging. Adding Sharing Protection provide protected sharing, half address space shared processesand half unique process, called global address space andlocal address space , respectively. half given descriptor table appropriate name. descriptor pointing shared segment placed global descriptor table,while descriptor private segment placed local descriptor table. program loads IA-32 segment register index table andab saying table desires. operation checked according attributes descriptor, physical address formed adding offset pro-cessor base descriptor, provided offset less limit field.Every segment descriptor separate 2-bit field give legal access level ofthis segment. violation occurs program tries use segment alower protection level segment descriptor. show invoke payroll program mentioned herein update year-to-date information without allowing update salaries. pro- gram could given descriptor information writable field clear, meaning read write data. trusted program suppliedthat write year-to-date information. given descriptor theB-52 ■Appendix B Review Memory Hierarchywritable field set ( Figure B.26 ). payroll program invokes trusted code using code segment descriptor conforming field set. setting meansthe called program takes privilege level code called rather thanthe privilege level caller. Hence, payroll program read salariesand call trusted program update year-to-date totals, yet payroll programcannot modify salaries. Trojan horse exists system, effective itmust located trusted code whose job update year-to-dateinformation. argument style protection limiting scope vulnerability enhances security.Attributes Base Limitstib 42 stib 23 stib 4 stib 8 PresentCode segment DPL 11 Conforming Readable Accessed PresentData segment DPL 10 Expand Writable Accessed Attributes Destination selector Destination offsetstib 61 stib 61 stib 8 Word count8 bits PresentCall gate DPL 0 00100GD Figure B.26 IA-32 segment descriptors distinguished bits attributes field. Base, limit, present, readable , writable self-explanatory. gives default addressing size instructions: 16 bits 32 bits. G gives granularity ofthe segment limit: 0 means bytes 1 means 4 KiB pages. G set 1 paging turned set size page tables. DPL means descriptor privilege level —this checked code privilege level see access allowed. Conforming says code takes privilege level code called rather priv- ilege level caller; used library routines. expand-down field flips check let base field high-water mark limit field low-water mark. Asyou might expect, used stack segments grow down. Word count controls number words copied current stack new stack call gate. two fields call gate descriptor, destination selector destination offset, select descriptor destination call offset it, respectively. many three segment descriptors IA-32 protection model.B.5 Protection Examples Virtual Memory ■B-53Adding Safe Calls User OS Gates Inheriting Protection Level Parameters Allowing user jump operating system bold step. How, then, hardware designer increase chances safe system without trusting theoperating system piece code? IA-32 approach restrictwhere user enter piece code, safely place parameters properstack, make sure user parameters ’t get protection level called code. restrict entry others ’code, IA-32 provides special segment descriptor, call gate , identified bit attributes field. Unlike descriptors, call gates full physical addresses object memory; offsetsupplied processor ignored. stated previously, purpose pre-vent user randomly jumping anywhere protected privilegedcode segment. programming example, means place payrollprogram invoke trusted code proper boundary. restriction isneeded make conforming segments work intended. happens caller callee “mutually suspicious, ”so neither trusts other? solution found word count field bottom descriptor inFigure B.26 . call instruction invokes call gate descriptor, descriptor cop- ies number words specified descriptor local stack onto stackcorresponding level segment. copying allows user passparameters first pushing onto local stack. hardware safely trans-fers onto correct stack. return call gate pop parameters offboth stacks copy return values proper stack. Note model incompatible current practice passing parameters registers. scheme still leaves open potential loophole operating system use user ’s address, passed parameters, operating system ’s security level, instead user ’s level. IA-32 solves problem dedicating 2 bits every processor segment register requested protection level. operating system routine invoked, execute instruction sets 2-bit field address parameters protection level theuser called routine. Thus, address parameters loaded segment registers, set requested protection level proper value. IA-32 hardware uses requested protection level preventany foolishness: segment accessed system routine using thoseparameters privileged protection level requested. Paged Virtual Memory Example: 64-Bit Opteron Memory Management AMD engineers found uses elaborate protection model described previous section. popular model flat, 32-bit address space, introduced bythe 80386, sets base values segment registers zero. Hence,B-54 ■Appendix B Review Memory HierarchyAMD dispensed multiple segments 64-bit mode. assumes segment base zero ignores limit field. page sizes 4 KiB, 2 MiB, 4 MiB. 64-bit virtual address AMD64 architecture mapped onto 52-bit physical addresses, although implementations implement fewer bits sim-plify hardware. Opteron, example, uses 48-bit virtual addresses 40-bit physical addresses. AMD64 requires upper 16 bits virtual addressbe sign extension lower 48 bits, calls canonical form . size page tables 64-bit address space alarming. Hence, AMD64 uses multilevel hierarchical page table map address space keep size reasonable. number levels depends size virtual address space.Figure B.27 shows four-level translation 48-bit virtual addresses Opteron. offsets page tables come four 9-bit fields. Address translation starts adding first offset page-map level 4 base registerand reading memory location get base next-level pagetable. next address offset turn added newly fetched address, 63 48 47 39 38 30 29 21 20 12 11 0 000 . . . 0 111 . . . 1Page-map L4 Page-dir-ptr Page-directory Page-table Page offset Page-map L4 base addr (CR3) Physical page frame number Page offsetPage-mp entryPage-map L4 table+ + Page-dir-ptr entryPage-directory pointer table + Page-dir entryPage-directory table + Page-table entryPage table Physical address Main memory Figure B.27 mapping Opteron virtual address. Opteron virtual memory implementation four page table levels supports effective physical address size 40 bits. page table 512 entries, levelfield 9 bits wide. AMD64 architecture document allows virtual address size grow current 48 bitsto 64 bits, physical address size grow current 40 bits 52 bits.B.5 Protection Examples Virtual Memory ■B-55memory accessed determine base third page table. happens fashion. last address field added final base address, memory read using sum (finally) get physical address pagebeing referenced. address concatenated 12-bit page offset get thefull physical address. Note page table Opteron architecture fits withina single 4 KiB page. Opteron uses 64-bit entry page tables. first 12 bits reserved future use, next 52 bits contain physical page frame number,and last 12 bits give protection use information. Although fields vary page table levels, basic ones: ■Presence —Says page present memory. ■Read/write —Says whether page read-only read-write. ■User/supervisor —Says whether user access page limited upper three privilege levels. ■Dirty —Says page modified. ■Accessed —Says page read written since bit last cleared. ■Page size —Says whether last level 4 KiB pages 4 MiB pages; ’s latter, Opteron uses three instead four levels pages. ■No execute —Not found 80386 protection scheme, bit added prevent code executing pages. ■Page level cache disable —Says whether page cached not. ■Page level write —Says whether page allows write back write data caches. Opteron usually goes four levels tables TLB miss, three potential places check protection restrictions. Opteronobeys bottom-level PTE, checking others sure valid bitis set. entry 8 bytes long, page table 512 entries, Opteron 4 KiB pages, page tables exactly one page long. four level fields 9 bits long, page offset 12 bits. derivation leaves64/C0(4/C29+12) 16 bits sign extended ensure canonical addresses. Although explained translation legal addresses, prevents user creating illegal address translations getting mischief? pagetables protected written user programs. Thus, theuser try virtual address, controlling page table entries oper-ating system controls physical memory accessed. Sharing memory processes accomplished page table entry address space point physical memory page. Opteron employs four TLBs reduce address translation time, two instruction accesses two data accesses. Like multilevel caches, OpteronB-56 ■Appendix B Review Memory Hierarchyreduces TLB misses two larger L2 TLBs: one instructions one data. Figure B.28 describes data TLB. Summary: Protection 32-Bit Intel Pentium Versus 64-Bit AMD Opteron Memory management Opteron typical desktop server com- puters today, relying page-level address translation correct operation ofthe operating system provide safety multiple processes sharing computer.Although presented alternatives, Intel followed AMD ’s lead embraced AMD64 architecture. Hence, AMD Intel support 64-bit extension 80x86; yet, compatibility reasons, support elaborate segmented pro- tection scheme. segmented protection model looks harder build AMD64 model, ’s is. effort must especially frustrating engi- neers, customers use elaborate protection mechanism. addition,the fact protection model mismatch simple paging protection ofUNIX-like systems means used someone writing operatingsystem especially computer, ’t happened yet. B.6 Fallacies Pitfalls Even review memory hierarchy fallacies pitfalls! Pitfall small address space. five years DEC Carnegie Mellon University collaborated design new PDP-11 computer family, apparent creation fatalParameter Description Block size 1 PTE (8 bytes) L1 hit time 1 clock cycleL2 hit time 7 clock cyclesL1 TLB size instruction data TLBs: 40 PTEs per TLBs, 32 4 KiB pages 8 2 MiB 4 MiB pages L2 TLB size instruction data TLBs: 512 PTEs 4 KiB pagesBlock selection LRUWrite strategy (Not applicable)L1 block placement Fully associativeL2 block placement 4-way set associative Figure B.28 Memory hierarchy parameters Opteron L1 L2 instruction data TLBs.B.6 Fallacies Pitfalls ■B-57flaw. architecture announced IBM six years PDP-11 still thriving, minor modifications, 25 years later. DEC VAX, criticized including unnecessary functions, sold millions units PDP-11 wentout production. Why? fatal flaw PDP-11 size addresses (16 bits) compared address sizes IBM 360 (24 –31 bits) VAX (32 bits). Address size limits program length, size program amount ofdata needed program must less 2 Address size. reason address size hard change determines minimum width anything contain address: PC, register, memory word, effective-address arith- metic. plan expand address start, chancesof successfully changing address size slim usually means endof computer family. Bell Strecker (1976) put like this: one mistake made computer design difficult recover —not enough address bits memory addressing memory management. PDP-11 followed unbroken tradition nearly every known computer. [p. 2] partial list successful computers eventually starved death lack address bits includes PDP-8, PDP-10, PDP-11, Intel 8080, Intel 8086, Intel 80186, Intel 80286, Motorola 6800, AMI 6502, Zilog Z80, CRAY-1, CRAY X-MP. venerable 80x86 line bears distinction extended twice, first 32 bits Intel 80386 1985 recently 64 bits AMDOpteron. Pitfall Ignoring impact operating system performance memory hierarchy. Figure B.29 shows memory stall time due operating system spent three large workloads. 25% stall time either spent misses operating system results misses application programs becauseof interference operating system. Pitfall Relying operating systems change page size time. Alpha architects elaborate plan grow architecture time growing page size, even building size virtual address. itcame time grow page sizes later Alphas, operating system designersbalked virtual memory system revised grow address space whilemaintaining 8 KiB page. Architects computers noticed high TLB miss rates, added multiple, larger page sizes TLB. hope operating systems pro-grammers would allocate object largest page made sense, therebypreserving TLB entries. decade trying, operating systems use these“superpages ”only handpicked functions: mapping display memory I/O devices, using large pages database code.B-58 ■Appendix B Review Memory HierarchyB.7 Concluding Remarks difficulty building memory system keep pace faster processors underscored fact raw material main memory found cheapest computer. principle locality helps us —its soundness demonstrated levels memory hierarchy current com-puters, disks TLBs. However, increasing relative latency memory, taking hundreds clock cycles 2016, means programmers compiler writers must aware theparameters caches TLBs want programs perform well. B.8 Historical Perspective References Section M.3 (available online) examine history caches, virtual mem- ory, virtual machines. (The historical section covers appendix andChapter 3 .) IBM plays prominent role history three. References reading included. Additional reference: Gupta, S. Xiang, P., Yang, Y., Zhou, H., Locality prin- ciple revisited: probability-based quantitative approach. J. Parallel Distrib. Com- put. 73 (7), 1011 –1027.Time Misses% Time due application misses % Time due directly OS misses % Time OS misses andapplication conflicts Workload%i n applications% OSInherent application missesOS conflicts applicationsOS instruction missesData misses migrationData misses block operationsRest OS misses Pmake 47% 53% 14.1% 4.8% 10.9% 1.0% 6.2% 2.9% 25.8% Multipgm 53% 47% 21.6% 3.4% 9.2% 4.2% 4.7% 3.4% 24.9%Oracle 73% 27% 25.7% 10.2% 10.6% 2.6% 0.6% 2.8% 26.8% Figure B.29 Misses time spent misses applications operating system. operating system adds 25% execution time application. processor 64 KiB instruction cache two-leveldata cache 64 KiB first level 256 KiB second level; caches direct mapped 16-byteblocks. Collected Silicon Graphics POWER station 4D/340, multiprocessor four 33 MHz R3000 processors running three application workloads UNIX System V —Pmake, parallel compile 56 files; Multipgm, parallel numeric program MP3D running concurrently Pmake five-screen edit session; Oracle,running restricted version TP-1 benchmark using Oracle database. Data Torrellas, J., Gupta, A., Hen- nessy, J., 1992. Characterizing caching synchronization performance multiprocessor operating system. In: Proceedings Fifth International Conference Architectural Support Programming Languages Oper-ating Systems (ASPLOS), October 12 –15, 1992, Boston (SIGPLAN Notices 27:9 (September)), pp. 162 –174.B.8 Historical Perspective References ■B-59Exercises Amr Zaky B.1 [10/10/10/15] <B.1>You trying appreciate important principle locality justifying use cache memory, experiment com-puter L1 data cache main memory (you exclusively focus dataaccesses). latencies (in CPU cycles) different kinds accesses asfollows: cache hit, 1 cycle; cache miss, 110 cycles; main memory access cachedisabled, 105 cycles. a.[10]<B.1>When run program overall miss rate 3%, average memory access time (in CPU cycles) be? b.[10]<B.1>Next, run program specifically designed produce completely random data addresses locality. Toward end, use anarray size 1 GB (all fits main memory). Accesses random elements array continuously made (using uniform random number generator generate elements indices). data cache size 64 KB,what average memory access time be? c.[10]<B.1>If compare result obtained part (b) main memory access time cache disabled, conclude aboutthe role principle locality justifying use cache memory? d.[15]<B.1>You observed cache hit produces gain 104 cycles (1 cycle vs. 105), produces loss 5 cycles case amiss (110 cycles vs. 105). general case, express twoquantities G (gain) L (loss). Using two quantities (G L),identify highest miss rate cache use would disadvanta-geous. B.2 [15/15] <B.1>For purpose exercise, assume 512- byte cache 64-byte blocks. also assume main memory 2 KBlarge. regard memory array 64-byte blocks: M0, M1, …, M31. Figure B.30 sketches memory blocks reside different cache blocks cache direct-mapped. a.[15]<B.1>Show contents table cache organized fully- associative cache. b.[15]<B.1>Repeat part (a) cache organized four-way set associative cache. B.3 [10/10/10/10/15/10/15/20] <B.1>Cache organization often influenced desire reduce cache's power consumption. purpose assume cache physically distributed data array (holding data), tag array(holding tags), replacement array (holding information needed replace-ment policy). Furthermore, every one arrays physically distributed intomultiple subarrays (one per way) individually accessed; example, afour-way set associative least recently used (LRU) cache would four data sub-arrays, four tag subarrays, four replacement subarrays. assume theB-60 ■Appendix B Review Memory Hierarchyreplacement subarrays accessed per access LRU replacement policy used, per miss first-in, first-out (FIFO) replacement policy used. needed random replacement policy used. specific cache, determined accesses different arrays followingpower consumption weights ( Figure B.31 ): a.[10]<B.1>A cache read hit. arrays read simultaneously. b.[10]<B.1>Repeat part (a) cache read miss. c.[10]<B.1>Repeat part (a) assuming cache access split across two cycles. first cycle, tag subarrays accessed. second cycle, subarray whose tag matched accessed. d.[10]<B.1>Repeat part (c) cache read miss (no data array accesses second cycle). e.[15]<B.1>Repeat part (c) assuming logic added predict cache way accessed. tag subarray predicted way accessed cycle one. way hit (address match predicted way) implies cache hit. Away miss dictates examining tag subarrays second cycle. case ofa way hit, one data subarray (the one whose tag matched) accessed incycle two. Assume way predictor hits.Cache block Set Way Possible memory blocks 0 0 0 M0, M8, M16, M24 1 1 0 M1, M9, M17, M252 2 0 M2, M10, M18, M2633 0 …. 44 0 …. 55 0 …. 66 0 …. 7 7 0 M7, M15, M23, M31 Figure B.30 Memory blocks distributed direct-mapped cache. ArrayPower consumption weight (per way accessed) Data array 20 units Tag Array 5 unitsMiscellaneous array 1 unitMemory access 200 units Figure B.31 Power consumption costs different operations.Exercises Amr Zaky ■B-61f.[10]<B.1>Repeat part (e) assuming way predictor misses (the way choses wrong). fails, way predictor adds extra cycle accesses tag subarrays. Assume way predictor miss followed acache read hit. g.[15]<B.1>Repeat part (f) assuming cache read miss. h.[20]<B.1>Use parts (e), (f), (g) general case workload following statistics: way predictor miss rate=5% cache miss rate=3%. (Consider different replacement policies.) Estimate memory system (cache+memory) power usage (in power units) forthe following configurations. assume cache four-way set associative.Provide answers LRU, FIFO, random replacement policies. B.4 [10/10/15/15/15/20] <B.1>We compare write bandwidth requirements write-through versus write-back caches using concrete example. Let us assumethat 64 KB cache line size 32 bytes. cache allocate aline write miss. configured write-back cache, write back thedirty line needs replaced. also assume cache connected lower level hierarchy 64-bit-wide (8-byte-wide) bus. number CPU cycles B-bytes write access bus 10 + 5 B 8/C01/C6/C7 , square brackets represent “ceiling ”function. example, 8-byte write would take 10 + 58 8/C01/C6/C7 ¼10 cycles, whereas using formula 12-byte write would take 15 cycles. Answer following questions referring C code snippet below: ... #define PORTION 1 ...base = 8*i; (unsigned int j = base; j <base + PORTION; j++) //assume j stored register { data[j] = j; } a.[10]<B.1>For write-through cache, many CPU cycles spent write transfers memory combined iterations j loop? b.[10]<B.1>If cache configured write-back cache, many CPU cycles spent writing back cache line? c.[15]<B.1>Change PORTION 8 repeat part (a). d.[15]<B.1>What minimum number array updates cache line (before replacing it) would render write-back cache superior? e.[15]<B.1>Think scenario words cache line written (not necessarily using code) write-through cache require fewer total CPU cycles write-back cache.B-62 ■Appendix B Review Memory HierarchyB.5 [10/10/10/10/] <B.2>You building system around processor in-order execution runs 1.1 GHz CPI 1.35 excluding memory accesses. instructions read write data memory loads (20% allinstructions) stores (10% instructions). memory system com-puter composed split L1 cache imposes penalty hits. I-cache D-cache direct-mapped hold 32 KB each. I-cache 2%miss rate 32-byte blocks, D-cache write-through 5% miss rateand 16-byte blocks. write buffer D-cache eliminates stalls for95% writes. 512 KB write-back, unified L2 cache 64-byte blocks access time 15 ns. connected L1 cache 128-bit data bus runs 266 MHz transfer one 128-bit word per bus cycle. memoryreferences sent L2 cache system, 80% satisfied without going tomain memory. Also, 50% blocks replaced dirty. 128-bit-wide mainmemory access latency 60 ns, number bus words maybe transferred rate one per cycle 128-bit-wide 133 MHz main mem-ory bus. a.[10]<B.2>What average memory access time instruction accesses? b.[10]<B.2>What average memory access time data reads? c.[10]<B.2>What average memory access time data writes? d.[10]<B.2>What overall CPI, including memory accesses? B.6 [10/15/15] <B.2>Converting miss rate (misses per reference) misses per instruction relies upon two factors: references per instruction fetched frac- tion fetched instructions actually commits. a.[10]<B.2>The formula misses per instruction page B-5 written first terms three factors: miss rate, memory accesses, instruction count.Each factors represents actual events. different writingmisses per instruction miss rate times factor memory accesses per instruction ? b.[15]<B.2>Speculative processors fetch instructions commit. formula misses per instruction page B-5 refers misses per instruc-tion execution path; is, instructions must actually beexecuted carry program. Convert formula misses per instruc-tion page B-5 one uses miss rate, references per instruction fetched, fraction fetched instructions commit. rely upon factors rather formula page B-5? c.[15]<B.2>The conversion part (b) could yield incorrect value extent value factor references per instruction fetched equal number references particular instruction. Rewrite formulaof part (b) correct deficiency. B.7 [20]<B.1, B.3 >In systems write-through L1 cache backed write-back L2 cache instead main memory, merging write buffer simplified.Explain done. situations full write buffer(instead simple version proposed) could helpful?Exercises Amr Zaky ■B-63B.8 [5/5/5] <B.3>We want observe following calculation di¼ai+bi∗ci, i:0:511ðÞ Arrays a,b,c, dmemory layout displayed (each 512 4-byte-wide integer elements). calculation employs loop runs 512 iterations . Assume 32 Kbyte 4-way set associative cache single cycle access time. miss penalty 100 CPU cycles/access, cost write-back. cache write-back hits write-allocate misses cache ( Figure B.32 ). a.[5]<B3>How many cycles iteration take three loads single store miss data cache? b.[5]<B3>If cache line size 16 bytes, average number cycles average iteration take? (Hint: Spatial locality!) c.[5]<B3>If cache line size 64 bytes, average number cycles average iteration take? d.If cache direct-mapped size reduced 2048 bytes, average number cycles average iteration take? B.9 [20]<B.3>Increasing cache's associativity (with parameters kept con- stant) statistically reduces miss rate. However, pathological cases increasing cache's associativity would increase miss rate partic- ular workload. Consider case direct-mapped compared two-way set associative cache equal size. Assume set associative cache uses LRU replace-ment policy. simplify, assume block size one word. Now, construct atrace word accesses would produce misses two-way associativecache. (Hint: Focus constructing trace accesses exclusively directed single set two-way set associative cache, trace would exclusively access two blocks direct-mapped cache.) B.10 [10/10/15] <B.3>Consider two-level memory hierarchy made L1 L2 data caches. Assume caches use write-back policy write hit block size. List actions taken response following events: a.[10]<B.3>An L1 cache miss caches organized inclusive hierarchy. Mem. address bytes Contents 0–2047 Array 2048–4095 Array b 4096–6143 Array c 6144–8191 Array Figure B.32 Arrays layout memory.B-64 ■Appendix B Review Memory Hierarchyb.[10]<B.3>An L1 cache miss caches organized exclusive hierarchy. c.[15]<B.3>In parts (a) (b), consider possibility evicted line might clean dirty. B.11 [15/20] <B.2, B.3 >excluding instructions entering cache reduce conflict misses. a.[15]<B.3>Sketch program hierarchy parts program would better excluded entering instruction cache. (Hint: Consider program code blocks placed deeper loop nests blocks.) b.[20]<B.2, B.3 >Suggest software hardware techniques enforce exclu- sion certain blocks instruction cache. B.12 [5/15] <B.3>Whereas larger caches lower miss rates, also tend longer hit times. Assume direct-mapped 8 KB cache 0.22 ns hit time miss rate m1; also assume 4-way associative 64 KB cache 0.52 ns hit time miss rate m2. a.[5]<B.3>If miss penalty 100 ns, would advantageous use smaller cache reduce overall memory access time? b.[15]<B.3>Repeat part (a) miss penalties 10 1000 cycles. Conclude might advantageous use smaller cache. B.13 [15]<B.4>A program running computer four-entry fully associa- tive (micro) translation lookaside buffer (TLB) ( Figure B.33 ): following trace virtual page numbers accessed program. access indicate whether produces TLB hit/miss and, accesses pagetable, whether produces page hit fault. Put X page table columnif accessed ( Figures B.34 andB.35 ). B.14 [15/15/15/15/] <B.4>Some memory systems handle TLB misses software (as exception), others use hardware TLB misses. a.[15]<B.4>What trade-offs two methods handling TLB misses? b.[15]<B.4>Will TLB miss handling software always slower TLB miss handling hardware? Explain. VP# PP# Entry valid 53 0 1 71 0 10 10 115 25 1 Figure B.33 TLB contents (problem B.12).Exercises Amr Zaky ■B-65c.[15]<B.4>Are page table structures would difficult handle hardware possible software? structures would bedifficult software handle easy hardware manage? d.[15]<B.4>Why TLB miss rates floating-point programs generally higher integer programs?Virtual page index Physical page # Present 03 17 N26 N35 Y41 4 Y53 0 Y62 6 Y71 1 Y81 3 N91 8 N 10 10 Y11 56 Y12 110 Y13 33 Y14 12 N15 25 Figure B.34 Page table contents. Virtual page accessed TLB (hit miss) Page table (hit fault) 1 59 1410 6 1512 72 Figure B.35 Page access trace.B-66 ■Appendix B Review Memory HierarchyB.15 [20/20] <B.5>It possible provide flexible protection Intel Pentium architecture using protection scheme similar used Hewlett-Packard Precision Architecture (HP/PA). scheme, pagetable entry contains “protection ID ”(key) along access rights page. reference, CPU compares protection ID page table entry withthose stored four protection ID registers (access registersrequires CPU supervisor mode). match protectionID page table entry access permitted access (writing read-only page, example), exception generated. a.[20]<B.5>Explain model could used facilitate construction operating systems relatively small pieces code cannot overwriteeach (microkernels). advantages might operating system monolithic operating system code OS write memory location? b.[20]<B.5>A simple design change system would allow two protection IDs page table entry, one read access either write orexecute access (the field unused neither writable executable bit isset). advantages might different protection IDs forread write capabilities? ( Hint: Could make easier share data code processes?)Exercises Amr Zaky ■B-67C.1 Introduction C-2 C.2 Major Hurdle Pipelining —Pipeline Hazards C-10 C.3 Pipelining Implemented? C-26 C.4 Makes Pipelining Hard Implement? C-37 C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations C-45 C.6 Putting Together: MIPS R4000 Pipeline C-55 C.7 Cross-Cutting Issues C-65 C.8 Fallacies Pitfalls C-70 C.9 Concluding Remarks C-71 C.10 Historical Perspective References C-71 Updated Exercises Diana Franklin C-71C Pipelining: Basic Intermediate Concepts quite three-pipe problem. Sir Arthur Conan Doyle, Adventures Sherlock HolmesC.1 Introduction Many readers text covered basics pipelining another text (such basic text Computer Organization Design ) another course. Chapter 3 builds heavily material, readers ensure familiar concepts discussed appendix proceed-ing. read Chapter 3 , may find helpful turn material quick review. begin appendix basics pipelining, including discussing data path implications, introducing hazards, examining performance ofpipelines. section describes basic five-stage RISC pipeline basisfor rest appendix. Section C.2 describes issue hazards, cause performance problems, dealt with. Section C.3 dis- cusses simple five-stage pipeline actually implemented, focusing oncontrol hazards dealt with. Section C.4 discusses interaction pipelining various aspects instruction set design, including discussing important topic exceptions interaction pipelining. Readers unfamiliar concepts preciseand imprecise interrupts resumption exceptions find materialuseful, key understanding advanced approaches inChapter 3 . Section C.5 discusses five-stage pipeline extended handle longer-running floating-point instructions. Section C.6 puts concepts together case study deeply pipelined processor, MIPS R4000/4400, including eight-stage integer pipeline floating-point pipeline. MIPS R40000 similar single-issue embedded processor, theARM Cortex-A5, became available 2010, used several smartphones tablets. Section C.7 introduces concept dynamic scheduling use scoreboards implement dynamic scheduling. introduced cross-cuttingissue, used serve introduction core concepts inChapter 3 , focused dynamically scheduled approaches. Section C.7 also gentle introduction complex Tomasulo ’s algorithm covered inChapter 3 . Although Tomasulo ’s algorithm covered understood with- introducing scoreboarding, scoreboarding approach simpler easier tocomprehend. Pipelining? Pipelining implementation technique whereby multiple instructions over- lapped execution; takes advantage parallelism exists among actionsneeded execute instruction. Today, pipelining key implementation tech-nique used make fast processors, even processors cost less dollarare pipelined.C-2 ■Appendix C Pipelining: Basic Intermediate ConceptsA pipeline like assembly line. automobile assembly line, many steps, contributing something construction car. step operates parallel steps, although different car. computerpipeline, step pipeline completes part instruction. Like theassembly line, different steps completing different parts different instruc-tions parallel. steps called pipe stage pipe segment . stages connected one next form pipe —instructions enter one end, progress stages, exit end, cars wouldin assembly line. automobile assembly line, throughput defined number cars per hour determined often completed car exits assembly line. Like-wise, throughput instruction pipeline determined often aninstruction exits pipeline. pipe stages hooked together, allthe stages must ready proceed time, would requirein assembly line. time required moving instruction one stepdown pipeline processor cycle . stages proceed time, length processor cycle determined time required slowest pipe stage, auto assembly line longest step would determine time advancing cars line. computer, processor cycle isalmost always 1 clock cycle. pipeline designer ’s goal balance length pipeline stage, designer assembly line tries balance time step theprocess. stages perfectly balanced, time per instruction thepipelined processor —assuming ideal conditions —is equal Time per instruction unpipelined machine Number pipe stages conditions, speedup pipelining equals number pipe stages, assembly line nstages ideally produce cars ntimes fast. Usually, however, stages perfectly balanced; furthermore,pipelining involve overhead. Thus, time per instruction pipe-lined processor minimum possible value, yet close. Pipelining yields reduction average execution time per instruction. starting point processor takes multiple clock cycles per instruction, pipelining reduces CPI. primary view take. Pipelining implementation technique exploits parallelism among instructions sequential instruction stream. substantial advantagethat, unlike speedup techniques (see Chapter 4 ), visible programmer. Basics RISC V Instruction Set Throughout book use RISC V, load-store architecture, illustrate thebasic concepts. Nearly ideas introduce book applicable otherC.1 Introduction ■C-3processors, implementation may much complicated complex instructions. section, make use core RISC V architecture; seeChapter 1 full description. Although use RISC V, concepts significantly similar apply RISC, including core archi-tectures ARM MIPS. RISC architectures characterized keyproperties: ■All operations data apply data registers typically change entireregister (32 64 bits per register). ■The operations affect memory load store operations movedata memory register memory register, respectively.Load store operations load store less full register (e.g., byte, 16 bits, 32 bits) often available. ■The instruction formats number, instructions typically beingone size. RISC V, register specifiers: rs1, rs2, rd always place simplifying control. simple properties lead dramatic simplifications implementation pipelining, instruction sets designed way. Chapter 1 contains full description RISC V ISA, assume reader readChapter 1 . Simple Implementation RISC Instruction Set understand RISC instruction set implemented pipelined fashion, need understand implemented without pipelining. section shows simple implementation every instruction takes 5clock cycles. extend basic implementation pipelined version, resulting much lower CPI. unpipelined implementation eco- nomical highest-performance implementation without pipelining. Instead, itis designed lead naturally pipelined implementation. Implementing theinstruction set requires introduction several temporary registers arenot part architecture; introduced section simplify pipelin-ing. implementation focus pipeline integer subset aRISC architecture consists load-store word, branch, integer ALUoperations. Every instruction RISC subset implemented in, most, 5 clock cycles. 5 clock cycles follows. 1.Instruction fetch cycle (IF): Send program counter (PC) memory fetch current instruction memory. Update PC next sequential instruction adding 4 (becauseeach instruction 4 bytes) PC.C-4 ■Appendix C Pipelining: Basic Intermediate Concepts2.Instruction decode/register fetch cycle (ID): Decode instruction read registers corresponding register source specifiers register file. equality test registers theyare read, possible branch. Sign-extend offset field instructionin case needed. Compute possible branch target address adding sign-extended offset incremented PC. Decoding done parallel reading registers, possible register specifiers fixed location RISC architecture. tech-nique known fixed-field decoding . Note may read register ’t use, ’t help also ’t hurt performance. (It waste energy read unneeded register, power-sensitive designs might avoid this.) Forloads ALU immediate operations, immediate field always sameplace, easily sign extend it. (For complete implementation RISC V, would need compute two different sign-extended values, immediate field store different location.) 3.Execution/effective address cycle (EX): ALU operates operands prepared prior cycle, performing one three functions, depending instruction type. ■Memory reference —The ALU adds base register offset form effective address. ■Register-Register ALU instruction —The ALU performs operation spec- ified ALU opcode values read register file. ■Register-Immediate ALU instruction —The ALU performs operation specified ALU opcode first value read register fileand sign-extended immediate. ■Conditional branch —Determine whether condition true. load-store architecture effective address execution cycles combined single clock cycle, instruction needs simulta-neously calculate data address perform operation data. 4.Memory access (MEM): instruction load, memory read using effective address computed previous cycle. store, memory writes datafrom second register read register file using effective address. 5.Write-back cycle (WB): ■Register-Register ALU instruction load instruction: Write result register file, whether comes memory system (for load) ALU (for ALU instruction). implementation, branch instructions require three cycles, store instruc- tions require four cycles, instructions require five cycles. Assuming aC.1 Introduction ■C-5branch frequency 12% store frequency 10%, typical instruction dis- tribution leads overall CPI 4.66. implementation, however, opti- mal either achieving best performance using minimal amount ofhardware given performance level; leave improvement designas exercise instead focus pipelining version. Classic Five-Stage Pipeline RISC Processor pipeline execution described previous section almost nochanges simply starting new instruction clock cycle. (See wechose design?) clock cycles previous section becomesapipe stage —a cycle pipeline. results execution pattern shown inFigure C.1 , typical way pipeline structure drawn. Although instruction takes 5 clock cycles complete, clock cycle hardwarewill initiate new instruction executing part five differentinstructions. may find hard believe pipelining simple this; ’s not. following sections, make RISC pipeline “real”by dealing problems pipelining introduces. start with, determine happens every clock cycle processor make sure ’t try perform two different operations data path resource clock cycle. example, single ALU can-not asked compute effective address perform subtract operation atthe time. Thus, must ensure overlap instructions pipelinecannot cause conflict. Fortunately, simplicity RISC instruction setmakes resource evaluation relatively easy. Figure C.2 shows simplified version RISC data path drawn pipeline fashion. see, major functionalunits used different cycles, hence overlapping execution multiple Instruction numberClock number 1 2 3 45678 9 Instruction ID EX MEM WB Instruction i+1 ID EX MEM WB Instruction i+2 ID EX MEM WB Instruction i+3 ID EX MEM WB Instruction i+4 ID EX MEM WB Figure C.1 Simple RISC pipeline. clock cycle, another instruction fetched begins five-cycle execution. instruction started every clock cycle, performance five times processorthat pipelined. names stages pipeline used cycles unpi- pelined implementation: ¼instruction fetch, ID ¼instruction decode, EX ¼execution, MEM¼memory access, WB¼write-back.C-6 ■Appendix C Pipelining: Basic Intermediate Conceptsinstructions introduces relatively conflicts. three observations fact rests. First, use separate instruction data memories, would typically implement separate instruction data caches (discussed Chapter 2 ). use separate caches eliminates conflict single memory would arisebetween instruction fetch data memory access. Notice pipelined pro-cessor clock cycle equal unpipelined version, memorysystem must deliver five times bandwidth. increased demand one cost ofhigher performance. Second, register file used two stages: one reading ID one writing WB. uses distinct, simply show register file two places. Hence, need perform two reads one write every clock cycle. ALU ALUReg Reg IM DM Reg IM DMTime (in clock cycles) CC 1 CC 2 CC 3 CC 4 CC 5 CC 6 CC 7Program execution order (in instructions) RegCC 8 CC 9 Reg IM DM RegALU Reg IM DM RegALU Reg IM DM RegALU Figure C.2 pipeline thought series data paths shifted time. figure shows overlap among parts data path, clock cycle 5 (CC 5) showing steady-state situation. reg- ister file used source ID stage destination WB stage, appears twice. show itis read one part stage written another using solid line, right left, respectively, adashed line side. abbreviation IM used instruction memory, DM data memory, CC clock cycle.C.1 Introduction ■C-7To handle reads write register (and another reason, become obvious shortly), perform register write first half clock cycle read second half. Third, Figure C.2 deal PC. start new instruction every clock, must increment store PC every clock, must done dur-ing stage preparation next instruction. Furthermore, must alsohave adder compute potential branch target address ID. One furtherproblem need ALU ALU stage evaluate branch condi-tion. Actually, ’t really need full ALU evaluate comparison two registers, need enough function occur pipestage. Although critical ensure instructions pipeline attempt use hardware resources time, must also ensure instructionsin different stages pipeline interfere one another. separationis done introducing pipeline registers successive stages pipeline, end clock cycle results given stage stored aregister used input next stage next clock cycle. Figure C.3 shows pipeline drawn pipeline registers. Although many figures omit registers simplicity, required make pipeline operate properly must present. course, similar reg-isters would needed even multicycle data path pipelining(because values registers preserved across clock boundaries). caseof pipelined processor, pipeline registers also play key role carryingintermediate results one stage another source destinationmay directly adjacent. example, register value stored store instruction read ID, actually used MEM; passed two pipeline registers reach data memory MEM stage.Likewise, result ALU instruction computed EX, actuallystored WB; arrives passing two pipeline registers. issometimes useful name pipeline registers, follow conventionof naming pipeline stages connect, registers calledIF/ID, ID/EX, EX/MEM, MEM/WB. Basic Performance Issues Pipelining Pipelining increases processor instruction throughput —the number instruc- tions completed per unit time —but reduce execution time individual instruction. fact, usually slightly increases execution timeof instruction due overhead control pipeline. increasein instruction throughput means program runs faster lower total exe- cution time, even though single instruction runs faster! fact execution time instruction decrease puts limits practical depth pipeline, see next section. Inaddition limitations arising pipeline latency, limits arise imbalanceC-8 ■Appendix C Pipelining: Basic Intermediate Conceptsamong pipe stages pipelining overhead. Imbalance among pipe stages reduces performance clock run faster timeneeded slowest pipeline stage. Pipeline overhead arises combina-tion pipeline register delay clock skew. pipeline registers add setuptime, time register input must stable clock signalthat triggers write occurs, plus propagation delay clock cycle. Clock skew, maximum delay clock arrives two registers,CC 1 CC 2 CC 3 CC 4 CC 5 CC 6Time (in clock cycles) DM IM ALU DM IM ALU DMIM ALU IM ALU IMReg Reg Reg Reg RegReg Reg Figure C.3 pipeline showing pipeline registers successive pipeline stages. Notice registers prevent interference two different instructions adjacent stages pipeline. registers also play critical role carrying data given instruction one stage other. edge-triggered property reg- isters—that is, values change instantaneously clock edge —is critical. Otherwise, data one instruction could interfere execution another!C.1 Introduction ■C-9also contributes lower limit clock cycle. clock cycle small sum clock skew latch overhead, pipelining useful, time left cycle useful work. interestedreader see Kunkel Smith (1986) . Example Consider unpipelined processor previous section. Assume 4 GHz clock (or 0.5 ns clock cycle) uses four cycles ALU oper-ations branches five cycles memory operations. Assume rel-ative frequencies operations 40%, 20%, 40%, respectively.Suppose due clock skew setup, pipelining processor adds 0.1 ns overhead clock. Ignoring latency impact, much speedup instruction execution rate gain pipeline? Answer average instruction execution time unpipelined processor Average instruction execution time ¼Clock cycle/C2Average CPI ¼0:5n s/C240%+2 0%ðÞ /C2 4+4 0%/C25 ½/C138 ¼0:5n s/C24:4 ¼2:2n pipelined implementation, clock must run speed slowest stage plus overhead, 0.5+0.1 0.6 ns; average instructionexecution time. Thus, speedup pipelining Speedup pipelining ¼Average instruction time unpipelined Average instruction time pipelined ¼2:2n 0:6n s¼3:7 times 0.1 ns overhead essentially establishes limit effectiveness pipelin- ing. overhead affected changes clock cycle, Amdahl ’s Law tells us overhead limits speedup. simple RISC pipeline would function fine integer instructions every instruction independent every instruction pipeline. Inreality, instructions pipeline depend one another; topic next section. C.2 Major Hurdle Pipelining —Pipeline Hazards situations, called hazards , prevent next instruction instruc- tion stream executing designated clock cycle. Hazards reduce performance ideal speedup gained pipelining. three classesof hazards:C-10 ■Appendix C Pipelining: Basic Intermediate Concepts1.Structural hazards arise resource conflicts hardware cannot sup- port possible combinations instructions simultaneously overlapped exe- cution. modern processors, structural hazards occur primarily specialpurpose functional units less frequently used (such floating pointdivide complex long running instructions). major per-formance factor, assuming programmers compiler writers aware thelower throughput instructions. Instead spending time thisinfrequent case, focus two hazards much frequent. 2.Data hazards arise instruction depends results previous instruction way exposed overlapping instructions thepipeline. 3.Control hazards arise pipelining branches instructions change PC. Hazards pipelines make necessary stallthe pipeline. Avoiding haz- ard often requires instructions pipeline allowed proceed whileothers delayed. pipelines discuss appendix, instructionis stalled, instructions issued later stalled instruction —and hence far along pipeline —are also stalled. Instructions issued earlier stalled instruction —and hence farther along pipeline —must continue, oth- erwise hazard never clear. result, new instructions fetched duringthe stall. see several examples pipeline stalls operate section — don’tw r r ,t h e ya r e n ’t complex might sound! Performance Pipelines Stalls stall causes pipeline performance degrade ideal performance. Let’s look simple equation finding actual speedup pipelining, starting formula previous section: Speedup pipelining ¼Average instruction time unpipelined Average instruction time pipelined ¼CPI unpipelined/C2Clock cycle unpipelined CPI pipelined/C2Clock cycle pipelined ¼CPI unpipelined/C2Clock cycle unpipelined CPI pipelined/C2Clock cycle pipelined Pipelining thought decreasing CPI clock cycle time. traditional use CPI compare pipelines, let ’s start assumption. ideal CPI pipelined processor almost always 1. Hence, computethe pipelined CPI: CPI pipelined¼Ideal CPI + Pipeline stall clock cycles per instruction ¼1 + Pipelines stall clock cycles per instructionC.2 Major Hurdle Pipelining —Pipeline Hazards ■C-11If ignore cycle time overhead pipelining assume stages perfectly balanced, cycle time two processors equal, leading Speedup¼CPI unpiplined 1 + Pipeline stall cycles per instruction One important simple case instructions take number cycles, must also equal number pipeline stages (also called depth pipeline ). case, unpipelined CPI equal depth pipeline, leading Speedup¼Pipeline depth 1 + Pipeline stall cycles per instruction pipeline stalls, leads intuitive result pipelining improve performance depth pipeline. Data Hazards major effect pipelining change relative timing instructions over-lapping execution. overlap introduces data control hazards. Datahazards occur pipeline changes order read/write accesses tooperands order differs order seen sequentially executinginstructions unpipelined processor. Assume instruction ioccurs program order instruction jand instructions use register x, three different types hazards occur iandj: 1.Read Write (RAW) hazard: common, occur read register xby instruction joccurs write register xby instruc- tioni. hazard prevented instruction j would use wrong value ofx. 2.Write Read (WAR) hazard: hazard occurs read register xby instruction ioccurs write register xby instruction j. case, instruction iwould use wrong value x. WAR hazards impossible simple five stage, integrer pipeline, occur instructions reordered, see discuss dynamically scheduled pipelinesbeginning page C.65. 3.Write Write (WAW) hazard: hazard occurs write register xby instruction ioccurs write register xby instruction j. occurs, register xwill wrong value going forward. WAR hazards also impossible simple five stage, integrer pipeline, occur wheninstructions reordered running times vary, see later. Chapter 3 explores issues data dependence hazards much detail. now, focus RAW hazards.C-12 ■Appendix C Pipelining: Basic Intermediate ConceptsConsider pipelined execution instructions: add x1,x2,x3 sub x4,x1,x5and x6,x1,x7or x8,x1,x9 xor x10,x1,x11 instructions add use result add instruction. shown Figure C.4 , theadd instruction writes value x1 WB pipe stage, sub instruction reads value ID stage, results RAW hazard. Unless precautions taken prevent it, sub instruction read wrong value try use it. fact, value used sub instruction even deterministic: though might think logical assume sub would always use value x1 assigned instruction prior add, Figure C.4 use result add instruction next three instructions causes hazard, register written instructions read it.C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-13always case. interrupt occur add andsub instructions, WB stage add complete, value x1 point result add. unpredictable behavior obviously unacceptable. Theand instruction also creates possible RAW hazard. see Figure C.4 , write x1 complete end clock cycle 5. Thus, theand instruction reads registers clock cycle 4 receive wrong results. Thexor instruction operates properly register read occurs clock cycle 6, register write. orinstruction also operates without incurring hazard perform register file reads second halfof cycle writes first half. Note xor instruction still depends add, longer creates hazard; topic explore detail Chapter 3 . next subsection discusses technique eliminate stalls hazard involving sub andand instructions. Minimizing Data Hazard Stalls Forwarding problem posed Figure C.4 solved simple hardware technique called forwarding (also called bypassing sometimes short-circuiting ). key insight forwarding result really needed sub add actually produces it. result moved pipeline register add stores sub needs it, need stall avoided. Using observation, forwarding works follows: 1.The ALU result EX/MEM MEM/WB pipeline registers always fed back ALU inputs. 2.If forwarding hardware detects previous ALU operation written register corresponding source current ALU operation, controllogic selects forwarded result ALU input rather value readfrom register file. Notice forwarding, sub stalled, add completed bypass activated. relationship also true case aninterrupt two instructions. example Figure C.4 shows, need forward results immediately previous instruction also possibly instruction thatstarted two cycles earlier. Figure C.5 shows example bypass paths place highlighting timing register read writes. code sequence executed without stalls. Forwarding generalized include passing result directly func- tional unit requires it: result forwarded pipeline register corre-sponding output one unit input another, rather fromC-14 ■Appendix C Pipelining: Basic Intermediate Conceptsthe result unit input unit. Take, example, following sequence: add x1,x2,x3 ld x4,0(x1)sd x4,12(x1) prevent stall sequence, would need forward values ALU output memory unit output pipeline registers theALU data memory inputs. Figure C.6 shows forwarding paths example.Figure C.5 set instructions depends add result uses forwarding paths avoid data hazard. inputs sub andand instructions forward pipeline registers first ALU input. receives result forwarding register file, easily accomplished reading registersin second half cycle writing first half, dashed lines registers indicate. Noticethat forwarded result go either ALU input; fact, ALU inputs could use forwarded inputs either pipeline register different pipeline registers. would occur, example, instruction x6,x1,x4 .C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-15Data Hazards Requiring Stalls Unfortunately, potential data hazards handled bypassing. Con- sider following sequence instructions: ld x1,0(x2) sub x4,x1,x5and x6,x1,x7 x8,x1,x9 pipelined data path bypass paths example shown Figure C.7 . case different situation back-to-back ALU oper- ations. ldinstruction data end clock cycle 4 (its MEM cycle), sub instruction needs data beginning clock cycle. Thus, data hazard using result load instructioncannot completely eliminated simple hardware. Figure C.7 shows, forwarding path would operate backward time —a capability yet available computer designers! canforward result immediately ALU pipeline registers use operation, begins 2 clock cycles load. Likewise, orinstruction problem, receives value register file. sub instruction, forwardedFigure C.6 Forwarding operand required stores MEM. result load forwarded memory output memory input stored. addition, ALU output forwarded ALU input address calculation load store (this different forwarding another ALU operation). store depended immediately preceding ALU operation (not shown herein), result would need beforwarded prevent stall.C-16 ■Appendix C Pipelining: Basic Intermediate Conceptsresult arrives late —at end clock cycle, needed beginning. load instruction delay latency cannot eliminated for- warding alone. Instead, need add hardware, called pipeline interlock ,t preserve correct execution pattern. general, pipeline interlock detects haz-ard stalls pipeline hazard cleared. case, interlock stallsthe pipeline, beginning instruction wants use data thesource instruction produces it. pipeline interlock introduces stall bubble, structural hazard. CPI stalled instruction increases length stall (1 clock cycle case). Figure C.8 shows pipeline stall using names pipeline stages. stall causes instructions starting sub move one cycle later time, forwarding instruction goes register file, forwarding needed orinstruction. insertion bubble causes number cycles complete sequenceto grow one. instruction started clock cycle 4 (and none finishes cycle 6).Figure C.7 load instruction bypass results andorinstructions, sub, would mean forwarding result “negative time. ”C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-17Branch Hazards Control hazards cause greater performance loss RISC V pipeline data hazards. branch executed, may may change PC something current value plus 4. Recall branch changes PC target address, taken branch; falls through, taken ,o runtaken . instruction iis taken branch, PC usually chan- ged end ID, completion address calculation andcomparison. Figure C.9 shows simplest method dealing branches redo fetch instruction following branch, detect branch ID(when instructions decoded). first cycle essentially stall, never performs useful work. may noticed branch untaken, repetition stage unnecessary correct instruction wasindeed fetched. develop several schemes take advantage factshortly. One stall cycle every branch yield performance loss 10% 30% depending branch frequency, examine techniques dealwith loss.ld x1,0(x2) ID EX MEM WB sub x4,x1,x5 ID EX MEM WB x6,x1,x7 ID EX MEM WB x8,x1,x9 ID EX MEM WB ld x1,0(x2) ID EX MEM WB sub x4,x1,x5 ID Stall EX MEM WB x6,x1,x7 Stall ID EX MEM WB x8,x1,x9 Stall ID EX MEM WB Figure C.8 top half, see stall needed: MEM cycle load produces value needed EX cycle sub, occurs time. problem solved inserting stall, shown bottom half. Branch instruction ID EX MEM WB Branch successor ID EX MEM WBBranch successor+1 ID EX MEMBranch successor+2 ID EX Figure C.9 branch causes one-cycle stall five-stage pipeline. instruction branch fetched, instruction ignored, fetch restarted oncethe branch target known. probably obvious branch taken, thesecond branch successor redundant. addressed shortly.C-18 ■Appendix C Pipelining: Basic Intermediate ConceptsReducing Pipeline Branch Penalties many methods dealing pipeline stalls caused branch delay; discuss four simple compile time schemes subsection. four schemes actions branch static —they fixed branch entire execution. software try minimize branch penalty usingknowledge hardware scheme branch behavior. look athardware-based schemes dynamically predict branch behavior, Chapter 3 looks powerful hardware techniques dynamic branch prediction. simplest scheme handle branches freeze orflush pipeline, holding deleting instructions branch branch destination known.The attractiveness solution lies primarily simplicity hardware software. solution used earlier pipeline shown Figure C.9 . case, branch penalty fixed cannot reduced software. higher-performance, slightly complex, scheme treat every branch taken, simply allowing hardware continue thebranch executed. Here, care must taken change processorstate branch outcome definitely known. complexity schemearises know state might changed instruction andhow “back ”such change. simple five-stage pipeline, predicted-not-taken orpredicted-untaken scheme implemented continuing fetch instructions branch anormal instruction. pipeline looks nothing ordinary happen-ing. branch taken, however, need turn fetched instruction ano-op restart fetch target address. Figure C.10 shows situations. alternative scheme treat every branch taken. soon branch decoded target address computed, assume branch taken Untaken branch instruction ID EX MEM WB Instruction i+1 ID EX MEM WB Instruction i+2 ID EX MEM WB Instruction i+3 ID EX MEM WB Instruction i+4 ID EX MEM WB Taken branch instruction ID EX MEM WB Instruction i+1 idle idle idle idle Branch target ID EX MEM WBBranch target+1 ID EX MEM WBBranch target+2 ID EX MEM WB Figure C.10 predicted-not-taken scheme pipeline sequence branch untaken (top) taken (bottom). branch untaken, determined ID, fetch fall-through continue. branch taken ID, restart fetch branch target. causes instructions following branch tostall 1 clock cycle.C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-19begin fetching executing target. buys us one-cycle improvement branch actually taken, know target address end ID, one cycle know whether branch condition satisfied theALU stage. either predicted-taken predicted-not-taken scheme, compilercan improve performance organizing code frequent pathmatches hardware ’s choice. fourth scheme, heavily used early RISC processors called delayed branch . delayed branch, execution cycle branch delay one branch instruction sequential successor 1 branch target taken sequential successor branch delay slot . instruction executed whether branch taken. pipeline behavior five-stage pipelinewith branch delay shown Figure C.11 . Although possible branch delay longer one, practice almost processors delayed branch single instruction delay; techniques used pipeline lon- ger potential branch penalty.The job compiler make successorinstructions valid useful. Although delayed branch useful short simple pipelines time hardware prediction expensive, technique complicates imple-mentation dynamic branch prediction. reason, RISC Vappropriately omitted delayed branches. Untaken branch instruction ID EX MEM WB Branch delay instruction ( i+1) ID EX MEM WB Instruction i+2 ID EX MEM WB Instruction i+3 ID EX MEM WB Instruction i+4 ID EX MEM WB Taken branch instruction ID EX MEM WB Branch delay instruction ( i+1) ID EX MEM WB Branch target ID EX MEM WBBranch target+1 ID EX MEM WBBranch target+2 ID EX MEM WB Figure C.11 behavior delayed branch whether branch taken. instructions delay slot (there one delay slot RISC architectures incorporated them) executed. branch untaken, execution continues instruction branch delay instruction; branch taken, execution continues branch target. instruction branch delay slot also branch, meaningis unclear: branch taken, happen branch branch delay slot? confusion, architectures delay branches often disallow putting branch delay slot.C-20 ■Appendix C Pipelining: Basic Intermediate ConceptsPerformance Branch Schemes effective performance schemes? effective pipeline speedup branch penalties, assuming ideal CPI 1, Pipeline speedup¼Pipeline depth 1 + Pipeline stall cycles branches following: Pipeline stall cycles branches ¼Branch frequency/C2Branch penalty obtain: Pipeline speedup¼Pipeline depth 1 + Branch frequency /C2Branch penalty branch frequency branch penalty component uncon- ditional conditional branches. However, latter dominate aremore frequent. Example deeper pipeline, MIPS R4000 later RISC processors, takes least three pipeline stages branch-target address known additional cycle branch condition evaluated, assuming nostalls registers conditional comparison. three-stage delay leadsto branch penalties three simplest prediction schemes listed inFigure C.12 . Find effective addition CPI arising branches pipeline, assuming following frequencies: Unconditional branch 4% Conditional branch, untaken 6%Conditional branch, taken 10% Answer find CPIs multiplying relative frequency unconditional, condi- tional untaken, conditional taken branches respective penalties. Theresults shown Figure C.13 . Branch scheme Penalty unconditional Penalty untaken Penalty taken Flush pipeline 2 3 3 Predicted taken 2 3 2Predicted untaken 2 0 3 Figure C.12 Branch penalties three simplest prediction schemes deeper pipeline.C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-21The differences among schemes substantially increased longer delay. base CPI 1 branches source stalls, idealpipeline would 1.56 times faster pipeline used stall-pipelinescheme. predicted-untaken scheme would 1.13 times better thestall-pipeline scheme assumptions. Reducing Cost Branches Prediction pipelines get deeper potential penalty branches increases, usingdelayed branches similar schemes becomes insufficient. Instead, need toturn aggressive means predicting branches. schemes fall intotwo classes: low-cost static schemes rely information available compiletime strategies predict branches dynamically based program behavior.We discuss approaches here. Static Branch Prediction key way improve compile-time branch prediction use profile informationcollected earlier runs. key observation makes worthwhile thatthe behavior branches often bimodally distributed; is, individualbranch often highly biased toward taken untaken. Figure C.14 shows suc- cess branch prediction using strategy. input data used runs collecting profile; studies shown changing inputso profile different run leads small change accuracyof profile-based prediction. effectiveness branch prediction scheme depends accu- racy scheme frequency conditional branches, vary SPECfrom 3% 24%. fact misprediction rate integer programs ishigher programs typically higher branch frequency major lim- itation static branch prediction. next section, consider dynamic branch predictors, recent processors employed.Additions CPI branch costs Branch schemeUnconditional branchesUntaken conditional branchesTaken conditional branchesAll branches Frequency event4% 6% 10% 20% Stall pipeline 0.08 0.18 0.30 0.56 Predicted taken 0.08 0.18 0.20 0.46Predicted untaken 0.08 0.00 0.30 0.38 Figure C.13 CPI penalties three branch-prediction schemes deeper pipeline.C-22 ■Appendix C Pipelining: Basic Intermediate ConceptsDynamic Branch Prediction Branch-Prediction Buffers simplest dynamic branch-prediction scheme branch-prediction buffer branch history table . branch-prediction buffer small memory indexed lower portion address branch instruction. memory contains bitthat says whether branch recently taken not. scheme simplestsort buffer; tags useful reduce branch delay islonger time compute possible target PCs. buffer, ’t know, fact, prediction correct —it may put another branch low-order address bits. ’t matter. prediction hint assumed correct, fetch- ing begins predicted direction. hint turns wrong, predic-tion bit inverted stored back. buffer effectively cache every access hit, and, see, performance buffer depends often prediction thebranch interest accurate prediction matches. weanalyze performance, useful make small, important, improvement accuracy branch-prediction scheme. Misprediction rate 0%25% 5%10%20% 15% BenchmarkInteger Floating-pointhcompresseqntottespressogccli doducear ydro2d mdljdp su2cor12%22% 18% 11%12% 5%6%9%10%15% Figure C.14 Misprediction rate SPEC92 profile-based predictor varies widely generally better floating-point programs, average mis-prediction rate 9% standard deviation 4%, integer programs, average misprediction rate 15% standard deviation 5%. actual performance depends prediction accuracy branch fre-quency, vary 3% 24%.C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-23This simple 1-bit prediction scheme performance shortcoming: even branch almost always taken, likely predict incorrectly twice, rather once, taken, misprediction causes prediction bit beflipped. remedy weakness, 2-bit prediction schemes often used. 2-bit scheme, prediction must miss twice changed. Figure C.15 shows finite-state processor 2-bit prediction scheme. branch-prediction buffer implemented small, special “cache ” accessed instruction address pipe stage, pair bits attached block instruction cache fetched instruction. instruction decoded branch branch predicted taken, fetch-ing begins target soon PC known. Otherwise, sequential fetch-ing executing continue. Figure C.15 shows, prediction turns wrong, prediction bits changed. kind accuracy expected branch-prediction buffer using 2 bits per entry real applications? Figure C.16 shows SPEC89 bench- marks branch-prediction buffer 4096 entries results prediction accuracy Taken Taken TakenTakenNot taken taken takenNot takenPredict taken 11Predict taken 10 Predict taken 01Predict taken 00 Figure C.15 states 2-bit prediction scheme. using 2 bits rather 1, branch strongly favors taken taken —as many branches —will mispre- dicted less often 1-bit predictor. 2 bits used encode four states system. 2-bit scheme actually specialization general scheme n-bit saturating counter entry prediction buffer. n-bit counter, counter take values 0 2n/C01: counter greater equal one-half maximum value (2n/C01), branch pre- dicted taken; otherwise, predicted untaken. Studies n-bit predictors shown 2-bit predictors almost well, thus systems rely 2-bit branchpredictors rather general n-bit predictors.C-24 ■Appendix C Pipelining: Basic Intermediate Conceptsranging 99% 82%, misprediction rate 1% –18%. 4K entry buffer, like used results, considered small 2017, largerbuffer could produce somewhat better results. try exploit ILP, accuracy branch prediction becomes critical. see Figure C.16 , accuracy predictors integer programs, typically also higher branch frequencies, lower loop-intensive scientific programs. attack problem two ways:by increasing size buffer increasing accuracy scheme weuse prediction. buffer 4K entries, however, Figure C.17 shows, performs quite comparably infinite buffer, least benchmarks like thosein SPEC. data Figure C.17 make clear hit rate buffer major limiting factor. mentioned, simply increasing number bitsper predictor without changing predictor structure also little impact. Instead, need look might increase accuracy predictor, Chapter 3 .18%tomcatv spiceSPEC89 benchmarksgcc li 2% 4% 6% 8% 10% 12% 14% 16%0% 1% 5% 9% 9% 12% 5% 10%18%nasa7 matrix300 doduc fpppp espresso eqntott1% 0% Frequenc mispredictions Figure C.16 Prediction accuracy 4096-entry 2-bit prediction buffer SPEC89 benchmarks. misprediction rate integer benchmarks (gcc, espresso, eqntott, li) substantially higher (average 11%) floating-pointprograms (average 4%). Omitting floating-point kernels (nasa7, matrix300, tomcatv) still yields higher accuracy FP benchmarks integer bench- marks. data, well rest data section, taken branch-prediction study done using IBM Power architecture optimized code system. See Pan et al. (1992) . Although data older version subset SPEC benchmarks, newer benchmarks larger would show slightlyworse behavior, especially integer benchmarks.C.2 Major Hurdle Pipelining —Pipeline Hazards ■C-25C.3 Pipelining Implemented? proceed basic pipelining, need review simple implementation unpipelined version RISC V. Simple Implementation RISC V section follow style Section C.1 , showing first simple unpipe- lined implementation pipelined implementation. time, however,our example specific RISC V architecture.nasa71% 0% matrix3000% 0% tomcatv1% 0% doduc spiceSPEC89 benchmarksfpppp gcc espresso eqntott li 0% 2% 4% 6% 8% 10% 12% 14% 16% 18%4096 entries: 2 bits per entry Unlimited entries: 2 bits per entry Frequenc mis predictions5% 5% 9% 9% 9% 9% 12% 11% 5% 5% 18% 18% 10% 10% Figure C.17 Prediction accuracy 4096-entry 2-bit prediction buffer versus infinite buffer SPEC89 benchmarks. Although data older version subset SPEC benchmark s, results would comparable newerversionswithperhapsasmanyas8Kentriesneededtomatchaninfinite2-bitpredictor.C-26 ■Appendix C Pipelining: Basic Intermediate ConceptsIn subsection, focus pipeline integer subset RISC V con- sists load-store word, branch equal, integer ALU operations. Later appendix incorporate basic floating-point operations. Althoughwe discussonly subset RISC V, basic principles extended handle instruc-tions; example, adding store involves additional computing immediatefield. initially used less aggressive implementation branch instruction. Weshow implement aggressive version end section. Every RISC V instruction implemented in, most, 5 clock cycles. 5 clock cycles follows: 1.Instruction fetch cycle (IF): IR Mem[PC]; NPC PC + 4; Operation —Send PC fetch instruction memory instruction register (IR); increment PC 4 address next sequentialinstruction. IR used hold instruction needed subsequentclock cycles; likewise, register NPC used hold next sequential PC. 2.Instruction decode/register fetch cycle (ID): Regs[rs1]; B Regs[rs2]; Imm sign-extended immediate field IR; Operation —Decode instruction access register file read reg- isters (rs1 rs2 register specifiers). outputs general-purposeregisters read two temporary registers (A B) use later clockcycles. lower 16 bits IR also sign extended stored thetemporary register Imm, use next cycle. Decoding done parallel reading registers, possible fields fixed location RISC V instruction format.Because immediate portion load ALU immediate located identical place every RISC V instruction, sign-extended immediate also calculated cycle case needed next cycle. Forstores, separate sign-extension needed, immediate field splitin two pieces. 3.Execution/effective address cycle (EX): ALU operates operands prepared prior cycle, performing one four functions depending RISC V instruction type: ■Memory reference: ALUOutput A+I ; Operation —The ALU adds operands form effective address places result register ALUOutput.C.3 Pipelining Implemented? ■C-27■Register-register ALU instruction: ALUOutput Afunc B; Operation —The ALU performs operation specified function code (a combination func3 func7 fields) value register Aand value register B. result placed temporary registerALUOutput. ■Register-Immediate ALU instruction: ALUOutput AopImm; Operation —The ALU performs operation specified opcode value register value register Imm. result placed inthe temporary register ALUOutput. ■Branch: ALUOutput NPC + (Imm <<2); Cond (A==B) Operation —The ALU adds NPC sign-extended immediate value Imm, shifted left 2 bits create word offset, compute theaddress branch target. Register A, read prior cycle,is checked determine whether branch taken, comparison Reg-ister B, consider branch equal. load-store architecture RISC V means effective address execution cycles combined single clock cycle, instruc- tion needs simultaneously calculate data address, calculate instruction target address, perform operation data. integer instruc-tions included herein jumps various forms, similar tobranches. 4.Memory access/branch completion cycle (MEM): PC updated instructions: PC NPC; ■Memory reference: LMD Mem[ALUOutput] Mem[ALUOutput] B; Operation —Access memory needed. instruction load, data return memory placed LMD (load memory data) register; store, data B register written memory. either case,the address used one computed prior cycle stored theregister ALUOutput.C-28 ■Appendix C Pipelining: Basic Intermediate Concepts■Branch: (cond) PC ALUOutput Operation —If instruction branches, PC replaced branch des- tination address register ALUOutput. 5.Write-back cycle (WB): ■Register-register Register-immediate ALU instruction: Regs[rd] ALUOutput; ■Load instruction: Regs[rd] LMD; Operation —Write result register file, whether comes memory system (which LMD) ALU (which ALUOutput)with rd designating register. Figure C.18 shows instruction flows data path. end clock cycle, every value computed clock cycle required ona later clock cycle (whether instruction next) written storagedevice, may memory, general-purpose register, PC, temporaryregister (i.e., LMD, Imm, A, B, IR, NPC, ALUOutput, Cond). temporaryregisters hold values clock cycles one instruction, stor-age elements visible parts state hold values successive instructions. Although processors today pipelined, multicycle implementation reasonable approximation processors would implemented inearlier times. simple finite-state machine could used implement controlfollowing five-cycle structure shown herein. much complex proces-sor, microcode control could used. either event, instruction sequence like theone described section would determine structure control. hardware redundancies could eliminated multi- cycle implementation. example, two ALUs: one increment PC one used effective address ALU computation. notneeded clock cycle, could merge adding additional mul-tiplexers sharing ALU. Likewise, instructions data could bestored memory, data instruction accesses happenon different clock cycles. Rather optimize simple implementation, leave design Figure C.18 , provides us better base pipelined implementation.C.3 Pipelining Implemented? ■C-29A Basic Pipeline RISC V before, pipeline data path Figure C.18 almost changes starting new instruction clock cycle. every pipe stage activeon every clock cycle, operations pipe stage must complete 1 clock cycleand combination operations must able occur once. Furthermore,pipelining data path requires values passed one pipe stage nextmust placed registers. Figure C.19 shows RISC V pipeline appro- priate registers, called pipeline registers orpipeline latches , pipeline stage. registers labeled names stages connect.Instruction fetchInstruction decode/ register fetchExecute/ address calculationMemory accessWrite- back BPC4 ALU 12 32Add Data memoryRegisters Sign- extendInstruction memoryM u x u xM u x u x=?Branch takenCondNPC lmmALU outputIRA LMD Figure C.18 implementation RISC V data path allows every instruction executed 4 5 clock cycles. Although PC shown portion data path used instruction fetch registers shown portion data path used instruction decode/register fetch, functional units read well written instruction. Although show functional units cycle corresponding read, PC written memory access clock cycle registers written thewrite-back clock cycle. cases, writes later pipe stages indicated multiplexer output (in mem- ory access write-back), carries value back PC registers. backward-flowing signals introduce much complexity pipelining, indicate possibility hazards.C-30 ■Appendix C Pipelining: Basic Intermediate ConceptsFigure C.19 drawn connections pipeline registers one stage another clear. registers needed hold values temporarily clock cycles within one instruction subsumed pipeline registers. fields ofthe instruction register (IR), part IF/ID register, labeled whenthey used supply register names. pipeline registers carry data andcontrol one pipeline stage next. value needed later pipeline stage must placed register copied one pipeline register next, longer needed. tried use temporary registerswe earlier unpipelined data path, values could overwritten alluses completed. example, field register operand used writeon load ALU operation supplied MEM/WB pipeline register ratherthan IF/ID register. want load ALU operation towrite register designated operation, register field theData memoryALU Sign- extendPC Instruction memoryADDIF/ID 4ID/EX EX/MEM MEM/WB IR19..15 MEM/WB.IRM u x u xM u xIR24..20 RegistersBranch taken IR 12 32M u x=? Figure C.19 data path pipelined adding set registers, one pair pipe stages. registers serve convey values control information one stage next. also think PC pipeline register, sits stage pipeline, leading one pipeline register pipe stage.Recall PC edge-triggered register written end clock cycle; hence, race conditionin writing PC. selection multiplexer PC moved PC written exactly one stage (IF). ’t move it, would conflict branch occurred, two instructions would try write different values PC. data paths flow left right, earlier time later.The paths flowing right left (which carry register write-back information PC information branch) introduce complications pipeline.C.3 Pipelining Implemented? ■C-31instruction currently transitioning ID! destination register field simply copied one pipeline register next, needed WB stage. instruction active exactly one stage pipeline time; therefore, actions taken behalf instruction occur pair pipeline reg-isters. Thus, also look activities pipeline examining hasto happen pipeline stage depending instruction type. Figure C.20 shows view. Fields pipeline registers named show flow Stage instruction IF/ID.IR Mem[PC] IF/ID.NPC,PC (if ((EX/MEM.opcode == branch) & EX/MEM.cond){EX/MEM. ALUOutput} else {PC+4}); ID ID/EX.A Regs[IF/ID.IR[rs1]]; ID/EX.B Regs[IF/ID.IR[rs2]]; ID/EX.NPC IF/ID.NPC; ID/EX.IR IF/ID.IR; ID/EX.Imm sign-extend(IF/ID.IR[immediate field]); ALU instruction Load instruction Branch instruction EX EX/MEM.IR ID/EX.IR; EX/MEM.ALUOutput ID/EX.A func ID/EX.B; EX/MEM.ALUOutput ID/EX.A opID/EX.Imm;EX/MEM.IR ID/EX.IR EX/MEM.ALUOutput ID/EX.A+ID/EX.Imm; EX/MEM.B ID/EX.B;EX/MEM.ALUOutput ID/EX.NPC +(ID/EX.Imm <<2); EX/MEM.cond (ID/EX.A == ID/EX.B); MEM MEM/WB.IR EX/MEM.IR; MEM/WB.ALUOutput EX/MEM.ALUOutput;MEM/WB.IR EX/MEM.IR; MEM/WB.LMD Mem[EX/MEM.ALUOutput];or Mem[EX/MEM.ALUOutput] EX/MEM.B; WB Regs[MEM/WB.IR[rd]] MEM/WB.ALUOutput;For load only: Regs[MEM/WB.IR[rd]] MEM/WB.LMD; Figure C.20 Events every pipe stage RISC V pipeline. Let’s review actions stages specific pipeline organization. IF, addition fetching instruction computing new PC, store theincremented PC PC pipeline register (NPC) later use computing branch-target address. structure organization Figure C.19 , PC updated one two sources. ID, fetch registers, extend sign 12 bits IR (the immediate field), pass alongthe IR NPC. EX, perform ALU operation address calculation; pass along IR Bregister (if instruction store). also set value cond 1 instruction taken branch. MEM phase, cycle memory, write PC needed, pass along values needed final pipe stage. Finally, WB, update register field either ALU output loaded value. simplicity wealways pass entire IR one stage next, although instruction proceeds pipeline, less less IR needed.C-32 ■Appendix C Pipelining: Basic Intermediate Conceptsof data one stage next. Notice actions first two stages independent current instruction type; must independent instruction decoded end ID stage. activity depends onwhether instruction EX/MEM taken branch. so, branch-targetaddress branch instruction EX/MEM written PC end IF;otherwise, incremented PC written back. (As said earlier, effectof branches leads complications pipeline deal next fewsections.) fixed-position encoding register source operands critical toallowing registers fetched ID. control simple pipeline need determine set con- trol four multiplexers data path Figure C.19 . two multi- plexers ALU stage set depending instruction type, isdictated IR field ID/EX register. top ALU input multiplexeris set whether instruction branch not, bottom multiplexer isset whether instruction register-register ALU operation othertype operation. multiplexer stage chooses whether use thevalue incremented PC value EX/MEM.ALUOutput (the branch target) write PC. multiplexer controlled field EX/MEM.cond. fourth multiplexer controlled whether instructionin WB stage load ALU operation. addition four mul-tiplexers, one additional multiplexer needed drawn inFigure C.19 , whose existence clear looking WB stage ALU operation. destination register field one two different placesdepending instruction type (register-register ALU versus either ALUimmediate load). Thus, need multiplexer choose correct por- tion IR MEM/WB register specify register destination field, assuming instruction writes register. Implementing Control RISC V Pipeline process letting instruction move instruction decode stage (ID)into execution stage (EX) pipeline usually called instruction issue ;a n instruction made step said issued . RISC V integer pipeline, data hazards checked ID phase pipeline. Ifa data hazard exists, instruction stalled issued. Likewise, candetermine forwarding needed ID set appropriate con- trols then. Detecting interlocks early pipeline reduces hardware complex- ity hardware never suspend instruction updated thestate processor, unless entire processor stalled. Alternatively, candetect hazard forwarding beginning clock cycle uses oper-and (EX MEM pipeline). show differences twoapproaches, show interlock read write (RAW) hazardwith source coming load instruction (called load interlock ) implemented check ID, implementation forwarding paths toC.3 Pipelining Implemented? ■C-33the ALU inputs done EX. Figure C.21 lists variety circum- stances must handle. Let’s start implementing load interlock. RAW hazard source instruction load, load instruction EX stagewhen instruction needs load data ID stage. Thus, wecan describe possible hazard situations small table, bedirectly translated implementation. Figure C.22 shows table detects load interlocks instruction using load result ID stage. hazard detected, control unit must insert pipeline stall prevent instructions ID stages advancing. said earlier, control information carried pipeline registers. (Carryingthe instruction along enough, control derived it.) Thus, whenwe detect hazard need change control portion ID/EX pipelineregister 0s, happens no-op (an instruction nothing,such add x0,x0,x0 ). addition, simply recirculate contents IF/ID registers hold stalled instruction. pipeline complex haz-ards, ideas would apply: detect hazard comparing set pipeline registers shift no-ops prevent erroneous execution.SituationExample code sequence Action dependence ld x1,45(x2) add x5,x6,x7sub x8,x6,x7 x9,x6,x7No hazard possible dependence exists x1 immediately followingthree instructions Dependence requiring stallld x1,45(x2) add x5,x1,x7 sub x8,x6,x7or x9,x6,x7Comparators detect use x1 add stall add (andsub andor) theadd begins EX Dependence overcome forwardingld x1,45(x2) add x5,x6,x7 sub x8,x1,x7or x9,x6,x7Comparators detect use x1 sub forward result load ALU time sub begin EX Dependence accesses orderld x1,45(x2) add x5,x6,x7 sub x8,x6,x7 x9,x1,x7No action required read x1 oroccurs second half ID phase, write loaded data occurred first half Figure C.21 Situations pipeline hazard detection hardware see com- paring destination sources adjacent instructions. table indicates comparison needed destination sources twoinstructions following instruction wrote destination. case stall,the pipeline dependences look like third case execution continues (depen- dence overcome forwarding). course, hazards involve x0 ignored register always contains 0, preceding test could extended todo this.C-34 ■Appendix C Pipelining: Basic Intermediate ConceptsImplementing forwarding logic similar, although cases consider. key observation needed implement forwarding logic thatthe pipeline registers contain data forwarded well sourceand destination register fields. forwarding logically happens ALU data memory output ALU input, data memory input, zero detec- tion unit. Thus, implement forwarding comparison desti-nation registers IR contained EX/MEM MEM/WB stages againstthe source registers IR contained ID/EX EX/MEM registers.Figure C.23 shows comparisons possible forwarding operations destination forwarded result ALU input instruction cur-rently EX. addition comparators combinational logic must determine forwarding path needs enabled, also must enlarge multiplexers ALU inputs add connections pipeline registers usedto forward results. Figure C.24 shows relevant segments pipelined data path additional multiplexers connections place. RISC V, hazard detection forwarding hardware reasonably sim- ple; see things become somewhat complicated extendthis pipeline deal floating point. that, need handlebranches. Dealing Branches Pipeline RISC V, conditional branches depend comparing two register values, whichwe assume occurs EX cycle, uses ALU function. willneed also compute branch target address. testing branch condi-tion determining next PC determine branch penalty is, wewould like compute possible PCs choose correct PC beforethe end EX cycle. adding separate adder computesthe branch target address ID. instruction yet decoded, computing possible target every instruction branch. isOpcode field ID/EX (ID/EX.IR 0..5)Opcode field IF/ID (IF/ID.IR 0..6) Matching operand fields Load Register-register ALU, load, store,ALU immediate, branchID/EX.IR[rd]¼¼IF/ ID.IR[rs1] Load Register-register ALU, branchID/EX.IR[rd]¼¼IF/ ID.IR[rs2] Figure C.22 logic detect need load interlocks ID stage instruction requires two comparisons, one possible source. Remember IF/ID register holds state instruction ID, potentially uses load result, ID/EX holds state instruction EX, load instruction.C.3 Pipelining Implemented? ■C-35likely faster computing target evaluating condition EX, use slightly energy. Figure C.25 shows pipelined data path assuming adder ID eval- uation branch condition EX, minor change pipeline structure. pipeline incur two-cycle penalty branches. early RISC proces-sors, MIPS, condition test branches restricted allow test tooccur ID, reducing branch delay one cycle. course, meant anALU operation register followed conditional branch based registerincurred data hazard, occur branch condition evaluatedin EX. pipeline depths increased, branch delay increased, made dynamic branch prediction necessary. example, processor separate decode andPipeline register ofsourceinstructionOpcode sourceinstructionPipeline register ofdestinationinstructionOpcode destinationinstructionDestination theforwardedresultComparison (if equal thenforward) EX/MEM Register- register ALU, ALU immediateID/EX Register-register ALU, ALU immediate, load, store,branchTop ALU inputEX/MEM.IR[rd] == ID/EX.IR[rs1] EX/MEM Register- register ALU, ALU immediateID/EX Register-register ALU Bottom ALU inputEX/MEM.IR[rd] == ID/EX.IR[rs2] MEM/WB Register- register ALU,ALUimmediate, LoadID/EX Register-register ALU, ALUimmediate, load, store,branchTop ALU inputMEM/WB.IR[rd] == ID/EX.IR[rs1] MEM/WB Register- register ALU,ALUimmediate, LoadID/EX Register-register ALU Bottom ALU inputMEM/WB.IR[rd] == ID/EX.IR[rs2] Figure C.23 Forwarding data two ALU inputs (for instruction EX) occur ALU result (in EX/MEM MEM/WB) load result MEM/WB. 10 separate comparisons needed tell whether forwarding operation occur. top bottom ALU inputs refer inputs correspondingto first second ALU source operands, respectively, shown explicitly Figure C.18 page C.30 Figure C.24 page C.36. Remember pipeline latch destination instruction EX ID/EX, source values come ALUOutput portion EX/MEM MEM/WB LMD portion MEM/WB. isone complication addressed logic: dealing multiple instructions write register. example, code sequence add x1, x2, x3; addi x1, x1, 2; sub x4, x3, x1 , logic must ensure sub instruction uses result addi instruction rather result add instruction. logic shown extended handle case simply testing forwarding MEM/WB enabled onlywhen forwarding EX/MEM enabled input. addi result EX/MEM, forwarded, rather add result MEM/WB.C-36 ■Appendix C Pipelining: Basic Intermediate Conceptsregister fetch stages probably branch delay least 1 clock cycle longer. branch delay, unless dealt with, turns branch penalty. Manyolder processors implement complex instruction sets branch delaysof 4 clock cycles more, large, deeply pipelined processors often branchpenalties 6 7. Aggressive high-end superscalars, Intel i7 discussedinChapter 3 , may branch penalties 10 –15 cycles! general, deeper pipeline, worse branch penalty clock cycles, critical thatbranches accurately predicted. C.4 Makes Pipelining Hard Implement? understand detect resolve hazards, deal somecomplications avoided far. first part section considersthe challenges exceptional situations instruction execution order ischanged unexpected ways. second part section, discuss challenges raised different instruction sets.Data memoryALU=?ID/EX EX/MEM MEM/WB u xM u x Figure C.24 Forwarding results ALU requires addition three extra inputs ALU multiplexer addition three paths new inputs.The paths correspond bypass of: (1) ALU output end EX, (2) ALU output end MEM stage, (3) memory output end MEM stage.C.4 Makes Pipelining Hard Implement? ■C-37Dealing Exceptions Exceptional situations harder handle pipelined processor over- lapping instructions makes difficult know whether instruction cansafely change state processor. pipelined processor, instruction isexecuted piece piece completed several clock cycles. Unfortunately,other instructions pipeline raise exceptions may force processor toabort instructions pipeline complete. discuss theseproblems solutions detail, need understand types situations arise architectural requirements exist supporting them. Types Exceptions Requirements terminology used describe exceptional situations normal execu- tion order instruction changed varies among processors. terms interrupt, fault, exception used, although consistent fashion. use term exception cover mechanisms, including following: ■I/O device request ■Invoking operating system service user programData memoryALU Sign- extend16 32PC Instruction memoryADDADD IF/ID 4EX/MEM MEM/WB IR6..10 MEM/WB.IRIR11..15 Registers=? u xM u x u xIRID/EX Figure C.25 minimize impact deciding whether conditional branch taken, compute branch target address ID conditional test final selection next PC EX. mentioned Figure C.19 , PC thought pipeline register (e.g., part ID/IF), written address next instruction end cycle.C-38 ■Appendix C Pipelining: Basic Intermediate Concepts■Tracing instruction execution ■Breakpoint (programmer-requested interrupt) ■Integer arithmetic overflow ■FP arithmetic anomaly ■Page fault (not main memory) ■Misaligned memory accesses (if alignment required) ■Memory protection violation ■Using undefined unimplemented instruction ■Hardware malfunctions ■Power failure wish refer particular class exceptions, use longer name, I/O interrupt, floating-point exception, page fault. Although use term exception cover events, individual events important characteristics determine action needed hardware. requirements exceptions characterized five semi-independent axes: 1.Synchronous versus asynchronous —If event occurs place every time program executed data memory allocation, theevent synchronous . exception hardware malfunctions, asynchro- nous events caused devices external processor memory. Asyn- chronous events usually handled completion current instruction, makes easier handle. 2.User requested versus coerced —If user task directly asks it, user- requested event. sense, user-requested exceptions really excep- tions, predictable. treated exceptions, however,because mechanisms used save restore state usedfor user-requested events. function instruction thattriggers exception cause exception, user-requested exceptions canalways handled instruction completed. Coerced exceptions caused hardware event control user program. Coerced exceptions harder implement predictable. 3.User maskable versus user nonmaskable —If event masked dis- abled user task, user maskable . mask simply controls whether hardware responds exception not. 4.Within versus instructions —This classification depends whether event prevents instruction completion occurring middle execu- tion—no matter short —or whether recognized instructions. Exceptions occur within instructions usually synchronous, instruction triggers exception. ’s harder implement exceptions thatC.4 Makes Pipelining Hard Implement? ■C-39occur within instructions instructions, instruc- tion must stopped restarted. Asynchronous exceptions occur within instructions arise catastrophic situations (e.g., hardware malfunction) andalways cause program termination. 5.Resume versus terminate —If program ’s execution always stops interrupt, terminating event. program ’s execution continues interrupt, resuming event. easier implement exceptions terminate execution, processor need able restart executionof program handling exception. Figure C.26 classifies preceding examples according five categories. difficult task implementing interrupts occurring within instructions instruction must resumed. Implementing exceptions requires another program must invoked save state executing program, cor-rect cause exception, restore state program Exception typeSynchronous vs. asynchronousUser request vs. coercedUser maskable vs.nonmaskableWithin vs. betweeninstructionsResume vs. terminate I/O device request Asynchronous Coerced Nonmaskable Resume Invoke operating system Synchronous User request Nonmaskable ResumeTracing instruction executionSynchronous User request User maskable Resume Breakpoint Synchronous User request User maskable Resume Integer arithmetic overflowSynchronous Coerced User maskable Within Resume Floating-point arithmetic overflow underflowSynchronous Coerced User maskable Within Resume Page fault Synchronous Coerced Nonmaskable Within Resume Misaligned memory accessesSynchronous Coerced User maskable Within Resume Memory protection violationsSynchronous Coerced Nonmaskable Within Resume Using undefined instructionsSynchronous Coerced Nonmaskable Within Terminate Hardware malfunctions Asynchronous Coerced Nonmaskable Within Terminate Power failure Asynchronous Coerced Nonmaskable Within Terminate Figure C.26 Five categories used define actions needed different exception types. Excep- tions must allow resumption marked resume, although software may often choose terminate theprogram. Synchronous, coerced exceptions occurring within instructions resumed difficultto implement. might expect memory protection access violations would always result termination; how- ever, modern operating systems use memory protection detect events first attempt use page first write page. Thus, processors able resume exceptions.C-40 ■Appendix C Pipelining: Basic Intermediate Conceptsinstruction caused exception tried again. process must effec- tively invisible executing program. pipeline provides ability processor handle exception, save state, restart without affecting theexecution program, pipeline processor said restartable . early supercomputers microprocessors often lacked property, almost allprocessors today support it, least integer pipeline, neededto implement virtual memory (see Chapter 2 ). Stopping Restarting Execution unpipelined implementations, difficult exceptions two prop- erties: (1) occur within instructions (that is, middle instructionexecution corresponding EX MEM pipe stages), (2) must restart-able. RISC V pipeline, example, virtual memory page fault resultingfrom data fetch cannot occur sometime MEM stage instruction. time fault seen, several instructions execution. page fault must restartable requires intervention another process, asthe operating system. Thus, pipeline must safely shut statesaved instruction restarted correct state. Restarting usu-ally implemented saving PC instruction restart. therestarted instruction branch, continue fetch sequentialsuccessors begin execution normal fashion. restarted instruc-tion branch, reevaluate branch condition begin fetching either target fall-through. exception occurs, pipeline control take following steps save pipeline state safely: 1.Force trap instruction pipeline next IF. 2.Until trap taken, turn writes faulting instruction instructions follow pipeline; done placing zeros thepipeline latches instructions pipeline, starting instructionthat generates exception, precede instruction. Thisprevents state changes instructions completed beforethe exception handled. 3.After exception-handling routine operating system receives control, immediately saves PC faulting instruction. value used toreturn exception later. exception handled, special instructions return processor exception reloading PCs restarting instruction stream (using theexception return RISC V). pipeline stopped instructionsjust faulting instruction completed restartedfrom scratch, pipeline said precise exceptions . Ideally, faulting instruction would changed state, correctly handling excep-tions requires faulting instruction effects. exceptions,C.4 Makes Pipelining Hard Implement? ■C-41such floating-point exceptions, faulting instruction processors writes result exception handled. cases, hardware must prepared retrieve source operands, even destination identicalto one source operands. floating-point operations may run manycycles, highly likely instruction may written sourceoperands (as see next section, floating-point operations often com-plete order). overcome this, many recent high-performance processorshave introduced two modes operation. One mode precise exceptions andthe (fast performance mode) not. course, precise exception mode slower, since allows less overlap among floating-point instructions. Supporting precise exceptions requirement many systems, others “just”valuable simplifies operating system interface. minimum, processor demand paging IEEE arithmetic trap han-dlers must make exceptions precise, either hardware soft-ware support. integer pipelines, task creating precise exceptions easier,and accommodating virtual memory strongly motivates support preciseexceptions memory references. practice, reasons led designers architects always provide precise exceptions integer pipeline. section describe implement precise exceptions RISC V integerpipeline. describe techniques handling complex challengesarising floating-point pipeline Section C.5 . Exceptions RISC V Figure C.27 shows RISC V pipeline stages problem exceptions might occur stage. pipelining, multiple exceptions may occur clock cycle multiple instructions execution. Forexample, consider instruction sequence: ld ID EX MEM WB add ID EX MEM WB Pipeline stage Problem exceptions occurring Page fault instruction fetch; misaligned memory access; memory protection violation ID Undefined illegal opcodeEX Arithmetic exceptionMEM Page fault data fetch; misaligned memory access; memory protection violation WB None Figure C.27 Exceptions may occur RISC V pipeline. Exceptions raised instruction data memory access account six eight cases.C-42 ■Appendix C Pipelining: Basic Intermediate ConceptsThis pair instructions cause data page fault arithmetic exception time, ldis MEM stage add EX stage. case handled dealing data page fault thenrestarting execution. second exception reoccur (but first, ifthe software correct), second exception occurs handledindependently. reality, situation straightforward simple example. Excep- tions may occur order; is, instruction may cause exception beforean earlier instruction causes one. Consider preceding sequence instruc- tions, ldfollowed add. Theldcan get data page fault, seen instruc- tion MEM, add get instruction page fault, seen add instruction IF. instruction page fault actually occur first, even thoughit caused later instruction! implementing precise exceptions, pipeline required handle exception caused ldinstruction first. explain works, let’s call instruction position ldinstruction i, instruction position add instruction i+1. pipeline cannot simply handle exception occurs time, lead exceptions occurring unpipelined order. Instead, hardware posts exceptions causedby given instruction status vector associated instruction. excep-tion status vector carried along instruction goes pipeline. anexception indication set exception status vector, control signal maycause data value written turned (this includes register writes andmemory writes). store cause exception MEM, hardwaremust prepared prevent store completing raises exception. instruction enters WB (or leave MEM), exception sta- tus vector checked. exceptions posted, handled order inwhich would occur time unpipelined processor —the exception cor- responding earliest instruction (and usually earliest pipe stage thatinstruction) handled first. guarantees exceptions seen oninstruction ibefore seen i+1. course, action taken earlier pipe stages behalf instruction imay invalid, writes register file memory disabled, state could changed. see inSection C.5 , maintaining precise model FP operations much harder. next subsection describe problems arise implementing excep- tions pipelines processors powerful, longer-running instructions. Instruction Set Complications RISC V instruction one result, RISC V pipeline writes thatresult end instruction ’s execution. instruction guaran- teed complete, called committed . RISC V integer pipeline, instruc- tions committed reach end MEM stage (or beginning ofWB) instruction updates state stage. Thus, precise exceptionsC.4 Makes Pipelining Hard Implement? ■C-43are straightforward. processors instructions change state middle instruction execution, instruction predecessors guaranteed complete. example, autoincrement addressing modes IA-32architecture cause update registers middle instruction execution. Insuch case, instruction aborted exception, leave theprocessor state altered. Although know instruction caused exception,without additional hardware support exception imprecise theinstruction half finished. Restarting instruction stream animprecise exception difficult. Alternatively, could avoid updating state instruction commits, may difficult costly, may dependences updated state: consider VAX instruction autoin-crements register multiple times. Thus, maintain precise exceptionmodel, processors instructions ability back statechanges made instruction committed. exception occurs, pro-cessor uses ability reset state processor value inter-rupted instruction started. next section, see powerful RISCV floating-point pipeline introduce similar problems, Section C.7 introduces techniques substantially complicate exception handling. related source difficulties arises instructions update memory state execution, string copy operations Intel architectureor IBM 360 (see Appendix K). make possible interrupt restart theseinstructions, instructions defined use general-purpose registers asworking registers. Thus, state partially completed instruction alwaysin registers, saved exception restored exception,allowing instruction continue. different set difficulties arises odd bits state may create addi- tional pipeline hazards may require extra hardware save restore. Conditioncodes good example this. Many processors set condition codes implic-itly part instruction. approach advantages, conditioncodes decouple evaluation condition actual branch. However,implicitly set condition codes cause difficulties scheduling pipelinedelays setting condition code branch, instructionsset condition code cannot used delay slots condition evaluation branch. Additionally, processors condition codes, processor must decide branch condition fixed. involves finding conditioncode set last time branch. processors withimplicitly set condition codes, done delaying branch condition eval-uation previous instructions chance set condition code. course, architectures explicitly set condition codes allow delay condition test branch scheduled; however, pipeline control must still track last instruction sets condition code know branch condition decided. effect, condition code must treated anoperand requires hazard detection RAW hazards branches, asRISC V must registers.C-44 ■Appendix C Pipelining: Basic Intermediate ConceptsA final thorny area pipelining multicycle operations. Imagine trying pipeline sequence x86 instructions this: mov BX, AX ; moves registers add 42(BX+SI),BX;adds memory contents register ; memory location sub BX,AX ;subtracts registers rep movsb ;moves character string ; length given register CX Although none instructions particularly long (an x86 instruction 15 bytes), differ radically number clock cycles theywill require, low one hundreds clock cycles. instructionsalso require different numbers data memory accesses, zero possiblyhundreds. data hazards complex occur withininstructions (nothing prevents movsb overlapping source anddestination!). simple solution making instructions execute thesame number clock cycles unacceptable introduces enormous number hazards bypass conditions makes immensely long pipeline. Pipelining x86 instruction level difficult, clever solutionwas found, similar one used VAX. pipeline microinstruction execution; microinstruction simple instruction used sequences imple-ment complex instruction set. microinstructions simple(they look lot like RISC V), pipeline control much easier. Since 1995,all Intel IA-32 microprocessors used strategy converting IA-32instructions microoperations, pipelining microoperations. fact, approach even used complex instructions ARM architecture. comparison, load-store processors simple operations similar amounts work pipeline easily. architects realize relationshipbetween instruction set design pipelining, design architectures formore efficient pipelining. next section, see RISC V pipelinedeals long-running instructions, specifically floating-point operations. many years, interaction instruction sets implementations believed small, implementation issues major focus designing instruction sets. 1980s, became clear difficulty inef-ficiency pipelining could increased instruction set complications. Inthe 1990s, companies moved simpler instructions sets goal reduc-ing complexity aggressive implementations. C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations want explore RISC V pipeline extended handle floating-point operations. section concentrates basic approach andC.5 Extending RISC V Integer Pipeline Handle Multicycle Operations ■C-45the design alternatives, closing performance measurements RISC V floating-point pipeline. impractical require RISC V FP operations complete 1 clock cycle, even 2. would mean accepting slow clock using enor-mous amounts logic FP units, both. Instead, FP pipeline allowfor longer latency operations. easier grasp imagine FPinstructions pipeline integer instructions, two impor-tant changes. First, EX cycle may repeated many times needed com-plete operation —the number repetitions vary different operations. Second, may multiple FP functional units. stall occur instruc- tion issued cause either structural hazard functional unit usesor data hazard. section, let ’s assume four separate functional units RISC V implementation: 1.The main integer unit handles loads stores, integer ALU operations, branches 2.FP integer multiplier 3.FP adder handles FP add, subtract, conversion 4.FP integer divider also assume execution stages functional units pipe- lined, Figure C.28 shows resulting pipeline structure. EX pipelined, instruction using functional unit may issue previ- ous instruction leaves EX. Moreover, instruction cannot proceed EX stage, entire pipeline behind instruction stalled. reality, intermediate results probably cycled around EX unit asFigure C.28 suggests; instead, EX pipeline stage number clock delays larger 1. generalize structure FP pipeline shown inFigure C.28 allow pipelining stages multiple ongoing operations. describe pipeline, must define latency functional unitsand also initiation interval orrepeat interval . define latency way defined earlier: number intervening cycles instruction produces result instruction uses result. initiation repeatinterval number cycles must elapse issuing two operationsof given type. example, use latencies initiation intervalsshown Figure C.29 . definition latency, integer ALU operations latency 0, results used next clock cycle, loads latencyof 1, results used one intervening cycle. operations consume operands beginning EX, latency usually number stages EX instruction produces result —for example, zero stages ALU operations one stage loads. primary exception isstores, consume value stored one cycle later. Hence, latency toC-46 ■Appendix C Pipelining: Basic Intermediate Conceptsa store value stored, base address register, one cycle less. Pipeline latency essentially equal one cycle less depth ofthe execution pipeline, number stages EX stage stagethat produces result. Thus, preceding example pipeline, numberof stages FP add four, number stages FP multiply seven. achieve higher clock rate, designers need put fewer logic levelsEX FP/integer multiplyEXInteger unit EX FP adder EX FP/integer dividerBW MEM DI FI Figure C.28 RISC V pipeline three additional unpipelined, floating-point, functional units. one instruction issues every clock cycle, instruc- tions go standard pipeline integer operations. FP operations simplyloop reach EX stage. finished EX stage, proceed MEM WB complete execution. Functional unit Latency Initiation interval Integer ALU 0 1 Data memory (integer FP loads) 1 1FP add 3 1FP multiply (also integer multiply) 6 1FP divide (also integer divide) 24 25 Figure C.29 Latencies initiation intervals functional units.C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations ■C-47in pipe stage, makes number pipe stages required complex operations larger. penalty faster clock rate thus longer latency operations. example pipeline structure Figure C.29 allows four outstanding FP adds, seven outstanding FP/integer multiplies, one FP divide. Figure C.30 shows pipeline drawn extending Figure C.28 . repeat inter- val implemented Figure C.30 adding additional pipeline stages, separated additional pipeline registers. units independent, wename stages differently. pipeline stages take multiple clock cycles, divide unit, subdivided show latency stages. complete stages, one operation may active. pipe-line structure also shown using familiar diagrams earlier theappendix, Figure C.31 shows set independent FP operations FP loads stores. Naturally, longer latency FP operations increases thefrequency RAW hazards resultant stalls, see later section. structure pipeline Figure C.30 requires introduction additional pipeline registers (e.g.,A1/A2, A2/A3, A3/A4) modification connections registers. ID/EX register must expanded EX M1FP/integer multiplyInteger unit FP adder FP/integer dividerIF ID MEM WBM2 M3 M4 M5 M6 A1 A2 A3 A4M7 DIV Figure C.30 pipeline supports multiple outstanding FP operations. FP multiplier adder fully pipelined depth seven four stages, respectively. FP divider pipelined, requires 24 clock cycles complete. latency instructions issue FP operation use result operation without incurring RAW stall determined number cycles spent execution stages.For example, fourth instruction FP add use result FP add. integer ALU operations, thedepth execution pipeline always one next instruction use results.C-48 ■Appendix C Pipelining: Basic Intermediate Conceptsconnect ID EX, DIV, M1, A1; refer portion register associated one next stages notation ID/EX, ID/DIV, ID/M1, ID/A1. pipeline register ID stages may bethought logically separate registers may, fact, implemented sep-arate registers. one operation pipe stage time, control information associated register head stage. Hazards Forwarding Longer Latency Pipelines number different aspects hazard detection forwarding pipeline like shown Figure C.30 . 1.Because divide unit fully pipelined, structural hazards occur. need detected issuing instructions need stalled. 2.Because instructions varying running times, number register writes required cycle larger 1. 3.Write write (WAW) hazards possible, instructions longer reach WB order. Note write read (WAR) hazards possible,because register reads always occur ID. 4.Instructions complete different order issued, causing problems exceptions; deal next subsection. 5.Because longer latency operations, stalls RAW hazards frequent. increase stalls arising longer operation latencies fundamentally integer pipeline. describing new problems arisein FP pipeline looking solutions, let ’s examine potential impact RAW hazards. Figure C.32 shows typical FP code sequence resultant stalls. end section, ’ll examine performance FP pipeline SPEC subset. look problems arising writes, described (2) (3) earlier list. assume FP register file one write port, sequences FP operations, well FP load together FP operations, cause conflictsfmul.d ID M1 M2 M3 M4 M5 M6 M7 MEM WB fadd.d ID A1 A2 A3 A4 MEM WB fadd.d ID EX MEM WB fsd ID EX MEM WB Figure C.31 pipeline timing set independent FP operations. stages italics show data needed, stages bold show result available. FP loads stores use 64-bit path memory pipelining timing like integer load store.C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations ■C-49for register write port. Consider pipeline sequence shown Figure C.33 .I n clock cycle 11, three instructions reach WB want write registerfile. single register file write port, processor must serialize theinstruction completion. single register port represents structural hazard.We could increase number write ports solve this, solution maybe unattractive additional write ports would used rarely. maximum steady-state number write ports needed 1. Instead, choose detect enforce access write port structural hazard. two different ways implement interlock. first track use write port ID stage stall instruction issues, aswe would structural hazard. Tracking use write port bedone shift register indicates already-issued instructions usethe register file. instruction ID needs use register file timeClock cycle number Instruction 1 2 3 4 5 67891 0 1 1 1 21 3 1 4 1 5 1 61 7 fld f4,0(x2) ID EX MEM WB fmul.d f0,f4,f6 ID Stall M1 M2 M3 M4 M5 M6 M7 MEM WB fadd.d f2,f0,f8 Stall ID Stall Stall Stall Stall Stall Stall A1 A2 A3 A4 MEM WB fsd f2,0(x2) Stall Stall Stall Stall Stall Stall ID EX Stall Stall Stall MEM Figure C.32 typical FP code sequence showing stalls arising RAW hazards. longer pipeline sub- stantially raises frequency stalls versus shallower integer pipeline. instruction sequence dependent previous proceeds soon data available, assumes pipeline full bypassingand forwarding. fsd must stalled extra cycle MEM conflict fadd.d . Extra hardware could easily handle case. Clock cycle number Instruction 1 2 3 4 5 6 7 8 9 10 11 fmul.d f0,f4,f6 ID M1 M2 M3 M4 M5 M6 M7 MEM WB ... ID EX MEM WB ... ID EX MEM WB fadd.d f2,f4,f6 ID A1 A2 A3 A4 MEM WB ... ID EX MEM WB ... ID EX MEM WB fld f2,0(x2) ID EX MEM WB Figure C.33 Three instructions want perform write-back FP register file simultaneously, shown clock cycle 11. notthe worst case, earlier divide FP unit could also finish clock. Note although fmul.d ,fadd.d , andfld MEM stage clock cycle 10, fld actually uses memory, structural hazard exists MEM.C-50 ■Appendix C Pipelining: Basic Intermediate Conceptsas instruction already issued, instruction ID stalled cycle. clock reservation register shifted 1 bit. implementation advan- tage: maintains property interlock detection stall insertion occursin ID stage. cost addition shift register write conflict logic.We assume scheme throughout section. alternative scheme stall conflicting instruction tries enter either MEM WB stage. wait stall conflicting instructions untilthey want enter MEM WB stage, choose stall either instruction.A simple, though sometimes suboptimal, heuristic give priority unit longest latency, one likely caused another instruction stalled RAW hazard. advantage scheme itdoes require us detect conflict entrance MEM WBstage, easy see. disadvantage complicates pipeline con-trol, stalls arise two places. Notice stalling enteringMEM cause EX, A4, M7 stage occupied, possibly forcing stallto trickle back pipeline. Likewise, stalling WB would cause MEM toback up. problem possibility WAW hazards. see exist, consider example Figure C.33 . fadd.d instruction issued one cycle earlier destination f2, would create WAW hazard, would write f2one cycle earlier fadd.d . Note hazard occurs result fadd.d overwritten without instruction ever using it! use f2 fadd.d fadd.d , pipeline would need stalled RAW hazard, fadd.d would issue fadd.d completed. could argue that, pipeline, WAW hazards occur useless instruction executed, must still detect make sure result fadd.d appears f2when done. (As see Section C.8 , sequences sometimes dooccur reasonable code.) two possible ways handle WAW hazard. first approach delay issue load instruction fadd.d enters MEM. sec- ond approach stamp result fadd.d detecting hazard changing control fadd.d write result. fadd.d issue right away. hazard rare, either scheme work fine —you pick whatever simpler implement. either case, hazard detected ID fadd.d issuing, stalling thefadd.d making fadd.d no-op easy. difficult situation detect fadd.d might finish fadd.d , requires knowing length pipeline current position fadd.d . Luck- ily, code sequence (two writes intervening read) rare, sowe use simple solution: instruction ID wants write reg- ister instruction already issued, issue instruction EX. Section C.7 , see additional hardware eliminate stalls hazards. First, let ’s put together pieces implementing hazard issue logic FP pipeline.C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations ■C-51In detecting possible hazards, must consider hazards among FP instruc- tions, well hazards FP instruction integer instruction. Except FP loads-stores FP-integer register moves, FP integerregisters distinct. integer instructions operate integer registers, whilethe FP operations operate registers. Thus, need considerFP loads-stores FP register moves detecting hazards FP integerinstructions. simplification pipeline control additional advantage ofhaving separate register files integer floating-point data. (The main advan-tages doubling number registers, without making either set larger, increase bandwidth without adding ports either set. main disadvantage, beyond need extra register file, small cost occa-sional moves needed two register sets.) Assuming pipelinedoes hazard detection ID, three checks must performedbefore instruction issue: 1.Check structural hazards —Wait required functional unit busy (this needed divides pipeline) make sure registerwrite port available needed. 2.Check RAW data hazard —Wait source registers listed pending destinations pipeline register available thisinstruction needs result. number checks must made here, dependingon source instruction, determines result avail-able, destination instruction, determines value isneeded. example, instruction ID FP operation source reg-ister f2, f2 cannot listed destination ID/A1, A1/A2, A2/A3, correspond FP add instructions finished instruction ID needs result. (ID/A1 portion output registerof ID sent A1.) Divide somewhat tricky, want allowthe last cycles divide overlapped, need handle thecase divide close finishing special. practice, designers mightignore optimization favor simpler issue test. 3.Check WAW data hazard —Determine instruction A1, …, A4, D, M1,…, M7 register destination instruction. so, stall issue instruction ID. Although hazard detection complex multicycle FP operations, concepts RISC V integer pipeline. true forthe forwarding logic. forwarding implemented checking thedestination register EX/MEM, A4/MEM, M7/MEM, D/MEM, orMEM/WB registers one source registers floating-point instruction.If so, appropriate input multiplexer enabled choose forwarded data. exercises, opportunity specify logic RAW WAW hazard detection well forwarding. Multicycle FP operations also introduce problems exception mecha- nisms, deal next.C-52 ■Appendix C Pipelining: Basic Intermediate ConceptsMaintaining Precise Exceptions Another problem caused long-running instructions illustrated following sequence code: fdiv.d f0,f2,f4 fadd.d f10,f10,f8 fsub.d f12,f12,f14 code sequence looks straightforward; dependences. problem arises, however, instruction issued early may complete instruc- tion issued later. example, expect fadd.d andfsub.d complete thefdiv.d completes. called out-of-order completion com- mon pipelines long-running operations (see Section C.7 ). hazard detection prevent dependence among instructions violated,why out-of-order completion problem? Suppose fsub.d causes floating-point arithmetic exception point fadd.d completed fdiv.d not. result imprecise exception, something trying avoid. may appear could handled letting floating-point pipeline drain, integer pipeline. exception may position possible. example, fdiv.d decided totake floating-point-arithmetic exception add completed, could nothave precise exception hardware level. fact, fadd.d destroysone operands, could restore state fdiv.d,even software help. problem arises instructions completing different order issued. four possible approaches dealing out- of-order completion. first ignore problem settle imprecise exceptions. approach used 1960s early 1970s. still usedin supercomputers thepast fifteen years, certain classes excep-tions allowed handled hardware without stopping pipe-line. difficult use approach processors built today offeatures virtual memory IEEE floating-point standard essen-tially require precise exceptions combination hardware software.As mentioned earlier, recent processors solved problem intro- ducing two modes execution: fast, possibly imprecise mode slower, precise mode. slower precise mode implemented either mode switchor insertion explicit instructions test FP exceptions. either case, theamount overlap reordering permitted FP pipeline significantlyrestricted effectively one FP instruction active time. solu-tion used DEC Alpha 21064 21164, IBM Power1 Power2,and MIPS R8000. second approach buffer results operation operations issued earlier complete. processors actually use solution, becomes expensive difference running times among operationsis large, number results buffer become large. Furthermore,C.5 Extending RISC V Integer Pipeline Handle Multicycle Operations ■C-53results queue must bypassed continue issuing instructions wait- ing longer instruction. requires large number comparators large multiplexer. two viable variations basic approach. first history file , used CYBER 180/990. history file keeps track original values ofregisters. exception occurs state must rolled back earlier thansome instruction completed order, original value register canbe restored history file. similar technique used autoincrement andautodecrement addressing processors VAXes. Another approach, future file , proposed Smith Pleszkun (1988) , keeps newer value register; earlier instructions completed, main register file isupdated future file. exception, main register file precisevalues interrupted state. Chapter 3 , see another approach needed support speculation, method executing instructions knowthe outcome previous branches. third technique use allow exceptions become somewhat impre- cise, keep enough information trap-handling routines create precise sequence exception. means knowing operations pipeline PCs. Then, handling exception, software finishesany instructions precede latest instruction completed, sequence canrestart. Consider following worst-case code sequence: Instruction 1—A long-running instruction eventually interrupts execution. Instruction 2,…, Instruction n/C01—A series instructions completed. Instruction n—An instruction finished. Given PCs instructions pipeline exception return PC, software find state instruction1 instruction n. instruction nhas completed, want restart execution instruction n+1. handling exception, software must simulate execution instruction 1,…, instruction n/C01. return exception restart instruction n+1. complexity executing instructions properly handler major difficulty scheme. important simplification simple RISC V-like pipelines: instruction 2,…, instruction nare integer instructions, know instruction nhas completed instruction 2,…, instruction n/C01have also completed. Thus, FP operations need handled. make schemetractable, number floating-point instructions overlapped inexecution limited. example, overlap two instructions,then interrupting instruction need completed software. restriction may reduce potential throughput FP pipelines deep significant number FP functional units. approach usedin SPARC implementations allow overlap floating-point integeroperations.C-54 ■Appendix C Pipelining: Basic Intermediate ConceptsThe final technique hybrid scheme allows instruction issue con- tinue certain instructions issuing instruction complete without causing exception. guarantees exceptionoccurs, instructions interrupting one completed ofthe instructions interrupting one completed. sometimesmeans stalling processor maintain precise exceptions. make schemework, floating-point functional units must determine exception possibleearly EX stage (in first 3 clock cycles RISC V pipeline), asto prevent instructions completing. scheme used MIPS R2000/3000, R4000, Intel Pentium. discussed Appendix J. Performance Simple RISC V FP Pipeline RISC V FP pipeline Figure C.30 page C.48 generate structural stalls divide unit stalls RAW hazards (it also WAW haz-ards, rarely occurs practice). Figure C.34 shows number stall cycles type floating-point operation per-instance basis (i.e., thefirst bar FP benchmark shows number FP result stalls eachFP add, subtract, convert). might expect, stall cycles per operation track latency FP operations, varying 46% 59% latency functional unit. Figure C.35 gives complete breakdown integer FP stalls five SPECfp benchmarks. four classes stalls shown: FP result stalls, FPcompare stalls, load branch delays, FP structural delays. Branch delaystalls, would small one cycle delay even modest branchpredictor, included. total number stalls per instruction varies from0.65 1.21. C.6 Putting Together: MIPS R4000 Pipeline section, look pipeline structure performance MIPS R4000 processor family, includes 4400. MIPS architecture RISC V similar, differing instructions, including delayedbranch MIPS ISA. R4000 implements MIPS64 uses deeper pipe-line five-stage design integer FP programs. dee-per pipeline allows achieve higher clock rates decomposing five-stageinteger pipeline eight stages. cache access particularly time critical,the extra pipeline stages come decomposing memory access. type ofdeeper pipelining sometimes called superpipelining . Figure C.36 shows eight-stage pipeline structure using abstracted version data path. Figure C.37 shows overlap successive instructions pipeline. Notice that, although instruction data memory occupy multiplecycles, fully pipelined, new instruction start every clock.C.6 Putting Together: MIPS R4000 Pipeline ■C-55In fact, pipeline uses data cache hit detection complete; Chapter 3 discusses done detail. function stage follows: ■IF—First half instruction fetch; PC selection actually happens here, together initiation instruction cache access. ■IS—Second half instruction fetch, complete instruction cache access. ■RF—Instruction decode register fetch, hazard checking, instruction cache hit detection.Number stalls0.0 25.0 5.0 10.0 20.0 15.0FP SPEC benchmarksdoduc ear hydro2d mdljdp su2cor 0.618.61.61.50.70.024.52.91.22.10.00.43.22.52.30.012.42.52.01.62.015.43.71.71.7 Add/subtract/convert Compares Multiply Divide Divide structural Figure C.34 Stalls per FP operation major type FP operation SPEC89 FP benchmarks. Except divide structural hazards, data depend frequency operation, latency number cycles result used. number stalls RAW hazards roughly tracks thelatency FP unit. example, average number stalls per FP add, subtract, convert 1.7 cycles, 56% latency (three cycles). Likewise, average num- ber stalls multiplies divides 2.8 14.2, respectively, 46% 59% ofthe corresponding latency. Structural hazards divides rare, divide frequency low.C-56 ■Appendix C Pipelining: Basic Intermediate ConceptsNumber stalls0.00 1.00 0.20 0.10 0.40 0.80 0.90 0.60 0.70 0.30 0.50FP SPEC benchmarksdoduc ear hydro2d mdljdp su2cor 0.010.010.020.610.000.030.100.880.000.040.220.540.000.070.090.520.080.080.070.98 FP result stalls FP compare stalls Branch/load stalls FP structural Figure C.35 stalls occurring simple RISC V FP pipeline five SPEC89 FP benchmarks. total number stalls per instruction ranges 0.65 su2cor 1.21 doduc, average 0.87. FP result stalls dominate cases, average 0.71 stalls per instruction, 82% stalled cycles. Com- pares generate average 0.1 stalls per instruction second largest source.The divide structural hazard significant doduc. Branch stalls accounted for, would small. Instruction memory Reg ALUData memory RegRF EX DF DS TC WB Figure C.36 eight-stage pipeline structure R4000 uses pipelined instruction data caches. pipe stages labeled detailed function described text. vertical dashed lines represent stage boundaries well location pipeline latches. instruction actually available end IS, tag check done RF, registers fetched. Thus, show instruction memory operating RF.The TC stage needed data memory access, cannot write data register knowwhether cache access hit not.C.6 Putting Together: MIPS R4000 Pipeline ■C-57■EX—Execution, includes effective address calculation, ALU operation, branch-target computation condition evaluation. ■DF—Data fetch, first half data cache access. ■DS—Second half data fetch, completion data cache access. ■TC—Tag check, determine whether data cache access hit. ■WB—Write-back loads register-register operations. addition substantially increasing amount forwarding required, longer-latency pipeline increases load branch delays. Figure C.37 shows load delays two cycles, data value available theend DS. Figure C.38 shows shorthand pipeline schedule use imme- diately follows load. shows forwarding required result load instruction destination three four cycles later. Figure C.39 shows basic branch delay three cycles, branch condition computed EX. MIPS architecture single-cycledelayed branch. R4000 uses predicted-not-taken strategy remainingtwo cycles branch delay. Figure C.40 shows, untaken branches simply one-cycle delayed branches, taken branches one-cycle delay slot fol-lowed two idle cycles. instruction set provides branch-likely instruction, described earlier helps filling branch delay slot.CC 1Time (in clock cycles) CC 2 Instruction memory Reg ALUData memory Reg Instruction memory Reg ALUData memory Reg Instruction memory Reg ALUData memory Reg Instruction memoryInstruction 1 Instruction 2 Reg ALUData memory RegCC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9 CC 10 CC 11 Figure C.37 structure R4000 integer pipeline leads x1 load delay. x1 delay possible data value available end DS bypassed. tag check TC indicates miss, pipeline isbacked cycle, correct data available.C-58 ■Appendix C Pipelining: Basic Intermediate ConceptsPipeline interlocks enforce x1 branch stall penalty taken branch data hazard stall arises use load result. R4000, allimplementations MIPS processor made use dynamic branch prediction. addition increase stalls loads branches, deeper pipeline increases number levels forwarding ALU operations. RISC Vfive-stage pipeline, forwarding two register-register ALU instructionscould happen ALU/MEM MEM/WB registers. R4000 pipe-line, four possible sources ALU bypass: EX/DF, DF/DS, DS/TC, TC/WB.Clock number Instruction number 1 2 3 4 5 6 7 8 9 ld x1,... RF EX DF DS TC WB add x2,x1,... RF Stall Stall EX DF DS sub x3,x1,... Stall Stall RF EX DF x4,x1,... Stall Stall RF EX Figure C.38 load instruction followed immediate use results x1 stall. Normal forwarding paths used two cycles, add andsub get value forwarding stall. orinstruction gets value register file. two instructions load could independent hence stall, thebypass instructions three four cycles load. CC 1Time (in clock cycles) CC 2 Instruction memory Reg ALUData memory Reg Instruction memory Reg ALUData memory Reg Instruction memory Reg ALUData memory Reg Instruction memoryBEQZ Instruction 1 Instruction 2 Instruction 3 TargetReg ALUData memory Reg Instruction memory Reg ALUData memoryCC 3 CC 4 CC 5 CC 6 CC 7 CC 8 CC 9 CC 10 CC 11 Figure C.39 basic branch delay three cycles, condition evaluation performed EX.C.6 Putting Together: MIPS R4000 Pipeline ■C-59The Floating-Point Pipeline R4000 floating-point unit consists three functional units: floating-point divider, floating-point multiplier, floating-point adder. adder logicis used final step multiply divide. Double-precision FP operationscan take 2 cycles (for negate) 112 cycles (for square root). addi-tion, various units different initiation rates. FP functional unit bethought eight different stages, listed Figure C.41 ; stages combined different orders execute various FP operations. single copy stages, various instructions may use stage zero times indifferent orders. FigureC.42 showsthelatency,initiation rate, pipeline stages used mos common double-precision FP operations.Clock number Instruction number 1 2 3 4 5 6 7 8 9 Branch instruction RF EX DF DS TC WB Delay slot RF EX DF DS TC WBStall Stall Stall Stall Stall Stall Stall StallStall Stall Stall Stall Stall Stall StallBranch target RF EX DF Branch instruction RF EX DF DS TC WB Delay slot RF EX DF DS TC WBBranch instruction+2 RF EX DF DS TCBranch instruction+3 RF EX DF DS Figure C.40 taken branch, shown top portion figure, one-cycle delay slot followed x1 stall, untaken branch, shown bottom portion, simply one-cycle delay slot. branch instruc- tion ordinary delayed branch branch-likely, cancels effect instruction delay slot ifthe branch untaken. Stage Functional unit Description FP adder Mantissa add stage FP divider Divide pipeline stageE FP multiplier Exception test stageM FP multiplier First stage multiplierN FP multiplier Second stage multiplierR FP adder Rounding stageS FP adder Operand shift stageU Unpack FP numbers Figure C.41 eight stages used R4000 floating-point pipelines.C-60 ■Appendix C Pipelining: Basic Intermediate ConceptsFrom information Figure C.42 , determine whether sequence different, independent FP operations issue without stalling. timing thesequence conflict occurs shared pipeline stage, stall willbe needed. Figures C.43 –C.46 show four common possible two-instruction sequences: multiply followed add, add followed multiply, dividefollowed add, add followed divide. figures show inter-esting starting positions second instruction whether second instruc- tion issue stall position. course, could three instructions active, case possibilities stalls much higher figuresmore complex. Performance R4000 Pipeline section, examine stalls occur SPEC92 benchmarks whenrunning R4000 pipeline structure. four major causes pipelinestalls losses: 1.Load stalls —Delays arising use load result one two cycles load 2.Branch stalls —Two-cycle stalls every taken branch plus unfilled canceled branch delay slots. version MIPS instruction set implemented R4000 supports instructions predict branch compile time cause theinstruction branch delay slot canceled branch behaviordiffers prediction. makes easier fill branch delay slots. 3.FP result stalls —Stalls RAW hazards FP operand 4.FP structural stalls —Delays issue restrictions arising conflicts functional units FP pipelineFP instruction Latency Initiation interval Pipe stages Add, subtract 4 3 U, S+A, A+R, R+S Multiply 8 4 U, E+M, M, M, M, N, N+A, RDivide 36 35 U, A, R, 28, D+A, D+R, D+A, D+R, A, R Square root 112 111 U, E, (A+R)108,A ,R Negate 2 1 U, SAbsolute value 2 1 U, SFP compare 3 2 U, A, R Figure C.42 latencies initiation intervals FP operations initiation intervals FP operations depend FP unit stages given operation must use. latency values assume destination instruction FP operation; latencies one cycle less destination store. pipe stages shown order used operation. notation S+ indicates clock cycle stages used. notation 28indicates stage used 28 times row.C.6 Putting Together: MIPS R4000 Pipeline ■C-61Figure C.47 shows pipeline CPI breakdown R4000 pipeline 10 SPEC92 benchmarks. Figure C.48 shows data tabular form. data Figures C.47 andC.48 , see penalty deeper pipelining. R4000 ’s pipeline much longer branch delays classic five-stage pipeline. longer branch delay substantially increases cycles spent branches, especially integer programs higher branchfrequency. reason almost subsequent processors moderateto deep pipelines (8 –16 stages typical today) employ dynamic branch predictors.Clock cycle Operation Issue/stall 0 1 2 34567891 0 1 1 1 2 Multiply Issue U E+M N N+A R Add Issue U S+A A+R R+S Issue U S+A A+R R+SIssue U S+A A+R R+SStall U S+A A+R R+S Stall U S+A A+R R+S Issue U S+A A+R R+SIssue U S+A A+R R+S Figure C.43 FP multiply issued clock 0 followed single FP add issued clocks 1 7. second column indicates whether instruction specified type stalls issued ncycles later, nis clock cycle number U stage second instruction occurs. stage stages cause stall arein bold. Note table deals interaction multiply oneadd issued clocks 1 7. case, add stall issued four five cycles multiply; otherwise, issues without stalling. Notice add stalled two cycles issues cycle 4 next clock cycle willstill conflict multiply; if, however, add issues cycle 5, stall 1 clock cycle, eliminate conflicts. Clock cycle Operation Issue/stall 0 1 2 3 4 5 6 7 8 9 10 11 12 Add Issue U S+A A+R R+S Multiply Issue U E+M N N+A R Issue U N N+A R Figure C.44 multiply issuing add always proceed without stalling, shorter instruction clears shared pipeline stages longer instruction reaches them.C-62 ■Appendix C Pipelining: Basic Intermediate ConceptsAn interesting effect observed FP programs latency FP functional units leads result stalls structural hazards, ariseboth initiation interval limitations conflicts functional unitsfrom different FP instructions. Thus, reducing latency FP operations shouldbe first target, rather pipelining replication functional units.Of course, reducing latency would probably increase structural stalls, many potential structural stalls hidden behind data hazards.Clock cycle Operation Issue/stall 25 26 27 28 29 30 31 32 33 34 35 36 Divide Issued cycle 0…DDD D+A D+R D+A D+R R Add Issue U S+A A+R R+S Issue U S+A A+R R+SStall U S+A A+R R+S Stall U S+A A+R R+S Stall U S+A A+R R+S Stall U S+A A+R R+S Stall U S+A A+R R+S Stall U S+A A+R R+S Issue U S+A A+RIssue US + Issue U Figure C.45 FP divide cause stall add starts near end divide. divide starts cycle 0 completes cycle 35; last 10 cycles divide shown. divide makes heavy use rounding hardware needed add, stalls add starts cycles 28 –33. Notice add starting cycle 28 stalled cycle 36. add started right divide, would conflict, add could complete divide needed shared stages, saw Figure C.44 multiply add. earlier figure, example assumes exactly one add reaches U stage clock cycles 26 35. Clock cycle Operation Issue/stall 0 1 2 3 4 5 6 7 8 9 10 11 12 Add Issue U S+A A+R R+S Divide Stall U AR DDDDDDD Issue U R DIssue U R Figure C.46 double-precision add followed double-precision divide. divide starts one cycle add, divide stalls, conflict.C.6 Putting Together: MIPS R4000 Pipeline ■C-63Pipeline CPI 0.003.00 0.501.002.00 1.502.50 SPEC92 benchmarkcompresseqntottespressogccli doducear hydro2dmdljdp su2corBase Load stalls Branch stalls FP result stalls FP structural stalls Figure C.47 pipeline CPI 10 SPEC92 benchmarks, assuming perfect cache. pipeline CPI varies 1.2 2.8. left-most five programs integer programs, branch delays major CPI contributor these. right-most five programs FP, FP result stalls major contributor these.Figure C.48 shows numbers used construct plot. Benchmark Pipeline CPI Load stalls Branch stalls FP result stalls FP structural stalls Compress 1.20 0.14 0.06 0.00 0.00 Eqntott 1.88 0.27 0.61 0.00 0.00Espresso 1.42 0.07 0.35 0.00 0.00Gcc 1.56 0.13 0.43 0.00 0.00Li 1.64 0.18 0.46 0.00 0.00Integer average 1.54 0.16 0.38 0.00 0.00 Doduc 2.84 0.01 0.22 1.39 0.22Mdljdp2 2.66 0.01 0.31 1.20 0.15Ear 2.17 0.00 0.46 0.59 0.12Hydro2d 2.53 0.00 0.62 0.75 0.17Su2cor 2.18 0.02 0.07 0.84 0.26FP average 2.48 0.01 0.33 0.95 0.18 Overall average 2.00 0.10 0.36 0.46 0.09 Figure C.48 total pipeline CPI contributions four major sources stalls shown. major contributors FP result stalls (both branches FP inputs) branch stalls, loads FP structural stalls adding less.C-64 ■Appendix C Pipelining: Basic Intermediate ConceptsC.7 Cross-Cutting Issues RISC Instruction Sets Efficiency Pipelining already discussed advantages instruction set simplicity building pipelines. Simple instruction sets offer another advantage: make easier toschedule code achieve efficiency execution pipeline. see this, con- sider simple example: suppose need add two values memory store result back memory. sophisticated instruction sets takeonly single instruction; others, take two three. typical RISC archi-tecture would require four instructions (two loads, add, store). Theseinstructions cannot scheduled sequentially pipelines without interven-ing stalls. RISC instruction set, individual operations separate instructions may individually scheduled either compiler (using techniques discussed earlier powerful techniques discussed Chapter 3 ) using dynamic hardware scheduling techniques (which discuss next furtherdetail Chapter 3 ). efficiency advantages, coupled greater ease implementation, appear significant almost recent pipelinedimplementations complex instruction sets actually translate complexinstructions simple RISC-like operations, schedule pipeline thoseoperations. recent Intel processors use approach, also used ARMprocessors complex instructions. Dynamically Scheduled Pipelines Simple pipelines fetch instruction issue it, unless data dependencebetween instruction already pipeline fetched instruction can-not hidden bypassing forwarding. Forwarding logic reduces effec-tive pipeline latency certain dependences result hazards. unavoidable hazard, hazard detection hardware stalls pipeline (start- ing instruction uses result). new instructions fetched orissued dependence cleared. overcome performance losses,the compiler attempt schedule instructions avoid hazard; approachis called compiler orstatic scheduling . Several early processors used another approach, called dynamic scheduling , whereby hardware rearranges instruction execution reduce stalls. Thissection offers simpler introduction dynamic scheduling explaining scor- eboarding technique CDC 6600. readers find easier read material plunging complicated Tomasulo scheme, thespeculation approaches extend it, covered Chapter 3 . techniques discussed appendix far use in-order instruction issue, means instruction stalled pipeline, later instruc-tions proceed. in-order issue, two instructions hazard betweenC.7 Cross-Cutting Issues ■C-65them, pipeline stall, even later instructions independent would stall. RISC V pipeline developed earlier, structural data hazards checked instruction decode (ID): instruction could execute properly,it issued ID. allow instruction begin execution soon oper-ands available, even predecessor stalled, must separate issue processinto two parts: checking structural hazards waiting absence datahazard. decode issue instructions order; however, want instructionsto begin execution soon data operands available. Thus, pipeline doout-of-order execution , implies out-of-order completion . implement out-of-order execution, must split ID pipe stage two stages: 1.Issue—Decode instructions, check structural hazards. 2.Read operands —Wait data hazards, read operands. stage proceeds issue stage, EX stage follows read oper- ands stage, RISC V pipeline. RISC V floating-point pipeline,execution may take multiple cycles, depending operation. Thus, mayneed distinguish instruction begins execution completes execution ; two times, instruction execution . allows mul- tiple instructions execution time. addition changes pipeline structure, also change functional unit design varying thenumber units, latency operations, functional unit pipelining asto better explore advanced pipelining techniques. Dynamic Scheduling Scoreboard dynamically scheduled pipeline, instructions pass issue stage inorder (in-order issue); however, stalled bypass sec-ond stage (read operands) thus enter execution order. Scoreboarding technique allowing instructions execute order sufficient resources data dependences; named CDC 6600 scoreboard, developed capability. see scoreboarding could used RISC V pipeline, important observe WAR hazards, exist RISC Vfloating-point integer pipelines, may arise instructions execute oforder. example, consider following code sequence: fdiv.d f0,f2,f4 fadd.d f10,f0,f8 fsub.d f8,f8,f14 potential WAR hazard fadd.d fsub.d :I f pipeline executes fsub.d fadd.d , violate yield incor- rect execution. Likewise, pipeline must avoid WAW hazards (e.g.,as wouldC-66 ■Appendix C Pipelining: Basic Intermediate Conceptsoccur destination fsub.d weref10). see, haz- ards avoided scoreboard stalling later instruction involved hazard. goal scoreboard maintain execution rate one instruction per clock cycle (when structural hazards) executing instruction asearly possible. Thus, next instruction execute stalled, otherinstructions issued executed depend active orstalled instruction. scoreboard takes full responsibility instruction issueand execution, including hazard detection. Taking advantage out-of-order execution requires multiple instructions EX stage simultaneously. achieved multiple functional units, pipelined functionalunits, both. two capabilities —pipelined functional units multiple functional units —are essentially equivalent purposes pipe- line control, assume processor multiple functional units. CDC 6600 16 separate functional units, including 4 floating-point units, 5 units memory references, 7 units integer operations. processor forthe RISC V architecture, scoreboards make sense primarily floating-point unit latency functional units small. Let ’s assume two multipliers, one adder, one divide unit, single integer unit mem-ory references, branches, integer operations. Although example simplerthan CDC 6600, sufficiently powerful demonstrate principles withouthaving mass detail needing long examples. RISC V theCDC 6600 load-store architectures, techniques nearly identical twoprocessors. Figure C.49 shows processor looks like. Every instruction goes scoreboard, record data dependences constructed; step corresponds instruction issue replaces part ID step RISC V pipeline. scoreboard determines whenthe instruction read operands begin execution. scoreboard decidesthe instruction cannot execute immediately, monitors every change hard-ware decides instruction canexecute. scoreboard also controls instruction write result destination register. Thus, hazarddetection resolution centralized scoreboard. see picture ofthe scoreboard later ( Figure C.49 page C.68), first need understand steps issue execution segment pipeline. instruction undergoes four steps executing. (Because concen- trating FP operations, consider step memory access.) Let ’s first examine steps informally look detail scoreboardkeeps necessary information determines progress one stepto next. four steps, replace ID, EX, WB steps standardRISC V pipeline, follows: 1.Issue—If functional unit instruction free active instruc- tion destination register, scoreboard issues instruction tothe functional unit updates internal data structure. step replaces aportion ID step RISC V pipeline. ensuring activeC.7 Cross-Cutting Issues ■C-67functional unit wants write result destination register, guaran- tee WAW hazards cannot present. structural WAW hazard exists,then instruction issue stalls, instructions issue thesehazards cleared. issue stage stalls, causes buffer betweeninstruction fetch issue fill; buffer single entry, instruction fetchstalls immediately. buffer queue multiple instructions, stallswhen queue fills. 2.Read operands —The scoreboard monitors availability source oper- ands. source operand available earlier issued active instruction going write it. source operands available, scoreboard tellsControl/ statusScoreboard Control/ status Integer unitFP addFP divideFP mult FP multData buses Registers Figure C.49 basic structure RISC V processor scoreboard. score- board ’s function control instruction execution (vertical control lines). data flow register file functional units buses (the horizontal lines, called trunks CDC 6600). two FP multipliers, FP divider, FP adder, integer unit. One set buses (two inputs one output) serves group functional units. explore scoreboarding extensions detail Chapter 3 .C-68 ■Appendix C Pipelining: Basic Intermediate Conceptsthe functional unit proceed read operands registers begin execution. scoreboard resolves RAW hazards dynamically step, instructions may sent execution order. step, together withissue, completes function ID step simple RISC V pipeline. 3.Execution —The functional unit begins execution upon receiving operands. result ready, notifies scoreboard completed execu-tion. step replaces EX step RISC V pipeline takes multiplecycles RISC V FP pipeline. 4.Write result —Once scoreboard aware functional unit completed execution, scoreboard checks WAR hazards stalls thecompleting instruction, necessary. WAR hazard exists code sequence like earlier example fadd.d andfsub.d use f8. example, code fdiv.d f0,f2,f4 fadd.d f10,f0,f8fsub.d f8,f8,f14 fadd.d source operand f8, register destination fsub.d . Butfadd.d actually depends earlier instruction. scoreboard still stall fsub.d write result stage fadd.d reads operands. general, then, completing instruction cannot allowed write results when: ■There instruction read operands precedes (i.e., order issue) completing instruction, ■One operands register result completinginstruction. WAR hazard exist, clears, scoreboard tells func- tional unit store result destination register. step replaces WBstep simple RISC V pipeline. first glance, might appear scoreboard difficulty separat- ing RAW WAR hazards. operands instruction read operands available register file, scoreboard take advantage forwarding.Instead, registers read available. large apenalty might initially think. Unlike simple pipeline earlier, instruc-tions write result register file soon complete execution(assuming WAR hazards), rather wait statically assigned write slot may several cycles away. effect reduces pipeline latency benefits forwarding. still one additional cycle latency arisesbecause write result read operand stages cannot overlap. would needadditional buffering eliminate overhead.C.7 Cross-Cutting Issues ■C-69Based data structure, scoreboard controls instruction pro- gression one step next communicating functional units. small complication, howeve r. limited number source operand buses result buses register file, representsa structural hazard. scoreboard must guarantee number func-t n lu n sa l l w e dt op r c e e di n os e p s2a n d4d e sn te x c e e dt h en u b e rof buses available. go detail this, men-tion CDC 6600 solved problem grouping 16 functionalunits together four groups supplying set buses, called data trunks , group. one unit group could read operands write result clock. C.8 Fallacies Pitfalls Pitfall Unexpected execution sequences may cause unexpected hazards . first glance, WAW hazards look like never occur code sequence compiler would ever generate two writes register without anintervening read, occur sequence unexpected. exam-ple, consider long running floating point divide causes trap. trap rou- tine writes register divide early on, may cause WAW hazard, writes register divide completes. Hardware software must avoidthis possibility. Pitfall Extensive pipelining impact aspects design, leading overall worse cost-performance . best example phenomenon comes two implementations VAX, 8600 8700. 8600 initially delivered, cycle time 80 ns. Subsequently, redesigned version, called 8650, 55 ns clock introduced. 8700 much simpler pipeline operates atthe microinstruction level, yielding smaller processor faster clock cycleof 45 ns. overall outcome 8650 CPI advantage about20%, 8700 clock rate 20% faster. Thus, 8700achieved performance much less hardware. Pitfall Evaluating dynamic static scheduling basis unoptimized code . Unoptimized code —containing redundant loads, stores, operations might eliminated optimizer —is much easier schedule “tight” optimized code. holds scheduling control delays (with delayedbranches) delays arising RAW hazards. gcc running anR3000, pipeline almost identical Section C.1 , fre- quency idle clock cycles increases 18% unoptimized sched-uled code optimized scheduled code. course, optimizedprogram much faster, fewer instructions. fairly evaluate compile-time scheduler runtime dynamic scheduling, must use optimizedC-70 ■Appendix C Pipelining: Basic Intermediate Conceptscode, real system derive good performance optimizations addition scheduling. C.9 Concluding Remarks beginning 1980s, pipelining technique reserved primarily supercomputers large multimillion-dollar mainframes. mid-1980s, thefirst pipelined microprocessors appeared helped transform world com- puting, allowing microprocessors bypass minicomputers performance eventually take outperform mainframes. early 1990s, high-endembedded microprocessors embraced pipelining, desktops headedtoward use sophisticated dynamically scheduled, multiple-issueapproaches discussed Chapter 3 . material appendix, con- sidered reasonably advanced graduate students text first appeared in1990, considered basic undergraduate material found proces-sors cost less $1! C.10 Historical Perspective References Section M.5 (available online) features discussion development pipelining instruction-level parallelism covering appendix thematerial Chapter 3 . provide numerous references reading exploration topics. Updated Exercises Diana Franklin C.1 [15/15/15/15/25/10/15] <A.2>Use following code fragment: Loop: ld x1,0(x2) ;load x1 address 0+x2 addi x1,x1,1 ;x1=x1+1sd x1,0,(x2) ;store x1 address 0+x2addi x2,x2,4 ;x2=x2+4sub x4,x3,x2 ;x4=x3-x2bnez x4,Loop ;branch Loop x4 !=0 Assume initial value x3 x2+396. a.[15]<C.2>Data hazards caused data dependences code. Whether dependency causes hazard depends machine implementation (i.e.,number pipeline stages). List data dependences code above.Record register, source instruction, destination instruction; example,there data dependency register x1 ldto addi . b.[15]<C.2>Show timing instruction sequence 5-stage RISC pipeline without forwarding bypassing hardware assuming reg-ister read write clock cycle “forwards ”through registerUpdated Exercises Diana Franklin ■C-71file, add andorshown Figure C.5 . Use pipeline timing chart like Figure C.8 . Assume branch handled flushing pipeline. memory references take 1 cycle, many cycles thisloop take execute? c.[15]<C.2>Show timing instruction sequence 5-stage RISC pipeline full forwarding bypassing hardware. Use pipeline timingchart like shown Figure C.8 . Assume branch handled pre- dicting taken. memory references take 1 cycle, many cyclesdoes loop take execute? d.[15]<C.2>Show timing instruction sequence 5-stage RISC pipeline full forwarding bypassing hardware, shown Figure C.6 . Use pipeline timing chart like shown Figure C.8 . Assume branch handled predicting taken. memory references take 1cycle, many cycles loop take execute? e.[25]<C.2>High-performance processors deep pipelines —more 15 stages. Imagine 10-stage pipeline every stage 5-stage pipeline split two. catch that, data for- warding, data forwarded end pair stages beginning two stages needed. example, data forwarded theoutput second execute stage input first execute stage, stillcausing 1-cycle delay. Show timing instruction sequence the10-stage RISC pipeline full forwarding bypassing hardware. Use apipeline timing chart like shown Figure C.8 (but stages labeled IF1, IF2, ID1, etc.). Assume branch handled predicting taken. memory references take 1 cycle, many cycles loop take execute? f.[10]<C.2>Assume 5-stage pipeline, longest stage requires 0.8 ns, pipeline register delay 0.1 ns. clock cycle timeof 5-stage pipeline? 10-stage pipeline splits stages half, whatis cycle time 10-stage machine? g.[15]<C.2>Using answers parts (d) (e), determine cycles per instruction (CPI) loop 5-stage pipeline 10-stage pipeline.Make sure count first instruction reaches write-backstage end. count start-up first instruction. Using clockcycle time calculated part (f), calculate average instruction execute timefor machine. C.2 [15/15] <C.2>Suppose branch frequencies (as percentages instructions) follows: Conditional branches 15% Jumps calls 1%Taken conditional branches 60% takenC-72 ■Appendix C Pipelining: Basic Intermediate Conceptsa.[15]<C.2>We examining four-stage pipeline branch resolved end second cycle unconditional branches end third cycle conditional branches. Assuming first pipestage always completed independent whether branch taken andignoring pipeline stalls, much faster would machine withoutany branch hazards? b.[15]<C.2>Now assume high-performance processor 15- deep pipeline branch resolved end fifth cycle forunconditional branches end tenth cycle conditionalbranches. Assuming first pipe stage always completed inde-pendent whether branch taken ignoring pipeline stalls, howmuch faster would machine without branch hazards? C.3 [5/15/10/10] <C.2>We begin computer implemented single-cycle implementation. stages split functionality, stages require exactly amount time. original machine clock cycle time 7 ns. stages split, measured times IF,1 ns; ID, 1.5 ns; EX, 1 ns; MEM, 2 ns; WB, 1.5 ns. pipeline register delayis 0.1 ns. a.[5]<C.2>What clock cycle time 5-stage pipelined machine? b.[15]<C.2>If stall every four instructions, CPI new machine? c.[10]<C.2>What speedup pipelined machine single-cycle machine? d.[10]<C.2>If pipelined machine infinite number stages, would speedup single-cycle machine? C.4 [15]<C.1, C.2 >A reduced hardware implementation classic five-stage RISC pipeline might use EX stage hardware perform branch instructioncomparison actually deliver branch target PC stage untilthe clock cycle branch instruction reaches MEM stage. Controlhazard stalls reduced resolving branch instructions ID, improvingperformance one respect may reduce performance circumstances. Writea small snippet code calculating branch ID stage causes datahazard, even data forwarding. C.5 [12/13/20/20/15/15] <C.2, C.3 >For problems, explore pipeline register-memory architecture. architecture two instruction formats:a register-register format register-memory format. single-memory addressing mode (offset+base register). set ALU operations format: ALUop Rdest, Rsrc1, Rsrc2 ALUop Rdest, Rsrc1, MEMUpdated Exercises Diana Franklin ■C-73where ALUop one following: add, subtract, AND, OR, load (Rsrc1 ignored), store. Rsrc Rdest registers. MEM base register offset pair. Branches use full compare two registers PC relative. Assume thatthis machine pipelined new instruction started every clock cycle. Thepipeline structure, similar used VAX 8700 micropipeline (Clark,1987), RF ALU1 MEM ALU2 WB RF ALU1 MEM ALU2 WB RF ALU1 MEM ALU2 WB RF ALU1 MEM ALU2 WB RF ALU1 MEM ALU2 WB RF ALU1 MEM ALU2 WB first ALU stage used effective address calculation memory references branches. second ALU cycle used operations branch compa-rison. RF decode register-fetch cycle. Assume registerread register write register occur clock, write dataare forwarded. a.[12]<C.2>Find number adders needed, counting adder incre- menter; show combination instructions pipe stages justify thisanswer. need give one combination maximizes adder count. b.[13]<C.2>Find number register read write ports memory read write ports required. Show answer correct showing com-bination instructions pipeline stage indicating instruction thenumber read ports write ports required instruction. c.[20]<C.3>Determine data forwarding ALUs needed. Assume separate ALUs ALU1 ALU2 pipe stages. Putin forwarding among ALUs necessary avoid reduce stalls. Show relationship two instructions involved forwarding using for- mat table Figure C.23 ignoring last two columns. careful consider forwarding across intervening instruction —for example, add x1, ... instructionadd ..., x1, ... d.[20]<C.3>Show data forwarding requirements necessary avoid reduce stalls either source destination unit ALU. Use thesame format Figure C.23 , ignoring last two columns. Remember forward memory references.C-74 ■Appendix C Pipelining: Basic Intermediate Conceptse.[15]<C.3>Show remaining hazards involve least one unit ALU source destination unit. Use table like shown Figure C.25 , replace last column lengths hazards. f.[15]<C.2>Show control hazards example state length stall. Use format like shown Figure C.11 , labeling example. C.6 [12/13/13/15/15] <C.1, C.2, C.3 >We add support register-memory ALU operations classic five-stage RISC pipeline. offset increase complexity, allmemory addressing restricted register indirect (i.e., addresses simply value held register; offset displacement maybe added register value). example, register-memory instructionadd x4, x5, (x1) means add contents register x5 contents memory location address equal value register x1 put thesum register x4. Register-register ALU operations unchanged. followingitems apply integer RISC pipeline: a.[12]<C.1>List rearranged order five traditional stages RISC pipeline support register-memory operations implemented exclusively register indirect addressing. b.[13]<C.2, C.3 >Describe new forwarding paths needed rear- ranged pipeline stating source, destination, information transferred needed new path. c.[13]<C.2, C.3 >For reordered stages RISC pipeline, new data hazards created addressing mode? Give instruction sequence illus- trating new hazard. d.[15]<C.3>List ways RISC pipeline register-memory ALU operations different instruction count given program thanthe original RISC pipeline. Give pair specific instruction sequences, one forthe original pipeline one rearranged pipeline, illustrate way. e.[15]<C.3>Assume instructions take 1 clock cycle per stage. List ways register-memory RISC V different CPI givenprogram compared original RISC V pipeline. C.7 [10/10] <C.3>In problem, explore deepening pipeline affects performance two ways: faster clock cycle increased stalls due dataand control hazards. Assume original machine 5-stage pipeline a1 ns clock cycle. second machine 12-stage pipeline 0.6 ns clockcycle. 5-stage pipeline experiences stall due data hazard every fiveinstructions, whereas 12-stage pipeline experiences three stalls every eight instructions. addition, branches constitute 20% instructions, mis- prediction rate machines 5%. a.[10]<C.3>What speedup 12-stage pipeline 5-stage pipe- line, taking account data hazards?Updated Exercises Diana Franklin ■C-75b.[10]<C.3>If branch mispredict penalty first machine 2 cycles second machine 5 cycles, CPIs each, taking account stalls due branch mispredictions? C.8 [15]<C.5>Construct table like shown Figure C.21 check WAW stalls RISC V FP pipeline Figure C.30 . consider FP divides. C.9 [20/22/22] <C.4, C.6 >In exercise, look common vector loop runs statically dynamically scheduled versions RISC V pipe- line. loop so-called DAXPY loop (discussed extensively AppendixG) central operation Gaussian elimination. loop implements thevector operation Y= a*X+Yfor vector length 100. MIPS code loop: foo: fld f2, 0(x1) ; load X(i) fmul.d f4, f2, f0 ; multiply *X(i) fld f6, 0(x2) ; load Y(i)fadd.d f6, f4, f6 ; add *X(i) + Y(i) fsd 0(x2), f6 ; store Y(i) addi x1, x1, 8 ; increment X index addi x2, x2, 8 ; increment index sltiu x3, x1, done ; test donebnez x3, foo ; loop done parts (a) (c), assume integer operations issue complete 1 clock cycle (including loads) results fully bypassed. use FP latencies (only) shown Figure C.29 , assume FP unit fully pipe- lined. scoreboards below, assume instruction waiting result fromanother function unit pass read operands time result iswritten. Also assume instruction WB completing allow currentlyactive instruction waiting functional unit issue sameclock cycle first instruction completes WB. a.[20]<C.5>For problem, use RISC V pipeline Section C.5 pipeline latencies Figure C.29 , fully pipelined FP unit, initi- ation interval 1. Draw timing diagram, similar Figure C.32 , showing timing instruction's execution. many clock cycles loop iteration take, counting first instruction enters WB stage towhen last instruction enters WB stage? b.[20]<C.8>Perform static instruction reordering reorder instructions minimize stalls loop, renaming registers necessary. Use thesame assumptions (a). Draw timing diagram, similar Figure C.32 , showing timing instruction's execution. many clock cyclesdoes loop iteration take, counting first instruction entersthe WB stage last instruction enters WB stage?C-76 ■Appendix C Pipelining: Basic Intermediate Conceptsc.[20]<C.8>Using original code above, consider instructions would executed using scoreboarding, form dynamic scheduling. Draw timing diagram, similar Figure C.32 , showing timing instructions stages IF, (issue), RO (read operands), EX (exe-cution), WR (write result). many clock cycles loop iter-ation take, counting first instruction enters WB stage towhen last instruction enters WB stage? C.10 [25]<C.8>It critical scoreboard able distinguish RAW WAR hazards, WAR hazard requires stalling instruction writinguntil instruction reading operand initiates execution, RAW hazardrequires delaying reading instruction writing instruction finishes —just opposite. example, consider sequence: fmul.d f0,f6,f4 fsub.d f8,f0,f2fadd.d f2,f10,f2 Thefsub.d depends fmul.d (a RAW hazard), thus fmul.d must allowed complete fsub.d . fmul.d stalled fsub.d due inability distinguish RAW WAR hazards, processor deadlock. sequence contains WAR hazard fadd.d fsub.d , fadd.d cannot allowed complete thefsub.d begins execution. difficulty lies distinguishing RAW haz- ard fmul.d andfsub.d , WAR hazard fsub.d fadd.d . see three-instruction scenario important, trace han- dling instruction stage stage issue, read operands, execute, andwrite result. Assume scoreboard stage execute takes 1 clockcycle. Assume fmul.d instruction requires 3 clock cycles execute fsub.d andfadd.d instructions take 1 cycle execute. Finally, assume processor two multiply function units two add function units. Present trace follows. 1.Make table column headings Instruction, Issue, Read Operands, Exe- cute, Write Result, Comment. first column, list instructions inprogram order (be generous space instructions; larger table cellswill better hold results analysis). Start table writing 1 theIssue column fmul.d instruction row show fmul.d completes issue stage clock cycle 1. Now, fill stage columns tablethrough cycle scoreboard first stalls instruction. 2.For stalled instruction write words “waiting clock cycle X, ”where X number current clock cycle, appropriate table column showthat scoreboard resolving RAW WAR hazard stalling stage. Comment column, state type hazard dependent instruc- tion causing wait.Updated Exercises Diana Franklin ■C-773.Adding words “completes clock cycle ”to a“waiting ”table entry, fill rest table time instructions complete. instruction stalled, add description Comments column tellingwhy wait ended deadlock avoided (Hint: Thinkabout WAW hazards prevented implies activeinstruction sequences.). Note completion order three instructionsas compared program order. C.11 [10/10/10] <C.5>For problem, create series small snippets illustrate issues arise using functional units different latencies.For one, draw timing diagram similar Figure C.32 illustrates concept, clearly indicate problem. a.[10]<C.5>Demonstrate, using code different used Figure C.32 , structural hazard hardware one MEM WB stage. b.[10]<C.5>Demonstrate WAW hazard requiring stall.C-78 ■Appendix C Pipelining: Basic Intermediate ConceptsD.1 Introduction D-2 D.2 Advanced Topics Disk Storage D-2 D.3 Definition Examples Real Faults Failures D-10 D.4 I/O Performance, Reliability Measures, Benchmarks D-15 D.5 Little Queuing Theory D-23 D.6 Crosscutting Issues D-34 D.7 Designing Evaluating I/O System —The Internet Archive Cluster D-36 D.8 Putting Together: NetApp FAS6000 Filer D-41 D.9 Fallacies Pitfalls D-43 D.10 Concluding Remarks D-47 D.11 Historical Perspective References D-48 Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau D-48D Storage Systems think Silicon Valley misnamed. look back dollars shipped products last decade, revenue magnetic disks silicon. ought rename place Iron Oxide Valley. Al Hoagland pioneer magnetic disks (1982) Combining bandwidth storage …enables swift reliable access ever expanding troves content proliferating disks …repositories Internet …the capacity storage arrays kinds rocketing ahead advance computer performance. George Gilder “The End Drawing Nigh, ” Forbes ASAP (April 4, 2000)D.1 Introduction popularity Internet services search engines auctions enhanced importance I/O computers, since one would want desktopcomputer ’t access Internet. rise importance I/O reflected names times. 1960s 1980s called ComputingRevolution; period since 1990 called Information Age, withconcerns focused advances information technology versus raw computa- tional power. Internet services depend upon massive storage, focus chapter, networking, focus Appendix F . shift focus computation communication storage infor- mation emphasizes reliability scalability well cost-performance.Although frustrating program crashes, people become hysterical theylose data; hence, storage systems typically held higher standard ofdependability rest computer. Dependability bedrock ofstorage, yet also rich performance theory —queuing theory —that balances throughput versus response time. software determines processor features get used compiler, operating system usurps thatrole storage. Thus, storage different, multifaceted culture processors, yet still found within architecture tent. start exploration advances mag-netic disks, dominant storage device today desktop servercomputers. assume readers already familiar basics storagedevices, covered Chapter 1 . D.2 Advanced Topics Disk Storage disk industry historically concentrated improving capacity disks. Improvement capacity customarily expressed improvement areal density , measured bits per square inch: Areal density ¼Tracks Inchon disk surface /C2Bits Inchon track 1988, rate improvement areal density 29% per year, thus doubling density every 3 years. 1996, therate improved 60% per year, quadrupling density every 3 years matchingthe traditional rate DRAMs. 1997 2003, rate increased to100%, doubling every year. innovations allowed renaissanceshad largely played out, rate dropped recently 30% per year. In2011, highest density commercial products 400 billion bits per square inch. Cost per gigabyte dropped least fast areal density increased, smaller diameter drives playing larger role improve-ment. Costs per gigabyte improved almost factor 1,000,000 between1983 2011.D-2 ■Appendix Storage SystemsMagnetic disks challenged many times supremacy secondary storage. Figure D.1 shows one reason: fabled access time gap disks DRAM. DRAM latency 100,000 times less disk, per-formance advantage costs 30 150 times per gigabyte DRAM. bandwidth gap complex. example, fast disk 2011 transfers 200 MB/sec disk media 600 GB storage costs $400. 4 GB DRAM module costing $200 2011 could transfer 16,000 MB/ sec (see Chapter 2 ), giving DRAM module 80 times higher bandwidth disk. However, bandwidth per GB 6000 times higher DRAM, bandwidth per dollar 160 times higher. Many tried invent technology cheaper DRAM faster disk fill gap, thus far failed. Challengers never prod-uct market right time. time new product ships, DRAMs diskshave made advances predicted earlier, costs dropped accordingly, thechallenging product immediately obsolete. closest challenger Flash memory. semiconductor memory non- volatile like disks, bandwidth disks, latency 100 1000 times faster disk. 2011, price per gigabyte Flash 15 20 times cheaper DRAM. Flash popular cell phones comes inmuch smaller capacities power efficient disks, despite costper gigabyte 15 25 times higher disks. Unlike disks DRAM, 0.1110100100010,000100,0001,000,000 1 10 100 1000 10,000 100,000 1,000,000 10,000,000 100,000,000Cost ($/GB) Access time (ns)Access time gap1980 19801985 19851990 19901995 19952000 20002005 2005DRAM Disk Figure D.1 Cost versus access time DRAM magnetic disk 1980, 1985, 1990, 1995, 2000, 2005. two-order-of-magnitude gap cost five-order-of-magnitude gap access times semiconductor mem- ory rotating magnetic disks inspired host competing technologies try fill them. far, attempts made obsolete production improvements magnetic disks, DRAMs, both. Notethat 1990 2005 cost per gigabyte DRAM chips made less improvement, disk cost made dra-matic improvement.D.2 Advanced Topics Disk Storage ■D-3Flash memory bits wear —typically limited 1 million writes —and popular desktop server computers. disks remain viable foreseeable future, conventional sector-track-cylinder model not. assumptions model nearbyblocks track, blocks cylinder take less time accesssince seek time, tracks closer others. First, disks started offering higher-level intelligent interfaces, like ATA SCSI, included microprocessor inside disk. speed sequentialtransfers, higher-level interfaces organize disks like tapes like ran- dom access devices. logical blocks ordered serpentine fashion across single surface, trying capture sectors recorded bit den-sity. (Disks vary recording density since hard electronics keep upwith blocks spinning much faster outer tracks, lowering linear den-sity simplifies task.) Hence, sequential blocks may different tracks. Wewill see later Figure D.22 page D-45 illustration fallacy assuming conventional sector-track model working modern disks. Second, shortly microprocessors appeared inside disks, disks included buffers hold data computer ready accept it, later caches avoid read accesses. joined command queue allowedthe disk decide order perform commands maximize perfor-mance maintaining correct behavior. Figure D.2 shows queue depth 50 double number I/Os per second random I/Os due better sched-uling accesses. Although ’s unlikely system would really 256 com- mands queue, would triple number I/Os per second. Given buffers,caches, out-of-order accesses, accurate performance model real disk much complicated sector-track-cylinder. 0300 200 100400I/O per second500600 0 300 250 200 150 Queue depthRandom 512-byte reads per second 100 50 Figure D.2 Throughput versus command queue depth using random 512- byte reads. disk performs 170 reads per second starting command queue doubles performance 50 triples 256 [Anderson 2003].D-4 ■Appendix Storage SystemsFinally, number platters shrank 12 past 4 even 1 today, cylinder less importance percentage data cylinder much less. Disk Power Power increasing concern disks well processors. typical ATAdisk 2011 might use 9 watts idle, 11 watts reading writing, 13watts seeking. efficient spin smaller mass, smaller- diameter disks save power. One formula indicates importance rota- tion speed size platters power consumed disk motor isthe following [Gurumurthi et al. 2005]: Power /C25Diameter4:6/C2RPM2:8/C2Number platters Thus, smaller platters, slower rotation, fewer platters help reduce disk motor power, power motor. Figure D.3 shows specifications two 3.5-inch disks 2011. Serial ATA (SATA) disks shoot high capacity best cost per gigabyte, 2000 GB drives cost less $0.05 per gigabyte. use widest platters fit form factor use four five them, spin 5900 RPM seek relatively slowly allow higher areal density lower power. corre- sponding Serial Attach SCSI (SAS) drive aims performance, spins 15,000 RPM seeks much faster. uses lower areal density spin thathigh rate. reduce power, platter much narrower form factor. Thiscombination reduces capacity SAS drive 600 GB. cost per gigabyte factor five better SATA drives, and, conversely, cost per I/O per second MB transferred per second afactor five better SAS drives. Despite using smaller platters many fewer them, SAS disks use twice power SATA drives, due much faster RPM seeks.Capacity (GB) Price Platters RPM Diameter (inches) Average seek (ms) Power (watts) I/O/sec Disk BW (MB/sec) Buffer BW (MB/sec) Buffer size (MB) MTTF (hrs) SATA 2000 $85 4 5900 3.7 16 12 47 45 –95 300 32 0.6 SAS 600 $400 4 15,000 2.6 3 –4 16 285 122 –204 750 16 1.6 Figure D.3 Serial ATA (SATA) versus Serial Attach SCSI (SAS) drives 3.5-inch form factor 2011. I/Os per second calculated using average seek plus time one-half rotation plus time transfer one sector 512 KB.D.2 Advanced Topics Disk Storage ■D-5Advanced Topics Disk Arrays innovation improves dependability performance storage systems disk arrays . One argument arrays potential throughput increased many disk drives and, hence, many disk arms, rather thanfewer large drives. Simply spreading data multiple disks, called striping , auto- matically forces accesses several disks data files large. (Althougharrays improve throughput, latency necessarily improved.) saw inChapter 1 , drawback devices, dependability decreases: N devices generally 1/ Nthe reliability single device. Although disk array would faults smaller number larger disks disk reliability, dependability improved addingredundant disks array tolerate faults. is, single disk fails, lostinformation reconstructed redundant information. danger inhaving another disk fail mean time repair (MTTR). Since mean time failure (MTTF) disks tens years, MTTR measured hours, redundancy make measured reliability many disks much higherthan single disk. redundant disk arrays become known acronym RAID , originally stood redundant array inexpensive disks , although prefer word independent forIin acronym. ability recover failures plus higher throughput, measured either megabytes per second I/Os per second,make RAID attractive.When combined advantagesof smaller size lowerpower small-diameter drives, RAIDs dominate large-scale storage systems. Figure D.4 summarizes five standard RAID levels, showing eight disks user data must supplemented redundant check disks RAID level, lists pros cons level. standard RAID levels well documented, quick review discuss advanced levels inmore depth. ■RAID 0 —It redundancy sometimes nicknamed JBOD , bunch disks , although data may striped across disks array. level generally included act measuring stick RAIDlevels terms cost, performance, dependability. ■RAID 1 —Also called mirroring orshadowing , two copies every piece data. simplest oldest disk redundancy scheme, also highest cost. array controllers optimize read performance allowing mirrored disks act independently reads, optimiza-tion means may take longer mirrored writes complete. ■RAID 2 —This organization inspired applying memory-style error- correcting codes (ECCs) disks. included diskarray product time original RAID paper, none since asother RAID organizations attractive. ■RAID 3 —Since higher-level disk interfaces understand health disk, it’s easy figure disk failed. Designers realized one extra diskD-6 ■Appendix Storage Systemscontains parity information data disks, single disk allows recovery disk failure. data organized stripes, Ndata blocks one parity block. failure occurs, “subtract ”the good data good blocks, remains missing data. (This workswhether failed disk data disk parity disk.) RAID 3 assumes thatthe data spread across disks reads writes, attractive whenreading writing large amounts data. ■RAID 4 —Many applications dominated small accesses. Since sectors error checking, safely increase number readsper second allowing disk perform independent reads. would seemthat writes would still slow, read every disk calculate parity.To increase number writes per second, alternative approach involvesonly two disks. First, array reads old data overwrit- ten, calculates bits would change writes new data. reads old value parity check disks, updates parity accord-ing list changes, writes new value parity checkRAID levelDisk failures tolerated, check space overhead 8 data disks Pros ConsCompany products 0 Nonredundant striped0 failures, 0 check disks space overhead protection Widely used 1 Mirrored 1 failure, 8 check disks parity calculation; fast recovery; small writes faster higher RAIDs; fast readsHighest check storage overheadEMC, HP (Tandem), IBM 2 Memory-style ECC1 failure, 4 check disks ’t rely failed disk self-diagnose/C24Log 2 check storage overheadNot used 3 Bit-interleaved parity1 failure, 1 check disk Low check overhead; high bandwidth large reads writesNo support small, random reads writesStorage Concepts 4 Block- interleavedparity1 failure, 1 check disk Low check overhead; bandwidth small readsParity disk small write bottleneckNetwork Appliance 5 Block- interleaved distributed parity1 failure, 1 check disk Low check overhead; bandwidth small reads writesSmall writes !4 disk accessesWidely used 6 Row-diagonal parity, EVEN-ODD2 failures, 2 check disks Protects 2 disk failures Small writes !6 disk accesses; 2 /C2 check overheadNetwork Appliance Figure D.4 RAID levels, fault tolerance, overhead redundant disks. paper introduced term RAID [Patterson, Gibson, Katz 1987] used numerical classification become popular. fact, non- redundant disk array often called RAID 0 , indicating data striped across several disks without redun- dancy. Note mirroring (RAID 1) instance survive eight disk failures provided one disk mirrored pair fails; worst case disks mirrored pair fail. 2011, may commercial implementationsof RAID 2; rest found wide range products. RAID 0+1, 1 +0, 01, 10, 6 discussed text.D.2 Advanced Topics Disk Storage ■D-7disk. Hence, so-called “small writes ”are still slower small reads — involve four disks accesses —but faster read disks every write. RAID 4 low check disk overhead RAID 3,and still large reads writes fast RAID 3 addition smallreads writes, control complex. ■RAID 5 —Note performance flaw small writes RAID 4 must read write check disk, performance bottleneck.RAID 5 simply distributes parity information across disks array,thereby removing bottleneck. parity block stripe rotated sothat parity spread evenly across disks. disk array controller must nowcalculate disk parity wants write given block, butthat simple calculation. RAID 5 low check disk overhead RAID 3 4, large reads writes RAID 3 small reads RAID 4, higher small write bandwidth RAID 4.Nevertheless, RAID 5 requires sophisticated controller classicRAID levels. completed quick review classic RAID levels, look two levels become popular since RAID introduced. RAID 10 versus 01 (or 1 + 0 versus RAID 0 + 1) One topic always described RAID literature involves mirroring inRAID 1 interacts striping. Suppose had, say, four disks ’worth data store eight physical disks use. Would create four pairs disks —each organized RAID 1 —and stripe data across four RAID 1 pairs? Alter- natively, would create two sets four disks —each organized RAID 0 —and mirror writes RAID 0 sets? RAID terminology evolved callthe former RAID 1+0 RAID 10 ( “striped mirrors ”) latter RAID 0+1 RAID 01 ( “mirrored stripes ”). RAID 6: Beyond Single Disk Failure parity-based schemes RAID 1 5 protect single self- identifying failure; however, operator accidentally replaces wrong diskduring failure, disk array experience two failures, data willbe lost. Another concern since disk bandwidth growing slowly thandisk capacity, MTTR disk RAID system increasing, turnincreases chances second failure. example, 500 GB SATA disk couldtake 3 hours read sequentially assuming interference. Given thedamaged RAID likely continue serve data, reconstruction could stretched considerably, thereby increasing MTTR. Besides increasing reconstruc- tion time, another concern reading much data reconstructionmeans increasing chance uncorrectable media failure, would resultin data loss. arguments concern simultaneous multiple failures areD-8 ■Appendix Storage Systemsthe increasing number disks arrays use ATA disks, slower larger SCSI disks. Hence, years, growing interest protecting one failure. Network Appliance (NetApp), example, started build-ing RAID 4 file servers. double failures becoming danger customers,they created robust scheme protect data, called row-diagonal parity RAID-DP [Corbett et al. 2004]. Like standard RAID schemes, row-diagonal parity uses redundant space based parity calculation per-stripe basis.Since protecting double failure, adds two check blocks per stripe data. Let ’s assume p+1 disks total, p/C01 disks data. Figure D.5 shows case pis 5. row parity disk like RAID 4; contains even parity across four data blocks stripe. block diagonal parity disk containsthe even parity blocks diagonal. Note diagonal notcover one disk; example, diagonal 0 cover disk 1. Hence, need justp/C01 diagonals protect pdisks, disk diagonals 0 3 Figure D.5 . Let’s see row-diagonal parity works assuming data disks 1 3 fail Figure D.5 . ’t perform standard RAID recovery using first row using row parity, since missing two data blocks disks 1 3. How-ever, perform recovery diagonal 0, since missing data blockassociated disk 3. Thus, row-diagonal parity starts recovering one thefour blocks failed disk example using diagonal parity. Since eachdiagonal misses one disk, diagonals miss different disk, two diagonalsare missing one block. diagonals 0 2 example, next restore block diagonal 2 failed disk 1. data blocks recovered, standard RAID recovery scheme used 0 1 2 312 3 423 4 034 0 140 1 201 2 3Data disk 0 Data disk 1 Data disk 2 Data disk 3 Row parity Diagonal parity Figure D.5 Row diagonal parity p55, protects four data disks double failures [Corbett et al. 2004]. figure shows diagonal groups parity calculated stored diagonal parity disk. Although shows check datain separate disks row parity diagonal parity RAID 4, rotated version row-diagonal parity analogous RAID 5. Parameter pmust prime greater 2; however, make plarger number data disks assum- ing missing disks zeros scheme still works. trick makes easy add disks existing system. NetApp picks pto 257, allows sys- tem grow 256 data disks.D.2 Advanced Topics Disk Storage ■D-9recover two blocks standard RAID 4 stripes 0 2, turn allows us recover diagonals. process continues two failed disks completely restored. EVEN-ODD scheme developed earlier researchers IBM similar row diagonal parity, bit computation operation andrecovery [Blaum 1995]. Papers recent show expandEVEN-ODD protect three failures [Blaum, Bruck, Vardy 1996;Blaum et al. 2001]. D.3 Definition Examples Real Faults Failures Although people may willing live computer occasionally crashesand forces programs restarted, insist information neverlost. prime directive storage remember information, matterwhat happens. Chapter 1 covered basics dependability, section expands information give standard definitions examples failures. first step clarify confusion terms. terms fault,error , fail- ureare often used interchangeably, different meanings depend- ability literature. example, programming mistake fault, error, failure?Does matter whether talking designed pro-gram run? running program ’t exercise mistake, still fault/ error/failure? Try another one. Suppose alpha particle hits DRAM memory cell. fault/error/failure ’t change value? fault/error/failure memory ’t access changed bit? fault/error/failure still occur memory error correction delivered corrected value CPU?You get drift difficulties. Clearly, need precise definitions discusssuch events intelligently. avoid imprecision, subsection based terminology used Laprie [1985] Gray Siewiorek [1991], endorsed IFIP Working Group10.4 IEEE Computer Society Technical Committee Fault Tolerance. talk system single module, terminology applies submo- dules recursively. Let ’s start definition dependability : Computer system dependability quality delivered service reli- ance justifiably placed service. service delivered system observed actual behavior perceived system(s) interacting system ’s users. module also ideal specified behavior , service specification agreed description expected behavior. system failure occurs actual behavior deviates specified behavior. failureoccurred error , defect module. cause error fault . fault occurs, creates latent error , becomes effective activated; error actually affects delivered service, failure occurs. TheD-10 ■Appendix Storage Systemstime occurrence error resulting failure error latency . Thus, error manifestation system fault, failure manifestation service error. [p. 3] Let’s go back motivating examples above. programming mistake fault. consequence error (orlatent error ) software. Upon activation, error becomes effective . effective error produces erroneous data affect delivered service, failure occurs. alpha particle hitting DRAM considered fault. changes memory, creates error. error remain latent affected memory word read. effective word error affects delivered service, failure occurs. ECC corrected error, failure would occur. mistake human operator fault. resulting altered data error. latent activated, before. clarify, relationship among faults, errors, failures follows: ■A fault creates one latent errors. ■The properties errors (1) latent error becomes effective activated;(2) error may cycle latent effective states; (3) effectiveerror often propagates one component another, thereby creating newerrors. Thus, either effective error formerly latent error component propagated another error component elsewhere. ■A component failure occurs error affects delivered service. ■These properties recursive apply component system. Gray Siewiorek classified faults four categories according cause: 1.Hardware faults —Devices fail, perhaps due alpha particle hitting memory cell 2.Design faults —Faults software (usually) hardware design (occasionally) 3.Operation faults —Mistakes operations maintenance personnel 4.Environmental faults —Fire, flood, earthquake, power failure, sabotage Faults also classified duration transient, intermittent, perma- nent [Nelson 1990]. Transient faults exist limited time recurring. Intermittent faults cause system oscillate faulty fault-free oper- ation. Permanent faults correct passing time. defined difference faults, errors, failures, ready see real-world examples. Publications real error ratesare rare two reasons. First, academics rarely access significant hardware resources measure. Second, industrial researchers rarely allowed publish failure information fear would used companies themarketplace. exceptions follow.D.3 Definition Examples Real Faults Failures ■D-11Berkeley ’s Tertiary Disk Tertiary Disk project University California created art image server Fine Arts Museums San Francisco 2000. database consisted ofhigh-quality images 70,000 artworks [Talagala et al., 2000]. databasewas stored cluster, consisted 20 PCs connected switchedEthernet containing 368 disks. occupied seven 7-foot-high racks. Figure D.6 shows failure rates various components Tertiary Disk. advance building system, designers assumed SCSI data diskswould least reliable part system, mechanical plen- tiful. Next would IDE disks since fewer them, power supplies, followed integrated circuits. assumed passive devices suchas cables would scarcely ever fail. Figure D.6 shatters assumptions. Since designers followed manufacturer ’s advice making sure disk enclosures reduced vibra- tion good cooling, data disks reliable. contrast, PC chassiscontaining IDE/ATA disks afford environmental controls.(The IDE/ATA disks store data helped application operating Component Total system Total failed Percentage failed SCSI controller 44 1 2.3% SCSI cable 39 1 2.6%SCSI disk 368 7 1.9%IDE/ATA disk 24 6 25.0%Disk enclosure —backplane 46 13 28.3% Disk enclosure —power supply 92 3 3.3% Ethernet controller 20 1 5.0%Ethernet switch 2 1 50.0%Ethernet cable 42 1 2.3%CPU/motherboard 20 0 0% Figure D.6 Failures components Tertiary Disk 18 months operation. type component, table shows total number system, number failed, percentage failure rate. Disk enclosures two entries table two types problems: backplane integrity failures power supplyfailures. Since enclosure two power supplies, power supply failure affect availability. cluster 20 PCs, contained seven 7-foot-high, 19-inch-wide racks, hosted 368 8.4 GB, 7200 RPM, 3.5-inch IBM disks. PCs P6-200 MHz with96 MB DRAM each. ran FreeBSD 3.0, hosts connected via switched 100 Mbit/sec Ethernet. SCSI disks connected two PCs via double-ended SCSI chains support RAID 1. primary application called Zoom Project, in1998 world ’s largest art image database, 72,000 images. See Talagala et al. [2000b].D-12 ■Appendix Storage Systemssystem boot PCs.) Figure D.6 shows SCSI backplane, cables, Ethernet cables reliable data disks themselves! Tertiary Disk large system many redundant components, could survive wide range failures. Components connected mirroredimages placed single failure could make image unavailable.This strategy, initially appeared overkill, proved vital. experience also demonstrated difference transient faults hard faults. Virtually failures Figure D.6 appeared first transient faults. operator decide behavior poor needed replaced could continue. fact, word “failure ”was used; instead, group borrowed terms normally used dealing problem employees, withthe operator deciding whether problem component be“fired. ” Tandem next example comes industry. Gray [1990] collected data faults Tandem Computers, one pioneering companies fault-tolerantcomputing used primarily databases. Figure D.7 graphs faults caused system failures 1985 1989 absolute faults per system percentage faults encountered. data show clear improvement reliability hardware maintenance. Disks 1985 required yearly serviceby Tandem, replaced disks required scheduled mainte-nance. Shrinking numbers chips connectors per system plus software ’s abil- ity tolerate hardware faults reduced hardware ’s contribution 7% failures 1989. Moreover, hardware fault, software embedded inthe hardware device (firmware) often culprit. data indicate soft-ware 1989 major source reported outages (62%), followed system operations (15%). problem statistics data refer reported; example, environmental failures due power outages notreported Tandem seen local problem. Data operationfaults difficult collect operators must report personal mistakes,which may affect opinion managers, turn affect job secu-rity pay raises. Gray suggested environmental faults operatorfaults underreported. study concluded achieving higher availability requires improvement software quality software fault tolerance, simpler operations, tolerance operational faults. Studies Role Operators Dependability Tertiary Disk Tandem storage-oriented dependability studies, weneed look outside storage find better measurements role humansin failures. Murphy Gent [1995] tried improve accuracy data onD.3 Definition Examples Real Faults Failures ■D-13operator faults system automatically prompt operator boot reason reboot. classified consecutive crashes samefault operator fault included operator actions directly resulted incrashes, giving parameters bad values, bad configurations, bad appli-cation installation. Although believed operator error under-reported,Unknown Environment (power, network) Operations (by customer) Maintenance (by Tandem) Hardware Software (applications + OS) 204060Faults per 1000 systems Percentage faults per category80100120 100% 80%4% 6% 9% 19% 29% 34%5% 6% 15% 5% 7% 62%5% 9% 12% 13% 22% 39%60% 40% 20% 0%0 9891 7891 5891 9891 7891 5891 Figure D.7 Faults Tandem 1985 1989. Gray [1990] collected data fault-tolerant Tandem Computers based reports component failures customers.D-14 ■Appendix Storage Systemsthey get accurate information Gray, relied form operator filled sent management chain. hardware/operating system went causing 70% failures VAX systems 1985 28% in1993, failures due operators rose 15% 52% period. Mur-phy Gent expected managing systems primary dependability chal-lenge future. final set data comes government. Federal Communications Commission (FCC) requires telephone companies submit explanationswhen experience outage affects least 30,000 people lasts 30 minutes. detailed disruption reports suffer self-reporting problem earlier figures, investigators determine cause outage ratherthan operators equipment. Kuhn [1997] studied causes outagesbetween 1992 1994, Enriquez [2001] follow-up study firsthalf 2001. Although significant improvement failures due over-loading network years, failures due humans increased, aboutone-third two-thirds customer-outage minutes. four examples others suggest primary cause failures large systems today faults human operators. Hardware faults declined due decreasing number chips systems fewer connectors. Hardwaredependability improved fault tolerance techniques memoryECC RAID. least operating systems considering reliability impli-cations adding new features, 2011 failures largely occurredelsewhere. Although failures may initiated due faults operators, poor reflec- tion state art systems processes maintenance upgrad- ing error prone. storage vendors claim today customers spend much managing storage lifetime purchasingthe storage. Thus, challenge dependable storage systems future iseither tolerate faults operators avoid faults simplifying tasksof system administration. Note RAID 6 allows storage system surviveeven operator mistakenly replaces good disk. covered bedrock issue dependability, giving definitions, case studies, techniques improve it. next step storage tour performance. D.4 I/O Performance, Reliability Measures, Benchmarks I/O performance measures counterparts design. One diversity: I/O devices connect computer system? Another iscapacity: many I/O devices connect computer system? addition unique measures, traditional measures performance (namely, response time throughput) also apply I/O. (I/O throughput some-times called I/O bandwidth response time sometimes called latency. ) next two figures offer insight response time throughput trade offD.4 I/O Performance, Reliability Measures, Benchmarks ■D-15against other. Figure D.8 shows simple producer-server model. pro- ducer creates tasks performed places buffer; server takestasks first in, first buffer performs them. Response time defined time task takes moment placed buffer server finishes task. Throughput simply average num-ber tasks completed server time period. get highest possible throughput, server never idle, thus buffer never empty. Response time, hand, counts time spent buffer, emptybuffer shrinks it. Another measure I/O performance interference I/O processor execution. Transferring data may interfere execution another process.There also overhead due handling I/O interrupts. concern howmuch longer process take I/O another process. Throughput versus Response Time Figure D.9 shows throughput versus response time (or latency) typical I/O system. knee curve area little throughput results much longer response time or, conversely, little shorter response time resultsin much lower throughput. architect balance conflicting demands? computer interacting human beings, Figure D.10 suggests answer. interaction, transaction , computer divided three parts: 1.Entry time —The time user enter command. 2.System response time —The time user enters command complete response displayed. 3.Think time —The time reception response user begins enter next command. sum three parts called transaction time . Several studies report user productivity inversely proportional transaction time. results inFigure D.10 show cutting system response time 0.7 seconds saves 4.9 seconds (34%) conventional transaction 2.0 seconds (70%) fromrevreS recudorPQueue Figure D.8 traditional producer-server model response time throughput. Response time begins task placed buffer ends completedby server. Throughput number tasks completed server unit time.D-16 ■Appendix Storage Systems300 0% Percenta ge maximum throu ghput (bandwidth)Response time (latency) (ms) 20% 40% 60% 80% 100%200 100 0 Figure D.9 Throughput versus response time. Latency normally reported response time. Note minimum response time achieves 11% thethroughput, response time 100% throughput takes seven times min- imum response time. Note also independent variable curve implicit; trace curve, typically vary load (concurrency). Chen et al. [1990] collected thesedata array magnetic disks. 0 Time (sec)High-function graphics workload (0.3 sec system response time) 5 10 15High-function graphics workload (1.0 sec system response time)Conventional interactive workload (0.3 sec system response time)Conventional interactive workload (1.0 sec system response time)Workload –70% total (–81% think)–34% total(–70% think) Entry time System response time Think time Figure D.10 user transaction interactive computer divided entry time, system response time, user think time conventional system graphics system. entry times same, independent system response time. entry time 4 seconds conventional system 0.25 seconds graphics sys- tem. Reduction response time actually decreases transaction time response time reduction. (From Brady [1986].)D.4 I/O Performance, Reliability Measures, Benchmarks ■D-17the graphics transaction. implausible result explained human nature: People need less time think given faster response. Although studyis 20 years old, response times often still much slower 1 second, even ifprocessors 1000 times faster. Examples long delays include starting appli-cation desktop PC due many disk I/Os, network delays clicking onWeb links. reflect importance response time user productivity, I/O bench- marks also address response time versus throughput trade-off. Figure D.11 shows response time bounds three I/O benchmarks. report maximum throughput given either 90% response times must less limit thatthe average response time must less limit. Let’s next look benchmarks detail. Transaction-Processing Benchmarks Transaction processing (TP, OLTP online transaction processing) chiefly concerned I/O rate (the number disk accesses per second), opposed data rate (measured bytes data per second). TP generally involves changes large body shared information many terminals, TP system guaranteeing proper behavior failure. Suppose, example, bank ’s computer fails customer tries withdraw money ATM. TheTP system would guarantee account debited customer receivedthe money andthat account unchanged money received. Air- line reservations systems well banks traditional customers TP. mentioned Chapter 1 , two dozen members TP community con- spired form benchmark industry and, avoid wrath legal departments, published report anonymously [Anon. et al. 1985]. report led Transaction Processing Council , turn led eight benchmarks since founding. Figure D.12 summarizes benchmarks. Let’s describe TPC-C give flavor benchmarks. TPC-C uses data- base simulate order-entry environment wholesale supplier, includingI/O benchmark Response time restrictionThroughput metric TPC-C: Complex Query OLTP/C2190% transaction must meet response time limit; 5 seconds types transactionsNew order transactions perminute TPC-W: Transactional Web benchmark/C2190% Web interactions must meet response time limit; 3 seconds types Web interactionsWeb interactions per second SPECsfs97 Average response time /C2040 ms NFS operations per second Figure D.11 Response time restrictions three I/O benchmarks.D-18 ■Appendix Storage Systemsentering delivering orders, recording payments, checking status orders, monitoring level stock warehouses. runs five concurrent trans-actions varying complexity, database includes nine tables scalablerange records customers. TPC-C measured transactions per minute(tpmC) price system, including hardware, software, three years maintenance support. Figure 1.17 page 42 Chapter 1 describes top sys- tems performance cost-performance TPC-C. TPC benchmarks first —and cases still ones — unusual characteristics: ■Price included benchmark results. cost hardware, software, maintenance agreements included submission, enables eval-uations based price-performance well high performance. ■The dataset generally must scale size throughput increases . benchmarks trying model real systems, demand sys-tem size data stored increase together. makes sense, example, thousands people per minute access hundreds bank accounts. ■The benchmark results audited . results submitted, must approved certified TPC auditor, enforces TPC rules try tomake sure fair results submitted. Results challenged anddisputes resolved going TPC. ■Throughput performance metric, response times limited . example, TPC-C, 90% new order transaction response times mustbe less 5 seconds.Benchmark Data size (GB) Performance metricDate first results A: debit credit (retired) 0.1 –10 Transactions per second July 1990 B: batch debit credit (retired) 0.1 –10 Transactions per second July 1991 C: complex query OLTP 100 –3000 (minimum 0.07*TPM)New order transactions per minute (TPM)September 1992 D: decision support (retired) 100, 300, 1000 Queries per hour December 1995 H: ad hoc decision support 100, 300, 1000 Queries per hour October 1999R: business reporting decision support (retired)1000 Queries per hour August 1999 W: transactional Web benchmark /C2550, 500 Web interactions per second July 2000 App: application server Web services benchmark/C252500 Web service interactions per second (SIPS)June 2005 Figure D.12 Transaction Processing Council benchmarks. summary results include performance met- ric price-performance metric. TPC-A, TPC-B, TPC-D, TPC-R retired.D.4 I/O Performance, Reliability Measures, Benchmarks ■D-19■An independent organization maintains benchmarks . Dues collected TPC pay administrative structure including chief operating office. organization settles disputes, conducts mail ballots approval changes tobenchmarks, holds board meetings, on. SPEC System-Level File Server, Mail, Web Benchmarks SPEC benchmarking effort best known characterization processorperformance, created benchmarks file servers, mail servers, Webservers. Seven companies agreed synthetic benchmark, called SFS, evaluate systems running Sun Microsystems network file service (NFS). bench- mark upgraded SFS 3.0 (also called SPEC SFS97_R1) include support NFS version 3, using TCP addition UDP transport protocol, andmaking mix operations realistic. Measurements NFS systems ledto synthetic mix reads, writes, file operations. SFS supplies default param-eters comparative performance. example, half writes done 8 KBblocks half done partial blocks 1, 2, 4 KB. reads, mix 85%full blocks 15% partial blocks. Like TPC-C, SFS scales amount data stored according reported throughput: every 100 NFS operations per second, capacity must increase 1 GB. also limits average response time, case 40 ms. Figure D.13 01235 46Response time (ms)78 0 150,000 125,00034,089 2 Xeons FAS3000 FAS60004 Xeons 8 Opterons 4 Opterons47,927 100,295136,048 100,000 75,000 Operations/second50,000 25,000 Figure D.13 SPEC SFS97_R1 performance NetApp FAS3050c NFS servers two configurations. Two processors reached 34,089 operations per second four processors 47,927. Reported May 2005, systems used Data ONTAP 7.0.1R1 operating system, 2.8 GHz Pentium Xeon microprocessors, 2 GB DRAM per processor, 1 GB nonvolatile memory per system, 168 15 K RPM, 72 GB, FibreChannel disks. disks connected using two four QLogic ISP-2322 FC diskcontrollers.D-20 ■Appendix Storage Systemsshows average response time versus throughput two NetApp systems. Unfor- tunately, unlike TPC benchmarks, SFS normalize different price configurations. SPECMail benchmark help evaluate performance mail servers Internet service provider. SPECMail2001 based standard Internet proto-cols SMTP POP3, measures throughput user response time whilescaling number users 10,000 1,000,000. SPECWeb benchmark evaluating performance World Wide Web servers, measuring number simultaneous user sessions. SPECWeb2005 workload simulates accesses Web service provider, server supports home pages several organizations. three workloads: Banking (HTTPS),E-commerce (HTTP HTTPS), Support (HTTP). Examples Benchmarks Dependability TPC-C benchmark fact dependability requirement. bench- marked system must able handle single disk failure, means practice submitters running RAID organization storagesystem. Efforts recent focused effectiveness fault tolerance systems. Brown Patterson [2000] proposed availability measured byexamining variations system quality-of-service metrics time faultsare injected system. Web server, obvious metrics performance(measured requests satisfied per second) degree fault tolerance (measured number faults tolerated storage subsystem, network connection topology, forth). initial experiment injected single fault —such write error disk sec- tor—and recorded system ’s behavior reflected quality-of-service met- rics. example compared software RAID implementations provided Linux,Solaris, Windows 2000 Server. SPECWeb99 used provide workloadand measure performance. inject faults, one SCSI disks softwareRAID volume replaced emulated disk. PC running software using SCSI controller appears devices SCSI bus disk. disk emulator allowed injection faults. faults injected included varietyof transient disk faults, correctable read errors, permanent faults, suchas disk media failures writes. Figure D.14 shows behavior system different faults. two top graphs show Linux (on left) Solaris (on right). RAID systemscan lose data second disk fails reconstruction completes, longer thereconstruction (MTTR), lower availability. Faster reconstruction implies decreased application performance, however, reconstruction steals I/O resources running applications. Thus, policy choice takinga performance hit reconstruction lengthening window vulnerabilityand thus lowering predicted MTTF.D.4 I/O Performance, Reliability Measures, Benchmarks ■D-21Although none tested systems documented reconstruction policies outside source code, even single fault injection able give insight intothose policies. experiments revealed Linux Solaris initiate auto-matic reconstruction RAID volume onto hot spare active disk istaken service due failure. Although Windows supports RAID0 1 02 03 04 05 06 07 08 09 0 1 0 0 1 1 0 0 1 02 03 04 0Reconstruction 50 60 70 80 90 100 110 0 5 10 15 20 25 30 35 40 45Time (minutes) ReconstructionHits per second Hits per secondHits per secondsiraloS xuniL WindowsTime (minutes) Time (minutes)Reconstruction 200 190 180170 160 150220225 215 210205 200 195 190 8090100110120130140150160 Figure D.14 Availability benchmark software RAID systems computer running Red Hat 6.0 Linux, Solaris 7, Windows 2000 operating systems. Note difference philosophy speed reconstruction Linux versus Windows Solaris. y-axis behavior hits per second running SPECWeb99. arrow indicates time fault insertion. lines top give 99% confidence interval performance fault isinserted. 99% confidence interval means variable outside range, probability 1% value would appear.D-22 ■Appendix Storage Systemsreconstruction, reconstruction must initiated manually. Thus, without human intervention, Windows system rebuild first failure remains susceptible second failure, increases window vulnerabil-ity. repair quickly told so. fault injection experiments also provided insight availability policies Linux, Solaris, Windows 2000 concerning automatic spare utiliza-tion, reconstruction rates, transient errors, on. Again, system documentedtheir policies. terms managing transient faults, fault injection experiments revealed Linux ’s software RAID implementation takes opposite approach RAID implementations Solaris Windows. Linux implementationis paranoid —it would rather shut disk controlled manner first error, rather wait see error transient. contrast, Solaris Win-dows forgiving —they ignore transient faults expectation recur. Thus, systems substantially robust tran-sients Linux system. Note Windows Solaris log tran-sient faults, ensuring errors reported even acted upon. faults permanent, systems behaved similarly. D.5 Little Queuing Theory processor design, simple back-of-the-envelope calculations perfor- mance associated CPI formula Chapter 1 , use full-scale sim- ulation greater accuracy greater cost. I/O systems, also bestcaseanalysis back-of-the-envelope calculation. Full-scale simulation also muchmore accurate much work calculate expected performance. I/O systems, however, also mathematical tool guide I/O design little work much accurate best-case analysis,but much less work full-scale simulation. probabilistic natureof I/O events sharing I/O resources, give set simpletheorems help calculate response time throughput entire I/O sys-tem. helpful field called queuing theory . Since many books courses subject, section serves first introduction topic.However, even small amount lead better design I/O systems. Let’s start black-box approach I/O systems, shown Figure D.15 . example, processor making I/O requests arrive I/O device,and requests “depart ”when I/O device fulfills them. usually interested long term, steady state, system rather initial start-up conditions. Suppose ’t. Although mathematics helps (Markov chains), except cases, wayto solve resulting equations simulation. Since purpose sectionis show something little harder back-of-the-envelope calculations less simulation, ’t cover analyses here. (See references Appen- dix details.)D.5 Little Queuing Theory ■D-23Hence, section make simplifying assumption evalu- ating systems multiple independent requests I/O service equi-librium: input rate must equal output rate. also assume asteady supply tasks independent long wait service. many realsystems, TPC-C, task consumption rate determined systemcharacteristics, memory capacity. leads us Little ’s law, relates average number tasks system, average arrival rate new tasks, average time perform atask: Mean number tasks system ¼Arrival rate /C2Mean response time Little ’s law applies system equilibrium, long nothing inside black box creating new tasks destroying them. Note arrival rate response time must use time unit; inconsistency time units common cause errors. Let’s try derive Little ’s law. Assume observe system Time observe minutes. observation, record long took task serviced, sum times. number tasks completed duringTime observe Number task, sum times task spends system Time accumulated . Note tasks overlap time, Time accumulated /C21 Time observed . Then, Mean number tasks system ¼Time accumulated Time observe Mean response time ¼Time accumulated Number tasks Arrival rate ¼Number tasks Time observe Algebra lets us split first formula: Time accumulated Time observe¼Time accumulated Number tasks∞Number tasks Time observeArrivals Departures Figure D.15 Treating I/O system black box. leads simple impor- tant observation: system steady state, number tasks entering thesystem must equal number tasks leaving system. flow-balanced state necessary sufficient steady state. system observed mea- sured sufficiently long time mean waiting times stabilize, say thesystem reached steady state.D-24 ■Appendix Storage SystemsIf substitute three definitions formula, swap resulting two terms right-hand side, get Little ’s law: Mean number tasks system ¼Arrival rate /C2Mean response time simple equation surprisingly powerful, shall see. open black box, see Figure D.16 . area tasks accu- mulate, waiting serviced, called queue ,o rwaiting line . device per- forming requested service called server . get last two pages section, assume single server. Little ’s law series definitions lead several useful equations: ■Time server—Average time service task; average service rate 1/Time server, traditionally represented symbol μin many queuing texts. ■Time queue—Average time per task queue. ■Time system —Average time/task system, response time, sum Time queue Time server. ■Arrival rate —Average number arriving tasks/second, traditionally represented symbol λin many queuing texts. ■Length server—Average number tasks service. ■Length queue—Average length queue. ■Length system —Average number tasks system, sum Length queue Length server. One common misunderstanding made clearer definitions: whether question long task must wait queue service starts (Time- queue) long task takes completed (Time system ). latter term mean response time, relationship terms isTime system¼Time queue+Time server. mean number tasks service (Length server) simply Arrival rate /C2 Time server, Little ’s law. Server utilization simply mean number tasks serviced divided service rate. single server, servicerate 1/Time server. Hence, server utilization (and, case, mean number tasks per server) simply: Server utilization ¼Arrival rate /C2Time serverArrivalsQueue Server I/O controller device Figure D.16 single-server model section. situation, I/O request “departs ”by completed server.D.5 Little Queuing Theory ■D-25Service utilization must 0 1; otherwise, would tasks arriving could serviced, violating assumption system equi-librium. Note formula restatement Little ’s law. Utilization also called traffic intensity andisrepresentedbythesymbol ρinmanyqueuingtheorytexts. Example Suppose I/O system single disk gets average 50 I/O requests per second. Assume average time disk service I/O request 10 ms. utilization I/O system? Answer Using equation above, 10 ms represented 0.01 seconds, get: 50 Server utilization ¼Arrival rate /C2Time server¼50 sec/C20:01sec ¼0:50 Therefore, I/O system utilization 0.5. queue delivers tasks server called queue discipline . sim- plest common discipline first in, first (FIFO). assume FIFO, relate time waiting queue mean number tasks queue: Time queue¼Lengthqueue/C2Time server+ Mean time complete service task new task arrives server busy is, time queue number tasks queue times mean service time plus time takes server complete whatever task beingserviced new task arrives. (There one restriction arrivalof tasks, reveal page D-28.) last component equation simple first appears. new task arrive instant, basis know long existing task server. Although requests random events, knowsomething distribution events, predict performance. Poisson Distribution Random Variables estimate last component formula need know little dis- tributions random variables . variable random takes one specified set values specified probability; is, cannot know exactly itsnext value be, may know probability possible values. Requests service I/O system modeled random variable operating system normally switching several processes thatgenerate independent I/O requests. also model I/O service times randomvariable given probabilistic nature disks terms seek rotational delays. One way characterize distribution values random variable discrete values histogram , divides range minimum maximum values subranges called buckets . Histograms plot number bucket columns.D-26 ■Appendix Storage SystemsHistograms work well distributions discrete values —for example, number I/O requests. distributions discrete values, time waiting I/O request, two choices. Either need curve plotthe values full range, estimate accurately value, weneed fine time unit get large number buckets estimatetime accurately. example, histogram built disk service times mea-sured intervals 10 μs although disk service times truly continuous. Hence, able solve last part previous equation need char- acterize distribution random variable. mean time measure variance sufficient characterization. first term, use weighted arithmetic mean time . Let’s first assume measuring number occurrences, say, n i, tasks, could compute frequency occurrence task i: fi¼ni Xn i¼1ni ! weighted arithmetic mean Weighted arithmetic mean time ¼f1/C2T1+f2/C2T2+…+fn/C2Tn Tiis time task iandfiis frequency occurrence task i. characterize variability mean, many people use standard devi- ation. Let ’s use variance instead, simply square standard deviation, help us characterizing probability distribution. Giventhe weighted arithmetic mean, variance calculated Variance ¼f1/C2T2 1+f2/C2T2 2+…+fn/C2T2 n/C0/C1 /C0Weighted arithmetic mean time2 important remember units computing variance. Let ’s assume distribution time. time 100 milliseconds, squaring yields10,000 square milliseconds. unit certainly unusual. would moreconvenient unitless measure. avoid unit problem, use squared coefficient variance , traditionally called C 2: C2¼Variance Weighted arithmetic mean time2 solve C, coefficient variance, C¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Variancep Weighted arithmetic mean time¼Standard deviation Weighted arithmetic mean time trying characterize random events, able predict performance need distribution random events mathematics tractable. popular distribution exponential distribution , C value 1. Note using constant characterize variability mean. invariance C time reflects property history events impactD.5 Little Queuing Theory ■D-27on probability event occurring now. forgetful property called mem- oryless , property important assumption used predict behavior using models. (Suppose memoryless property exist; then, would haveto worry exact arrival times requests relative other, wouldmake mathematics considerably less tractable!) One widely used exponential distributions called Poisson dis- tribution , named mathematician Sim /C19eon Poisson. used characterize random events given time interval several desirable mathematicalproperties. Poisson distribution described following equation (called probability mass function): Probability kðÞ ¼e/C0a/C2ak k! a¼Rate events /C2Elapsed time. interarrival times exponentially dis- tributed use arrival rate rate events, number arrivals time interval tis aPoisson process , Poisson distribution a¼Arrival rate /C2t. mentioned page D-26, equation Time server another restriction task arrival: holds Poisson processes. Finally, answer question length time new task must wait server complete task, called average residual service time , assumes Poisson arrivals: Average residual service time ¼1=2/C2Arithemtic mean /C21+C2/C0/C1 Although ’t derive formula, appeal intuition. dis- tribution random possible values equal average, standard deviation 0 C 0. average residual service time half theaverage service time, would expect. distribution random isPoisson, C 1 average residual service time equals weighted arith-metic mean time. Example Using definitions formulas above, derive average time waiting queue (Time queue) terms average service time (Time server) server utilization. Answer tasks queue (Length queue) ahead new task must completed task serviced; takes average Time server. task server, takes average residual service time complete. chance serveris busy server utilization ; hence, expected time service Server utiliza- tion/C2Average residual service time. leads initial formula: Time queue¼Lengthqueue/C2Time server + Server utilization /C2Average residual service time Replacing average residual service time definition Length queue Arrival rate /C2Time queue yieldsD-28 ■Appendix Storage SystemsTime queue¼Server utilization /C21=2/C2Time server/C21+C2/C0/C1 /C2/C3 + Arrival rate /C2Time queue/C0/C1 /C2Time server Since section concerned exponential distributions, C2is 1. Thus Time queue¼Server utilization /C2Time server+ Arrival rate /C2Time queue/C0/C1 /C2Time server Rearranging last term, let us replace Arrival rate /C2Time server Server utilization: Time queue¼Server utilization /C2Time server+ Arrival rate /C2Time server ðÞ /C2 Time queue ¼Server utilization /C2Time server+ Server utilization /C2Time queue Rearranging terms simplifying gives us desired equation: Time queue¼Server utilization /C2Time server+ Server utilization /C2Time queue Time queue/C0Server utilization /C2Time queue¼Server utilization /C2Time server Time queue/C21/C0Server utilizationðÞ ¼ Server utilization /C2Time server Time queue¼Time server/C2Server utilization 1/C0Server utilizationðÞ Little ’s law applied components black box well, since must also equilibrium: Lengthqueue¼Arrival rate /C2Time queue substitute Time queue above, get: Lengthqueue¼Arrival rate /C2Time server/C2Server utilization 1/C0Server utilizationðÞ Since Arrival rate /C2Time server¼Server utilization, simplify further: Lengthqueue¼Server utilization /C2Server utilization 1/C0Server utilizationðÞ¼Server utilization2 1/C0Server utilizationðÞ relates number items queue service utilization. Example system example page D-26, server utilization 0.5, mean number I/O requests queue? Answer Using equation above, Lengthqueue¼Server uti1ization2 1/C0Server uti1izationðÞ¼0:52 1/C00:5 ðÞ¼0:25 0:50¼0:5 Therefore, 0.5 requests average queue. mentioned earlier, equations section based area applied mathematics called queuing theory , offers equations predictD.5 Little Queuing Theory ■D-29behavior random variables. Real systems complex queuing theory provide exact analysis, hence queuing theory works best approximate answers needed. Queuing theory makes sharp distinction past events, characterized measurements using simple arithmetic, future events, whichare predictions requiring sophisticated mathematics. computer systems,we commonly predict future past; one example least recently usedblock replacement (see Chapter 2 ). Hence, distinction measurements predicted distributions often blurred; use measurements verify type distribution rely distribution thereafter. Let’s review assumptions queuing model: ■The system equilibrium. ■The times two successive requests arriving, called interarrival times , exponentially distributed, characterizes arrival rate mentionedearlier. ■The number sources requests unlimited. (This called infinite population model queuing theory; finite population models used arrival rates vary number jobs already system.) ■The server start next job immediately finishing prior one. ■There limit length queue, follows first in, first outorder discipline, tasks line must completed. ■There one server. queue called M/M/1: M5exponentially random request arrival (C2¼1), Mstanding A. A. Markov, mathematician defined analyzed memorylessprocesses mentioned earlier M5exponentially random service time (C 2¼1), Magain Markov 1¼single server M/M/1 model simple widely used model. assumption exponential distribution commonly used queuing exam- ples three reasons —one good, one fair, one bad. good reason superpositionofmanyarbitrarydistributionsactsasanexponentialdistribution.Manytimes computer systems, particular behavior result many componentsinteracting, exponential distribution interarrival times right model.The fair reason variability unclear, exponential distribution withintermediate variability (C ¼1) safer guess low variability (C /C250) high variability (large C). bad reason math simpler assume expo- nential distributions.D-30 ■Appendix Storage SystemsLet’s put queuing theory work examples. Example Suppose processor sends 40 disk I/Os per second, requests exponen- tially distributed, average service time older disk 20 ms. Answerthe following questions: 1.On average, utilized disk? 2.What average time spent queue? 3.What average response time disk request, including queuing time disk service time? Answer Let’s restate facts: Average number arriving tasks/second 40. Average disk time service task 20 ms (0.02 sec). server utilization Server utilization ¼Arrival rate /C2Time server¼40/C20:02¼0:8 Since service times exponentially distributed, use simplified for- mula average time spent waiting line: Time queue¼Time server/C2Server utilization 1/C0Server utilizationðÞ ¼20 ms /C20:8 1/C00:8¼20/C20:8 0:2¼20/C24¼80 ms average response time Time system ¼Time queue+ Time server¼80 + 20 ms ¼100 ms Thus, average spend 80% time waiting queue! Example Suppose get new, faster disk. Recalculate answers questions above, assuming disk service time 10 ms. Answer disk utilization Server utilization ¼Arrival rate /C2Time server¼40/C20:01¼0:4 formula average time spent waiting line: Time queue¼Time server/C2Server utilization 1/C0Server utilizationðÞ ¼10 ms /C20:4 1/C00:4¼10/C20:4 0:6¼10/C22 3¼6:7m average response time 10+6.7 ms 16.7 ms, 6.0 times faster old response time even though new service time 2.0 times faster.D.5 Little Queuing Theory ■D-31Thus far, assuming single server, single disk. Many real systems multiple disks hence could use multiple servers, inFigure D.17 . system called M/M/m model queuing theory. Let’s give formulas M/M/m queue, using N servers represent number servers. first two formulas easy: Utilization ¼Arrival rate /C2Time server Nservers Lengthqueue¼Arrival rate /C2Time queue time waiting queue Time queue¼Time server/C2Ptasks/C21Nservers Nservers/C21/C0UtilizationðÞ formula related one M/M/1, except replace utilization single server probability task queued opposed immediately serviced, divide time queue number servers. Alas, calculating probability jobs queue much compli-cated N servers . First, probability tasks system Prob 0 tasks¼1+Nservers/C2Utilization ðÞNservers Nservers !/C21/C0UtilizationðÞ+XNservers/C01 n¼1Nservers/C2Utilization ðÞn n!"#/C01 probability many tasks servers Prob tasks/C21Nservers¼Nservers/C2UtilizationNservers Nservers !/C21/C0UtilizationðÞ/C2Prob 0 tasksArrivalsQueueServer I/O controller device Server I/O controller deviceServer I/O controller device Figure D.17 M/M/m multiple-server model.D-32 ■Appendix Storage SystemsNote N servers 1, Prob task/C21Nserverssimplifies back Utilization, get formula M/M/1. Let ’s try example. Example Suppose instead new, faster disk, add second slow disk duplicate data reads serviced either disk. Let ’s assume requests reads. Recalculate answers earlier questions, time using M/M/ queue. Answer average utilization two disks Server utilization ¼Arrival rate /C2Time server Nservers¼40/C20:02 2¼0:4 first calculate probability tasks queue: Prob 0 tasks¼1+2/C2UtilizationðÞ2 2!/C21/C0UtilizationðÞ+X1 n¼12/C2UtilizationðÞn n!"#/C01 ¼1+2/C20:4 ðÞ2 2/C21/C00:4 ðÞ+2/C20:4 ðÞ"#/C01 ¼1+0:640 1:2+0:800/C20/C21/C01 ¼1+0:533 + 0 :800 ½/C138/C01¼2:333/C01 use result calculate probability tasks queue: Prob tasks/C21Nservers¼2/C2Utilization2 2!/C21/C0UtilizationðÞ/C2Prob 0 tasks ¼2/C20:4 ðÞ2 2/C21/C00:4 ðÞ/C22:333/C01¼0:640 1:2/C22:333/C01 ¼0:533=2:333¼0:229 Finally, time waiting queue: Time queue¼Time server/C2Prob tasks/C21Nservers Nservers/C21/C0UtilizationðÞ ¼0:020/C20:229 2/C21/C00:4 ðÞ¼0:020/C20:229 1:2 ¼0:020/C20:190¼0:0038 average response time 20+3.8 ms 23.8 ms. workload, two disks cut queue waiting time factor 21 single slow disk factor 1.75 versus single fast disk. mean service time system single fastdisk, however, still 1.4 times faster one two disks since disk servicetime 2.0 times faster.D.5 Little Queuing Theory ■D-33It would wonderful could generalize M/M/m model multiple queues multiple servers, step much realistic. Alas, models veryhard solve use, ’t cover here. D.6 Crosscutting Issues Point-to-Point Links Switches Replacing Buses Point-to-point links switches increasing popularity Moore ’s law con- tinues reduce cost components. Combined higher I/O bandwidth demands faster processors, faster disks, faster local area networks, thedecreasing cost advantage buses means days buses desktop servercomputers numbered. trend started high-performance computers thelast edition book, 2011 spread throughout storage.Figure D.18 shows old bus-based standards replacements. number bits bandwidth new generation per direction, double directions. Since new designs use many fewer wires, common way increase bandwidth offer versions several times num- ber wires bandwidth. Block Servers versus Filers Thus far, largely ignored role operating system storage. amanner analogous way compilers use instruction set, operating systemsdetermine I/O techniques implemented hardware actually used.The operating system typically provides file abstraction top blocks storedon disk. terms logical units ,logical volumes , physical volumes related terms used Microsoft UNIX systems refer subset collections disk blocks. StandardWidth (bits)Length (meters) Clock rate MB/secMax I/O devices (Parallel) ATA 8 0.5 133 MHz 133 2 Serial ATA 2 2 3 GHz 300 ? SCSI 16 12 80 MHz 320 15 Serial Attach SCSI 1 10 (DDR) 375 16,256 PCI 32/64 0.5 33/66 MHz 533 ? PCI Express 2 0.5 3 GHz 250 ? Figure D.18 Parallel I/O buses point-to-point replacements. Note bandwidth wires per direction, bandwidth doubles sending bothdirections.D-34 ■Appendix Storage SystemsA logical unit element storage exported disk array, usually con- structed subset array ’s disks. logical unit appears server single virtual “disk.”In RAID disk array, logical unit configured par- ticular RAID layout, RAID 5. physical volume device file used bythe file system access logical unit. logical volume provides level vir-tualization enables file system split physical volume across multiplepieces stripe data across multiple physical volumes. logical unit anabstraction disk array presents virtual disk operating system, whilephysical logical volumes abstractions used operating system divide virtual disks smaller, independent file systems. covered terms collections blocks, must ask: file illusion maintained: server end thestorage area network? traditional answer server. accesses storage disk blocks maintains metadata. file systems use file cache, server must main-tain consistency file accesses. disks may direct attached —found inside server connected I/O bus —or attached storage area network, server transmits data blocks storage subsystem. alternative answer disk subsystem maintains file abstraction, server uses file system protocol communicate withstorage. Example protocols Network File System (NFS) UNIX systemsand Common Internet File System (CIFS) Windows systems. devicesare called network attached storage (NAS) devices since makes sense storage directly attached server. name something misnomerbecause storage area network like FC-AL also used connect block servers. term fileris often used NAS devices provide file service file storage. Network Appliance one first companies makefilers. driving force behind placing storage network make easier many computers share information operators maintain sharedsystem. Asynchronous I/O Operating Systems Disks typically spend much time mechanical delays transferringdata. Thus, natural path higher I/O performance parallelism, trying get many disks simultaneously access data program. straightforward approach I/O request data start using it. operating system switches another process desired data arrive,and operating system switches back requesting process. styleis called synchronous I/O —the process waits data read disk. alternative model process continue making request, blocked tries read requested data. asynchronous I/OD.6 Crosscutting Issues ■D-35allows process continue making requests many I/O requests operating simultaneously. Asynchronous I/O shares philosophy caches out-of-order CPUs, achieve greater bandwidth multiple out-standing events. D.7 Designing Evaluating I/O System — Internet Archive Cluster art I/O system design find design meets goals cost, depend- ability, variety devices avo iding bottlenecks I/O performance dependability. Avoiding bottlenecks means components must bal-anced main memory I/O device, performance anddependability —and hence effective cost-performance cost-dependability — good weakest link I/O chain. architect must alsoplan expansion customers tailor I/O applications. Thisexpansibility, numbers types I/O devices, costs longer I/Obuses networks, larger power supplies support I/O devices, larger cabinets. designing I/O system, analyze performance, cost, capacity, avail- ability using varying I/O connection schemes different numbers I/O devicesof type. one series steps follow designing I/O system. Theanswers step may dictated market requirements simply cost,performance, availability goals. 1.List different types I/O devices connected machine, list standard buses networks machine support. 2.List physical requirements I/O device. Requirements include size, power, connectors, bus slots, expansion cabinets, on. 3.List cost I/O device, including portion cost controller needed device. 4.List reliability I/O device. 5.Record processor resource demands I/O device. list include: ■Clock cycles instructions used initiate I/O, support operation ofan I/O device (such handling interrupts), complete I/O ■Processor clock stalls due waiting I/O finish using memory, bus,or cache ■Processor clock cycles recover I/O activity, cache flush 6.List memory I/O bus resource demands I/O device. Even processor using memory, bandwidth main memory I/Oconnection limited.D-36 ■Appendix Storage Systems7.The final step assessing performance availability different ways organize I/O devices. afford it, try avoid single points failure. Performance properly evaluated simulation,although may estimated using queuing theory. Reliability calculatedassuming I/O devices fail independently times failure expo-nentially distributed. Availability computed reliability estimat-ing MTTF devices, taking account time failure repair. Given cost, performance, availability goals, select best organization. Cost-performance goals affect selection I/O scheme physical design. Performance measured either megabytes per second I/Osper second, depending needs application. high performance,the limits speed I/O devices, number I/O devices, speedof memory processor. low cost, cost I/O devicesthemselves. Availability goals depend part cost unavailability anorganization. Rather create paper design, let ’s evaluate real system. Internet Archive Cluster make ideas clearer, ’ll estimate cost, performance, availability large storage-oriented cluster Internet Archive. Internet Archivebegan 1996 goal making historical record Internet chan-ged time. use Wayback Machine interface Internet Archiveto perform time travel see Web site URL looked like sometime inthe past. contains petabyte (10 15bytes) growing 20 terabytes (1012bytes) new data per month, expansible storage requirement. addi- tion storing historical record, hardware used crawl Web every months get snapshots Internet. Clusters computers connected local area networks become economical computation engine works well applications. Clustersalso play important role Internet services Google search engine,where focus storage computation, case here. Although used variety hardware years, Internet Archive moving new cluster become efficient power floor space. basic building block 1U storage node called PetaBox GB2000 Capricorn Technologies. 2006, used four 500 GB Parallel ATA (PATA) diskdrives, 512 MB DDR266 DRAM, one 10/100/1000 Ethernet interface, a1 GHz C3 processor VIA, executes 80x86 instruction set. Thisnode dissipates 80 watts typical configurations. Figure D.19 shows cluster standard VME rack. Forty GB2000s fit standard VME rack, gives rack 80 TB raw capacity. 40nodes connected together 48-port 10/100 10/100/1000 switch, itD.7 Designing Evaluating I/O System —The Internet Archive Cluster ■D-37dissipates 3 KW. limit usually 10 KW per rack computer facilities, well within guidelines. petabyte needs 12 racks, connected higher-level switch connects Gbit links coming switches racks. Estimating Performance, Dependability, Cost Internet Archive Cluster illustrate evaluate I/O system, ’ll make guesses cost, performance, reliability components cluster. make following assumptions cost performance: ■The VIA processor, 512 MB DDR266 DRAM, ATA disk controller, power supply, fans, enclosure cost $500. ■Each four 7200 RPM Parallel ATA drives holds 500 GB, average time seek 8.5 ms, transfers 50 MB/sec disk, costs $375. PATA link speed 133 MB/sec. Figure D.19 TB-80 VME rack Capricorn Systems used Internet Archive. cables, switches, displays accessible front side, back side used airflow. allows two racks placed back-to-back, reduces floor space demands machine rooms.D-38 ■Appendix Storage Systems■The 48-port 10/100/1000 Ethernet switch cables rack cost $3000. ■The performance VIA processor 1000 MIPS. ■The ATA controller adds 0.1 ms overhead perform disk I/O. ■The operating system uses 50,000 CPU instructions disk I/O. ■The network protocol stacks use 100,000 CPU instructions transmit data block cluster external world. ■The average I/O size 16 KB accesses historical record via Way-back interface, 50 KB collecting new snapshot. Example Evaluate cost per I/O per second (IOPS) 80 TB rack. Assume every disk I/O requires average seek average rotational delay. Assume theworkload evenly divided among disks devices used at100% capacity; is, system limited weakest link, operate link 100% utilization. Calculate average I/O sizes. Answer I/O performance limited weakest link chain, evaluate max- imum performance link I/O chain organization determinethe maximum performance organization. Let’s start calculating maximum number IOPS CPU, main memory, I/O bus one GB2000. CPU I/O performance determinedby speed CPU number instructions perform disk I/O send network: Maximum IOPS CPU ¼1000 MIPS 50,000 instructions per =O + 100,000 instructions per message ¼6667 IOPS maximum performance memory system determined memory bandwidth size I/O transfers: Maximum IOPS main memory ¼266/C28 16 KB per =O/C25133,000 IOPS Maximum IOPS main memory ¼266/C28 50 KB per =O/C2542,500 IOPS Parallel ATA link performance limited bandwidth size I/O: Maximum IOPS =O bus ¼133 MB =sec 16 KB per =O/C258300 IOPS Maximum IOPS =O bus ¼133 MB =sec 50 KB per =O/C252700 IOPS Since box two buses, I/O bus limits maximum performance 18,600 IOPS 16 KB blocks 5400 IOPS 50 KB blocks.D.7 Designing Evaluating I/O System —The Internet Archive Cluster ■D-39Now ’s time look performance next link I/O chain, ATA controllers. time transfer block PATA channel Parallel ATA transfer time ¼16 KB 133 MB =sec/C250:1m Parallel ATA transfer time ¼50 KB 133 MB =sec/C250:4m Adding 0.1 ms ATA controller overhead means 0.2 ms 0.5 ms per I/O, mak- ing maximum rate per controller Maximum IOPS per ATA controller ¼1 0:2m s¼5000 IOPS Maximum IOPS per ATA controller ¼1 0:5m s¼2000 IOPS next link chain disks themselves. time average disk I/O I=O time ¼8:5m +0:5 7200 RPM+16 KB 50 MB =sec¼8:5+4:2+0:3¼13:0m I=O time ¼8:5m +0:5 7200 RPM+50 KB 50 MB =sec¼8:5+4:2+1:0¼13:7m Therefore, disk performance Maximum IOPS using average seeks ðÞ per disk ¼1 13:0m s/C2577 IOPS Maximum IOPS using average seeks ðÞ per disk ¼1 13:7m s/C2573 IOPS 292 308 IOPS four disks. final link chain network connects computers out- side world. link speed determines limit: Maximum IOPS per 1000 Mbit Ethernet link ¼1000 Mbit 16 K/C28¼7812 IOPS Maximum IOPS per 1000 Mbit Ethernet link ¼1000 Mbit 50 K/C28¼2500 IOPS Clearly, performance bottleneck GB2000 disks. IOPS whole rack 40 /C2308 12,320 IOPS 40 /C2292 11,680 IOPS. network switch would bottleneck ’t support 12,320 /C216 K/C28 1.6 Gbits/sec 16 KB blocks 11,680 /C250 K/C28 4.7 Gbits/sec 50 KB blocks. assume extra 8 Gbit ports 48-port switch connectsthe rack rest world, could support full IOPS collective160 disks rack. Using assumptions, cost 40 /C2($500+4 /C2$375)+ $3000+ $1500 $84,500 80 TB rack. disks almost 60% cost. cost per terabyte almost $1000, factor 10 15 better storage cluster prior edition 2001. cost per IOPS $7.D-40 ■Appendix Storage SystemsCalculating MTTF TB-80 Cluster Internet services Google rely many copies data application level provide dependability, often different geographic sites protect againstenvironmental faults well hardware faults. Hence, Internet Archive hastwo copies data site sites San Francisco, Amsterdam,and Alexandria, Egypt. site maintains duplicate copy high-value con-tent—music, books, film, video —and single copy historical Web crawls. keep costs low, redundancy 80 TB rack. Example Let’s look resulting mean time fail rack. Rather use man- ufacturer ’s quoted MTTF 600,000 hours, ’ll use data recent survey disk drives [Gray van Ingen 2005]. mentioned Chapter 1 , 3% 7% ATA drives fail per year, MTTF 125,000 300,000 hours.Make following assumptions, assuming exponential lifetimes: ■CPU/memory/enclosure MTTF 1,000,000 hours. ■PATA Disk MTTF 125,000 hours. ■PATA controller MTTF 500,000 hours. ■Ethernet Switch MTTF 500,000 hours. ■Power supply MTTF 200,000 hours. ■Fan MTTF 200,000 hours. ■PATA cable MTTF 1,000,000 hours. Answer Collecting together, compute failure rates: Failure rate ¼40 1,000,000+160 125,000+40 500,000+1 500,000+40 200,000+40 200,000+80 1,000,000 ¼40 + 1280 + 80 + 2 + 200 + 200 + 80 1,000,000 hours¼1882 1,000,000 hours MTTF system inverse failure rate: MTTF ¼1 Failure rate¼1,000,000 hours 1882¼531 hours is, given assumptions MTTF components, something rack fails average every 3 weeks. 70% failures would disks, 20% would fans power supplies. D.8 Putting Together: NetApp FAS6000 Filer Network Appliance entered storage market 1992 goal providing easy-to-operate file server running NSF using log-structured file systemand RAID 4 disk array. company later added support Windows CIFSD.8 Putting Together: NetApp FAS6000 Filer ■D-41file system RAID 6 scheme called row-diagonal parity orRAID-DP (see page D-8). support applications want access raw data blocks without overhead file system, database systems, NetApp filers servedata blocks standard Fibre Channel interface. NetApp also supports iSCSI , allows SCSI commands run TCP/IP network, thereby allowing theuse standard networking gear connect servers storage, Ethernet, andhence greater distance. latest hardware product FAS6000. multiprocessor based AMD Opteron microprocessor connected using HyperTransport links. microprocessors run NetApp software stack, including NSF, CIFS, RAID-DP, SCSI, on. FAS6000 comes either dual processor(FAS6030) quad processor (FAS6070). mentioned Chapter 5 , DRAM distributed microprocessor Opteron. FAS6000 connects 8 GBof DDR2700 Opteron, yielding 16 GB FAS6030 32 GB theFAS6070. mentioned Chapter 4 , DRAM bus 128 bits wide, plus extra bits SEC/DED memory. models dedicate four HyperTransport linksto I/O. filer, FAS6000 needs lot I/O connect disks connect servers. integrated I/O consists of: ■8 Fibre Channel (FC) controllers ports ■6 Gigabit Ethernet links ■6 slots x8 (2 GB/sec) PCI Express cards ■3 slots PCI-X 133 MHz, 64-bit cards ■Standard I/O options IDE, USB, 32-bit PCI 8 Fibre Channel controllers attached 6 shelves containing 14 3.5-inch FC disks. Thus, maximum number drives integrated I/O is8/C26/C214 672 disks. Additional FC controllers added option slots connect 1008 drives, reduce number drives per FC network asto reduce contention, on. 500 GB per FC drive, assume RAIDRDP group 14 data disks 2 check disks, available data capacity 294 TBfor 672 disks 441 TB 1008 disks. also connect Serial ATA disks via Fibre Channel SATA bridge controller, which, name suggests, allows FC SATA communicate. six 1-gigabit Ethernet links connect servers make FAS6000 look like file server running NTFS CIFS like block server running iSCSI. greater dependability, FAS6000 filers paired one fails, take over. Clustered failover requires filers access alldisks pair filers using FC interconnect. interconnect also allowseach filer copy log data NVRAM filer keepthe clocks pair synchronized. health filers constantly monitored, failover happens automatically. healthy filer maintains network identity primary functions, also assumes network identityD-42 ■Appendix Storage Systemsof failed filer handles data requests via virtual filer admin- istrator restores data service original state. D.9 Fallacies Pitfalls Fallacy Components fail fast good deal fault-tolerant literature based simplifying assumption component operates perfectly latent error becomes effective, failure occurs stops component. Tertiary Disk project opposite experience. Many components started acting strangely long failed, generally sys-tem operator determine whether declare component failed. compo-nent would generally willing continue act violation serviceagreement operator “terminated ”that component. Figure D.20 shows history four drives terminated, num- ber hours started acting strangely replaced. Fallacy Computers systems achieve 99.999% availability ( “five nines ”), advertised Marketing departments companies making servers started bragging availability computer hardware; terms Figure D.21 , claim avail- ability 99.999%, nicknamed five nines . Even marketing departments oper- ating system companies tried give impression. Five minutes unavailability per year certainly impressive, given failure data collected surveys, ’s hard believe. example, Hewlett-Packard claims HP-9000 server hardware HP-UX operating system deliver Messages system log failed diskNumber log messagesDuration (hours) Hardware Failure (Peripheral device write fault [for] Field Replaceable Unit)1763 186 Ready (Diagnostic failure: ASCQ ¼Component ID [of] Field Replaceable Unit)1460 90 Recovered Error (Failure Prediction Threshold Exceeded [for] Field Replaceable Unit)1313 5 Recovered Error (Failure Prediction Threshold Exceeded [for] Field Replaceable Unit)431 17 Figure D.20 Record system log 4 368 disks Tertiary Disk replaced 18 months. See Talagala Patterson [1999]. messages, match- ing SCSI specification, placed system log device drivers. Messages started occurring much week one drive replaced operator. Thethird fourth messages indicate drive ’s failure prediction mechanism detected predicted imminent failure, yet still hours drives replaced operator.D.9 Fallacies Pitfalls ■D-43a 99.999% availability guarantee “in certain pre-defined, pre-tested customer envi- ronments ”(see Hewlett-Packard [1998]). guarantee include failures due operator faults, application faults, environmental faults, likelythe dominant fault categories today. include scheduled downtime. isalso unclear financial penalty company system matchits guarantee. Microsoft also promulgated five nines marketing campaign. January 2001, www.microsoft.com unavailable 22 hours. Web site achieve 99.999% availability, require clean slate 250 years. contrast marketing suggestions, well-managed servers typically achieve 99% 99.9% availability. Pitfall function implemented affects reliability theory, fine move RAID function software. practice, difficult make work reliably. software culture generally based eventual correctness via series releases patches. also difficult isolate layers software. example, proper software behavior often based proper version patch release operating system. Thus, many customers lost data due tosoftware bugs incompatibilities environment software RAID systems. Obviously, hardware systems immune bugs, hardware culture tends place greater emphasis testing correctness initial release. Inaddition, hardware likely independent version oper-ating system. Fallacy Operating systems best place schedule disk accesses Higher-level interfaces ATA SCSI offer logical block addresses host operating system. Given high-level abstraction, best OS totry sort logical block addresses increasing order. Since diskknows mapping logical addresses onto physical geometry sectors,tracks, surfaces, reduce rotational seek latencies.Unavailability (minutes per year)Availability (percent)Availability class (“number nines ”) 50,000 90% 1 5000 99% 2 500 99.9% 3 50 99.99% 4 5 99.999% 50.5 99.9999% 60.05 99.99999% 7 Figure D.21 Minutes unavailable per year achieve availability class. (From Gray Siewiorek [1991].) Note five nines mean unavailable five minutes per year.D-44 ■Appendix Storage SystemsFor example, suppose workload four reads [Anderson 2003]: Operation Starting LBA Length Read 724 8 Read 100 16 Read 9987 1 Read 26 128 host might reorder four reads logical block order: Read 26 128 Read 100 16 Read 724 8 Read 9987 1 Depending relative location data disk, reordering could make worse, Figure D.22 shows. disk-scheduled reads complete three-quarters disk revolution, OS-scheduled reads take three revolutions. Fallacy time average seek disk computer system time seek one-third number cylinders fallacy comes confusing way manufacturers market disks expected performance, false assumption seek times linear dis- tance. one-third-distance rule thumb comes calculating distance seek one random location another random location, including current track assuming large number tracks. past, manufacturers listed seek distance offer consistent basis comparison. (Today, 724100 26 9987Host-ordered queue Drive-ordered queue Figure D.22 Example showing OS versus disk schedule accesses, labeled host- ordered versus drive-ordered. former takes 3 revolutions complete 4 reads, latter completes 3/4 revolution. (From Anderson [2003].)D.9 Fallacies Pitfalls ■D-45calculate “average ”by timing seeks dividing number.) Assuming (incorrectly) seek time linear distance, using manufacturer ’sr e p r e minimum “average ”seek times, common technique predict seek time Time seek¼Time minimum +Distance Distance average/C2Time average/C0Time minimum/C0/C1 fallacy concerning seek time twofold. First, seek time notlinear distance; arm must accelerate overcome inertia, reach maximum travelingspeed, decelerate reaches requested position, wait allow thearm stop vibrating ( settle time ). Moreover, sometimes arm must pause control vibrations. disks 200 cylinders, Chen Lee[1995] modeled seek distance as: Seek time Distance ðÞ ¼ a/C2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ Distance /C01p +b/C2Distance /C01 ðÞ +c a,b, care selected particular disk formula match quoted times Distance ¼1, Distance ¼max, Distance ¼1/3 max. Figure D.23 plots equation versus fallacy equation. Unlike first equa- tion, square root distance reflects acceleration deceleration. second problem average product specification would true locality disk activity. Fortunately, temporaland spatial locality (see page B-2 Appendix B ). example, Figure D.24 shows sample measurements seek distances two workloads: UNIX time-sharingworkload business-processing workload. Notice high percentage disk 30 25 20 15 10 5Access time (ms) 0 Seek distance0 250 500 750 1000 1250 1500Naive seek formula New seek formula 1750 2000 2250 2500 = 3 ×Number cylinders– 10 × Timemin+ 15 × Timeavg– 5 × Timemaxb = 3 × Number cylinders7 × Timemin– 15 × Timeavg+ 8 × Timemaxc =Timemin Figure D.23 Seek time versus seek distance sophisticated model versus naive model. Chen Lee [1995] found equations shown param- eters a,b, cworked well several disks.D-46 ■Appendix Storage Systemsaccesses cylinder, labeled distance 0 graphs, workloads. Thus, fallacy ’t misleading. D.10 Concluding Remarks Storage one technologies tend take granted. yet, look true status things today, storage king. One even argue thatservers, become commodities, becoming peripheral tostorage devices. Driving point home estimates IBM, whichexpects storage sales surpass server sales next two years. Michael Vizard Editor-in-chief, Infoworld (August 11, 2001) value becoming increasingly evident, storage systems become target innovation investment. challenges storage systems today dependability maintainabil- ity. users want sure data never lost (reliability),0% 10% Percenta ge seeks (UNIX time-sharin g workload)23%8%4% 20% 40% 30% 50% 60% 70%24%3% 3%1%3%3%3% 3% 3% 2%2% 0% 10% Percentage seeks (business workload)Seek distanceSeek distance 11% 20% 40% 30% 50% 60% 70%61%3%0% 3%0% 0% 1% 1% 1% 1% 1% 3%0% 195 180 165 150 135 120 105 90 75 6045 30 15 0208 192176160144 128 112 96 80 64 48 32 16 0 Figure D.24 Sample measurements seek distances two systems. measurements left taken UNIX time-sharing system. measurements right taken business-processing application disk seek activity scheduled improve throughput. Seek distance 0 means access made tothe cylinder. rest numbers show collective percentage distances numbers y- axis. example, 11% bar labeled 16 business graph means percentage seeks 1 16 cylinders 11%. UNIX measurements stopped 200 1000 cylinders, captured 85% theaccesses. business measurements tracked 816 cylinders disks. seek distances 1% greater seeks graph 224 4%, 304, 336, 512, 624, 1%. total 94%, difference small nonzero distances categories. Measurements courtesyof Dave Anderson Seagate.D.10 Concluding Remarks ■D-47applications today increasingly demand data always available access (availability). Despite improvements hardware software reliability fault tolerance, awkwardness maintaining systems problem costand availability. widely mentioned statistic customers spend $6t $8 operating storage system every $1 purchase price. dependability attacked many redundant copies higher level system —such search —then large systems sensitive price-performance storage components. Today, challenges storage dependability maintainability dominate challenges I/O. D.11 Historical Perspective References Section M.9 (available online) covers development storage devices tech- niques, including invented disks, story behind RAID, history operating systems databases. References reading included. Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau Case Study 1: Deconstructing Disk Concepts illustrated case study ■Performance Characteristics ■Microbenchmarks internals storage system tend hidden behind simple interface, linear array blocks. many advantages common inter-face storage systems: operating system use storage system with- modification, yet storage system free innovate behind interface. example, single disk map internal <sector, track, surfa- ce>geometry linear array whatever way achieves best performance; similarly, multidisk RAID system map blocks number disks tothis linear array. However, fixed interface number disadvantages,as well; particular, operating system able perform perfor-mance, reliability, security optimizations without knowing precise layoutof blocks inside underlying storage system. case study, explore software used uncover internal structure storage system hidden behind block-based interface.The basic idea fingerprint storage system: running well-defined work- load top storage system measuring amount time required fordifferent requests, one able infer surprising amount detail under-lying system.D-48 ■Appendix Storage SystemsThe Skippy algorithm, work Nisha Talagala colleagues Uni- versity California –Berkeley, uncovers parameters single disk. key factor disk rotational effects making consecutive seeks individual sec-tors addresses differ linearly increasing amount (increasing 1, 2, 3,and forth). Thus, basic algorithm skips disk, increasing dis-tance seek one sector every write, outputs distance andtime write. raw device interface used avoid file system optimi-zations. SECTOR SIZE set equal minimum amount data read disk (e.g., 512 bytes). (Skippy described detail Talagala Patterson [1999].) fd = open("raw disk device"); (i = 0; <measurements; i++) { begin_time = gettime(); lseek(fd, i*SECTOR_SIZE, SEEK_CUR); write(fd, buffer, SECTOR_SIZE);interval_time = gettime() -begin_time; printf("Stride: %d Time: %d\n", i, interval_time); }close(fd); graphing time required write function seek distance, one infer minimal transfer time (with seek rotational latency), head switch time, cylinder switch time, rotational latency, number heads inthe disk. typical graph four distinct lines, slope, butwith different offsets. highest lowest lines correspond requests thatincur different amounts rotational delay, cylinder head switch costs;the difference two lines reveals rotational latency disk. Thesecond lowest line corresponds requests incur head switch (in addition toincreasing amounts rotational delay). Finally, third line corresponds requests incur cylinder switch (in addition rotational delay). D.1 [10/10/10/10/10] <D.2>The results running Skippy shown mock disk (Disk Alpha) Figure D.25 . a.[10]<D.2>What minimal transfer time? b.[10]<D.2>What rotational latency? c.[10]<D.2>What head switch time? d.[10]<D.2>What cylinder switch time? e.[10]<D.2>What number disk heads? D.2 [25]<D.2>Draw approximation graph would result running Skippy Disk Beta, disk following parameters: ■Minimal transfer time, 2.0 ms ■Rotational latency, 6.0 msCase Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-49■Head switch time, 1.0 ms ■Cylinder switch time, 1.5 ms ■Number disk heads, 4 ■Sectors per track, 100 D.3 [10/10/10/10/10/10/10] <D.2>Implement run Skippy algorithm disk drive choosing. a.[10]<D.2>Graph results running Skippy. Report manufacturer model disk. b.[10]<D.2>What minimal transfer time? c.[10]<D.2>What rotational latency? d.[10]<D.2>What head switch time? e.[10]<D.2>What cylinder switch time? f.[10]<D.2>What number disk heads? g.[10]<D.2>Do results running Skippy real disk differ qual- itative way mock disk?Time (ms)14 1210 06 428 0 Distance (sectors)300 250 200 150 100 50 Figure D.25 Results running Skippy Disk Alpha.D-50 ■Appendix Storage SystemsCase Study 2: Deconstructing Disk Array Concepts illustrated case study ■Performance Characteristics ■Microbenchmarks Shear algorithm, work Timothy Denehy colleagues Uni- versity Wisconsin [Denehy et al. 2004], uncovers parameters RAID sys-tem. basic idea generate workload requests RAID array time requests; observing sets requests take longer, one infer blocks allocated disk. define RAID properties follows. Data allocated disks RAID block level, block minimal unit data file system reads writes storage system; thus, block size known file system andthe fingerprinting software. chunk set blocks allocated contiguously within disk. stripe set chunks across Ddata disks. Finally, pattern minimum sequence data blocks block offset iwithin pattern always located disk j. D.4 [20/20] <D.2>One uncover pattern size following code. code accesses raw device avoid file system optimizations. key allof Shear algorithms use random requests avoid triggering theprefetch caching mechanisms within RAID within individual disks.The basic idea code sequence access Nrandom blocks fixed interval pwithin RAID array measure completion time interval. (p = BLOCKSIZE; p <= testsize; p += BLOCKSIZE) { (i = 0; <N; i++) { request[i] = random()*p; } begin_time = gettime(); issues request[N] raw device parallel; wait request[N] complete; interval_time = gettime() - begin_time;printf("PatternSize: %d Time: %d\n", p, interval_time); } run code RAID array plot measured time N requests function p, see time highest N requests fall disk; thus, value pwith highest time corre- sponds pattern size RAID. a.[20]<D.2>Figure D.26 shows results running pattern size algorithm unknown RAID system.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-51■What pattern size storage system? ■What measured times 0.4, 0.8, 1.6 seconds correspond storage system? ■If RAID 0 array, many disks present? ■If RAID 0 array, chunk size? b.[20]<D.2>Draw graph would result running Shear code storage system following characteristics: ■Number requests, N¼1000 ■Time random read disk, 5 ms ■RAID level, RAID 0 ■Number disks, 4 ■Chunk size, 8 KB D.5 [20/20] <D.2>One uncover chunk size following code. basic idea perform reads Npatterns chosen random always controlled offsets, candc/C01, within pattern. (c = 0; c <patternsize; c += BLOCKSIZE) { (i = 0; <N; i++) { requestA[i] = random()*patternsize + c;requestB[i] = random()*patternsize + (c-1)%patternsize; } begin_time = gettime(); issue requestA[N] requestB[N] raw device parallel; wait requestA[N] requestB[N] complete; interval_time = gettime() - begin_time; printf("ChunkSize: %d Time: %d\n", c, interval_time); }Time (s)1.5 01.0 0.50.0 Pattern size assumed (KB)256 160 192 224 128 96 64 32 Figure D.26 Results running pattern size algorithm Shear mock storage system.D-52 ■Appendix Storage SystemsIf run code plot measured time function c, see measured time lowest requestA andrequestB reads fall two different disks. Thus, values cwith low times correspond chunk boundaries disks RAID. a.[20]<D.2>Figure D.27 shows results running chunk size algorithm unknown RAID system. ■What chunk size storage system? ■What measured times 0.75 1.5 seconds correspond storage system? b.[20]<D.2>Draw graph would result running Shear code storage system following characteristics: ■Number requests, N¼1000 ■Time random read disk, 5 ms ■RAID level, RAID 0 ■Number disks, 8 ■Chunk size, 12 KB D.6 [10/10/10/10] <D.2>Finally, one determine layout chunks disks following code. basic idea select Nrandom patterns exhaustively read together pairwise combinations chunks within pattern. (a = 0; <numchunks; += chunksize) { (b = a; b <numchunks; b += chunksize) { (i = 0; <N; i++) { requestA[i] = random()*patternsize + a;requestB[i] = random()*patternsize + b; }begin_time = gettime();issue requestA[N] requestB[N] raw device parallel;Time (s)1.5 01.0 0.50.0 Boundary offset assumed (KB)64 48 32 16 Figure D.27 Results running chunk size algorithm Shear mock stor- age system.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-53wait requestA[N] requestB[N] complete; interval_time = gettime() - begin_time; printf("A: %d B: %d Time: %d\n", a, b, interval_time); } } running code, report measured time function aandb. simplest way graph create two-dimensional table aandbas parameters time scaled shaded value; use darker shadings forfaster times lighter shadings slower times. Thus, light shading indicatesthat two offsets aandbwithin pattern fall disk. Figure D.28 shows results running layout algorithm storage system known pattern size 384 KB chunk size 32 KB. a.[20]<D.2>How many chunks pattern? b.[20]<D.2>Which chunks pattern appear allocated disks? c.[20]<D.2>How many disks appear storage system? d.[20]<D.2>Draw likely layout blocks across disks. D.7 [20]<D.2>Draw graph would result running layout algorithm storage system shown Figure D.29 . storage system four disks chunk size four 4 KB blocks (16 KB) using RAID 5 Left-Asymmetriclayout. Chunk10 06 428 0 Chunk10 8 6 4 2 Figure D.28 Results running layout algorithm Shear mock storage system.D-54 ■Appendix Storage SystemsCase Study 3: RAID Reconstruction Concepts illustrated case study ■RAID Systems ■RAID Reconstruction ■Mean Time Failure (MTTF) ■Mean Time Data Loss (MTDL) ■Performability ■Double Failures RAID system ensures data lost disk fails. Thus, one key responsibilities RAID reconstruct data disk itfailed; process called reconstruction explore case study. consider RAID system tolerate one disk failure RAID-DP, tolerate two disk failures. Reconstruction commonly performed two different ways. offline recon- struction , RAID devotes resources performing reconstruction service requests workload. online reconstruction , RAID continues service workload requests performing reconstruction;the reconstruction process often limited use fraction total band-width RAID system. reconstruction performed impacts reliability perform- ability system. RAID 5, data lost second disk fails data first disk recovered; therefore, longer reconstruction time(MTTR), lower reliability mean time data loss (MTDL). Per- formability metric meant combine performance system its00 01 02 03 04 05 06 07 08 09 10 11 PPPP 12 13 14 15 16 17 18 19 PPPP 20 21 22 23 24 25 26 27 PPPP 28 29 30 31 32 33 34 35 PPPP 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 PPPP 60 61 62 63 64 65 66 67 PPPP 68 69 70 71 72 73 74 75 PPPP 76 77 78 79 80 81 82 83 PPPP 84 85 86 87 88 89 90 91 92 93 94 95 Parity: RAID 5 Left-Asymmetric, stripe = 16, pattern = 48 Figure D.29 storage system four disks, chunk size four 4 KB blocks, using RAID 5 Left-Asymmetric layout. Two repetitions pattern shown.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-55availability; defined performance system given state multi- plied probability state. RAID array, possible states include nor- mal operation disk failures, reconstruction one disk failure, andshutdown due multiple disk failures. exercises, assume built RAID system six disks, plus sufficient number hot spares. Assume disk 37 GB SCSIdisk shown Figure D.3 disk sequentially read data peak 142 MB/sec sequentially write data peak 85 MB/sec. Assume thedisks connected Ultra320 SCSI bus transfer total 320 MB/ sec. assume disk failure independent ignore potential failures system. reconstruction process, assume over-head XOR computation memory copying negligible. onlinereconstruction, assume reconstruction process limited use total band-width 10 MB/sec RAID system. D.8 [10]<D.2>Assume RAID 4 system six disks. Draw simple diagram showing layout blocks across disks RAID system. D.9 [10]<D.2, D.4 >When single disk fails, RAID 4 system perform recon- struction. expected time reconstruction needed? D.10 [10/10/10] <D.2, D.4 >Assume reconstruction RAID 4 array begins time t. a.[10]<D.2, D.4 >What read write operations required perform reconstruction? b.[10]<D.2, D.4 >For offline reconstruction, reconstruction pro- cess complete? c.[10]<D.2, D.4 >For online reconstruction, reconstruction pro- cess complete? D.11 [10/10/10/10] <D.2, D.4 >In exercise, investigate mean time data loss (MTDL). RAID 4, data lost second disk fails thefirst failed disk repaired. a.[10]<D.2, D.4 >What likelihood second failure off- line reconstruction? b.[10]<D.2, D.4 >Given likelihood second failure reconstruc- tion, MTDL offline reconstruction? c.[10]<D.2, D.4 >What likelihood second failure online reconstruction? d.[10]<D.2, D.4 >Given likelihood second failure reconstruc- tion, MTDL online reconstruction? D.12 [10]<D.2, D.4 >What performability RAID 4 array offline recon- struction? Calculate performability using IOPS, assuming random readonlyworkload evenly distributed across disks RAID 4 array.D-56 ■Appendix Storage SystemsD.13 [10]<D.2, D.4 >What performability RAID 4 array online recon- struction? online repair, assume IOPS drop 70% peak rate. offline online reconstruction lead better performability? D.14 [10]<D.2, D.4 >RAID 6 used tolerate two simultaneous disk failures. Assume RAID 6 system based row-diagonal parity, RAID- DP; six-disk RAID-DP system based RAID 4, p¼5, shown Figure D.5 . data disk 0 data disk 3 fail, disks recon- structed? Show sequence steps required compute missingblocks first four stripes. Case Study 4: Performance Prediction RAIDs Concepts illustrated case study ■RAID Levels ■Queuing Theory ■Impact Workloads ■Impact Disk Layout case study, explore simple queuing theory used pre- dict performance I/O system. investigate storage sys-tem configuration workload influence service time, disk utilization, andaverage response time. configuration storage system large impact performance. Dif- ferent RAID levels modeled using queuing theory different ways. example, RAID 0 array containing Ndisks modeled Nseparate systems M/M/1 queues, assuming requests appropriately distributed across N disks. behavior RAID 1 array depends upon workload: read operationcan sent either mirror, whereas write operation must sent disks.Therefore, read-only workload, two-disk RAID 1 array modeled asan M/M/2 queue, whereas write-only workload, modeled M/M/1 queue. behavior RAID 4 array containing Ndisks also depends upon workload: read sent particular data disk, whereas writes must update parity disk, becomes bottleneck system. Therefore, aread-only workload, RAID 4 modeled N/C01 separate systems, whereas write-only workload, modeled one M/M/1 queue. layout blocks within storage system significant impact performance. Consider single disk 40 GB capacity. workload ran-domly accesses 40 GB data, layout blocks disk nothave much impact performance. However, workload randomly accesses half disk ’s capacity (i.e., 20 GB data disk), layout matter: reduce seek time, 20 GB data compactedwithin 20 GB consecutive tracks instead allocated uniformly distributed overthe entire 40 GB capacity.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-57For problem, use rather simplistic model estimate service time disk. basic model, average positioning transfer time small random request linear function seek distance. 40 GB disk inthis problem, assume service time 5 ms* space utilization. Thus, entire 40 GB disk used, average positioning transfer time ran-dom request 5 ms; first 20 GB disk used, averagepositioning transfer time 2.5 ms. Throughout case study, assume processor sends 167 small random disk requests per second requests exponentially distributed. assume size requests equal block size 8 KB. disk system capacity 40 GB. Regardless storagesystem configuration, workload accesses total 40 GB data; shouldallocate 40 GB data across disks system efficientmanner. D.15 [10/10/10/10/10] <D.5>Begin assuming storage system consists single 40 GB disk. a.[10]<D.5>Given workload storage system, average service time? b.[10]<D.5>On average, utilization disk? c.[10]<D.5>On average, much time request spend waiting disk? d.[10]<D.5>What mean number requests queue? e.[10]<D.5>Finally, average response time disk requests? D.16 [10/10/10/10/10/10] <D.2, D.5 >Imagine storage system config- ured contain two 40 GB disks RAID 0 array; is, data striped inblocks 8 KB equally across two disks redundancy. a.[10]<D.2, D.5 >How 40 GB data allocated across disks? Given random request workload total 40 GB, expectedservice time request? b.[10]<D.2, D.5 >How queuing theory used model storage system? c.[10]<D.2, D.5 >What average utilization disk? d.[10]<D.2, D.5 >On average, much time request spend waiting disk? e.[10]<D.2, D.5 >What mean number requests queue? f.[10]<D.2, D.5 >Finally, average response time disk requests? D.17 [20/20/20/20/20] <D.2, D.5 > Instead imagine storage system config- ured contain two 40 GB disks RAID 1 array; is, data mirroredD-58 ■Appendix Storage Systemsacross two disks. Use queuing theory model system read-only workload. a.[20]<D.2, D.5 >How 40 GB data allocated across disks? Given random request workload total 40 GB, expected service time request? b.[20]<D.2, D.5 >How queuing theory used model storage system? c.[20]<D.2, D.5 >What average utilization disk? d.[20]<D.2, D.5 >On average, much time request spend waiting disk? e.[20]<D.2, D.5 >Finally, average response time disk requests? D.18 [10/10] <D.2, D.5 >Imagine instead read-only workload, write-only workload RAID 1 array. a.[10]<D.2, D.5 >Describe use queuing theory model sys- tem workload. b.[10]<D.2, D.5 >Given system workload, average utili- zation, average waiting time, average response time? Case Study 5: I/O Subsystem Design Concepts illustrated case study ■RAID Systems ■Mean Time Failure (MTTF) ■Performance Reliability Trade-Offs case study, design I/O subsystem, given monetary budget. system minimum required capacity optimize per-formance, reliability, both. free use many disks controllers asfit within budget. building blocks: ■A 10,000 MIPS CPU costing $1000. MTTF 1,000,000 hours. ■A 1000 MB/sec I/O bus room 20 Ultra320 SCSI buses controllers. ■Ultra320 SCSI buses transfer 320 MB/sec support 15 disks per bus (these also called SCSI strings ). SCSI cable MTTF 1,000,000 hours. ■An Ultra320 SCSI controller capable 50,000 IOPS, costs $250, MTTF 500,000 hours.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-59■A$2000 enclosure supplying power cooling eight disks. enclosure MTTF 1,000,000 hours, fan MTTF 200,000 hours, power supply MTTF 200,000 hours. ■The SCSI disks described Figure D.3 . ■Replacing failed component requires 24 hours. may make following assumptions workload: ■The operating system requires 70,000 CPU instructions disk I/O. ■The workload consists many concurrent, random I/Os, average size 16 KB. constructed systems must following properties: ■You monetary budget $28,000. ■You must provide least 1 TB capacity. D.19 [10]<D.2>You begin designing I/O subsystem optimized capacity performance (and reliability), specifically IOPS. Discuss theRAID level block size deliver best performance. D.20 [20/20/20/20] <D.2, D.4, D.7 >What configuration SCSI disks, controllers, enclosures results best performance given monetary capacityconstraints? a.[20]<D.2, D.4, D.7 >How many IOPS expect deliver system? b.[20]<D.2, D.4, D.7 >How much system cost? c.[20]<D.2, D.4, D.7 >What capacity system? d.[20]<D.2, D.4, D.7 >What MTTF system? D.21 [10]<D.2, D.4, D.7 >You redesign system optimize reliabil- ity, creating RAID 10 RAID 01 array. storage system robustnot disk failures also controller, cable, power supply, fan failures well; specifically, single component failure prohibit accessing replicas pair. Draw diagram illustrating blocks allocated across disksin RAID 10 RAID 01 configurations. RAID 10 RAID 01 appro-priate environment? D.22 [20/20/20/20/20] <D.2, D.4, D.7 >Optimizing RAID 10 RAID 01 array reliability (but staying within capacity monetary constraints),what RAID configuration? a.[20]<D.2, D.4, D.7 >What overall MTTF components system?D-60 ■Appendix Storage Systemsb.[20]<D.2, D.4, D.7 >What MTDL system? c.[20]<D.2, D.4, D.7 >What usable capacity system? d.[20]<D.2, D.4, D.7 >How much system cost? e.[20]<D.2, D.4, D.7 >Assuming write-only workload, many IOPS expect deliver? D.23 [10]<D.2, D.4, D.7 >Assume access disk twice capacity, price. continue design reliability, howwould change configuration storage system? Why? Case Study 6: Dirty Rotten Bits Concepts illustrated case study ■Partial Disk Failure ■Failure Analysis ■Performance Analysis ■Parity Protection ■Checksumming put charge avoiding problem “bit rot ”—bits blocks file going bad time. problem particularly important archival scenarios,where data written perhaps accessed many years later; without takingextra measures protect data, bits blocks file may slowly change orbecome unavailable due media errors I/O faults. Dealing bit rot requires two specific components: detection recovery. detect bit rot efficiently, one use checksums block file inquestion; checksum function kind takes (potentially long) string data input outputs fixed-size string (the checksum) data output. property exploit data changes computedchecksum likely change well. detected, recovering bit rot requires form redundancy. Examples include mirroring (keeping multiple copies block) parity(some extra redundant information, usually space efficient mirroring). case study, analyze effective techniques given various scenarios. also write code implement data integrity protection set files. D.24 [20/20/20] <D.2>Assume use simple parity protection Exercises D.24 D.27. Specifically, assume computing oneparity block file file system. Further, assume also use a20-byte MD5 checksum per 4 KB block file.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-61We first tackle problem space overhead. According studies Douceur Bolosky [1999], file size distributions found modern PCs: /C201 KB 2 KB 4 KB 8 KB 16 KB 32 KB 64 KB 128 KB 256 KB 512 KB /C211M B 26.6% 11.0% 11.2% 10.9% 9.5% 8.5% 7.1% 5.1% 3.7% 2.4% 4.0% study also finds file systems usually half full. Assume 37 GB disk volume roughly half full follows distribu-tion, answer following questions: a.[20]<D.2>How much extra information (both bytes percent vol- ume) must keep disk able detect single error checksums? b.[20]<D.2>How much extra information (both bytes percent volume) would need able detect single error checksumsas well correct it? c.[20]<D.2>Given file distribution, block size using com- pute checksums big, little, right? D.25 [10/10] <D.2, D.3 >One big problem arises data protection error detec- tion. One approach perform error detection lazily —that is, wait file accessed, point, check make sure correct data there. Theproblem approach files accessed frequently may slowlyrot away finally accessed many errors corrected. Hence, aneager approach perform sometimes called disk scrubbing — periodically go data find errors proactively. a.[10]<D.2, D.3 >Assume bit flips occur independently, rate 1 flip per GB data per month. Assuming 20 GB volume half full, assuming using SCSI disk specified Figure D.3 (4 ms seek, roughly 100 MB/sec transfer), often scan files tocheck repair integrity? b.[10]<D.2, D.3 >At bit flip rate become impossible maintain data integrity? assume 20 GB volume SCSI disk. D.26 [10/10/10/10] <D.2, D.4 >Another potential cost added data protection found performance overhead. study performance overhead thisdata protection approach. a.[10]<D.2, D.4 >Assume write 40 MB file SCSI disk sequentially, write extra information implement data protectionscheme disk once. much write traffic (both total volume bytes percentage total traffic) scheme generate? b.[10]<D.2, D.4 >Assume updating file randomly, similar database table. is, assume perform series 4 KB random writes tothe file, time perform single write, must update on-diskprotection information. Assuming perform 10,000 random writes, howD-62 ■Appendix Storage Systemsmuch I/O traffic (both total volume bytes percentage total traf- fic) scheme generate? c.[10]<D.2, D.4 >Now assume data protection information always kept separate portion disk, away file guarding (that is, assume file A,there another file Achecksums holds check-sums A). Hence, one potential overhead must incur arises upon reads —that is, upon read, use checksum detect data corruption. Assume read 10,000 blocks 4 KB sequentially disk. Assuming 4 ms average seek cost 100 MB/sec transfer rate (like SCSI disk inFigure D.3 ), long take read file (and corresponding check- sums) disk? time penalty due adding checksums? d.[10]<D.2, D.4 >Again assuming data protection information kept separate part (c), assume read 10,000 random blocks of4 KB large file (much bigger 10,000 blocks, is). Foreach read, must use checksum ensure data integrity. longwill take read 10,000 blocks disk, assuming disk characteristics? time penalty due adding checksums? D.27 [40]<D.2, D.3, D.4 >Finally, put theory practice developing user- level tool guard file corruption. Assume write simple set tools detect repair data integrity. first tool used checksums parity. called build used like this: build <filename > Thebuild program store needed checksum redundancy information file filename file directory called .file name.cp (so easy find later). second program used check potentially repair damaged files. called repair used like this: repair <filename > Therepair program consult .cp file filename question verify stored checksums match computed checksums data. Ifthe checksums ’t match single block, repair use redundant information reconstruct correct data fix file. However, two blocks bad, repair simply report file corrupted beyond repair. test system, provide tool corrupt files calledcorrupt . works follows: corrupt <filename ><blocknumber > Allcorrupt fill specified block number file random noise. checksums using MD5. MD5 takes input string gives aCase Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-63128-bit “fingerprint ”or checksum output. great simple implementation MD5 available here: http://sourceforge.net/project/showfiles.php?group_ id=42360 Parity computed XOR operator. C code, compute parity two blocks, size BLOCKSIZE , follows: unsigned char block1[BLOCKSIZE]; unsigned char block2[BLOCKSIZE]; unsigned char parity[BLOCKSIZE]; // first, clear parity block (int = 0; <BLOCKSIZE; i++) parity[i] = 0; // compute parity; carat symbol XOR C (int = 0; <BLOCKSIZE; i++) { parity[i] = block1[i] ^block2[i]; } Case Study 7: Sorting Things Concepts illustrated case study ■Benchmarking ■Performance Analysis ■Cost/Performance Analysis ■Amortization Overhead ■Balanced Systems database field long history using benchmarks compare systems. question, explore one benchmarks introduced Anon. et al. [1985] (see Chapter 1 ): external, disk-to-disk, sorting. Sorting exciting benchmark number reasons. First, sorting exercises computer system across components, including disk, memory, proces-sors. Second, sorting highest possible performance requires great deal ofexpertise CPU caches, operating systems, I/O subsystems work.Third, simple enough implemented student (see below!). Depending much data have, sorting done one multiple passes. Simply put, enough memory hold entire dataset mem- ory, read entire dataset memory, sort it, write out; called “one-pass ”sort. enough memory, must sort data multiple passes. many different approaches possible. One simple approach sort eachD-64 ■Appendix Storage Systemschunk input file write disk; leaves (input file size)/(memory size) sorted files disk. Then, merge sorted temporary file final sorted output. called “two-pass ”sort. passes needed unlikely case cannot merge streams second pass. case study, analyze various aspects sorting, determining effectiveness cost-effectiveness different scenarios. also write yourown version external sort, measuring performance real hardware. D.28 [20/20/20] <D.4>We start configuring system complete sort least possible time, limits much spend. get peak band- width sort, make sure paths system havesufficient bandwidth. Assume simplicity time perform in-memory sort keys lin- early proportional CPU rate memory bandwidth given machine (e.g., sorting 1 MB records machine 1 MB/sec memory bandwidthand 1 MIPS processor take 1 second). Assume carefullywritten I/O phases sort achieve sequential bandwidth. And, ofcourse, realize ’t enough memory hold data sort take two passes. One problem may encounter performing I/O systems often perform extra memory copies ; example, read() system call invoked, data may first read disk system buffer subsequentlycopied specified user buffer. Hence, memory bandwidth I/O bean issue. Finally, simplicity, assume overlap reading, sorting, writ- ing. is, reading data disk, doing; whensorting, using CPU memory bandwidth; writing, arejust writing data disk. job task configure system extract peak performance sorting 1 GB data (i.e., roughly 10 million 100-byte records). Use followingtable make choices machine, memory, I/O interconnect, disks buy. CPU I/O interconnect Slow 1 GIPS $200 Slow 80 MB/sec $50 Standard 2 GIPS $1000 Standard 160 MB/sec $100 Fast 4 GIPS $2000 Fast 320 MB/sec $400 Memory Disks Slow 512 MB/sec $100/GB Slow 30 MB/sec $70 Standard 1 GB/sec $200/GB Standard 60 MB/sec $120 Fast 2 GB/sec $500/GB Fast 110 MB/sec $300Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-65Note: Assume buying single-processor system two I/O interconnects. However, amount memory number disks (assume limit disks per I/O interconnect). a.[20]<D.4>What total cost machine? (Break part, including cost CPU, amount memory, number disks, I/O bus.) b.[20]<D.4>How much time take complete sort 1 GB worth records? (Break time spent reads disk, writes disk,and time spent sorting.) c.[20]<D.4>What bottleneck system? D.29 [25/25/25] <D.4>We examine cost-performance issues sorting. all, easy buy high-performing machine; much harder buy acosteffective one. One place issue arises PennySort competition ( research. microsoft.com/barc/SortBenchmark/ ). PennySort asks sort many records single penny. compute this, assume thata system buy last 3 years (94,608,000 seconds), divide bythe total cost pennies machine. result time budget per penny. task little simpler. Assume fixed budget $2000 (or less). fastest sorting machine build? Use hardwaretable Exercise D.28 configure winning machine. (Hint: might want write little computer program generate pos- sible configurations.) a.[25]<D.4>What total cost machine? (Break part, including cost CPU, amount memory, number disks, I/Obus.) b.[25]<D.4>How reading, writing, sorting time break configuration? c.[25]<D.4>What bottleneck system? D.30 [20/20/20] <D.4, D.6 >Getting good disk performance often requires amortiza- tion overhead . idea simple: must incur overhead kind, much useful work possible paying cost hence reduce itsimpact. idea quite general applied many areas computersystems; disks, arises seek rotational costs (overheads) thatyou must incur transferring data. amortize expensive seekand rotation transferring large amount data. exercise, focus amortize seek rotational costs second pass two-pass sort. Assume second pass begins, areNsorted runs disk, size fits within main memory. task read chunk sorted run merge results final sortedD-66 ■Appendix Storage Systemsoutput. Note read one run incur seek rotation, likely last read different run. a.[20]<D.4, D.6 >Assume disk transfer 100 MB/sec, average seek cost 7 ms, rotational rate 10,000 RPM. Assume every time read run, read 1 MB data 100 runs size 1 GB. Also assume writes (to final sorted out-put) take place large 1 GB chunks. long merge phase take,assuming I/O dominant (i.e., only) cost? b.[20]<D.4, D.6 >Now assume change read size 1 MB 10 MB. total time perform second pass sort affected? c.[20]<D.4, D.6 >In cases, assume wish maximize disk efficiency . compute disk efficiency ratio time spent transferring data total time spent accessing disk. disk efficiency ineach scenarios mentioned above? D.31 [40]<D.2, D.4, D.6 >In exercise, write external sort. generate data set, provide tool generate works follows: generate <filename ><size (in MB) > running generate , create file named filename size size MB. file consists 100 byte keys, 10-byte records (the part must besorted). also provide tool called check checks whether given input file sorted not. run follows: check <filename > basic one-pass sort following: reads data, sorts data, writes data out. However, numerous optimizations available you: overlapping reading sorting, separating keys rest record forbetter cache behavior hence faster sorting, overlapping sorting writing,and forth. One important rule data must always start disk (and file system cache). easiest way ensure unmount remount file system. One goal: Beat Datamation sort record. Currently, record sorting 1 mil- lion 100-byte records 0.44 seconds, obtained cluster 32 machines. careful, might able beat single PC config-ured disks.Case Studies Exercises Andrea C. Arpaci-Dusseau Remzi H. Arpaci-Dusseau ■D-67E.1 Introduction E-2 E.2 Signal Processing Embedded Applications: Digital Signal Processor E-5 E.3 Embedded Benchmarks E-12 E.4 Embedded Multiprocessors E-14 E.5 Case Study: Emotion Engine Sony PlayStation 2 E-15 E.6 Case Study: Sanyo VPC-SX500 Digital Camera E-19 E.7 Case Study: Inside Cell Phone E-20 E.8 Concluding Remarks E-25E Embedded Systems Thomas M. Conte North Carolina State University calculator ENIAC equipped 18,000 vacuum tubes weighs 30 tons, computers future may 1,000 vacuum tubes perhaps weigh 1 1/2 tons. Popular Mechanics March 1949E.1 Introduction Embedded computer systems —computers lodged devices pres- ence computers immediately obvious —are fastest-growing portion computer market. devices range everyday machines (mostmicrowaves, washing machines, printers, network switches, automobilescontain simple advanced embedded microprocessors) handheld digitaldevices (such PDAs, cell phones, music players) video game consoles digital set-top boxes. Although applications (such PDAs) com- puters programmable, many embedded applications programmingoccurs connection initial loading application code later soft-ware upgrade application. Thus, application carefully tuned theprocessor system. process sometimes includes limited use assemblylanguage key loops, although time-to-market pressures good software engi-neering practice restrict assembly language coding fraction theapplication. Compared desktop server systems, embedded systems much wider range processing power cost —from systems containing low-end 8- bit 16-bit processors may cost less dollar, containing full32-bit microprocessors capable operating 500 MIPS range costapproximately 10 dollars, containing high-end embedded processors thatcost hundreds dollars execute several billions instructions per second.Although range computing power embedded systems market verylarge, price key factor design computers space. Performance requirements exist, course, primary goal often meeting perfor- mance need minimum price, rather achieving higher performance ahigher price. Embedded systems often process information different ways general-purpose processors. Typically applications include deadline-drivenconstraints —so-called real-time constraints. applications, particular computation must completed certain time system fails (there otherconstraints considered real time, discussed next subsection). Embedded systems applications typically involve processing information signals. lay term “signal ”often connotes radio transmission, true embedded systems (e.g., cell phones). signal may image, amotion picture composed series images, control sensor measurement, andso on. Signal processing requires specific computation many embedded pro-cessors optimized for. discuss depth below. wide range bench-mark requirements exist, ability run small, limited code segments theability perform well applications involving tens hundreds thousands lines code. Two key characteristics exist many embedded applications: need minimize memory need minimize power. many embedded applica-tions, memory substantial portion system cost, importantto optimize memory size cases. Sometimes application expected fitE-2 ■Appendix E Embedded Systemsentirely memory processor chip; times application needs fit entirety small, off-chip memory. either case, importance memory size translates emphasis code size, since data size dictatedby application. architectures special instruction set capabilitiesto reduce code size. Larger memories also mean power, optimizingpower often critical embedded applications. Although emphasis lowpower frequently driven use batteries, need use less expensivepackaging (plastic versus ceramic) absence fan cooling also limittotal power consumption. examine issue power detail later appendix. Another important trend embedded systems use processor cores together application-specific circuitry —so-called “core plus ASIC ”or“sys- tem chip ”(SOC), may also viewed special-purpose multiproces- sors (see Section E.4). Often application ’s functional performance requirements met combining custom hardware solution together soft-ware running standardized embedded processor core, designed tointerface special-purpose hardware. practice, embedded problems usually solved one three approaches: 1.The designer uses combined hardware/software solution includes custom hardware embedded processor core integrated thecustom hardware, often chip. 2.The designer uses custom software running off-the-shelf embedded processor. 3.The designer uses digital signal processor custom software proces- sor. Digital signal processors processors specially tailored signal- processing applications. discuss important differences betweendigital signal processors general-purpose embedded processors below. Figure E.1 summarizes three classes computing environments important characteristics. Real-Time Processing Often, performance requirement embedded application real-time requirement. real-time performance requirement one segment application absolute maximum execution time allowed. exam-ple, digital set-top box time process video frame limited, since theprocessor must accept process frame next frame arrives (typi-cally called hard real-time systems ). applications, sophisticated requirement exists: average time particular task constrained well number instances maximum time exceeded. Suchapproaches (typically called soft real-time ) arise possible occasionally miss time constraint event, long many missed. Real-timeE.1 Introduction ■E-3performance tends highly application dependent. usually measured using kernels either application standardized benchmark (seeSection E.3). construction hard real-time system involves three key variables. first rate particular task must occur. Coupled hard- ware software required achieve real-time rate. Often, structures advantageous desktop enemy hard real-time analysis. Forexample, branch speculation, cache memories, introduce uncertainty code. particular sequence code may execute either efficiently veryinefficiently, depending whether hardware branch predictors caches “do jobs. ”Engineers must analyze code assuming worst-case execution time (WCET). case traditional microprocessor hardware, one assumes thatall branches mispredicted andall caches miss, WCET overly pessimistic. Thus, system designer may end overdesigning system achieve given WCET, much less expensive system would sufficed. order address challenges hard real-time systems, yet still exploit well-known architectural properties branch behavior access locality, itis possible change processor designed. Consider branch prediction:Although dynamic branch prediction known perform far accurately thanstatic “hint bits ”added branch instructions, behavior static hints much predictable. Furthermore, although caches perform better software- managed on-chip memories, latter produces predictable memory latencies. embedded processors, caches converted software-managedon-chip memories via line locking. approach, cache line locked cache cannot replaced line unlockedFeature Desktop Server Embedded Price system $1000 –$10,000 $10,000 –$10,000,000 $10–$100,000 (including network routers high end) Price microprocessor module$100–$1000 $200–$2000 (per processor)$0.20–$200 (per processor) Microprocessors sold per year (estimates 2000)150,000,000 4,000,000 300,000,000 (32-bit 64-bit processors only) Critical system design issues Price-performance, graphics performanceThroughput, availability, scalabilityPrice, power consumption, application-specific performance Figure E.1 summary three computing classes system characteristics. Note wide range system price servers embedded systems. servers, range arises need large-scale mul-tiprocessor systems high-end transaction processing Web server applications. embedded systems, one significant high-end application network router, could include multiple processors well lots mem- ory electronics. total number embedded processors sold 2000 estimated exceed 1 billion, ifyou include 8-bit 16-bit microprocessors. fact, largest-selling microprocessor time 8-bit micro-controller sold Intel! difficult separate low end server market desktop market, since low- end servers —especially costing less $5000 —are essentially different desktop PCs. Hence, million PC units may effectively servers.E-4 ■Appendix E Embedded SystemsE.2 Signal Processing Embedded Applications: Digital Signal Processor digital signal processor (DSP) special-purpose processor optimized executing digital signal processing algorithms. algorithms, fromtime-domain filtering (e.g., infinite impulse response finite impulse responsefiltering), convolution, transforms (e.g., fast Fourier transform, discrete cosinetransform), even forward error correction (FEC) encodings, theirkernel operation: multiply-accumulate operation. example, thediscrete Fourier transform form: XkðÞ ¼XN/C01 n¼0xnðÞWkn Nwhere Wkn N¼ej2πkn N¼cos 2 πkn N/C18/C19 +jsin 2 πkn N/C18/C19 discrete cosine transform often replacement require complex number operations. Either transform core sum aproduct . accelerate this, DSPs typically feature special-purpose hardware perform multiply-accumulate (MAC). MAC instruction “MAC A,B,C ” semantics “A¼A+B*C . ”In situations, performance operation critical DSP selected application based solely upon itsMAC operation throughput. DSPs often employ fixed-point arithmetic. think integers binary point right least-significant bit, fixed point binary pointjust right sign bit. Hence, fixed-point data fractions /C01 +1. Example three simple 16-bit patterns: 0100 0000 0000 0000 0000 1000 0000 00000100 1000 0000 1000 values represent two ’s complement integers? Fixedpoint numbers? Answer Number representation tells us ith digit left binary point represents 2i/C01and ith digit right binary point represents 2/C0i. First assume three patterns integers. binary point far right,so represent 2 14,211, (214+211+23), 16,384, 2048, 18,440. Fixed point places binary point right sign bit, fixed point patterns represent 2/C01,2/C04, (2/C01+2/C04+2/C012). fractions 1/2, 1/16, (2048 + 256 + 1)/4096 2305/4096, represents about0.50000, 0.06250, 0.56274. Alternatively, n-bit two ’s complement,E.2 Signal Processing Embedded Applications: Digital Signal Processor ■E-5fixed-point number could divide integer presentation 2n/C01to derive results: 16,384 =32,768 ¼1=2, 2048 =32,768 ¼1=16, 18,440 =32,768 ¼2305=4096: Fixed point thought low-cost floating point. ’t include exponent every word ’t hardware automatically aligns normalizes operands. Instead, fixed point relies DSP programmer keepthe exponent separate variable ensure result shifted left rightto keep answer aligned variable. Since exponent variable oftenshared set fixed-point variables, style arithmetic also calledblocked floating point , since block variables common exponent. support manual calculations, DSPs usually registers wider guard round-off error, floating-point units internally haveextra guard bits. Figure E.2 surveys four generations DSPs, listing data sizes width accumulating registers. Note DSP architects bound thepowers 2 word sizes. Figure E.3 shows size data operands TI TMS320C55 DSP. addition MAC operations, DSPs often also operations accelerate portions communications algorithms. important class algorithms revolve around encoding decoding forward error correction codes —codes extra information added digital bit stream guard errorsin transmission. code rate m/nhasminformation bits ( m+n) check bits. So, example, 1/2 rate code would 1 information bit per every 2 bits. Suchcodes often called trellis codes one popular graphical flow diagram Generation Year Example DSP Data width Accumulator width 1 1982 TI TMS32010 16 bits 32 bits 2 1987 Motorola DSP56001 24 bits 56 bits3 1995 Motorola DSP56301 24 bits 56 bits4 1998 TI TMS320C6201 16 bits 40 bits Figure E.2 Four generations DSPs, data width, width registers reduces round-off error. Data size Memory operand operation Memory operand data transfer 16 bits 89.3% 89.0% 32 bits 10.7% 11.0% Figure E.3 Size data operands TMS320C55 DSP. 90% operands 16 bits. DSP two 40-bit accumulators. floating-point operations, typical many DSPs, data fixed-point integers.E-6 ■Appendix E Embedded Systemstheir encoding resembles garden trellis. common algorithm decoding trellis codes due Viterbi. algorithm requires sequence compares selects order recover transmitted bit ’s true value. Thus DSPs often compare- select operations support Viterbi decode FEC codes. explain DSPs better, take detailed look two DSPs, pro- duced Texas Instruments. TMS320C55 series DSP family targetedtoward battery-powered embedded applications. stark contrast this, theTMS VelociTI 320C6x series line powerful, eight-issue VLIW processorstargeted toward broader range applications may less power sensitive. TI 320C55 one end DSP spectrum TI 320C55 architecture. C55 opti-mized low-power, embedded applications. overall architecture shown inFigure E.4 . heart it, C55 seven-staged pipelined CPU. stages outlined below: ■Fetch stage reads program data memory instruction buffer queue. ■Decode stage decodes instructions dispatches tasks primary functional units. ■Address stage computes addresses data accesses branch addresses program discontinuities. ■Access 1/Access 2 stages send data read addresses memory. ■Read stage transfers operand data B bus, C bus, bus. ■Execute stage executes operation unit unit performs writes E bus F bus. Instruction buffer unit (IU)Program flow unit (PU)Address data flow unit (AU)Data computation unit (DU)Data read buses BB, CB, DB (3 x 16) Data read address buses BAB, CAB, DAB (3 x 24) CPU Data write address buses EAB, FAB (2 x 24) Data write buses EB, FB (2 x 16)Program address bus PAB (24) Program read bus PB (32) Figure E.4 Architecture TMS320C55 DSP. C55 seven-stage pipelined pro- cessor unique instruction execution facilities. (Courtesy Texas Instruments.)E.2 Signal Processing Embedded Applications: Digital Signal Processor ■E-7The C55 pipeline performs pipeline hazard detection stall write read (WAR) read write (RAW) hazards. C55 24 KB instruction cache, configurable support various workloads. may configured two-way set associative, direct-mapped, “ramset. ”This latter mode way support hard realtime appli- cations. mode, blocks cache cannot replaced. C55 also advanced power management. allows dynamic power man- agement software-programmable “idle domains. ”Blocks circuitry device organized idle domains. domain operate nor- mally placed low-power idle state. programmer-accessible Idle Control Register (ICR) determines domains placed idle statewhen execution next IDLE instruction occurs. six domains CPU, direct memory access (DMA), peripherals, clock generator, instruction cache, andexternal memory interface. domain idle state, functions ofthat particular domain available. However, peripheral domain, eachperipheral Idle Enable bit controls whether peripheral willrespond changes idle state. Thus, peripherals individually con- figured idle remain active peripheral domain idled. Since C55 DSP, central feature MAC units. C55 two MAC units, comprised 17-bit 17-bit multiplier coupled 40-bitdedicated adder. MAC unit performs work single cycle; thus, theC55 execute two MACs per cycle full pipelined operation. kind ofcapability critical efficiently performing signal processing applications.The C55 also compare, select, store unit (CSSU) add/comparesection Viterbi decoder. TI 320C6x stark contrast C55 DSP family high-end Texas Instruments VelociTI320C6x family processors. C6x processors closer traditional verylong instruction word (VLIW) processors seek exploit highlevels instruction-level parallelism (ILP) many signal processing algorithms.Texas Instruments alone selecting VLIW exploiting ILP embed- ded space. VLIW DSP vendors include Ceva, StarCore, Philips/TriMedia, STMicroelectronics. vendors favor VLIW superscalar? Forthe embedded space, code compatibility less problem, new applica-tions either hand tuned recompiled newest generation proces-sor. reason superscalar excels desktop compilercannot predict memory latencies compile time. embedded, however, memorylatencies often much predictable. fact, hard real-time constraints forcememory latencies statically predictable. course, superscalar would also perform well environment constraints, extra hardware dynamically schedule instructions wasteful terms precious chip areaand terms power consumption. Thus VLIW natural choice high-performance embedded.E-8 ■Appendix E Embedded SystemsThe C6x family employs different pipeline depths depending family member. C64x, example, pipeline 11 stages. first four stages pipeline perform instruction fetch, followed two stages instructiondecode, finally four stages instruction execution. overall architectureof C64x shown Figure E.5 . C6x family ’s execution stage divided two parts, left “1”side right “2”side. L1 L2 units perform logical arithmetic oper- ations. units contrast perform subset logical arithmetic operations butalso perform memory accesses (loads stores). two units perform multi- plication related operations (e.g., shifts). Finally units perform compari- sons, branches, SIMD operations (see next subsection detailedexplanation SIMD operations). side 32-entry, 32-bit register file(the file 1 side, B file 2 side). side may access side ’s registers, 1- cycle penalty. Thus, instruction executing side 1 mayaccess B5, example, take 1- cycle extra execute this. VLIWs traditionally bad comes code size, runs con- trary needs embedded systems. However, C6x family ’s approach “compresses ”instructions, allowing VLIW code achieve density equivalent RISC (reduced instruction set computer) code. so, instructionfetch carried “instruction packet, ”shown Figure E.6 . instruc- tion pbit specifies whether instruction member current Program cache/program memory 32-bit address 256-bit data Data cache/data memory 32-bit address 8-, 16-, 32-, 64-bit dataProgram fetch Instruction dispatch Instruction decodeControl registersC6000 CPU Control logic Test Emulation InterruptsEDMA, EMIF Additional peripherals: timers, serial ports, etc.Register file AData path .L1 .S1 .M1 .D1Power Register file BData path B .D2 .M2 .S2 .L2 Figure E.5 Architecture TMS320C64x family DSPs. C6x eight-issue traditional VLIW processor. (Courtesy Texas Instruments.)E.2 Signal Processing Embedded Applications: Digital Signal Processor ■E-9VLIW word next VLIW word (see figure detailed explanation). Thus, NOPs needed VLIW encoding. Software pipelining important technique achieving high performance VLIW. software pipelining relies iteration loop identical schedule iterations. conditional branch instructionsdisrupt pattern, C6x family provides means conditionally executeinstructions using predication. predication, instruction performs work. done executing, additional register, example A1, checked.If A1 zero, instruction write results. A1 nonzero, instruc-tion proceeds normally. allows simple if-then if-then-else structures becollapsed straight-line code software pipelining. Media Extensions middle ground DSPs microcontrollers: media extensions. extensions add DSP-like capabilities microcontroller architectures rela-tively low cost. media processing judged human perception, datafor multimedia operations often much narrower 64-bit data word mod-ern desktop server processors. example, floating-point operations forgraphics normally single precision, double precision, often pre-cision less required IEEE 754. Rather waste 64-bit arithmetic- logical units (ALUs) operating 32-bit, 16-bit, even 8-bit integers, mul- timedia instructions operate several narrower data items time.Thus, partitioned add operation 16-bit data 64-bit ALU would perform four 16-bit adds single clock cycle. extra hardware cost simply preventcarries four 16-bit partitions ALU. example, instructionsmight used graphical operations pixels. operations commonlycalled single-instruction multiple-data (SIMD) vector instructions. graphics multimedia applications use 32-bit floating-point operations. computers double peak performance single-precision, floating-point oper- ations; allow single instruction launch two 32-bit operations operandsfound side side double-precision register. two partitions must insu-lated prevent operations one half affecting other. floating-pointoperations called paired single operations. example, operation3 10 3 10 3 10 3 10 3 10 3 10 3 10 3 10 Instruction Apppppppp Instruction BInstruction CInstruction DInstruction EInstruction FInstruction GInstruction H Figure E.6 Instruction packet TMS320C6x family DSPs. Thepbits determine whether instruction begins new VLIW word not. pbit instruction iis 1, instruction i+ 1 executed parallel (in cycle as) instruction i. pbit instruction iis 0, instruction i+ 1 executed cycle instruc- tion i. (Courtesy Texas Instruments.)E-10 ■Appendix E Embedded Systemsmight used graphical transformations vertices. doubling perfor- mance typically accomplished doubling number floating-point units, making expensive suppressing carries integer adders. Figure E.7 summarizes SIMD multimedia instructions found several recent computers. DSPs also provide operations found first three rows Figure E.7 , change semantics bit. First, often used real-timeapplications, option causing exception arithmetic overflow(otherwise could miss event); thus, result used matter inputs. support unyielding environment, DSP architectures use saturat- ing arithmetic : result large represented, set largest rep- resentable number, depending sign result. contrast, two ’s complement arithmetic add small positive number large positive. Instruction category Alpha MAXHP PA-RISC MAX2Intel Pentium MMXPowerPC AltiVec SPARC VIS Add/subtract 4H 8B, 4H, 2W 16B, 8H, 4W 4H, 2W Saturating add/subtract 4H 8B, 4H 16B, 8H, 4WMultiply 4H 16B, 8HCompare 8B ( >¼) 8B, 4H, 2W (¼,>)16B, 8H, 4W (¼,>,>¼,<,<¼)4H, 2W (¼, not¼,>,<¼) Shift right/left 4H 4H, 2W 16B, 8H, 4WShift right arithmetic 4H 16B, 8H, 4WMultiply add 8HShift add (saturating)4H AND/OR/XOR 8B, 4H, 2W 8B, 4H, 2W 8B, 4H, 2W 16B, 8H, 4W 8B, 4H, 2W Absolute difference 8B 16B, 8H, 4W 8BMaximum/minimum 8B, 4W 16B, 8H, 4WPack (2 nbits!nbits) 2W !2B, 4H!4B2*4H!8B 4H !4B, 2W!2H4W!4B, 8H!8B2W!2H, 2W!2B, 4H!4B Unpack/merge 2B !2W, 4B!4H2B!2W, 4B!4H4B!4W, 8B!8H4B!4H, 2*4B!8B Permute/shuffle 4H 16B, 8H, 4W Figure E.7 Summary multimedia support desktop processors. Note diversity support, little common across five architectures. fixed-width operations, performing multiple narrow operations either 64-bit 128-bit ALU. B stands byte (8 bits), H half word (16 bits), W word (32 bits). Thus,8B means operation 8 bytes single instruction. Note AltiVec assumes 128-bit ALU, rest assume 64 bits. Pack unpack use notation 2*2W mean 2 operands 2 words. table sim- plification full multimedia architectures, leaving many details. example, HP MAX2 includes instruc-tion calculate averages, SPARC VIS includes instructions set registers constants. Also, table notinclude memory alignment operation AltiVec, MAX, VIS.E.2 Signal Processing Embedded Applications: Digital Signal Processor ■E-11E.3 Embedded Benchmarks used case couple years ago embedded market, many manufacturers quoted Dhrystone performance, benchmark criticized andgiven desktop systems 20 years ago! mentioned earlier, theenormous variety embedded applications, well differences performancerequirements (hard real time, soft real time, overall cost-performance), makethe use single set benchmarks unrealistic. practice, many designers embedded systems devise benchmarks reflect application, either ker- nels stand-alone versions entire application. embedded applications characterized well kernel per- formance, best standardized set benchmarks appears new benchmarkset: EDN Embedded Microprocessor Benchmark Consortium (or EEMBC,pronounced “embassy ”). EEMBC benchmarks fall six classes (called “subcommittees ”in parlance EEMBC): automotive/industrial, consumer, telecommunications, digital entertainment, networking (currently second ver- sion), office automation (also second version subcommittee). Figure E.8 shows six different application classes, include 50 benchmarks. Although many embedded applications sensitive performance small kernels, remember often overall performance entire application(which may thousands lines) also critical. Thus, many embedded sys-tems, EMBCC benchmarks used partially assess performance. Benchmark type (“subcommittee ”)Number kernels Example benchmarks Automotive/industrial 16 6 microbenchmarks (arithmetic operations, pointer chasing, memory performance, matrix arithmetic, table lookup, bit manipulation), 5 automobile control benchmarks, 5 filter FFT benchmarks Consumer 5 5 multimedia benchmarks (JPEG compress/decompress, filtering, RGB conversions) Telecommunications 5 Filtering DSP benchmarks (autocorrelation, FFT, decoder, encoder)Digital entertainment 12 MP3 decode, MPEG-2 MPEG-4 encode decode (each applied five different datasets), MPEG Encode Floating Point, 4 benchmark tests common cryptographic standards algorithms (AES, DES, RSA,and Huffman decoding data decompression), enhanced JPEG andcolor-space conversion tests Networking version 2 6 IP Packet Check (borrowed RFC1812 standard), IP Reassembly, IP Network Address Translator (NAT), Route Lookup, OSPF, Quality Service (QOS), TCP Office automation version 26 Ghostscript, text parsing, image rotation, dithering, B /C19ezier Figure E.8 EEMBC benchmark suite, consisting 50 kernels six different classes. Seewww.eembc.org information benchmarks scores.E-12 ■Appendix E Embedded SystemsPower Consumption Efficiency Metric Cost power often least important performance embedded market. addition cost processor module (which includes anyrequired interface chips), memory often next costly part embeddedsystem. Unlike desktop server system, embedded systems havesecondary storage; instead, entire application must reside either FLASH orDRAM. many embedded systems, PDAs cell phones, con-strained cost physical size, amount memory needed appli-cation critical. Likewise, power often determining factor choosing processor, especially battery-powered systems. EEMBC EnergyBench provides data amount energy processor con- sumes running EEMBC ’s performance benchmarks. EEMBC-certified Energymark score optional metric device manufacturer may chooseto supply conjunction certified scores device performance wayof indicating processor ’s efficient use power energy. EEMBC stan- dardized use National Instruments ’LabVIEW graphical development environment data acquisition hardware implement EnergyBench. Figure E.9 shows relative performance per watt typical operating power. Compare figure Figure E.10 , plots raw performance, notice different results are. NEC VR 4122 clear advantage performanceper watt, second-lowest performing processor! viewpoint ofpower consumption, NEC VR 4122, designed battery-based sys-tems, big winner. IBM PowerPC displays efficient use power toachieve high performance, although 6 W typical, probably suitablefor battery-based devices. Relative performance per watt 03.54.0 3.0 2.5 2.01.5 1.0 0.5 Automotive Office TelecommAMD ElanSC520 AMD K6-2E+ IBM PowerPC 750CX NEC VR 5432 NEC VR 4122 Figure E.9 Relative performance per watt five embedded processors. power measured typical operating power processor include interface chips.E.3 Embedded Benchmarks ■E-13E.4 Embedded Multiprocessors Multiprocessors common server environments, several desktop multiprocessors available vendors, Sun, Compaq, Apple.In embedded space, number special-purpose designs used customizedmultiprocessors, including Sony PlayStation 2 (see Section E.5). Many special-purpose embedded designs consist general-purpose pro- grammable processor DSP special-purpose, finite-state machines used stream-oriented I/O. applications ranging computer graphics media processing telecommunications, style special-purpose multi-processor becoming common. Although interprocessor interactions suchdesigns highly regimented relatively simple —consisting primarily simple communication channel —because much design committed sil- icon, ensuring communication protocols among input/output proces-sors general-purpose processor correct major challenge suchdesigns. recently, seen first appearance, embedded space, embedded multiprocessors built several general-purpose processors. Thesemultiprocessors focused primarily high-end telecommunicationsand networking market, scalability critical. example design isthe MXP processor designed empowerTel Networks use voice-over-IPsystems. MXP processor consists four main components: ■An interface serial voice streams, including support handling jitter ■Support fast packet routing channel lookup ■A complete Ethernet interface, including MAC layer ■Four MIPS32 R4000-class processors, cache (a total of48 KB 12 KB per processor) Performance relative AMD Elan SC52014.0 12.010.0 8.06.04.02.0 0 Automotive Office TelecommAMD Elan SC520 AMD K6-2E+IBM PowerPC 750CXNEC VR 5432NEC VR 4122 Figure E.10 Raw performance five embedded processors. performance presented relative performance AMD ElanSC520.E-14 ■Appendix E Embedded SystemsThe MIPS processors used run code responsible maintaining voice-over-IP channels, including assurance quality service, echo cancel- lation, simple compression, packet encoding. Since goal run manyindependent voice streams possible, multiprocessor ideal solution. small size MIPS cores, entire chip takes 13.5 transistors. Future generations chip expected handle voice chan-nels, well sophisticated echo cancellation, voice activity detection,and sophisticated compression. Multiprocessing becoming widespread embedded computing arena two primary reasons. First, issues binary software compatibility, pla- gue desktop server systems, less relevant embedded space. Oftensoftware embedded application written scratch applicationor significantly modified (note also reason VLIW favored oversuperscalar embedded instruction-level parallelism). Second, applicationsoften natural parallelism, especially high end embedded space.Examples natural parallelism abound applications settop box, anetwork switch, cell phone (see Section E.7) game system (see Section E.5). lower barriers use thread-level parallelism together greater sen- sitivity die cost (and hence efficient use silicon) leading widespreadadoption multiprocessing embedded space, application needs growto demand performance. E.5 Case Study: Emotion Engine Sony PlayStation 2 Desktop computers servers rely memory hierarchy reduce average access time relatively static data, embedded applications data often continuous stream. applications still spatial locality, temporal locality much limited. give another look memory performance beyond desktop, section examines microprocessor heart Sony PlayStation 2. see,the steady stream graphics audio demanded electronic games leads adifferent approach memory design. style high bandwidth via many ded-icated independent memories. Figure E.11 shows block diagram Sony PlayStation 2 (PS2). sur- prisingly game machine, interfaces video, sound, DVD player. Surprisingly, two standard computer I/O buses, USB IEEE1394, PCMCIA slot found portable PCs, modem. additionsshow Sony greater plans PS2 beyond traditional games. Althoughit appears I/O processor (IOP) simply handles I/O devices gameconsole, includes 34 MHz MIPS processor also acts emulation com-puter run games earlier Sony PlayStations. also connects standard PCaudio card provide sound games.E.5 Case Study: Emotion Engine Sony PlayStation 2 ■E-15Thus, one challenge memory system embedded application act source destination extensive number I/O devices. PS2 designers met challenge two PC800 (400 MHz) DRDRAM chips using two channels,offering 32 MB storage peak memory bandwidth 3.2 GB/sec. ’s left figure basically two big chips: Graphics Synthesizer Emotion Engine.300 MHz Superscalar CPU Core w/128-bit SIMDEmotion Engine I/O processorGraphics Synthesizer 16 parallel pixel processors (150 MHz) Video memory (4 MB multiported embedded DRAM)Memory control10- channel DMAIPU (MPEG decoder)I/O I/FVector Unit 0 (VPU0) 128-bit/150-MHz busVector Unit 1 (VPU1)64-bit16-bit 16-bit150 MHz 400 MHz Graphics I/F32-bit 1024-bit NTSC, PAL, DTV, VESA1024-bit 512 37.5 MHz Main memory 32 MB DRDRAM34 MHz MIPS CPU (PlayStation compatible)48-channel sound chip Local bus USBDVD-ROM PCMCIA ModemI/O circuits IEEE-1394 Figure E.11 Block diagram Sony PlayStation 2. 10 DMA channels orchestrate transfers small memories chip, completed head toward Graphics Interface rendered Graphics Synthesizer. Graphics Synthesizer uses DRAM chip provide entire frame buffer plusgraphics processors perform rendering desired based display commands given Emotion Engine. embedded DRAM allows 1024-bit transfers pixel processors display buffer. Superscalar CPU 64-bit MIPS III two-instruction issue, comes two-way, set associative, 16 KBinstruction cache; two-way, set associative, 8 KB data cache; 16 KB scratchpad memory. extendedwith 128-bit SIMD instructions multimedia applications (see Section E.2 ). Vector Unit 0 primarily DSP-like coprocessor CPU (see Section E.2 ), operate 128-bit registers SIMD manner 8 bits 32 bits per word. 4 KB instruction memory 4 KB data memory. Vector Unit 1 similar functionsto VPU0, normally operates independently CPU contains 16 KB instruction memory 16 KB data memory. three units communicate 128-bit system bus, also 128-bit dedicated path CPU VPU0 128-bit dedicated path VPU1 Graphics Interface. Although VPU0and VPU1 identical microarchitectures, differences memory size units direct con- nections affect roles take game. 0.25-micron line widths, Emotion Engine chip uses 13.5M transistors 225 mm 2, Graphics Synthesizer 279 mm2. put perspective, Alpha 21264 microprocessor 0.25-micron technology 160 mm2and uses 15M transistors. (This figure based Figure 1 “Sony ’s Emotionally Charged Chip, ”Microprocessor Report 13:5.)E-16 ■Appendix E Embedded SystemsThe Graphics Synthesizer takes rendering commands Emotion Engine commonly called display lists. lists 32-bit commands tell renderer shape use place them, plus colors andtextures fill them. chip also highest bandwidth portion memory system. using embedded DRAM Graphics Synthesizer, chip contains fullvideo buffer andhas 2048-bit-wide interface pixel filling bottle- neck. embedded DRAM greatly reduces bandwidth demands theDRDRAM. illustrates common technique found embedded applications: separate memories dedicated individual functions inexpensively achieve greater memory bandwidth entire system. remaining large chip Emotion Engine, job accept inputs IOP create display lists video game enable 3D video trans-formations real time. major insight shaped design Emotion Engine:Generally, racing car game foreground objects constantlychanging background objects change less reaction events,although background screen. observation led split responsibilities. CPU works VPU0 tightly coupled coprocessor, every VPU0 instruction standard MIPS coprocessor instruction, addressesare generated MIPS CPU. VPU0 called vector processor, similarto 128-bit SIMD extensions multimedia found several desktop processors(see Section E.2). VPU1, contrast, fetches instructions data acts parallel CPU/VPU0, acting like traditional vector unit. split, flexible CPU/VPU0 handles foreground action VPU1 handles back- ground. deposit resulting display lists Graphics Interface sendthe lists Graphics Synthesizer. Thus, programmers Emotion Engine three processor sets choose implement programs: traditional 64-bit MIPS architectureincluding floating-point unit, MIPS architecture extended multimediainstructions (VPU0), independent vector processor (VPU1). accelerateMPEG decoding, another coprocessor (Image Processing Unit) act independent two. split function, question connect units together, make data flow units, provide thememory bandwidth needed units. mentioned earlier, Emo-tion Engine designers chose many dedicated memories. CPU 16 KBscratch pad memory (SPRAM) additi 16 KB instruction cache 8 KB data cache. VPU0 4 KB instruction memory 4 KB datamemory, VPU1 16 KB instruction memory 16 KB data mem- ory. Note four memories , caches larger memory else- where. memory latency 1 clock cycle. VPU1 morememory VPU0 creates bulk display lists becauseit largely acts independently.E.5 Case Study: Emotion Engine Sony PlayStation 2 ■E-17The programmer organizes memories two double buffers, one pair incoming DMA data one pair outgoing DMA data. programmer uses various processors transform data input buffer tothe output buffer. keep data flowing among units, programmer nextsets 10 DMA channels, taking care meet real-time deadline real-istic animation 15 frames per second. Figure E.12 shows organization supports two main operating modes: serial, CPU/VPU0 acts preprocessor give VPU1 tocreate Graphics Interface using scratchpad memory buffer, parallel, CPU/VPU0 VPU1 create display lists. display lists Graphics Synthesizer multiple context identifiers distinguishthe parallel display lists produce coherent final image. units Emotion Engine linked common 150 MHz, 128-bit- wide bus. offer greater bandwidth, also two dedicated buses: a128-bit path CPU VPU0 128-bit path VPU1and Graphics Interface. programmer also chooses bus use whensetting DMA channels. Looking big picture, server-oriented designer given problem, might see single common bus many local caches cache-coherent mechanisms keep data consistent. contrast, PlayStation 2 fol-lowed tradition embedded designers least nine distinctmemory modules. keep data flowing real time memory dis-play, PS2 uses dedicated memories, dedicated buses, DMA channels.Coherency responsibility programmer, and, given continuous flowfrom main memory graphics interface real-time requirements, programmer-controlled coherency works well application. VPU0 SPRAMParallel connection VPU1 Rendering engineMain memory CPUVPU0 SPRAMSerial connection Rendering engineCPUVPU1 Figure E.12 Two modes using Emotion Engine organization. first mode divides work two units allows Graphics Interface prop-erly merge display lists. second mode uses CPU/VPU0 filter send VPU1, display lists. programmer choose serial parallel data flow. SPRAM scratchpad memory.E-18 ■Appendix E Embedded SystemsE.6 Case Study: Sanyo VPC-SX500 Digital Camera Another familiar embedded system digital camera. consider Sanyo VPC-SX500. powered on, microprocessor camera firstruns diagnostics components writes error messages liquidcrystal display (LCD) back camera. camera uses 1.8-inchlow-temperature polysilicon thin-film transistor (TFT) color LCD. pho-tographer takes picture, first holds shutter halfway micropro- cessor take light reading. microprocessor keeps shutter open get necessary light, captured charge-coupled device (CCD) asred, green, blue pixels. CCD 1/2-inch, 1360 /C21024-pixel, progressive-scan chip. pixels scanned row row; passed throughroutines white balance, color, aliasing correction; stored a4 MB frame buffer. next step compress image standard format,such JPEG, store removable Flash memory. photographerpicks compression, camera called either fineornormal , com- pression ratio 10 20 times. 512 MB Flash memory store least 1200 fine-quality compressed images appr oximately 2000 normal-quality com- pressed images. microprocessor updates LCD display show thatthere room one less picture. Although previous paragraph covers basics digital camera, many features included: showing recorded images thecolor LCD display, sleep mode save battery life, monitoring battery energy,buffering allow recording rapid sequence uncompressed images, and, camera, video recording using MPEG format audio recording using WAV format. electronic brain camera embedded computer several special functions embedded chip [Okada et al. 1999]. Figure E.13 shows block diagram chip similar one camera. mentioned inSection E.1, chips called systems chip (SOCs) essentially integrate single chip parts found ona small printed circuit board pas t. SOC generally reduces size lowers power compared less integrat ed solutions. Sanyo claims SOC enables camera operate half number batteries offer asmaller form factor competitors ’cameras. higher performance, two buses. 16-bit bus many slower I/O devices: SmartMediainterface, program data memory , DMA. 32-bit bus SDRAM, signal processor (which connected CCD), MotionJPEG encoder, NTSC/PAL encoder (which connected theLCD). Unlike desktop microprocessors, note large variety I/O buses chip must integrate. 32-bit RISC MPU proprietary design runs 28.8 MHz, clock rate buses. 700 mW chip contains1.8M transistors 10.5 /C210.5 mm die implemented using 0.35-micron process.E.6 Case Study: Sanyo VPC-SX500 Digital Camera ■E-19E.7 Case Study: Inside Cell Phone Although gaming consoles digital cameras familiar embedded systems, today familiar embedded system cell phone. 1999, 76 million cellular subscribers United States, 25% growth rate year before. growth rate almost 35% per year worldwide, developingcountries find much cheaper install cellular towers copper-wire-basedinfrastructure. Thus, many countries, number cell phones use exceedsthe number wired phones use. surprisingly, cellular handset market growing 35% per year, 280 million cellular phone handsets sold worldwide 1999. put inperspective, year sales personal computers 120 million. numbers mean tremendous engineering resources available improve cell phones, cell phones probably leaders engineering innovation per cubicinch [Grice Kanellos 2000]. unveiling anatomy cell phone, let ’s try short introduction wireless technology.Audio D/A, A/D DRAM controllerDMA controllerPCMCIA controllerUART x 2SIO PIO PWMIrDACCD SDRAM DRAMSignal processorMJPEGNTSC/PAL encoder2-channel video D/ALCD/TV MIC SpeakerSmart Media Flash (program)SSFDC controllerRISCSDRAM controller RS-23216 bits16 bits 32 bits10 bits 16 bitsSignal bus CPU busBus bridge IrDA portPCMCIA cardOthers Figure E.13 system chip (SOC) found Sanyo digital cameras. block diagram, found Okada et al. [1999], predecessor SOC camera described text. successor SOC, called Super Advanced IC, uses three buses instead two, operates 60 MHz, consumes 800 mW, fits 3.1M transistors 10.2 /C2 10.2 mm die using 0.35-micron process. Note embedded system twice many transistors thestate-of-the-art, high-performance microprocessor 1990! SOC figure limited processing 1024 /C2 768 pixels, successor supports 1360 /C21024 pixels.E-20 ■Appendix E Embedded SystemsBackground Wireless Networks Networks created thin air well copper glass, creating wireless networks. Much section based report National Research Council [1997]. radio wave electromagnetic wave propagated antenna. Radio waves modulated, means sound signal superimposed onthe stronger radio wave carries sound signal, hence called carrier signal. Radio waves particular wavelength frequency: measured either length complete wave number waves per second. Long waves low frequencies, short waves high frequencies. FM radio stations transmit band 88 MHz 108 MHz using frequency mod-ulations (FM) record sound signal. tuning different frequencies, radio receiver pick specific signal. addition FM radio, frequencies reserved citizensband radio, television, pagers, air traffic control radar, Global Positioning System,and on. United States, Federal Communications Commission decideswho gets use frequencies purpose. Thebit error rate (BER) wireless link determined received signal power, noise due interference caused receiver hardware, interference fromother sources, characteristics channel. Noise typically proportional tothe radio frequency bandwidth, key measure signal-to-noise ratio (SNR) required achieve given BER. Figure E.14 lists challenges wire- less communication. Typically, wireless communication selected communicating devices mobile wiring inconvenient, means wireless network must rearrange dynamically. rearrangement makes routing Challenge Description Impact Path loss Received power divided transmitted power; radio must overcome signal-to-noise ratio(SNR) noise interference. Path loss exponential distance depends interference 100 meters.1 W transmit power, 1 GHz transmit frequency, 1 Mbit/sec data rate 10/C07BER, distance radios 728 meters free space vs. 4 meters dense jungle. Shadow fading Received signal blocked objects, buildings outdoors, walls indoors; increase power improve received SNR. depends number objects dielectric properties.If transmitter moving, need change transmit power ensure received SNR region. Multipath fading Interference multiple versions signal arrive different times, determined timebetween fastest signal slowest signal relative signal bandwidth.900 MHz transmit frequency signal power changes every 30 cm. Interference Frequency reuse, adjacent channel, narrow band interference.Requires filters, spread spectrum. Figure E.14 Challenges wireless communication.E.7 Case Study: Inside Cell Phone ■E-21more challenging. second challenge wireless signals protected hence subject mutual interference, especially devices move. Power another challenge wireless communication, devices tend tobe battery powered antennas radiate power communicate littleof reaches receiver. result, raw bit error rates typically thousand amillion times higher copper wire. two primary architectures wireless networks: base station archi- tectures peer-to-peer architectures. Base stations connected landlines longer-distance communication, mobile units communicate single local base station. Peer-to-peer architectures allow mobile units commu- nicate other, messages hop one unit next deliveredto desired unit. Although peer-to-peer reconfigurable, base stationstend reliable since one hop device thestation. Cellular telephony , popular example wireless networks, relies radio base stations. Cellular systems exploit exponential path loss reuse frequency spatially separated locations, thereby greatly increasing number customers served. Cellular systems divide city nonoverlapping hexagonal cells use different frequencies nearby, reusing frequency cells farenough apart mutual interference acceptable. intersection three hexagonal cells base station transmitters antennas connected switching office coordinates handoffs whena mobile device leaves one cell goes another, well accepts placescalls landlines. Depending topography, population, on, radius ofa typical cell 2 10 miles. Cell Phone Figure E.15 shows components radio, heart cell phone. Radio signals first received antenna, amplified, passed mixer,then filtered, demodulated, finally decoded. antenna acts interfacebetween medium radio waves travel electronics thetransmitter receiver. Antennas designed work best particular direc- tions, giving transmission reception directional properties. Modulation encodes information amplitude, phase, frequency signal increaseits robustness impaired conditions. Radio transmitters go samesteps, opposite order. Originally, components analog, time replaced digital components, requiring radio signal converted analog dig-ital. desire flexibility number radio bands led software routinesreplacing functions programmable chips, digital signal processors. processors typically found mobile devices, empha- sis placed performance per joule extend battery life, performance persquare millimeter silicon reduce size cost, bytes per task reducememory size.E-22 ■Appendix E Embedded SystemsFigure E.16 shows generic block diagram electronics cell phone handset, DSP performing signal processing microcontrollerhandling rest tasks. Cell phone handsets basically mobile computersacting radio. include standard I/O devices —keyboard LCD dis- play—plus microphone, speaker, antenna wireless networking. Battery efficiency affects sales, standby power waiting call forminutes speaking. cell phone turned on, first task find cell. scans full bandwidth find strongest signal, keeps every seven secondsor signal strength drops, since designed work moving vehicles.It picks unused radio channel. local switching office registers cellphone records phone number electronic serial number, assigns voice channel phone conversation. sure cell phone got right channel, base station sends special tone it, cell phone sends back toacknowledge it. cell phone times 5 seconds ’t hear super- visory tone, starts process again. original base station makes ahandoff request incoming base station signal strength drops offs.RF amp FilterAntennaDemodulator Decoder Mixer Figure E.15 radio receiver consists antenna, radio frequency amplifier, mixer, filters, demodulator, decoder. mixer accepts two signal inputs forms out- put signal sum difference frequencies. Filters select narrower band fre-quencies pass next stage. Modulation encodes information make robust. Decoding turns signals information. Depending application, elec- trical components either analog digital. example, car radio analogcomponents, PC modem digital except amplifier. Today analog silicon chips used RF amplifier first mixer cellular phones. Speaker MicrophoneDSP Micro- controllerAntennaRF receiver (Rx) RF transmitter (Tx) Display Keyboard Figure E.16 Block diagram cell phone. DSP performs signal processing steps Figure E.15 , microcontroller controls user interface, battery manage- ment, call setup. (Based Figure 1.3 Groe Larson [2000].)E.7 Case Study: Inside Cell Phone ■E-23To achieve two-way conversation radio, frequency bands set aside direction, forming frequency pair channel. original cellular base stations transmitted 869.04 893.97 MHz (called forward path ), cell phones transmitted 824.04 848.97 MHz (called reverse path ), frequency gap keep interfering other. Cells might 4 80 channels. Channels divided setup channels call setup voice channels handle data voice traffic. communication done digitally, like modem, 9600 bits/sec. Since wireless lossy medium, especially moving vehicle, handset sends message five times. preserve battery life, original cell phones typically transmit two signal strengths —0.6 W 3.0 W —depending distance cell. relatively low power allows smaller batteries thus smal- ler cell phones, also aids frequency reuse, key cellular telephony. Figure E.17 shows circuit board Nokia digital phone, com- ponents identified. Note board contains two processors. Z-80 microcon- troller responsible controlling functions board, I/O keyboard display, coordinating base station. DSP handles signal compression decompression. addition dedicated chips analog-to-digital digital-to-analog conversion, amplifiers, power manage- ment, RF interfaces. 2001, cell phone 10 integrated circuits, including parts made exotic technologies like gallium arsinide silicon germanium well standard CMOS. economics desire flexibility shrunk chips. However, SOCs still contain separate microcontroller DSP, code implementing many functions described. RF power Audio D/A A/D Battery Memory Microprocessor control logic Figure E.17 Circuit board Nokia cell phone. (Courtesy HowStuffWorks, Inc.)E-24 ■Appendix E Embedded SystemsCell Phone Standards Evolution Improved communication speeds cell phones developed multiple standards. Code division multiple access (CDMA), one popular example, uses wider radio frequency band path original cell phones, calledadvanced mobile phone service (AMPS), mostly analog system. wider fre- quency makes difficult block called spread spectrum. stan- dards time division multiple acces (TDMA) global system mobile communication (GSM). second-generation standards —CDMA, GSM, TDMA —are mostly digital. big difference CDMA callers share channel, operates much higher rate, distinguishes different calls byencoding one uniquely. CDMA phone call starts 9600 bits/sec; itis encoded transmitted equal-sized messages 1.25 Mbits/sec. Ratherthan send signal five times AMPS, bit stretched takes 11times minimum frequency, thereby accommodating interference yet suc-cessful transmission. base station receives messages, separates theminto separate 9600 bit/sec streams call. enhance privacy, CDMA uses pseudorandom sequences set 64 predefined codes. synchronize handset base station pick com-mon pseudorandom seed, CDMA relies clock Global PositioningSystem, continuously transmits accurate time signal. carefully select-ing codes, shared traffic sounds like random noise listener. Hence, asmore users share channel noise, signal-to-noise ratio grad-ually degrades. Thus, capacity CDMA system matter taste,depending upon sensitivity listener background noise. addition, CDMA uses speech compression varies rate data trans- ferred depending upon much activity going call. tech-niques preserve bandwidth, allows calls per cell. CDMA mustregulate power carefully signals near cell tower overwhelm thosefrom far away, goal signals reaching tower samelevel. side benefit CDMA handsets emit less power, helpsbattery life increases capacity users close tower. Thus, compared AMPS, CDMA improves capacity system order magnitude, better call quality, better battery life, enhances users’privacy. considerable commercial turmoil, new third- generation standard called International Mobile Telephony 2000 (IMT-2000), based primarily two competing versions CDMA one TDMA. stan-dard may lead cell phones work anywhere world. E.8 Concluding Remarks Embedded systems broad category computing devices. appendix shown aspects this. example, TI 320C55 DSP rela- tively “RISC-like ”processor designed embedded applications, veryE.8 Concluding Remarks ■E-25fine-tuned capabilities. end spectrum, TI 320C64x high-performance, eight-issue VLIW processor demanding tasks. processors must operate battery power alone; others luxury beingplugged line current. Unifying need perform level ofsignal processing embedded applications. Media extensions attempt mergeDSPs general-purpose processing abilities make proces-sors usable signal processing applications. examined several case studies,including Sony PlayStation 2, digital cameras, cell phones. PS2 per-forms detailed three-dimensional graphics, whereas cell phone encodes decodes signals according elaborate communication standards. system architectures different general-purpose desktop serverplatforms. general, architectural decisions seem practical general-purpose applications, multiple levels caching out-of-order superscalarexecution, much less desirable embedded applications. due chiparea, cost, power, real-time constraints. programming model thesesystems present places demands programmer compilerfor extracting parallelism.E-26 ■Appendix E Embedded SystemsF.1 Introduction F-2 F.2 Interconnecting Two Devices F-6 F.3 Connecting Two Devices F-20 F.4 Network Topology F-30 F.5 Network Routing, Arbitration, Switching F-44 F.6 Switch Microarchitecture F-56 F.7 Practical Issues Commercial Interconnection Networks F-66 F.8 Examples Interconnection Networks F-73 F.9 Internetworking F-85 F.10 Crosscutting Issues Interconnection Networks F-89 F.11 Fallacies Pitfalls F-92 F.12 Concluding Remarks F-100 F.13 Historical Perspective References F-101 References F-109 Exercises F-111F Interconnection Networks Revised Timothy M. Pinkston, University Southern California; Jos/C19e Duato, Universitat Politècnica de València, Simula “The Medium Message ”because medium shapes controls search form human associations actions. Marshall McLuhan Understanding Media (1964) marvels —of film, radio, television —are marvels one- way communication, communication all. Milton Mayer Remote Possibility Communication (1967) interconnection network heart parallel architecture. Chuan-Lin Wu Tse-Yun Feng Interconnection Networks Parallel Distributed Processing (1984) Indeed, system complexity integration continues increase, many designers finding efficient route packets, wires. Bill Dally Principles Practices Interconnection Networks (2004)F.1 Introduction Previous chapters appendices cover components single computer give little consideration interconnection components mul-tiple computer systems interconnected. aspects computer architecturehave gained significant importance recent years. appendix see toconnect individual devices together community communicating devices,where term device generically used signify anything component set components within computer single computer system com- puters. Figure F.1 shows various elements comprising community: end nodes consisting devices associated hardware software interfaces,links end nodes interconnection network, interconnection net-work. Interconnection networks also called networks, communication subnets , orcommunication subsystems . interconnection multiple networks called internetworking. relies communication standards convert information one kind network another, Internet. several reasons computer architects devote attention interconnection networks. addition providing external connectivity, networksare commonly used interconnect components within single computer atmany levels, including processor microarchitecture. Networks long beenused mainframes, today designs found personal computers aswell, given high demand communication bandwidth needed enableincreased computing power storage capacity. Switched networks replacingbuses normal means communication computers, I/O devices, boards, chips, even modules inside chips. Computer architects must understand interconnect problems solutions orderto effectively design evaluate computer systems. Interconnection networks cover wide range application domains, much like memory hierarchy covers wide range speeds sizes. Networksimplemented within processor chips systems tend share characteristicsmuch common processors memory, relying high-speed hard-ware solutions less flexible software stack. Networks implemented across systems tend share much common storage I/O, relying operating system software protocols high-speed hardware —though seeing convergence days. Across domains, performance includeslatency effective bandwidth, queuing theory valuable analytical toolin evaluating performance, along simulation techniques. topic vast —portions Figure F.1 subject entire books college courses. goal appendix provide computer architectan overview network problems solutions. appendix gives introductory explanations key concepts ideas, presents architectural implications inter- connection network technology techniques, provides useful references tomore detailed descriptions. also gives common framework evaluating alltypes interconnection networks, using single set terms describe basicF-2 ■Appendix F Interconnection Networksalternatives. see, many types networks common preferred alter- natives, others best solutions quite different. differencesbecome apparent crossing networking domains. Interconnection Network Domains Interconnection networks designed use different levels within acrosscomputer systems meet operational demands various application areas — high-performance computing, storage I/O, cluster/workgroup/enterprise systems, internetworking, on. Depending number devices connectedand proximity, group interconnection networks four major net-working domains: ■On-chip networks (OCNs) —Also referred network-on-chip (NoC), type network used interconnecting microarchitecture functional units,register files, caches, compute tiles, processor IP cores within chips multichip modules. Current near future OCNs support connection tens hundred devices maximum interconnectiondistance order centimeters. OCNs used high-performancechips custom designed mitigate chip-crossing wire delay problemscaused increased technology scaling transistor integration, though someproprietary designs gaining wider use (e.g., IBM ’s CoreConnect, ARM ’s AMBA, Sonic ’s Smart Interconnect). Examples current OCNs found Intel Teraflops processor chip [Hoskote07], connecting 80 simple cores; Intel Single-Chip Cloud Computer (SCCC) [Howard10], connecting 48 IA-32 architecture cores; Tilera ’s TILE-Gx line processors [TILE- GX], connecting 100 processing cores 4Q 2011 using TSMC ’s 40 nanome- ter process 200 cores planned 2013 (code named “Stratton ”) usingDevice LinkSW interfaceEnd node HW interfaceDevice LinkSW interfaceEnd node HW interfaceDevice LinkSW interfaceEnd node HW interfaceDevice LinkSW interfaceEnd node HW interface Interconnection network Figure F.1 conceptual illustration interconnected community devices.F.1 Introduction ■F-3TSMC ’s 28 nanometer process. networks peak 256 GBps Intel prototypes 200 Tbps TILE-Gx100 processor. detailed information OCNs provided Flich [2010] . ■System/storage area networks (SANs) —This type network used inter- processor processor-memory interconnections within multiprocessor multicomputer systems, also connection storage I/O compo-nents within server data center environments. Typically, several hundredsof devices connected, although supercomputer SANs supportthe interconnection many thousands devices, like IBM Blue Gene/Lsupercomputer. maximum interconnection distance covers relativelysmall area —on order tens meters usually —but SANs distances spanning hundred meters. example, InfiniBand , popular SAN standard introduced late 2000, supports system storage I/O inter- connects 120 Gbps distance 300 m. ■Local area networks (LANs) —This type network used intercon- necting autonomous computer systems distributed across machine roomor throughout building campus environment. Interconnecting PCs ina cluster prime example. Originally, LANs connected hun-dred devices, bridging LANs connect thou-sand devices. maximum interconnect distance covers area fewkilometers usually, distance spans tens kilome-ters. instance, popular enduring LAN, Ethernet , 10 Gbps standard version supports maximum performance distance 40 km. ■Wide area networks (WANs) —Also called long-haul networks , WANs con- nect computer systems distributed across globe, requires internet-working support. WANs connect many millions computers distancescales many thousands kilometers. Asynchronous Transfer Mode(ATM) example WAN. Figure F.2 roughly shows relationship networking domains terms number devices interconnected distance scales. Overlap exists networks one dimensions, leads product competition. network solutions become commercial stan-dards others remain proprietary. Although preferred solutions may sig-nificantly differ one interconnection network domain another dependingon design requirements, problems concepts used address networkproblems remain remarkably similar across domains. matter targetdomain, networks designed bottleneck systemperformance cost efficiency. Hence, ultimate goal computer architects design interconnection networks lowest possible cost capable transferring maximum amount available information shortestpossible time.F-4 ■Appendix F Interconnection NetworksApproach Organization Appendix Interconnection networks well understood taking top-down approach unveiling concepts complexities involved designing them. thisby viewing network initially opaque “black box ”that simply ideally performs certain necessary functions. systematically open various layersof black box, allowing complex concepts nonideal network behaviorto revealed. begin discussion first considering interconnection two devices Section F.2 , black box network viewed simple dedicated link network —that is, wires collections wires running bidi- rectionally devices. consider interconnection thantwo devices Section F.3 , black box network viewed shared linknetwork switched point-to-point network connecting devices. continue peel away various layers black box considering moredetail network topology ( Section F.4 ); routing, arbitration, switching (Section F.5 ); switch microarchitecture ( Section F.6 ). Practical issues com- mercial networks considered Section F.7 , followed examples illustrating trade-offs type network Section F.8 . Internetworking briefly discussed Section F.9 , additional crosscutting issues interconnection net- works presented Section F.10 .Section F.11 gives common fallacies1 10 100 1000 Number devices interconnectedSAN OCNLANWAN 10,000 >100,000Distance (meters)5 × 106 5 × 103 5 × 100 5 × 10–3 Figure F.2 Relationship four interconnection network domains terms number devices connected distance scales: on-chip network (OCN), sys-tem/storage area network (SAN), local area network (LAN), wide area network (WAN). Note overlapping ranges networks compete. supercomputer systems use proprietary custom networks intercon-nect several thousands computers, systems, multicomputer clus- ters, use standard commercial networks.F.1 Introduction ■F-5and pitfalls related interconnection networks, Section F.12 presents concluding remarks. Finally, provide brief historical perspective suggested reading Section F.13. F.2 Interconnecting Two Devices section introduces basic concepts required understand communi-cation two networked devices takes place. includes concepts thatdeal situations receiver may ready process incomingdata sender situations transport errors may occur. easeunderstanding, black box network point conceptualized anideal network behaves simple dedicated links two devices. Figure F.3 illustrates this, unidirectional wires run device device B vice versa , end node contains buffer hold data. Regardless network complexity, whether dedicated link not, connection exists fromeach end node device network inject receive information to/from thenetwork. first describe basic functions must performed endnodes commence complete communication, discuss networkmedia basic functions must performed network carryout communication. Later, simple performance model given, along sev- eral examples highlight implications key network parameters. Network Interface Functions: Composing Processing Messages Suppose want two networked devices read word ’s mem- ory. unit information sent received called message . acquire desired data, two devices must first compose send certain type messagein form request containing address data within device. address (i.e., memory operand location) allows receiver identifywhere find information requested. processing request, device composes sends another type message, reply , containing data. address data information typically referred messagepayload. B enihcaM enihcaM Figure F.3 simple dedicated link network bidirectionally interconnecting two devices.F-6 ■Appendix F Interconnection NetworksIn addition payload, every message contains control bits needed network deliver message process receiver. typical bits distinguish different types messages (e.g., request, reply, requestacknowledge, reply acknowledge) bits allow network transport theinformation properly destination. additional control bits encodedin header and/or trailer portions message, depending location relative message payload. example, Figure F.4 shows format message simple dedicated link network shown Figure F.3 . example shows single-word payload, messages interconnection networks include several thousands words. message transport network occurs, messages com- posed. Likewise, upon receipt network, must processed. Theseand functions described role network interface (also referred channel adapter ) residing end nodes. Together direct memory access (DMA) engine link drivers transmit/receive messagesto/from network, dedicated memory register(s) may used bufferoutgoing incoming messages. Depending network domain design specifications network, network interface hardware may consist noth- ing communicating device (i.e., OCNs SANs) ora separate card integrates several embedded processors DMA engines withthousands megabytes RAM (i.e., many SANs LANsand WANs). addition hardware, network interfaces include software firmware perform needed operations. Even simple example shown Figure F.3 may invoke messaging software translate requests replies messages appropriate headers. way, user applications need worry composing processing messages tasks performed automatically lowerlevel. application program usually cooperates operating runtime Destination port Message ID DataSequence number Type 00 = Request 01 = Reply 10 = Request acknowledge 11 = Reply acknowled geChecksumHeader PayloadTrailer Figure F.4 example packet format header, payload, checksum trailer.F.2 Interconnecting Two Devices ■F-7system send receive messages. network likely shared many processes running device, operating system cannot allow messages intended one process received another. Thus, messaging softwaremust include protection mechanisms distinguish processes. dis-tinction could made expanding header portnumber known sender intended receiver processes. addition composing processing messages, additional functions need performed end nodes establish communication among commu-nicating devices. Although hardware support reduce amount work, done software. example, networks specify maximum amount information transferred (i.e., maximum transfer unit ) network buffers dimensioned appropriately. Messages longer maximumtransfer unit divided smaller units, called packets (ordatagrams ), transported network. Packets reassembled messages des-tination end node delivery application. Packets belonging samemessage distinguished others including message ID field packet header. packets arrive order destination, reordered reassembled message. Another field packet header containing sequence number usually used purpose. sequence steps end node follows commence complete com- munication network called communication protocol . generally symmetric reversed steps sending receiving information. Commu-nication protocols implemented combination software hardware toaccelerate execution. instance, many network interface cards implement hard-ware timers well hardware support split messages packets reas- semble them, compute cyclic redundancy check (CRC) checksum, handle virtual memory addresses, on. network interfaces include extra hardware offload protocol processing host computer, TCP offload engines LANs WANs. But, interconnection networks SANs low latency requirements,this may enough even lighter-weight communication protocols areused message passing interface (MPI). Communication performancecan improved bypassing operating system (OS). OS bypassing implemented directly allocating message buffers network interface memory applications directly write read buffers. Thisavoids extra memory-to-memory copies. corresponding protocols referredto zero-copy protocols user-level communication protocols. Protection still maintained calling OS allocate buffers initializationand preventing unauthorized memory accesses hardware. general, following steps needed send message end node devices network: 1.The application executes system call, copies data sent operating system network interface buffer, divides message packets(if needed), composes header trailer packets.F-8 ■Appendix F Interconnection Networks2.The checksum calculated included header trailer packets. 3.The timer started, network interface hardware sends packets. Message reception reverse order: 3.The network interface hardware receives packets puts buffer operating system buffer. 2.The checksum calculated packet. checksum matches sender ’s checksum, receiver sends acknowledgment back packet sender. not, deletes packet, assuming sender resend thepacket associated timer expires. 1.Once packets pass test, system reassembles message, copies data user ’s address space, signals corresponding application. sender must still react packet acknowledgments: ■When sender gets acknowledgment, releases copy corre- sponding packet buffer. ■If sender reaches time-out instead receiving acknowledgment, itresends packet restarts timer. protocol implemented network end nodes support communi- cation, protocols also used across network structure physical, datalink, network layers responsible primarily packet transport, flow control,error handling, functions described next. Basic Network Structure Functions: Media Form Factor, Packet Transport, Flow Control, Error Handling packet ready transmission source, injected network using dedicated hardware network interface. hardware includes transceiver circuits drive physical network media —either electrical optical. type media andform factor depends largely interconnect distances certain signaling rates (e.g., transmission speed) besustainable. centimeter less distances chip multichip module, typi-cally middle upper copper metal layers used interconnects multi-Gbps signaling rates per line. dozen layers copper traces tracksimprinted circuit boards, midplanes, backplanes used Gbpsdifferential-pair signaling rates distances meter so. Category 5E unshielded twisted-pair copper wiring allows 0.25 Gbps transmission speed distances 100 meters. Coaxial copper cables deliver 10 Mbps kilometerdistances. conductor lines, distance usually traded highertransmission speed, certain point. Optical media enable faster transmissionF.2 Interconnecting Two Devices ■F-9speeds distances kilometers. Multimode fiber supports 100 Mbps transmis- sion rates kilometers, expensive single-mode fiber supports Gbps transmission speeds distances several kilometers. Wavelength divi-sion multiplexing allows several times bandwidth achieved fiber (i.e.,by factor number wavelengths used). hardware used drive network links may also include encoders encode signal format binary suitable given trans-port distance. Encoding techniques use multiple voltage levels, redundancy,data control rotation (e.g., 4b5b encoding), and/or guaranteed minimum number signal transitions per unit time allow clock recovery receiver. signal decoded receiver end, packet stored inthe corresponding buffer. operations performed networkphysical layer, details beyond scope appendix. Fortu-nately, need worry them. perspective data linkand higher layers, physical layer viewed long linear pipeline withoutstaging signals propagate waves network transmissionmedium. functions generally referred packet transport . Besides packet transport, network hardware software jointly responsible data link network protocol layers ensuring reliabledelivery packets. responsibilities include: (1) preventing senderfrom sending packets faster rate processed receiver,and (2) ensuring packet neither garbled lost transit. firstresponsibility met either discarding packets receiver bufferis full later notifying sender retransmit them, notifying senderto stop sending packets buffer becomes full resume later room packets. latter strategy generally known flow control . several interesting techniques commonly used implement flow control beyond simple handshaking sender receiver. popular techniques Xon/Xoff (also referred Stop & Go ) credit-based flow control. Xon/Xoff consists receiver notifying sender either stop orto resume sending packets high low buffer occupancy levels reached,respectively, hysteresis reduce number notifications. Notifica-tions sent “stop”and“go”signals using additional control wires encoded control packets. Credit-based flow control typically uses credit counter sender initially contains number credits equal number buffers atthe receiver. Every time packet transmitted, sender decrements creditcounter. receiver consumes packet buffer, returns credit tothe sender form control packet notifies sender increment itscounter upon receipt credit. techniques essentially control flow ofpackets network throttling packet injection sender receiver reaches low watermark sender runs credits. Xon/Xoff usually generates much less control traffic credit-based flow control notifications sent high low buffer occupancylevels crossed. hand, credit-based flow control requires less thanhalf buffer size required Xon/Xoff. Buffers Xon/Xoff must largeF-10 ■Appendix F Interconnection Networksenough prevent overflow “stop”control signal reaches sender. Overflow cannot happen using credit-based flow control sender run credits, thus stopping transmission. schemes, full linkbandwidth utilization possible buffers large enough distanceover communication takes place. Let’s compare buffering requirements two flow control techniques simple example covering various interconnection network domains. Example Suppose dedicated-link network raw data bandwidth 8 Gbps link direction interconnecting two devices. Packets 100 bytes (including header) continuously transmitted one device fully utilize network bandwidth. minimum amount credits bufferspace required credit-based flow control assuming interconnect distances of1 cm, 1 m, 100 m, 10 km link propagation delay taken account?How minimum buffer space compare Xon/Xoff? Answer start, receiver buffer initially empty sender contains number credits equal buffer capacity. sender consume credit every time packet transmitted. sender continue transmitting packets network speed, first returned credit must reach sender sender runs outof credits. receiving first credit, sender keep receiving creditsat rate transmits packets. considering propagation delayover link sources delay overhead, null processing time thesender receiver assumed. time required first credit reach thesender since started transmission first packet equal round-trippropagation delay packet transmitted receiver return credit transmitted back sender. time must less equal packet transmission time multiplied initial credit count: Packet propagation delay + Credit propagation delay /C20Packet size Bandwidth/C2Credit count speed light 300,000 km/sec. Assume achieve 66% conductor. Thus, minimum number credits distance given Distance 2=3/C2300,000 km =sec/C18/C19 /C22/C20100 bytes 8 Gbits =sec/C2Credit count credit represents one packet-sized buffer entry, minimum amount credits (and, likewise, buffer space) needed device one 1 cm and1 distances, 10 100 distance, 1000 packets 10 km distance.For Xon/Xoff, minimum buffer size corresponds buffer fragment fromthe high occupancy level top buffer low occupancy level bottom buffer. added hysteresis occupancy levels reduce notifications, minimum buffer space Xon/Xoff turns outto twice credit-based flow control.F.2 Interconnecting Two Devices ■F-11Networks implement flow control need drop packets sometimes referred lossless networks; networks drop packets some- times referred lossy networks. single difference way packets handled network drastically constrains kinds solutions implemented address related network problems, including packet routing, congestion, deadlock, reliability, see later appendix. Thisdifference also affects performance significantly dropped packets need beretransmitted, thus consuming link bandwidth suffering extra delay.These behavioral performance differences ultimately restrict interconnec-tion network domains certain solutions applicable. instance, mostnetworks delivering packets relatively short distances (e.g., OCNs SANs)tend implement flow control; hand, networks delivering packets relatively long distances (e.g., LANs WANs) tend designed drop packets. shorter distances, delay propagating flow control informa-tion back sender negligible, longer distance scales. Thekinds applications usually run also influence choice lossless ver-sus lossy networks. instance, dropping packets sent Internet client like aWeb browser affects delay observed corresponding user. However,dropping packet sent process parallel application may lead sig-nificant increase overall execution time application packet ’s delay critical path. second responsibility ensuring packets neither garbled lost transit met implementing mechanisms detect recover fromtransport errors. Adding checksum error detection field thepacket format, shown Figure F.4 , allows receiver detect errors. redundant information calculated packet sent checked uponreceipt. receiver sends acknowledgment form control packetif packet passes test. Note acknowledgment control packet may simultaneously contain flow control information (e.g., credit stop signal), thus reducing control packet overhead. described earlier, common way torecover errors timer record time packet sent topresume packet lost erroneously transported timer expires beforean acknowledgment arrives. packet resent. communication protocol across network network end nodes must handle many issues packet transport, flow control, reliability.For example, two devices different manufacturers, might order bytes differently within word (Big Endian versus Little Endian byte ordering). protocol must reverse order bytes word part deliverysystem. must also guard possibility duplicate packets delayedpacket become unstuck. Depending system requirements, pro-tocol may implement pipelining among operations improve perfor- mance. Finally, protocol may need handle network congestion preventperformance degradation two devices connected, describedlater Section F.7 .F-12 ■Appendix F Interconnection NetworksCharacterizing Performance: Latency Effective Bandwidth covered basic steps sending receiving messages two devices, discuss performance. start discussing thelatency transporting single packet. discuss effective bandwidth(also known throughput) achieved transmission multiplepackets pipelined network packet level. Figure F.5 shows basic components latency single packet. Note latency components broken later sections inter-nals “black box ”network revealed. timing parameters Figure F.5 apply many interconnection network domains: inside chip, chips board, boards chassis, chassis within computer, betweencomputers cluster, clusters, on. values may change, thecomponents latency remain same. following terms often used loosely, leading confusion, define precisely: ■Bandwidth —Strictly speaking, bandwidth transmission medium refers range frequencies attenuation per unit length introduced medium certain threshold. must distinguished thetransmission speed , amount information transmitted medium per unit time. example, modems successfully increased transmis-sion speed late 1990s fixed bandwidth (i.e., 3 KHz bandwidthprovided voice channels telephone lines) encoding voltagelevels and, hence, bits per signal cycle. However, consistent Sender overhead Sender ReceiverTransmission time (bytes/bandwidth) Time flightTransmission time (bytes/bandwidth)Receiver overhead Transport latency Total latency Time Figure F.5 Components packet latency. Depending whether OCN, SAN, LAN, WAN, relative amounts sending receiving overhead, time flight,and transmission time usually quite different illustrated here.F.2 Interconnecting Two Devices ■F-13its widely understood meaning, use term band-width refer maximum rate information transferred, information includes packet header, payload, trailer. units traditionally bitsper second, although bytes per second sometimes used. term bandwidth also used mean measured speed medium (i.e., network links).Aggregate bandwidth refers total data bandwidth supplied net- work, effective bandwidth orthroughput fraction aggregate band- width delivered network application. ■Time flight —This time first bit packet arrive receiver, including propagation delay links delays due otherhardware network link repeaters network switches. unitof measure time flight milliseconds WANs, microseconds LANs, nanoseconds SANs, picoseconds OCNs. ■Transmission time —This time packet pass network, including time flight. One way measure difference time first bit packet arrives receiver thelast bit packet arrives receiver. definition, transmission timeis equal size packet divided data bandwidth networklinks. measure assumes packets contending thatbandwidth (i.e., zero-load no-load network). ■Transport latency —This sum time flight transmission time. Transport latency time packet spends interconnection net-work. Stated alternatively, time first bit packetis injected network last bit packet arrives receiver. include overhead preparing packet sender processing arrives receiver. ■Sending overhead —This time end node prepare packet (as opposed message) injection network, including hard-ware software components. Note end node busy entiretime, hence use term overhead . end node free, sub- sequent delays considered part transport latency. assume thatoverhead consists constant term plus variable term depends onpacket size. constant term includes memory allocation, packet headerpreparation, setting DMA devices, on. variable term mostly due copies buffer buffer usually negligible short packets. ■Receiving overhead —This time end node process incoming packet, including hardware software components. also assume overhead consists constant term plus variable term dependson packet size. general, receiving overhead larger sendingoverhead. example, receiver may pay cost interrupt mayhave reorder reassemble packets messages.F-14 ■Appendix F Interconnection NetworksThe total latency packet expressed algebraically following: Latency ¼Sending overhead + Time flight +Packet size Bandwidth+ Receiving overhead Let’s see various components transport latency sending receiving overheads change importance go across interconnectionnetwork domains: OCNs SANs LANs WANs. Example Assume dedicated link network data bandwidth 8 Gbps link direction interconnecting two devices within OCN, SAN,LAN, WAN, wish transmit packets 100 bytes (including theheader) devices. end nodes per-packet sending overheadofx+0.05 ns/byte receiving overhead 4/3( x)+0.05 ns/byte, xis 0μs OCN, 0.3 μs SAN, 3 μs LAN, 30 μs WAN, typical network types. Calculate total latency send packets fromone device interconnection distances 0.5 cm, 5 m, 5000 m, and5000 km assuming time flight consists link propagation delay(i.e., switching sources delay). Answer Using expression calculation propagation delay conductor given previous example, plug parameters networks find total packet latency. OCN: Latency ¼Sending overhead + Time flight +Packet size Bandwidth+ Receiving overhead ¼5n +0:5c 2=3/C2300,000 km =sec+100 bytes 8 Gbits =sec+5 n Converting terms nanosecond (ns) leads following OCN: Total latency OCN ðÞ ¼ 5n +0:5c 2=3/C2300,000 km =sec+100/C28 8ns + 5 ns ¼5n + 0 :025 ns + 100 ns + 5 ns ¼110:025 ns Substituting appropriate values SAN gives following latency: Total latency SAN ðÞ ¼ 0:305μs+5m 2=3/C2300,000 km =sec+100 bytes 8 Gbits =sec+0:405μs ¼0:305μs+0:025μs+0:1μs+0:405μs ¼0:835μsF.2 Interconnecting Two Devices ■F-15Substituting appropriate values LAN gives following latency: Total latency LAN ðÞ ¼ 3:005μs+5k 2=3/C2300,000 km =sec+100 bytes 8 Gbits =sec+4:005μs ¼3:005μs+2 5 μs+0:1μs+4:005μs ¼32:11μs Substituting appropriate values WAN gives following latency: Total latency WAN ðÞ ¼ 30:005μs+5000 km 2=3/C2300,000 km =sec+100 bytes 8 Gbits =sec+4 0:005μs ¼30:005μs + 25000 μs+0:1μs+4 0 :005μs ¼25:07 ms increased fraction latency required time flight longer distances along greater likelihood errors longer distances among thereasons whyWANsand LANs usemore sophisticated time-consuming communication protocols, increase sending receiving overheads. needfor standardization another reason. Complexity also increases due require-ments imposed protocol typical applications run variousinterconnectionnetwork domains aswe go tensto hundredstothousandsto manythousands devices. consider later sections discuss connect-ing two devices. example shows propagation delay com-ponent time flight WANs LANs long latency components —including sending receiving overheads —can practically ignored. SANs OCNs propagation delay pales com-parisonto overheadsand transmission delay. Remember time-of-flight latencydue switches hardware network besides sheer propagation delaythrough links neglected example. noncongested networks,switch latency generally small compared overheads propagation delaythrough links WANs LANs, necessarily multiprocessorSANs multicore OCNs, see later sections. far, considered transport single packet computed associated end-to-end total packet latency. order compute effective band-width two networked devices, consider continuous stream ofpackets transported them. must keep mind that, addition min-imizing packet latency, goal network optimized given cost andpower consumption target transfer maximum amount available infor-mation shortest possible time, measured effective bandwidth deliv-ered network. applications require response sending next packet, sender overlap sending overhead later packets transport latency receiver overhead prior packets. essentially pipe-lines transmission packets network, also known link pipelining . Fortunately, discussed prior chapters book, many applicationF-16 ■Appendix F Interconnection Networksareas communication either several applications several threads application run concurrently (e.g., Web server concurrently serving thousands client requests streaming media), thus allowing deviceto send stream packets without wait acknowledgment areply. Also, long messages usually divided packets maximum sizebefore transport, number packets injected network successionfor cases. overlap possible, packets would wait forprior packets acknowledged transmitted thus suffer signif-icant performance degradation. Packets transported pipelined fashion acknowledged quite straight- forwardly simply keeping copy source unacknowledged packetsthat sent keeping track correspondence returnedacknowledgments packets stored buffer. Packets removed fromthe buffer corresponding acknowledgment received sender. Thiscan done including message ID packet sequence number associatedwith packet packet ’s acknowledgment. Furthermore, separate timer must associated buffered packet, allowing packet resent associated time-out expires. Pipelining packet transport network many similarities pipe- lining computation within processor. However, among differences thatit require staging latches. Information simply propagated throughnetwork links sequence signal waves. Thus, network consideredas logical pipeline consisting many stages required time offlight affect effective bandwidth achieved. Transmission ofa packet start immediately transmission previous one, thus over- lapping sending overhead packet transport receiver latency previous packets. sending overhead smaller transmission time,packets follow back-to-back, effective bandwidth approachesthe raw link bandwidth continuously transmitting packets. hand,if sending overhead greater transmission time, effective band-width injection point remain well raw link bandwidth. Theresulting link injection bandwidth ,B W LinkInjection , link injecting contin- uous stream packets network calculated following expression: BW LinkInjection ¼Packet size max Sending overhead,Transmission timeðÞ must also consider happens receiver unable consume packets rate arrive. occurs receiving overhead greater thesending overhead receiver cannot process incoming packets fast enough.In case, link reception bandwidth ,B W LinkReception , reception link network less link injection bandwidth obtained thisexpression: BW LinkReception ¼Packet size max Receiving overhead,Transmission timeðÞF.2 Interconnecting Two Devices ■F-17When communication takes place two devices interconnected ded- icated links, packets sent one device received other. thereceiver cannot process packets fast enough, receiver buffer become full,and flow control throttle transmission sender. situation produced causes external network, consider here. Moreover, receiving overhead greater sending overhead, receiver buffer willfill flow control will, likewise, throttle transmission sender. case,the effect flow control is, average, replace sending overheadwith receiving overhead. Assuming ideal network behaves like two dedi-cated links running opposite directions full link bandwidth thetwo devices —which consistent black box view network point —the resulting effective bandwidth smaller twice injection band- width (to account two injection links, one device) twice recep- tion bandwidth. results following expression effective bandwidth: Effective bandwidth ¼min 2 /C2BW LinkInjection ,2/C2BW LinkReception/C0/C1 ¼2/C2Packet size max Overhead,Transmission timeðÞ Overhead ¼max(Sending overhead, Receiving overhead). Taking account expression transmission time, obvious effectivebandwidth delivered network identical aggregate network band-width transmission time greater overhead. Therefore, fullnetwork utilization achieved regardless value time flight and, thus, regardless distance traveled packets, assuming ideal network behavior (i.e., enough credits buffers provided credit-based Xon/Xoff flow control). analysis assumes sender receiver networkinterfaces process one packet time. multiple packets pro-cessed parallel (e.g., done IBM ’s Federation network interfaces), overheads packets overlapped, increases effective band-width overlap factor amount bounded transmission time. Let’s use equation page F-17 explore impact packet size, trans- mission time, overhead BW Link Injection ,B W LinkReception , effective band- width various network domains: OCNs, SANs, LANs, WANs. Example previous example, assume dedicated link network data bandwidth 8 Gbps link direction interconnecting twodevices within OCN, SAN, LAN, WAN. Plot effective bandwidth versuspacket size type network packets ranging size 4 bytes(i.e., single 32-bit word) 1500 bytes (i.e., maximum transfer unit Ether- net), assuming end nodes per-packet sending receiving overheads before: x+0.05 ns/byte 4/3( x)+0.05 ns/byte, respectively, xis 0μs OCN, 0.3 μs SAN, 3 μs LAN, 30 μs WAN. limits effective bandwidth, packet sizes effec-tive bandwidth within 10% aggregate network bandwidth?F-18 ■Appendix F Interconnection NetworksAnswer Figure F.6 plots effective bandwidth versus packet size four network domains using simple equation parameters given above. packetsizes OCN, transmission time greater overhead (sending receiv- ing), allowing full utilization aggregate bandwidth, 16 Gbps —that is, injection link (alternatively, reception link) bandwidth times two account forboth devices. SAN, overhead —specifically, receiving overhead —is larger transmission time packets less 800 bytes; consequently, packetsof 655 bytes larger needed utilize 90% aggregate band-width. LANs WANs, link bandwidth utilized since over-head example many times larger transmission time allpacket sizes. example highlights importance reducing sending receiving overheads relative packet transmission time order maximize effectivebandwidth delivered network. analysis suggests possible provide upper bound effective bandwidth analyzing path followed packets determiningwhere bottleneck occurs. extend idea beyond networkinterfaces defining model considers entire network end Effective bandwidth (Gbits/sec)100 10 1 40.010.1 0.001 Packet size (bytes)1400 1200 1000 800 600 400 200OCN SANLANWAN Figure F.6 Effective bandwidth versus packet size plotted semi-log form four network domains. Overhead amortized increasing packet size, large overhead (e.g., WANs LANs) scaling packet size little help. considerations come play limit maximum packet size.F.2 Interconnecting Two Devices ■F-19end pipe identifying narrowest section pipe. three areas interest pipe: aggregate network injection links corresponding network injection bandwidth (BW NetworkInjection ), aggregate network reception links corresponding network reception bandwidth (BW NetworkReception ), aggregate network links corresponding network bandwidth (BW Network ). Expressions given later sections various layers black box view network arepeeled away. point, assumed two interconnected devices black box network behaves ideally network bandwidth equal aggregate raw network bandwidth. reality, much less aggre-gate bandwidth see following sections. general, effectivebandwidth delivered end-to-end network application upper boundedby minimum across three potential bottleneck areas: Effective bandwidth ¼min BW NetworkInjection ,B W Network ,B W NetworkReception/C0/C1 expand upon expression following sections reveal interconnection networks consider general case inter-connecting two devices. sections appendix, w e show concepts introduced section take shape example high-end commercial products. Figure F.7 lists several commercial computers tha t, one point time existence, among highest-performing systems world within class.Although systems capable interconnecting two devices,they implement basic functions needed interconnecting twodevices. addition applicable SANs used systems,the issues discussed section also apply interconnect domains:from OCNs WANs. F.3 Connecting Two Devices point, considered connection two devices communi-cating network viewed black box, makes interconnection net-works interesting ability connect hundreds even many thousands ofdevices together. Consequently, makes interesting also makes themmore challenging build. order connect two devices, suitable structure functionality must supported network. section continues black box approach introducing, conceptual level, addi-tional network structure functions must supported interconnect-ing two devices. details individual subjects given inSections F.4 F.7 . applicable, relate additional structure functions network media, flow control, basics presented previ-ous section. section, also classify networks two broad categoriesF-20 ■Appendix F Interconnection Networksbased connection structure —shared-media versus switched-media net- works —and compare them. Finally, expanded expressions characterizing network performance given, followed example. Additional Network Structure Functions: Topology, Routing, Arbitration, Switching Networks interconnecting two devices require mechanisms physi- cally connect packet source destination order transport packetand deliver correct destination. mechanisms implemented different ways significantly vary across interconnection network domains. However, types network structure functions performed mech-anisms much same, regardless domain. multiple devices interconnected network, connections oftentimes cannot permanently established dedicated links.Company System [network] name Intro year Max. number compute nodes[ # CPUs] System footprint max. configuration Packet [header] max size (bytes) Injection [reception] node BW MB/sec Minimum send/ receive overhead Maximum copper link length; flowcontrol; error Intel ASCI Red Paragon2001 4510 [/C22]2500 ft21984 [4]400 [400]Few μs Handshaking; CRC+parity IBM ASCI White SP Power3 [Colony]2001 512 [/C216]10,000 ft21024 [6]500 [500]/C243μs 25 m; credit- based; CRC Intel Thunder Itanium2Tiger4[QsNet II]2004 1024 [/C24]120 m22048 [14]928 [928]0.240 μs 13 m; credit- based; CRC forlink, dest. Cray XT3 [SeaStar] 2004 30,508 [/C21]263.8 m280 [16]3200 [3200]Few μs 7 m; credit- based; CRC Cray X1E 2004 1024 [/C21]27 m232 [16]1600 [1600]0 (direct LD ST accesses)5 m; credit- based; CRC IBM ASC Purple pSeries 575 [Federation]2005 >1280 [/C28]6720 ft22048 [7]2000 [2000]/C241μs 4 packets processed k25 m; credit- based; CRC IBM Blue Gene/L eServer Sol. [Torus Net.]2005 65,536 [/C22]2500 ft2 (.9/C2.9/C21.9 m3/ 1 K node rack)256 [8]612.5 [1050]/C243μs (2300 cycles)8.6 m; credit- based; CRC (header/pkt) Figure F.7 Basic characteristics interconnection networks commercial high-performance computer systems.F.3 Connecting Two Devices ■F-21This could either restrictive packets given source would go one destination (and others) prohibitively expensive ded- icated link would needed every source every destination (we eval-uate next section). Therefore, networks usually share pathsamong different pairs devices, paths shared determinedby network connection structure, commonly referred network topol- ogy. Topology addresses important issue “What paths possible packets? ”so packets reach intended destinations. Every network interconnects two devices also requires mechanism deliver packet correct destination. associated func- tion referred routing , defined set operations need performed compute valid path packet source destinations.Routing addresses important issue “Which possible paths allow- able (valid) packets? ”so packets reach intended destinations. Depending network, function may executed packet source compute theentire path, intermediate devices compute fragments path onthe fly, even every possible destination device verify whether device intended destination packet. Usually, packet header shown Figure F.4 extended include necessary routing information. general, networks usually contain shared paths parts thereof among dif- ferent pairs devices, packets may request shared resources. severalpackets request resources time, arbitration function required resolve conflict. Arbitration, along flow control, addressesthe important issue “When paths available packets? ”Every time arbitra- tion performed, winner possibly several losers. losers granted access requested resources typically buffered. indicated previous section, flow control may implemented prevent buffer overflow.The winner proceeds toward destination granted resources switchedin, providing path packet advance. function referred switch- ing. Switching addresses important issue “How paths allocated packets? ”To achieve better utilization existing communication resources, networks establish entire end-to-end path once. Instead, explained inSection F.5 , paths usually established one fragment time. three network functions —routing, arbitration, switching —must implemented every network connecting two devices, matter whatform network topology takes. addition basic functions men-tioned previous section. However, complexity functions andthe order performed depends category network topol-ogy, discussed below. general, routing, arbitration, switching requiredto establish valid path source destination among possible pathsprovided network topology. path established, packet transport functions previously described used reliably transmit packets receive corresponding destination. Flow control, implemented, pre-vents buffer overflow throttling sender. implemented end-to-end level, link level within network, both.F-22 ■Appendix F Interconnection NetworksShared-Media Networks simplest way connect multiple devices share network media, shown bus Figure F.8 (a). traditional way interconnecting devices. shared media operate half-duplex mode, data carried either direction media simultaneous trans-mission reception data device allowed, full-duplex, data carried directions simultaneously transmitted andreceived device. recently, I/O devices systems typ-ically shared single I/O bus, early system-on-chip (SoC) designs made use shared bus interconnect on-chip components. popular LAN, Ether- net, originally implemented half-duplex bus shared hundredcomputers, although switched-media versions also exist. Given network media shared, must mechanism coordinate arbitrate use shared media one packet sent time. Ifthe physical distance network devices small, may possible havea central arbiter grant permission send packets. case, network nodesmay use dedicated control lines interface arbiter. Centralized arbitration impractical, however, networks large number nodes spread large distances, distributed forms arbitration also used. casefor original Ethernet shared-media LAN. first step toward distributed arbitration shared media “looking leap. ”A node first checks network avoid trying send packet another packet already network. Listening transmission avoidcollisions called carrier sensing . interconnection idle, node tries send. Looking first guarantee success, course, node may also decide send instant. two nodes send time, Node NodeShared-media networkSwitched-media network (B)Switch fabric (A)NodeNode Node Node Node Figure F.8 (a) shared-media network versus (b) switched-media network. Ether- net originally shared media network, switched Ethernet available. nodes shared-media networks must dynamically share raw bandwidth one link, switched-media networks support multiple links, providing higher rawaggregate bandwidth.F.3 Connecting Two Devices ■F-23acollision occurs. Let ’s assume network interface detect resulting collisions listening hear data become garbled data appearing line. Listening detect collisions called collision detection . second step distributed arbitration. problem solved yet. If, detecting collision, every node network waited exactly amount time, listened sure notraffic, tried send again, could still synchronized nodes thatwould repeatedly bump heads. avoid repeated head-on collisions, nodewhose packet gets garbled waits (or backs ) random amount time resending. Randomization breaks synchronization. Subsequent collisions result exponentially increasing time attempts retransmit, notto tax network. Although approach controls congestion shared media, guar- anteed fair —some subsequent node may transmit collided waiting. network high demand many nodes, thissimple approach works well. high utilization, however, performancedegrades since media shared fairness ensured. Another distrib- uted approach arbitration shared media support fairness pass token nodes. function token grant acquiring node theright use network. token circulates cyclic fashion thenodes, certain amount fairness ensured arbitration process. arbitration performed device granted access shared media, function switching straightforward. granted devicesimply needs connect shared media, thus establishing path everypossible destination. Also, routing simple implement. Given media shared attached devices, every device see every packet. Therefore, device needs check whether given packet isintended device. beneficial side effect strategy devicecan send packet devices attached shared media singletransmission. style communication called broadcasting , contrast unicasting, packet intended one device. shared media make easy broadcast packet every device or, alternatively, subset ofdevices, called multicasting. Switched-Media Networks alternative sharing entire network media across attached nodes switch disjoint portions shared nodes. por-tions consist passive point-to-point links active switch components dynamically establish communication sets source-destination pairs.These passive active components make referred networkswitch fabric ornetwork fabric , end nodes connected. approach shown conceptually Figure F.8(b) . switch fabric described greater detail Sections F.4 F.7 , various black box layers switched- media networks revealed. Nevertheless, high-level view shownF-24 ■Appendix F Interconnection NetworksinFigure F.8(b) illustrates potential bandwidth improvement switched- media networks shared-media networks: aggregate bandwidth many times higher shared-media networks, allowing possibility greatereffective bandwidth achieved. best, one node time transmitpackets shared media, whereas possible attached nodes soover switched-media network. Like shared-media counterparts, switched-media networks must imple- ment three additional functions previously mentioned: routing, arbitration,and switching. Every time packet enters network, routed order select path toward destination provided topology. path requested packet must granted centralized distributed arbiter, resolvesconflicts among concurrent requests resources along path. therequested resources granted, network “switches ”the required connec- tions establish path allows packet forwarded toward desti-nation. requested resources granted, packet usually buffered, asmentioned previously. Routing, arbitration, switching functions usuallyperformed within switched networks order, whereas shared-media net- works routing typically last function performed. Comparison Shared- Switched-Media Networks general, advantage shared-media networks low cost, but, conse-quently, aggregate network bandwidth scale number interconnected devices. Also, global arbitration scheme required resolveconflicting demands, possibly introducing another type bottleneck againlimiting scalability. Moreover, every device attached shared media increasesthe parasitic capacitance electrical conductors, thus increasing time offlight propagation delay accordingly and, possibly, clock cycle time. addition,it difficult pipeline packet transmission network sharedmedia continuously granted different requesting devices. main advantage switched-media networks amount network resources implemented scales number connected devices, increasingthe aggregate network bandwidth. networks allow multiple pairs nodesto communicate simultaneously, allowing much higher effective network band-width provided shared-media networks. Also, switched-media net-works allow system scale large numbers nodes, notfeasible using shared media. Consequently, scaling advantage can, atthe time, disadvantage network resources grow superlinearly. Net- works superlinear cost provide effective network bandwidth grows sublinearly number interconnected devices inefficient designsfor many applications interconnection network domains. Characterizing Performance: Latency Effective Bandwidth routing, switching, arbitration functionality described introducessome additional components packet transport latency must taken intoF.3 Connecting Two Devices ■F-25account expression total packet latency. Assuming contention network resources —as would case unloaded network —total packet latency given following: Latency ¼Sending overhead + TTotalProp +TR+TA+TS/C0/C1 +Packet size Bandwidth+ Receiving overhead TR,TA, TSare total routing time, arbitration time, switching time experienced packet, respectively, either measured quantities cal-culated quantities derived detailed analyses. components areadded total propagation delay network links, TotalProp , give overall time flight packet. expression gives lower bound total packet latency account additional delays due contention resources mayoccur. network heavily loaded, several packets may request network resources concurrently, thus causing contention degrades perfor- mance. Packets lose arbitration buffered, increases packetlatency contention delay amount waiting time. additional delay included expression. network part approachessaturation, contention delay may several orders magnitude greater thetotal packet latency suffered packet zero load even slightlyloaded network conditions. Unfortunately, easy compute analyticallythe total packet latency network moderately loaded. Mea- surement quantities using cycle-accurate simulation detailed network model better precise way estimating packet latency suchcircumstances. Nevertheless, expression given useful calculatingbest-case lower bounds packet latency. similar reasons, effective bandwidth easy compute exactly, estimate best-case upper bounds appropriately extending model presented end previous section. need find nar-rowest section end-to-end network pipe finding network injection bandwidth (BW NetworkInjection ), network reception bandwidth (BW NetworkRecep- tion), network bandwidth (BW Network ) across entire network intercon- necting devices. BW NetworkInjection calculated simply multiplying expression link injection bandwidth, BW LinkInjection , total number network injec- tion links. BW NetworkReception calculated similarly using BW LinkReception , must also scaled factor reflects application traffic charac-teristics. two interconnected devices, longer valid assume one-to-one relationship among sources destinations analyzing effect flow control link reception bandwidth. could happen, example, thatseveral packets different injection links arrive concurrently recep-tion link applications many-to-one traffic characteristics, whichcauses contention reception links. effect taken accountby average reception factor parameter, σ, either measured quantity calculated quantity derived detailed analysis. defined averageF-26 ■Appendix F Interconnection Networksfraction percentage packets arriving reception links accepted. packets immediately delivered, thus reducing network reception bandwidth factor. reduction occurs result application behaviorregardless internal network characteristics. Finally, BW Network takes account internal characteristics network, including contention. Wewill progressively derive expressions following sections enableus calculate details revealed internals blackbox interconnection network. Overall, effective bandwidth delivered network end-to-end application determined minimum across three sections, described following: Effective bandwidth ¼min BW NetworkInjection ,BW Network ,σ/C2BW NetworkReception/C0/C1 ¼min N /C2BW LinkInjection ,BW Network ,σ/C2N/C2BW LinkReception/C0/C1 Let’s use expressions compare latency effective bandwidth shared-media networks switched-media networks four intercon-nection network domains: OCNs, SANs, LANs, WANs. Example Plot total packet latency effective bandwidth number intercon- nected nodes, N, scales 4 1024 shared-media switched-media OCNs, SANs, LANs, WANs. Assume network links, including injection reception links nodes, data bandwidth 8 Gbps, unicast packets 100 bytes transmitted. Shared-media networks share onelink, switched-media networks least many network links arenodes. both, ignore latency bandwidth effects due contention withinthe network. End nodes per-packet sending receiving overheads ofx+0.05 ns/byte 4/3( x)+0.05 ns/byte, respectively, xis 0μs OCN, 0.3 μs SAN, 3 μs LAN, 30 μs WAN, inter- connection distances 0.5 cm, 5 m, 5000 m, 5000 km, respectively. Also assume total routing, arbitration, switching times constants func- tions number interconnected nodes: R¼2.5 ns, TA¼2.5(N) ns, TS¼2.5 ns shared-media networks TR¼TA¼TS¼2.5(log 2N) ns switched-media networks. Finally, taking account application traffic charac-teristics network structure, average reception factor, σ, assumed N /C01for shared media polylogarithmic (log 2N)/C01/4for switched media. Answer components total packet latency example given previous section except time flight, additional routing, arbi- tration, switching delays. shared-media networks, additional delaystotal 5+2.5( N) ns; switched-media networks, total 7.5(log 2N) ns. Latency plotted OCNs SANs Figure F.9 networks give interesting results. OCNs, TR,TA, TScombine dominate time flightF.3 Connecting Two Devices ■F-27and much greater latency components moderate large number nodes. particularly shared-media network. Thelatency increases much dramatically number nodes sharedmedia compared switched media given difference arbitration delaybetween two. SANs, R,TA, TSdominate time flight net- work sizes greater latency components shared-media networks large-sized networks; less latency components switched-media networks negligible. LANs WANs, time flight dominated propagation delay, dominates otherlatency components calculated previous section; thus, R,TA, TSare negligible shared switched media. Figure F.10 plots effective bandwidth versus number interconnected nodes four network domains. effective bandwidth shared-media net-works constant network scaling one unicast packet bereceived time network reception links, limited receiving overhead network OCN. effective band- width switched-media networks increases number intercon-nected nodes, scaled average reception factor. Thereceiving overhead limits effective bandwidth OCN. Latency (ns)10,000 1000 4100 Number nodes (N)512 1024 256 128 64 32 16 8SAN— shared OCN— sharedSAN— switchedOCN— switched Figure F.9 Latency versus number interconnected nodes plotted semi-log form OCNs SANs. Routing, arbitration, switching animpact latency networks two domains, particularly networks large number nodes, given low sending receiving overheads low propagation delay.F-28 ■Appendix F Interconnection NetworksGiven obvious advantages, ’t switched networks always used? Earlier computers much slower could share network media littleimpact performance. addition, switches earlier LANs WANs tookup several large boards large entire computer. con-sequence Moore ’s law, size switches reduced considerably, sys- tems much greater need high-performance communication. Switchednetworks allow communication harvest rapid advancements sil-icon processors main memory. Whereas switches telecommunication companies size mainframe computers, today see single-chip switches even entire switched networks within chip. Thus, technology andapplication trends favor switched networks today. single-chip processorsled processors replacing logic circuits surprising number places,single-chip switches switched on-chip networks increasingly replacingshared-media networks (i.e., buses) several application domains. example,PCI-Express (PCIe) —a switched network —was introduced 2005 replace traditional PCI-X bus personal computer motherboards. previous example also highlights importance optimizing routing, arbitration, switching functions OCNs SANs. networkdomains particular, interconnect distances overheads typically smallEffective bandwidth (Gbits/sec)10,000 1000 100 10 1 10.1 0.01 Number nodes (N)1200 1000 800 600 400 200OCN— switched SAN— switchedLAN— switchedWAN— switchedOCN— sharedSAN— sharedLAN— shared WAN— shared Figure F.10 Effective bandwidth versus number interconnected nodes plotted semi-log form four network domains. disparity effective bandwidth shared- switched-media networks inter- connect domains widens significantly number nodes network increases. switched on-chip network able achieve effective bandwidth equal aggregate bandwidth parameters given example.F.3 Connecting Two Devices ■F-29enough make latency effective bandwidth much sensitive well functions implemented, particularly larger-sized networks. leads mostly implementations based mainly faster hardware solutions thesedomains. LANs WANs, implementations based slower flex-ible software solutions suffice given performance largely determined byother factors. design topology switched-media networks also playsa major role determining close lower bound latency upperbound effective bandwidth network achieve OCN SANdomains. next three sections touch important issues switched networks, next section focused topology. F.4 Network Topology number devices small enough, single switch sufficient inter-connect within switched-media network. However, number switch ports limited existing very-large-scale integration (VLSI) technology, cost considerations, power consumption, on. number required net- work ports exceeds number ports supported single switch, fabric interconnected switches needed. embody necessary property full access (i.e., connectedness ), network switch fabric must provide path every end node device every device. connections networkfabric switches within fabric use point-to-point links opposedto shared links —that is, links one switch end node device either end. interconnection structure across components —including switches, links, end node devices —is referred network topology . number network topologies described literature would difficult count, number used commercially dozen so. 1970s early 1980s, researchers struggled propose new topologies could reduce number switches packetsmust traverse, referred hop count . 1990s, thanks introduction pipelined transmission switching techniques, hop count became less crit-ical. Nevertheless, today, topology still important, particularly OCNs andSANs, subtle relationships exist topology network designparameters impact performance, especially number end nodesis large (e.g., 64 K Blue Gene/L supercomputer) latency critical (e.g., multicore processor chips). Topology also greatly impacts implementation cost network. Topologies parallel supercomputer SANs visible imaginative, usually converging regularly structured ones simplify routing,packaging, scalability. LANs WANs tend haphazardor ad hoc, challenges long distance connectingacross different communication subnets. Switch-based topologies OCNs areonly recently emerging quickly gaining popularity. sectionF-30 ■Appendix F Interconnection Networksdescribes popular topologies used commercial products. advan- tages, disadvantages, constraints also briefly discussed. Centralized Switched Networks mentioned above, single switch suffices interconnect set devices number switch ports equal larger number devices. Thissimple network usually referred crossbar orcrossbar switch . Within crossbar, crosspoint switch complexity increases quadratically number ofports, illustrated Figure F.11(a) . Thus, cheaper solution desirable number devices interconnected scales beyond point supportable byimplementation technology. common way addressing crossbar scaling problem consists splitting large crossbar switch several stages smaller switches interconnected insuch way single pass switch fabric allows destination bereached source. Topologies arranged way usually referred asmultistage interconnection networks ormultistage switch fabrics , net- works typically complexity increases proportion NlogN. Multistage interconnection networks (MINs) initially proposed telephone exchangesin 1950s since used build communication backbone parallel supercomputers, symmetric multiprocessors, multicomputer clusters, IP router switch fabrics. (B) (A)00 11 22 33 44 55 66 70 123456 70 12345677 Figure F.11 Popular centralized switched networks: (a) crossbar network requires N2crosspoint switches, shown black dots; (b) Omega, MIN, requires N/2 log 2Nswitches, shown vertical rectangles. End node devices shown numbered squares (total eight). Links unidirectional —data enter left exit top right.F.4 Network Topology ■F-31The interconnection pattern patterns MIN stages permutations represented mathematically set functions, one stage. Figure F.11(b) shows well-known MIN topology, Omega , uses perfect-shuffle permutation interconnection pattern stage, followedby exchange switches, giving rise perfect-shuffle exchange stage. example, eight input-output ports interconnected three stages 2 /C22 switches. easy see single pass three stages allows inputport reach output port. general, using k/C2kswitches, MIN N input-output ports requires least log kNstages, contains N/k switches, total N/k(log kN) switches. Despite internal structure, MINs seen centralized switch fabrics end node devices connected network periphery, hence namecentralized switched network . another perspective, MINs viewed interconnecting nodes set switches may nodesdirectly connected them, gives rise another popular name central-ized switched networks —indirect networks . Example Compute cost interconnecting 4096 nodes using single crossbar switch relative using MIN built 2 /C22, 4/C24, 16 /C216 switches. Con- sider separately relative cost unidirectional links relative cost switches. Switch cost assumed grow quadratically number ofinput (alternatively, output) ports, k, k/C2kswitches. Answer switch cost network using single crossbar proportional 4096 2. unidirectional link cost 8192, accounts set links end nodes crossbar also crossbar back end nodes.When using MIN k/C2kswitches, cost switch proportional tok 2but 4096/ k(log k4096) total switches. Likewise, (log k 4096) stages Nunidirectional links per stage switches plus Nlinks MIN end nodes. Therefore, relative costs crossbar withrespect MIN given following: Relative cost 2 /C22ðÞswitches ¼40962=22/C24096=2/C2log 24096/C0/C1 ¼170 Relative cost 4 /C24ðÞswitches ¼40962=42/C24096=4/C2log 44096/C0/C1 ¼170 Relative cost 16 /C216ðÞswitches ¼40962=162/C24096=16/C2log 164096/C0/C1 ¼85 Relative cost 2 /C22ðÞlinks¼8192=4096/C2log 24096 + 1 ðÞ ðÞ ¼ 2=13¼0:1538 Relative cost 4 /C24ðÞlinks¼8192=4096/C2log 44096 + 1 ðÞ ðÞ ¼ 2=7¼0:2857 Relative cost 16 /C216ðÞlinks¼8192=4096/C2log 164096 + 1 ðÞ ðÞ ¼ 2=4¼0:5 cases, single crossbar much higher switch cost MINs. dramatic reduction cost comes MIN composed smallestsized largest number switches, interesting see MINs with2/C22 4 /C24 switches yield relative switch cost. relative link costF-32 ■Appendix F Interconnection Networksof crossbar lower MINs, less order magnitude cases. must keep mind end node links different switch linksin length packaging requirements, usually different associ-ated costs. Despite lower link cost, crossbar higher overall relative cost. reduction switch cost MINs comes price performance: con- tention likely occur network links, thus degrading performance. Con-tention form packets blocking network arises due paths different sources different destinations simultaneously sharing one links. amount contention network depends communication traffic behavior. Omega network shown Figure F.11(b) , example, packet port 0 port 1 blocks first stage switches waiting packetfrom port 4 port 0. crossbar, blocking occurs links notshared among paths unique destinations. crossbar, therefore, nonblock- ing. course, two nodes try send packets destination, blocking reception link even crossbar networks. accounted forby average reception factor parameter ( σ) analyzing performance, dis- cussed end previous section. reduce blocking MINs, extra switches must added larger ones need used provide alternative paths every source every destination. Thefirst commonly used solution add minimum log kN/C01 extra switch stages MIN way mirror original topology. resulting net-work rearrangeably nonblocking allows nonconflicting paths among new source-destination pairs established, also doubles hop count andcould require paths existing communicating pairs rearranged centralized control. second solution takes different approach. Instead using switch stages, larger switches —which implemented multiple stages desired —are used middle two switch stages way enough alternative paths middle-stage switches allowfor nonconflicting paths established first last stages. Thebest-known example Clos network, nonblocking. multi-path property three-stage Clos topology recursively applied themiddle-stage switches reduce size switches 2 /C22, assum- ing switches size used first last stages begin with. results Bene ŝtopology consisting 2(log 2N)/C01 stages, rearrange- ably nonblocking. Figure F.12(a) illustrates topologies, switches first last stages comprise middle-stage switches (recursively) theClos network. MINs described far unidirectional network links, bidirectional forms easily derived symmetric networks Clos Bene ŝsim- ply folding them. overlapping unidirectional links run different direc- tions, thus forming bidirectional links, overlapping switches merge single switch twice ports (i.e., 4 /C24 switch). Figure F.12(b) shows resulting folded Bene ŝtopology case end nodes connectedF.4 Network Topology ■F-33to innermost switch stage original Bene ŝ. Ports remain free side network used later expansion network larger sizes. kind networks referred bidirectional multistage intercon- nection networks. Among many useful properties networks mod- ularity ability exploit communication locality, saves packetsfrom hop across network stages. regularity also reduces routingcomplexity multipath property enables traffic routed evenlyacross network resources tolerate faults. Another way deriving bidirectional MINs nonblocking (rearrangeable) properties form balanced tree, end node devices occupy leaves tree switches occupy vertices within tree. Enough links tree level must provided total link bandwidth remains constant across alllevels. Also, except root, switch ports vertex typically grow k i/C2ki, iis tree level. accomplished using ki/C01total switches vertex, switch kinput koutput ports, kbidirectional ports (i.e., k/C2kinput-output ports). Networks topologies called fat tree networks. half kbidirectional ports used direction, 2 N/k switches needed stage, totaling 2 N/k(log k/2N) switches fat tree. number switches root stage halved forward links needed, reducing switch count N/k.Figure F.12(b) shows fat tree 4 /C24 switches. seen, identical folded Bene ŝ. fat tree topology choice across wide range network sizes commercial systems use multistage interconnection networks. MostSANs used multicomputer clusters, many used powerful0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 150 1 2 3 4 5 6 7 8 9 10 11 12 (A) (B)13 14 1501 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Figure F.12 Two Bene ŝnetworks. (a) 16-port Clos topology, middle-stage switches shown darker shading implemented another Clos network whose middle-stage switches shown lighter shading areimplemented yet another Clos network, on, Bene ŝnetwork produced uses 2 /C22 switches everywhere. (b) folded Bene ŝnetwork (bidirectional) 4 /C24 switches used; end nodes attach innermost set Bene ŝnetwork (unidirectional) switches. topology equivalent fat tree, tree vertices shown shades.F-34 ■Appendix F Interconnection Networkssupercomputers, based fat trees. Commercial communication subsystems offered Myrinet, Mellanox, Quadrics also built fat trees. Distributed Switched Networks Switched-media networks provide flexible framework design communi- cation subsystems external devices need communicate, presentedabove. However, cases convenient tightly integrate theend node devices network resources used enable communicate.Instead centralizing switch fabric external subsystem, alternativeapproach distribute network switches among end nodes, thenbecome network nodes simply nodes , yielding distributed switched network . consequence, network switch one end node devices directly connected it, thus forming network node. nodes directly connected toother nodes without indirectly going external switch, giving rise toanother popular name networks —direct networks . topology distributed switched networks takes form much differ- ent centralized switched networks end nodes connected acrossthe area switch fabric, n eo rt w oo ft h ep e r p h e r le g e f fabric. causes number switches system equal total number nodes. quite obvious way interconnecting nodes consists connecting dedicated link node every node thenetwork. fully connected topology provides best connectivity (full con- nectivity fact), costly crossbar network, following example shows. Example Compute cost interconnecting Nnodes using fully connected topology rel- ative using crossbar topology. Consider separately relative cost unidirectional links relative cost switches. Switch cost assumed grow quadratically number unidirectional ports k/C2kswitches grow linearly 1 /C2kswitches. Answer crossbar topology requires N/C2Nswitch, switch cost proportional toN 2. link cost 2 N, accounts unidirectional links end nodes centralized crossbar, vice versa . fully connected topology, two sets 1 /C2(N/C01) switches (possibly merged one set) used theNnodes connect nodes directly nodes. Thus, total switch cost Nnodes proportional 2 N(N/C01). Regarding link cost, Nnodes requires two unidirectional links opposite directions end node device local switch. addition, Nnodes N/C01 unidirectional links local switch switches distributed acrossall end nodes. Thus, total number unidirectional links is2N+N(N/C01), equal N(N+1) Nnodes. relative costs fully connected topology respect crossbar is, therefore, following:F.4 Network Topology ■F-35Relative cost switches ¼2NN/C01ðÞ =N2¼2N/C01ðÞ =N¼21/C01=N ðÞ Relative cost links¼NN +1ðÞ =2N¼N+1ðÞ =2 number interconnected devices increases, switch cost fully connected topology nearly double crossbar, high(i.e., quadratic growth). Moreover, fully connected topology always higherrelative link cost, grows linearly number nodes. Again, keep mind end node links different switch links length pack- aging, particularly direct networks, usually different associatedcosts. Despite higher cost, fully connected topology provides extra per-formance benefits crossbar nonblocking. Thus, crossbar net-works usually used practice instead fully connected networks. lower-cost alternative fully connecting nodes network directly connect nodes sequence along ring topology, shown Figure F.13 . bidirectional rings, Nnodes uses 3 /C23 switches two bidirectional network links (shared neighboring nodes),for total Nswitches Nbidirectional network links. linear cost excludes theNinjection-reception bidirectional links required within nodes. Unlike shared-media networks, rings allow many simultaneous transfers: first node send second second sends third, on.However, dedicated links exist logically nonadjacent node pairs,packets must hop across intermediate nodes arriving destination,increasing transport latency. bidirectional rings, packets trans-ported either direction, shortest path destination usually beingthe one selected. case, packets must travel N/4 network switch hops, average, total switch hop count one account local switch packet source node. Along way, packets may block network resourcesdue packets contending resources simultaneously. Fully connected ring-connected networks delimit two extremes dis- tributed switched topologies, many points interest agiven set cost-performance requirements. Generally speaking, idealswitched-media topology cost approaching ring performance Figure F.13 ring network topology, folded reduce length longest link. Shaded circles represent switches, black squares represent end node devices. gray rectangle signifies network node consisting switch, device, itsconnecting link.F-36 ■Appendix F Interconnection Networksapproaching fully connected topology. Figure F.14 illustrates three pop- ular direct network topologies commonly used systems spanning cost- performance spectrum. consist sets nodes arranged along multipledimensions regular interconnection pattern among nodes beexpressed mathematically. mesh orgrid topology, nodes dimension form linear array. torus topology, nodes dimen- sion form ring. topologies provide direct communication neigh-boring nodes aim reducing number hops suffered packets inthe network respect ring. achieved providing greater connec- tivity additional dimensions, typically three commercial systems. hypercube orn-cube topology particular case mesh two nodes interconnected along dimension, leading num-ber dimensions, n, must large enough interconnect Nnodes system (i.e., n¼log 2N). hypercube provides better connectivity meshes(A) 2D grid mesh 16 nodes (B) 2D torus 16 nodes (C) Hypercube 16 nodes (16 = 24 n = 4) Figure F.14 Direct network topologies appeared commercial systems, mostly supercomputers.The shaded circles represent switches, black squares represent end node devices. Switches many bidirectional network links, least one link goes end node device. basic topologies supplemented extra linksto improve performance reliability. example, connecting switches periphery 2D mesh, shown (a), using unused ports switch forms 2D torus, shown (b). hypercube topology, shown (c) n-dimensional inter- connect 2 nnodes, requiring n+ 1 ports per switch: one nnearest neighbor nodes one end node device.F.4 Network Topology ■F-37and tori expense higher link switch costs, terms number links number ports per node. Example Compute cost interconnecting Ndevices using torus topology relative using fat tree topology. Consider separately relative cost bidi-rectional links relative cost switches —which assumed grow quadratically number bidirectional ports. Provide approximateexpression case switches similar size. Answer Using k/C2kswitches, fat tree requires 2 N/k(log k/2N) switches, assuming last stage (the root) number switches stages.Given number bidirectional ports switch k(i.e., k input ports koutput ports k/C2kswitch) switch cost grows quadratically this, total network switch cost proportional 2 kNlog k/2 N. link cost Nlogk/2Nas log k/2Nstages requires Nbidirectional links, including devices fat tree. torus requires many switches nodes, 2 n+1 bidirectional ports, including port attach communicating device, nis number dimensions. Hence, total switch cost torus (2 n+1)2N. torus nodes requires 2n+1 bidirectional links ndifferent dimensions connection end node device, dimensional links shared two nodes, totalnumber links (2 n/2+1) N¼(n+1)Nbidirectional links Nnodes. Thus, relative costs torus topology respect fat tree Relative cost switches ¼2n+1ðÞ2N=2kNlog k=2N¼2n+1ðÞ2=2klog k=2N Relative cost links¼n+1ðÞ N=Nlog k=2N¼n+1ðÞ =log k=2N switch sizes similar, 2 n+1ﬃk. case, relative cost Relative cost switches ¼2n+1ðÞ2=2klog k=2N¼2n+1ðÞ =2log k=2N¼k=2log k=2N number switch ports (also called switch degree ) small, tori lower cost, particularly number dimensions low. espe- cially useful property Nis large. hand, larger switches and/or high number tori dimensions used, fat trees less costly pref-erable. example, interconnecting 256 nodes, fat tree four times moreexpensive terms switch link costs 4 /C24 switches used. higher cost compensated lower network contention, average. Thefat tree comparable cost torus 8 /C28 switches used (e.g., interconnecting 256 nodes). larger switch sizes beyond this, torus costsmore fat tree node includes switch. cost amortized connecting multiple end node devices per switch, called bristling . topologies depicted Figure F.14 common interesting characteristic network links arranged several orthogonal dimensions regular way. fact, topologies happen particularF-38 ■Appendix F Interconnection Networksinstances larger class direct network topologies known k-ary n-cubes, ksignifies number nodes interconnected ndimen- sions. symmetry regularity se topologies simplify network imple- mentation (i.e, packaging) packet routing movement apacket along given network dimension modify number remain-ing hops dimension toward destination. see thenext section, topological property readily exploited simple rout- ing algorithms. Like indirect counterpart, direct networks introduce blocking among packets concurrently request path, part it. exception fully connected networks. way number stages switch hopsin indirect networks reduced using larger switches, hop count indirect networks likewise reduced increasing number topologicaldimensions via increased switch degree. may seem good idea always maximize number dimen- sions system certain size switch cost. However, nec-essarily case. electronic systems built within three-dimensional (3D) world using planar (2D) packaging technology integrated circuit chips, printed circuit boards, back p l n e .D r e c tn e w r k sw hu pt ot h r e e dimensions implemented using relatively short links within 3Dspace, independent system size. Link higher-dimensioned networks would require increasingly longer wires fiber. increase link lengthwith system size also indicative MI Ns, including fat trees, require either long links within stages increasingly longer links stages added. saw first example given Section F.2 , flow-controlled buffers increase size proportionally link length, thus requiring greater sil- icon area. among reasons supercomputer largestnumber compute nodes existing 2005, IBM Blue Gene/L, implementeda 3D torus network interprocessor communication. fat tree would haverequired much longer links, rendering 64K node system less feasible. Thishighlights importance correctly selecting proper network topologythat meets system requirements. Besides link length, constraints derived implementing topology may also limit degree topology scale. available pin-out achievable bisection bandwidth . Pin count local restriction band- width chip, printed circuit board, backplane (or chassis) connector. Ina direct network integrates processor cores switches single chip ormultichip module, pin bandwidth used interfacing main memoryand implementing node links. case, limited pin count could reduce thenumber switch ports bit lines per link. indirect network, switches areimplemented separately processor cores, allowing pins ded- icated communication bandwidth. However, switches grouped onto boards, aggregate input-output links switch fabric boardfor given topology must exceed board connector pin-outs. bisection bandwidth global restriction gives interconnect density bandwidth achieved given implementationF.4 Network Topology ■F-39(packaging) technology. Interconnect density clock frequency related other: wires packed closer together, crosstalk parasitic capac- itance increase, usually impose lower clock frequency. example, theavailability spacing metal layers limit wire density frequency on-chipnetworks, copper track density limits wire density frequency printedcircuit board. implementable, topology network must exceed theavailable bisection bandwidth implementation technology. networksimplemented date constrained pin-out limitations rather thanbisection bandwidth, particularly recent move blade-based systems. Nevertheless, bisection bandwidth largely affects performance. given topology, bisection bandwidth, BW Bisection , calculated dividing network two roughly equal parts —each half nodes —and summing bandwidth links crossing imaginary divid- ing line. nonsymmetric topologies, bisection bandwidth smallest allpairs equal-sized divisions network. fully connected network, thebisection bandwidth proportional N 2/2 unidirectional links (or N2/4 bidi- rectional links), Nis number nodes. bus, bisection bandwidth bandwidth one shared half-duplex link. topologies, values lie two extrem es. Network injection reception bisection bandwidth commonly used reference value, N/2 network Ninjection reception links, respectively. network topology provides bisec tion bandwidth said full bisection bandwidth. Figure F.15 summarizes number switches links required, corre- sponding switch size, maximum average switch hop distances nodes, bisection bandwidth terms links several topologies discussed section interconnecting 64 nodes. Evaluation category Bus Ring 2D mesh 2D torus Hypercube Fat tree Fully connected Performance BW Bisection # links 1 2 8 16 32 32 1024 Max (ave.) hop count 1 (1) 32 (16) 14 (7) 8 (4) 6 (3) 11 (9) 1 (1) Cost I/O ports per switch NA 3 5 5 7 4 64 Number switches NA 64 64 64 64 192 64Number net. links 1 64 112 128 192 320 2016Total number links 1 128 176 192 256 384 2080 Figure F.15 Performance cost several network topologies 64 nodes. bus standard reference unit network link cost bisection bandwidth. Values given terms bidirectional links ports. Hop count includes switch output link, injection link end nodes. Except bus, values given forthe number network links total number links, including injection/reception links end node devicesand network.F-40 ■Appendix F Interconnection NetworksEffects Topology Network Performance Switched network topologies require packets take one hops reach destination, hop represents transport packet aswitch one corresponding links. Interestingly, switch corre-sponding links modeled black box network connecting twodevices, described previous section, term “devices ”here refers end nodes switches. differences sending andreceiving overheads null switches, routing, switching, andarbitration delays cumulative but, instead, delays associated switch. consequence above, average packet traverse dhops destination, R+TA+TS¼(Tr+Ta+Ts)/C2d,where Tr,Ta, Tsare routing, arbitration, switching delays, respectively, switch. theassumption pipelining network staged hop packetlevel (this assumption challenged next section), transmissiondelay also increased factor number hops. Finally, simpli-fying assumption injection links first switch stage switches links (including reception links) switches approximately length delay, total propagation delay network TotalProp propagation delay single link, TLinkProp , multiplied d+1, hop count plus one account injection link. Thus, best-case lower-bound expression average packet latency network (i.e., latency inthe absence contention) given following expression: Latency ¼Sending overhead + TLinkProp /C2d+1ðÞ +Tr+Ta+Ts ðÞ /C2 d+Packet size Bandwidth/C2d+1ðÞ + Receiving overhead Again, expression page F-40 assumes switches able pipeline packet transmission packet level. Following method presented previously, estimate best-case upper bound effective bandwidth finding narrowest section theend-to-end network pipe. Focusing internal network portion pipe,network bandwidth determined blocking properties topology.Non-blocking behavior achieved providing many alternative pathsbetween every source-destination pair, leading aggregate network bandwidth many times higher aggregate network injection reception band- width. quite costly. solution usually prohibitively expensive, mostnetworks different degrees blocking, reduces utilization theaggregate bandwidth provided topology. This, too, costly termsof performance. amount blocking network depends topology traffic distribution. Assuming bisection bandwidth, BW Bisection , topology implementable (as typically case), used constant measure maximum degree blocking network. ideal case, network always achieves full bisection bandwidth irrespective traffic behavior, thusF.4 Network Topology ■F-41transferring bottlenecking point injection reception links. However, packets destined locations half network necessarily must cross bisection links, links pose potential bottleneck links —potentially reducing network bandwidth full bisection bandwidth. Fortunately,not traffic must cross network bisection, allowing aggre-gate network bandwidth provided topology utilized. Also, networktopologies higher number bisection links tend less blocking asmore alternative paths possible reach destinations and, hence, higher per-centage aggregate network bandwidth utilized. fraction traffic must cross network bisection, captured bisection traffic fraction parameter γ(0<γ/C201), network pipe bisection is, effectively, widened reciprocal fraction, assuming traffic distribution loads bisec-tion links least heavily, average, network links. defines theupper limit achievable network bandwidth, BW Network : BW Network ¼BW Bisection γ Accordingly, expression effective bandwidth becomes following whennetwork topology taken consideration: Effective bandwidth ¼min N/C2BW LinkInjection ,BW Bisection γ,σ/C2N/C2BW LinkReception/C18/C19 important note γdepends heavily traffic patterns generated applications. measured quantity calculated detailed traffic analysis. Example common communication pattern scientific programs nearest neigh- bor elements two-dimensional array communicate given direction. Thispattern sometimes called NEWS communication , standing north, east, west, south —the directions compass. Map 8 /C28 array elements one-to- one onto 64 end node devices interconnected following topologies: bus, ring,2D mesh, 2D torus, hypercube, fully connected, fat tree. long takein best case node send one message northern neighbor one eastern neighbor, assuming packets allowed use minimal path pro- vided topology? corresponding effective bandwidth? Ignore ele-ments northern eastern neighbors. simplify analysis, assumethat networks experience unit packet transport time network hop —that is,T LinkProp ,Tr,Ta,Ts, packet transmission time hop sum one. Also assume delay injection links included unit time, sending/receiving overhead null. Answer communication pattern requires us send 2 /C2(64/C08) 112 total packets — is, 56 packets two communication phases: northward east-ward. number hops suffered packets depends topology. Commu-nication sources destinations one-to-one, σis 100%.F-42 ■Appendix F Interconnection NetworksThe injection reception bandwidth cap effective bandwidth maximum 64 BW units (even though communication pattern requires 56 BWunits). However, maximum may get scaled achievable networkbandwidth, determined bisection bandwidth fraction oftraffic crossing it, γ, topology dependent. various cases: ■Bus—The mapping 8 /C28 array elements nodes makes difference bus nodes equally distant one hop away. However,the 112 transfers done sequentially, taking total 112 time units. bisection bandwidth 1, γis 100%. Thus, effective bandwidth 1 BW unit. ■Ring—Assume first row array mapped nodes 0 7, second row nodes 8 15, on. takes one time unit nodes simul-taneously send eastern neighbor (i.e., transfer node ito node i+1). mapping, northern neighbor node exactly eight hops away takes eight time units, also done parallel allnodes. Total communication time is, therefore, 9 time units. bisectionbandwidth 2 bidirectional links (assuming bidirectional ring), isless full bisection bandwidth 32 bidirectional links. eastward communication, 2 eastward 56 packets must cross bisection worst case, bisection links pose bottlenecks.For northward communication, 8 56 packets must cross two bisec-tion links, yielding γof 10/112 ¼8.93%. Thus, network bandwidth 2/.0893 ¼22.4 BW units. limits effective bandwidth 22.4 BW units well, less half bandwidth required commu-nication pattern. ■2D mesh —There eight rows eight columns grid 64 nodes, perfect match NEWS communication. takes total just2 time units nodes send simultaneously northern neighborsfollowed simultaneous communication eastern neighbors. bisection bandwidth 8 bidirectional links, less full bisection bandwidth. However, perfect matching nearest neighbor communi-cation pattern topology allows maximum effective bandwidth beachieved regardless. eastward communication, 8 56 packets mustcross bisection worst case, exceed bisection band-width. None northward communications crosses network bisec-tion, yielding γof 8/112 ¼7.14% network bandwidth 8/0.0714 ¼112 BW units. effective bandwidth is, therefore, limited communication pattern 56 BW units opposed mesh network. ■2D torus —Wrap-around links torus used communication pattern, torus mapping performance mesh.F.4 Network Topology ■F-43■Hypercube —Assume elements row mapped location within eight 3-cubes comprising hypercube consecutive rowelements mapped nodes one hop away. Northern neighbors besimilarly mapped nodes one hop away orthogonal dimension. Thus,the communication pattern takes 2 time units. hypercube provides fullbisection bandwidth 32 links, 8 112 packets must cross bisection. Thus, effective bandwidth limited communication pattern 56 BW units, hypercube network. ■Fully connected —Here, nodes equally distant one hop away, regardless mapping. Parallel transfer packets northern easterndirections would take 1 time unit injection reception linkscould source sink two packets time. case, 2 time unitsare required. Effective bandwidth limited communication patternat 56 BW units, 1024 network bisection links largely go underutilized. ■Fat tree —Assume mapping elements nodes done ring use switches eight bidirectional ports. allows simul-taneous communication eastern neighbors takes three hops and,therefore, 3 time units three bidirectional stages interconnecting eight nodes eight groups nodes. northern neighbor node resides adjacent group eight nodes, requires fivehops, 5 time units. Thus, total time required fat tree 8 time units.The fat tree provides full bisection bandwidth, worst case half thetraffic needing cross bisection, effective bandwidth 56 BW units (aslimited communication pattern fattree network) isachieved packets continually injected. example lead one wrong conclusion meshes good tori, hypercubes, fat trees, networks higher bisec-tion bandwidth. number simplifications benefit low-bisection networkswere assumed ease analysis. practice, packets typically larger thelink width occupy links many one network cycle. Also,many communication patterns map cleanly 2D mesh networktopology; instead, usually global irregular nature. factors combine increase chances packets blocking low- bisection networks, increasing latency reducing effective bandwidth. put discussion topologies perspective, Figure F.16 listsvariousattributesoftopologiesusedincommercialhigh-performancecomputers. F.5 Network Routing, Arbitration, Switching Routing, arbitration, switching performed every switch along packet ’s path switched media network, matter network topology. Numerous interesting techniques accomplishing network functions beenF-44 ■Appendix F Interconnection Networksproposed literature. section, focus describing representative set approaches used commercial systems commonly used net-work topologies. impact performance also highlighted. Routing Therouting algorithm defines network path, paths, allowed packet. Ideally, routing algorithm supplies shortest paths packets thatCompanySystem [network] nameMax. numberof nodes [×# CPUs]Basic network topologyInjection [reception] node BW MB/sec# data bits per link per directionRaw networklink BWper direction MB/secRaw network bisection BW (bidirectional) GB/sec Intel ASCI Red Paragon4816 [/C22]2D mesh 64/C264400 [400] 16 bits 400 51.2 IBM ASCI White SP Power3 [Colony]512 [/C216]Bidirectional MIN 8- port bidirectionalswitches(typically fat tree Omega)500 [500] 8 bits (+1 bit control)500 256 Intel Thunder Itanium2Tiger4[QsNet II]1024 [/C24]Fat tree 8- portbidirectionalswitches928 [928] 8 bits (+2 controlfor 4b/5bencoding)1333 1365 Cray XT3 [SeaStar]30,508 [/C21]3D torus 40/C232/C2243200 [3200] 12 bits 3800 5836.8 Cray X1E 1024 [/C21]4-way bristled 2D torus(/C2423/C211) express links1600 [1600] 16 bits 1600 51.2 IBM ASC Purple pSeries 575[Federation]>1280 [/C28]Bidirectional MIN 8-port bidirectional switches(typically fattree Omega)2000 [2000] 8 bits (+2 bits ofcontrol novel 5b/ 6bencodingscheme)2000 2560 IBM Blue Gene/ L eServer Sol. [TorusNet.]65,536 [/C22]3D torus 32/C232/C264612.5 [1050]1 bit (bit serial)175 358.4 Figure F.16 Topological characteristics interconnection networks used commercial high-performance machines.F.5 Network Routing, Arbitration, Switching ■F-45traffic load evenly distributed across network links minimize contention. However, paths provided network topology may allowed order guarantee packets delivered, matter trafficbehavior. Paths unbounded number allowed nonminimal hops frompacket sources, instance, may result packets never reaching destina-tions. situation referred livelock . Likewise, paths cause set packets block network forever waiting network resources (i.e.,links associated buffers) held packets set also prevent packetsfrom reaching destinations. situation referred deadlock . dead- lock arises due finiteness network resources, probability occur- rence increases increased network traffic decreased availability ofnetwork resources. network function properly, routing algorithm mustguard anomaly, occur various forms —for example, routing deadlock, request-reply (protocol) deadlock, fault-induced (reconfiguration)deadlock, etc. time, network provide highest possible per-formance, routing algorithm must efficient —allowing many routing options packets paths provided topology, best case. simplest way guarding livelock restrict routing minimal paths sources destinations allowed or, less restrictively,only limited number nonminimal hops. strictest form added benefitof consuming minimal amount network bandwidth, prevents packetsfrom able use alternative nonminimal paths case contention faultsalong shortest (minimal) paths. Deadlock difficult guard against. Two common strategies used practice: avoidance recovery. deadlock avoidance , routing algorithm restricts paths allowed packets keep global network state deadlock-free. common way consists establishing order-ing set resources —the minimal set necessary support network full access —and granting resources packets total partial order cyclic dependency cannot form resources. allows escape pathalways supplied packets matter network avoidentering deadlock state. deadlock recovery , resources granted packets without regard avoiding deadlock. Instead, deadlock possible, mech- anism used detect likely existence deadlock. detected, one packets removed resources deadlock set —possibly regressively dropping packets progressively redirecting packets onto special dead-lock recovery resources. freed network resources granted otherpackets needing resolve deadlock. Let us consider routing algorithms designed distributed switched networks. Figure F.17(a) illustrates one many possible deadlocked configurations packets within region 2D mesh network. routing algorithm avoid deadlocks (and livelocks) allowing use minimal paths cross network dimensions total order. is, links given dimen-sion supplied packet routing algorithm links areneeded packet preceding dimensions reach itsF-46 ■Appendix F Interconnection Networksdestination. illustrated Figure F.17(b) , dimensions crossed XYdimension order. packets must follow order traversing dimensions, exiting dimension links longer required dimension. well-known algorithm referred dimension-order routing (DOR) e-cube routing hypercubes. used many commercial systems built distributed switched networks on-chip networks. routingalgorithm always supplies path given source-destination pair, itis adeterministic routing algorithm. Crossing dimensions order minimal set resources required support network full access avoids deadlock meshes hypercubes. However, distributed switched topologies wrap-around links (e.g., rings tori), total ordering minimal set resources within dimension alsoneeded resources used full capacity. Alternatively, emptyresources bubbles along dimensions would required remain full capacity avoid deadlock. allow full access, either physical links must beduplicated logical buffers associated link must duplicated,resulting physical channels orvirtual channels , respectively, ordering done. Ordering necessary network resources avoid dead- lock—it needed minimal set required support network full access (i.e., escape resource set ). Routing algorithms based technique (called Duato ’s protocol) defined allow alternative paths provided topology used given source-destination pair addition escaperesource set. One allowed paths must selected, preferably most(A) (B)s1s2 d3d4d5 d2d1 s4s5s3s1s2 d3d4d5 d2d1 s4s5s3 Figure F.17 mesh network packets routing sources, si, destinations, di.(a) Deadlock forms packets destined d1through d4blocking others set fully occupy requested buffer resources one hop away destinations. deadlock cycle causes packets needing resourcesalso block, like packets 5destined d5that reached node s3. (b) Deadlock avoided using dimension- order routing. case, packets exhaust routes Xdimension turning Ydimension order complete routing.F.5 Network Routing, Arbitration, Switching ■F-47efficient one. Adapting path response prevailing network traffic condi- tions enables aggregate network bandwidth better utilized contention reduced. routing capability referred adaptive routing used many commercial systems. Example many possible dimensional turns eliminated dimension-order routing n-dimensional mesh network? fewest number turns actually need eliminated still maintaining connectedness dead-lock freedom? Explain using 2D mesh network. Answer dimension-order routing algorithm eliminates exactly half possible dimensional turns easily proven turns lower-ordered dimension higher-ordered dimension allowed, converse isnot true. example, eight possible turns 2D mesh shown inFigure F.17 , four turns X+t Y+,X+t Y/C0,X/C0toY+, X/C0toY/C0 allowed, signs (+ /C0) refer direction travel within dimen- sion. four turns Y+t X+,Y+t X/C0,Y/C0toX+, Y/C0toX/C0are dis- allowed turns. elimination turns prevents cycles kind fromforming —and, thus, avoids deadlock —while keeping network connected. However, expense allowing routing adaptivity. TheTurn Model routing algorithm proves minimum number elim- inated turns prevent cycles maintain connectedness quarter pos-sible turns, right set turns must chosen. particular set ofeliminated turns allow requirements satisfied. elimination ofthe wrong set quarter turns, possible combinations allowedturns emulate eliminated ones (and, thus, form cycles deadlock) forthe network connected. 2D mesh, example, possible eliminate two turns ending westward direction (i.e., Y+t X/C0 andY/C0toX/C0) requiring packets start routes westward direction (if needed) maintain connectedness. Alternatives west-first routing 2Dmeshes negative-first routing north-last routing. these, extra quarterof turns beyond supplied DOR allows partial adaptivity routing, mak-ing adaptive routing algorithms. Routing algorithms centralized switched networks similarly defined avoid deadlocks restric ting use resources total partial order. fat trees, resources totally ordered along paths start-ing input leaf stage upward root back out- put leaf stage. routing algorithm allow packets use resources increasing partial order, first traver sing tree reach least common ancestor (LCA) source destination, back tree reach destinations. many least common ances-tors given destination, multiple al ternative paths allowed going tree, making routing algorit hm adaptive. However, singleF-48 ■Appendix F Interconnection Networksdeterministic path destination p rovided fat tree topology least common ancestor. self-routing property common many MINs readily exploited: switch utput port stage given sim- ply shifts destination node address. generally, tree graph mapped onto topology —whether direct indirect —and links nodes tree level allowed assigning directions them, “up”designates paths moving toward tree root “down ”designates paths moving away root node. allows generic up*/down* routing defined topology packets follow paths (possibly adaptively) consisting zero links followed zero links destination. Up/down ordering pre-vents cycles forming, avoiding deadlock. routing technique used inAutonet —a self-configuring switched LAN —and early Myrinet SANs. Routing algorithms implemented practice combination rout- ing information placed packet header source node routingcontrol mechanism incorporated switches. source routing , entire routing path precomputed source —possibly table lookup —and placed packet header. usually consists output port ports supplied switch along predetermined path source destination, whichcan stripped routing control mechanism switch. additionalbit field included header signify whether adaptive routing isallowed (i.e., one supplied output ports used). distributed routing , routing information usually consists destination address. used routing control mechanism switch along path determinethe next output port, either computing using finite-state machine look- ing local routing table (i.e., forwarding table). Compared distributed routing, source routing simplifies routing control mechanism within net-work switches, requires routing bits header packet, thusincreasing header overhead. Arbitration Thearbitration algorithm determines requested network paths available packets. Ideally, arbiters maximize matching free network resources andpackets requesting resources. switch level, arbiters maximize thematching free output ports packets located switch input ports requesting output ports. requests cannot granted simultaneously, switch arbiters resolve conflicts granting output ports packets fair way thatstarvation requested resources packets prevented. could happen packets shorter queues serve-longest-queue (SLQ) scheme used. Forpackets priority level, simple round-robin (RR) age-basedschemes sufficiently fair straightforward implement. Arbitration distributed avoid centralized bottlenecks. straightfor- ward technique consists two phases: request phase grant phase. Let us assume switch input port associated queue hold incomingF.5 Network Routing, Arbitration, Switching ■F-49packets switch output port associated local arbiter implement- ing round-robin strategy. Figure F.18(a) shows possible set requests four-port switch. request phase , packets head input port queue send single request arbiters corresponding output ports requested bythem. Then, output port arbiter independently arbitrates among requests itreceives, selecting one. grant phase , one requests arbiter granted requested output port. two packets different input portsrequest output port, one receives grant, shown figure. consequence, output port bandwidth remains unused even though input queues packets transmit. simple two-phase technique improved allowing several simul- taneous requests made input port, possibly coming differentvirtual channels multiple adaptive routing options. requests sentto different output port arbiters. submitting one request per inputport, probability matching increases. Now, arbitration requires three phases:request, grant, acknowledgment. Figure F.18(b) shows case two requests made packets input port. request phase, requests submitted output port arbiters, arbiters select one ofthe received requests, done two-phase arbiter. Likewise, grantphase, selected requests granted corresponding requesters. Takinginto account input port submit one request, may receivemore one grant. Thus, selects among possibly multiple grants using somearbitration strategy round-robin. selected grants confirmed thecorresponding output port arbiters acknowledgment phase. seen Figure F.18(b) , could happen input port submits several requests receive grants, requested portsremain free. this, second arbitration iteration improve prob-ability matching. iteration, requests corresponding non-matched input output ports submitted. Iterative arbiters multiple(B) (A)tnemgdelwonkcA tnarG tseuqeR Request Grant Figure F.18 Two arbitration techniques. (a) Two-phased arbitration two four input ports granted requested output ports. (b) Three-phased arbitration three four input ports successful gaining requested outputports, resulting higher switch utilization.F-50 ■Appendix F Interconnection Networksrequests per input port able increase utilization switch output ports and, thus, network link bandwidth. However, comes expense additional arbiter complexity increased arbitration delay, could increase routerclock cycle time critical path. Switching Theswitching technique defines connections established network. Ide- ally, connections network resources established “switched ”only long actually needed exactly point ready andneeded used, considering time space. allows efficient use avail-ablenetworkbandwidthbycompetingtrafficflowsandminimallatency.Connectionsateachhopalong thetopologicalpathallowedbytherouting algorithm grantedby arbitration algorithm established three basic ways: prior packet arrival using circuit switching , upon receipt entire packet using store-and-forward packet switching , upon receipt portions packet unit size smal- ler packet header using cut-through packet switching . Circuit switching establishes circuit priori network bandwidth allocated packet transmissions along entire source-destination path. ispossible pipeline packet transmission across circuit using staging eachhop along path, technique known pipelined circuit switching . routing, arbitration, switching performed one packets, routing bits needed header packets, thus reducing latency overhead. Thiscan efficient information continuously transmitted devicesfor circuit setup. However, network bandwidth removed theshared pool preallocated regardless whether sources need consumingit not, circuit switching inefficient highly wasteful bandwidth. Packet switching enables network bandwidth shared used efficiently packets transmitted intermittently, common case. Packet switching comes two main varieties —store-and-forward cutthrough switching, allow network link bandwidth multi-plexed packet-sized smaller units information. better enables band-width sharing packets originating different sources. finer granularityof sharing, however, increases overhead needed perform switching: Routing,arbitration, switching must performed every packet, routing andflow control bits required every packet flow control used. Store-and-forward packet switching establishes connections packet forwarded next hop sequence along source-destination path entire packet first stored (staged) receiving switch. packets arecompletely stored every switch transmitted, links completelydecoupled, allowing full link bandwidth utilization even links differentbandwidths. property important WANs, price pay ispacket latency; total routing, arbitration, switching delay multiplicativewith number hops, seen Section F.4 analyzing perfor- mance assumption.F.5 Network Routing, Arbitration, Switching ■F-51Cut-through packet switching establishes connections packet “cut ”switches pipelined manner header portion packet (or equivalent amount payload trailing header) staged receiving switches.That is, rest packet need arrive switching granted resources.This allows routing, arbitration, switching delay additive number ofhops rather multiplicative reduce total packet latency. Cut-through comesin two varieties, main differences size unit information onwhich flow control applied and, consequently, buffer requirements switches.Virtual cut-through switching implements flow control packet level, whereas wormhole switching implements flow units, flits, smaller maximum packet size usually least large packet header. Sincewormhole switches need capable storing small portion packet,packets block network may span several switches. cause otherpackets block links occupy, leading premature network saturationand reduced effective bandwidth unless centralized buffer used within theswitch store —a technique called buffered wormhole switching .A sc h p implement relatively large buffers current technology, virtual cut-through commonly used switching technique. However, wormhole switching may still preferred OCNs designed minimize silicon resources. Premature network saturation caused wormhole switching mitigated allowing several packets share physical bandwidth link simulta-neously via time-multiplexed switching flit level. requires physical linksto set virtual channels (i.e., logical buffers mentioned previously) ateach end, packets switched. Before, saw virtual channelscan used decouple physical link bandwidth buffered packets way avoid deadlock. Now, virtual channels multiplexed way bandwidth switched used flits packet advance even though thepacket may share links common blocked packet ahead. This, again,allows network bandwidth used efficiently, which, turn, reduces theaverage packet latency. Impact Network Performance Routing, arbitration, switching impact packet latency loadednetwork reducing contention delay experienced packets. unloadednetwork contention, algorithms used perform routing andarbitration impact latency determine amount delay incurred implementing functions switches —typically, pin-to-pin latency switch chip several tens nanoseconds. change thebest-case packet latency expression given previous section comes fromthe switching technique. Store-and-forward packet switching assumed beforein transmission delay entire packet incurred dhops plus source node. cut-through packet switching, transmission delay pipelinedacross network links comprising packet ’s path granularity packet header instead entire packet. Thus, delay component reduced, shown following lower-bound expression packet latency:F-52 ■Appendix F Interconnection NetworksLatency ¼Sending overhead + TLinkProp /C2d+1ðÞ +Tr+τa+TS ðÞ /C2 d+Packet + d/C2HeaderðÞ ðÞ Bandwidth+ Receiving overhead effective bandwidth impacted efficiently routing, arbitration, switching allow network bandwidth used. routing algorithm distrib-ute traffic evenly across loaded network increase utilization aggregate bandwidth provided topology —particularly, bisection links. arbitration algorithm maximize number switch output portsthat accept packets, also increases utilization network bandwidth. Theswitching technique increase degree resource sharing packets, whichfurther increases bandwidth utilization. combine affect network band-width, BW Network ,b ya n efficiency factor ,ρ, 0 <ρ/C201: BW Network ¼ρ/C2BW Bisection γ efficiency factor, ρ, difficult calculate quantify means simulation. Nevertheless, parameter estimate best-case upper- bound effective bandwidth using following expression takes account effects routing, arbitration, switching: Effective bandwidth ¼min N/C2BW LinkInjection ,ρ/C2BW Bisection γ,σ/C2N/C2BW LinkReception/C18/C19 note ρalso depends well network handles traffic generated applications. instance, ρcould higher circuit switching cut-through switching large streams packets continually transmittedbetween source-destination pair, whereas converse could true packets transmitted intermittently. Example Compare performance deterministic routing versus adaptive routing 3D torus network interconnecting 4096 nodes. plotting latency versusapplied load throughput versus applied load. Also compare efficiencyof best worst networks. Assume virtual cut-through switching,three-phase arbitration, virtual channels implemented. Consider separatelythe cases two four virtual channels, respectively. Assume one thevirtual channels uses bubble flow control dimension order avoid dead- lock; virtual channels used either dimension order (for deterministic routing) minimally along shortest paths (for adaptive routing), done theIBM Blue Gene/L torus network. Answer difficult compute analytically performance routing algorithms given behavior depends several network design parameters com-plex interdependences among them. consequence, designers typically resortto cycle-accurate simulators evaluate performance. One way evaluate effect certain design decision run sets simulations range net- work loads, time modifying one design parameters interest whileF.5 Network Routing, Arbitration, Switching ■F-53keeping remaining ones fixed. use synthetic traffic loads quite fre- quent evaluations allows network stabilize certain workingpoint behavior analyzed detail. method use here(alternatively, trace-driven execution-driven simulation used). Figure F.19 shows typical interconnection network performance plots. left, average packet latency (expressed network cycles) plotted func- tion applied load (traffic generation rate) two routing algorithms two four virtual channels each; right, throughput (traffic delivery rate) issimilarly plotted. Applied load normalized dividing number nodesin network (i.e., bytes per cycle per node). Simulations run theassumption uniformly distributed traffic consisting 256-byte packets, whereflits byte sized. Routing, arbitration, switching delays assumed sumto 1 network cycle per hop time-of-flight delay link assumedto 10 cycles. Link bandwidth 1 byte per cycle, thus providing results independent network clock frequency. seen, plots within graph similar characteristic shapes, different values. latency graph, start no-load latencyAverage packet latency (cycles)10,000 8000 6000 4000 0.012000 0 Applied load (bytes/cycle/node) (A)0.41 0.33 0.25 0.17 0.09 Throughput (bytes/cycle/node)0.4 0.3 0.2 0.1 0.010 Applied load (bytes/cycle/node) (B)0.97 0.49 0.61 0.73 0.85 0.25 0.37 0.13Deterministic DOR, 2 VC Deterministic DOR, 4 VCAdaptive routing, 2 VCAdaptive routing, 4 VC Adaptive routing, 4 VCDeterministic DOR, 4 VCAdaptive routing, 2 VCDeterministic DOR, 2 VC Figure F.19 Deterministic routing compared adaptive routing, either two four virtual channels, assuming uniformly distributed traffic 4 K node 3D torus network virtual cut-through switch-ing bubble flow control avoid deadlock. (a) Average latency plotted versus applied load, (b) through- put plotted versus applied load (the upper grayish plots show peak throughput, lower black plots show sustained throughput). Simulation data collected P. Gilabert J. Flich Universidad Politècnica de València, Spain (2006).F-54 ■Appendix F Interconnection Networksas predicted latency expression given above, slightly increase traf- fic load contention network resources increases. higher applied loads, latency increases exponentially, network approaches saturation point unable absorb applied load, ¼causing packets queue source nodes awaiting injection. simulations, queues keep growingover time, making latency tend toward infinity. However, practice, queues reachtheir capacity trigger application stall packet generation, theapplication throttles waiting acknowledgments/responses outstandingpackets. Nevertheless, latency grows slower rate adaptive routing alter-native paths provided packets along congested resources. reason, adaptive routing allows network reach higher peak throughput number virtual channels compared deterministicrouting. nonsaturation loads, throughput increases fairly linearly appliedload. network reaches saturation point, however, unable delivertraffic rate traffic generated. saturation point, therefore,indicates maximum achievable “peak”throughput, would predicted effective bandwidth expression given above. Beyondsaturation, throughput tends drop consequence massive head-of-line blocking across network (as explained Section F.6 ), much like cars tend advance slowly rush hour. importantregion throughput graph shows significant performance dropthe routing algorithm cause congestion management techniques (discussedbriefly Section F.7 ) used effectively. case, adaptive routing performance drop saturation deterministic routing, mea-sured postsaturation sustained throughput. routing algorithms, virtual channels (i.e., four) give packets greater ability pass blocked packets ahead, allowing higher peak throughput compared fewer virtual channels (i.e., two). adaptive routingwith four virtual channels, peak throughput 0.43 bytes/cycle/node near themaximum 0.5 bytes/cycle/node obtained 100% efficiency (i.e.,ρ¼100%), assuming enough injection reception bandwidth make network bisection bottlenecking point. case, network bandwidthis simply 100% times network bisection bandwidth (BW Bisection ) divided fraction traffic crossing bisection ( γ), given expression above. Tak- ing account bisection splits torus two equally sized halves, γis equal 0.5 uniform traffic half injected traffic destined nodeat side bisection. BW Bisection 4096-node 3D torus network 16/C216/C24 unidirectional links times link bandwidth (i.e., 1 byte/cycle). normalize bisection bandwidth dividing number nodes (as wedid network bandwidth), BW Bisection 0.25 bytes/cycle/node. Dividing γgives ideal maximally obtainable network bandwidth 0.5 bytes/ cycle/node. find efficiency factor, ρ, simulated network simply divid- ing measured peak throughput ideal throughput. efficiency factor forF.5 Network Routing, Arbitration, Switching ■F-55the network fully adaptive routing four virtual channels 0.43/(0.25/ 0.5)¼86%, whereas network deterministic routing two virtual channels 0.37/(0.25/0.5) ¼74%. Besides 12% difference efficiency two, another 14% gain efficiency might obtained even bet-ter routing, arbitration, switching, virtual channel designs. put discussion routing, arbitration, switching perspective, Figure F.20 lists techniques used SANs designed commercial high- performance computers. addition applied SANs shown figure, issues discussed section also apply interconnect domains: OCNs WANs. F.6 Switch Microarchitecture Network switches implement routing, arbitration, switching functions switched-media networks. Switches also implement buffer management mecha- nisms and, case lossless networks, associated flow control. somenetworks, switches also implement part network management functions thatexplore, configure, reconfigure network topology response boot-upand failures. Here, reveal internal structure network switches describ-ing basic switch microarchitecture various alternatives suitable differentrouting, arbitration, switching techniques presented previously. Basic Switch Microarchitecture internal data path switch provides connectivity among input output ports. Although shared bus multiported central memory could used, solutions insufficient expensive, respectively, required aggre-gate switch bandwidth high. high-performance switches implement aninternal crossbar provide nonblocking connectivity within switch, thusallowing concurrent connections multiple input-output port pairs. Buffer-ing blocked packets done using first in, first (FIFO) circularqueues, implemented dynamically allocatable multi-queues (DAMQs) static RAM provide high capacity flexibility. queues placed input ports (i.e., input buffered switch ), output ports (i.e., output buffered switch ), centrally within switch (i.e., centrally buffered switch ), input output ports switch (i.e., input-output-buffered switch ). Figure F.21 shows block diagram input-output-buffered switch. Routing implemented using finite-state machine forwarding table within routing control unit switches. former case, routing infor-mation given packet header processed finite-state machine deter-mines allowed switch output port (or ports routing adaptive), according routing algorithm. Portions routing information header usuallyF-56 ■Appendix F Interconnection NetworksCompanySystem [network] nameMax. numberof nodes[×# CPUs]Basic network topologySwitch queuing (buffers)Network routing algorithmSwitch arbitration techniqueNetwork switching technique Intel ASCI Red Paragon4510 [/C22]2D mesh (64/C264)Input buffered(1 flit)Distributed dimension-order routing2-phased RR,distributedacross switchWormhole novirtualchannels IBM ASCI White SP Power3[Colony]512 [/C216]Bidirectional MIN 8-portbidirectional switches (typically fattree orOmega)Input centralbufferwith output queuing(8-wayspeedup)Source-based LCAadaptive,shortest-path routing, table-basedmulticastrouting2-phased RR,centralizedand distributed outputsfor bypasspathsBuffered wormholeand virtualcut-through multicasting,no virtualchannels Intel Thunder Itanium2 Tiger4[QsNet II]1024 [/C24]Fat tree 8-port bidirectionalswitchesInput bufferedSource-based LCA adaptive,shortest-path routing2-phased RR, priority,aging, distributed outputportsWormhole 2 virtual channels Cray XT3 [SeaStar]30,508 [/C21]3D torus (40/C232/C224)Input staging outputDistributed table-based dimension- order routing2-phased RR, distributed outputportsVirtual cut- 4 virtual channels Cray X1E 1024 [/C21]4-way bristled 2D torus (/C2423/C211) expresslinksInput virtual output queuingDistributed table-based dimension- order routing2-phased wavefront (pipelined) globalarbiterVirtual cut- 4 virtual channels IBM ASC Purple pSeries 575 [Federation]>1280 [/C28]Bidirectional MIN 8- port bidirectionalswitches (typically fat tree orOmega)Input central buffer withoutput queuing (8-wayspeedup)Source distributed table-based LCA adaptive, shortest-path routing, andmulticast2-phased RR, centralized anddistributed outputs bypasspathsBuffered wormhole virtual cut-throughfor multicasting 8 virtualchannels IBM Blue Gene/ L eServer Solution [Torus Net.]65,536 [/C22]3D torus (32/C232/C264)Input- output bufferedDistributed, adaptive bubble escape virtualchannel2-phased SLQ, distributed input andoutputVirtual cut- 4 virtual channels Figure F.20 Routing, arbitration, switching characteristics interconnections networks commercial machines.F.6 Switch Microarchitecture ■F-57stripped modified routing control unit use simplify processing next switch along path. routing implemented using forwardingtables, routing information given packet header used address access forwarding table entry contains allowed switch output port(s) pro- vided routing algorithm. Forwarding tables must preloaded theswitches outset network operation. Hybrid approaches also exist wherethe forwarding table reduced small set routing bits combined witha small logic block. routing bits used routing control unit knowwhat paths allowed decide output ports packets need take. Thegoal approaches build flexible yet compact routing control units,eliminating area power wastage large forwarding table thus suitable OCNs. routing control unit usually implemented centralized resource, although could replicated every input port become abottleneck. Routing done every packet, packets typically arelarge enough take several cycles flow switch, centralizedrouting control unit rarely becomes bottleneck. Figure F.21 assumes centralized routing control unit within switch. Arbitration required two packets concurrently request output port, described previous section. Switch arbitration implemented centralized distributed way. former case, requests status information transmitted central switch arbitrationunit; latter case, arbiter distributed across switch, usually amongthe input and/or output ports. Arbitration may performed multiple times onpackets, may multiple queues associated input port,Link controlPhysical channelInput buffers Demux Mux Crossbar Demux DemuxLink controlPhysical channelLink control Link controlInput buffers Demux Routing control arbitration unit MuxPhysicalchannel Physical channelOutput buffers Mux Output buffers Mux Figure F.21 Basic microarchitectural components input-output-buffered switch.F-58 ■Appendix F Interconnection Networksincreasing number arbitration requests must processed. Thus, many implementations use hierarchical arbitration approach, arbitration first performed locally every input port select one request among corre-sponding packets queues, later arbitration performed globally processthe requests made local input port arbiters. Figure F.21 assumes centralized arbitration unit within switch. basic switch microarchitecture depicted Figure F.21 functions fol- lowing way. packet starts arrive switch input port, link controllerdecodes incoming signal generates sequence bits, possibly deserializ- ing data adapt width internal data path different external link width. Information also extracted packet header linkcontrol signals determine queue packet buffered.As packet received buffered (or entire packet beenbuffered, depending switching technique), header sent routingunit. unit supplies request one output ports arbitration unit.Arbitration requested output port succeeds port free enoughspace buffer entire packet flit, depending switching technique. wormhole switching virtual channels implemented, additional arbitration allocation steps may required transmission individual flit.Once resources allocated, packet transferred across internal cross-bar corresponding output buffer link packets ahead itand link free. Link-level flow control implemented link controller pre-vents input queue overflow neighboring switch end link. Ifvirtual channel switching implemented, several packets may time-multiplexed across link flit-by-flit basis. various input output ports operate independently, several incoming packets may processed concur- rently absence contention. Buffer Organizations mentioned above, queues located switch input, output, bothsides. Output-buffered switches advantage completely eliminatinghead-of-line blocking . Head-of-line (HOL) blocking occurs two packets buffered queue, blocked packet head queue blocks packets queue would otherwise able advance theywere queue head. cannot occur output-buffered switches thepackets given queue status; require output port.However, may case switch input ports simultaneously receive apacket output port. buffers input side, outputbuffers must able store incoming packets time. Thisrequires implementing output queues internal switch speedup ofk. is, output queues must write bandwidth ktimes link bandwidth, kis number switch ports. oftentimes expensive. Hence, solu- tion rarely implemented lossless networks. probabilityof concurrently receiving many packets output port usually small,F.6 Switch Microarchitecture ■F-59commercial systems use output-buffered switches typically implement moderate switch speedup, dropping packets rare buffer overflow. Switches buffers input side able receive packets without hav- ing switch speedup; however, HOL blocking occur within input portqueues, illustrated Figure F.22(a) . reduce switch output port utiliza- tion less 60% even packet destinations uniformly distributed. Asshown Figure F.22(b) , use virtual channels (two case) mitigate HOL blocking eliminate it. effective solution organize theinput queues virtual output queues (VOQs), shown Figure F.22(c) . this, input port implements many queues output ports, thus provid- ing separate buffers packets destined different output ports. populartechnique widely used ATM switches IP routers. main drawbacks Input buffers Crossbar (A)Input port Output port X+ DemuxY– Y+ Y+ X–X+X+ Output port X – Output port Y+ Output port –Input buffers Crossbar (B)Input port Output port X+ X–X+ Y+ Y– Y+X+ Output port X – Output port Y+ Output port –DemuxInput buffers Crossbar (C)Input port Output port X+ X+ X– Y+ Y+ Y–X+ Output port X – Output port Y+ Output port – Figure F.22 (a) Head-of-line blocking input buffer, (b) use two virtual channels reduce HOL block- ing, (c) use virtual output queuing eliminate HOL blocking within switch. shaded input buffer one crossbar currently allocated. assumes input port one access port switch ’s internal crossbar.F-60 ■Appendix F Interconnection NetworksVOQs, however, cost lack scalability: number VOQs grows qua- dratically switch ports. Moreover, although VOQs eliminate HOL blocking within switch, HOL blocking occurring network level end-to-end notsolved. course, possible design switch VOQ support networklevel also —that is, implement many queues per switch input port output ports across entire network —but extremely expensive. alter- native dynamically assign fraction queues store (cache) sep-arately packets headed congested destinations. Combined input-output-buffered switches minimize HOL blocking sufficient buffer space output side buffer packets, minimize switch speedup required due buffers input side. solution thefurther benefit decoupling packet transmission internal crossbar ofthe switch transmission external links. especially usefulfor cut-through switching implementations use virtual channels, flittransmissions time-multiplexed links. Many designs used commer-cial systems implement input-output-buffered switches. Routing Algorithm Implementation important distinguish routing algorithm implementation. routing algorithm describes rules forward packets across net- work affects packet latency network throughput, implementation affectsthe delay suffered packets reaching node, required silicon area, thepower consumption associated routing computation. Several techniqueshave proposed pre-compute routing algorithm and/or hide routingcomputation delay. However, significantly less effort devoted reducesilicon area power consumption without significantly affecting routing flexibil-ity. issues become important, particularly OCNs. Many existing designs address issues implementing relatively simple routing algorithms, sophisticated routing algorithms likely needed future dealwith increasing manufacturing defects, process variability, complicationsarising continued technology scaling, discussed briefly below. mentioned previous section, depending routing algorithm computed, two basic forms routing exist: source distributed routing. Insource routing, complexity implementation moved end nodes wherepaths need stored tables, path given packet selected based destination end node identifier. distributed routing, however, complexity moved switches where, hop along path packet, selectionof output port take performed. distributed routing, two basic implemen-tations exist. first one consists using logic block implements fixedrouting algorithm particular topology. common example animplementation dimension-order routing, dimensions offset anestablished order. Alternatively, distributed routing implemented for-warding tables, entry encodes output port used particularF.6 Switch Microarchitecture ■F-61destination. Therefore, worst case, many entries destination nodes required. methods implementing distributed routing benefits drawbacks. Logic-based routing features short computation delay, usuallyrequires small silicon area, low power consumption. However, logic-based routing needs designed specific topology mind and, therefore,is restricted topology. Table-based distributed routing quite flexible andsupports topology routing algorithm. Simply, tables need filled withthe proper contents based applied routing algorithm (e.g., up*/down* routing algorithm defined irregular topology). However, side table-based distributed routing non-negligible area power cost.Also, scalability problematic table-based solutions as, worst case, sys-tem N end nodes (and switches) requires many N tables Nentries, thus quadratic cost. Depending network domain, one solution suitable other. instance, SANs, usual find table-based solutions case withInfiniBand. environments, like OCNs, table-based implementations avoided due aforementioned costs power silicon area. envi- ronments, advisable rely logic-based implementations. Herein liessome challenges OCN designers face: ever continuing technology scalingthrough device miniaturization leads increases number manufacturingdefects, higher failure rates (either transient permanent), significant process var-iations (transistors behaving differently design specs), need differentclock frequency voltage domains, tight power energy budgets. ofthese challenges translate network needing support heterogeneity. Dif- ferent —possibly irregular —regions network created owing failed components, powered switches links, disabled components (due tounacceptable variations performance) on. Hence, heterogeneous systemsmay emerge homogeneous design. framework, important effi-ciently implement routing algorithms designed provide enough flexibility toaddress new challenges. well-known solution providing certain degree flexibility much compact traditional table-based approaches interval routing [Leeuwen 1987 ], range destinations defined output port. Although approach flexible enough, provides clue addressemerging challenges. recent approach provides plausible implementationdesign point lies logic-based implementation (efficiency) table-based implementation (flexibility). Logic-Based Distributed Routing (LBDR) ahybrid approach takes reference regular 2D mesh allows irregularnetwork derived due changes topology induced manufactur-ing defects, failures, anomalies. Due faulty, disabled, powered- components, regularity compromised dimension-order routing algorithm longer used. support topologies, LBDR defines aset configuration bits switch. Four connectivity bits used eachswitch indicate connectivity switch neighbor switches theF-62 ■Appendix F Interconnection Networkstopology. Thus, one connectivity bit per port used. connectivity bits used, instance, disable output port leading faulty component. Addi- tionally, eight routing bits used, two per output port, define available routing options. value routing bits set power-on computed routing algorithm implemented network. Basically, routing bit set, indicates packet leave switch asso- ciated output port allowed perform certain turn next switch. respect, LBDR similar interval routing, defines geographical areas instead ranges destinations. Figure F.23 shows example topology-agnostic routing algorithm implemented LBDR irregular topology. figure shows computed configuration bits. connectivity routing bits used implement routing algorithm. purpose, small set logic gates used combination con- figuration bits. Basically, LBDR approach takes reference initial topol- ogy (a 2D mesh), makes decision based current coordinates router, coordinates destination router, configuration bits. Figure F.24 shows required logic, Figure F.25 shows example packet forwarded source destination use config- uration bits. noticed, routing restrictions enforced preventing use west port switch 10. LBDR represents method efficient routing implementation OCNs. mechanism recently ext ended support non-minimal paths, collective communication operations, traffic isolation. improve- ments made maintaining compact efficient implementation use small set configuration bits. detailed description LBDR extensions, current research OCNs found Flich [2010] . 0123 7 6 5 4 89 13 Bidirectional routing restriction0 11 1 1 1 1 1 1 1 1 ---- ---- ---- ------ ---- ---- -- -- ---- -------- -- ---- -- 1 1111111 1 1 1 1 1 1 1111111111 1 11 11 1111 11 11 11 1 1 1 1 1 11 1 1 11111 1 1 1 1 11 1 1 1 11 1 1 1 1 11 11 11 11 11 11 11 11 1 11 1 1 1 1 1111 11 11 11 11 11 11 112 3 4 5 6 7 8 9 10 11 12 13 14 15Router Cn Ce Cw Cs Rne Rnw Ren Res Rwn Rws Rse Rsw 00 0 0 0 0 00000 000 0 0 00 00 0 0 0000 12 Figure F.23 Shown example irregular network uses LBDR implement routing algorithm. router, connectivity routing bits defined.F.6 Switch Microarchitecture ■F-63Pipelining Switch Microarchitecture Performance enhanced pipelining switch microarchitecture. Pipe- lined processing packets switch similarities pipelined execution instructions vector processor. vector pipeline, single instruction indi- cates operation apply vector elements executed pipelined way. Similarly, switch pipeline, single packet header indicates pro- cess internal data path physical transfer units (or phits ) packet, processed pipelined fashion. Also, packets different input ports independent other, processed parallel similar way mul- tiple independent instructions threads pipelined instructions executed parallel. 0123 7 6 5 4 8 9 10 11 15 14 13 12 Message Bidirectional routing restriction0123 7 6 5 4 8 9 10 11 15 14 13 120123 7 6 5 4 8 9 10 11 15 14 13 12 Figure F.25 Example routing message Router 14 Router 5 using LBDR router. ComparatorXdst Xcurr YcurrYdst E =Ce·(E'·!N'·!S' +E'·N'·Ren +E'·S'·Res) W=Cw·(W'·!N'·!S' +W'·N'·Rwn +W'·S'·Rws) =Cs·(S'·!E'·!W' +S'·E'·Rse +S'·W'·Rsw) L =!N'·!E'·!W'·!S'W' W'E'N' E'N' N'RneCn N Rnw1st stage N' E' W' S'2nd stage Figure F.24 LBDR logic input port router.F-64 ■Appendix F Interconnection NetworksThe switch microarchitecture pipelined analyzing basic functions performed within switch organizing several stages. Figure F.26 shows block diagram five-stage pipelined organization basic switchmicroarchitecture given Figure F.21 , assuming cut-through switching use forwarding table implement routing. receiving header portionof packet first stage, routing information (i.e., destination address) isused second stage look allowed routing option(s) forwardingtable. Concurrent this, portions packet received bufferedin input port queue first stage. Arbitration performed third stage. crossbar configured allocate granted output port packet fourth stage, packet header buffered switch output port readyfor transmission external link fifth stage. Note second Link controlPhysical channelInput buffers5 egatS 4 egatS 3 egatS 2 egatS 1 egatS Demux Mux Crossbar Demux Arbitration unit Crossbar control Output port # Forwarding tableHeader fill DemuxLink controlPhysical channel Packet header Payload fragment Payload fragment Payload fragmentIBLink control Link controlInput buffers Demux Routing control unit MuxPhysicalchannel PhysicalchannelOutput buffers Mux Output buffers Mux RC SA ST OB IB IB IB ST OB IB IB IB ST OB IB IB IB ST OB Figure F.26 Pipelined version basic input-output-buffered switch. notation figure follows: IB input link control buffer stage, RC route computation stage, SA crossbar switch arbitration stage,ST crossbar switch traversal stage, OB output buffer link control stage. Packet fragments (flits) coming header remain IB stage header processed crossbar switch resources provided.F.6 Switch Microarchitecture ■F-65third stages used packet header; payload trailer portions packet use three stages —those used data flow-thru internal data path switch set up. virtual channel switch usually requires additional stage virtual channel allocation. Moreover, arbitration required every flit transmissionthrough crossbar. Finally, depending complexity routing arbi-tration algorithms, several clock cycles may required operations. Switch Microarchitecture Enhancements mentioned earlier, internal switch speedup sometimes implemented toincrease switch output port utilization. speedup usually implemented byincreasing clock frequency and/or internal data path width (i.e., phit size)of switch. alternative solution consists implementing several parallel datapaths input port ’s set queues output ports. One way increasing number crossbar input ports. implementing several physical queues per input port, achieved devoting separate crossbarport input queue. example, IBM Blue Gene/L implements twocrossbar access ports two read ports per switch input port. Another way implementing parallel data paths input output ports move buffers crossbar crosspoints. switch architectureis usually referred buffered crossbar switch . buffered crossbar provides independent data paths input port different output ports, thus mak- ing possible send kpackets time given input port kdifferent output ports. implementing independent crosspoint memories input-output port pair, HOL blocking eliminated switch level. Moreover, arbi-tration significantly simpler switch architectures. Effectively, eachoutput port receive packets disjoint subset crosspoint mem-ories. Thus, completely independent arbiter implemented switchoutput port, arbiters simple. buffered crossbar would ideal switch architecture expensive. number crosspoint memories increases quadratically number switch ports, dramatically increasing cost reducing scalabilitywith respect basic switch architecture. addition, crosspoint memorymust large enough efficiently implement link-level flow control. reducecost, designers prefer input-buffered combined input-output-bufferedswitches enhanced mechanisms described previously. F.7 Practical Issues Commercial Interconnection Networks practical issues addition technical issues described thus far important considerations interconnection networks within certain domains. mention below.F-66 ■Appendix F Interconnection NetworksConnectivity type number devices communicate communication require- ments affect complexity interconnection network protocols. Theprotocols must target largest network size handle types anomaloussystemwide events might occur. Among issues following:How lightweight network interface hardware/software be? itattach memory network I/O network? support cache coher-ence? operating system must get involved every network transaction,the sending receiving overhead becomes quite large. network interface attaches I/O network (PCI-Express HyperTransport interconnect), injection reception bandwidth limited I/O network. Thisis case Cray XT3 SeaStar, Intel Thunder Tiger 4 QsNet II, many supercomputer cluster networks. support coherence, sender may toflush cache send, receiver may flush cache beforeeach receive prevent stale-data problem. flushes increase sendingand receiving overhead, often causing network interface networkbottleneck. Computer systems typically multiplicity interconnects different functions cost-performance objectives. example, processor-memory inter-connects usually provide higher bandwidth lower latency I/O interconnectsand likely support cache coherence, less likely follow orbecome standards. Personal computers typically processormemory intercon-nect I/O interconnect (e.g., PCI-X 2.0, PCIe Hyper-Transport) designed toconnect fast slow devices (e.g., USB 2.0, Gigabit Ethernet LAN, Firewire800). Blue Gene/L supercomputer uses five interconnection networks, one 3D torus used interprocessor application traffic. others include tree-based collective communication network broadcastand multicast; tree-based barrier network combining results (scatter, gather);a control network diagnostics, debugging, initialization; Gigabit EthernetnetworkforI/Obetweenthenodesanddisk.TheUniversityofTexasatAustin ’sTRI PS Edge processor eight specialized on-chip networks —some bidirectional channels wide 128 bits 168 bits direction —to interconnect 106 heterogeneous tiles composing two processor cores L2 on-chip cache. also chip-to-chip switched network interconnect multiple chips multi- processor configuration. Two on-chip networks switched networks: One isused operand transport used on-chip memory communication.The others essentially fan-out trees recombination dedicated link networksused status control. portion chip area allocated interconnect issubstantial, five seven metal layers used global network wiring. Standardization: Cross-Company Interoperability Standards useful many places computer design, including interconnectionnetworks. Advantages successful standards include low cost stability.F.7 Practical Issues Commercial Interconnection Networks ■F-67The customer many vendors choose from, keeps price close cost due competition. makes viability interconnection independent stability single company. Components designed standard interconnectionmay also larger market, higher volume reduce vendors ’ costs, benefiting customer. Finally, standard allows many companiesto build products interfaces standard, customer towait single company develop interfaces products interest. One drawback standards time takes committees special- interest groups agree definition standards, problem technology changing rapidly. Another problem standardize: one hand, designers would like standard anything built;on hand, would better something built standardizationto avoid legislating useless features omitting important ones. done tooearly, often done entirely committee, like asking chefsin France prepare single dish food —masterpieces rarely served. Stan- dards also suppress innovation level, since standards fix interfaces — least next version standards surface, every years longer. often, seeing consortiums companies getting together define agree technology serve “de facto ”industry standards. case InfiniBand. LANs WANs use standards interoperate effectively. WANs involve many types companies must connect many brands computers, itis difficult imagine proprietary WAN ever successful. ubiquitousnature Ethernet shows popularity standards LANs well asWANs, seems unlikely many customers would tie viability LAN stability single company. SANs standardized Fibre Channel, proprietary. OCNs part proprietarydesigns, gaining widespread commercial use system-on-chip(SoC) applications, IBM ’s CoreConnect ARM ’s AMBA. Congestion Management Congestion arises many packets try use link set links. leads situation bandwidth required exceeds bandwidthsupplied. Congestion degrade network performance: simply, thecongested links running maximum capacity. Performance degradation occurs presence HOL blocking where, consequence packets going noncongested destinations getting blocked packets going congested des-tinations, link bandwidth wasted network throughput drops, illus-trated example given end Section F.4 .Congestion control refers schemes reduce traffic collective traffic nodes large forthe network handle. One advantage circuit-switched network that, circuit estab- lished, ensures sufficient bandwidth deliver informationF-68 ■Appendix F Interconnection Networkssent along circuit. Interconnection bandwidth reserved circuits estab- lished, network full, circuits established. switch- ing techniques generally reserve interconnect bandwidth advance, theinterconnection network become clogged many packets. withpoor rush-hour commuters, traffic jam packets increases packet latency and, inextreme cases, fewer packets per second get delivered interconnect. orderto handle congestion packet-switched networks, form congestion man- agement must implemented. two kinds mechanisms used control congestion eliminate performance degradation intro- duced congestion. three basic schemes used congestion control interconnection networks, weaknesses: packet discarding, flow control, andchoke packets. simplest scheme packet discarding , discussed briefly Section F.2 . packet arrives switch room buffer, packet discarded. scheme relies higher-level software thathandles errors transmission resend lost packets. leads significant band-width wastage due (re)transmitted packets later discarded and, therefore, typically used lossy networks like Internet. second scheme relies flow control , also discussed previously. buffers become full, link-level flow control provides feedback prevents thetransmission additional packets. backpressure feedback rapidly propagates backward reaches sender(s) packets producing congestion,forcing reduction injection rate packets network. main draw-backs scheme sources become aware congestion late thenetwork already congested, nothing done alleviate congestion. Back- pressure flow control common lossless networks like SANs used supercom- puters enterprise systems. elaborate way using flow control implementing directly sender receiver end nodes, generically called end-to-end flow control .Windowing one version end-to-end credit-based flow control window size large enough efficiently pipeline packets thenetwork. goal window limit number unacknowledgedpackets, thus bounding contribution source congestion, arise. TCP protocol uses sliding window. Note end-to-end flow control describes interaction two nodes interconnection network,not entire interconnection network end nodes. Hence, flow controlhelps congestion control, global solution. Choke packets used third scheme, built upon premise traffic injection throttled congestion exists across net-work. idea switch see busy enter warningstate passes threshold. packet received switch warning state sent back source via choke packet includes intended des- tination. source expected reduce traffic destination fixed per-centage. Since likely already sent packets along path, thesource node waits packets transit returned acting onF.7 Practical Issues Commercial Interconnection Networks ■F-69the choke packets. scheme, congestion controlled reducing packet injection rate traffic reduces, metering lights guard on-ramps con- trol rate cars entering freeway. scheme works efficiently thefeedback delay short. congestion notification takes long time, usuallydue long time flight, congestion control scheme may become unsta-ble—reacting slowly producing oscillations packet injection rate, lead poor network bandwidth utilization. alternative congestion control consists eliminating negative consequences congestion. done eliminating HOL blocking every switch network discussed previously. Virtual output queues used purpose; however, would necessary implement manyqueues every switch input port devices attached network. solutionis expensive, scalable all. Fortunately, possible achievegood results dynamically assigning set-aside queues store onlythe congested packets travel hot-spot regions network,very much like caches intended store frequently accessedmemory locations. strategy referred regional explicit congestion notification (RECN). Fault Tolerance probability system failures increases transistor integration density number devices system increases. Consequently, system reliability andavailability become major concerns even important futuresystems proliferation interconnected devices. practical issue arises, therefore, whether interconnection network relies devices operational order network work properly. Since software failuresare generally much frequent hardware failures, another question sur-faces whether software crash single device prevent rest ofthe devices communicating. Although hardware designers try buildfault-free networks, practice, question rate failures, notwhether prevented. Thus, communication subsystem must havemechanisms dealing faults —not —they occur. two main kinds failure interconnection network: transient andpermanent . Transient failures usually produced electromagnetic inter- ference detected corrected using techniques described inSection F.2 . Oftentimes, dealt simply retransmitting packet either link level end-to-end. Permanent failures occur somecomponent stops working within specifications. Typically, produced byoverheating, overbiasing, overuse, aging, cannot recovered fromsimply retransmitting packets help higher-layer software pro- tocol. Either alternative physical path must exist network supplied routing algorithm circumvent fault network crippled,unable deliver packets whose paths faulty resources. Three major categories techniques used deal permanent failures: resource sparing, fault-tolerant routing , network reconfiguration . firstF-70 ■Appendix F Interconnection Networkstechnique, faulty resources switched bypassed, spare resources switched replace faulty ones. example, ServerNet intercon- nection network designed two identical switch fabrics, one isusable given time. case failure one fabric, used. Thistechnique also implemented without switching spare resources, leadingto degraded mode operation failure. IBM Blue Gene/L supercom-puter, instance, facility bypass failed network resources retain-ing base topological structure routing algorithm. main drawback thistechnique relatively large number healthy resources (e.g., midplane node boards) may need switched failure order retain base topological structure (e.g., 3D torus). Fault-tolerant routing, hand, takes advantage multiple paths already existing network topology route messages presence fail-ures without requiring spare resources. Alternative paths supported faultcombination identified design time incorporated routing algo-rithm. fault detected, suitable alternative path used. main dif-ficulty using technique guaranteeing routing algorithm remain deadlock-free using alternative paths, given arbitrary fault patterns may occur. especially difficult direct networks whose regularitycan compromised fault pattern. Cray T3E example system thatsuccessfully applies technique 3D torus direct network. manyexamples technique systems using indirect networks, thebidirectional multistage networks ASCI White ASC Purple. net-works provide multiple minimal paths end nodes and, inherently, norouting deadlock problems (see Section F.5 ). networks, alternative paths selected source node case failure. Network reconfiguration yet another, general technique handle vol- untary involuntary changes network topology due either failures tosome cause. order network reconfigured, nonfaulty por-tions topology must first discovered, followed computation newrouting tables distribution routing tables corresponding networklocations (i.e., switches and/or end node devices). Network reconfigurationrequires use programmable switches and/or network interfaces, depending routing performed. may also make use generic routing algorithms (e.g., up*/down* routing) configured possible network topol-ogies may result faults. strategy relieves designer tosupply alternative paths possible fault combination design time. Pro-grammable network components provide high degree flexibility theexpense higher cost latency. standard proprietary interconnectionnetworks clusters SANs —including Myrinet, Quadrics, InfiniBand, Advanced Switching, Fibre Channel —incorporate software (re)configur- ing network routing accordance prevailing topology. Another practical issue ties node failure tolerance. interconnection net- work survive failure, also continue operation new node addedto removed network, usually referred hot swapping ? not, addition removal new node disables interconnection network, isF.7 Practical Issues Commercial Interconnection Networks ■F-71impractical WANs LANs usually intolerable SANs. Online system expansion requires hot swapping, networks allow it. Hot swap- ping usually supported implementing dynamic network reconfiguration ,i n network reconfigured without stop user traffic. maindifficulty guaranteeing deadlock-free routing routing tablesfor switches and/or end node devices dynamically asynchronously updatedas one routing algorithm may alive (and, perhaps, clashing) thenetwork time. WANs solve problem dropping packetswhenever required, dynamic network reconfiguration much complex lossless networks. Several theories practical techniques recently developed address problem efficiently. Example Figure F.27 shows number failures 58 desktop computers local area network period one year. Suppose one local area net- work based network requires machines operational theinterconnection network send data; node crashes, cannot accept mes-sages, interconnection becomes ch oked data waiting delivered. alternative traditional local area network, operate thepresence node failures; interconnection simply discards messages anode decides accept them. Assuming need yourworkstation connecting LAN get work done, much greater chances prevented getting work done using failure-intolerant LAN versus traditional LANs? Assume downtime acrash less 30 minutes. Calculate using one-hour intervals thisfigure. Answer Assuming numbers Figure F.27 , percentage hours ’t get work done using failure-intolerant network Intervals failures Total intervals¼Total intervals /C0Intervals failures Total intervals ¼8974/C08605 8974¼369 8974¼4:1% percentage hours ’t get work done using traditional network time workstation crashed. failures equallydistributed among workstations, percentage Failures =Machines Total intervals¼654=58 8974¼11:28 8974¼0:13% Hence, 30 times likely prevented getting work done failure-intolerant LAN traditional LAN, accord-ing failure statistics Figure F.27 . Stated alternatively, person respon- sible maintaining LAN would receive 30-fold increase phone calls fromirate users!F-72 ■Appendix F Interconnection NetworksF.8 Examples Interconnection Networks provide mass concepts described previous sections, look five example networks four interconnection network domains consid-ered appendix. addition one OCN, LAN, WAN areas, look two examples SAN area: one system area networksFailed machines per time intervalOne-hour intervals number failed machines first columnTotal failures per one-hour intervalOne-day intervals number failed machines first columnTotal failures per one-day interval 0 8605 0 184 0 1 264 264 105 1052 50 100 35 7032 5 7 5 1 1 3 341 0 4 06 2 45 7 35 9 456 3 18 6 3671 7 4 2 881 8 4 3 29 2 18 2 18 10 2 2011 1 11 2 2212 1 1217 1 1720 1 2021 1 21 1 2131 1 3138 1 3858 1 58 Total 8974 654 373 573 Figure F.27 Measurement reboots 58 DECstation 5000 running Ultrix 373-day period. reboots distributed time intervals one hour one day. first column sorts intervals according num-ber machines failed interval. next two columns concern one-hour intervals, last two col-umns concern one-day intervals. second fourth columns show number intervals number failed machines. third fifth columns product number failed machines number intervals. example, 50 occurrences one-hour intervals 2 failed machines, total 100 failedmachines, 35 days 2 failed machines, total 70 failures. would expect, number failures per interval changes size interval. example, day 31 failures might include one hour 11 failures one hour 20 failures. last row shows total number column; number offailures ’t agree multiple reboots machine interval result separate entries. (Randy Wang University California –Berkeley collected data.)F.8 Examples Interconnection Networks ■F-73and one system/storage area networks. first two examples proprietary networks used high-performance systems; latter three examples network standards widely used commercial systems. On-Chip Network: Intel Single-Chip Cloud Computer continued increases transistor integration predicted Moore ’s law, processor designers gun find ways combating chip-crossing wire delay problems associated deep submicron technology scaling.Multicore microarchitectures gained popularity, given advantages ofsimplicity, modularity, ability exploit parallelism beyond canbe achieved aggressive pipelining multiple instruction/data issuingon single core. matter whether processor consists single core mul-tiple cores, higher higher demands placed intrachip communica-tion bandwidth keep pace —not mention interchip bandwidth. spurred great amount interest OCN designs efficiently support commu- nication instructions, register operands, memory, I/O data within andbetween processor cores chip. focus one suchon-chip network: Intel Single-chip Cloud Computer prototype. Single-chip Cloud Computer (SCC) prototype chip multiprocessor 48 Intel IA-32 architecture cores. Cores laid (see Figure F.28 )o na network 2D mesh topology (6 /C24). network connects 24 tiles, 4 on- die memory controllers, voltage regulator controller (VRC), external system interface controller (SIF). tile two cores connected router. four memory controllers connected boundaries mesh, two oneach side, VRC SIF controllers connected bottom border ofthe mesh. memory controller address two DDR3 DIMMS, 8 GB memory, thus resulting maximum 64 GB memory. VRC controllerallows core system interface adjust voltage six pre-defined regions configuring network (two 2-tile regions). clock also adjusted finer granularity tile operating frequency. regions turned scaled large power savings. methodallows full application control power state cores. Indeed, applicationshave API available define voltage frequency region. SIFcontroller used communicate network outside chip. tiles includes two processor cores (P54C-based IA) associated L1 16 KB data cache 16 KB instruction cache 256 KB L2 cache (withthe associated controller), 5-port router, traffic generator (for testing purposes only), mesh interface unit (MIU) handling message passing requests, memory look-up tables (with configuration registers set mapping core ’s physical addresses extended memory map system), message-passing buffer,and circuitry clock generation synchronization crossing asynchro-nous boundaries.F-74 ■Appendix F Interconnection NetworksFocusing OCN, MIU unit charge interfacing cores network, including packetization de-packetization large messages; com-mand translation address decoding/lookup; link-level flow control creditmanagement; arbiter decisions following round-robin scheme. credit-based flow control mechanism used together virtual cut-through switching(thus making necessary split long messages packets). routers con- nected 2D mesh layout, power supply clock source. Links connecting routers 16B+2B side bands running 2 GHz. Zero-load latencyis set 4 cycles, including link traversal. Eight virtual channels used per-formance (6 VCs) protocol-level deadlock handling (2 VCs). message-levelarbitration implemented wrapped wave-front arbiter. dimension-orderXY routing algorithm used pre-computation output port performedat every router. Besides tiles regions defined voltage frequency, network (made routers links) single region. Thus, network com- ponents run speed use power supply. asynchronousclock transition required router tile. One distinctive features SCC architecture support messaging-based communication protocol rather hardware cache-coherentRTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTile RTileMC(x, y) =(0, 3) (x, y) =(5, 0)(x, y) =(5, 3)DIMM MCMC MCDIMM DIMM DIMM VRC System interface System FPGA PCIeSCC die Management console PC(x, y)=(0, 0) (x, y)=(3, 0) Figure F.28 SCC Top-level architecture. Howard, J. et al., IEEE International Solid-State Circuits Conference Digest Technical Papers , pp. 58 –59.F.8 Examples Interconnection Networks ■F-75memory inter-core communication. Message passing buffers located every router APIs provided take full control MPI structures. Cache coherency implemented software. SCC router represents significant improvement Teraflops pro- cessor chip implementation 2D on-chip interconnect. Contrasted withthe 2D mesh implemented Teraflops processor, implementation tunedfor wider data path multiprocessor interconnect latency, area, andpower optimized width. targets lower 2-GHz frequency operationcompared 5 GHz predecessor Teraflops processor, yet higher- performance interconnect architecture. System Area Network: IBM Blue Gene/L 3D Torus Network IBM BlueGene/L largest-scaled, highest-performing computer system world 2005, according www.top500.org .With 65,536 dual-processor compute nodes 1024 I/O nodes, 360 TFLOPS (peak) supercomputer asystem footprint approximately 2500 square feet. processors nodecan used computation handle communication protocolprocessing virtual mode or, alternatively, one processors usedfor computation network interface processing. Packets range size 32 bytes maximum 256 bytes, 8 bytes used header. header includes routing, virtual channel, link-level flow control,packet size, information, along 1 byte CRC protectthe header. Three bytes used CRC packet level, 1 byte servesas valid indicator. main interconnection network proprietary 32 /C232/C264 3D torus SAN interconnects 64 K nodes. node switch six 350 MB/sec bidirec-tional links neighboring torus nodes, injection bandwidth 612.5 MB/sec two node processors, reception bandwidth 1050 MB/sec two node processors. reception bandwidth network equals theinbound bandwidth across switch ports, prevents reception links frombottlenecking network performance. Multiple packets sunk concurrentlyat destination node higher reception link bandwidth. Two nodes implemented 2 /C21/C21 compute card, 16 compute cards 2 I/O cards implemented 4 /C24/C22 node board, 16 node boards implemented 8 /C28/C28 midplane, 2 midplanes form 1024-node rack physical dimensions 0.9 /C20.9/C21.9 cubic meters. Links maximum physical length 8.6 meters, thus enabling efficient link-level flow control withreasonably low buffering requirements. Low latency achieved implementingvirtual cut-through switching, distributing arbitration switch input outputports, precomputing current routing path previous switch using afinite-state machine part routing delay removed criticalpath switches. High effective bandwidth achieved using input-bufferedF-76 ■Appendix F Interconnection Networksswitches dual read ports, virtual cut-through switching four virtual chan- nels, fully adaptive deadlock-free routing based bubble flow control. key feature networks size fault tolerance. Failure rate reduced using relatively low link clock frequency 700 MHz (same processorclock) edges clock used (i.e., 1.4 Gbps 175 MB/sectransfer rate supported bit-serial network link direction), butfailures may still occur network. case failure, midplane node boardscontaining fault(s) switched nd bypassed isolate fault, com- putation resumes last checkpoint. Bypassing done using separate bypass switch boards associated midplane additional set torus node boards. bypass switch board configured connecteither corresponding links midplane node boards next bypassboard, effectively removing corresponding set midplane node boards.Although number processing nodes reduced degree net-work dimensions, machine retains topological structure routingalgorithm. collective communication operations barrier synchronization, broadcast/multicast, reduction, performed well 3D torus network would flooded traffic. remedy this, two separatetree networks higher per-link bandwidth used implement collectiveand combining operations efficien tly. addition providing support efficient synchronization broadcast/multicast, hardware used performsome arithmetic reduction operations efficient way (e.g., compute thesum maximum value set values, one processing node).In addition 3D torus two tree networks, Blue Gene/L imple- ments I/O Gigabit Ethernet network control system Fast Ethernet net- work lower bandwidth provide parallel I/O, configuration, debugging,and maintenance. System/Storage Area Network: InfiniBand InfiniBand industrywide de facto networking standard developed October 2000 consortium companies belonging InfiniBand Trade Associa-tion. InfiniBand used system area network interprocessor commu-nication storage area network server I/O. switch-basedinterconnect technology provides flexibility topology, routing algo- rithm, arbitration technique implemented vendors users. InfiniBand supports data transmission rates 2 120 Gbp/link per direction across distancesof 300 meters. uses cut-through switching, 16 virtual channels service levels,credit-based link-level flow control, weighted round-robin fair scheduling andimplements programmable forwarding tables. also includes features useful forincreasing reliability system availability, communication subnet man-agement, end-to-end path establishment, virtual destination naming.F.8 Examples Interconnection Networks ■F-77Figure F.30 shows packet format InfiniBand juxtaposed two net- work standards LAN WAN areas. Figure F.31 compares various char- acteristics InfiniBand standard two proprietary system area networkswidely used research commercial high-performance computer systems.Institution andprocessor[network] nameYear builtNumber networkports [coresor tiles +other ports]Basic network topology# data bits perlink per directionLink bandwidth[link clock speed]Routing; arbitration; switching# chip metal layers;flow control;#virtual channels MIT Raw [GeneralDynamicNetwork]2002 16 ports [16 tiles]2D mesh (4/C24)32 bits 0.9 GB/sec [225 MHz,clocked atproc speed]XY DOR request-replydeadlockrecovery; RR arbitration; wormhole6 layers; credit-basedno virtualchannels IBM Power5 2004 7 ports [2 PE cores+5 otherports]Crossbar 256 bits Inst fetch;64 bits stores; 256 bitsLDs[1.9 GHz, clocked atproc speed]Shortest-path; nonblocking;circuit switch7 layers; handshaking;no virtual channels U.T. Austin TRIP Edge [Operand Network]2005 25 ports [25 execution unit tiles]2D mesh (5/C25)110 bits 5.86 GB/sec [533 MHz clock scaled 80%]YX DOR; distributed RR arbitration; wormhole7 layers; on/ flow control; virtualchannels U.T. Austin TRIP Edge [On-ChipNetwork]2005 40 ports [16 L2 tiles +24 networkinterface tile]2D mesh (10/C24)128 bits 6.8 GB/sec [533 MHz clock scaledby 80%]YX DOR; distributed RR arbitration;VCT switched7 layers; credit-based flow control;4 virtualchannels Sony, IBM, Toshiba Cell [ElementInterconnectBus]2005 12 ports [1 PPE 8 SPEs+3other ports formemory, I/O interface] Ring (4total, 2 eachdirection)128 bits data (+16 bits tag)25.6 GB/sec [1.6 GHz, clocked athalf procspeed]Shortest-path; tree-based RR arbitration(centralized);pipelined circuit switch8 layers; credit-based flow control;no virtualchannels Sun UltraSPARCT1 processor2005 13 ports [8 PE cores+4 L2 banks+1 shared I/O]Crossbar 128 bits forthe 8 coresand 4L 2 banks19.2 GB/sec [1.2 GHz,clocked atproc speed]Shortest-path; age-basedarbitration;VCT switched9 layers; handshaking;no virtualchannels Figure F.29 Characteristics on-chip networks implemented recent research commercial processors. processors implement multiple on-chip networks (not shown) —for example, two MIT Raw eight TRIP Edge.F-78 ■Appendix F Interconnection NetworksATM Data (48)Destination ChecksumTInfiniBand Sequence numberTVersion 32 bitsEthernet Preamble Preamble Pad (0–46) Checksum ChecksumChecksum32 bitsDestinationDestination Source DestinationPartition key Destination queue TypeLengthLength SourceSource Data (0–1500) Data (0–4096) 32 bits Figure F.30 Packet format InfiniBand, Ethernet, ATM. ATM calls messages “cells ”instead packets, proper name ATM cell format. width drawing 32 bits. three formats destination addres- sing fields, encoded differently situation. three also checksum field catch transmission errors, although ATM checksum field calculated header; ATM relies higher-level protocols catcherrors data. InfiniBand Ethernet length field, since packets hold variable amount data, former counted 32-bit words latter bytes. InfiniBand ATM headers type field (T) gives type packet. remaining Ethernet fields preamble allow receiver recover clock fromthe self-clocking code used Ethernet, source address, pad field make sure smallest packet 64bytes (including header). InfiniBand includes version field protocol version, sequence number allow in- order delivery, field select destination queue, partition key field. Infiniband many small fields shown many packet formats; simplified view. ATM ’s short, fixed packet good match real-time demand digital voice.F.8 Examples Interconnection Networks ■F-79InfiniBand offers two basic mechanisms support user-level communica- tion: send/receive remote DMA (RDM A). send/receive, receiver explicitly post recei buffer (i.e., allocate space channel adapter network interface) sender transmit data. RDMA, sendercan remotely DMA data directly receiver device ’s memory. exam- ple, nominal packet size 4 byt es measured Mellanox MHEA28-XT channel adapter connected 3.4 GHz Intel Xeon host device, sending andreceiving overhead 0.946 1.423 μs, respectively, send/receive mechanism, whereas 0.910 0.323 μs, respectively, RDMA mechanism. discussed Section F.2 , packet size important getting full benefit network bandwidth. One might ask, “What natural size messages? ” Figure F.32(a) shows size messages commercial fluid dynamics sim- ulation application, called Fluent, collected InfiniBand network OhioState University ’s Network-Based Computer Laboratory. One plot cumulative messages sent cumulative data bytes sent. Messages graph message passing interface (MPI) units information, gets divided InfiniBand maximum transfer units (packets) transferred network. Asshown, maximum message size 512 KB, approximately 90% ofthe messages less 512 bytes. Messages 2 KB represent approximately50% bytes transferred. Integer Sort application kernel NAS ParallelNetwork name[vendors]Used top 10 supercomputerclusters (2005)Number nodesBasic networktopologyRaw link bidirectionalBWRouting algorithmArbitration techniqueSwitching technique;flow control InfiniBand [Mellanox, Voltair]SGI Altrix Dell Poweredge Thunderbird>Millions (2 128 GUIDaddresses,like IPv6)Completely configurable (arbitrary)4–240 Gbps Arbitrary (table- driven), typicallyup*/down*Weighted RR fair scheduling (2-levelpriority)Cut-through, 16 virtual channels (15 data);credit-based Myrinet- 2000 [Myricom]Barcelona Supercomputer Center Spain8192 nodesBidirectional MIN 16- port bidirectionalswitches(Clos net.)4 Gbps Source- based dispersive (adaptive)minimalroutingRound- robin arbitrationCut-through switching virtual channels; Xon/Xoff flowcontrol QsNet II [Quadrics]Intel Thunder Itanium2 Tiger4>Tens thousandsFat tree 8-port bidirectionalswitches21.3 Gbps Source- based LCA adaptiveshortest-path routing2-phased RR, priority, aging,distributedat output portsWormhole 2 virtual channels;credit-based Figure F.31 Characteristics system area networks implemented various top 10 supercomputer clusters 2005.F-80 ■Appendix F Interconnection NetworksBenchmark suite also measured 75% messages 512 bytes (plots shown). Many applications send far small messages thanlarge ones, particularly since requests acknowledgments frequentthan data responses block writes. InfiniBand reduces protocol processi ng overhead allowing off- loaded host computer controller InfiniBand network inter- face card. benefits protocol offloading bypassing operating system shown Figure F.32(b) MVAPICH, widely used implemen- tation MPI InfiniBand. Effective bandwidth plotted messagesize MVAPICH configured two modes two network speeds. Onemode runs IPoIB, InfiniBan communication handled IP layer implemented host ’s operating system (i.e., OS bypass). mode runs MVAPICH directly VAPI, native MellanoxInfiniBand interface offloads tran sport protocol processing channel adapter hardware (i.e., OS bypass). R esults shown 10 Gbps single data rate (SDR) 20 Gbps double data rate (DDR) InfiniBand networks. Theresults clearly show offloading protocol processing bypassingthe OS significantly reduce sending receiving overhead allow nearwire-speed effective bandwidth achieved.Percentage100% 90%80%70%60%50%40%30%20%10% 640% Message size (bytes)256K 64K 16K 4K 1K 256 Measured effective bandwidth (MB/sec)1600 140012001000 800600400200 40 Message size (bytes) (A) (B)4M 256K 16K 1K 64Number messages Data volumeMVAPICH native DDRMVAPICH native SDRMVAPICH 1PoIB SDRMVAPICH 1PoIB DDR Figure F.32 Data collected D.K. Panda, S. Sur, L. Chai (2005) Network-Based Computing Laboratory Ohio State University. (a) Cumulative percentage messages volume data transferred message size varies Fluent application ( www.fluent.com ). x-axis entry includes bytes next one; example, 128 represents 1 byte 128 bytes. 90% messages less 512 bytes, represents 40%of total bytes transferred. (b) Effective bandwidth versus message size measured SDR DDR InfiniBandnetworks running MVAPICH ( http://nowlab.cse.ohio-state.edu/projects/mpi-iba )with OS bypass (native) without (IPoIB).F.8 Examples Interconnection Networks ■F-81Ethernet: TheLocal Area Network Ethernet extraordinarily successful LAN —from 10 Mbit/sec stan- dard proposed 1978 used practically everywhere today recent 10Gbit/sec standard likely widely used. Many classes computersinclude Ethernet standard communication interface. Ethernet, codified IEEEstandard 802.3, packet-switched network routes packets using desti-nation address. originally designed coaxial cable today uses primarilyCat5E copper wire, optical fiber reserved longer distances higherbandwidths. even wireless version (802.11), testimony ubiquity. 20-year span, computers became thousands times faster 1978, shared media Ethernet network remained same. Hence,engineers invent temporary solutions faster, higher-bandwidth net-work became available. One solution use multiple Ethernets interconnectmachines connect Ethernets internetworking devices couldtransfer traffic one Ethernet another, needed. devices allow indi-vidual Ethernets operate parallel, thereby increasing aggregate intercon- nection bandwidth collection computers. effect, devices provide similar functionality switches described previously point-to-pointnetworks. Figure F.33 shows potential parallelism gained. Depending pass traffic kinds interconnections join together,these devices different names: Single Ethernet: one packet time Multiple Ethernets: multiple packets time Node Node Node Node Node Node NodeNode Node Node Node Bridge BridgeNode Node Node Node Node Node NodeNode Node Node Node Figure F.33 potential increased bandwidth using many Ethernets bridges.F-82 ■Appendix F Interconnection Networks■Bridges —These devices connect LANs together, passing traffic one side another depending addresses packet. Bridges operate Ethernet protocol level usually simpler cheaper routers, dis-cussed next. Using notation OSI model described next section(seeFigure F.36 page F-85), bridges operate layer 2, data link layer. ■Routers gateways —These devices connect LANs WANs, WANs WANs, resolve incompatible addressing. Generally slower bridges,they operate OSI layer 3, network layer. WAN routers divide networkinto separate smaller subnets, simplifies manageability improvessecurity. final internetworking devices hubs, merely extend multiple seg- ments single LAN. Thus, hubs help performance, one message transmit time. Hubs operate OSI layer 1, called physical UCB1. BARRNet.net 192.31.161.4mojave. Stanford.edu 36.22.0.120CIS-Gateway. Stanford.edu 36.1.0.22SU-CM. BARRNet.net 131.119.5.3 Ethernet FDDI T1 lineT3 line inr-108-eecs. Berkeley.edu 128.32.120.108 128.32.120.111 inr-111-cs2. Berkeley.edu 128.32.149.13 mammoth. Berkeley.edu 128.32.149.78 FDDIFDDI Ethernet EthernetInternet fd-0.enss128.t3. ans.net 192.31.48.244 Stanford, California Berkeley, California Figure F.34 connection established mojave.stanford.edu mammoth.berkeley.edu (1995). FDDI 100 Mbit/sec LAN, T1 line 1.5 Mbit/sec telecommunications line T3 45 Mbit/sec telecom- munications line. BARRNet stands Bay Area Research Network. Note inr-111-cs2.Berkeley.edu router two Internet addresses, one port.F.8 Examples Interconnection Networks ■F-83layer. Since devices planned part Ethernet standard, ad hoc nature added difficulty cost maintaining LANs. 2011, Ethernet link speeds available 10, 100, 10,000, 100,000 Mbits/sec. Although 10 100 Mbits/sec Ethernets share media multipledevices, 1000 Mbits/sec Ethernets rely point-to-point links andswitches. Ethernet switches normally use form store-and-forward. Ethernet real flow control, dating back first instantiation. orig- inally used carrier sensing exponential back-off (see page F-23) arbitrate forthe shared media. switches try use interface retrofit version flow control, flow control part Ethernet standard. Wide Area Network: ATM Asynchronous Transfer Mode (ATM) wide area networking standard set telecommunications industry. Although flirted competition Ethernet aLAN 1990s, ATM since retreated WAN stronghold.Applications NetworksInternetworking Figure F.35 role internetworking. width indicates relative number items level. Layer numberLayer name Main functionExample protocolNetwork component 7 Application Used applications specifically written run networkFTP, DNS, NFS, httpGateway, smart switch 6 Presentation Translates application network format, vice versaGateway 5 Session Establishes, maintains, ends sessions across networkNamed pipes, RPCGateway 4 Transport Additional connection session layer TCP Gateway 3 Network Translates logical network address names physical address (e.g., computer name MAC address)IP Router, ATM switch 2 Data Link Turns packets raw bits receiving end turns bits packetsEthernet Bridge, network interface card 1 Physical Transmits raw bit stream physical cable IEEE 802 Hub Figure F.36 OSI model layers. Based www.geocities.com/SiliconValley/Monitor/3131/ne/osimodel.html .F-84 ■Appendix F Interconnection NetworksThe telecommunications standard scalable bandwidth built in. starts 155 Mbits/sec scales factors 4 620 Mbits/sec, 2480 Mbits/sec, on. Since WAN, ATM ’s medium fiber, single mode multimode. Although switched medium, unlike examples relies virtual con-nections communication. ATM uses virtual channels routing multiplex dif-ferent connections single network segment, thereby avoiding inefficienciesof conventional connection-based networking. WAN focus also led store-and-forward switching. Unlike protocols, Figure F.30 shows ATM small, fixed-sized packet 48 bytes payload. uses credit-based flow con- trol scheme opposed IP routers implement flow control. reason virtual connections small packets quality service. Since telecommunications industry concerned voice traffic, predictabilitymatters well bandwidth. Establishing virtual connection less variabilitythan connectionless networking, simplifies store-and-forward switching.The small, fixed packet also makes simpler fast routers switches.Toward goal, ATM even offers protocol stack compete withTCP/IP. Surprisingly, even though switches simple, ATM suite pro- tocols large complex. dream seamless infrastructure LAN WAN, avoiding hodgepodge routers common today. dream fadedfrom inspiration nostalgia. F.9 Internetworking Undoubtedly one important innovations communications com-munity internetworking. allows computers independent incom-patible networks communicate reliably efficiently. Figure F.34 illustrates need traverse networks. shows networks machinesinvolved transferring file Stanford University University Cal- ifornia Berkeley, distance 75 km. low cost internetworking remarkable. example, vastly less expensive send electronic mail make coast-to-coast telephone calland leave message answering machine. dramatic cost improvementis achieved using long-haul communication lines telephone call,which makes improvement even impressive. enabling technologies internetworking software standards allow reliable communication without demanding reliable networks. underly- ing principle successful standards composed hierar- chy layers, layer taking responsibility portion overallcommunication task. computer, network, switch implements layerof standards, relying components faithfully fulfill respon-sibilities. layered software standards called protocol families protocolsuites. enable applications work interconnection without extrawork application programmer. Figure F.35 suggests hierarchical model communication.F.9 Internetworking ■F-85The popular internetworking standard TCP/IP (Transmission Control Protocol/Internet Protocol). protocol family basis humbly named Internet, connects hundreds millions computers around world.This popularity means TCP/IP used even communicating locally acrosscompatible networks; example, network file system (NFS) uses IP eventhough likely communicating across homogenous LAN suchas Ethernet. use TCP/IP protocol family example; protocolfamilies follow similar lines. Section F.13 gives history TCP/IP. goal family protocols simplify standard dividing respon- sibilities hierarchically among layers, layer offering services needed layer above. application program top, bottom phys-ical communication medium, sends bits. abstract data types sim-plify programmer ’s task shielding programmer details implementation data type, layered strategy makes standard easierto understand. many efforts network protocols, led confusion terms. Hence, Open Systems Interconnect (OSI) developed model popularized describing networks series layers. Figure F.36 shows model. Although protocols exactly follow layering, nomenclature differentlayers widely used. Thus, hear discussions simple layer 3 switchversus layer 7 smart switch. key protocol families communication occurs logically level protocol sender receiver, services lower levelimplement it. style communication called peer-to-peer . analogy, imagine General needs send message General B battlefield. General writes message, puts envelope addressed General B, gives colonel orders deliver it. colonel puts envelope, andwrites name corresponding colonel reports General B, gives itto major instructions delivery. major thing gives itto captain, gives lieutenant, gives sergeant. sergeanttakes envelope lieutenant, puts envelope name asergeant General B ’s division, finds private orders take large envelope. private borrows motorcycle delivers envelope sergeant. arrives, passed chain command, person removing outer envelope name passing innerenvelope superior. far General B tell, note another gen-eral. Neither general knows involved transmitting envelope, norhow transported one division other. Protocol families follow analogy closely might think, Figure F.37 shows. original message includes header possibly trailer sent lower-level protocol. next-lower protocol turn adds header message, possibly breaking smaller messages large layer. Reusing analogy, long message general isdivided placed several envelopes could fit one. divisionof message appending headers trailers continues messageF-86 ■Appendix F Interconnection Networksdescends physical transmission medium. message sent des- tination. level protocol family receiving end check mes- sage level peel headers trailers, passing next higherlevel putting pieces back together. nesting protocol layers spe-cific message called protocol stack , reflecting last in, first nature addition removal headers trailers. analogy, danger layered approach considerable latency added message delivery. Clearly, one way reduce latency reducethe number layers, keep mind protocol families define standard force implement standard. many ways imple- ment instruction set architecture, many ways implement protocolfamily. protocol stack example TCP/IP. Let ’s assume bottom protocol layer Ethernet. next level Internet Protocol IP layer; officialterm IP packet datagram. IP layer routes datagram des-tination machine, may involve many intermediate machines switches. IPmakes best effort deliver packets guarantee delivery, content, order datagrams. TCP layer IP makes guarantee reliable, in- order delivery prevents corruption datagrams. Following example Figure F.37 , assume application program wants send message machine via Ethernet. starts TCP. largest numberof bytes sent 64 KB. Since data may much larger than64 KB, TCP must divide smaller segments reassemble inproper order upon arrival. TCP adds 20-byte header ( Figure F.38 ) every data- gram passes IP. IP layer physical layer adds 20-byte header, also shown Figure F.38 . data sent IP levelTMessage HT HHT HHT HHT HHT HHT THT HTMessage HT HT HTlautcA lautcA Actual ActualLogicalLogical Actual Figure F.37 generic protocol stack two layers. Note communication peer-to-peer, headers trailers peer added sending layer andremoved receiving layer. layer offers services one shield unnecessary details.F.9 Internetworking ■F-87IP header IP data TCP dataIdentifier Fragment Header checksum Source Source Sequence number (length)Destination DestinationLength Type Time ProtocolVL TCP header Urgent pointerWindow TCP data 32 bitsPiggyback acknowledgment Flags ChecksumL (0–65,516 bytes) Figure F.38 headers IP TCP. drawing 32 bits wide. standard headers 20 bytes, allow headers optionally lengthen rarely transmitted information. headers length header field (L) accommodate optional fields, well source destination fields. length field thewhole datagram separate length field IP, TCP combines length datagram sequence number datagram giving sequence number bytes. TCP uses checksum field sure datagram corrupted, sequence number field sure datagrams assembled properorder arrive. IP provides checksum error detection header, since TCP protected rest packet. One optimization TCP send sequence datagrams waiting permission send more. number datagrams sent without waiting approval called window , window field tells many bytes may sent beyond byte acknowledged datagram. TCP adjust size window depending success IP layer sending datagrams; reliable faster is, larger TCP makes window. Since window slides forward data arrive acknowledged, thistechnique called sliding window protocol. piggyback acknowledgment field TCP another optimization. Since applications send data back forth connection, seems wasteful send datagram containing acknowledgment. piggyback field allows datagram carrying data also carry acknowl- edgment previous transmission, “piggybacking ”on top data transmission. urgent pointer field TCP gives address within datagram important byte, break character. pointer allows appli- cation software skip data user ’t wait prior data processed seeing character tells software stop. identifier field fragment field IP allow intermediary machines tobreak original datagram many smaller datagrams. unique identifier associated original datagram placed every fragment, fragment field saying piece which. time-to-live field allows datagram killed going maximum number intermediate switches matter isin network. Knowing maximum number hops take datagram arrive —if ever arrives —simplifies protocol software. protocol field identifies possible upper layer protocol sent IP datagram; case, TCP. V (for version) type fields allow different versions IP protocol software network. Explicit version numbering included software upgraded gracefully machineby machine, without shutting entire network. Nowadays, version six Internet protocol (IPv6) widely used.to Ethernet sent packets format shown Figure F.30 . Note TCP packet appears inside data portion IP datagram, Figure F.37 suggests. F.10 Crosscutting Issues Interconnection Networks section describes five topics discussed chapters fundamen- tally impacted interconnection networks, vice versa . Density-Optimized Processors versus SPEC-Optimized Processors Given people world accessing Web sites, ’t really mat- ter servers located. Hence, many servers kept collocation sites , charge network bandwidth reserved used space occupiedand power consumed. Desktop microprocessors past designedto fast possible whatever heat could dissipated, little regardfor size package surrounding chips. fact, desktop micropro-cessors Intel AMD recently 2006 burned much 130 watts!Floor space efficiency also largely ignored. result priorities, power major cost collocation sites, processor density limited power consumed dissipated, including within interconnect! proliferation portable computers (notebook sales exceeded desktop sales first time 2005) reduced power consumption coolingdemands, opportunity exists using technology create considerablydenser computation. instance, power consumption Intel PentiumM 2006 25 watts, yet delivered performance close desktopmicroprocessor wide set applications. therefore conceivable per- formance per watt performance per cubic foot could replace performance per microprocessor important figure merit. key many applicationsalready make use large clusters, possible replacing 64 power-hungryprocessors with, say, 256 power-efficient processors could cheaper yet soft-ware compatible. places greater importance power- performance-efficient interconnection network design. Google cluster prime exa mple migration many “cooler ” processors versus fewer “hotter ”processors. uses racks 80 Intel Pen- tium III 1 GHz processors instead power-hungry high-end processors. examples include blade servers consisting 1-inch-wide 7-inch-highrack unit blades designed based mobile processors. HP ProLiant BL10eG2 blade server supports 20 1-GHz ultra-low-voltage Intel Pentium processors 400-MHz front-side bus, 1-MB L2 cache, 1 GBmemory. Fujitsu Primergy BX300 bl ade server supports 20 1.4- 1.6-GHz Intel Pentium processors, 512 MB memory expandableto 4 GB.F.10 Crosscutting Issues Interconnection Networks ■F-89Smart Switches versus Smart Interface Cards Figure F.39 shows trade-off intelligence located within net- work. Generally, question whether either smarter network interfacesor smarter switches. Making one smarter generally makes simpler andless expensive. inexpensive interface, possible Ethernetto become standard part desktop server computers. Lower-costswitches made available people small configurations, needingsophisticated forwarding tables spanning-tree protocols larger Ethernetswitches. Myrinet followed opposite approach. switches dumb components that, implementing flow control arbitration, simply extract firstbyte packet header use directly select output port. routingtables implemented, intelligence network interface cards (NICs).The NICs responsible providing support efficient communication andfor implementing distributed protocol network (re)configuration. InfiniBandtakes hybrid approach offering lower-cost, less sophisticated interface cardscalled target channel adapters (or TCAs) less demanding devices disks —in hope included within I/O devices —and offer- ing expensive, powerful interface cards hosts called host channel adapters(or HCAs). switches implement routing tables. Switch Interface cardSmall-scale Ethernet switchLarge-scale Ethernet switch teniryM tenrehtEMyrinet InfiniBand InfiniBand target channel adapterInfiniBand host channel adapter intelli gence Figure F.39 Intelligence network: switch versus network interface card. Note Ethernet switches come two styles, depending size network, InfiniBand network interfaces come two styles, depending whether attached computer storage device. Myrinet proprietary system areanetwork.F-90 ■Appendix F Interconnection NetworksProtection User Access Network challenge ensure safe communication across network without invoking operating system common case. Cray Research T3D supercomputeroffers interesting case study. Like recent Cray X1E, T3D supports aglobal address space, loads stores access memory across network.Protection ensured access checked TLB. support trans-fer larger objects, block transfer engine (BLT) added hardware. Pro-tection access requires invoking operating system using BLT tocheck range accesses sure protection violations. Figure F.40 compares bandwidth delivered size object varies reads writes. large reads (e.g., 512 KB), BLT achieves thehighest performance: 140 MB/sec. simple loads get higher performance for8 KB less. write case, achieve peak 90 MB/sec, presumablybecause limitations memory bus. But, writes, BLT onlymatch performance simple stores transfers 2 MB; anything smallerand ’s faster send stores. Clearly, BLT avoid invoking operating system common case would useful. Efficient Interface Memory Hierarchy versus Network Traditional evaluations processor performance, SPECint SPECfp,encourage integration memory hierarchy processor efficiencyof memory hierarchy translates directly processor performance. Hence, 128 256 5121024 2048 4096 819216,384 32,768 65,536131,072 262,144 524,2881,048,576 2,097,152 4,194,304 8,388,608 Transfer size (bytes)020406080100120140160 CPU writeBLT read BLT write CPU readBandwidth (MB/sec) Figure F.40 Bandwidth versus transfer size simple memory access instructions versus block transfer device Cray Research T3D. (From Arpaci et al. [1995] .)F.10 Crosscutting Issues Interconnection Networks ■F-91microprocessors multiple levels caches chip along buffers writes. benchmarks SPECint SPECfp reward good interfaces interconnection networks, many machines make access time tothe network delayed full memory hierarchy. Writes must lumber theirway full write buffers, reads must go cycles first-,second-, often third-level cache misses reaching interconnectionnetwork. hierarchy results newer systems higher latencies theinterconnect older machines. Let’s compare three machines past: 40-MHz SPARCstation-2, 50- MHz SPARCstation-20 without external cache, 50-MHz SPARCstation- 20 external cache. According SPECint95, list order increas-ing performance. time access I/O bus (S-bus), however, increases thissequence: 200 ns, 500 ns, 1000 ns. SPARCstation-2 fastest ithas single bus memory I/O, one level cache. TheSPARCstation-20 memory access must first go memory bus (M-bus) andthen I/O bus, adding 300 ns. Machines second-level cache pay anextra penalty 500 ns accessing I/O bus. Compute-Optimized Processors versus Receiver Overhead overhead receive message likely involves interrupt, bears thecost flushing restarting processor pipeline, offloaded. Asmentioned earlier, reading network sta tus receiving data network interface likely operate cache miss speeds. microprocessors become superscalar go even faster clock rates, number missed instructionissue opportunities per message reception likely rise unacceptablelevels. F.11 Fallacies Pitfalls Myths hazards widespread interconnection networks. sectionmentions several warnings, proceed carefully. Fallacy interconnection network fast need improved interconnection network provides certain functionality system, much like memory I/O subsystems. designed allow proces- sors execute instructions maximum rate. interconnection network sub-system provide high enough bandwidth keep continuously enteringsaturation becoming overall system bottleneck. 1980s, wormhole switching introduced, became feasible design large-diameter topologies w ith single-chip switches band- width capacity network limiting factor. led flawed belief interconnection networks need improvement.F-92 ■Appendix F Interconnection NetworksSince 1980s, much attention placed improving processor per- formance, comparatively less focused interconnection net- works. technology advances, interconnection network tends torepresent increasing fraction system resources, cost, power consumption,and various attributes impact functionality performance. Scaling bandwidth simply overdimensioning certain network parameters nolonger cost-viable option. Designers must carefully consider end-to-end interconnection network design n concert processor, memory, I/O subsystems order achieve th e required cost, power, functionality, performance objectives entire system. obvious case point multicore processors on-chip networks. Fallacy Bisection bandwidth accurate cost constraint network Despite popular, bisection bandwidth never practical con- straint implementation interconnection network, although may beone future designs. useful performance measure costmeasure. Chip pin-outs realistic bandwidth constraint. Pitfall Using bandwidth (in particular, bisection bandwidth) measure network performance seldom case aggregate network bandwidth (likewise, network bisec- tion bandwidth) end-to-end bottlenecking point across network. Even itwere case, networks almost never 100% efficient transporting packetsacross bisection (i.e., ρ<100%) receiving network endpoints (i.e., σ<100%). former highly dependent upon routing, switching, arbitra- tion, factors former latter highly depen- dent upon traffic characteristics. Ignoring important factors andconcentrating raw bandwidth give misleading performance pre-dictions. example, perfectly conceivable network could higheraggregate bandwidth and/or bisection bandwidth relative another network butalso lower measured performance! Apparently, given sophisticated protocols like TCP/IP maximize delivered bandwidth, many network companies believe one figure merit networks. may true applications, video streaming, little interaction sender receiver. Many appli-cations, however, request-response nature, every large messagethere must one small messages. One example NFS. Figure F.41 compares shared 10-Mbit/sec Ethernet LAN switched 155- Mbit/sec ATM LAN NFS traffic. Ethernet drivers better tuned theATM drivers, 10-Mbit/sec Ethernet faster 155-Mbit/secATM payloads 512 bytes less. Figure F.41 shows overhead time, trans- mission time, total time send NFS messages Ethernet ATM. peak link speed ATM 15 times faster, measured link speed 8-KB messages almost 9 times faster. Yet, higher overheads offset benefitsso ATM would transmit NFS traffic 1.2 times faster.F.11 Fallacies Pitfalls ■F-93Pitfall providing sufficient reception link bandwidth, causes network end nodes become even bottleneck performance Unless traffic pattern permutati on, several packets concurrently arrive destinations ource devices inject traffic, thus pro- ducing contention. problem addressed, contention may turn congestion spread across network. dealt ana-lyzing traffic patterns providing extra reception bandwidth. example, itis possible implement reception bandwidth injection bandwidth.The IBM Blue Gene/L, example, impl ements on-chip switch 7-bitSizeNumber messagesOverhead (sec)Number data bytesTransmission (sec) Total time (sec) ATM Ethernet ATM Ethernet ATM Ethernet 32 771,060 532 389 33,817,052 4 48 536 436 64 56,923 39 29 4,101,088 0 5 40 3496 4,082,014 2817 2057 428,346,316 46 475 2863 2532 128 5,574,092 3846 2809 779,600,736 83 822 3929 3631160 328,439 227 166 54,860,484 6 56 232 222192 16,313 11 8 3,316,416 0 3 12 12224 4820 3 2 1,135,380 0 1 3 4256 24,766 17 12 9,150,720 1 9 18 21512 32,159 22 16 25,494,920 3 23 25 40 1024 69,834 48 35 70,578,564 8 72 56 1081536 8842 6 4 15,762,180 2 14 8 192048 9170 6 5 20,621,760 2 19 8 232560 20,206 14 10 56,319,740 6 51 20 613072 13,549 9 7 43,184,992 4 39 14 463584 4200 3 2 16,152,228 2 14 5 174096 67,808 47 34 285,606,596 29 255 76 2905120 6143 4 3 35,434,680 4 32 8 356144 5858 4 3 37,934,684 4 34 8 377168 4140 3 2 31,769,300 3 28 6 308192 287,577 198 145 2,390,688,480 245 2132 444 2277 Total 11,387,913 7858 5740 4,352,876,316 452 4132 8310 9872 Figure F.41 Total time 10-Mbit Ethernet 155-Mbit ATM, calculating total overhead transmis- sion time separately. Note size headers needs added data bytes calculate transmission time. higher overhead software driver ATM offsets higher bandwidth network. mea-surements performed 1994 using SPARCstation 10s, ForeSystems SBA-200 ATM interface card, Fore Systems ASX-200 switch. (NFS measurements taken Mike Dahlin University California –Berkeley.)F-94 ■Appendix F Interconnection Networksinjection 12-bit reception links, w reception BW equals aggre- gate switch input link BW. Pitfall Using high-performance network interface cards forgetting I/O sub- system sits network interface host processor issue related previous one. Messages usually composed user space buffers later sent calling send function communicationslibrary. Alternatively, cache controller implementing cache coherence protocolmay compose message SANs OCNs. cases, messages copied network interface memory transmission. I/O band- width lower link bandwidth introduces significant overhead, isgoing affect communication performance significantly. example, first10-Gigabit Ethernet cards market PCI-X bus interface systemwith significantly lower bandwidth 10 Gbps. Fallacy Zero-copy protocols require copying messages fragments one buffer another Traditional communication protocols computer networks allow access com- munication devices system calls supervisor mode. conse-quence this, communication routines need copy correspondingmessage user buffer kernel buffer sending message. Note thatthe communication protocol may need keep copy message retrans-mission case error, application may modify contents userbuffer system call returns control application. buffer-to-buffercopy eliminated zero-copy protocols communication routines executed user space protocols much simpler. However, messages still need copied application buffer memory network interface card (NIC) card hardwarecan transmit network. Although feasible eliminate copy allocating appli cation message buffers directly NIC memory (and, indeed, done protocols), may beconvenient current systems access NIC memory usuallyperformed I/O subsystem, usually much slower accessing main memory. Thus, gene rally efficient compose e g ei nm nm e r ya n dl e tD Ad e vices take care transfer NIC memory. Moreover, people count copy message frag- ments computed (usually ALU, results stored processor reg-ister) main memory. systolic-like architectures 1980s, like theiWarp, able directly transmit message fragments processor tothe network, effectively eliminating message copies. approach taken Cray X1E shared-memory multiprocessor supercomputer. Similar comments made regarding reception side; however, mean zero-copy protocols inefficient. protocols representthe efficient kind implementation used current systems.F.11 Fallacies Pitfalls ■F-95Pitfall Ignoring software overhead determining performance Low software overhead requires cooperation operating system well communication libraries, even protocol offloading con-tinues dominate hardware overhead must ignored.Figures F.32 andF.41 give two examples, one SAN standard WAN standard. exam ples come proprietary SANs supercomputers. Connection Mac hine CM-5 supercomputer early 1990s software overhead 20 μs send message hardware overhead 0.5 μs. first Intel Paragon supercomputer built early 1990s hardware overhead 0.2 μs, initial release software overhead 250 μs. Later releases reduced overhead w nt o2 5 μs and, recently, microseconds, still dominates hardware overhead. IBM Blue Gene/L MPI sending/receiving overhead approximately 3 μs, third (at most) attributed hardware. pitfall simply Amdahl ’s law applied networks: Faster network hardware superfluous corresponding decrease softwareoverhead. software overhead much reduced days OSbypass, lightweight protocols, pr otocol offloading micro- seconds less, typically, remain significant factor determining performance. Fallacy MINs cost-effective direct networks MIN usually implemented using significantly fewer switches number devices need connected. hand, direct networks usuallyinclude switch integral part node, thus requiring many switches asnodes interconnect. However, nothing prevents implementation nodeswith multiple computing devices (e.g., multicore processor on-chipswitch) several devices attached switch (i.e., bristling). cases, direct network may (or even more) cost-effective MIN. Note that, MIN, several network interfaces may required node matchthe bandwidth delivered multiple links per node provided directnetwork. Fallacy Low-dimensional direct networks achieve higher performance high-dimensional networks hypercubes conclusion drawn several studies analyzed optimal number dimensions main physical constraint bisection bandwidth. However, studies consider link pipelining, considered shortlinks, and/or consider switch architecture design constraints. misplacedassumption bisection bandwidth serves main limit help matters.Nowadays, researchers designers believe high-radix switches aremore cost-effective low-radix switches, including concluded theopposite before.F-96 ■Appendix F Interconnection NetworksFallacy Wormhole switching achieves better performance switching techniques Wormhole switching delivers no-load latency pipelined switch- ing techniques, like virtual cut-through switching. introduction wormholeswitches late 1980s coinciding dramatic increase network band- width led many believe wormhole switching main reason performance boost. Instead, performance increase came drasticincrease link bandwidth, which, turn, enabled ability wormholeswitching buffer packet fragments using on-chip buffers, instead using thenode ’s main memory off-chip source task. recently, much larger on-chip buffers become feasible, virtual cutthrough achievedthe no-load latency wormhole delivering much higher throughput.This mean wormhole switching dead. continues switch- ing technique choice applications small buffers used (e.g., perhaps on-chip networks). Fallacy Implementing virtual channels always increases throughput allowing packets pass blocked packets ahead general, implementing virtual channels wormhole switch good idea packets likely pass blocked packets ahead them,thus reducing latency significantly increasing throughput. However, improvements dramatic virtual cut-through switches. virtual cut-through, buffers large enough store several packets. con-sequence, virtual channel may introduce HOL blocking, possibly degrad-ing performance high loads. Adding virtual channels increases cost, may deliver little additional performance unless many virtual chan-nels switch ports packets mapped virtual channels according totheir destination (i.e., virtual output que ueing). certainly case vir- tual channels useful virtual cut -through networks segregate differ- ent traffic classes, b eneficial. However, multiplexing packets physical link flit-by-flit basis causes packets fromdifferent virtual channels get delay ed. average packet delay signifi- cantly shorter multiplexing takes pla ce packet-by-packet basis, case packet size bounded prevent one packet frommonopolizing majority link bandwidth. Fallacy Adaptive routing causes out-of-order packet delivery, thus introducing much overhead needed reorder packets destination device Adaptive routing allows packets follow alternative paths network depending network traffic; therefore, adaptive routing usually introducesoutof-order packet delivery. However, necessarily imply reorder-ing packets destination device going introduce large overhead, makingadaptive routing useful. example, efficient adaptive routing algo-rithms date support fully adaptive routing virtual channels requiredF.11 Fallacies Pitfalls ■F-97deterministic routing implemented virtual channels order prevent deadlocks ( àla IBM Blue Gene/L). case, easy select adaptive deterministic routing individual packet. single bitin packet header indicate switches whether virtual channelscan used implementing deterministic routing. hardware sup-port used indicated eliminate packet reordering overhead thedestination. communication protocols parallel computers clusters implement two different protocols depending message size. short messages, eager protocol used messages directly transmitted, receiving nodes use preallocated buffer temporarily store incoming message.On hand, long messages, rendezvous protocol used. case,a control message sent first, requesting destination node allocate bufferlarge enough store entire message. destination node confirms bufferallocation returning acknowledgment, sender proceed frag-menting message bounded-size packets, transmitting thedestination. eager messages use determinis tic routing, obvious introduce reordering overhead destination. hand,packets belonging long message transmitted using adaptive routing.As every packet contains sequence number within message (or off-set beginning message), destination node store everyincoming packet directly correct location within message buffer, thusincurring overhead respect using deterministic routing. onlything differs completion condition. Instead checking last packet message arrived, necessary count arrived packets, notifying end reception count equals message size.Taking account long messages, even frequent, usually consumemost network bandwidth, clear packets benefit fromadaptive routing without introducing r eordering overhead using pro- tocol described above. Fallacy Adaptive routing always improves network fault tolerance allows packets follow alternative paths Adaptive routing enough tolerate link and/or switch failures. mechanism required detect failures notify them, routinglogic could exclude faulty paths use remaining ones. Moreover, agiven link switch failure affects certain number paths using determin-istic routing, many source/destination pairs could affected samefailure using adaptive routing. consequence this, switchesimplementing adaptive routing transition deterministic routing presence failures. case, failures usually tolerated sending messages alternative paths source node. example, Cray T3E implementsdirection-order routing tolerate failures. fault-tolerant routingtechnique avoids cycles use resources crossing directions orderF-98 ■Appendix F Interconnection Networks(e.g., X+,Y+,Z+,Z/C0,Y/C0, X/C0). time, provides easy way send packets nonminimal paths, necessary, avoid crossing faulty com- ponents. instance, packet initially forwarded hops X+ direction even go X/C0direction point later. Pitfall Trying provide features within network versus end-to-end concern providing lower level features accomplished highest level, thus partially satisfying communicationdemand. Saltzer, Reed, Clark [1984] gave end-to-end argument follows: function question completely correctly specified knowledge help application standing endpoints thecommunication system. Therefore, providing questioned function fea-ture communication system possible. [page 278] example pitfall network MIT used several gateways, added checksum one gateway next. programmers theapplication assumed checksum guaranteed accuracy, incorrectly believingthat message protected stored memory gateway. Onegateway developed transient failure swapped one pair bytes per millionbytes transferred. time, source code one operating system repeat-edly passed gateway, thereby corrupting code. solutionwas correct infected source files comparing paper listings repair- ing code hand! checksums calculated checked appli- cation running end systems, safety would ensured. useful role intermediate checks link level, however, pro- vided end-to-end checking available. End-to-end checking may show thatsomething broken two nodes, ’t point problem is. Intermediate checks discover broken component. second issue regards performance using intermediate checks. Although sufficient retransmit whole case failures end point, much faster retransmit portion message intermediate point rather wait time-out full message retransmit end point. Pitfall Relying TCP/IP networks, regardless latency, bandwidth, software requirements network designers first workstations decided would elegant use single protocol stack matter destination message: Across aroom across ocean, TCP/IP overhead must paid. might wise decision back then, especially given unreliability early Ethernet hard- ware, sets high software overhead barrier commercial systems today.Such obstacle lowers enthusiasm low-latency network interface hard-ware low-latency interconnection networks software going towaste hundreds microseconds message must travel dozens ofmeters less. also use significant processor resources. One rough rule ofF.11 Fallacies Pitfalls ■F-99thumb Mbit/sec TCP/IP bandwidth needs 1 MHz processor speed, 1000-Mbit/sec link could saturate processor 800- 1000- MHz clock. flip side that, software perspective, TCP/IP desirable target since connected and, hence, provides largest number ofopportunities. downside using software optimized particular LAN orSAN limited. example, communication Java programdepends TCP/IP, optimization another protocol would require creationof glue software interface Java it. TCP/IP advocates point protocol theoretically bur- densome current implementations, progress modest commercialsystems. also TCP/IP offloading engines market, hope ofpreserving universal software model reducing processor utilization andmessage latency. processors continue improve much faster networkspeeds, multiple processors become ubiquitous, software TCP/IP may becomeless significant processor utilization message latency. F.12 Concluding Remarks Interconnection network design one exciting areas computer archi- tecture development today. advent new multicore processor paradigms advances traditional multiprocessor/cluster systems Internet, manychallenges opportunities exist interconnect architecture innovation. Theseapply levels computer systems: communication cores chip,between chips board, boards system, computers amachine room, local area across globe. Irrespective domainof application, interconnection networks transfer maximum amount ofinformation within least amount time given cost power constraints bottleneck system. Topology, routing, arbitration, switching, flow control among key concepts realizing high-performancedesigns. design interconnection networks end-to-end: includes injection links, reception links, inter faces network end points much topology, switches, links within network fabric. oftenthe case bandwidth overhead end node interfaces thebottleneck, yet many mistakenly think f interconnection network mean network fabric. bad processor designers thinking com- puter architecture mean instruction set architecture themicroarchitecture! End-to-end issues understanding traffic charac- teristics make design interconnection networks challenging verymuch relevant even today. instance, need low end-to-end latencyis driving developmen efficient network interfaces located closer processor/memory cont roller. may soon see mos multicore processors used multiprocessor systems implementing network interfaces on-chip,F-100 ■Appendix F Interconnection Networksdevoting core(s) execute communication tasks. already case IBM Blue Gene/L supercomputer, uses one two cores processor chip purpose. Networking long way go humble shared-media beginnings. “catch-up ”mode, switched-media point-to-point networks recently displacing traditional bus-based networks many networking domains, includingon chip, I/O, local area. near performance plateaus, weexpect rapid advancement WANs, LANs, SANs, especially OCNs thenear future. Greater interconnection network performance key information- communication-centric vision future field, which, far, benefited many millions people around world various ways.As quotes beginning appendix suggest, revolution two- waycommunication heart changes form human associa- tions actions. Acknowledgments express sincere thanks following persons who, way, havecontributed contents previous edition appendix: Lei Chai, ScottClark, Jos /C19e Flich, Jose Manuel Garcia, Paco Gilabert, Rama Govindaraju, Manish Gupta, Wai Hong Ho, Siao Jer, Steven Keckler, Dhabaleswar (D.K.) Panda, Fab- rizio Petrini, Steve Scott, Jeonghee Shin, Craig Stunkel, Sayantan Sur, Michael B.Taylor, Bilal Zafar. especially appreciate new contributions JoseFlich edition appendix. F.13 Historical Perspective References appendix taken perspective interconnection networks different domains —from on-chip networks within processor chip wide area networks connecting co mputers across globe —share many concerns. this, interconnection network concepts presented uni-fied way, irrespective applicati on; however, histories vastly different, evidenced differen solutions adopted address similar problems. lack significant intera ction research communities different domains certainly c ontributed diversity implemen- ted solutions. Highlighted relevant readings topic. addi- tion, good general texts featuring WAN LAN networking written Davie, Peterson, Clark [1999] Kurose Ross [2001] . Good texts focused SANs multiprocessors clusters written Duato, Yalamanchili, Ni [2003] Dally Towles [2004] . informative chapter devoted dead-lock resolution interconnection networks written Pinkston [2004] . Finally, edited work Jantsch Tenhunen [2003] OCNs multicore processors system-on-chips also interesting reading.F.13 Historical Perspective References ■F-101Wide Area Networks Wide area networks earliest data interconnection networks. fore- runner Internet ARPANET, 1969 connected computer sci-ence departments across United States research grants funded theAdvanced Research Project Agency (ARPA), U.S. government agency. wasoriginally envisioned using reliable communications lower levels. Practicalexperience failures underlying technology led failure-tolerantTCP/IP, basis Internet today. Vint Cerf Robert Kahnare credited developing TCP/IP protocols mid-1970s, winning ACM Software Award recognition achievement. Kahn [1972] early reference ideas ARPANET. interested learning moreabout TPC/IP, Stevens [1994 –1996] written classic books topic. 1975, roughly 100 networks ARPANET; 1983, 200. 1995, Internet encompassed 50,000 networks worldwide, halfof United States. number hard calculate now, butthe number IP hosts grew factor 15 1995 2000, reaching 100million Internet hosts end 2000. grown much faster since then. service providers assigning dynamic IP addresses, many local area networks using private IP addresses, networks allowing wireless connections,the total number hosts Internet nearly impossible compute. July2005, Internet Systems Consortium ( www.isc.org ) estimated 350 million Internet hosts, annual increase 25% projected. Althoughkey government networks made Internet possible (i.e., ARPANET andNSFNET), networks taken commercial sector, allow-ing Internet thrive. major innovations Internet still likely come government-sponsored research projects rather commer- cial sector. National Science Foundation ’s Global Environment Network Innovation (GENI) initiative example this. exciting application Internet World Wide Web, devel- oped 1989 Tim Berners-Lee, programmer European Center Par-ticle Research (CERN), information access. 1992, young programmer atthe University Illinois, Marc Andreessen, developed graphical interface theWeb called Mosaic. became immensely popular. later became founder Netscape, popularized commercial browsers. May 1995, time second edition book, 30,000 Web pages, numberwas doubling every two months. writing third edition text,there 1.3 billion Web pages. December 2005, number ofWeb servers approached 75 million, increased 30% thatsame year. Asynchronous Transfer Mode (ATM) attempt design definitive communication standard. provided good support data transmission well digital voice transmission (i.e., phone calls). technical point view, combined best packet switching circuit switching, also providingexcellent support providing quality service (QoS). Alles [1995] offers goodF-102 ■Appendix F Interconnection Networkssurvey ATM. 1995, one doubted ATM going future community. Ten years later, high equipment personnel training costs basically killed ATM, returned back simplicity TCP/IP. Anotherimportant blow ATM defeat Ethernet family LAN domain,where packet switching achieved significantly lower latencies ATM, whichrequired establishing connection data transmission. ATM connectionlessservers later introduced attempt fix problem, expen-sive represented central bottleneck LAN. Finally, WANs today rely optical fiber. Fiber technology made many advances today WAN fiber bandwidth often underutilized. main reason commercial introduction wavelength division multiplexing (WDM), allows fiber transmit many data streams simultaneously different wavele ngths, thus allowing three orders mag- nitude bandwidth increase one ge neration, is, 3 5 years (a good text Senior [1993] discusses optical fiber communications). However, IP routers may still become bottleneck. 10- 40-Gbps link rates, withthousands ports large core IP routers, packets must processed quickly —that is, within tens nanoseconds. time-consuming operation routing. way IP addresses defined assigned toInternet hosts makes routing complicated, usually requiring complexsearch tree structure every packet. Network processors becomepopular cost-effective solution implementing routing otherpacket-filtering operations. us ually RISC-like highly multi- threaded implement local stores instead caches. Local Area Networks ARPA ’s success wide area networks led directly popular local area networks. Many researchers Xerox Palo Alto Research Center fundedby ARPA working universities, knew value networking.In 1974, group invented Alto, forerunner today ’s desktop computers [Thacker et al. 1982 ], Ethernet [ Metcalfe Boggs 1976 ], today ’s LAN. group —David Boggs, Butler Lampson, Ed McCreight, Bob Sprowl, Chuck Thacker —became luminaries computer science engineering, collect- ing treasure chest awards among them. first Ethernet provided 3-Mbit/sec interconnection, seemed like unlimited amount communication bandwidth computers era. Itrelied interconnect technology developed cable television industry.Special microcode support gave round-trip time 50 μs Alto Ether- net, still respectable latency. Boggs ’experience ham radio operator led design need central arbiter, instead listenedbefore use varied back-off times case conflicts. announcement Digital Equipment Corporation, Intel, Xerox standard 10-Mbit/sec Ethernet critical commercial success ofF.13 Historical Perspective References ■F-103Ethernet. announcement short-circuited lengthy IEEE standards effort, eventually publish IEEE 802.3 standard Ethernet. several unsuccessful candidates tried replace Ethernet. Fiber Data Distribution Interconnect (FDDI) committee, unfortu-nately, took long time agree standard, resulting interfaceswere expensive. also shared medium switches becomingaffordable. ATM also missed opportunity part long timeto standardize LAN version ATM, part high latencyand poor behavior ATM connectionless servers, mentioned above. Infini- Band reasons discussed also faltered. result, Ethernet con- tinues absolute leader LAN environment, remains strongopponent high-performance computing market well, competing againstthe SANs delivering high bandwidth low cost. main drawback Ether-net high-end systems relatively high latency lack support mostinterface cards implement necessary protocols. failures past, LAN modernization efforts centered extending Ethernet lower-cost media unshielded twisted pair (UTP), switched interconnects, higher link speeds well new domains wireless communication. Practically new PC motherboards laptops imple-ment Fast/Gigabit Ethernet port (100/1000 Mbps), laptops implement a54 Mbps Wireless Ethernet connection. Also, home wired wireless LANs con-necting home appliances, set-top boxes, desktops, laptops sharedInternet connection common. Spurgeon [2006] provided nice online summary Ethernet technology, including history. System Area Networks One first nonblocking multistage interconnection networks proposed byClos [1953] use telephone exchange offices. Building this, many early inventions system area networks came use massively parallel pro-cessors (MPPs). One first MPPs Illiac IV, SIMD array built early 1970s 64 processing elements ( “massive ”at time) interconnected using topology based 2D torus provided neighbor-to-neighbor commu-nication. Another representative early MPP Cosmic Cube, usedEthernet interface chips connect 64 processors 6-cube. Communicationbetween nonneighboring nodes made possible store-and-forwarding ofpackets intermediate nodes toward final destination. much larger andtruly “massive ”MPP built mid-1980s Connection Machine, SIMD multiprocessor consisting 64 K 1-bit processing elements, also used hypercube store-and-forwarding. Since early MPP machines, intercon- nection networks improved considerably. 1970s 1990s, considerable research went trying opti- mize topology and, later, routing algorithm, switching, arbitration, flowcontrol techniques. Initially, research focused maximizing performance withF-104 ■Appendix F Interconnection Networkslittle attention paid implementation constraints crosscutting issues. Many exotic topologies proposed interesting properties, complicated routing. Rising fray hypercube, pop-ular network 1980s disappeared MPPs since 1990s.What contributed shift performance model Dally [1990] showed implementation wire limited, lower-dimensional topologiesachieve better performance higher-dimensional ones widerlinks given wire budget. Many designers followed trend assuming theirdesigns wire limited, even though implementations (and still are) pin limited. Several supercomputers since 1990s implemented low- dimensional topologies, including Intel Paragon, Cray T3D, Cray T3E, HPAlphaServer, Intel ASCI Red, IBM Blue Gene/L. Meanwhile, designers followed different approach, implementing bidirectional MINs order reduce number required switches thenumber network nodes. popular bidirectional MIN fat treetopology, originally proposed Leiserson [1985] first used Connection Machine CM-5 supercomputer and, later, IBM ASCI White ASC Purple supercomputers. indirect topology also used several European parallel computers based Transputer. Quadrics network inherited character-istics Transputer-based networks. Myrinet also evolvedsignificantly first version, Myrinet 2000 incorporating fat treeas principal topology. Indeed, current implementations SANs, includingMyrinet, InfiniBand, Quadrics well future implementations PCI-Express Advanced Switching, based fat trees. Although topology visible aspect network, features also significant impact performance. seminal work raised aware- ness deadlock properties computer systems published Holt [1972] . Early techniques avoiding deadlock store-and-forward networks pro-posed Merlin Schweitzer [1980] Gunther [1981] . Pipelined switch- ing techniques first introduced Kermani Kleinrock [1979] (virtual cut- through) improved upon Dally Seitz [1986] (wormhole), signif- icantly reduced low-load latency topology ’s impact message latency previously proposed techniques. Wormhole switching initially better virtual cut-through largely flow control could implemented granularity smaller packet, allowing high-bandwidth links notas constrained available switch memory bandwidth. Today, virtual cut-throughis usually preferred wormhole achieves higher throughput due toless HOL blocking effects enabled current integration technology thatallows implementation many packet buffers per link. Tamir Frazier [1992] laid groundwork virtual output queuing notion dynamically allocated multiqueues. Around time, Dally [1992] contributed concept virtual channels, key develop- ment efficient deadlock-free routing algorithms congestion-reducingflow control techniques improved network throughput. Another highly relevantcontribution routing new theory proposed Duato [1993] allowedF.13 Historical Perspective References ■F-105the implementation fully adaptive routing one “escape ”virtual channel avoid deadlock. Previous this, required number virtual channels avoid deadlock increased exponentially number network dimensions.Pinkston Warnakulasuriya [1997] went show deadlock actually occur infrequently, giving credence deadlock recovery routing approaches.Scott Goodman [1994] among first analyze usefulness pipe- lined channels making link bandwidth independent time flight. Theseand many innovations become quite popular, finding use high-performance interconnection networks, past present. IBM Blue Gene/L, example, implements virtual cut-through switching, four virtual chan- nels per link, fully adaptive routing one escape channel, pipelined links. MPPs represent small (and currently shrinking) fraction informa- tion technology market, giving way bladed servers clusters. UnitedStates, government programs Advanced Simulation Computing(ASC) program (formerly known Accelerated Strategic Computing Initia-tive, ASCI) promoted design machines, resulting seriesof increasingly powerful one-of-a-kind MPPs costing $50 million $100 million. days, many basically lower-cost clusters symmetric multiprocessors (SMPs) (see Pfister [1998] andSterling [2001] two perspectives clustering). fact, 2005, nearly 75% TOP500 supercomputers clusters. Nev-ertheless, design generation MPPs even clusters pushes inter-connection network research forward confront new problems arising due toshear size scaling factors. instance, source-based routing —the sim- plest form routing —does scale well large systems. Likewise, fat trees require increasingly longer links network size increases, led IBM Blue Gene/L designers adopt 3D torus network distributed routing implemented bounded-length links. Storage Area Networks System area networks originally designed single room single floor(thus distances tens hundreds meters) use MPPs clusters. intervening years, acronym SAN co-opted also mean storage area networks, whereby networking technology used connectstorage devices compute servers. Today, many refer “storage ”when say SAN. widely used SAN example 2006 Fibre Channel(FC), comes many varieties, including various versions Fibre ChannelArbitrated Loop (FC-AL) Fibre Channel Switched (FC-SW). diskarrays attached servers via FC links, even disks FC linksattached switches storage area networks enjoy benefits greater bandwidth interconnectivity switching. October 2000, InfiniBand Trade Association announced version 1.0 specification InfiniBand [ InfiniBand Trade Association 2001 ]. Led Intel, HP, IBM, Sun, companies, targeted high-performanceF-106 ■Appendix F Interconnection Networkscomputing market successor PCI bus point-to-point links switches set protocols. characteristics desirable potentially system area networks connect clusters storage area networks toconnect disk arrays servers. Consequently, strong competition fromboth fronts. storage area networking side, chief competition Infini-Band rapidly improving Ethernet technology widely used LANs.The Internet Engineering Task Force proposed standard called iSCSI sendSCSI commands IP networks [ Satran et al. 2001 ]. Given cost advantages higher-volume Ethernet switches interface cards, Gigabit Ethernet dom- inates low-end medium range market. ’s more, slow intro- duction InfiniBand small market share delayed development chipsets incorporating native support InfiniBand. Therefore, network interfacecards plugged PCI PCI-X bus, thus never delivering thepromise replacing PCI bus. another I/O standard, PCI-Express, finally replaced PCI bus. Like InfiniBand, PCI-Express implements switched network point-to-point serial links. credit, maintains software compatibility PCI bus, drastically simplifying migration new I/O interface. Moreover, PCI-Express benefited significantly mass market production foundapplication desktop market connecting one high-end graphicscards, making gamers happy. Every PC motherboard implements oneor 16x PCI-Express interfaces. PCI-Express absolutely dominates I/Ointerface, current standard provide support interprocessorcommunication. Yet another standard, Advanced Switching Interconnect (ASI), may emerge complementary technology PCI-Express. ASI compatible PCI-Express, thus linking directly current motherboards, also implements support forinterprocessor communication well I/O. defenders believe even-tually replace SANs LANs unified network data center mar-ket, ironically also said InfiniBand. interested reader referredtoPinkston et al. [2003] detailed discussion this. also new disk interface standard called Serial Advanced Technology Attachment (SATA) isreplacing parallel Integrated Device Electronics (IDE) serial signaling tech- nology allow increased bandwidth. disks market use new interface, keep mind Fibre Channel still alive well. Indeed, mostof promises made InfiniBand SAN market satisfied FibreChannel first, thus increasing share market. believe Ethernet, PCI-Express, SATA edge LAN, I/O interface, disk interface areas, respectively. fate theremaining storage area networking contenders depends many factors. won-derful characteristic computer architecture issues remain endless academic debates, unresolved people rehash arguments repeat- edly. Instead, battle fought marketplace, well-funded talentedgroups giving best efforts shaping future. Moreover, constant changesto technology reward either astute lucky. best combination ofF.13 Historical Perspective References ■F-107technology follow-through often determined commercial success. Time tell us win lose, least next round! On-Chip Networks Relative network domains, on-chip networks infancy. recently late 1990s, traditional way interconnecting devices ascaches, register files, ALUs, functional units within chip use dedicated links aimed minimizing latency shared buses aimed simplicity. subsequent increases volume interconnected devices singlechip, length delay wires cross chip, chip power consumption, ithas become important share on-chip interconnect bandwidth struc-tured way, giving rise notion network on-chip. Among first rec-ognize Agarwal [ Waingold et al. 1997 ] Dally [ Dally 1999; Dally Towles 2001 ]. others argued on-chip networks route packets allow efficient sharing burgeoning wire resources many communica- tion flows also facilitate modularity mitigate chip-crossing wire delay prob- lems identified Ho, Mai, Horowitz [2001] . Switched on-chip networks also viewed providing better fault isolation tolerance. Challenges indesigning networks later described Taylor et al. [2005] , also proposed 5-tuple model characterizing delay OCNs. design processfor OCNs provides complete synthesis flow proposed Bertozzi et al. [2005] . Following early works, much research development gone on-chip network design, making hot area microarchitecture activity. Multicore tiled designs featuring on-chip networks become pop- ular since turn millennium. Pinkston Shin [2005] provide survey on-chip networks used early multicore/tiled systems. designs exploit thereduced wiring complexity switched OCNs paths cores/tiles canbe precisely defined optimized early design process, thus enablingimproved power performance characteristics. typically tens thousandsof wires attached four edges core tile “pinouts, ”wire resources traded improved network performance wide channels data sent broadside (and possibly scaled accordingto power management technique), opposed serializing data fixednarrow channels. Rings, meshes, crossbars straightforward implement planar chip technology routing easily defined them, popular topolog-ical choices early switched OCNs. interesting see trend con-tinues future several tens hundreds heterogeneous cores tiles likely interconnected within single chip, possibly using 3D integration technology. Considering processor microarchitecture evolved signifi-cantly early beginnings response application demands technolog-ical advancements, would expect see vast architectural improvements on-chip networks well.F-108 ■Appendix F Interconnection NetworksReferences Agarwal, A., 1991. Limits interconnection network performance. IEEE Trans. Parallel Dis- tributed Systems 2 (4 (April)), 398 –412. Alles, A., 1995. “ATM internetworking ”(May). www.cisco.com/warp/public/614/12.html . Anderson, T.E., Culler, D.E., Patterson, D., 1995. case (networks workstations). IEEE Micro 15 (1 (February)), 54 –64. Anjan, K.V., Pinkston, T.M., 1995. efficient, fully-adaptive deadlock recovery scheme: Disha. In: Proc. 22nd Annual Int ’l. Symposium Computer Architecture, June 22 –24, 1995. Santa Mar- gherita Ligure, Italy. Arpaci, R.H., Culler, D.E., Krishnamurthy, A., Steinberg, S.G., Yelick, K., 1995. Empirical evaluation Cray-T3D: compiler perspective. In: Proc. 22nd Annual Int ’l. Symposium Computer Architecture, June 22 –24, 1995. Santa Margherita Ligure, Italy. Bell, G., Gray, J., 2001. Crays, Clusters Centers. Microsoft Corporation, Redmond, Wash. MSR- TR-2001-76. Benes, V.E., 1962. Rearrangeable three stage connecting networks. Bell Syst. Tech. J. 41, 1481 –1492. Bertozzi, D., Jalabert, A., Murali, S., Tamhankar, R., Stergiou, S., Benini, L., De Micheli, G., 2005. NoC synthesis flow customized domain specific multiprocessor systems-on-chip. IEEE Trans. Parallel Distributed Systems 16 (2 (February)), 113 –130. Bhuyan, L.N., Agrawal, D.P., 1984. Generalized hypercube hyperbus structures computer network. IEEE Trans. Computers 32 (4 (April)), 322 –333. Brewer, E.A., Kuszmaul, B.C., 1994. get good performance CM-5 data network. In: Proc. Eighth Int ’l Parallel Processing Symposium, April 26 –29, 1994. Cancun, Mexico. Clos, C., 1953. study non-blocking switching networks. Bell Systems Technical Journal 32 (March), 406 –424. Dally, W.J., 1990. Performance analysis k-ary n-cube interconnection networks. IEEE Trans. Computers 39 (6 (June)), 775 –785. Dally, W.J., 1992. Virtual channel flow control. IEEE Trans. Parallel Distributed Systems 3 (2 (March)), 194 –205. Dally, W.J., 1999. Interconnect limited VLSI architecture. In: Proc. Int ’l. Interconnect Technol- ogy Conference, May 24 –26, 1999. San Francisco, Calif. Dally, W.J., Seitz, C.I., 1986. torus routing chip. Distributed Computing 1 (4), 187 –196. Dally, W.J., Towles, B., 2001. Route packets, wires: On-chip interconnection networks. In: Proc. 38th Design Automation Conference, June 18 –22, 2001. Las Vegas, Nev. Dally, W.J., Towles, B., 2004. Principles Practices Interconnection Networks. Morgan Kauf- mann Publishers, San Francisco. Davie, B.S., Peterson, L.L., Clark, D., 1999. Computer Networks: Systems Approach, second ed. Morgan Kaufmann Publishers, San Francisco. Duato, J., 1993. new theory deadlock-free adaptive routing wormhole networks. IEEE Trans. Parallel Distributed Systems 4 (12 (December)), 1320 –1331. Duato, J., Pinkston, T.M., 2001. general theory deadlock-free adaptive routing using mixed set resources. IEEE Trans. Parallel Distributed Systems 12 (12 (December)), 1219 –1235. Duato, J., Yalamanchili, S., Ni, L., 2003. Interconnection Networks: Engineering Approach. Mor- gan Kaufmann Publishers, San Francisco. 2nd printing. Duato, J., Johnson, I., Flich, J., Naven, F., Garcia, P., Nachiondo, T., 2005a. new scalable cost- effective congestion management strategy lossless multistage interconnection networks.In: Proc. 11th Int ’l. Symposium High Performance Computer Architecture, February 12 –16, 2005 San Francisco. Duato, J., Lysne, O., Pang, R., Pinkston, T.M., 2005b. Part I: theory deadlock-free dynamic recon- figuration interconnection networks. IEEE Trans. Parallel Distributed Systems 16 (5 (May)), 412 –427. Flich, J., Bertozzi, D., 2010. Designing Network-on-Chip Architectures Nanoscale Era. CRC Press, Boca Raton, FL. Glass, C.J., Ni, L.M., 1992. Turn Model adaptive routing. In: Proc. 19th Int ’l. Symposium Computer Architecture. May, Gold Coast, Australia. Gunther, K.D., 1981. Prevention deadlocks packet-switched data transport systems. IEEE Trans. Communications, 512 –524. COM –29:4 (April). Ho, R., Mai, K.W., Horowitz, M.A., 2001. future wires. In: Proc. IEEE 89:4 (April), pp. 490 –504. Holt, R.C., 1972. deadlock properties computer systems. ACM Computer Surveys 4 (3 (September)), 179 –196.F.13 Historical Perspective References ■F-109Hoskote, Y., Vangal, S., Singh, A., Borkar, N., Borkar, S., 2007. 5-ghz mesh interconnect tera- flops processor. IEEE Micro 27 (5), 51 –61. Howard, J., Dighe, S., Hoskote, Y., Vangal, S., Finan, S., Ruhl, G., Jenkins, D., Wilson, H., Borka, N., Schrom, G., Pailet, F., Jain, S., Jacob, T., Yada, S., Marella, S., Salihundam, P., Erraguntla, V.,Konow, M., Riepen, M., Droege, G., Lindemann, J., Gries, M., Apel, T., Henriss, K., Lund-Larsen, T., Steibl, S., Borkar, S., De, V., Van Der Wijngaart, R., Mattson, T., 2010. 48-coreIA-32 message-passing processor DVFS 45 nm CMOS. In: IEEE International Solid-StateCircuits Conference Digest Technical Papers, pp. 58 –59. InfiniBand Trade Association, 2001. InfiniBand Architecture Specifications Release 1.0.a. www. infinibandta.org . Jantsch, A., Tenhunen, H. (Eds.), 2003. Networks Chips. Kluwer Academic Publishers, Netherlands. Kahn, R.E., 1972. Resource-sharing computer communication networks. In: Proc. IEEE 60:11 (Novem- ber), pp. 1397 –1407. Kermani, P., Kleinrock, L., 1979. Virtual cut-through: new computer communication switching tech- nique. Computer Networks 3 (January), 267 –286. Kurose, J.F., Ross, K.W., 2001. Computer Networking: Top-Down Approach Featuring Internet. Addison-Wesley, Boston. Leiserson, C.E., 1985. Fat trees: Universal networks hardware-efficient supercomputing. IEEE Trans. Computers, 892 –901. C –34:10 (October). Merlin, P.M., Schweitzer, P.J., 1980. Deadlock avoidance store-and-forward networks. I. Store-and- forward deadlock. IEEE Trans. Communications, 345 –354. COM –28:3 (March). Metcalfe, R.M., 1993. Computer/network interface design: Lessons Arpanet Ethernet. IEEE J. Selected Areas Communications 11 (2 (February)), 173 –180. Metcalfe, R.M., Boggs, D.R., 1976. Ethernet: Distributed packet switching local computer networks. Comm. ACM 19 (7 (July)), 395 –404. Partridge, C., 1994. Gigabit Networking. Addison-Wesley, Reading, Mass. Peh, L.S., Dally, W.J., 2001. delay model speculative architecture pipelined routers. In: Proc. 7th Int ’l. Symposium High Performance Computer Architecture, January 20 –24, 2001. Monter- rey, Mexico. Pfister, G.F., 1998. Search Clusters, second ed. Prentice Hall, Upper Saddle River, N.J.Pinkston, T.M., 2004. Deadlock characterization resolution interconnection networks. In: Zhu, M.C., Fanti, M.P. (Eds.), Deadlock Resolution Computer-Integrated Systems. CRCPress, Boca Raton, Fl, pp. 445 –492. Pinkston, T.M., Shin, J., 2005. Trends toward on-chip networked microsystems. Int ’l. J. High Per- formance Computing Networking 3 (1), 3 –18. Pinkston, T.M., Warnakulasuriya, S., 1997. deadlocks interconnection networks. In: Proc. 24th Int’l. Symposium Computer Architecture, June 2 –4, 1997. Denver, Colo. Pinkston, T.M., Benner, A., Krause, M., Robinson, I., Sterling, T., 2003. InfiniBand: ‘de facto ’ future standard system local area networks scalable replacement PCI buses? ” Special Issue Communication Architecture Clusters 6:2 (April). Cluster Computing, 95 –104. Puente, V., Beivide, R., Gregorio, J.A., Prellezo, J.M., Duato, J., Izu, C., 1999. Adaptive bubble router: design improve performance torus networks. In: Proc. 28th Int ’l. Conference Parallel Processing, September 21 –24, 1999. Aizu-Wakamatsu, Japan. Rodrigo, S., Flich, J., Duato, J., Hummel, M., 2008. Efficient unicast multicast support CMPs. In: Proc. 41st Annual IEEE/ACM International Symposium Microarchitecture (MICRO-41),November 8 –12, 2008. Lake Como, Italy, pp. 364 –375. Saltzer, J.H., Reed, D.P., Clark, D.D., 1984. End-to-end arguments system design. ACM Trans. Computer Systems 2 (4 (November)), 277 –288. Satran, J., Smith, D., Meth, K., Sapuntzakis, C., Wakeley, M., Von Stamwitz, P., Haagens, R., Zeidner, E., Dalle Ore, L., Klein, Y., 2001. “iSCSI ”, IPS working group IETF, Internet draft. www.ietf.org/internet-drafts/draft-ietf-ips-iscsi-07.txt . Scott, S.L., Goodman, J., 1994. impact pipelined channels k-ary n-cube networks. IEEE Trans. Parallel Distributed Systems 5 (1 (January)), 1 –16. Senior, J.M., 1993. Optical Fiber Commmunications: Principles Practice, second ed. Prentice Hall, Hertfordshire, U.K.. Spurgeon, C., 2006. Charles Spurgeon ’s Ethernet Web Site. www.etherman-age.com/ethernet/ethernet. html.F-110 ■Appendix F Interconnection NetworksSterling, T., 2001. Beowulf PC Cluster Computing Windows Beowulf PC Cluster Computing Linux. MIT Press, Cambridge, Mass. Stevens, W.R., 1994 –1996. TCP/IP Illustrated (three volumes). Addison-Wesley, Reading, Mass. Tamir, Y., Frazier, G., 1992. Dynamically-allocated multi-queue buffers VLSI communication switches. IEEE Trans. Computers 41 (6 (June)), 725 –734. Tanenbaum, A.S., 1988. Computer Networks, second ed. Prentice Hall, Englewood Cliffs, N.J.Taylor, M.B., Lee, W., Amarasinghe, S.P., Agarwal, A., 2005. Scalar operand networks. IEEE Trans. Parallel Distributed Systems 16 (2 (February)), 145 –162. Thacker, C.P., McCreight, E.M., Lampson, B.W., Sproull, R.F., Boggs, D.R., 1982. Alto: personal computer. In: Siewiorek, D.P., Bell, C.G., Newell, A. (Eds.), Computer Structures: Principles andExamples. McGraw-Hill, New York, pp. 549 –572. TILE-GX, http://www.tilera.com/sites/default/files/productbriefs/PB025_TILE-Gx_Processor_A_v3. pdf. Vaidya, A.S., Sivasubramaniam, A., Das, C.R., 1997. Performance benefits virtual channels adaptive routing: application-driven study. In: Proc. 11th ACM Int ’l Conference Supercom- puting, July 7 –11, 1997. Vienna, Austria. Van Leeuwen, J., Tan, R.B., 1987. Interval Routing. Computer Journal 30 (4), 298 –307. von Eicken, T., Culler, D.E., Goldstein, S.C., Schauser, K.E., 1992. Active messages: mechanism integrated communication computation. In: Proc. 19th Annual Int ’l. Symposium Computer Architecture, May 19 –21, 1992. Gold Coast, Australia. Waingold, E., Taylor, M., Srikrishna, D., Sarkar, V., Lee, W., Lee, V., Kim, J., Frank, M., Finch, P., Barua, R., Babb, J., Amarasinghe, S., Agarwal, A., 1997. Baring software: Raw Machines. IEEE Computer 30 (September), 86 –93. Yang, Y., Mason, G., 1991. Nonblocking broadcast switching networks. IEEE Trans. Computers 40 (9 (September)), 1005 –1015. Exercises Solutions “starred ”exercises available instructors register text- books.elsevier.com. ✪F.1 [15]<F.2, F.3 >Is electronic communication always faster nonelectronic means longer distances? Calculate time send 1000 GB using 25 8-mmtapes overnight delivery service versus sending 1000 GB FTP overthe Internet. Make following four assumptions: ■The tapes picked 4 P.M. Pacific time delivered 4200 km away 10A.M. Eastern time (7 A.M. Pacific time). ■On one route slowest link T3 line, transfers 45 Mbits/sec. ■On another route slowest link 100-Mbit/sec Ethernet. ■You use 50% slowest link two sites. bytes sent either Internet route arrive overnight deliveryperson arrives? ✪F.2 [10]<F.2, F.3 >For assumptions Exercise F.1, bandwidth overnight delivery 1000-GB package? ✪F.3 [10]<F.2, F.3 >For assumptions Exercise F.1, minimum bandwidth slowest link beat overnight delivery? standard networkoptions match speed?Exercises ■F-111✪F.4 [15]<F.2, F.3 >The original Ethernet standard 10 Mbits/sec max- imum distance 2.5 km. many bytes could flight original Ether- net? Assume use 90% peak bandwidth. ✪F.5 [15]<F.2, F.3 >Flow control problem WANs due long time flight, example page F-14 illus trates. Ethernet include flow control first standardized 10 Mbits/sec. Calculate number bytes flight 10-Gbit/sec Ethernet 100 meter link, assuming youcan use 90% peak bandwidth. answer mean networkdesigners? ✪F.6 [15]<F.2, F.3 >Assume total overhead send zero-length data packet Ethernet 100 μs unloaded network transmit 90% peak 1000-Mbit/sec rating. purposes question, assume size theEthernet header trailer 56 bytes. Assume continuous stream packets ofthe size. Plot delivered bandwidth user data Mbits/sec payload data size varies 32 bytes maximum size 1500 bytes 32-byte increments. ✪F.7 [10]<F.2, F.3 >Exercise F.6 suggests delivered Ethernet bandwidth single user may disappointing. Making assumptions exercise,by much would maximum payload size increased deliver halfof peak bandwidth? ✪F.8 [10]<F.2, F.3 >One reason ATM fixed transfer size short message behind long message, node may need wait entiretransfer complete. applications time sensitive, whentransmitting voice video, large ransfer size may result transmission delays long application. unloaded interconnection,what worstcase delay microseconds node must wait one full-size Ethernet packet versus ATM transfer? See Figure F.30 (page F- 78) find packet sizes. question assume transmitat 100% 622-Mbits/sec ATM network 100% 1000-Mbit/sec Ethernet. ✪F.9 [10]<F.2, F.3 >Exercise F.7 suggests need expanding maximum pay-load increase delivered ba ndwidth, Exercise F.8 suggests impact worst-case latency maki ng longer. would impact latency increasing maximum payload size answer ExerciseF.7? ✪F.10 [12/12/20] <F.4>The Omega network shown Figure F.11 page F-31 con- sists three columns four switches, two inputs two outputs. Eachswitch set straight , connects upper switch input upper switch output lower input lower output, exchange, con- nects upper input lower output vice versa lower input. column switches, label inputs outputs 0, 1, …, 7 top bottom, correspond numbering processors.F-112 ■Appendix F Interconnection Networksa.[12]<F.4>When switch set exchange message passes through, relationship label values switch input output used message? ( Hint: Think terms operations digits binary representation label number.) b.[12]<F.4>Between two switches adjacent columns connected link, relationship label output connected tothe input? c.[20]<F.4>Based results parts (a) (b), design describe simple routing scheme distributed control Omega network. messagewill carry routing tag computed sending processor. Describe processor computes tag switch set examining bitof routing tag. ✪F.11 [12/12/12/12/12/12] <F.4>Prove whether possible realize fol- lowing permutations (i.e., communication patterns) eight-node Omega net-work shown Figure F.11 page F-31: a.[12]<F.4>Bit-reversal permutation —the node binary coordinates n/C01, an/C02,…,a1,a0communicates node a0,a1,…,an/C02,an/C01. b.[12]<F.4>Perfect shuffle permutation —the node binary coordinates an/C01,an/C02,…,a1,a0communicates node an/C02,an/C03,…,a0,an/C01 (i.e., rotate left 1 bit). c.[12]<F.4>Bit-complement permutation —the node binary coordinates an/C01,an/C02,…,a1,a0communicates node an/C01,an/C02,…,a1,a0 (i.e., complement bit). d.[12]<F.4>Butterfly permutation —the node binary coordinates an/C01, an/C02,…,a1,a0communicates node a0,an/C02,…,a1,an/C01(i.e., swap least significant bits). e.[12]<F.4>Matrix transpose permutation —the node binary coordinates an/C01,an/C02,…,a1,a0communicates node an/2/C01,…,a0,an/C01,…, an/2(i.e., transpose bits positions approximately halfway around). f.[12]<F.4>Barrel-shift permutation —node icommunicates node i+1 modulo N/C01, Nis total number nodes 0 /C20i. ✪F.12 [12]<F.4>Design network topology using 18-port crossbar switches minimum number switches connect 64 nodes. switch port supports communication one device. ✪F.13 [15]<F.4>Design network topology minimum latency switches 64 nodes using 18-port crossbar switches. Assume unit delay switches zero delay wires. ✪F.14 [15]<F.4>Design switch topology balances bandwidth required links 64 nodes using 18-port crossbar switches. Assume uniform trafficpattern.Exercises ■F-113✪F.15 [15]<F.4>Compare interconnection latency crossbar, Omega network, fat tree eight nodes. Use Figure F.11 page F-31, Figure F.12 page F- 33, Figure F.14 page F-37. Assume fat tree built entirely two-input, two-output switches hardware resources comparableto Omega network. Assume switch costs unit time delay.Assume fat tree randomly picks path, give best case worstcase example. long take send message node 0 tonode 6? long take node 1 node 7 communicate? ✪F.16 [15]<F.4>Draw topology 6-cube manner 4-cube Figure F.14 page F-37. maximum average number hops needed packets assuming uniform distribution packet destinations? ✪F.17 [15]<F.4>Complete table similar Figure F.15 page F-40 captures performance cost various network topologies, general caseofNnodes using k/C2kswitches instead specific case 64 nodes. ✪F.18 [20]<F.4>Repeat example given page F-41, use bit-complement communication pattern given Exercise F.11 instead NEWS communication. ✪F.19 [15]<F.5>Give four specific conditions necessary deadlock exist interconnection network. removed dimension-order routing? removed adaptive routing use “escape ”routing paths? removed adaptive routing technique dead-lock recovery (regressive progressive)? Explain answer. ✪F.20 [12/12/12/12] <F.5>Prove whether following routing algorithms based prohibiting dimensional turns suitable used escape paths 2Dmeshes analyzing whether connected deadlock-free. Explainyour answer. ( Hint: may wish refer Turn Model algorithm and/or prove answer drawing directed graph 4 /C24 mesh depicts depen- dencies channels verifying channel dependency graph free ofcycles.) routing algorithms expressed following abbreviations:W¼west, E ¼east, N ¼north, ¼south. a.[12]<F.5>Allowed turns W N, E N, W, E. b.[12]<F.5>Allowed turns W S, E S, N E, E. c.[12]< F.5>Allowed turns W S, E S, N W, E, W N, St oW . d.[12]<F.5>Allowed turns E, E S, W, N W, N E, E N. ✪F.21 [15]<F.5>Compute compare upper bound efficiency factor, ρ, dimension-order routing up*/down* routing assuming uniformly distributedtraffic 64-node 2D mesh network. up*/down* routing, assume optimalplacement root node (i.e., node near middle mesh). ( Hint: find loading links across network bisection carries global load determined routing algorithm.)F-114 ■Appendix F Interconnection Networks✪F.22 [15]<F.5>For assumptions Exercise F.21, find efficiency factor up*/down* routing 64-node fat tree network using 4 /C24 switches. Com- pare result ρfound up*/down* routing 2D mesh. Explain. ✪F.23 [15]<F.5>Calculate probability matching two-phased arbitration requests kinput ports switch simultaneously koutput ports assuming uniform distribution requests grants to/from output ports. thiscompare matching probability three-phased arbitration eachof kinput ports make two simultaneous requests (again, assuming uni- form random distribution requests grants)? ✪F.24 [15]<F.5>The equation page F-52 shows value cut-through switching. Ethernet switches used build clusters often support cut-through switching.Compare time transfer 1500 bytes 1000-Mbit/sec Ethernet andwithout cut-through switching 64-node cluster. Assume Ethernetswitch takes 1.0 μs message goes seven intermediate switches. ✪F.25 [15]<F.5>Making assumptions Exercise F.24, differ- ence cut-through store-and-forward switching 32 bytes? ✪F.26 [15]<F.5>One way reduce latency use larger switches. Unlike Exercise F.24, let ’s assume need three intermediate switches connect two nodes cluster. Make assumptions Exercise F.24 remain- ing parameters. difference cut-through store-and-forwardfor 1500 bytes? 32 bytes? ✪F.27 [20]<F.5>Using FlexSim 1.2 ( http://ceng.usc.edu/smart/FlexSim/flexsim.html ) cycle-accurate network simulator, simulate 256-node 2D torus net-work assuming wormhole routing, 32-flit packets, uniform (random) communica-tion pattern, four virtual channels. Compare performance deterministicrouting using DOR, adaptive routing using escape paths (i.e., Duato ’s Protocol), true fully adaptive routing using progressive deadlock recovery (i.e., Disharouting). plotting latency versus applied load through-put versusapplied load each, done Figure F.19 example page F-53. Also run simulations plot results two eight virtual channels each. Com- pare explain results addressing how/why number use virtualchannels various routing algorithms affect network performance. ( Hint:B e sure let simulation reach steady state allowing warm-up period sev-eral thousand network cycles gathering results.) ✪F.28 [20]<F.5>Repeat Exercise F.27 using bit-reversal communication instead uniform random communication pattern. Compare explain results byaddressing how/why communication pattern affects network performance. ✪F.29 [40]<F.5>Repeat Exercises F.27 F.28 using 16-flit packets 128-flit packets. Compare explain results addressing how/why packet sizealong design parameters affect network performance. F.30 [20]<F.2, F.4, F.5, F.8 >Figures F.7, F.16 , F.20 show interconnection network characteristics several top 500 supercomputers machine typeExercises ■F-115as publication fourth edition. Update figure recent top 500. systems networks changed since data orig- inal figure? similar comparisons OCNs used microprocessors SANstargeted clusters using Figures F.29 andF.31. ✪F.31 [12/12/12/15/15/18] <F.8>Use M/M/1 queuing model answer exer- cise. Measurements network bridge show packets arrive 200 packetsper second gateway forwards 2 ms. a.[12]<F.8>What utilization gateway? b.[12]<F.8>What mean number packets gateway? c.[12]<F.8>What mean time spent gateway? d.[15]<F.8>Plot response time versus utilization vary arrival rate. e.[15]<F.8>For M/M/1 queue, probability finding tasks system Utilization n. chance overflow FIFO hold 10 messages? f.[18]<F.8>How big must gateway packet loss due FIFO over- flow less one packet per million? ✪F.32 [20]<F.8>The imbalance time sending receiving cause problems network performance. Sending fast cause network backup increase latency messages, since receivers able pullout message fast enough. technique called bandwidth matching proposes simple solution: Slow sender matches performance receiver [ Brewer Kuszmaul 1994 ]. two machines exchange equal number messages using protocol like UDP, one get ahead other, causing tosend messages first. receiver puts messages away, willthen send messages. Estimate performance case versus bandwidth-matched case. Assume send overhead 200 μs, receive overhead 300μs, time flight 5 μs, latency 10 μs, two machines want exchange 100 messages. F.33 [40]<F.8>Compare performance UDP without bandwidth matching slowing UDP send code match receive code advised bandwidth matching [ Brewer Kuszmaul 1994 ]. Devise exper- iment see much performance changes result. changethe send rate two nodes send destination? one sendersends two destinations? ✪F.34 [40]<F.6, F.8 >If access SMP cluster, write program measure latency communication bandwidth communication pro-cessors, plotted Figure F.32 page F-80. F.35 [20/20/20] <F.9>If access UNIX system, use ping explore Internet. First read manual page. use ping without option flags sure reach following sites. say X alive . Depending system, may able see path setting flags verbose modeF-116 ■Appendix F Interconnection Networks(-v) trace route mode ( -R) see path machine example machine. Alternatively, may need use program trace route see path. so, try manual page. may want use UNIX commandscript make record session. a.[20]<F.9>Trace route another machine local area network. latency? b.[20]<F.9>Trace route another machine campus noton local area network.What latency? c.[20]<F.9>Trace route another machine campus . example, friend send email to, try tracing route. See discoverwhat types networks used along route.What latency? F.36 [15]<F.9>Use FTP transfer file remote site local sites LAN. difference bandwidth transfer? Trythe transfer different times day days week. WAN LAN thebottleneck? ✪F.37 [10/10] <F.9, F.11 >Figure F.41 page F-93 compares latencies high- bandwidth network high overhead low-bandwidth network lowoverhead different TCP/IP message sizes. a.[10]<F.9, F.11 >For message sizes delivered bandwidth higher high-bandwidth network? b.[10]<F.9, F.11 >For answer part (a), delivered bandwidth network? ✪F.38 [15]<F.9, F.11 >Using statistics Figure F.41 page F-93, estimate per-message overhead network. ✪F.39 [15]<F.9, F.11 >Exercise F.37 calculates message sizes faster two networks different overhead peak bandwidth. Using statistics inFigure F.41 page F-93, percentage messages transmitted quickly network low overhead bandwidth? per-centage data transmitted quickly network high overhead andbandwidth? ✪F.40 [15]<F.9, F.11 >One interesting measure latency bandwidth inter-connection calculate size message needed achieve one-half peak bandwidth. halfway point sometimes referred n 1/2, taken terminology vector processing. Using Figure F.41 page F-93, esti- mate n1/2for TCP/IP message using 155-Mbit/sec ATM 10-Mbit/sec Ethernet. F.41 [Discussion] <F.10>The Google cluster used constructed 1 rack unit (RU) PCs, one processor two disks. Today considerablydenser options. much less floor space would take replacethe 1 RU PCs modern alternatives? Go Compaq Dell Web sitesto find densest alternative. would estimated impact cost ofthe equipment? would estimated impact rental cost floor space?Exercises ■F-117What would impact interconnection network design achieving power/ performance efficiency? F.42 [Discussion] <F.13>At time writing fourth edition, unclear would happen Ethernet versus InfiniBand versus Advanced Switching machine room. technical advantages each? eco- nomic advantages each? would people maintaining system prefer oneto other? popular network today? compare toproprietary commercial networks Myrinet Quadrics?F-118 ■Appendix F Interconnection NetworksG.1 Introduction G-2 G.2 Vector Performance Depth G-2 G.3 Vector Memory Systems Depth G-9 G.4 Enhancing Vector Performance G-11 G.5 Effectiveness Compiler Vectorization G-14 G.6 Putting Together: Performance Vector Processors G-15 G.7 Modern Vector Supercomputer: Cray X1 G-21 G.8 Concluding Remarks G-25 G.9 Historical Perspective References G-26 Exercises G-29G Vector Processors Depth Revised Krste Asanovic Massachusetts Institute Technology I’m certainly inventing vector processors. three kinds know existing today. represented Illiac-IV, (CDC) Star processor, TI (ASC) processor. three pioneering processors. …One problems pioneer always make mistakes never, never want pioneer. ’s always best come second look mistakes pioneers made. Seymour Cray Public lecture Lawrence Livermore Laboratorieson introduction Cray-1 (1976)G.1 Introduction Chapter 4 introduces vector architectures places Multimedia SIMD extensions GPUs proper context vector architectures. appendix, go detail vector architectures, including accurate performance models descriptions previous vector architectures.Figure G.1 shows characteristics typical vector processors, including size count registers, number types functional units, number load-store units, number lanes. G.2 Vector Performance Depth chime approximation reasonably accurate long vectors. Another source overhead far significant issue limitation. important source overhead ignored chime model vector start-up time . start-up time comes pipelining latency vector operation principally determined deep pipeline func-tional unit used. start-up time increases effective time execute convoyto one chime. assumption convoys overlap intime, start-up time delays execution subsequent convoys. course, theinstructions successive convoys either structural conflicts somefunctional unit data dependent, assumption overlap reason-able. actual time complete convoy determined sum vector length start-up time. vector lengths infinite, start-up overhead would amortized, finite vector lengths expose it, following exampleshows. Example Assume start-up overhead functional units shown Figure G.2 . Show time convoy begin total number cycles needed. time compare chime approximation vector oflength 64? Answer Figure G.3 provides answer convoys, assuming vector length n. One tricky question assume vector sequence done; deter-mines whether start-up time SVis visible not. assume instructions following cannot fit convoy, already assumedthat convoys overlap. Thus, total time given time lastvector instruction last convoy completes. approximation, thestart-up time last vector instruction may seen sequences inothers. simplicity, always include it. time per result vector length 64 4+(42/64) ¼4.65 clock cycles, chime approximation would 4. execution time startupoverhead 1.16 times higher.G-2 ■Appendix G Vector Processors DepthProcessor (year)Vector clock rate (MHz)Vector registersElements per register (64-bit elements) Vector arithmetic unitsVector load-store units Lanes Cray-1 (1976) 80 8 64 6: FP add, FP multiply, FP reciprocal, integer add, logical, shift11 Cray X-MP (1983) 118 8 64 8: FP add, FP multiply, FP reciprocal, integer add, 2 logical, shift, population count/parity2 loads 1 store1 Cray Y-MP (1988) 166 Cray-2 (1985) 244 8 64 5: FP add, FP multiply, FP reciprocal/sqrt, integer add/shift/population count, logical11 Fujitsu VP100/ VP200 (1982)133 8 –256 32 –1024 3: FP integer add/logical, multiply, divide 2 1 (VP100) 2 (VP200) Hitachi S810/S820 (1983)71 32 256 4: FP multiply-add, FP multiply/divide-add unit, 2 integer add/logical3 loads 1 store1 (S810) 2 (S820) Convex C-1 (1985) 10 8 128 2: FP integer multiply/divide, add/logical 1 1 (64 bit) 2 (32 bit) NEC SX/2 (1985) 167 8+32 256 4: FP multiply/divide, FP add, integer add/ logical, shift14 Cray C90 (1991) 240 8 128 8: FP add, FP multiply, FP reciprocal, integer add, 2 logical, shift, population count/parity2 loads 1 store2 Cray T90 (1995) 460 NEC SX/5 (1998) 312 8+64 512 4: FP integer add/shift, multiply, divide, logical11 6 Fujitsu VPP5000 (1999)300 8 –256 128 –4096 3: FP integer multiply, add/logical, divide 1 load 1 store16 Cray SV1 (1998) 300 8 64 (MSP) 8: FP add, FP multiply, FP reciprocal, integer add, 2 logical, shift, population count/parity1 load-store 1 load2 8 (MSP) SV1ex (2001) 500 VMIPS (2001) 500 8 64 5: FP multiply, FP divide, FP add, integer add/ shift, logical1 load-store 1 NEC SX/6 (2001) 500 8+64 256 4: FP integer add/shift, multiply, divide, logical18 NEC SX/8 (2004) 2000 8+64 256 4: FP integer add/shift, multiply, divide, logical14 Cray X1 (2002) 800 32 64 256 (MSP)3: FP integer, add/logical, multiply/shift, divide/square root/logical1 load 1 store2 8 (MSP) Cray XIE (2005) 1130 Figure G.1 Characteristics several vector-register architectures. machine multiprocessor, entries correspond characteristics one processor. Several machines different clock rates vector scalar units; clock rates shown vector units. Fujitsu machines ’vector registers configurable: size count 8K 64-bit entries may varied inversely one another (e.g., VP200, eightregisters 1K elements long 256 registers 32 elements long). NEC machines eight foregroundvector registers connected arithmetic units plus 32 64 background vector registers connected memory system foreground vector registers. Add pipelines perform add subtract. multiply/divide- add unit Hitachi S810/820 performs FP multiply divide followed add subtract (while multiply-add unit performs multiply followed add subtract). Note processors use vector FP multiply divide units vector integer multiply divide, several processors use units FP scalar FP vector operations. vector load-store unit represents ability independent, overlapped transferto vector registers. number lanes number parallel pipelines functional units described Section G.4 . example, NEC SX/5 complete 16 multiplies per cycle multiply functional unit. Several machines split 64-bit lane two 32-bit lanes increase performance applications thatrequire reduced precision. Cray SV1 Cray X1 group four CPUs two lanes act unison single larger CPU eight lanes, Cray calls Multi-Streaming Processor (MSP).For simplicity, use chime approximation running time, incorporat- ing start-up time effects want performance detailed toillustrate benefits enhancement. long vectors, typical situation,the overhead effect large. Later appendix, explore ways toreduce start-up overhead. Start-up time instruction comes pipeline depth functional unit implementing instruction. initiation rate kept 1 clock cycleper result, Pipeline depth ¼Total functional unit time Clock cycle time/C20/C21 example, operation takes 10 clock cycles, must pipelined 10 deep achieve initiation rate one per clock cycle. Pipeline depth, then, deter- mined complexity operation clock cycle time proces-sor. pipeline depths functional units vary widely —2t o2 0s g e sa r e common —although heavily used units pipeline depths 4 8 clock cycles. VMIPS, use pipeline depths Cray-1, although laten- cies modern processors tended increase, especially loads. Allfunctional units fully pipelined. Chapter 4 , pipeline depths 6 clock cycles floating-point add 7 clock cycles floating-point multiply. VMIPS, vector processors, independent vector operations usingdifferent functional units issue convoy. addition start-up overhead, need account overhead executing strip-mined loop. strip-mining overhead, arises fromUnit Start-up overhead (cycles) Load store unit 12 Multiply unit 7Add unit 6 Figure G.2 Start-up overhead. Convoy Starting time First-result time Last-result time 1.LV 01 2 1 1 + n 2.MULVS.D LV 12+ n 12+ n+12 23+2 n 3.ADDV.D 24+2 n 24+2 n+6 29+3 n 4.SV 30+3 n 30+3 n+12 41+4 n Figure G.3 Starting times first- last-result times convoys 1 4. vector length n.G-4 ■Appendix G Vector Processors Depththe need reinitiate vector sequence set Vector Length Register (VLR) effectively adds vector start-up time, assuming convoy over-lap instructions. overhead convoy 10 cycles, theeffective overhead per 64 elements increases 10 cycles, 0.15 cycles perelement. Two key factors contribute running time strip-mined loop consisting sequence convoys: 1.The number convoys loop, determines number chimes. use notation chime execution time chimes. 2.The overhead strip-mined sequence convoys. overhead consists cost executing scalar code strip-mining block, loop, plus vector start-up cost convoy, start. may also fixed overhead associated setting vector sequence first time. recent vector processors, overhead become quite small, sowe ignore it. components used state total running time vector sequence operating vector length n, call n: Tn¼n MVLhi /C2Tloop+Tstart/C0/C1 +n/C2Tchime values start,Tloop, chime compiler processor dependent. register allocation scheduling instructions affect goes con-voy start-up overhead convoy. simplicity, use constant value loopon VMIPS. Based variety measurements Cray-1 vector execution, value chosen 15 forT loop. first glance, might think value small. overhead loop requires setting vector starting addresses strides, incre- menting counters, executing loop branch. practice, scalar instruc- tions totally partially overlapped vector instructions,minimizing time spent overhead functions. value loopof course depends loop structure, dependence slight compared withthe connection vector code values chime start.Operation Start-up penalty Vector add 6 Vector multiply 7Vector divide 20Vector load 12 Figure G.4 Start-up penalties VMIPS. start-up penalties clock cycles VMIPS vector operations.G.2 Vector Performance Depth ■G-5Example execution time VMIPS vector operation A¼B/C2s,where sis scalar length vectors AandBis 200? Answer Assume addresses B initially RaandRb,sis inFs, recall MIPS (and VMIPS) R0 always holds 0. Since (200 mod 64) ¼8, first iteration strip-mined loop execute vector length 8 elements, following iterations execute vector length 64 elements. starting byte addresses next segment vector eight times vectorlength. Since vector length either 8 64, increment address registersby 8/C28¼64 first segment 8 /C264¼512 later segments. total number bytes vector 8 /C2200¼1600, test completion comparing address next vector segment initial address plus1600. actual code: DADDUI R2,R0,#1600 ;total # bytes vector DADDU R2,R2,Ra ;address end vectorDADDUI R1,R0,#8 ;loads length 1st segmentMTC1 VLR,R1 ;load vector length VLR DADDUI R1,R0,#64 ;length bytes 1st segment DADDUI R3,R0,#64 ;vector length segments Loop: LV V1,Rb ;load B MULVS.D V2,V1,Fs ;vector * scalarSV Ra,V2 ;store ADADDU Ra,Ra,R1 ;address next segment ADADDU Rb,Rb,R1 ;address next segment BDADDUI R1,R0,#512 ;load byte offset next segment MTC1 VLR,R3 ;set length 64 elements DSUBU R4,R2,Ra ;at end A?BNEZ R4,Loop ;if not, go back three vector instructions loop dependent must go three convoys, hence chime¼3. Let ’s use basic formula: Tn¼n MVLhi /C2Tloop+Tstart/C0/C1 +n/C2Tchime T200¼4/C215 + start ðÞ + 200/C23 T200¼60 + 4 /C2TstartðÞ + 600¼660 + 4 /C2TstartðÞ value startis sum of: ■The vector load start-up 12 clock cycles ■A 7-clock-cycle start-up multiply ■A 12-clock-cycle start-up store Thus, value startis given by: Tstart¼12 + 7 + 12 ¼31G-6 ■Appendix G Vector Processors DepthSo, overall value becomes: T200¼660 + 4 /C231¼784 execution time per element start-up costs 784/200 ¼3.9, compared chime approximation three. Section G.4 , ambitious —allowing overlapping separate convoys. Figure G.5 shows overhead effective rates per element previous example ( A¼B/C2s) various vector lengths. chime-counting model would lead 3 clock cycles per element, two sources overhead add 0.9 clock cycles per element limit. Pipelined Instruction Start-Up Multiple Lanes Adding multiple lanes increases peak performance change start-uplatency, becomes critical reduce start-up overhead allowing startof one vector instruction overlapped completion preceding vector instructions. simplest case consider two vector instructions access different set vector registers. example, code sequence ADDV.D V1,V2,V3 ADDV.D V4,V5,V6Total time per element Total overheadper element 10Clock cycles 30 50 70 90 110 130 150 170 190012345678 Vector size9 Figure G.5 total execution time per element total overhead time per element versus vector length example page F-6. short vectors, total start-up time one-half total time, long vectors reduces one-third total time. sudden jumps occur vector length crosses multiple 64, forcing another iteration strip-mining code andexecution set vector instructions. operations increase nby loop+Tstart.G.2 Vector Performance Depth ■G-7An implementation allow first element second vector instruction follow immediately last element first vector instruction FP adder pipeline. reduce complexity control logic, vector machines require recovery time ordead time two vector instructions dis- patched vector unit. Figure G.6 pipeline diagram shows start-up latency dead time single vector pipeline. following example illustrates impact dead time achievable vector performance. Example Cray C90 two lanes requires 4 clock cycles dead time two vector instructions functional unit, even data depen- dences. maximum vector length 128 elements, reduction achievable peak performance caused dead time? would reduc- tion number lanes increased 16? Answer maximum length vector 128 elements divided two lanes occupies vector functional unit 64 clock cycles. dead time adds another 4 cycles occupancy, reducing peak performance 64/(64+4) ¼94.1% value with- dead time. number lanes increased 16, maximum length vector instructions occupy functional unit 128/16 ¼8 cycles, dead time reduce peak performance 8/(8+4) ¼66.6% value without dead time. second case, vector units never 2/3 busy! Figure G.6 Start-up latency dead time single vector pipeline. element 5-cycle latency: 1 cycle read vector-register file, 3 cycles execution, 1 cycle write vector-register file. Elements vector instruction follow pipeline, machine inserts 4 cycles dead time two different vector instructions. dead time eliminated complex control logic. (Reproduced permission Asanovic [1998] .)G-8 ■Appendix G Vector Processors DepthPipelining instruction start-up becomes complicated multiple instruc- tions reading writing vector register instructions may stall unpredictably —for example, vector load encountering memory bank conflicts. However, number lanes pipeline latencies increase, itbecomes increasingly important allow fully pipelined instruction start-up. G.3 Vector Memory Systems Depth maintain initiation rate one word fetched stored per clock, memorysystem must capable producing accepting much data. saw inChapter 4 , usually done spreading accesses across multiple independent memory banks. significant numbers banks useful dealing vec-tor loads stores access rows columns data. desired access rate bank access time determined many banks needed access memory without stalls. example shows tim- ings work vector processor. Example Suppose want fetch vector 64 elements starting byte address 136, memory access takes 6 clocks. many memory banks must supportone fetch per clock cycle? addresses banks accessed? willthe various elements arrive CPU? Answer Six clocks per access require least 6 banks, want number banks power 2, choose 8 banks. Figure G.7 shows timing first sets accesses 8-bank system 6-clock-cycle access latency. timing real memory banks usually split two different components, access latency bank cycle time (or bank busy time ). access latency time address arrives bank bank returns data value,while busy time time bank occupied one request. accesslatency adds start-up cost fetching vector memory (the total memorylatency also includes time traverse pipelined interconnection networks thattransfer addresses data CPU memory banks). bank busytime governs effective bandwidth memory system processor can- issue second request bank bank busy time elapsed. simple unpipelined SRAM banks used previous examples, access latency busy time approximately same. pipelinedSRAM bank, however, access latency larger busy time becauseeach element access occupies one stage memory bank pipeline. aDRAM bank, access latency usually shorter busy time aDRAM needs extra time restore read value destructive read oper-ation. memory systems support multiple simultaneous vector accessesG.3 Vector Memory Systems Depth ■G-9or allow nonsequential accesses vector loads stores, number mem- ory banks larger min imum; otherwise, memory bank con- flicts exist. Memory bank conflicts occur within single vector memory instruc- tion stride number banks relatively prime respect otherand enough banks avoid conflicts unit stride case. areno bank conflicts, multiword unit strides run rates. Increasing thenumber memory banks number greater minimum prevent stallswith stride length 1 decrease stall frequency strides. Forexample, 64 banks, stride 32 stall every access, rather every access. originally stride 8 16 banks, every access would stall; 64 banks, stride 8 stall every eighth access. havemultiple memory pipelines and/or multiple processors sharing memorysystem, also need banks prevent conflicts. Even machines witha single memory pipeline experience memory bank conflicts unit strideBank Cycle no. 0 1 2 3 4567 0 136 1 Busy 1442 Busy Busy 1523 Busy Busy Busy 1604 Busy Busy Busy Busy 1685 Busy Busy Busy Busy Busy 1766 Busy Busy Busy Busy Busy 1847 192 Busy Busy Busy Busy Busy8 Busy 200 Busy Busy Busy Busy9 Busy Busy 208 Busy Busy Busy 10 Busy Busy Busy 216 Busy Busy 11 Busy Busy Busy Busy 224 Busy12 Busy Busy Busy Busy Busy 23213 Busy Busy Busy Busy Busy 24014 Busy Busy Busy Busy Busy 24815 256 Busy Busy Busy Busy Busy16 Busy 264 Busy Busy Busy Busy Figure G.7 Memory addresses (in bytes) bank number time slot access begins. memory bank latches element address start access busy 6 clock cycles returning value CPU. Note CPU cannot keep 8 banks busy time limited supplying one new address receiving one data item cycle.G-10 ■Appendix G Vector Processors Depthaccesses last elements one instruction first elements next instruction, increasing number banks reduce prob- ability inter-instruction conflicts. 2011, vector supercomputersspread accesses CPU across hundreds memory banks. Becausebank conflicts still occur non-unit stride cases, programmers favor unit strideaccesses whenever possible. modern supercomputer may dozens CPUs, multiple mem- ory pipelines connected thousands memory banks. would impractical toprovide dedicated path memory pipeline memory bank, so, typically, multistage switching network used connect memory pipelines memory banks. Congestion arise switching network different vec-tor accesses contend circuit paths, causing additional stalls thememory system. G.4 Enhancing Vector Performance section, present techniques improving performance vectorprocessor depth Chapter 4 . Chaining Depth Early implementations chaining worked like forwarding, restricted timing source destination instructions chain. Recent implementa-tions use flexible chaining , allows vector instruction chain essentially active vector instruction, assuming structural hazard generated. Flexible chaining requires simultaneous access vector register dif- ferent vector instructions, implemented either adding readand write ports organizing vector-register file storage interleavedbanks similar way memory system. assume type chainingthroughout rest appendix. Even though pair operations depends one another, chaining allows operations proceed parallel separate elements vector. permitsthe operations scheduled convoy reduces number chimes required. previous sequence, sustained rate (ignoring start-up) two floating-point operations per clock cycle, one chime, achieved, even thoughthe operations dependent! total running time sequence becomes: Vector length + Start-up time ADDV + Start-up time MULV Figure G.8 shows timing chained unchained version pair vector instructions vector length 64. convoy requires one chime; however, uses chaining, start-up overhead seen actual timing convoy. Figure G.8 , total time chained operation 77 clock cycles, 1.2 cycles per result. 128 floating-point operations donein time, 1.7 FLOPS per clock cycle obtained. unchained version,there 141 clock cycles, 0.9 FLOPS per clock cycle.G.4 Enhancing Vector Performance ■G-11Although chaining allows us reduce chime component execution time putting two dependent instructions convoy, eliminatethe start-up overhead. want accurate running time estimate, must count start-up time within across convoys. chaining, number chimes sequence determined number different vector functional unitsavailable processor number required application. particular,no convoy contain structural hazard. means, example, sequencecontaining two vector memory instructions must take least two convoys, hencetwo chimes, processor like VMIPS one vector load-store unit. Chaining important every modern vector processor supports flexible chaining. Sparse Matrices Depth Chapter 4 shows techniques allow programs sparse matrices execute vector mode. Let ’s start quick review. sparse matrix, elements vector usually stored compacted form accessed indirectly. Assuming simplified sparse structure, might see code looks like this: 100 = 1,n 100 A(K(i)) = A(K(i)) + C(M(i)) code implements sparse vector sum arrays AandC, using index vectors KandMto designate nonzero elements AandC.(AandCmust number nonzero elements —nof them.) Another common representa- tion sparse matrices uses bit vector show elements exist densevector nonzero elements. Often representations exist pro-gram. Sparse matrices found many codes, many ways imple-ment them, depending data structure used program. simple vectorizing compiler could automatically vectorize source code compiler would know elements Kare distinct values thus dependences exist. Instead, programmer directive wouldtell compiler could run loop vector mode. sophisticated vectorizing compilers vectorize loop automatically without programmer annotations inserting run time checks dataUnchained Chained Total = 77Total = 14176 4 76 4 MULV 64 ADDV64 MULV ADDV6 6 Figure G.8 Timings sequence dependent vector operations ADDV andMULV , unchained chained. 6- 7-clock-cycle delays latency adder multiplier.G-12 ■Appendix G Vector Processors Depthdependences. run time checks implemented vectorized software version advanced load address table (ALAT) hardware described Appen- dix H Itanium processor. associative ALAT hardware replaced asoftware hash table detects two element accesses within stripmineiteration address. dependences detected, stripmine iter-ation complete using maximum vector length. dependence detected,the vector length reset smaller value avoids dependency violations,leaving remaining elements handled next iteration strip-mined loop. Although scheme adds considerable software overhead loop, overhead mostly vectorized common case dependences; result, loop still runs considerably faster scalar code(although much slower programmer directive provided). scatter-gather capability included many recent supercomputers. operations often run slowly strided accesses aremore complex implement susceptible bank conflicts, theyare still much faster alternative, may scalar loop. sparsityproperties matrix change, new index vector must computed. Many pro- cessors provide support computing index vector quickly. CVI (create vector index) instruction VMIPS creates index vector given stride ( m), values index vector 0, m,2/C2m,…,6 3/C2m. processors provide instruction create compressed index vector whose entries corre-spond positions one mask register. vector architecturesprovide method compress vector. VMIPS, define CVI instruction always create compressed index vector using vector mask. vectormask ones, standard index vector created. indexed loads-stores CVI instruction provide alternative method support conditional vector execution. Let us first recall code Chapter 4 : low = 1 VL = (n mod MVL) /*find odd-size piece*/ do1 j = 0,(n/MVL) /*outer loop*/ do10 = low, low + VL - 1 /*runs length VL*/ Y(i) =a*X ( )+ Y(i) /*main operation*/ 10 continue low = low + VL /*start next vector*/ VL = MVL /*reset length max*/ 1 continue vector sequence implements loop using CVI:LV V1,Ra ;load vector V1 L.D F0,#0 ;load FP zero F0SNEVS.D V1,F0 ;sets VM 1 V1(i)!=F0 CVI V2,#8 ;generates indices V2 POP R1,VM ;find number 1 ’si nV MTC1 VLR,R1 ;load vector-length registerCVM ;clears maskG.4 Enhancing Vector Performance ■G-13LVI V3,(Ra+V2) ;load nonzero elements LVI V4,(Rb+V2) ;load corresponding B elements SUBV.D V3,V3,V4 ;do subtractSVI (Ra+V2),V3 ;store back Whether implementation using scatter-gather better condition- ally executed version depends frequency condition holds andthe cost operations. Ignoring chaining, running time original ver-sion 5 n+c 1. running time second version, using indexed loads stores running time one element per clock, 4 n+4fn+c2, fis fraction elements condition true (i.e., A(i) ¦0). assume values c1andc2are comparable, much smaller n,w e find second technique better. Time 1¼5nðÞ Time 2¼4n+4fn want Time 1>Time 2,s 5n>4n+4fn 1 4>f is, second method faster less one-quarter elements non- zero. many cases, frequency execution much lower. index vectorcan reused, number vector statements within statement grows,the advantage scatter-gather approach increase sharply. G.5 Effectiveness Compiler Vectorization Two factors affect success program run vector mode. first factor structure program itself: loops true data dependences, restructured dependences? Thisfactor influenced algorithms chosen and, extent, arecoded. second factor capability compiler. compiler canvectorize loop parallelism among loop iterations exists, tre-mendous variation ability compilers determine whether loop bevectorized. techniques used vectorize programs thosediscussed Chapter 3 uncovering ILP; here, simply review well techniques work. tremendous variation well different compilers vectorizing programs. summary state vectorizing compilers, consider data inFigure G.9 , shows extent vectorization different processors using test suite 100 handwritten FORTRAN kernels. kernels designed testvectorization capability vectorized hand; see severalexamples loops exercises.G-14 ■Appendix G Vector Processors DepthG.6 Putting Together: Performance Vector Processors section, look performance measures vector processors tell us processors. determine performance processor vectorproblem must look start-up cost sustained rate. simplest andbest way report performance vector processor loop give execution time vector loop. vector loops, people often give MFLOPS (millions floating-point operations per second) rating rather execution time.We use notation R nfor MFLOPS rating vector length n. Using measurements n(time) R n(rate) equivalent number FLOPS agreed upon. event, either measurement include overhead. section, examine performance VMIPS DAXPY loop (see Chapter 4 ) looking performance different viewpoints. continue compute execution time vector loop using equation developed Section G.2. time, look different ways measure perfor- mance using computed time. constant values loopused section introduce small amount error, ignored. Measures Vector Performance vector length important establishing performance proces-sor, length-related measures often applied addition time MFLOPS.These length-related measures tend vary dramatically across different processorsProcessor CompilerCompletely vectorizedPartially vectorizedNot vectorized CDC CYBER 205 VAST-2 V2.21 62 5 33 Convex C-series FC5.0 69 5 26Cray X-MP CFT77 V3.0 69 3 28Cray X-MP CFT V1.15 50 1 49Cray-2 CFT2 V3.1a 27 1 72ETA-10 FTN 77 V1.0 62 7 31Hitachi S810/820 FORT77/HAP V20-2B 67 4 29IBM 3090/VF VS FORTRAN V2.4 52 4 44NEC SX/2 FORTRAN77 / SX V.040 66 5 29 Figure G.9 Result applying vectorizing compilers 100 FORTRAN test kernels. processor indicate many loops completely vectorized, partially vectorized, unvectorized. loops collected Callahan, Dongarra, Levine [1988] . Two different compilers Cray X-MP show large dependence compiler technology.G.6 Putting Together: Performance Vector Processors ■G-15and interesting compare. (Remember, though, time always mea- sure interest comparing relative speed two processors.) Three important length-related measures ■R∞—The MFLOPS rate infinite-length vector. Although measure may interest estimating peak performance, real problems lim-ited vector lengths, overhead penalties encountered real problemswill larger. ■N1/2—The vector length needed reach one-half R ∞. good mea- sure impact overhead. ■Nv—The vector length needed make vector mode faster scalar mode. measures overhead speed scalars relative vectors. Let’s look measures DAXPY problem running VMIPS. chained, inner loop DAXPY code convoys looks like Figure G.10 (assuming RxandRyhold starting addresses). Recall performance equation execution time vector loop n elements, n: Tn¼n MVLhi /C2Tloop+Tstart/C0/C1 +n/C2Tchime Chaining allows loop run three chimes (and less, since one memory pipeline); thus, chime¼3. chime w e r eac p l e ei n c fp e r - f r n c e ,t h el pw u l dr u na ta nM F L P Sr eo f2 / 3 /C2clock rate (since 2 FLOPS per iteration). Thus, based chime count, 500 MHz VMIPS would run loop 333 MFLOPS assuming strip-mining start-up overhead. several ways improve performance: Add addi-tional vector load-store units, allow convoys overlap reduce impact ofstart-up overheads, decrease number loads required vector-registerallocation. examine first two extensions section. lastoptimization actually used Cray-1, VMIPS ’s cousin, boost per- formance 50%. Reducing number loads requires interproceduraloptimization; examine transformation Exercise G.6. exam- ine first two extensions, let ’s see real performance, including overhead, is. LV V1,Rx MULVS.D V2,V1,F0 Convoy 1: chained load multiply LV V3,Ry ADDV.D V4,V2,V3 Convoy 2: second load add, chained SV Ry,V4 Convoy 3: store result Figure G.10 inner loop DAXPY code chained convoys.G-16 ■Appendix G Vector Processors DepthThe Peak Performance VMIPS DAXPY First, determine peak performance, R ∞, really is, since know must differ ideal 333 MFLOPS rate. now, continue touse simplifying assumption convoy cannot start instructionsin earlier convoy completed; later remove restriction. Usingthis simplification, start-up overhead vector sequence simply sumof start-up times instructions: Tstart¼12 + 7 + 12 + 6 + 12 ¼49 Using MVL ¼64, loop¼15, start¼49, chime¼3 performance equation, assuming nis exact multiple 64, time n- element operation Tn¼n 64hi /C215 + 49ðÞ +3n /C20n+6 4ðÞ +3n ¼4n+6 4 sustained rate actually 4 clock cycles per iteration, rather the- oretical rate 3 chimes, ignores overhead. major part differenceis cost start-up overhead block 64 elements (49 cycles versus15 loop overhead). compute R ∞for 500 MHz clock as: R∞¼lim n!∞Operations per iteration /C2C1ock rate C1ock cyc1es per iteration/C18/C19 numerator independent n, hence R∞¼Operations per iteration /C2C1ock rate lim n!∞C1ock cyc1es per iterationðÞ lim n!∞Clock cycles per iterationðÞ ¼ lim n!∞Tn n/C18/C19 ¼lim n!∞4n+6 4 n/C18/C19 ¼4 R∞¼2/C2500 MHz 4¼250 MFLOPS performance without start-up overhead, peak performance given vector functional unit structure , 1.33 times higher. actuality, gap peak sustained performance benchmark evenlarger! Sustained Performance VMIPS Linpack Benchmark Linpack benchmark Gaussian elimination 100 /C2100 matrix. Thus, vector element lengths range 99 1. vector length kis used k times. Thus, average vector length given by:G.6 Putting Together: Performance Vector Processors ■G-17X99 i¼1i2 X99 i¼1i¼66:3 obtain accurate estimate performance DAXPY using vector length 66: T66¼2/C215 + 49ðÞ +6 6/C23¼128 + 198 ¼326 R66¼2/C266/C2500 326MFLOPS ¼202 MFLOPS peak number, ignoring start-up overhead, 1.64 times higher estimate sustained performance real vector lengths. actual practice,the Linpack benchmark contains nontrivial fraction code cannot bevectorized. Although code accounts less 20% time vectorization, runs less one-tenth performance counted FLOPS. Thus, Amdahl ’s law tells us overall performance significantly lower performance estimated analyzing theinner loop. Since vector length significant impact performance, N 1/2and N v measures often used comparing vector machines. Example N 1/2for inner loop DAXPY VMIPS 500 MHz clock? Answer Using R ∞as peak rate, want know vector length achieve 125 MFLOPS. start formula MFLOPS assuming themeasurement made N 1/2elements: MFLOPS ¼FLOPS executed N 1=2iterations C1ock cyc1es execute N 1=2iterations/C2C1ock cycles Second/C210/C06 125¼2/C2N1=2 TN1=2/C2500 Simplifying assuming N 1/2<64, N1=2<64¼64 + 3/C2n, yields: TN1=2¼8/C2N1=2 64 + 3/C2N1=2¼8/C2N1=2 5/C2N1=2¼64 N1=2¼12:8 N 1/2¼13; is, vector length 13 gives approximately one-half peak performance DAXPY loop VMIPS.G-18 ■Appendix G Vector Processors DepthExample vector length, N v, vector operation runs faster scalar? Answer Again, know N v<64. time one iteration scalar mode estimated 10+12+12+7+6+12 ¼59 clocks, 10 estimate loop overhead, known somewhat less strip-mining loop overhead.In last problem, showed vector loop runs vector mode timeT n/C2064¼64+3/C2nclock cycles. Therefore, 64 + 3N v¼59N v Nv¼64 56/C20/C21 Nv¼2 DAXPY loop, vector mode faster scalar long vector least two elements. number surprisingly small. DAXPY Performance Enhanced VMIPS DAXPY, like many vector problems, memory limited. Consequently, perfor- mance could improved adding memory access pipelines. major architectural difference Cray X-MP (and later processors)and Cray-1. Cray X-MP three memory pipelines, compared withthe Cray-1 ’s single memory pipeline, X-MP flexible chaining. affect performance? Example would value 66for DAXPY VMIPS added two memory pipelines? Answer three memory pipelines, instructions fit one convoy take one chime. start-up overheads same, T66¼66 64/C20/C21 /C2Tloop+Tstart/C0/C1 +6 6/C2Tchime T66¼2/C215 + 49ðÞ +6 6/C21¼194 three memory pipelines, reduced clock cycle count sustained performance 326 194, factor 1.7. Note effect Amdahl ’s law: improved theoretical peak rate measured number chimes factor 3, achieved overall improvement factor 1.7 sustainedperformance.G.6 Putting Together: Performance Vector Processors ■G-19Another improvement could come allowing different convoys overlap also allowing scalar loop overhead overlap vector instructions.This requires one vector operation allowed begin using functional unitbefore another operation completed, complicates instruction issue logic. Allowing overlap eliminates separate start-up overhead every convoy except first hides loop overhead well. achieve maximum hiding strip-mining overhead, need able overlap strip-mined instances loop, allowing two instances convoy aswell possibly two instances scalar code execution simulta-neously. requires techniques looked Chapter 3 avoid WAR hazards, although overlapped read write single vectorelement possible, copying avoided. technique, called tailgating , used Cray-2. Alternatively, could unroll outer loop create several instances vector sequence using different register sets (assumingsufficient registers), Chapter 3 . allowing maximum overlap convoys scalar loop overhead, start-up loop overheads willonly seen per vector sequence, independent number convoys instructions convoy. way, processor vector registers canhave low start-up overhead short vectors high peak performance forvery long vectors. Example would values R ∞and 66for DAXPY VMIPS added two memory pipelines allowed strip-mining start-up overheads befully overlapped? Answer R∞¼lim n!∞Operations per iteration /C2C1ock rate C1ock cyc1es per iteration/C18/C19 lim n!∞Clock cycles per iterationðÞ ¼ lim n!∞Tn n/C18/C19 Since overhead seen once, n¼n+49+15 ¼n+64. Thus, lim n!∞Tn n/C18/C19 ¼lim n!∞n+6 4 n/C18/C19 ¼1 R∞¼2/C2500 MHz 1¼1000 MFLOPS Adding extra memory pipelines flexible issue logic yields improvement peak performance factor 4. However, 66¼130, shorter vectors sustained performance improvement 326/130¼2.5 times.G-20 ■Appendix G Vector Processors DepthIn summary, examined several measures vector performance. Theoretical peak performance calculated based purely value Tchime as: Number FLOPS per iteration /C2Clock rate Tchime including loop overhead, calculate values peak performance infinite-length vector (R ∞) also sustained performance, R nfor vector length n, computed as: Rn¼Number FLOPS per iteration /C2n/C2Clock rate Tn Using measures also find N 1/2and N v, give us another way looking start-up overhead vectors ratio vector scalar speed. Awide variety measures performance vector processors useful under-standing range performance applications may see vector processor. G.7 Modern Vector Supercomputer: Cray X1 Cray X1 introduced 2002, and, together NEC SX/8, representsthe state art modern vector supercomputers. X1 system architecture supports thousands powerful vector processors sharing single global memory. Cray X1 unusual processor architecture, shown Figure G.11 .A large Multi-Streaming Processor (MSP) formed ganging together four Single-Streaming Processors (SSPs). SSP complete single-chip vector micropro-cessor, containing scalar unit, scalar caches, two-lane vector unit. SSPscalar unit dual-issue out-of-order superscalar processor 16 KB instruc-tion cache 16 KB scalar write-through data cache, two-way set associa-tive 32-byte cache lines. SSP vector unit contains vector register file, three vector arithmetic units, one vector load-store unit. much easier pipeline deeply vector functional unit superscalar issue mechanism, sothe X1 vector unit runs twice clock rate (800 MHz) scalar unit(400 MHz). lane perform 64-bit floating-point add 64-bitfloating-point multiply cycle, leading peak performance 12.8 GFLOPSper MSP. previous Cray machines could trace instruction set architecture (ISA) lineage back original Cray-1 design 1976, 8 primary registers addresses, scalar data, vector data. X1, ISA redesigned scratch incorporate lessons learned last 30 years compiler micro-architecture research. X1 ISA includes 64 64-bit scalar address registers and64 64-bit scalar data registers, 32 vector data registers (64 bits per element)and 8 vector mask registers (1 bit per element). large increase number ofregisters allows compiler map program variables registers toreduce memory traffic also allows better static scheduling code improveG.7 Modern Vector Supercomputer: Cray X1 ■G-21run time overlap instruction execution. Earlier Crays compact variable- length instruction set, X1 ISA fixedlength instructions simplifysuperscalar fetch decode. Four SSP chips packaged multichip module together four cache chips implementing external 2 MB cache (Ecache) shared SSPs. Ecache two-way set associative 32-byte lines write-back policy. Ecache used cache vectors, reducing memory traffic codes exhibittemporal locality. ISA also provides vector load store instruction variantsthat allocate cache avoid polluting Ecache data knownto low locality. Ecache sufficient bandwidth supply one 64-bitword per lane per 800 MHz clock cycle, 50 GB/sec per MSP. next level X1 packaging hierarchy, shown Figure G.12 , four MSPs placed single printed circuit board together 16 memory con- troller chips DRAM form X1 node. memory controller chip eight separate Rambus DRAM channels, channel provides 1.6 GB/sec memory bandwidth. Across 128 memory channels, node over200 GB/sec main memory bandwidth. X1 system contain 1024 nodes (4096 MSPs 16,384 SSPs), connected via high-bandwidth global network. network connectionsare made via memory controller chips, memory system directlyaccessible processor using load store instructions. provides much faster global communication message-passing protocols used cluster-based systems. Maintaining cache coherence across large numberof high-bandwidth shared-memory nodes would challenging. approachtaken X1 restrict Ecache cache data local nodeDRAM. memory controllers implement directory scheme maintainMSP SSP Superscalar unit V Vector unitV VSSP V VSSP V VSSP V V 0.5 MB Ecache0.5 MB Ecache0.5 MB Ecache0.5 MB Ecache Figure G.11 Cray MSP module. (From Dunnigan et al. [2005] .)G-22 ■Appendix G Vector Processors Depthcoherency four Ecaches node. Accesses remote nodes obtain recent version location, remote stores invalidate localEcaches updating memory, remote node cannot cache local locations. Vector loads stores particularly useful presence long-latency cache misses global communications, relatively simple vector hardware cangenerate track large number in-flight memory requests. Contemporarysuperscalar microprocessors support 8 16 outstanding cache misses,whereas MSP processor 2048 outstanding memory requests(512 per SSP). compensate, superscalar microprocessors movingto larger cache line sizes (128 bytes above) bring data cache miss, leads significant wasted bandwidth non-unit stride accesses large datasets. X1 design uses short 32-byte lines throughoutto reduce bandwidth waste instead relies supporting many independentcache misses sustain memory bandwidth. latency tolerance together withthe huge memory bandwidth non-unit strides explains vector machines canprovide large speedups superscalar microprocessors certain codes. Multi-Streaming Processors Multi-Streaming concept first introduced Cray SV1, considerably enhanced X1. four SSPs within MSP share Ecache, hardware support barrier synchronization across four SSPs withinan MSP. X1 SSP two-lane vector unit 32 vector registers eachholding 64 elements. compiler several choices use SSPswithin MSP.M memM memM memM memM memM memM memM memM memM memM memM memM memM memM memM mem 51 GFLOPS, 200 GB/sec IOIO IOP P P P SP P P P SP P P P SP P P P Figure G.12 Cray X1 node. (From Tanqueray [2002] .)G.7 Modern Vector Supercomputer: Cray X1 ■G-23The simplest use gang together four two-lane SSPs emulate single eight-lane vector processor. X1 provides efficient barrier synchronization primitives SSPs node, compiler responsible generatingthe MSP code. example, vectorizable inner loop 1000 elements, thecompiler allocate iterations 0 –249 SSP0, iterations 250 –499 SSP1, iter- ations 500 –749 SSP2, iterations 750 –999 SSP3. SSP process loop iterations independently must synchronize back SSPsbefore moving next loop nest. inner loops many iterations, eight-lane MSP low efficiency, SSP elements process execution time dominated start-up time synchronization overheads. Anotherway use MSP compiler parallelize across outer loop, givingeach SSP different inner loop process. example, following nested loopsscale upper triangle matrix constant: /* Scale upper triangle constant K. */ (row = 0; row <MAX_ROWS; row++) (col = row; col <MAX_COLS; col++) A[row][col] = A[row][col] * K; Consider case MAX_ROWS andMAX_COLS 100 elements. vector length inner loop steps 100 1 iterations ofthe outer loop. Even first inner loop, loop length would much lessthan maximum vector length (256) eight-lane MSP, code wouldtherefore inefficient. Alternatively, compiler assign entire inner loops single SSP. example, SSP0 might process rows 0, 4, 8, on, SSP1 processes rows 1, 5, 9, on. SSP sees longer vector. effect, thisapproach parallelizes scalar overhead makes use individual scalarunits within SSP. application code uses MSPs, also possible compile code use SSPs individual processors limited vector parallelism butsignificant thread-level parallelism. Cray X1E 2004, Cray announced upgrade original Cray X1 design. X1Euses newer fabrication technology allows two SSPs placed singlechip, making X1E first multicore vector microprocessor. physicalnode contains eight MSPs, organized two logical nodesof four MSPs retain programming model X1. addition,the clock rates raised 400 MHz scalar 800 MHz vector to565 MHz scalar 1130 MHz vector, giving improved peak performanceof 18 GFLOPS.G-24 ■Appendix G Vector Processors DepthG.8 Concluding Remarks 1980s 1990s, rapid performance increases pipelined scalar pro- cessors led dramatic closing gap traditional vector supercom-puters fast, pipelined, superscalar VLSI microprocessors. 2011, possibleto buy laptop computer $1000 higher CPU clock rate available vector supercomputer, even costing tens millions dollars.Although vector supercomputers lower clock rates, support greater parallelism using multiple lanes (up 16 Japanese designs) versus lim- ited multiple issue superscalar microprocessors. Nevertheless, peakfloating-point performance low-cost microprocessors within factor oftwo leading vector supercomputer CPUs. course, high clock rates andhigh peak performance necessarily translate sustained applicationperformance. Main memory bandwidth key distinguishing feature betweenvector supercomputers superscalar microprocessor systems. Providing large non-unit stride memory bandwidth one major expenses vector supercomputer, traditionally SRAM used main memory reduce number memory banks needed reduce vectorstart-up penalties. SRAM access time several times lower thatof DRAM, costs roughly 10 times much per bit! reduce main memory costsand allow larger capacities, modern vector supercomputers use DRAMfor main memory, taking advantage new higher-bandwidth DRAM interfacessuch synchronous DRAM. adoption DRAM main memory (pioneered Seymour Cray Cray-2) one example vector supercomputers adapted commodity technology improve price-performance. Another example vectorsupercomputers including vector data caches. Caches effectivefor vector codes, however, vector caches designed allow highmain memory bandwidth even presence many cache misses. example,the Cray X1 MSP 2048 outstanding memory loads; microprocessors,8 16 outstanding cache misses per CPU typical maximum numbers. Another example demise bipolar ECL gallium arsenide technol- ogies choice supercomputer CPU logic. huge investment CMOS technology made possible success desktop computer, CMOSnow offers competitive transistor performance much greater transistor densityand much reduced power dissipation compared exotic technolo-gies. result, leading vector supercomputers built sameCMOS technology superscalar microprocessors. primary reason vectorsupercomputers lower clock rates commodity microprocessors thatthey developed using standard cell ASIC techniques rather full custom circuit design reduce engineering design cost. microprocessor design may sell tens millions copies amortize design cost overthis large number units, vector supercomputer considered success overa hundred units sold!G.8 Concluding Remarks ■G-25Conversely, via superscalar microprocessor designs begun absorb techniques made popular earlier vector computer systems, Multimedia SIMD extensions. showed Chapter 4 , invest- ment hardware SIMD performance increasing rapidly, perhaps even morethan multiprocessors. even wider SIMD units GPUs become well inte-grated scalar cores, including scatter-gather support, may well con-clude vector architectures architecture wars! G.9 Historical Perspective References historical perspective adds details references left theversion Chapter 4 . CDC STAR processor descendant, CYBER 205, memory- memory vector processors. keep hardware simple support highbandwidth requirements (up three memory references per floating-point opera-tion), processors efficiently handle non-unit stride. loopshave unit stride, non-unit stride loop poor performance processorsbecause memory-to-memory data movements required gather together(and scatter back) nonadjacent vector elements; operations used specialscatter-gather instructions. addition, special support sparse vectors used bit vector represent zeros nonzeros dense vector nonzero values. complex vector operations slow ofthe long memory latency, often faster use scalar mode sparseor non-unit stride operations. Schneck [1987] described several early pipe- lined processors (e.g., Stretch) first vector processors, including the205 Cray-1. Dongarra [1986] another good survey, focusing recent processors. 1980s also saw arrival smaller-scale vector processors, called mini-supercomputers. Priced roughly one-tenth cost supercomputer ($0.5 $1 million versus $5t $10 million), processors caught quickly. Although many companies joined market, two companies mostsuccessful Convex Alliant. Convex started uniprocessor C-1vector processor offered series small multiprocessors, ending withthe C-4 announced 1994. keys success Convex periodwere emphasis Cray software capability, effectiveness com-piler (see Figure G.9 ), quality UNIX OS implementation. C-4 last vector machine Convex sold; switched making large-scale multiprocessors using Hewlett-Packard RISC microprocessors bought byHP 1995. Alliant [1987] concentrated multiprocessor aspects; built eight-processor computer, processor offering vector capability.Alliant ceased operation early 1990s. early 1980s, CDC spun group, called ETA, build new super- computer, ETA-10, capable 10 GFLOPS. ETA processor deliveredin late 1980s (see Fazio [1987] ) used low-temperature CMOS aG-26 ■Appendix G Vector Processors Depthconfiguration 10 processors. processor retained memory- memory architecture based CYBER 205. Although ETA-10 achieved enormous peak performance, scalar speed comparable. 1989,CDC, first supercomputer vendor, closed ETA left supercomputerdesign business. 1986, IBM introduced System/370 vector architecture (see Moore et al. [1987] ) first implementation 3090 Vector Facility. architecture extended System/370 architecture 171 vector instructions. 3090/VFwas integrated 3090 CPU. Unlike vector processors time, 3090/VF routed vectors cache. IBM 370 machines contin- ued evolve time called IBM zSeries. vector extensionshave removed architecture opcode space reusedto implement 64-bit address extensions. late 1989, Cray Research split two companies, aimed build- ing high-end processors available early 1990s. Seymour Cray headed thespin-off, Cray Computer Corporation, demise 1995. initial pro-cessor, Cray-3, implemented gallium arsenide, unable develop reliable cost-effective implementation technology. sin- gle Cray-3 prototype delivered National Center AtmosphericResearch (NCAR) evaluation purposes 1993, paying customers werefound design. Cray-4 prototype, first pro-cessor run 1 GHz, close completion company filed bank-ruptcy. Shortly tragic death car accident 1996, Seymour Craystarted yet another company, SRC Computers, develop high-performance sys-tems time using commodity components. 2000, SRC announced SRC-6 system, combined 512 Intel microprocessors, 5 billion gates reconfigurable logic, high-performance vector-style memory system. Cray Research focused C90, new high-end processor 16 processors clock rate 240 MHz. processor delivered 1991.The J90 CMOS-based vector machine using DRAM memory starting at$250,000, typical configurations running $1 million. mid- 1995, Cray Research acquired Silicon Graphics, 1998 releasedthe SV1 system, grafted considerably faster CMOS processors onto J90 memory system, also added data cache vectors CPU help meet increased memory bandwidth demands. SV1 also intro-duced MSP concept, developed provide competitive single-CPUperformance ganging together multiple slower CPUs. Silicon Graphics soldCray Research Tera Computer 2000, joint company renamedCray Inc. basis modern vectorizing compiler technology notion data dependence developed Kuck colleagues [1974] University Illinois. Banerjee [1979] developed test named him. Padua Wolfe [1986] gave good overview vectorizing compiler technology. Benchmark studies various supercomputers, including attempts under- stand performance differences, undertaken Lubeck, Moore,G.9 Historical Perspective References ■G-27and Mendez [1985], Bucher [1983] , Jordan [1987] . several bench- mark suites aimed scientific usage often employed supercomputer benchmarking, including Linpack Lawrence Livermore Laboratories FOR-TRAN kernels. University Illinois coordinated collection set ofbenchmarks supercomputers, called Perfect Club. 1993, Perfect Clubwas integrated SPEC, released set benchmarks, SPEChpc96, aimedat high-end scientific processing 1996. NAS parallel benchmarks developedat NASA Ames Research Center [ Bailey et al. 1991 ] become popular set kernels applications used supercomputer evaluation. new benchmark suite, HPC Challenge, introduced consisting kernels stress machine memory interconnect bandwidths addition floating-point perfor-mance [ Luszczek et al. 2005 ]. Although standard supercomputer benchmarks useful rough measure machine capabilities, large supercomputer purchasesare generally preceded careful performance evaluation actual mix ofapplications required customer site. References Alliant Computer Systems Corp, 1987. Alliant FX/Series: Product Summary. Mass, Acton (June). Asanovic, K., 1998. Vector microprocessors, ”Ph.D. thesis, Computer Science Division. University California Berkeley (May). Bailey, D.H., Barszcz, E., Barton, J.T., Browning, D.S., Carter, R.L., Dagum, L., Fatoohi, R.A., Frederickson, P.O., Lasinski, T.A., Schreiber, R.S., Simon, H.D., Venkatakrishnan, V.,Weeratunga, S.K., 1991. NAS parallel benchmarks. Int ’l. J. Supercomputing Applications 5, 63 –73. Banerjee, U., 1979. Speedup ordinary programs, ”Ph.D. thesis, Department Computer Science. University Illinois Urbana-Champaign (October). Baskett, F., Keller, T.W., 1977. Evaluation Cray-1 Processor. In: Kuck, D.J., Lawrie, D.H., Sameh, A.H. (Eds.), High Speed Computer Algorithm Organization. Academic Press, SanDiego, pp. 71 –84. Brandt, M., Brooks, J., Cahir, M., Hewitt, T., Lopez-Pineda, E., Sandness, D., 2000. Benchmarker ’s Guide Cray SV1 Systems. Cray Inc., Seattle, Wash. Bucher, I.Y., 1983. computational speed supercomputers. In: Proc. ACM SIGMETRICS Conf. Measurement Modeling Computer Systems, August 29 –31, 1983. Minneapolis, Minn, pp. 151 –165. Callahan, D., Dongarra, J., Levine, D., 1988. Vectorizing compilers: test suite results. In: Supercomputing ’88: Proceedings 1988 ACM/IEEE Conference Supercomputing, November 12 –17, pp. 98 –105. Orlando, FL. Chen, S., 1983. Large-scale high-speed multiprocessor system scientific applications. In: Hwang, K. (Ed.), Superprocessors: Design applications. Proc. NATO Advanced ResearchWorkshop High-Speed Computing, June 20 –22, 1983, Julich, Kernforschungsanlage, Federal Republic Germany. IEEE, (August), 1984. Dongarra, J.J., 1986. survey high performance processors. COMPCON, IEEE, 8 –11 (March). Dunnigan, T.H., Vetter, J.S., White III, J.B., Worley, P.H., 2005. Performance evaluation Cray X1 distributed shared-memory architecture. IEEE Micro 25 (1 (January –February)), 30 –40. Fazio, D., 1987. ’s really much fun building supercomputer simply inventing one. COMPCON, IEEE, 102 –105 (February). Flynn, M.J., 1966. high-speed computing systems. In: Proc. IEEE 54:12 (December), pp. 1901 –1909. Hintz, R.G., Tate, D.P., 1972. Control data STAR-100 processor design. COMPCON, IEEE 1 –4 (September). Jordan, K.E., 1987. Performance comparison large-scale scientific processors: Scalar mainframes, mainframes vector facilities, supercomputers. Computer 20 (3 (March)), 10 –23.G-28 ■Appendix G Vector Processors DepthKitagawa, K., Tagaya, S., Hagihara, Y., Kanoh, Y., 2003. hardware overview SX-6 SX-7 supercomputer. NEC Research & Development J 44 (1 (January)), 2 –7. Kuck, D., Budnik, P.P., Chen, S.-C., Lawrie, D.H., Towle, R.A., Strebendt, R.E., Davis Jr., E.W., Han, J., Kraska, P.W., Muraoka, Y., 1974. Measurements parallelism ordinary FORTRANprograms. Computer 7 (1 (January)), 37 –46. Lincoln, N.R., 1982. Technology design trade offs creation modern supercomputer. IEEE Trans. Computers, 363 –376. C-31:5 (May). Lubeck, O., Moore, J., Mendez, R., 1985. benchmark comparison three supercomputers: Fujitsu VP-200, Hitachi S810/20, Cray X-MP/2. Computer 18 (1 (January)), 10 –29. Luszczek, P., Dongarra, J.J., Koester, D., Rabenseifner, R., Lucas, B., Kepner, J., McCalpin, J., Bailey, D., Takahashi, D., 2005. In: Introduction HPC challenge benchmark suite, ”Lawrence Berkeley National Laboratory, Paper LBNL-57493 (April 25). http://repositories.cdlib.org/lbnl/LBNL-57493 . Miranker, G.S., Rubenstein, J., Sanguinetti, J., 1988. Squeezing Cray-class supercomputer single-user package. COMPCON, IEEE, 452 –456 (March). Miura, K., Uchida, K., 1983. FACOM vector processing system: VP100/200. In: Proc. NATO Advanced Research Workshop High-Speed Computing. June 20 –22, 1983, Julich, Ker- nforschungsanlage, Federal Republic Germany; also K. Hwang, ed., “Superprocessors: Design applications, ”IEEE (August), 1984, 59 –73. Moore, B., Padegs, A., Smith, R., Bucholz, W., 1987. Concepts System/370 vector architecture. In: Proc. 14th Int ’l. Symposium Computer Architecture, June 3 –6, 1987. Pittsburgh, Penn, pp. 282 –292. Padua, D., Wolfe, M., 1986. Advanced compiler optimizations supercomputers. Comm. ACM 29 (12 (December)), 1184 –1201. Russell, R.M., 1978. Cray-1 processor system. Comm. ACM 21 (1 (January)), 63 –72. Schneck, P.B., 1987. Superprocessor Architecture. Kluwer Academic Publishers, Norwell, Mass.Smith, B.J., 1981. Architecture applications HEP multiprocessor system. Real-Time Signal Processing IV 298, 241 –248. August. Sporer, M., Moss, F.H., Mathais, C.J., 1988. introduction architecture Stellar Graphics supercomputer. COMPCON, IEEE 464 (March). Tanqueray, D., 2002. Cray X1 supercomputer road map. In: Proc. 13th Daresbury Machine Evaluation Workshop, December 11 –12. Cheshire, England. Vajapeyam, S., 1991. Instruction-level characterization Cray Y-MP processor. Ph.D. thesis, Com- puter Sciences Department. University Wisconsin-Madison. Watanabe, T., 1987. Architecture performance NEC supercomputer SX system. Parallel Computing 5, 247 –255. Watson, W.J., 1972. TI ASC —a highly modular flexible super processor architecture. In: Proc. AFIPS Fall Joint Computer Conf, pp. 221 –228. Exercises exercises assume VMIPS clock rate 500 MHz loop¼15. Use start-up times Figure G.2 , assume store latency always included running time. G.1 [10]<G.1, G.2 >Write VMIPS vector sequence achieves peak MFLOPS performance processor (use functional unit instruction description Section G.2 ). Assuming 500-MHz clock rate, peak MFLOPS? G.2 [20/15/15] <G.1–G.6>Consider following vector code run 500 MHz version VMIPS fixed vector length 64: LV V1,Ra MULV.D V2,V1,V3ADDV.D V4,V1,V3SV Rb,V2SV Rc,V4Exercises ■G-29Ignore strip-mining overhead, assume store latency must included time perform loop. entire sequence produces 64 results. a.[20]<G.1–G.4>Assuming chaining single memory pipeline, many chimes required? many clock cycles per result (including stores one result) vector sequence require, including start-up overhead? b.[15]<G.1–G.4>If vector sequence chained, many clock cycles per result sequence require, including overhead? c.[15]<G.1–G.6>Suppose VMIPS three memory pipelines chaining. bank conflicts accesses loop, many clock cycles required per result sequence? G.3 [20/20/15/15/20/20/20] <G.2–G.6>Consider following FORTRAN code: do10 i=1,n A(i)=A(i)+B(i) B(i)=x * B(i) 10 continue Use techniques Section G.6 estimate performance throughout exercise, assuming 500 MHz version VMIPS. a.[20]<G.2–G.6>Write best VMIPS vector code inner portion loop. Assume xis F0 addresses AandBare RaandRb, respectively. b.[20]<G.2–G.6>Find total time loop VMIPS (T 100). MFLOPS rating loop (R 100)? c.[15]<G.2–G.6>Find R ∞for loop. d.[15]<G.2–G.6>Find N 1/2for loop. e.[20]<G.2–G.6>Find N vfor loop. Assume scalar code pipe- line scheduled memory reference takes six cycles FP oper-ation takes three cycles. Assume scalar overhead also loop. f.[20]<G.2–G.6>Assume VMIPS two memory pipelines. Write vector code takes advantage second memory pipeline. Show layout inconvoys. g.[20]<G.2–G.6>Compute 100and R 100for VMIPS two memory pipelines. G.4 [20/10] <G.2>Suppose version VMIPS eight memory banks (each double word wide) memory access time eight cycles. a.[20]<G.2>If load vector length 64 executed stride 20 double words, many cycles load take complete? b.[10]<G.2>What percentage memory bandwidth achieve 64-element load stride 20 versus stride 1?G-30 ■Appendix G Vector Processors DepthG.5 [12/12] <G.5–G.6>Consider following loop: C=0.0 do10 i=1,64 A(i)=A(i)+B(i)C=C+A(i) 10 continue a.[12]<G.5–G.6>Split loop two loops: one dependence one dependence. Write loops FORTRAN —as source-to-source transformation. optimization called loop fission. b.[12]<G.5–G.6>Write VMIPS vector code loop without dependence. G.6 [20/15/20/20] <G.5–G.6>The compiled Linpack performance Cray-1 (designed 1976) almost doubled better compiler 1989.Let’s look simple example might occur. Consider DAXPY-like loop (where kis parameter procedure containing loop): do10 i=1,64 do10 j=1,64 Y(k,j)=a*X(i,j)+Y(k,j) 10 continue a.[20]<G.5–G.6>Write straightforward code sequence inner loop VMIPS vector instructions. b.[15]<G.5–G.6>Using techniques Section G.6 , estimate perfor- mance code VMIPS finding 64in clock cycles. may assume loopof overhead incurred iteration outer loop. limits performance? c.[20]<G.5–G.6>Rewrite VMIPS code reduce performance limita- tion; show resulting inner loop VMIPS vector instructions. ( Hint: Think establishes chime; affect it?) Find total time resulting sequence. d.[20]<G.5–G.6>Estimate performance new version, using techniques Section G.6 finding 64. G.7 [15/15/25] <G.4>Consider following code: do10 i=1,64 if(B(i) .ne. 0) A(i)=A(i)/B(i) 10 continue Assume addresses AandBare RaandRb, respectively, F0 contains 0.Exercises ■G-31a.[15]<G.4>Write VMIPS code loop using vector-mask capability. b.[15]<G.4>Write VMIPS code loop using scatter-gather. c.[25]<G.4>Estimate performance (T 100in clock cycles) two vector loops, assuming divide latency 20 cycles. Assume vector instruc-tions run one result per clock, independent setting vector-maskregister. Assume 50% entries Bare 0. Considering hardware costs, would build loop typical? G.8 [15/20/15/15] <G.1–G.6>The difference peak sustained perfor- mance large. one problem, Hitachi S810 peak speed twice high Cray X-MP, another realistic problem, CrayX-MP twice fast Hitachi processor. Let ’s examine might occur using two versions VMIPS following code sequences: C Code sequence 1 do10 i=1,10000 A(i)=x * A(i)+y * A(i) 10 continue C Code sequence 2 do10 i=1,100 A(i)=x * A(i) 10 continue Assume version VMIPS (call VMIPS-II) two copies every floating-point functional unit full chaining among them. Assume bothVMIPS VMIPS-II two load-store units. extra functional units increased complexity assigning operations units, over- heads (T loopand start) doubled VMIPS-II. a.[15]<G.1–G.6>Find number clock cycles code sequence 1 VMIPS. b.[20]<G.1–G.6>Find number clock cycles code sequence 1 VMIPS-II. compare VMIPS? c.[15]<G.1–G.6>Find number clock cycles code sequence 2 VMIPS. d.[15]<G.1–G.6>Find number clock cycles code sequence 2 VMIPS-II. compare VMIPS? G.9 [20]<G.5>Here tricky piece code two-dimensional arrays. loop dependences? loops written parallel? so, how?Rewrite source code clear loop vectorized, possible. do290 j=2,n do290 i=2,j aa(i,j)=aa(i-1,j)*aa(i-1,j)+bb(i,j) 290 continueG-32 ■Appendix G Vector Processors DepthG.10 [12/15] <G.5>Consider following loop: do10 i=2,n A(i)=B 10 C(i)=A(i - 1) a.[12]<G.5>Show loop-carried dependence code fragment. b.[15]<G.5>Rewrite code FORTRAN vectorized two separate vector sequences. G.11 [15/25/25] <G.5>As saw Section G.5 , loop structures easily vectorized. One common structure reduction —a loop reduces array single value repeated application operation. special case arecurrence. common example occurs dot product: dot=0.0 do10 i=1,64 10 dot=dot+A(i) * B(i) loop obvious loop-carried dependence (on dot) cannot vec- torized straightforward fashion. first thing good vectorizing compilerwould split loop separate vectorizable portion recurrenceand perhaps rewrite loop as: 10 i=1,64 10 dot(i)=A(i) * B(i) 20 i=2,64 20 dot(1)=dot(1)+dot(i) variable dot expanded vector; transformation called scalar expansion . try vectorize second loop either relying strictly compiler (part (a)) hardware support well (part (b)). animportant caveat use vector techniques reduction. make reductionwork, relying associativity operator used reduc-tion. rounding finite range, however, floating-point arithmetic isnot strictly associative. reason, compilers require programmer indicate whether associativity used efficiently compile reductions. a.[15]<G.5>One simple scheme compiling loop recurrence add sequences progressively shorter vectors —two 32-element vectors, two 16-element vectors, on. technique called recursive doubling . faster operations scalar mode. Show FORTRAN code would look execution second loop preced-ing code fragment using recursive doubling. b.[25]<G.5>In vector processors, vector registers addressable, operands vector operation may two different parts vectorregister. allows another solution reduction, called partial sums .Exercises ■G-33The key idea partial sums reduce vector msums mis total latency vector functional unit, including operand read write times. Assume VMIPS vector registers addressable (e.g., youcan initiate vector operation operand V1(16), indicating inputoperand began element 16). Also, assume total latency adds,including operand read write, eight cycles. Write VMIPS codesequence reduces contents V1 eight partial sums. donewith one vector operation. c.[25]<G.5>Discuss adding extension part (b) would affect machine multiple lanes. G.12 [40]<G.3–G.4>Extend MIPS simulator VMIPS simulator, including ability count clock cycles. Write short benchmark programs MIPSand VMIPS assembly language. Measure speedup VMIPS, percentageof vectorization, usage functional units. G.13 [50]<G.5>Modify MIPS compiler include dependence checker. Run scientific code loops measure percentage state- ments could vectorized. G.14 [Discussion] proponents vector processors might argue vector processors provided best path ever-increasing amounts processor power focusing attention boosting peak vector performance. Otherswould argue emphasis peak performance misplaced anincreasing percentage programs dominated nonvector performance.(Remember Amdahl ’s law?) proponents would respond programmers work make programs vectorizable. think thisargument?G-34 ■Appendix G Vector Processors DepthH.1 Introduction: Exploiting Instruction-Level Parallelism Statically H-2 H.2 Detecting Enhancing Loop-Level Parallelism H-2 H.3 Scheduling Structuring Code Parallelism H-12 H.4 Hardware Support Exposing Parallelism: Predicated Instructions H-23 H.5 Hardware Support Compiler Speculation H-27 H.6 Intel IA-64 Architecture Itanium Processor H-32 H.7 Concluding Remarks H-43H Hardware Software VLIW EPIC EPIC approach based application massive resources. resources include load-store, computational, branch units, well larger, lower-latency caches would required superscalar processor. Thus, IA-64 gambles that, future, power critical limitation, massive resources, along machinery exploit them, penalize performance adverse effect clock speed, path length, CPI factors. M. Hopkins commentary EPIC approach IA-64 architecture (2000)H.1 Introduction: Exploiting Instruction-Level Parallelism Statically chapter, discuss compiler technology increasing amount par- allelism exploit program well hardware support thesecompiler techniques. next section defines loop parallel, depen-dence prevent loop parallel, techniques eliminating sometypes dependences. following section discusses topic scheduling codeto improve parallelism. two sections serve introduction thesetechniques. attempt explain details ILP-oriented compiler techniques, since would take hundreds pages, rather 20 allotted.Instead, view material providing general background enablethe reader basic understanding compiler techniques used exploitILP modern computers. Hardware support compiler techniques greatly increase effectiveness, Sections H.4 andH.5explore support. IA-64 repre- sents culmination compiler hardware ideas exploiting parallelism statically includes support many concepts proposed researchers decade research area compiler-based instruction-level parallelism. Section H.6 provides description perfor- mance analyses Intel IA-64 architecture second-generation imple-mentation, Itanium 2. core concepts exploit statically based techniques —finding par- allelism, reducing control data dependences, using speculation —are techniques saw exploited Chapter 3 using dynamic techniques. key difference techniques appendix applied compile time compiler, rather runtime hardware. advantages compiletime techniques primarily two: burden runtime execution anyinefficiency, take account wider range program aruntime approach might able incorporate. example latter, nextsection shows compiler might determine entire loop executedin parallel; hardware techniques might might able find parallel-ism. major disadvantage static approaches use compile time information. Without runtime information, compile time techniques must often conservative assume worst case. H.2 Detecting Enhancing Loop-Level Parallelism Loop-level parallelism normally analyzed source level close it, whilemost analysis ILP done instructions generated compiler. Loop-level analysis involves determining dependences exist among operands loop across iterations loop. now, willH-2 ■Appendix H Hardware Software VLIW EPICconsider data dependences, arise operand written point read later point. Name dependences also exist may removed renaming techniques like explored Chapter 3 . analysis loop-level parallelism focuses determining whether data accesses later iterations dependent data values produced earlieriterations; dependence called loop-carried dependence . examples considered Section 3.2 loop-carried dependences and,thus, loop-level parallel. see loop parallel, let us first look thesource representation: for(i=1000; >0; i=i-1) x[i] = x[i] + s; loop, dependence two uses x[i] , depen- dence within single iteration loop carried. dependence successive uses iin different iterations, loop carried, dependence involves induction variable easily recognized elim-inated. saw examples eliminate dependences involving inductionvariables loop unrolling Section 3.2, look additional exam-ples later section. finding loop-level parallelism involves recognizing structures loops, array references, induction variable computations, compiler analysis easily near source level, opposed machine-code level. Let ’s look complex example. Example Consider loop like one: for(i=1; <=100; i=i+1) { A[i+1] = A[i] + C[i]; /* S1 */B[i+1] = B[i] + A[i+1]; /* S2 */ } Assume A,B, Care distinct, nonoverlapping arrays. (In practice, arrays may sometimes may overlap. arrays may bepassed parameters procedure, includes loop, determining whetherarrays overlap identical often requires sophisticated, interprocedural analysis program.) data dependences among statements S1 S2 loop? Answer two different dependences: 1.S1 uses value computed S1 earlier iteration, since iteration icom- putes A[i+1] , read iteration i+1. true S2 B[i] andB[i+1] . 2.S2 uses value, A[i+1] , computed S1 iteration.H.2 Detecting Enhancing Loop-Level Parallelism ■H-3These two dependences different different effects. see differ, let ’s assume one dependences exists time. dependence statement S1 earlier iteration S1, depen-dence loop carried. dependence forces successive iterations loop toexecute series. second dependence (S2 depending S1) within iteration loop carried. Thus, dependence, multiple iterations loop could execute parallel, long pair statements iteration keptin order. saw type dependence example Section 3.2, whereunrolling able expose parallelism. also possible loop-carried dependence prevent par- allelism, next example shows. Example Consider loop like one: for(i=1; <=100; i=i+1) { A[i] = A[i] + B[i]; /* S1 */B[i+1] = C[i] + D[i]; /* S2 */ } dependences S1 S2? loop parallel? not, show make parallel. Answer Statement S1 uses value assigned previous iteration statement S2, loop-carried dependence S2 S1. Despite loop-carrieddependence, loop made parallel. Unlike earlier loop, depen-dence circular: Neither statement depends itself, and, although S1depends S2, S2 depend S1. loop parallel writtenwithout cycle dependences, since absence cycle means thedependences give partial ordering statements. Although circular dependences loop, must trans- formed conform partial ordering expose parallelism. Two obser-vations critical transformation: 1.There dependence S1 S2. were, would cycle dependences loop would parallel. Since otherdependence absent, interchanging two statements affect exe-cution S2. 2.On first iteration loop, statement S1 depends value B[1] computed prior initiating loop. two observations allow us replace loop following code sequence:H-4 ■Appendix H Hardware Software VLIW EPICA[1] = A[1] + B[1]; for(i=1; <=99; i=i+1) { B[i+1] = C[i] + D[i];A[i+1] = A[i+1] + B[i+1]; }B[101] = C[100] + D[100]; dependence two statements longer loop carried, iter- ations loop may overlapped, provided statements iteration arekept order. analysis needs begin finding loop-carried dependences. dependence information inexact , sense tells us depen- dence may exist. Consider following example: for(i=1;i <=100;i=i+1) { A[i] = B[i] + C[i]D[i] = A[i] * E[i] } second reference example need translated load instruc- tion, since know value computed stored previous state-ment; hence, second reference Acan simply reference register Awas computed. Performing optimization requires knowing two references always memory address inter- vening access location. Normally, data dependence analysis tellsthat one reference maydepend another; complex analysis required determine two references must exact address. example above, simple version analysis suffices, since two references basic block. Often loop-carried dependences form recurrence: for(i=2;i <=100;i=i+1) { Y[i] = Y[i-1] + Y[i]; } recurrence variable defined based value variable earlier iteration, often one immediately preceding, fragment. Detecting recurrence important two reasons: architectures (espe- cially vector computers) special support executing recurrences, somerecurrences source reasonable amount parallelism. see howthe latter true, consider loop: for(i=6;i <=100;i=i+1) { Y[i] = Y[i-5] + Y[i]; }H.2 Detecting Enhancing Loop-Level Parallelism ■H-5On iteration i,the loop references element i/C05. loop said dependence distance 5. Many loops carried dependences depen- dence distance 1. larger distance, potential parallelism canbe obtained unrolling loop. example, unroll first loop, witha dependence distance 1, successive statements dependent one another;there still parallelism among individual instructions, much. Ifwe unroll loop dependence distance 5, sequence fivestatements dependences, thus much ILP. Although manyloops loop-carried dependences dependence distance 1, cases larger distances arise, longer distance may well provide enough paral- lelism keep processor busy. Finding Dependences Finding dependences program important part three tasks: (1) goodscheduling code, (2) determining loops might contain parallelism, (3)eliminating name dependences. complexity dependence analysis arises presence arrays pointers languages like C C++, pass-by-reference parameter passing FORTRAN. Since scalar variable refer-ences explicitly refer name, usually analyzed quite easily, withaliasing pointers reference parameters causing complicationsand uncertainty analysis. compiler detect dependences general? Nearly dependence analysis algorithms work assumption array indices affine . sim- plest terms, one-dimensional array index affine written form a/C2i+b, aandbare constants iis loop index variable. index multidimensional array affine index dimension affine. Sparsearray accesses, typically form x[y[i]] , one major examples nonaffine accesses. Determining whether dependence two references array loop thus equivalent determining whether two affine functions canhave value different indices bounds loop. Forexample, suppose stored array element index value a/C2i+ band loaded array index value c/C2i+d, iis for-loop index variable runs mton.A dependence exists two condi- tions hold: 1.There two iteration indices, jandk, within limits loop. is, m/C20j/C20n, m/C20k/C20n. 2.The loop stores array element indexed a/C2j+band later fetches thatsame array element indexed c/C2k+d. is, a/C2j+b¼c/C2 k+d.H-6 ■Appendix H Hardware Software VLIW EPICIn general, cannot determine whether dependence exists compile time. example, values a,b,c, dmay known (they could values arrays), making impossible tell dependence exists. cases, thedependence testing may expensive decidable compile time. Forexample, accesses may depend iteration indices multiple nested loops.Many programs, however, contain primarily simple indices a, b, c , dare constants. cases, possible devise reasonable compile time testsfor dependence. example, simple sufficient test absence dependence thegreatest common divisor (GCD) test. based observation loop-carried dependence exists, GCD ( c,a) must divide ( d/C0b). (Recall integer, x, divides another integer, y, get integer quotient division y/xand remainder.) Example Use GCD test determine whether dependences exist following loop: for(i=1; <=100; i=i+1) { X[2*i+3] = X[2*i] * 5.0; } Answer Given values a¼2,b¼3,c¼2, d¼0, GCD( a,c)¼2, d/C0b¼ /C03. Since 2 divide /C03, dependence possible. GCD test sufficient guarantee dependence exists; however, cases GCD test succeeds dependence exists. Thiscan arise, example, GCD test take loop bounds intoaccount. general, determining whether dependence actually exists NP complete. practice, however, many common cases analyzed precisely low cost. Recently, approaches using hierarchy exact tests increasing generalityand cost shown accurate efficient. (A test exact precisely determines whether dependence exists. Although general caseis NP complete, exist exact tests restricted situations muchcheaper.) addition detecting presence dependence, compiler wants classify type dependence. cl assification allows compiler recog- nize name dependences eliminate compile time renaming andcopying. Example following loop multiple types dependences. Find true depen- dences, output dependences, antidependences, eliminate output depen- dences antidependences renaming.H.2 Detecting Enhancing Loop-Level Parallelism ■H-7for(i=1; <=100; i=i+1) { Y[i] = X[i] / c; /* S1 */X[i] = X[i] + c; /* S2 */Z[i] = Y[i] + c; /* S3 */Y[i] = c - Y[i]; /* S4 */ } Answer following dependences exist among four statements: 1.There true dependences S1 S3 S1 S4 Y[i] . loop carried, prevent loop consid- ered parallel. dependences force S3 S4 wait S1 tocomplete. 2.There antidependence S1 S2, based X[i] . 3.There antidependence S3 S4 Y[i] . 4.There output dependence S1 S4, based Y[i] . following version loop eliminates false (or pseudo) dependences: for(i=1; <=100; i=i+1 { /* renamed remove output dependence */ T[i] = X[i] / c;/* X renamed X1 remove antidependence */X1[i] = X[i] + c;/* renamed remove antidependence */Z[i] = T[i] + c;Y[i] = c - T[i]; } loop, variable Xhas renamed X1. code follows loop, compiler simply replace name XbyX1. case, renaming require actual copy operation done substituting names reg-ister allocation. cases, however, renaming require copying. Dependence analysis critical technology exploiting parallelism. instruction level, provides information needed interchange memory referenceswhen scheduling, well determine benefits unrolling loop. Fordetecting loop-level parallelism, dependence analysis basic tool. Effectively compiling programs either vector computers multiprocessors depends criti- cally analysis. major drawback dependence analysis appliesonly limited set circumstances —namely, among references within sin- gle loop nest using affine index functions. Thus, wide variety sit-uations array-oriented dependence analysis cannot tell us might want know, including following:H-8 ■Appendix H Hardware Software VLIW EPIC■When objects referenced via pointers rather array indices (but see discussion below) ■When array indexing indirect another array, happens many representations sparse arrays ■When dependence may exist value inputs exist inactuality code run since inputs never take values ■When optimization depends knowing possibility adependence needs know write variable read variable depend deal issue analyzing programs pointers, another type anal- ysis, often called points-to analysis, required (see Wilson Lam [1995]). key question want answered dependence analysis pointers whether twopointers designate address. case complex dynamic data struc-tures, problem extremely difficult. example, may want know whethertwopointerscanreferencethe nodeinalistatagivenpointinaprogram,whichin general undecidable practice extremely difficult answer. may, how- ever, able answer simpler question: two pointers designate nodes thesame list, even may separate nodes? restricted analysis still quite useful scheduling memory accesses performed pointers. basic approach used points-to analysis relies information three major sources: 1.Type information, restricts pointer point to. 2.Information derived object allocated address object taken, used restrict pointer point to. example, ifpalways points object allocated given source line qnever points object, pandqcan never point object. 3.Information derived pointer assignments. example, pmay assigned value q, pmay point anything qpoints to. several cases analyzing pointers successfully applied extremely useful: ■When pointers used pass address object parameter, ispossible use points-to analysis determine possible set objects refer-enced pointer. One important use determine two pointer parameters may designate object. ■When pointer point one several types, sometimes possible todetermine type data object pointer designates different parts program. ■It often possible separate pointers may point local objectversus global one.H.2 Detecting Enhancing Loop-Level Parallelism ■H-9There two different types limitations affect ability accu- rate dependence analysis large progra ms. first type limitation arises restrictions analysis algorithms. Often, limited lackof applicability analysis rather shortcoming dependence analysisper se . example, dependence analysis fo r pointers essentially impossible programs use pointers arbitrary fashion —such arithmetic pointers. second limitation need analyze behavior across procedure bound- aries get accurate information. example, procedure accepts two param- eters pointers, determining whether values could requires analyzing across procedure boundaries. type analysis, called interproce- dural analysis , much difficult complex analysis within single procedure. Unlike case analyzing array indices within single loop nest,points-to analysis usually requires interprocedural analysis. reason thisis simple. Suppose analyzing program segment two pointers; theanalysis know anything two pointers start programsegment, must conservative assume worst case. worst case two pointers may designate object, guaranteed designate object. worst case likely propagate anal-ysis, producing useless information. practice, getting fully accurate interproce-dural information usually expensive real programs. Instead, compilersusually use approximations interprocedural analysis. result infor-mation may inaccurate useful. Modern programming languages use strong typing, Java, make analysis dependences easier. time extensive use procedures structure programs, well abstract data types, makes analysis diffi- cult. Nonetheless, expect continued advances analysis algorithms, com-bined increasing importance pointer dependency analysis, meanthat continued progress important problem. Eliminating Dependent Computations Compilers reduce impact dependent computations achieve moreinstruction-level parallelism (ILP). key technique eliminate reduce adependent computation back substitution, increases amount par-allelism sometimes increases amount computation required. tech-niques applied within basic block within loops, describe differently. Within basic block, algebraic simplifications expressions optimi- zation called copy propagation , eliminates operations copy values, used simplify sequences like following: DADDUI R1,R2,#4DADDUI R1,R1,#4H-10 ■Appendix H Hardware Software VLIW EPICto DADDUI R1,R2,#8 assuming use R1. fact, techniques used reduce multiple increments array indices loop unrolling move incre-ments across memory addresses Section 3.2 examples type ofoptimization. examples, computations actually eliminated, also pos- sible may want increase parallelism code, possibly even increasing number operations. optimizations called tree height reduction reduce height tree structure representing computation, making wider shorter. Consider following codesequence: ADD R1,R2,R3 ADD R4,R1,R6ADD R8,R4,R7 Notice sequence requires least three execution cycles, since instructions depend immediate predecessor. taking advantage asso-ciativity, transform code rewrite ADD R1,R2,R3 ADD R4,R6,R7ADD R8,R1,R4 sequence computed two execution cycles. loop unrolling used, opportunities types optimizations occur frequently. Although arithmetic unlimited range precision associative, com- puter arithmetic associative, either integer arithmetic, limitedrange, floating-point arithmetic, range precision. Thus,using restructuring techniques sometimes lead erroneous behavior,although occurrences rare. reason, compilers require optimizations rely associativity explicitly enabled. loops unrolled, sort algebraic optimization important reduce impact dependences arising recurrences. Recurrences expressions whose value one iteration given function depends onthe previous iterations. loop recurrence unrolled, may ableto algebraically optimize unrolled loop, recurrence need beevaluated per unrolled iteration. One common type recurrence arises froman explicit program statement, as: sum = sum + x;H.2 Detecting Enhancing Loop-Level Parallelism ■H-11Assume unroll loop recurrence five times. let value xon five iterations given x1,x2,x3,x4, x5, write value sum end unroll as: sum = sum + x1 + x2 + x3 + x4 + x5; unoptimized, expression requires five dependent operations, rewritten as: sum = ((sum + x1) + (x2 + x3)) + (x4 + x5); evaluated three dependent operations. Recurrences also arise implicit calculations, associated array indexing. array index translates address computed based onthe loop index variable. Again, unrolling algebraic optimization, thedependent computations minimized. H.3 Scheduling Structuring Code Parallelism already seen one compiler technique, loop unrolling, useful touncover parallelism among instructions creating longer sequences straight-line code. two important techniques developed forthis purpose: software pipelining andtrace scheduling . Software Pipelining: Symbolic Loop Unrolling Software pipelining technique reorganizing loops iteration software-pipelined code made instructions chosen different iter-ations original loop. approach easily understood looking atthe scheduled code unrolled loop, appeared example inSection 2.2. scheduler example essentially interleaves instructions fromdifferent loop iterations, separate dependent instructions occurwithin single loop iteration. choosing instructions different iterations, dependent computations separated one another entire loop body, increasing possibility unrolled loop scheduled without stalls. software-pipelined loop interleaves instructions different iterations without unrolling loop, illustrated Figure H.1 . technique soft- ware counterpart Tomasulo ’s algorithm hardware. software- pipelined loop earlier example would contain one load, one add, onestore, different iteration. also start-up code neededbefore loop begins well code finish loop completed. ignore discussion, simplicity.H-12 ■Appendix H Hardware Software VLIW EPICExample Show software-pipelined version loop, increments elements array whose starting address R1 contents F2: Loop: L.D F0,0(R1) ADD.D F4,F0,F2S.D F4,0(R1)DADDUI R1,R1,#-8BNE R1,R2,Loop may omit start-up clean-up code. Answer Software pipelining symbolically unrolls loop selects instructions iteration. Since unrolling symbolic, loop overhead instructions (theDADDUI andBNE) need replicated. ’s body unrolled loop without overhead instructions, highlighting instructions taken eachiteration: Iteration i: L.D F0,0(R1) ADD.D F4,F0,F2 S.D F4,0(R1) Iteration i+1: L.D F0,0(R1) ADD.D F4,F0,F2S.D F4,0(R1) Iteration i+2: L.D F0,0(R1) ADD.D F4,F0,F2S.D F4,0(R1)Software- pipelined iterationIteration 0Iteration 1Iteration 2Iteration 3Iteration 4 Figure H.1 software-pipelined loop chooses instructions different loop iter- ations, thus separating dependent instructions within one iteration theoriginal loop. start-up finish-up code correspond portions software-pipelined iteration.H.3 Scheduling Structuring Code Parallelism ■H-13The selected instructions different iterations put together loop loop control instructions: Loop: S.D F4,16(R1) ;stores M[i] ADD.D F4,F0,F2 ;adds M[i-1] L.D F0,0(R1) ;loads M[i-2]DADDUI R1,R1,#-8BNE R1,R2,Loop loop run rate 5 cycles per result, ignoring start-up clean- portions, assuming DADDUI scheduled ADD.D theL.D instruction, adjusted offset, placed branch delay slot. load store separated offsets 16 (two iterations), loopshould run two fewer iterations. Notice reuse registers (e.g., F4, F0,and R1) requires hardware avoid write read (WAR) hazards would occur loop. hazard problem case, since data-dependent stalls occur. looking unrolled version see start-up code finish-up code need be. start-up, need execute instructions cor-respond iteration 1 2 executed. instructions L.D iterations 1 2 ADD.D iteration 1. finish-up code, need execute instructions executed final two iterations. Theseinclude ADD.D last iteration S.D last two iterations. Register management software-pipelined loops tricky. previous example hard since registers written one loop iteration read next. cases, may need increase number iterationsbetween issue instruction result used. increase isrequired small number instructions loop body thelatencies large. cases, combination software pipelining loopunrolling needed. Software pipelining thought symbolic loop unrolling. Indeed, algorithms software pipelining use loop-unrolling algorithms figure software-pipeline loop. major advantage software pipelining straight loop unrolling software pipelining consumes less code space.Software pipelining loop unrolling, addition yielding better scheduledinner loop, reduce different type overhead. Loop unrolling reduces theoverhead loop —the branch counter update code. Software pipelining reduces time loop running peak speed per loop atthe beginning end. unroll loop 100 iterations constant num-ber times, say, 4, pay overhead 100/4 ¼25 times —every time inner unrolled loop initiated. Figure H.2 shows behavior graphically. techniques attack two different types overhead, best performancecan come both. practice, compilation using software pipelining isquite difficult several reasons: Many loops require significant transformationH-14 ■Appendix H Hardware Software VLIW EPICbefore software pipelined, trade-offs terms overhead versus efficiency software-pipelined loop complex, issue register management creates additional complexities. help deal last two ofthese issues, IA-64 added extensive hardware sport software pipelining.Although hardware make efficient apply software pipelining,it eliminate need complex compiler support, need makedifficult decisions best way compile loop. Global Code Scheduling Section 3.2 examined use loop unrolling code scheduling toimprove ILP. techniques Section 3.2 work well loop body isstraight-line code, since resulting unrolled loop looks like single basic block.Similarly, software pipelining works well body single basic block, since easier find repeatable schedule. body unrolled loop contains internal control flow, however, scheduling code much com-plex. general, effective scheduling loop body internal control flow willrequire moving instructions across branches, global code scheduling.In section, first examine challenge limitations global code(a) Software pipelining Proportional number unrollsOverlap unrolled iterationsTimeWind-down codeStart-up code (b) Loop unrollin gTimeNumber overlapped operations Number overlapped operations Figure H.2 execution pattern (a) software-pipelined loop (b) unrolled loop. shaded areas times loop running max- imum overlap parallelism among instructions. occurs beginning end software-pipelined loop. unrolled loop occurs m/n times loop total miterations unrolled ntimes. block represents unroll niterations. Increasing number unrollings reduce start-up clean-up overhead. overhead one iteration overlaps overhead thenext, thereby reducing impact. total area polygonal region case same, since total number operations execution rate multiplied time.H.3 Scheduling Structuring Code Parallelism ■H-15scheduling. Section H.4 examine hardware support eliminating control flow within inner loop, examine two compiler techniques used eliminating control flow viable approach. Global code scheduling aims compact code fragment internal control structure shortest possible sequence preserves data controldependences. data dependences force partial order operations, whilethe control dependences dictate instructions across code cannot easilymoved. Data dependences overcome unrolling and, case memoryoperations, using dependence analysis determine two references refer address. Finding shortest possible sequence piece code means finding shortest sequence critical path , longest sequence dependent instructions. Control dependences arising loop branches reduced unrolling. Global code scheduling reduce effect control dependences arising fromconditional nonloop branches moving code. Since moving code across brancheswill often affect frequency execution code, effectively using globalcode motion requires estimates relative frequency different paths. Although global code motion cannot guarantee faster code, frequency infor- mation accurate, compiler determine whether code movement islikely lead faster code. Global code motion important since many inner loops contain conditional statements. Figure H.3 shows typical code fragment, may thought iteration unrolled loop, highlights common control flow. A(i) = A(i) + B(i) F X B(i) =A(i) = 0? C(i) = Figure H.3 code fragment common path shaded gray. Moving assignments B C requires complex analysis straight-line code. section focus scheduling code segment efficiently without hardwareassistance. Predication conditional instructions, discuss next section,provide another way schedule code.H-16 ■Appendix H Hardware Software VLIW EPICEffectively scheduling code could require move assignments B C earlier execution sequence, test A. global code motion must satisfy set constraints legal. addition, movement thecode associated B, unlike associated C, speculative: speedthe computation path containing code would taken. perform movement B, must ensure neither data flow exception behavior changed. Compilers avoid changing exception behav-ior moving certain classes instructions, memory references, thatcan cause exceptions. Section H.5 , see hardware support allows opportunities speculative code motion removes control dependences. Although enhanced support speculation make possible exploremore opportunities, difficulty choosing best compile coderemains complex. compiler ensure assignments B C moved without affecting data flow? see ’s involved, let ’s look typical code generation sequence flowchart Figure H.3 . Assuming addresses A, B, C R1, R2, R3, respectively, sequence: LD R4,0(R1) ;load LD R5,0(R2) ;load BDADDU R4,R4,R5 ;Add ASD R4,0(R1) ;Store A...BNEZ R4,elsepart ;Test A... ;then part SD ...,0(R2) ;Stores B ...J join ;jump else elsepart: ... ;else part X ;code X... join: ... ;after SD ...,0(R3) ;store C[i] Let’s first consider problem moving assignment B BNEZ instruction. Call last instruction assign B statement i. B referenced assigned either code segment X statement, call referencing instruction j.If instruction j, moving assignment B change data flow program. particular,moving assignment B cause jto become data dependent moved version assignment B rather i, joriginally depended. could imagine clever schemes allow B moved even value used: example, first case, could make shadow copyof B statement use shadow copy X. schemes usu-ally avoided, complex implement willH.3 Scheduling Structuring Code Parallelism ■H-17slow program trace selected optimal operations end requiring additional instructions execute. Moving assignment C first branch requires two steps. First, assignment moved join point else part portion corre-sponding part. movement makes instructions C controldependent branch means execute else path, whichis infrequent path, chosen. Hence, instructions data dependent theassignment C, execute code fragment, affected. Toensure correct value computed instructions, copy made instructions compute assign C else path. Second, move C part branch across branch condition, affectany data flow branch condition. C moved test, copy ofC else branch usually eliminated, since redundant. see example global code scheduling subject many constraints. observation led designers provide hardware support tomake code motion easier, Sections H.4 andH.5explores support detail. Global code scheduling also requires complex trade-offs make code motion decisions. example, assuming assignment B moved theconditional branch (possibly compensation code alternativebranch), movement make code run faster? answer is, possibly!Similarly, moving copies C else branches makes codeinitially bigger! compiler successfully move computation acrossthe test likely benefit. Consider factors compiler would consider moving computation assignment B: ■What relative execution frequencies case else case branch? Ifthe thencase ismuchmorefrequent, codemotionmay bebeneficial.If not, less likely, although impossible, consider moving code. ■What cost executing computation assignment B thebranch? may number empty instruction issue slots thecode branch instructions B placed theseslots would otherwise go empty. opportunity makes computationof B “free”at least first order. ■How movement B change execution time case? B start critical path case, moving may highlybeneficial. ■Is B best code fragment moved branch? itcompare moving C statements within case? ■What cost compensation code may necessary elsecase? effectively code scheduled, impact execution time?H-18 ■Appendix H Hardware Software VLIW EPICAs see partial list, global code scheduling extremely complex problem. trade-offs depend many factors, individual decisions globally schedule instructions highly interdependent. Even choos-ing instructions start considering candidates global code motion iscomplex! try simplify process, several different methods global code sched- uling developed. two methods briefly explore rely asimple principle: Focus attention compiler straight-line code seg-ment representing estimated frequently executed code path. Unrolling used generate straight-line code, but, course, complexity arises conditional branches handled. cases, effectivelystraightened choosing scheduling frequent path. Trace Scheduling: Focusing Critical Path Trace scheduling useful processors large number issues per clock,where conditional predicated execution (see Section H.4 ) inappropriate unsupported, simple loop unrolling may sufficient uncover enough ILP keep processor busy. Trace scheduling way orga-nize global code motion process, simplify code scheduling incur-ring costs possible code motion less frequent paths. cangenerate significant overheads designated infrequent path, best used profile information indicates significant differences frequency betweendifferent paths profile information highly indicative programbehavior independent input. course, limits effective applicability certain classes programs. two steps trace scheduling. first step, called trace selection, tries find likely sequence basic blocks whose operations put togetherinto smaller number instructions; sequence called trace . Loop unrol- ling used generate long traces, since loop branches taken high prob-ability. Additionally, using static branch prediction, conditional branchesare also chosen taken taken, resultant trace straight-linesequence resulting concatenating many basic blocks. If, example, pro- gram fragment shown Figure H.3 corresponds inner loop highlighted path much frequent, loop unwound fourtimes, primary trace would consist four copies shaded portion ofthe program, shown Figure H.4 . trace selected, second process, called trace compaction , tries squeeze trace small number wide instructions. Trace compaction iscode scheduling; hence, attempts move operations early asequence (trace), packing operations wide instructions (or issue packets) possible. advantage trace scheduling approach simplifies deci- sions concerning global code motion. particular, branches viewed jumpsinto selected trace, assumed probable path.H.3 Scheduling Structuring Code Parallelism ■H-19A(i) = A(i) + B(i) F B(i) =A(i) = 0? C(i) = Trace entrance A(i) = A(i) + B(i) TF B(i) =A(i) = 0? C(i) =Trace exitTrace exit Trace entrance A(i) = A(i) + B(i) TF B(i) =A(i) = 0? C(i) =Trace exitTrace entrance A(i) = A(i) + B(i) TF B(i) = C(i) =A(i) = 0? Trace exit Figure H.4 trace obtained assuming program fragment Figure H.3 inner loop unwinding four times, treating shaded portion Figure H.3 likely path. trace exits correspond jumps frequent path, trace entrances correspond returns trace.H-20 ■Appendix H Hardware Software VLIW EPICWhen code moved across trace entry exit points, additional bookkeep- ing code often needed entry exit point. key assumption trace much probable alternatives cost book-keeping code need deciding factor: instruction moved andthereby make main trace execute faster, moved. Although trace scheduling successfully applied scientific code intensive loops accurate profile data, remains unclear whether thisapproach suitable programs less simply characterized less loopintensive. programs, significant overheads compensation code may make trace scheduling unattractive approach, or, best, effective use extremely complex compiler. Superblocks One major drawbacks trace scheduling entries exits themiddle trace cause significant complications, requiring compiler gen-erate track compensation code often making difficult assess cost code. Superblocks formed process similar used traces, form extended basic blocks, restricted single entrypoint allow multiple exits. superblocks single entry point, compacting superblock easier compacting trace since code motion across exit need beconsidered. earlier example, would form superblocks contained onlyone entrance; hence, moving C would easier. Furthermore, loops asingle loop exit based count (for example, loop loop exit loop termination condition), resulting superblocks one exit well one entrance. blocks scheduled easily. superblock one entrance constructed? answer usetail duplication create separate block corresponds portion trace entry. previous example, unrolling loop wouldcreate exit superblock residual loop handles remainingiterations. Figure H.5 shows superblock structure code fragment Figure H.3 treated body inner loop unrolled four times. residual loop handles iterations occur superblock exited, which, turn, occurs unpredicted path selected. expected frequency ofthe residual loop still high, superblock could created loop well. superblock approach reduces complexity bookkeeping sched- uling versus general trace generation approach may enlarge code sizemore trace-based approach. Like trace scheduling, superblock schedulingmay appropriate techniques (e.g., conversion) fail. Evenin cases, assessing cost code duplication may limit usefulness approach certainly complicate compilation process. Loop unrolling, software pipelining, trace scheduling, superblock sched- uling aim trying increase amount ILP exploited pro-cessor issuing one instruction every clock cycle. effectiveness ofH.3 Scheduling Structuring Code Parallelism ■H-21C(i) =A(i) = A(i) + B(i) F B(i) =A(i) = 0? C(i) = A(i) = A(i) + B(i) F B(i) =A(i) = 0? C(i) = A(i) = A(i) + B(i) F B(i) =A(i) = 0? C(i) = A(i) = A(i) + B(i) F B(i) =A(i) = 0?A(i) = A(i) + B(i) F X B(i) =A(i) = 0? C(i) = Execute n timesSuperblock exit n = 4 Superblock exit n = 3 Superblock exit n = 2 Superblock exit n = 1 Figure H.5 superblock results unrolling code Figure H.3 four times creating superblock.H-22 ■Appendix H Hardware Software VLIW EPICeach techniques suitability various architectural approaches among hottest topics actively pursued researchers designers high-speed processors. H.4 Hardware Support Exposing Parallelism: Predicated Instructions Techniques loop unrolling, software pipelining, trace scheduling used increase amount parallelism available behavior branches fairly predictable compile time. behavior branches well known, compiler techniques alone may able uncover muchILP. cases, control dependences may severely limit amount par-allelism exploited. overcome problems, architect extendthe instruction set include conditional orpredicated instructions . instruc- tions used eliminate branches, converting control dependence adata dependence potentially improving performance. approaches use-ful either hardware-intensive schemes Chapter 3 software- intensive approaches discussed appendix, since cases predication used eliminate branches. concept behind conditional instructions quite simple: instruction refers condition, evaluated part instruction execution. Ifthe condition true, instruction executed normally; condition false,the execution continues instruction no-op. Many newer architec-tures include form conditional instructions. common example ofsuch instruction conditional move, moves value one register another condition true. instruction used completely elim- inate branch simple code sequences. Example Consider following code: (A==0) {S=T;} Assuming registers R1,R2, R3hold values A,S,a n T, respec- tively, show code statement branch theconditional move. Answer straightforward code using branch statement (remember assuming normal rather delayed branches) BNEZ R1,L ADDU R2,R3,R0 L: Using conditional move performs move third operand equal zero, implement statement one instruction: CMOVZ R2,R3,R1H.4 Hardware Support Exposing Parallelism: Predicated Instructions ■H-23The conditional instruction allows us convert control dependence present branch-based code sequence data dependence. (This transformation alsoused vector computers, called conversion .) pipelined pro- cessor, moves place dependence must resolved nearthe front pipeline, resolved branches, end pipeline,where register write occurs. One obvious use conditional move implement absolute value func- tion: = abs (B) , implemented (B<0) {A=-B;} else {A=B;} . statement implemented pair conditional moves, one unconditional move (A=B) one conditional move (A=-B) . example compilation absolute value, conditional moves used change control dependence data dependence. Thisenables us eliminate branch possibly improve pipeline behavior.As issue rates increase, designers faced one two choices: executemultiple branches per clock cycle find method eliminate branches toavoid requirement. Handling multi ple branches per clock complex, since one branch must control dependent other. difficulty accurately predicting two branch outcomes, updating prediction tables, executingthe correct sequence far caused designers avoid processors thatexecute multiple branches per clock. Conditional moves predicated instruc-tions provide way reducing branch pressure. addition, conditionalmove often eliminate branch hard predict, increasing thepotential gain. Conditional moves simplest form conditional predicated instructions and, although useful short sequences, limitations. par- ticular, using conditional move elimi nate branches guard execution large blocks code inefficient, since many conditional moves mayneed introduced. remedy inefficiency using conditional moves, architectures support full predication, whereby execution instructions controlledby predicate. predicate false, instruction becomes no-op. Fullpredication allows us simply convert large blocks code branch depen- dent. example, if-then-else statement within loop entirely converted predicated execution, code case executes valueof condition true code else case executes value thecondition false. Predication particularly valuable global code scheduling,since eliminate nonloop branches, significantly complicate instructionscheduling. Predicated instructions also used speculatively move instruction time critical, may cause exception moved guarding branch. Although possible conditional move, costly.H-24 ■Appendix H Hardware Software VLIW EPICExample code sequence two-issue superscalar issue combination one memory reference one ALU operation, branch itself, every cycle: First instruction slot Second instruction slot LW R1,40 (R2)ADD R3,R4,R5 ADD R6,R3,R7 BEQZ R10,LLW R8,0 (R10) LW R9,0(R8) sequence wastes memory operation slot second cycle incur data dependence stall branch taken, since second LWafter branch depends prior load. Show code improved using apredicated form LW. Answer Call predicated version load word LWC assume load occurs unless third operand 0. LWimmediately following branch converted anLWC moved second issue slot: First instruction slot Second instruction slot LW R1,40(R2) ADD R3,R4,R5 LWC R8,0(R10),R10 ADD R6,R3,R7BEQZ R10,LLW R9,0(R8) improves execution time several cycles since eliminates one instruc- tion issue slot reduces pipeline stall last instruction sequence.Of course, compiler mispredicted branch, predicated instruction willhave effect improve running time. transforma-tion speculative. sequence following branch short, entire block code might converted predicated execution branch eliminated. convert entire code segment predicated execution specula- tively move instruction make predicted, remove control dependence. Correct code generation conditional execution predicated instructionsensure maintain data flow enforced branch. ensure thatthe exception behavior also maintained, predicated instruction must gen-erate exception predicate false. property causing exceptions isH.4 Hardware Support Exposing Parallelism: Predicated Instructions ■H-25quite critical, previous example shows: register R10 contains zero, instruction LW R8,0(R10) executed unconditionally likely cause protec- tion exception, exception occur. course, condition issatisfied (i.e., R10 zero), LWmay still cause legal resumable excep- tion (e.g., page fault), hardware must take exception knowsthat controlling condition true. major complication implementing predicated instructions deciding annul instruction. Predicated instructions may either annulled duringinstruction issue later pipeline commit results raise exception. choice disadvantage. predicated instructions annulled early pipeline, value controlling condition must known early toprevent stall data hazard. Since data-dependent branch conditions, whichtend less predictable, candidates conversion predicated execution,this choice lead pipeline stalls. potential data hazardstalls, design predicated execution (or conditional move) annuls instruc-tions early. Instead, existing processors annul instructions later pipeline,which means annulled instructions consume functional unit resources potentially negative impact performance. variety pipeline implementation techniques, forwarding, interact predicated instruc-tions, complicating implementation. Predicated conditional instructions extremely useful implementing short alternative control flows, eliminating unpredictable branches,and reducing overhead global code scheduling. Nonetheless, useful-ness conditional instructions limited several factors: ■Predicated instructions annulled (i.e., whose conditions false) stilltake processor resources. annulled predicated instruction requiresfetch resources minimum, processors functional unit executiontime. Therefore, moving instruction across branch making condi-tional slow program whenever moved instruction wouldnot normally executed. Likewise, predicating control-dependentportion code eliminating branch may slow processor thatcode would executed. important exception situations occurs cycles used moved instruction performed would idle anyway (as earlier superscalar example). Movingan instruction across branch converting code segment predicated exe-cution essentially speculating outcome branch. Conditionalinstructions make easier eliminate execution time takenby incorrect guess. simple cases, trade conditional movefor branch move, using conditional moves predication almostalways better. longer code sequences made conditional, benefits limited. ■Predicated instructions useful predicate evaluated early. condition evaluation predicated instructions cannot beH-26 ■Appendix H Hardware Software VLIW EPICseparated (because data dependences determining condition), conditional instruction may result stall data hazard. branch pre- diction speculation, stalls avoided, least branchesare predicted accurately. ■The use conditional instructions limited control flowinvolves simple alternative sequence. example, moving aninstruction across multiple branches requires making conditional bothbranches, requires two conditions specified requires additionalinstructions compute controlling predicate. capabilities notpresent, overhead conversion larger, reducing advantage. ■Conditional instructions may speed penalty compared uncon-ditional instructions. may show higher cycle count suchinstructions slower clock rate overall. conditional instructions moreexpensive, need used judiciously. reasons, many architectures included simple conditional instructions (with conditional move frequent), archi-tectures include conditional versions majority instructions. TheMIPS, Alpha, PowerPC, SPARC, Intel x86 (as defined Pentium proces-sor) support conditional move. IA-64 architecture supports full predicationfor instructions, see Section H.6 . H.5 Hardware Support Compiler Speculation saw Chapter 3 , many programs branches accurately pre- dicted compile time either program structure using profile. Insuch cases, compiler may want speculate either improve scheduling orto increase issue rate. Predicated instructions provide one method speculate,but really useful control dependences completely elim- inated conversion. many cases, would like move speculated instruc- tions branch also condition evaluation, andpredication cannot achieve this. speculate ambitiously requires three capabilities: 1.The ability compiler find instructions that, possible use reg- ister renaming, speculatively moved affect program data flow 2.The ability ignore exceptions speculated instructions, know exceptions really occur 3.The ability speculatively interchange loads stores, stores stores, may address conflicts first compiler capability, last two require hardware sup- port, explore next.H.5 Hardware Support Compiler Speculation ■H-27Hardware Support Preserving Exception Behavior speculate ambitiously, must able move type instruction still preserve exception behavior. key able observe thatthe results speculated sequence mispredicted used finalcomputation, speculated instruction cause exception. four methods investigated supporting ambi- tious speculation without introducing erroneous exception behavior: 1.The hardware operating system cooperatively ignore exceptions spec- ulative instructions. see later, approach preserves exception behavior correct programs, incorrect ones. approach maybe viewed unacceptable programs, used, pro-gram control, “fast mode ”in several processors. 2.Speculative instructions never raise exceptions used, checks introduced determine exception occur. 3.A set status bits, called poison bits , attached result registers written speculated instructions instructions cause exceptions. poisonbits cause fault normal instruction attempts use register. 4.A mechanism provided indicate instruction speculative, hardware buffers instruction result certain instruction nolonger speculative. explain schemes, need distinguish exceptions indi- cate program error would normally cause termination, memory pro- tection violation, handled normally resumed, page fault. Exceptions resumed accepted processed specula-tive instructions normal instructions. speculative instruc-tion executed, handling unneeded exception may havesome negative performance effects, cannot cause incorrect execution. Thecost exceptions may high, however, processors use hardwaresupport avoid taking exceptions, processors hardware specu-lation may take exceptions speculative mode, avoiding others instruction known speculative. Exceptions indicate program error occur correct programs, result program gets exception well defined, exceptperhaps program running debugging mode. exceptions arisein speculated instructions, cannot take exception know theinstruction longer speculative. simplest method preserving exceptions, hardware oper- ating system simply handle resumable exceptions exception occurs simply return undefined value exception would cause termina- tion. instruction generating terminating exception speculative,then program error. Note instead terminating program, theH-28 ■Appendix H Hardware Software VLIW EPICprogram allowed continue, although almost certainly generate incorrect results. instruction generating terminating exception speculative, program may correct speculative result simply unused; thus,returning undefined value instruction cannot harmful. schemecan never cause correct program fail, matter much speculation done.An incorrect program, formerly might received terminating excep-tion, get incorrect result. acceptable programs, assumingthe compiler also generate normal version program, notspeculate receive terminating exception. Example Consider following code fragment if-then-else statement form if(A==0 )A=B ; else A=A + 4 ; Ais at0(R3) andBis at0(R2) : LD R1,0(R3) ;load BNEZ R1,L1 ;test LD R1,0(R2) ;then clauseJ L2 ;skip else L1: DADDI R1,R1,#4 ;else clause L2: SD R1,0(R3) ;store Assume clause almost always executed. Compile code using compiler-based speculation. Assume R14 unused available. Answer new code: LD R1,0(R3) ;load LD R14,0(R2) ;speculative load B BEQZ R1,L3 ;other branch ifDADDI R14,R1,#4 ;the else clause L3: SD R14,0(R3) ;nonspeculative store clause completely speculated. introduce temporary register avoid destroying R1when Bis loaded; load speculative, R14 use- less. entire code segment executed, Awill R14. else clause could also compiled speculatively conditional move, thebranch highly predictable low cost, might slow code down, sincetwo extra instructions would always executed opposed one branch. scheme, necessary know instruction speculative. Indeed, helpful program error receives terminatingexception normal instruction; cases, instruction markedas speculative, program could terminated.H.5 Hardware Support Compiler Speculation ■H-29In method handling speculation, next one, renaming often needed prevent speculative instructions destroying live values. Renam- ing usually restricted register values. restriction, targets ofstores cannot destroyed stores cannot speculative. small number ofregisters cost spilling act one constraint amount spec-ulation. course, major constraint remains cost executing speculativeinstructions compiler ’s branch prediction incorrect. second approach preserving exception behavior speculating intro- duces speculative versions instructions generate terminating excep- tions instructions check exceptions. combination preserves exception behavior exactly. Example Show previous example coded using speculative load ( sLD) speculation check instruction (SPECCK) completely preserve exception behav- ior. Assume R14 unused available. Answer code achieves this: LD R1,0(R3) ;load sLD R14,0(R2) ;speculative, termination BNEZ R1,L1 ;test SPECCK 0(R2) ;perform speculation checkJ L2 ;skip else L1: DADDI R14,R1,#4 ;else clause L2: SD R14,0(R3) ;store Notice speculation check requires maintain basic block case. speculated portion case, basic blockrepresenting case would exist event. importantly, notice thatchecking possible exception requires extra code. third approach preserving exception behavior tracks exceptions occur postpones terminating exception value actually used, pre-serving occurrence exception, although completely precise fash- ion. scheme simple: poison bit added every register, another bit added every instruction indicate whether instruction speculative. Thepoison bit destination register set whenever speculative instructionresults terminating exception; exceptions handled immediately.If speculative instruction uses register poison bit turned on, destina-tion register instruction simply poison bit turned on. normalinstruction attempts use register source poison bit turned on, theinstruction causes fault. way, program would generated exception still generates one, albeit first instance result used instruction speculative. Since poison bits exist registerH-30 ■Appendix H Hardware Software VLIW EPICvalues memory values, stores never speculative thus trap either operand “poison. ” Example Consider code fragment page H-29 show would compiled speculative instructions poison bits. Show exception thespeculative memory reference would recognized. Assume R14 unused available. Answer code (an spreceding opcode indicates speculative instruction): LD R1,0(R3) ;load sLD R14,0(R2) ;speculative load BBEQZ R1,L3 ; DADDI R14,R1,#4 ; L3: SD R14,0(R3) ;exception speculative LW speculative sLD generates terminating exception, poison bit R14 turned on. nonspeculative SWinstruction occurs, raise exception poison bit R14 on. One complication must overcome OS saves user registers context switch poison bit set. special instruction needed saveand reset state poison bits avoid problem. fourth final approach listed earlier relies hardware mechanism operates like reorder buffer. approach, instructions marked compiler speculative include indicator many branchesthe instruction speculatively moved across branch action (taken/not taken) compiler assumed. last piece information basically tellsthe hardware location code block speculated instruction orig-inally was. practice, benefit speculation gained allowingmovement across single branch; thus, 1 bit saying whether speculatedinstruction came taken taken path required. Alternatively, original location speculative instruction marked sentinel , tells hardware earlier speculative instruction longer speculative andvalues may committed. instructions placed reorder buffer issued forced commit order, hardware speculation approach. (Notice, though, thatno actual speculative branch prediction dynamic scheduling occurs.) reor-der buffer tracks instructions ready commit delays “write- back”portion speculative instruction. Speculative instructions allowed commit branches speculatively moved also ready commit, or, alternatively, corresponding sentinel isreached. point, know whether speculated instruction havebeen executed not. executed generated terminatingH.5 Hardware Support Compiler Speculation ■H-31exception, know program terminated. instruction executed, exception ignored. Notice compiler, rather hardware, job register renaming ensure cor-rect usage speculated result, well correct program execution. Hardware Support Memory Reference Speculation Moving loads across stores usually done compiler certain theaddresses conflict. saw examples Section 3.2, trans-formations critical reducing critical path length code segment. Toallow compiler undertake code motion cannot absolutelycertain movement correct, special instruction check address conflicts included architecture. special instruction left original location load instruction (and acts like guardian), loadis moved across one stores. speculated load executed, hardware saves address accessed memory location. subsequent store changes location thecheck instruction, speculation failed. location beentouched, speculation successful. Speculation failure handledin two ways. load instruction speculated, suffices redo load point check instruction (which could supply target register addition memory address). additional instructions depended theload also speculated, fix-up sequence reexecutes speculatedinstructions starting load needed. case, check instructionspecifies address fix-up code located. section, seen variety hardware assist mechanisms. mechanisms key achieving good support compiler-intensiveapproaches Chapter 3 appendix. addition, several eas- ily integrated hardware-intensive approaches Chapter 3 provide addi- tional benefits. H.6 Intel IA-64 Architecture Itanium Processor section overview Intel IA-64 architecture, advanced VLIW-style processor, implementation Itanium processor. Intel IA-64 Instruction Set Architecture IA-64 RISC-style, register-register instruction set, many novel features designed support compiler-based exploitation ILP. focus hereis unique aspects IA-64 ISA. aspects dis-cussed already appendix, including predication, compiler-based parallelismdetection, support memory reference speculation.H-32 ■Appendix H Hardware Software VLIW EPICWhen announced IA-64 architecture, HP Intel introduced term EPIC (Explicitly Parallel Instruction Computer) distinguish new architec- tural approach earlier VLIW architectures RISC architec-tures. Although VLIW EPIC architectures share many features, EPICapproach includes several concepts extend earlier VLIW approach. Theseextensions fall two main areas: 1.EPIC greater flexibility indicating parallelism among instructions instruction formats. Rather relying fixed instruction format operations instruction must capable executed parallel format completely rigid, EPIC uses explicit indicators possibleinstruction dependence well variety instruction formats. EPICapproach express parallelism flexibly rigid VLIWmethod reduce increases code size caused typically inflex-ible VLIW instruction format. 2.EPIC extensive support software speculation earlier VLIW schemes minimal support. addition, IA-64 architecture includes variety features improve perfor- mance, register windows rotating floating-point register (FPR) stack. IA-64 Register Model components IA-64 register state ■128 64-bit general-purpose registers, see shortly actually65 bits wide ■128 82-bit floating-point registers, provide two extra exponent bits overthe standard 80-bit IEEE format ■64 1-bit predicate registers ■8 64-bit branch registers, used indirect branches ■A variety registers used system control, memory mapping, performancecounters, communication OS integer registers configured help accelerate procedure calls using register stack mechanism similar developed Berkeley RISC-I proces-sor used SPARC architecture. Registers 0 31 always accessible addressed 0 31. Registers 32 128 used register stack, procedure allocated set registers (from 0 96) use. newregister stack frame created called procedure renaming registers inhardware; special register called current frame pointer (CFM) points setof registers used given procedure. frame consists two parts:the local area output area. local area used local storage, whileH.6 Intel IA-64 Architecture Itanium Processor ■H-33the output area used pass values called procedure. alloc instruc- tion specifies size areas. integer registers register stack support. procedure call, CFM pointer updated R32 called pro- cedure points first register output area called procedure. Thisupdate enables parameters caller passed addressable reg-isters callee. callee executes alloc instruction allocate number required local registers, include output registers caller,and number output registers needed parameter passing called pro- cedure. Special load store instructions available saving restoring register stack, special hardware (called register stack engine ) handles over- flow register stack. addition integer registers, three sets registers: floating-point registers, predicate registers, branch registers. Thefloating-point registers used floating-point data, branch registersare used hold branch destination addresses indirect branches. predicationregisters hold predicates, control execution predicated instructions; describe predication mechanism later section. integer floating-point registers support register rotation reg- isters 32 128. Register rotation designed ease task allocating registersin software-pipelined loops, problem discussed Section H.3 . addi- tion, combined use predication, possible avoid need forunrolling separate prologue epilogue code software-pipelinedloop. capability reduces code expansion incurred use software pipelin-ing makes technique usable loops smaller numbers iterations, overheads would traditionally negate many advantages. Instruction Format Support Explicit Parallelism IA-64 architecture designed achieve major benefits VLIW approach —implicit parallelism among operations instruction fixed for- matting operation fields —while maintaining greater flexibility VLIW normally allows. combination achieved relying compiler detect ILP schedule instructions parallel instruction slots, adding flexibility formatting instructions allowing compiler indicate aninstruction cannot executed parallel successors. IA-64 architecture uses two different concepts achieve benefits implicit parallelism ease instruction decode. Implicit parallelism achievedby placing instructions instruction groups , fixed formatting mul- tiple instructions achieved introduction concept called bundle , contains three instructions. Let ’s start defining instruction group. instruction group sequence consecutive instructions register data dependences among (there minor exceptions). instruc-tions group could executed parallel, sufficient hardware resourcesexisted dependences memory preserved. instructiongroup arbitrarily long, compiler must explicitly indicate theH-34 ■Appendix H Hardware Software VLIW EPICboundary one instruction group another. boundary indicated placing stop two instructions belong different groups. understand stops indicated, must first explain instructions areplaced bundles. IA-64 instructions encoded bundles, 128 bits wide. bun- dle consists 5-bit template field three instructions, 41 bits length.(Actually, 41-bit quantities truly instructions, since beinterpreted conjunction template field. name syllable sometimes used operations. simplicity, continue use term “instruc- tion.”) simplify decoding instruction issue process, template field bundle specifies types execution units instruction bundlerequires. Figure H.6 shows five different execution unit types describes instruction classes may hold, together examples. 5-bit template field within bundle describes presence stops associated bundle execution unit type required eachinstruction within bundle. Figure H.7 shows possible formats tem- plate field encodes position stops specifies. bundle formats specify subset possible combinations instruction types stops. see bundle works, let ’s consider example. Example Unroll array increment example, x[i] = x[i] + , seven times place instructions bundles, first ignoring pipeline latencies (to minimize numberof bundles) scheduling code minimize stalls. scheduling codeassume one bundle executes per clock stalls cause entire bundle toExecution unit slotInstruction typeInstruction description Example instructions I-unit Integer ALU Add, subtract, and, or, compare Non-ALU integer Integer multimedia shifts, bit tests, moves M-unit Integer ALU Add, subtract, and, or, compare Memory access Loads stores integer/FP registers F-unit F Floating point Floating-point instructionsB-unit B Branches Conditional branches, calls, loop branches L + X L + X Extended Extended immediates, stops no-ops Figure H.6 five execution unit slots IA-64 architecture instruc- tions types may hold shown. A-type instructions, correspond integer ALU instructions, may placed either I-unit M-unit slot. L + X slots special,as occupy two instruction slots; L + X instructions used encode 64-bit imme- diates special instructions. L + X instructions executed either I-unit B-unit.H.6 Intel IA-64 Architecture Itanium Processor ■H-35be stalled. Use pipeline latencies Figure 3.2. Use MIPS instruction mne- monics simplicity. Answer two different versions shown Figure H.8 . Although latencies dif- ferent Itanium, common bundle, MMF, must issued byitself Itanium, example assumes.Figure H.7 24 possible template values (8 possible values reserved) instruction slots stops format. Stops indicated heavy lines may appear within and/or end bundle. example, template 9 specifies instruction slots M, M, (in order) stop bun- dle next. Template 11 type instruction slots also includes astop first slot. L + X format used slot 1 L slot 2 X.H-36 ■Appendix H Hardware Software VLIW EPICFigure H.8 IA-64 instructions, including bundle bits stops, unrolled version x[i] 5x[i] + s, unrolled seven times scheduled (a) minimize number instruction bundles (b) minimize number cycles (assuming hazard stalls entire bundle). Blank entries indicate unused slots, encoded no-ops. absence stops indicates bundles could executed parallel. Minimizing thenumber bundles yields 9 bundles versus 11 needed minimize number cycles. scheduled version executes half number cycles. Version (a) fills 85% instruction slots, (b) fills 70%. number empty slots scheduled code use bundles may lead code sizes much larger thanother RISC architectures. Note branch last bundle sequences depends DADD bundle. IA-64 instruction set, sequence would coded setting predication register branch would predicated register. Normally, dependent operations could occur bundle,but case one exceptions mentioned earlier.H.6 Intel IA-64 Architecture Itanium Processor ■H-37Instruction Set Basics turning special support speculation, briefly discuss major instruction encodings survey instructions five primary instruction classes (A, I, M, F, B). IA-64 instruction 41 bits length. high-order 4 bits, together bundle bits specify execution unitslot, used major opcode. (That is, 4-bit opcode field reused acrossthe execution field slots, appropriate think opcode 4 bitsplus M, F, I, B, L + X designation.) low-order 6 bits every instruction areused specifying predicate register guards instruction (see nextsubsection). Figure H.9 summarizes major instruction formats, multimedia instructions, gives examples instructions encoded format. Predication Speculation Support IA-64 architecture provides comprehensive support predication: Nearlyevery instruction IA-64 architecture predicated. instruction ispredicated specifying predicate register, whose identity placed lower 6 bits instruction field. nearly instructions predicated, conversion code motion lower overhead would withonly limited support conditional instructions. One consequence full predi-cation conditional branch simply branch guarding predicate! Predicate registers set using compare test instructions. compare instruction specifies one ten different comparison tests two predicate reg-isters destinations. two predicate registers written either resultof comparison (0 1) complement, logical function combines two tests (such and)and complement. capability allows multiple comparisons done one instruction. Speculation support IA-64 architecture consists separate support control speculation, deals deferring exception speculated instruc-tions, memory reference speculation, supports speculation loadinstructions. Deferred exception handling speculative instructions supported pro- viding equivalent poison bits. general-purpose registers (GPRs), bits called NaTs (Not Thing), extra bit makes GPRs effec- tively 65 bits wide. FP registers capability obtained using specialvalue, NaTVal (Not Thing Value); value encoded using significand of0 exponent outside IEEE range. speculative load instructionsgenerate values, instructions affect memory cause aNaT NaTVal propagated result register. (There speculativeand non-speculative loads; latter raise immediate exceptions can-not defer them.) Floating-point exceptions handled mechanism instead use floating-point status registers record exceptions.H-38 ■Appendix H Hardware Software VLIW EPICInstruction typeNumber formats Representative instructionsExtra opcode bitsGPRs/ FPRsImmediate bits Other/comment 8 Add, subtract, and, 9 3 0 Shift left add 7 3 0 2-bit shift countALU immediates 9 2 8Add immediate 3 2 14Add immediate 0 2 22Compare 4 2 0 2 predicate registerdestinations Compare immediate 3 1 8 2 predicate register destinations 29 Shift R/L variable 9 3 0 Many multimedia instructions usethis format. Test bit 6 3 6-bit field specifier2 predicate register destinations Move BR 6 1 9-bit branch predictBranch register specifier 46 Integer/FP load store, line prefetch10 2 0 Speculative/ nonspeculative Integer/FP load store, line prefetch post- increment immediate9 2 8 Speculative/ nonspeculative Integer/FP load prefetch register postincrement10 3 Speculative/ nonspeculative Integer/FP speculation check 3 1 21 two fields B 9 PC-relative branch, counted branch70 2 1 PC-relative call 4 0 21 1 branch register F 15 FP arithmetic 2 4 FP compare 2 2 2 6-bit predicate regs L + X 4 Move immediate long 2 1 64 Figure H.9 summary instruction formats IA-64 ISA. major opcode bits guarding predication register specifier add 10 bits every instruction. number formats indicated instruction class second column (a total 111) strict interpretation: different use field, even size, considered different format. number formats actually different field sizes one-third one-half large. instructions unused bits reserved; included table. Immediate bits include sign bit. branch instructions include prediction bits, used predictor valid prediction. one many formats multimedia instructions shown table.H.6 Intel IA-64 Architecture Itanium Processor ■H-39A deferred exception resolved two different ways. First, non- speculative instruction, store, receives NaT NaTVal source operand, generates immediate unrecoverable exception. Alternatively,achk.s instruction used detect presence NaT NaTVal branch routine designed compiler recover speculativeoperation. recovery approach kes sense memory reference speculation. inability store contents instructions NaT NaTVal set would make impossible OS save state processor. Thus, IA-64 includes special instructions save restore registers cause exception NaT NaTVal also save restore theNaT bits. Memory reference support IA-64 uses concept called advanced loads . advanced load load speculatively moved store instruc-tions potentially dependent. speculatively perform load, ld. a(for advanced load) instruction used. Executing instruction creates entry special table, called ALAT . ALAT stores register destination load address accessed memory location. store exe- cuted, associative lookup active ALAT entries performed. thereis ALAT entry memory address store, ALAT entry ismarked invalid. nonspeculative instruction (i.e., store) uses value generated advanced load value derived result advanced load, explicitcheck required. check specifies destination register advanced load.If ALAT register still valid, speculation legal effect check clear ALAT entry. check fails, action taken depends two different types checks employed. first type ofcheck instruction ld.c , simply causes data reloaded memory point. ld.c instruction used onlythe load advanced. alternative form check, chk.a , specifies address fix-up routine used reexecute load speculated code depended value load. Itanium 2 Processor Itanium 2 processor second implementation IA-64 architecture.The first version, Itanium 1, became available 2001 800 MHz clock.The Itanium 2, first delivered 2003, maximum clock rate 2005 of1.6 GHz. two processors similar, differences pipelinestructure greater differences memory hierarchies. Itanium 2 four times faster Itanium 1. performance improvement comes doubling clock rate, aggressive memory hierarchy, additional func-tional units improve instruction throughput, complete bypassing, aH-40 ■Appendix H Hardware Software VLIW EPICshorter pipeline reduces stalls, mature compiler system. Dur- ing roughly period elapsed Itanium 1 Itanium 2, Pen- tium processors improved slightly factor three. greaterimprovement Itanium reasonable given novelty architectureand software system versus established IA-32 implementations. Itanium 2 fetch issue two bundles, six instructions, per clock. Itanium 2 uses three-level memory hierarchy on-chip. firstlevel uses split instruction data caches, 16 KB; floating-point data arenot placed first-level cache. second third levels unified caches 256 KB 3 MB 9 MB, respectively. Functional Units Instruction Issue 11 functional units Itanium 2 processor: two I-units, four M-units (two loads two stores), three B-units, two F-units. functionalunits pipelined. Figure H.10 gives pipeline latencies typical instructions. addition, result bypassed one unit another, thereis usually least one additional cycle delay. Itanium 2 issue six instructions per clock two bundles. worst case, bundle split issued, hardware could see asfour instructions: one first bundle executed three sec- ond bundle. Instructions allocated functional units based bundle bits, ignoring presence no-ops predicated instructions untrue predicates.In addition, issue functional unit blocked next instructionto issued needs already committed unit, resulting bundle split. splitbundle still occupies one two bundle slots, even one instructionremaining. Instruction Latency Integer load 1 Floating-point load 5 –9 Correctly predicted taken branch 0 –3 Mispredicted branch 6Integer ALU operations 0FP arithmetic 4 Figure H.10 latency typical instructions Itanium 2. latency defined smallest number intervening instructions two dependent instructions. Integer load latency assumes hit first-level cache. FP loads alwaysbypass primary cache, latency equal access time second-level cache. minor restrictions functional units, pri- marily involve execution infrequent instructions.H.6 Intel IA-64 Architecture Itanium Processor ■H-41The Itanium 2 processor uses eight-stage pipeline divided four major parts: ■Front-end (stages IPG Rotate) —Prefetches 32 bytes per clock (two bundles) prefetch buffer, hold eight bundles (24 instruc-tions). Branch prediction done using multilevel adaptive predictor likethose described Chapter 3 . ■Instruction delivery (stages EXP REN) —Distributes six instructions 11 functional units. Implements register renaming rotation andregister stacking. ■Operand delivery (REG) —Accesses register file, performs register bypass- ing, accesses updates register scoreboard, checks predicate depen-dences. scoreboard used detect individual instructions canproceed, stall one instruction (for example, due unpredictable event like cache miss) bundle need cause entire bundle stall. (As saw Figure H.8 , stalling entire bundle leads poor performance unless instructions carefully scheduled.) ■Execution (EXE, DET, WRB) —Executes instructions ALUs load-store units, detects exceptions posts NaTs, retires instructions, andperforms write-back. Itanium 1 Itanium 2 many features com- monly associated dynamically scheduled pipelines described inChapter 3 : dynamic branch prediction, register renaming, scoreboarding, pipeline number stages execution (to handle instruction alignment, renam- ing, etc.), several stages following execution handle exception detection.Although mechanisms generally simpler advanceddynamically scheduled superscalar, overall effect Itanium proces-sors, rely much compiler technology, seem complex asthe dynamically scheduled processors saw Chapter 3 ! One might ask features included processor relies primar- ily compile time techniques finding exploiting parallelism. two main motivations. First, dynamic techniques sometimes significantly bet- ter, omitting would hurt performance significantly. inclusion ofdynamic branch prediction case. Second, caches absolutely necessary achieve high performance, caches come cache misses, unpredictable current pro-cessors take relatively long time. early VLIW processors, entire pro-cessor would freeze cache miss occurred, retaining lockstep parallelisminitially specified compiler. approach totally unrealistic mod- ern processor cache misses cost tens hundreds cycles. Allowing instructions continue others stalled, however, requires intro-duction form dynamic scheduling, case scoreboarding. addi-tion, stall likely long, antidependences likely prevent muchH-42 ■Appendix H Hardware Software VLIW EPICprogress waiting cache miss; hence, Itanium implementations also introduce register renaming. Itanium 2 Performance Figure H.11 shows performance 1.5 GHz Itanium 2 versus Pentium 4, AMD Athlon processor, IBM Power5 five SPECint five SPECfpbenchmarks. Overall, Itanium 2 slightly slower Power5 fullset SPEC floating-point benchmarks 35% faster AMD Athlonor Pentium 4. SPECint, Itanium 2 15% faster Power5, boththe AMD Athlon Pentium 4 15% faster Itanium 2. Ita-nium 2 Power5 much higher power larger die sizes. fact, Power5 contains two processors, one active normal SPEC benchmarks, still less half transistor count Itanium. wewere reduce die size, transistor count, power Power5 eliminat-ing one processors, Itanium would far largest highest-power processor. H.7 Concluding Remarks design IA-64 architecture began, joint effort Hewlett-Packard Intel many designers benefited experience withearly VLIW processors well years research building early concepts.The clear goal IA-64 architecture achieve levels ILP good orwupwise galgel mesa swim sixtrack gcc gzip crafty gap twolf05000 40003000 2000 100060007000SPEC Ratio80009000 Itanium 2 Pentium 4 AMD Athlon 64 Power5 Figure H.11 performance four multiple-issue processors five SPECfp SPECint benchmarks. clock rates four processors Itanium 2 1.5 GHz, Pentium 4 Extreme Edition 3.8 GHz, AMD Athlon 64 2.8 GHz, IBM Power5 1.9 GHz.H.7 Concluding Remarks ■H-43better achieved hardware-based approaches, also allowing much simpler hardware implementation. simpler hardware implementation, designers hoped much higher clock rates could achieved.Indeed, IA-64 architecture first Itanium announced, theywere announced successor RISC approaches clearly superioradvantages. Unfortunately, practical reality quite different. IA-64 Ita- nium implementations appear least complicated dynamicallybased speculative processors, neither approach significant consistent performance advantage. fact Itanium designs also power efficient led situation Itanium design adopted byonly small number customers primarily interested FP performance. Intel planned IA-64 new 64-bit architecture well. combination mediocre integer performance (especially Itanium 1) andlarge die size, together AMD ’s introduction 64-bit version IA- 32 architecture, forced Intel extend address space IA-32. availabilityof larger address space IA-32 processor strong integer performance fur- ther reduced interest IA-64 Itanium. recently, Intel introduced name IPF replace IA-64, since former name made less sense theolder x86 architecture extended 64 bits. Reference Wilson, R.P., Lam, M.S., 1995. Efficient context-sensitive pointer analysis C programs. In: Proc. ACM SIGPLAN ’95 Conf. Programming Language Design Implementation, June 18 –21, 1995, La Jolla, Calif, pp. 1 –12.H-44 ■Appendix H Hardware Software VLIW EPICI.1 Introduction I-2 I.2 Interprocessor Communication: Critical Performance Issue I-3 I.3 Characteristics Scientific Applications I-6 I.4 Synchronization: Scaling I-12 I.5 Performance Scientific Applications Shared-Memory Multiprocessors I-21 I.6 Performance Measurement Parallel Processors Scientific Applications I-33 I.7 Implementing Cache Coherence I-34 I.8 Custom Cluster Approach: Blue Gene/L I-41 I.9 Concluding Remarks I-44I Large-Scale Multiprocessors Scientific Applications Hennessy Patterson move MPPs Chapter 11. Jim Gray, Microsoft Research asked coverage massively parallel processors (MPPs) third edition 2000 Unfortunately companies MPP business, third edition ten chapters MPP business grow anticipated first second edition written.I.1 Introduction primary application large-scale multiprocessors true parallel program- ming, opposed multiprogramming transaction-oriented computing whereindependent tasks executed parallel without much interaction. true parallelcomputing, set tasks execute collaborative fashion one application.The primary target parallel computing scientific technical applications.In contrast, loosely coupled commercial applications, Web servers transaction-processing applications, little communication among tasks. applications, loosely coupled clusters generally adequate andmost cost-effective, since intertask communication rare. true parallel computing involves cooperating tasks, nature com- munication tasks communication supported thehardware vital importance determining performance application.The next section appendix examines issues characteristics ofdifferent communication models. comparison sequential programs, whose performance largely dictated cache behavior issues related instruction-level parallelism, parallelprograms several additional characteristics important performance,including amount parallelism, size parallel tasks, frequency andnature intertask communication, frequency nature synchroniza-tion. aspects affected underlying nature application aswell programming style. Section I.3 reviews important characteristics several scientific applications give flavor issues. saw Chapter 5 , synchronization quite important achieving good performance. larger number parallel tasks may need synchro-nize makes contention involving synchronization much serious problemin large-scale multiprocessors. Section I.4 examines methods scaling synchronization mechanisms Chapter 5 . Section I.5 explores detailed performance shared-memory parallel appli- cations executing moderate-scale shared-memory multiprocessor. willsee, behavior performance characteristics quite bit complicated small-scale shared-memory multiprocessors. Section I.6 discusses general issue examine parallel performance different sizedmultiprocessors. Section I.7 explores implementation challenges distributed shared-memory cache coherence, key architectural approach used moderate-scale multiprocessors. Sections I.7 andI.8rely basic understanding inter- connection networks, reader least quickly review Appendix F reading sections. Section I.8 explores design one newest exciting large- scale multiprocessors recent times, Blue Gene. Blue Gene cluster-based mul- tiprocessor, uses custom, highly dense node designed specifically thisfunction, opposed nodes earlier cluster multiprocessors used anode architecture similar desktop smaller-scale multiprocessorI-2 ■Appendix Large-Scale Multiprocessors Scientific Applicationsnode. using custom node design, Blue Gene achieves significant reduction cost, physical size, power consumption node. Blue Gene/L, 64 K- node version, world ’s fastest computer 2006, measured linear algebra benchmark, Linpack. I.2 Interprocessor Communication: Critical Performance Issue multiprocessors larger processor counts, interprocessor communication becomes expensive, since distance processors increases. Furthermore, truly parallel applications threads applicationmust communicate, usually communication loosely coupledset distinct processes independent transactions, characterize manycommercial server applications. factors combine make efficient interpro-cessor communication one important determinants parallel perfor-mance, especially scientific market. Unfortunately, characterizing communication needs application capabilities architecture complex. section examines key hard- ware characteristics determine communication performance, nextsection looks application behavior communication needs. Three performance metrics critical hardware communication mechanism: 1.Communication bandwidth —Ideally, communication bandwidth limited processor, memory, interconnection bandwidths, rather aspect communication mechanism. interconnection network deter- mines maximum communication capacity system. bandwidthin single node, often important total system bandwidth,is affected architecture within node communicationmechanism. communication mechanism affect communica-tion bandwidth node? communication occurs, resources within thenodes involved communication tied occupied, preventing otheroutgoing incoming communication. occupancy incurred word message, sets absolute limit communication band- width. limit often lower network memory system canprovide. Occupancy may also component incurred com-munication event, incoming outgoing request. latter case,the occupancy limits communication rate, impact occupancyon overall communication bandwidth depends size messages. 2.Communication latency —Ideally, latency low possible. Appendix F explains: Communication latency ¼Sender overhead + Time flight + Transmission time + Receiver overheadI.2 Interprocessor Communication: Critical Performance Issue ■I-3assuming contention. Time flight fixed transmission time deter- mined interconnection network. software hardware overheads sending receiving messages largely determined communicationmechanism implementation. latency crucial? Latency affectsboth performance easy program multiprocessor. Unlesslatency hidden, directly affects performance either tying processorresources causing processor wait. Overhead occupancy closely related, since many forms overhead also tie part node, incurring occupancy cost, turn limitsbandwidth. Key features communication mechanism may directly affectoverhead occupancy. example, destination address aremote communication named, protection implemented? Whennaming protection mechanisms provided processor, shared address space, additional overhead small. Alternatively, mecha- nisms must provided operating system communication, thisincreases overhead occupancy costs communication, turnreduce bandwidth increase latency. 3.Communication latency hiding —How well communication mechanism hide latency overlapping communication computation othercommunication? Although measuring simple measuring firsttwo metrics, important characteristic quantified measuringthe running time multiprocessors communication latency butdifferent support latency hiding. Although hiding latency certainly good idea, poses additional burden software system ultimately programmer. Furthermore, amount latency hidden applica-tion dependent. Thus, usually best reduce latency wherever possible. performance measures affected characteristics communications needed application, see next section.The size data items communicated obvious characteristic,since affects latency bandwidth directly, well affecting efficacy different latency-hiding approaches. Similarly, regularity communi- cation patterns affects cost naming protection, hence commu-nication overhead. general, mechanisms perform well smaller aswell larger data communication requests, irregular well regular com-munication patterns, flexible efficient wider class applica-tions. course, considering communication mechanism, designers mustconsider cost well performance. Advantages Different Communication Mechanisms two primary means communicating data large-scale multiprocessor aremessage passing shared memory. two primary communicationI-4 ■Appendix Large-Scale Multiprocessors Scientific Applicationsmechanisms advantages. shared-memory communication, advan- tages include ■Compatibility well-understood mechanisms use centralized multiprocessors, use shared-memory communication. OpenMPconsortium (see www.openmp.org description) proposed standardized programming interface shared-memory multiprocessors. Although mes-sage passing also uses standard, MPI Message Passing Interface, stan-dard used either shared-memory multiprocessors loosely coupled clusters use throughput-oriented environments. ■Ease programming communication patterns among processors complex vary dynamically execution. Similar advantages simplify compiler design. ■The ability develop applications using familiar shared-memory model,focusing attention accesses performance critical. ■Lower overhead communication better use bandwidth commu-nicating small items. arises implicit nature communication use memory mapping implement protection hardware, rather thanthrough I/O system. ■The ability use hardware-controlled caching reduce frequency ofremote communication supporting automatic caching data, bothshared private. see, caching reduces latency contentionfor accessing shared data. advantage also comes disadvantage,which mention below. major advantages message-passing communication include following: ■The hardware simpler, especially comparison scalable shared- memory implementation supports coherent caching remote data. ■Communication explicit, means simpler understand. shared-memory models, difficult know communication occurringand not, well costly communication is. ■Explicit communication focuses programmer attention costly aspectof parallel computation, sometimes leading improved structure multi-processor program. ■Synchronization naturally associated sending messages, reducing thepossibility errors introduced incorrect synchronization. ■It makes easier use sender-initiated communication, may someadvantages performance. ■If communication less frequent structured, easier toimprove fault tolerance using transaction-like structure. Furthermore,I.2 Interprocessor Communication: Critical Performance Issue ■I-5the less tight coupling nodes explicit communication make fault isola- tion simpler. ■The largest multiprocessors use cluster structure, inherently based message passing. Using two different communication models may introduce complexity warranted. course, desired communication model created software top hardware model supports either mechanisms. Supporting messagepassing top shared memory considerably easier: messages essen-tially send data one memory another, sending message implemen-ted copy one portion address space another. majordifficulties arise dealing messages may misaligned arbi- trary length memory system normally oriented toward transferring aligned blocks data organized cache blocks. difficulties over-come either small performance penalties software essentially nopenalties, using small amount hardware support. Supporting shared memory efficiently top hardware message passing much difficult. Without explicit hardware support shared memory, allshared-memory references need involve operating system provide addresstranslation memory protection, well translate memory references message sends receives. Loads stores usually move small amounts data, high overhead handling communications software severely limitsthe range applications performance software-based sharedmemory acceptable. reasons, never practical use messagepassing implement shared memory commercial system. I.3 Characteristics Scientific Applications primary use scalable shared-memory multiprocessors true parallelprogramming, opposed multiprogramming transaction-oriented comput-ing. primary target parallel computing scientific technical applica-tions. Thus, understanding design issues requires insight thebehavior applications. section provides introduction. Characteristics Scientific Applications scientific/technical parallel workload consists two applications twocomputational kernels. kernels fast Fourier transformation (FFT) anLU decomposition, chosen represent commonly usedtechniques wide variety applications performance characteristics typical many parallel scientific applications. addition, kernels small code segments whose behavior understand directly track specificarchitectural characteristics. Like many scientific applications, I/O essentiallynonexistent workload.I-6 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsThe two applications use appendix Barnes Ocean, represent two important different types parallel computation. briefly describe applications kernels characterize basicbehavior terms parallelism communication. describe prob-lem decomposed distributed shared-memory multiprocessor; certain datadecompositions describe necessary multiprocessors havea single, centralized memory. FFT Kernel FFT key kernel applications use spectral methods, arise infieldsrangingfrom signalprocessingtofluidflowtoclimate modeling. TheFFT appli- cationwe study one-dimensional versionofa parallel algorithm fora complex number FFT. sequential execution time ndata points nlogn. algo- rithm uses high radix (equal toﬃﬃﬃnp) minimizes communication. measure- ments shown appendix collected million-point input data set. three primary data structures: input output arrays data transformed roots unity matrix, precomputed onlyread execution. arrays organized square matrices. sixsteps algorithm follows: 1.Transpose data matrix. 2.Perform 1D FFT row data matrix. 3.Multiply roots unity matrix data matrix write result data matrix. 4.Transpose data matrix. 5.Perform 1D FFT row data matrix. 6.Transpose data matrix. data matrices roots unity matrix partitioned among processors contiguous chunks rows, processor ’s partition falls local memory. first row roots unity matrix accessed heavily pro- cessors often replicated, do, first step algorithm justshown. data transposes ensure good locality individual FFT steps,which would otherwise access nonlocal data. communication transpose phases, require all-to-all communication large amounts data. Contiguous subcolumns rowsassigned processor grouped blocks, transposed placedinto proper location destination matrix. Every processor transposes one block locally sends one block processors system. Although reuse individual words transpose, long cacheblocks makes sense block transpose take advantage spatial localityafforded long blocks source matrix.I.3 Characteristics Scientific Applications ■I-7The LU Kernel LU LU factorization dense matrix representative many dense linear algebra computations, QR factorization, Cholesky factorization, eigenvalue methods. matrix size n/C2nthe running time n3and parallelism proportional n2. Dense LU factorization performed efficiently blocking algorithm, using techniques Chapter 2 , leads highly efficient cache behavior low communication. blockingthe algorithm, dominant computation dense matrix multiply occurs inthe innermost loop. block size chosen small enough keep cachemiss rate low large enough reduce time spent less parallel partsof computation. Relatively small block sizes (8 /C28o r1 6 /C216) tend satisfy criteria. Two details important reducing interprocessor communication. First, blocks matrix assigned processors using 2D tiling: n B/C2n B (where block B/C2B) matrix blocks allocated laying grid size p/C2pover matrix blocks cookie-cutter fashion blocks allocated processor. Second, dense matrix multiplication performedby processor owns destination block. blocking allocation scheme, communication reduction regular predictable. measurements appendix, input 512 /C2512 matrix block 16/C216 used. natural way code blocked LU factorization 2D matrix shared address space use 2D array represent matrix. blocks allo-cated tiled decomposition, block contiguous address spacein 2D array, difficult allocate blocks local memories theprocessors them. solution ensure blocks assigned aprocessor allocated locally contiguously using 4D array (with first two dimensions specifying block number 2D grid blocks, next two specifying element block). Barnes Application Barnes implementation Barnes-Hut n-body algorithm solving problem galaxy evolution. N-body algorithms simulate interaction among large number bodies forces interacting among them. thisinstance, bodies represent collec tions stars force gravity. reduce computational time required model completely individual interactions among bodies, grow n 2,n-body algorithms take advan- tage fact forces drop distance. (Gravity, example,drops 1/ 2,w h e r e dis distance two bodies.) Barnes-Hut algorithm takes advantage property treating collection bodies “far away ”from another body single point center mass collection mass equal collection. body farenough body collectio n, error introduced beI-8 ■Appendix Large-Scale Multiprocessors Scientific Applicationsnegligible. collections structure hierarchical fashion, represented tree. algorithm yields nlognrunning time par- allelism proportional n. Barnes-Hut algorithm uses octree (each node eight children) represent eight cubes portion space. node represents thecollection bodies subtree rooted node, call cell. density space varies leaves represent individual bodies,the depth tree varies. tree traversed per body computethe net force acting body. force calculation algorithm body starts root tree. every node tree visits, algorithm determines center mass cell represented subtree rooted node is“far enough away ”from body. so, entire subtree node approximated single point center mass cell, forcethat center mass exerts body computed. hand, ifthe center mass far enough away, cell must “opened ”and subtrees visited. distance body cell, together withthe error tolerances, determines cells must opened. force calcula- tion phase dominates execution time. appendix takes measurements using 16K bodies; criterion determining whether cell needs openedis set middle range typically used practice. Obtaining effective parallel performance Barnes-Hut challenging distribution bodies nonuniform changes time, making partition-ing work among processors maintenance good locality referencedifficult. helped two properties: (1) system evolves slowly, (2)because gravitational forces fall quickly, high probability, cell requires touching small number cells, used last time step. tree partitioned allocating processor subtree.Many accesses needed compute force body subtree beto bodies subtree. Since amount work associated subtreevaries (cells dense portions space need access cells), size ofthe subtree allocated processor based measure work todo (e.g., many cells needs visit), rather number ofnodes subtree. partitioning octree representation, obtain good load balance good locality reference, keeping partitioning cost low. Although partitioning scheme results good locality reference, theresulting data references tend small amounts data unstructured.Thus, scheme requires efficient implementation shared-memorycommunication. Ocean Application Ocean simulates influence eddy boundary currents large-scale flowin ocean. uses restricted red-black Gauss-Seidel multigrid technique tosolve set elliptical partial differential equations. Red-black Gauss-Seidel iteration technique colors points grid consistently updateI.3 Characteristics Scientific Applications ■I-9each point based previous values adjacent neighbors. Multigrid methods solve finite difference equations iteration using hierarchical grids. grid hierarchy fewer points grid approximation thelower grid. finer grid increases accuracy thus rate convergence,while requiring execution time, since data points. Whether tomove hierarchy grids used next iteration deter-mined rate change data values. estimate error everytime step used decide whether stay grid, move coarsergrid, move finer grid. iteration converges finest level, solution reached. iteration n 2work n/C2ngrid amount parallelism. arrays representing grid dynamically allocated sized particular problem. entire ocean basin partitioned square subgrids(as close possible) allocated portion address space corre-sponding local memory individual processors, assignedresponsibility subgrid. measurements appendix use aninput 130 /C2130 grid points. five steps time iteration. Since data exchanged steps, processors present synchronize end step proceeding next. Communication occurs theboundary points subgrid accessed adjacent subgrid nearest-neighbor fashion. Computation/Communication Parallel Programs key characteristic determining performance parallel programs theratio computation communication. ratio high, means applicationhas lots computation datum communicated. saw Section I.2 , communication costly part parallel computing; therefore, highcomputation-to-communication ratios beneficial. parallel processingenvironment, concerned ratio computation communica-tion changes increase either number processors, size prob-lem, both. Knowing ratio changes increase processor count sheds light well application sped up. often inter- ested running larger problems, vital understand changing data setsize affects ratio. understand happens quantitatively computation-to- communication ratio add processors, consider happens separately tocomputation communication either add processors increase prob-lem size. Figure I.1 shows add processors, applications, amount computation per processor falls proportionately amount com- munication per processor falls slowly. increase problem size, computation scales O( ) complexity algorithm dictates. Communica- tion scaling complex depends details algorithm; describethe basic phenomena application caption Figure I.1 .I-10 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsThe overall computation-to-communication ratio computed indi- vidual growth rate computation communication. general, ratio risesslowly increase dataset size decreases add processors. Thisreminds us performing fixed-size problem processors leads toincreasing inefficiencies amount communication among processorsgrows. also tells us quickly must scale dataset size add processorsto keep fraction time communication fixed. following example illus-trates trade-offs. Example Suppose know given multiprocessor Ocean application spends 20% execution time waiting communication run four processors.Assume cost communication event independent processor count, true general, since communication costs rise processor count. much faster might expect Ocean run 32-processor machinewith problem size? fraction execution time spent com-munication case? much larger problem run want thefraction time spent communicating same? Answer computation-to-communication ratio Ocean isﬃﬃﬃnp=ﬃﬃﬃpp, problem size same, communication frequency scales byﬃﬃﬃpp. means communication time increases byﬃﬃﬃ 8p . use variation Amdahl ’s law,Application Scaling computation Scaling communicationScaling computation-to- communication FFT nlogn pnplogn LU n pﬃﬃﬃnp ﬃﬃﬃppﬃﬃﬃnp ﬃﬃﬃpp Barnes nlogn papproximatelyﬃﬃﬃnplognðÞﬃﬃﬃpp approximatelyﬃﬃﬃnp ﬃﬃﬃpp Oceann pﬃﬃﬃnp ﬃﬃﬃppﬃﬃﬃnp ﬃﬃﬃpp Figure I.1 Scaling computation, communication, ratio critical factors determining perfor- mance parallel multiprocessors. table, pis increased processor count nis increased dataset size. Scaling per-processor basis. computation scales nat rate given O( ) analysis scales linearly pis increased. Communication scaling complex. FFT, data points must interact, com- munication increases nand decreases p. LU Ocean, communication proportional boundary block, scales dataset size rate proportional side square npoints, namely,ﬃﬃﬃnp; reason communication two applications scales inversely toﬃﬃﬃpp. Barnes complex scaling properties. fall-off interaction bodies, basic number interactions among bodies thatrequire communication scales asﬃﬃﬃnp. additional factor log nis needed maintain relationships among bodies. processor count increased, communication scales inversely toﬃﬃﬃpp.I.3 Characteristics Scientific Applications ■I-11recognizing computation decreased communication time increased. T4is total execution time four processors, execution time 32 processors T32¼Compute time + Communication time ¼0:8/C2T4 8+0:2/C2T4 ðÞ /C2ﬃﬃﬃ 8p ¼0:1/C2T4+0:57/C2T4¼0:67/C2T4 Hence, speedup Speedup ¼T4 T32¼T4 0:67/C2T4¼1:49 fraction time spent communication goes 20% 0.57/ 0.67¼85%. fraction communication time remain same, must keep computation-to-communication ratio same, problem size must scaleat rate processor count. Notice that, changed theproblem size, cannot fairly compare speedup original problem thescaled problem. return critical issue scaling applications mul- tiprocessors Section I.6 . I.4 Synchronization: Scaling section, focus first synchronization performance problems larger multiprocessors solutions problems. Synchronization Performance Challenges understand simple spin lock scheme presented Chapter 5 scale well, imagine large multiprocessor processors contending thesame lock. directory bus acts point serialization processors, leading lots contention, well traffic. following example shows bad things be. Example Suppose 10 processors bus tries lock variable simul- taneously. Assume bus transaction (read miss write miss) 100 clock cycles long. ignore time actual read write lock held cache, well time lock held (they ’t matter much!). Determine number bus transactions required 10 processors toacquire lock, assuming spinning lock released time0. long take process 10 requests? Assume bus isI-12 ■Appendix Large-Scale Multiprocessors Scientific Applicationstotally fair every pending request serviced new request processors equally fast. Answer iprocesses contending lock, perform following sequence actions, generates bus transaction: iload linked operations access lock istore conditional operations try lock lock 1 store (to release lock) Thus, iprocesses, total 2 i+1 bus transactions. Note assumes critical section time negligible, lock released beforeany processors whose store conditional failed attempt another load linked. Thus, nprocesses, total number bus operations Xn i¼12i+1ðÞ ¼nn+1ðÞ +n¼n2+2n 10 processes 120 bus transactions requiring 12,000 clock cycles 120 clock cycles per lock acquisition! difficulty example arises contention lock serialization lock access, well latency bus access. (The fairness property thebus actually makes things worse, since delays processor claims lockfrom releasing it; unfortunately, bus arbitration scheme worst-casescenario exist.) key advantages spin locks —that low over- head terms bus network cycles offer good performance locks reused processor —are lost example. consider alternative implementations next section, that, let ’s con- sider use spin locks implement another common high-level synchroniza-tion primitive. Barrier Synchronization One additional common synchronization operation programs parallelloops barrier . barrier forces processes wait processes reach barrier releases processes. typical implementationof barrier done two spin locks: one protect counter talliesthe processes arriving barrier one hold processes lastprocess arrives barrier. implement barrier, usually use ability tospin variable satisfies test; use notation spin(condi- tion) indicate this. Figure I.2 typical implementation, assuming lock andunlock provide basic spin locks total number pro- cesses must reach barrier.I.4 Synchronization: Scaling ■I-13In practice, another complication mak es barrier implementation slightly complex. Frequently barrier used within loop, processesreleased barrier would work reach barrier again. Assume one processes never actually leaves barrier (it stays spin operation), could happen OS scheduled another process,for example. possible one process races ahead gets thebarrier last process left. “fast”process traps remaining “slow”process barrier resetting flag release . processes wait infinitely next instance barrierbecause one process trapped last instance, number processescan never reach value total. important observation example programmer nothing wrong. Instead, implementer barrier made assump-tions forward progress cannot assumed. One obvious solutionto count processes exit barrier (just onentry) allow process reenter reinitialize barrier untilall processes left prior instanc e barrier. extra step would significantly increase latency barrier contention, aswe see shortly already large. alternative solution sense- reversing barrier , makes use private per-process variable, local_sense , initialized 1 process. Figure I.3 shows code sense-reversing barrier. version barrier safelyusable; next example shows, however, performance still bequite poor.lock (counterlock);/* ensure update atomic */ if(count==0) release=0;/* first= >reset release */ count = count + 1;/* count arrivals */unlock(counterlock);/* release lock */ if(count==total) {/* arrived */ count=0;/* reset counter */release=1;/* release processes */ } else {/* come */ spin (release==1);/* wait arrivals */ } Figure I.2 Code simple barrier. lock counterlock protects counter atomically incremented. variable count keeps tally many processes reached barrier. variable release used hold processes last one reaches barrier. operation spin (release==1) causes process wait processes reach barrier.I-14 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsExample Suppose 10 processors bus tries execute barrier simul- taneously. Assume bus transaction 100 clock cycles, before. canignore time actual read write lock held cache time toexecute nonsynchronization operations barrier implementation. Deter-mine number bus transactions required 10 processors reach bar-rier, released barrier, exit barrier. Assume bus totallyfair, every pending request serviced new request theprocessors equally fast. ’t worry counting processors barrier. long entire process take? Answer assume load linked store conditional used implement lock unlock. Figure I.4 shows sequence bus events processor traverse barrier, assuming first process grab bus lock. Thereis slight difference last process reach barrier, described thecaption. ith process, number bus transactions 3 i+4. last process reach barrier requires one less. Thus, nprocesses, number bus trans- actions Xn i¼13i+4ðÞ ! /C01¼3n2+1 1n 2/C01 10 processes, 204 bus cycles 20,400 clock cycles! barrier operation takes almost twice long 10-processor lock-unlock sequence.local_sense =! local_sense; /* toggle local_sense */ lock (counterlock);/* ensure update atomic */ count=count+1;/* count arrivals */ if(count==total) {/* arrived */ count=0;/* reset counter */release=local_sense;/* release processes */ } unlock (counterlock);/* unlock */spin (release==local_sense);/* wait signal */ } Figure I.3 Code sense-reversing barrier. key making barrier reusable use alternating pattern values flag release , controls exit barrier. process races ahead next instance barrier someother processes still barrier, fast process cannot trap processes, since reset value release Figure I.2 .I.4 Synchronization: Scaling ■I-15As see examples, synchronization performance real bottleneck substantia l contention among multiple processes. little contention synchronization operations infrequent,we primarily concerned latency synchronization primitive — is, long takes individual process complete synchronization operation. basic spin lock operati nc nd ot h si nt w ob u sc c l e :o n et initially read lock one write it. could improve single buscycle variety methods. example, could simply spin swapoperation. lock almost always free, could better, thelock free, would lead lots bus traffic, since attempt tolock variable would lead bus cycl e. practice, latency spin lock quite bad seen example, since write miss data item present cache treated upgrade cheaper true read miss. serious problem examples serialization pro- cess’s attempt complete synchronization. serialization problem contention greatly increases time complete thesynchronization operation. example, time complete 10 lockand unlock operations depended latency uncontended case,then would take 1000 rather 15,000 cycles complete synchroniza- tion operations. barrier situation bad, ways worse, since highly likely incur contention. use bus interconnect exacerbatesthese problems, serialization could serious directory-basedmultiprocessor, latency would large. next subsection presentssome solutions useful either contention high processorcount large.EventNumber times process Corresponding source line Comment LLcounterlock lock (counterlock); processes try lock. Store conditional lock (counterlock); processes try lock. LDcount 1 count = count + 1; Successful process. Load linked i/C01 lock (counterlock); Unsuccessful process; try again. SDcount 1 count = count + 1; Miss get exclusive access. SDcounterlock 1 unlock(counterlock); Miss get lock. LDrelease 2 spin (release==local_sense);/ Read release: misses initially finally written. Figure I.4 actions, require bus transaction, taken ith process reaches barrier. last process reach barrier requires one less bus transaction, since read release spin hit cache!I-16 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsSynchronization Mechanisms Larger-Scale Multiprocessors would like synchronization mechanisms low latency uncontended cases minimize serialization case contentionis significant. begin showing software implementations improvethe performance locks barriers contention high; explore twobasic hardware primitives reduce serialization keeping latency low. Software Implementations major difficulty spin lock implementation delay due conten-tion many processes spinning lock. One solution artificiallydelay processes fail acquire lock. best performance isobtained increasing delay exponentially whenever attempt acquirethe lock fails. Figure I.5 shows spin lock exponential back-off imple- mented. Exponential back-off common technique reducing contention inshared resources, including access shared networks buses (see Sections F.4toF.8). implementation still attempts preserve low latency con- tention small delaying initial spin loop. result many pro- cesses waiting, back-off affect processes first attemptto acquire lock. could also delay process, result would poorer DADDUI R3,R0,#1 ;R3 = initial delay lockit: R2,0(R1) ;load linked BNEZ R2,lockit ;not available-spinDADDUI R2,R2,#1 ;get locked value SC R2,0(R1) ;store conditional BNEZ R2,gotit ;branch store succeedsDSLL R3,R3,#1 ;increase delay factor 2PAUSE R3 ;delays value R3 J lockit gotit: use data protected lock Figure I.5 spin lock exponential back-off. store conditional fails, process delays value R3. delay implemented decrementinga copy value R3 reaches 0. exact timing delay multiproces- sor dependent, although start value approximately time perform critical section release lock. statement pause R3 cause delay R3 time units. value R3 increased factor 2 every time store conditional fails, causes process wait twice long trying acquire lock again. small variations rate competing processorsexecute instructions usually sufficient ensure processes continually collide. natural perturbation execution time insufficient, R3 could initial- ized small random value, increasing variance successive delays andreducing probability successive collisions.I.4 Synchronization: Scaling ■I-17performance lock use two processes first one hap- pened find locked. Another technique implementing locks use queuing locks. Queuing locks work constructing queue waiting processors; whenever processorfrees lock, causes next processor queue attempt access. Thiseliminates contention lock freed. show queuing locks oper-ate next section using hardware implementation, software implementa-tions using arrays achieve benefits. look athardware primitives, let ’s look better mechanism barriers. barrier implementation suffers contention gather stage, must atomically update count, release stage, processes must read release flag. former serious itrequires exclusive access synchronization variable thus creates muchmore serialization; comparison, latter generates read contention. Wecan reduce contention using combining tree , structure multiple requests locally combined tree fashion. combining tree usedto implement release process, reducing contention there. combining tree barrier uses predetermined n-ary tree structure. use variable kto stand fan-in; practice, k¼4 seems work well. thekth process arrives node tree, signal next level tree. process arrives root, release waiting processes. ear-lier example, use sense-reversing technique. tree-based barrier, shown inFigure I.6 , uses tree combine processes single signal release barrier. MPPs (e.g., T3D CM-5) also included hardware sup-port barriers, recent machines relied software libraries support. Hardware Primitives subsection, look two hardware synchronization primitives. first primitive deals locks, second useful barriers number ofother user-level operations require counting supplying distinct indices. Inboth cases, create hardware primitive latency essentially iden- tical earlier version, much less serialization, leading better scaling contention. major problem original lock implementation introduces large amount unneeded contention. example, lock released allprocessors generate read write miss, although one processorcan successfully get lock unlocked state. sequence happens eachof 10 lock/unlock sequences, saw example page I-12. improve situation explicitly handing lock one waiting processor next. Rather simply allowing processors compete every time lock released, keep list waiting processors hand lockto one explicitly, turn comes. sort mechanism called aqueuing lock . Queuing locks implemented either hardware, weI-18 ■Appendix Large-Scale Multiprocessors Scientific Applicationsdescribe here, software using array keep track waiting processes. basic concepts either case. hardware implementation assumes directory-based multiprocessor individual processor caches addressable. bus-based multiprocessor, software implementation wouldbe appropriate would processor using different address forthe lock, permitting explicit transfer lock one process another. queuing lock work? first miss lock variable, miss sent synchronization controller, may integrated thememory controller (in bus-based system) directory controller. Ifthe lock free, simply returned processor. lock unavailable,struct node{/* node combining tree */ intcounterlock; /* lock node */ intcount; /* counter node */ intparent; /* parent tree = 0..P-1 except root */ }; struct node tree [0..P –1]; /* tree nodes */ intlocal_sense; /* private per processor */ intrelease; /* global release flag */ /* function implement barrier */ barrier (int mynode, int local_sense) { lock (tree[mynode].counterlock); /* protect count */tree[mynode].count=tree[mynode].count+1; /* increment count */ if(tree[mynode].count==k) {/* arrived mynode */ if(tree[mynode].parent >=0) { barrier(tree[mynode].parent); }else { release = local_sense; }; tree[mynode].count = 0; /* reset next time */ unlock (tree[mynode].counterlock); /* unlock */spin (release==local_sense); /* wait */ }; /* code executed processor join barrier */local_sense =! local_sense;barrier (mynode); Figure I.6 implementation tree-based barrier reduces contention consider- ably. tree assumed prebuilt statically using nodes array tree. node tree combines kprocesses provides separate counter lock, kprocesses contend node. kth process reaches node tree, goes parent, incrementing count parent. count parent node reaches k, release flag set. count node reset last process arrive. Sense-reversing used avoid races simple barrier. Thevalue tree[root].parent set /C01 tree initially built.I.4 Synchronization: Scaling ■I-19the controller creates record node ’s request (such bit vector) sends processor back locked value variable, processor spins on. lock freed, controller selects processor go aheadfrom list waiting processors. either update lock variablein selected processor ’s cache invalidate copy, causing processor miss fetch available copy lock. Example many bus transactions long take 10 processors lock unlock variable using queuing lock updates lock miss? Make assumptions system earlier example page I-12. Answer Fornprocessors, initially attempt lock access, generating bus trans- action; one succeed free lock, total n+1 transactions first processor. subsequent processor requires two bus transactions, one toreceive lock one free up. Thus, total number bus transactionsis (n+1)+2( n/C01)¼3n/C01. Note number bus transactions linear number processors contending lock, rather quadratic, spin lock examined earlier. 10 processors, requires 29 buscycles 2900 clock cycles. couple key insights implementing queuing lock capability. First, need able distinguish initial access lock, per-form queuing operation, also lock release, provide lock toanother processor. queue waiting processes implemented varietyof mechanisms. directory-based multiprocessor, queue akin shar-ing set, similar hardware used implement directory queuinglock operations. One complication hardware must prepared reclaimsuch locks, since process requested lock may context- switched may even scheduled processor. Queuing locks used improve performance barrier operation. Alternatively, introduce primitive reduces amount time neededto increment barrier count, thus reducing serialization bottleneck,which yield comparable performance using queuing locks. One primitivethat introduced building synchronization operationsisfetch-and-increment , atomically fetches variable increments value. returned value either incremented value fetched value. Using fetch-and-increment dramatically improve barrier implementa- tion, compared simple code-sensing barrier. Example Write code barrier using fetch-and-increment. Making assump- tions earlier example also assuming fetch-and-increment oper-ation, returns incremented value, takes 100 clock cycles, determine thetime 10 processors traverse barrier. many bus cycles required?I-20 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsAnswer Figure I.7 shows code barrier. nprocessors, implementation requires nfetch-and-increment operations, ncache misses access count, andncache misses release operation, total 3 nbus transactions. 10 processors, 30 bus transactions 3000 clock cycles. Like queue-ing lock, time linear number processors. course, fetch-and-increment also used implementing combining tree barrier, reducingthe serialization node tree. seen, synchronization problems become quite acute largerscale multiprocessors. challenges posed synchronization combinedwith challenges posed long memory latency potential load imbalancein computations, see getting efficient usage large-scale parallel pro- cessors challenging. I.5 Performance Scientific Applications Shared-Memory Multiprocessors section covers performance scientific applications Section I.3 symmetric shared-memory distributed shared-memory multiprocessors. Performance Scientific Workload Symmetric Shared-Memory Multiprocessor evaluate performance four scientific applications symmetric shared-memory multiprocessor using following problem sizes: ■Barnes-Hut —16 K bodies run six time steps (the accuracy control set 1.0, typical, realistic value)local_sense =! local_sense; /* toggle local_sense */ fetch_and_increment(count);/* atomic update */if (count==total) {/* arrived */ count=0;/* reset counter */ release=local_sense;/* release processes */ }else {/* come */ spin (release==local_sense);/* wait signal */ } Figure I.7 Code sense-reversing barrier using fetch-and-increment counting.I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-21■FFT—1 million complex data points ■LU—A 512 /C2512 matrix used 16 /C216 blocks ■Ocean —A 130 /C2130 grid typical error tolerance looking miss rates vary processor count, cache size, block size, decompose total miss rate coherence misses normal uni- processor misses. normal uniprocessor misses consist capacity, conflict, compulsory misses. label misses capacity misses thatis dominant cause benchmarks. measurements, weinclude coherence miss write misses needed upgrade block fromshared exclusive, even though one sharing cache block. mea-surement reflects protocol distinguish private andshared cache block. Figure I.8 shows data miss rates four applications, increase number processors 1 16, keeping problem size constant. increase number processors, total amount cache increases, usuallycausing capacity misses drop. contrast, increasing processor countusually causes amount communication increase, turn causing coher-ence misses rise. magnitude two effects differs application. FFT, capacity miss rate drops (from nearly 7% 5%) coherence miss rate increases (from 1% 2.7%), leading constantoverall miss rate. Ocean shows combination effects, including relate partitioning grid grid boundaries map cache blocks. typical 2D grid code communication-generated misses proportional theboundary partition grid, capacity misses proportional tothe area grid. Therefore, increasing total amount cache keepingthe total problem size fixed significant effect capacity missrate, least subgrid fits within individual processor ’s cache. sig- nificant jump miss rate one two processors occurs con-flicts arise way multiple grids mapped caches. conflict present direct-mapped two-way set associative caches, fades higher associativities. conflicts unusual array-based appli-cations, especially multiple grids use once. Barnes LU,the increase processor count little effect miss rate, sometimes causinga slight increase sometimes causing slight decrease. Increasing cache size usually beneficial effect performance, since reduces frequency costly cache misses. Figure I.9 illustrates change miss rate cache size increased 16 processors, showing portion miss rate due coherence misses uniprocessor capacity misses. Two effects lead miss rate decrease —at least quickly might expect —as cache size increases: inherent communication plateaus miss rate. Inherent communication leads certain frequency coherence misses thatare significantly affected increasing cache size. Thus, cache size isincreased maintaining fixed problem size, coherence miss rateI-22 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsMiss rate 0%3% 2% 1% 124 Processor countFFT 81 68% 4%7% 6% 5% Miss rate 0%6% 4% 2% 124 Processor countOcean 81 616%18%20% 8%14% 12% 10%Miss rate0%1% 124 Processor countLU 81 62%Miss rate0% 124 Processor countBarnes 81 61% Coherence miss rate Capacity miss rate Figure I.8 Data miss rates vary nonobvious ways processor count increased 1 16. miss rates include coherence capacity miss rates. compulsory misses benchmarks small included thecapacity misses. misses applications generated accesses data potentially shared, although applications larger miss rates (FFT Ocean), capacity misses rather coherence misses comprise themajority miss rate. Data potentially shared allocated portion ofthe address space used shared data. except Ocean, potentially shared data heavily shared, Ocean boundaries subgrids actually shared, although entire grid treated potentially shared data object. course,since boundaries change increase processor count (for fixed-size prob- lem), different amounts grid become shared. anomalous increase capacity miss rate Ocean moving 1 2 processors arises conflict misses inaccessing subgrids. cases except Ocean, fraction cache misses caused coherence transactions rises fixed-size problem run increas- ing number processors. Ocean, coherence misses initially fall add pro-cessors due large number misses write ownership misses data potentially, actually, shared. subgrids begin fit aggregate cache (around 16 processors), effect lessens. single-processor numbers includewrite upgrade misses, occur protocol even data actuallyshared, since shared state. runs, cache size 64 KB, two-way set associative, 32-byte blocks. Notice scale y-axis benchmark different, behavior individual benchmarks seenclearly.I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-23eventually limits decrease cache miss rate. effect obvious Barnes, coherence miss rate essentially becomes entire miss rate. less important effect temporary plateau capacity miss rate arises application fraction data present cache significant portion dataset fit cache caches areslightly bigger. LU, small cache (about 4 KB) capture pair of16/C216 blocks used inner loop; beyond that, next big improvement capacity miss rate occurs matrices fit caches, occurs whenthe total cache size 4 MB 8 MB. effect, sometimes called aworking set effect , partly work 32 KB 128 KB FFT, capacity miss rate drops 0.3%. Beyond cache size, faster decrease capacity miss rate seen, major data structure begins reside cache. plateaus common programs deal large arrays structuredfashion. Increasing block size another way change miss rate cache. uniprocessors, larger block sizes often optimal larger caches. Miss rate 0%4% 2% 32 64 128 Cache size (KB)FFT 25610% 6%8% Miss rate 0%1.5% 1.0% 32 64 128 Cache size (KB)LU 2562.5% 2.0%Miss rate 0%6% 2%4% 32 64 128 Cache size (KB)Ocean 25614% 10% 8%12%Miss rate 0%1.0% 32 64 128 Cache size (KB)Barnes 2562.0% 1.5% Coherence miss rate Capacity miss rate Figure I.9 miss rate usually drops cache size increased, although coher- ence misses dampen effect. block size 32 bytes cache two-way set associative. processor count fixed 16 processors. Observe scale graph different.I-24 ■Appendix Large-Scale Multiprocessors Scientific Applicationsmultiprocessors, two new effects come play: reduction spatial locality shared data potential increase miss rate due false sharing. Several studies shown shared data l ower spatial locality unshared data. Poorer locality means that, shared data, fetching larger blocks lesseffective uniprocessor probability higher blockwill replaced contents referenced. Likewise, increasing thebasic size also increases potential frequency false sharing, increasingthe miss rate. Figure I.10 shows miss rates cache block size increased 16- processor run 64 KB cache. interesting behavior Barnes, miss rate initially declines rises due increase numberof coherence misses, probably occurs false sharing. otherbenchmarks, increasing block size decreases overall miss rate. Ocean andLU, block size increase affects coherence capacity miss rates aboutequally. FFT, coherence miss rate actually decreased faster rate thanthe capacity miss rate. reduction occurs communication FFT isstructured efficient. less optimized programs, would expect Miss rate 0%6% 4% 2% 16 32 64 Block size (bytes)FFT 12814% 10% 8%12% Miss rate 0%2% 1% 16 32 64 Block size (bytes)LU 1284% 3%Miss rate 0%6% 2%4% 16 32 64 Block size (bytes)Ocean 12814% 10% 8%12%Miss rate 0% 16 32 64 Block size (bytes)Barnes 1281% Coherence miss rate Capacity miss rate Figure I.10 data miss rate drops cache block size increased. results 16-processor run 64 KB cache two-way set associativity. Onceagain use different scales benchmark.I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-25false sharing less spatial locality shared data, resulting behavior like Barnes. Although drop miss rates longer blocks may lead believe choosing longer block size best decision, bottleneck bus-basedmultiprocessors often limited memory bus bandwidth. Larger blocksmean bytes bus per miss. Figure I.11 shows growth bus traffic block size increased. growt h serious programs high miss rate, especially Ocean. growth traffic actually lead performance slowdowns due longer miss penalties increased bus contention. Performance Scientific Workload Distributed-Memory Multiprocessor performance directory-based multiprocessor depends many factors influence performance bus-based multiprocessors (e.g., cachesize, processor count, block size), well distribution misses var-ious locations memory hierarchy. location requested data itemdepends initial allocation sharing patterns. start exam- ining basic cache performance scientific/technical workload look effect different types misses.7.0 4.05.06.0 3.0 2.0 1.0Bytes per data reference 0.0 Block size (bytes)16 32 64 128FFT LU BarnesOcean Figure I.11 Bus traffic data misses climbs steadily block size data cache increased. factor 3 increase traffic Ocean best argument larger block sizes. Remember protocol treats ownership upgrade misses misses, slightly increasing penalty large cache blocks;in Ocean FFT, simplification accounts less 10% traffic.I-26 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsBecause multiprocessor larger longer latencies snooping-based multiprocessor, begin slightly larger cache (128 KB) larger block size 64 bytes. distributed-memory architectures, distribution memory requests local remote key performance affects con-sumption global bandwidth latency seen requests. Therefore, forthe figures section, separate cache misses local remoterequests. looking figures, keep mind that, applications, mostof remote misses arise coherence misses, although capacity mis- ses also remote, applications poor data distribution misses significant. AsFigure I.12 shows, miss rates cache sizes affected much changes processor count, exception Ocean, missrate rises 64 processors. rise results two factors: increase map-ping conflicts cache occur grid becomes small, leads arise local misses, increase number coherence misses, whichare remote. Figure I.13 shows miss rates change cache size increased, assuming 64-processor execution 64-byte blocks. miss rates decreaseat rates might expect, although dampening effect caused little noreduction coherence misses leads slower decrease remote misses thanin local misses. time reach largest cache size shown, 512 KB, theremote miss rate equal greater local miss rate. Larger caches wouldamplify trend. examine effect changing block size Figure I.14 . applications good spatial locality, increases block size reduce miss rate, even large blocks, although performance benefits going largestblocks small. Furthermore, improvement miss rate comes froma reduction local misses. Rather plot memory traffic, Figure I.15 plots number bytes required per data reference versus block size, breaking requirement localand global bandwidth. case bus, simply aggregate demands ofeach processor find total demand bus memory bandwidth. scal- able interconnect, use data Figure I.15 compute required per- node global bandwidth estimated bisection bandwidth, next exampleshows. Example Assume 64-processor multiprocessor 1 GHz processors sustain one memory reference per processor clock. 64-byte block size, remote missrate 0.7%. Find per-node estimated bisection bandwidth FFT.Assume processor stall remote memory requests; mightbe true if, example, remote data prefetched. bandwidthrequirements compare various interconnection technologies?I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-27FFT performs all-to-all communication, bisection bandwidth equal number processors times per-node bandwidth, 64 /C2448 MB/ sec¼28.7 GB/sec. SGI Origin 3000 64 processors bisection band- width 50 GB/sec. standard networking technology comes close. Answer per-node bandwidth simply number data bytes per reference times reference rate: 0.7% /C21 GB/sec /C264¼448 MB/sec. rate somewhat higher hardware sustainable transfer rate CrayT3E (using block prefetch) Miss rate 0%3% 2% 1% 81 6 3 2 Processor countFFT 646% 4%5% Miss rate 0.0%0.5% 81 6 3 2 Processor countLU 641.0% Miss rate 0%4% 2% 81 6 3 2 Processor countOcean 648% 6%Miss rate 0.0% 81 6 3 2 Processor countBarnes 640.5% Local misses Remote misses Figure I.12 data miss rate often steady processors added benchmarks. grid structure, Ocean initially decreasing miss rate, rises 64 processors. Ocean, local miss rate drops 5% 8 processors 2% 32, rising 4% 64. remote miss rate Ocean, driven primarily communication, rises monotonically 1% 2.5%. Note that, toshow detailed behavior benchmark, different scales used y-axis. cache runs 128 KB, two-way set associative, 64-byte blocks. Remote misses include misses require communication another node,whether fetch data deliver invalidate. particular, figure data section, measurement remote misses includes write upgrade misses data date local memory cached elsewhere and, therefore,require invalidations sent. invalidations indeed generate remote traffic,but may may delay write, depending consistency model.I-28 ■Appendix Large-Scale Multiprocessors Scientific Applicationsand lower SGI Origin 3000 (1.6 GB/processor pair). FFT per-node bandwidth demand exceeds bandwidth sustainable fasteststandard networks factor 5. previous example looked bandwidth demands. key issue parallel program remote memory access time, latency. get insight this,we use simple example directory-based multiprocessor. Figure I.16 shows parameters assume simple multiprocessor model. assumes thetime first word local memory access 85 processor cycles path local memory 16 bytes wide, network interconnect 4 bytes wide. model ignores effects contention, probably serious inthe parallel benchmarks examine, possible exception FFT, whichuses all-to-all communication. Contention could serious performanceimpact workloads. Miss rate 0%4% 2% 32 64 128 Cache size (KB)FFT 256 51210% 6%8% Miss rate 0.0%1.0% 0.5% 32 64 128 Cache size (KB)LU Ocean256 5122.5% 1.5%2.0%Miss rate 0.0%0.5% 32 64 128 Cache size (KB)Barnes 256 5121.5% 1.0% Miss rate 0%10% 5% 32 64 128 Cache size (KB)256 51220% 15% Local misses Remote misses Figure I.13 Miss rates decrease cache sizes grow. Steady decreases seen local miss rate, remote miss rate declines varying degrees, depending onwhether remote miss rate large capacity component driven primarily communication misses. cases, decrease local miss rate larger decrease remote miss rate. plateau miss rate FFT, men-tioned last section, ends cache exceeds 128 KB. runs done 64 processors 64-byte cache blocks.I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-29Figure I.17 shows cost cycles average memory reference, assuming parameters Figure I.16 . latencies reference type counted. bar indicates contribution cache hits, localmisses, remote misses, three-hop r emote misses. cost influenced total frequency cache misses upgrades, well distri-bution location miss satisfied. cost remote mem-ory reference fairly steady processor count increased, except Ocean. increasing miss rate Ocean 64 processors clear Figure I.12 . miss rate increases, expect time spent memory references increase also. Although Figure I.17 shows memory access cost, dominant multiprocessor cost benchmarks, complete performance model wouldneed consider effect contention memory system, well thelosses arising synchronization delays. Miss rate 0%4%6% 2% 16 32 64 Block size (bytes)FFT 12812% 8%10% Miss rate 0%2% 1% 16 32 64 Block size (bytes)LU 1284% 3%Miss rate 0%5%10% 16 32 64 Block size (bytes)Ocean 12815%Miss rate 0.0%0.1% 16 32 64 Block size (bytes)Barnes 1280.3% 0.2% Local misses Remote misses Figure I.14 Data miss rate versus block size assuming 128 KB cache 64 pro- cessors total. Although difficult see, coherence miss rate Barnes actually rises largest block size, last section.I-30 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsBytes per data reference Bytes per data reference Bytes per data reference Bytes per data reference0.02.03.0 1.0 16 32 64 Block size (bytes)FFT 1286.0 4.05.0 0.00.20.3 0.1 16 32 64 Block size (bytes)LU 1280.6 0.40.5 0.02.04.06.0 5.0 3.0 1.0 16 32 64 Block size (bytes)Ocean 1287.0 0.00.1 16 32 64 Block size (bytes)Barnes 1280.4 0.3 0.2 Local Global Figure I.15 number bytes per data reference climbs steadily block size increased. data used determine bandwidth required per node internally globally. data assume 128 KB cache 64 processors. CharacteristicProcessor clock cycles ≤16 processorsProcessor clock cycles 17–64 processors Cache hit 1 1 Cache miss local memory 85 85Cache miss remote home directory125 150 Cache miss remotely cached data (three-hop miss)140 170 Figure I.16 Characteristics example directory-based multiprocessor. Misses serviced locally (including local directory), remote home node,or using services home node another remote node caching exclusive copy. last case called three-hop miss higher cost requires interrogating home directory remote cache. Note thissimple model account invalidation time include factorfor increasing interconnect time. remote access latencies based SGI Origin 3000, fastest scalable interconnect system 2001, assume 500 MHz processor.I.5 Performance Scientific Applications Shared-Memory Multiprocessors ■I-31Average cycles per reference 0.0 81 6 3 2 Processor countFFT 645.5 5.04.54.03.53.02.52.01.51.00.5 Average cycles per reference 0.0 81 6 3 2 Processor countLU 645.5 5.04.54.03.53.02.52.01.51.00.5Average cycles per memory reference 0.0 81 6 3 2 Processor countBarnes 645.5 5.0 4.5 4.03.5 3.0 2.52.0 1.5 1.0 0.5 Average cycles per reference 0.0 81 6 3 2 Processor countOcean 645.5 5.0 4.5 4.03.5 3.0 2.52.0 1.5 1.0 0.5 Cache hit Local miss Remote miss Three-hop miss remote cache Figure I.17 effective latency memory references DSM multiprocessor depends relative frequency cache misses location memory accesses served. plots show memory access cost (a metric called average memory access time Chapter 2 ) benchmarks 8, 16, 32, 64 processors, assuming 512 KB data cache two-way set asso- ciative 64-byte blocks. average memory access cost composed four dif- ferent types accesses, cost type given Figure I.16 . Barnes LU benchmarks, low miss rates lead low overall access times. FFT, thehigher access cost determined higher local miss rate (1 –4%) significant three-hop miss rate (1%). improvement FFT comes reduction local miss rate 4% 1%, aggregate cache increases. Ocean shows biggestchange cost memory accesses, highest overall cost 64 processors. high cost driven primarily high local miss rate (average 1.6%). memory access cost drops 8 16 processors grids easily fit individualcaches. 64 processors, dataset size small map properly local misses coherence misses rise, saw Figure I.12 .I-32 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsI.6 Performance Measurement Parallel Processors Scientific Applications One controversial issues parallel processing mea- sure performance parallel processors. course, straightforward answeris measure benchmark supplied examine wall-clock time. Measuringwall-clock time obviously makes sense; parallel processor, measuring CPUtime misleading processors may idle unavailable forother uses. Users designers often interested knowing well mul- tiprocessor performs certain fixed number processors, also performance scales processors added. many cases, makes sense toscale application benchmark, since benchmark unscaled, effects aris-ing limited parallelism increases communication lead resultsthat pessimistic expectation processors used tosolve larger problems. Thus, often useful measure speedup processorsare added fixed-size problem scaled version problem,providing unscaled scaled version speedup curves. choice measure uniprocessor algorithm also important avoid anomalous results, since using parallel version benchmark may understate theuniprocessor performance thus overstate speedup. decided measure scaled speedup, question howto scale application. Let ’s assume determined running benchmark sizenonpprocessors makes sense. question scale benchmark run m/C2pprocessors. two obvious ways scale problem: (1) keeping amount memory used per processor constant, (2) keeping total execution time, assuming perfect speedup, constant. first method, called memory-constrained scaling , specifies running problem size m/C2n onm/C2pprocessors. second method, called time-constrained scaling , requires know relationship running time problem size, sincethe former kept constant. example, suppose running time appli-cation data size nonpprocessors proportional n 2/p. Then, time- constrained scaling, problem run problem whose ideal running timeonm/C2pprocessors still n 2/p. problem ideal running time sizeﬃﬃﬃﬃmp/C2n. Example Suppose problem whose execution time problem size nis pro- portional n3. Suppose actual running time 10-processor multiprocessor 1 hour. time-constrained memory-constrained scaling models,find size problem run effective running time a100-processor multiprocessor. Answer time-constrained problem, ideal running time same, 1 hour, problem size isﬃﬃﬃﬃﬃ 10 3p /C2n 2.15 times larger original. ForI.6 Performance Measurement Parallel Processors Scientific Applications ■I-33memory-constrained scaling, size problem 10 nand ideal execution time 103/10, 100 hours! Since users reluctant run problem order magnitude processors 100 times longer, size problem isprobably unrealistic. addition scaling methodology, questions pro- gram scaled increasing problem size affects quality theresult. Often, must change application parameters deal effect.As simple example, consider effect time convergence solving dif- ferential equation. time typically increases problem size increases, since, example, often require iterations larger problem. Thus,when increase problem size, total running time may scale faster thebasic algorithmic scaling would indicate. example, suppose number iterations grows log prob- lem size. Then, problem whose algorithmic running time linear size ofthe problem, effective running time actually grows proportional nlogn.I fw e scaled problem size mon 10 processors, purely algorithmic scaling would allow us run problem size 10 mon 100 processors. Accounting increase iterations means problem size k/C2m, klogk¼10, running time 100 processors. problem size yields scalingof 5.72 m, rather 10 m. practice, scaling deal error requires good understanding application may involve factors, error tolerances (for example,it affects cell-opening criteria Barnes-Hut). turn, effects often signif-icantly affect communication parallelism properties application well choice problem size. Scaled speedup unscaled (or true) speedup; confusing two led erroneous claims (e.g., see discussion Section I.6 ). Scaled speedup important role, scaling methodology sound theresults clearly reported using scaled version application. Singh,Hennessy, Gupta [1993] described issues detail. I.7 Implementing Cache Coherence section, explore challenge implementing cache coherence, starting first dealing challenges snooping coherence protocol, simply alluded Chapter 5 . Implementing directory protocol adds additional complexity snooping protocol, primarily arising fromthe absence broadcast, forces use different mechanism toresolve races. Furthermore, larger processor count directory-basedmultiprocessor means cannot retai n assumptions unlimited buffering must find new ways avoid deadlock, Let ’s start snooping protocols.I-34 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsAs mentioned Chapter 5 , challenge implementing misses snooping coherence protocol without bus lies finding way make multi- step miss process appear atomic. upgrade miss write miss require thesame basic processing generate implementation challenges; sim-plicity, focus upgrade misses. steps handling upgrademiss: 1.Detect miss compose invalidate message transmission caches. 2.When access broadcast communication link available, transmit message. 3.When invalidates processed, processor updates state cache block proceeds write caused upgrade miss. two related difficulties arise. First, two processors, P1 P2, attempt upgrade cache block time resolve therace? Second, step 3, processor know invalidates havebeen processed complete step? solution finding winner race lies ordering imposed broadcast communication medium. communication medium must broadcastany cache miss nodes. P1 P2 attempt broadcast time,we must ensure either P1 ’s message reach P2 first P2 ’s reach P1 first. property true single channel ingoing outgoing requests node must pass communication networkdoes accept message unless guarantee delivery (i.e., effectively circuitswitched, see Appendix F ). P1 P2 initiate attempts broadcast invalidate simultaneously, network accept one operationsand delay other. ordering ensures either P1 P2 complete com-munication step 2 first. network explicitly signal accepts mes-sage guarantee next transmission; alternatively, processor simply watch network request, knowing request seen, fully transmitted processors subsequent messages. Now, suppose P1 wins race transmit invalidate; knows race, continue step 3 complete miss handling. apotential problem, however, P2. P2 undertook step 1, believed theblock shared state, P1 advance step 3, must know P2has processed invalidate, must change state block P2 inva-lid! One simple solution P2 notice lost race, observing P1’s invalidate broadcast invalidate. P2 invalidate block generate write miss get data. P1 see invalidate beforeP2’s, change block modified update data, guarantees forward progress avoids deadlock. P1 sees subsequent invalidate ablock Modified state (a possibility cannot arise basic protocoldiscussed Chapter 5 ), knows winner race. simplyI.7 Implementing Cache Coherence ■I-35ignore invalidate, knowing followed write miss, write block back memory make state invalid. Another solution give precedence incoming requests outgoing requests, P2 transmit invalidate must handle pending inval-idates write misses. misses blocks address apending outgoing message, processor must prepared restart write oper-ation, since incoming request may cause state block change. Noticethat P1knows invalidates willbeprocessed onceit successfully completedthe broadcast, since precedence given invalidate messages outgoing requests.(Becauseitdoesnotemploybroadcast,aprocessorusingadirectoryprotocol cannot know invalidate received; instead, explicit acknowledgments arerequired, discuss next section. Indeed, see, cannot even knowit race become owner request acknowledged.) Reads also require multiple-step process, since need get data back memory remote cache (in write-back cache system), reads donot introduce fundamentally new problems beyond exists writes. are, however, additional tricky edge cases must handled cor- rectly. example, write-back cache, processor generate read miss requires write-back, could delay, giving read miss priority. asnoop request appears cache block written back, processor mustdiscover send data back. Failure create deadlock situation. Asimilar tricky situation exists processor generates write miss, willmake block exclusive, but, processor receives data updatethe block, processors generate readmissesfor block. readmissescannotbe processed writing processor receives data updates block. One difficult problems occurs write-back cache data read write miss come either memory one processorcaches, requesting processor know priori data come from. bus-based systems, single global signal used indicatewhether processor exclusive (and hence up-to-date) copy; oth-erwise, memory responds. schemes work pipelined intercon-nection requiring processors signal whether exclusive copywithin fixed number cycles miss broadcast. modern multiprocessor, however, essentially impossible bound amount time required snoop request processed. Instead, mechanismis required determine whether memory up-to-date copy. One solutionis add coherence bits memory, indicating whether data exclusive ina remote cache. mechanism begins move toward directory approach,whose implementation challenges consider next. Implementing Cache Coherence DSM Multiprocessor Implementing directory-based cache coherence protocol requires overcomingall problems related nonatomic act ions snooping protocol withoutI-36 ■Appendix Large-Scale Multiprocessors Scientific Applicationsthe use broadcast (see Chapter 5 ) ,w h c hf r c e das e r l z nc p e n g writes also ensured serializatio n required memory consistency model. Avoiding need broadcast central goal directory-basedsystem, another method ensuring serialization necessary. serialization requests exclusive access memory block eas- ily enforced since requests serialized reach unique directory specified block. th e directory controller simply ensures one request completely serviced next begun, writes seri- alized. requesters cannot know ahead time win race communication broadcast, directory must sig- nal winner completes processing winner ’sr e q u e . done message supplies data write miss anexplicit acknowledgment message grants ownership response aninvalidation request. loser race? simplest solution system send anegative acknowledge ,o rNAK , requires requesting node regen- erate request. (This equivalent collision broadcast network snooping scheme, requires one transmitting nodes retry com- munication.) see next section NAK approach, opposed tobuffering request, attractive. Although acknowledgment requesting node ownership com- pleted write miss ownershi p acknowledgment message transmit- ted, still know invalidates received processed bythe nodes sharing set. memory consistency models eventuallyrequire (either next cache miss synchronization point, exam- ple) processor knows invalidates write pro- cessed. snooping scheme, nature broadcast network providesthis assurance. know invalidates complete directory scheme? way know invalidates completed des-tination nodes invalidate messages (the members sharing set) explic-itly acknowledge invalidation messages sent directory. shouldthey acknowledged to? two possibilities. first acknowledg- ments sent directory, count them, acknowl- edgments received, confirm single message originalrequester. Alternatively, granting ownership, directory tell reg-ister many acknowledgments expect. destinations invalidatemessages send acknowledgment directly requester, whose iden-tity provided directory. existing implementations use latterscheme, since reduces possibility creating bottleneck directory.Although requirement acknowledgments additional complexity directory protocols, requirement arises avoidance serialization mechanism, snooping broadcast operation, limitto scalability.I.7 Implementing Cache Coherence ■I-37Avoiding Deadlock Limited Buffering new complication implementation introduced potential scale directory-based multiprocessor. Chapter 5 , assumed network could always accept coherence message request would acted upon atsome point. much larger multiprocessor, assumption unlimited buffer-ing may unreasonable. happens network unlimitedbuffering? major implication limit cache directory controllermay unable complete message send. could lead deadlock. potential deadlock arises three properties, characterize many deadlock situations: 1.More one resource needed complete transaction: Message buffers needed generate requests, create replies acknowledgments, andaccept replies. 2.Resources held nonatomic transaction completes: buffer used create reply cannot freed reply accepted, reasons willsee shortly. 3.There global partial order acquisition resources: Nodes gen- erate requests replies will. characteristics lead deadlock, avoiding deadlock requires breaking one properties. Freeing resources without completing transaction difficult, since transaction must completely backed cannot left half-finished. Hence, approach try resolve need multipleresources. cannot simply eliminate need, try ensure theresources always available. One way ensure transaction always complete guarantee always buffers accept messages. Although possible smallmultiprocessor processors block cache miss small numberof outstanding misses, may practical directory protocol, since single write could generate many invalidate messages. addition, features prefetch multiple outstanding misses increase amount bufferingrequired. alternative strategy, systems use whichensures transaction actually initiated guarantee thatit resources complete. strategy four parts: 1.A separate network (physical virtual) used requests replies, reply message controller waits transitioning states. ensures new requests cannot block replies free buffers. 2.Every request expects reply allocates space accept reply request generated. space available, request waits. ensures node always accept reply message, allow replying node tofree buffer.I-38 ■Appendix Large-Scale Multiprocessors Scientific Applications3.Any controller reject NAK request, never NAK reply. prevents transaction starting controller cannot guarantee buffer space reply. 4.Any request receives NAK response simply retried. see deadlocks four properties above, must ensure replies accepted every request eventually ser-viced. Since cache controller directory controller always allocates buffer tohandle reply issuing request, always accept reply itreturns. see every request eventually serviced, need show thatany request could completed. Since every request starts read writemiss cache, sufficient show read write miss eventually serviced. Since write miss case includes actions read miss sub- set, focus showing write misses serviced. simplest situation iswhen block uncached; since case subsumed case theblock shared, focus shared exclusive cases. Let ’s consider case block shared: ■The CPU attempts write generates write miss sent thedirectory. simplicity, assume processor stalled. Although may issue requests, issue request cache block first one completed. Requests independent blocks behandled separately. ■The write miss sent directory controller memory block. Notethat although one cache controller handles requests given cacheblock, regardless memory contents, directory controller handlesrequests different blocks independent events (assuming sufficient buff-ering, allocated directory issues messages onbehalf request). conflict directory controller whentwo requests arrive block. controller must wait first operation completed. simply NAK second request buffer it, service second request given memory block untilthe first completed. ■Now consider happens directory controller: Suppose writemiss next thing arrive directory controller. controllersends invalidates, always accepted limiteddelay cache controller. Note one possibility cachecontroller outstanding miss block. dual caseto snooping scheme, must break tie forcingthe cache controller accept act directory request. Depending exact timing, cache controller either get cache line later directory receive NAK restart theprocess.I.7 Implementing Cache Coherence ■I-39The case block exclusive somewhat trickier. analysis begins write miss arrives directory controller processing. two cases consider: ■The directory controller sends fetch/invalidate message processorwhere arrives find block exclusive state. cache controllersends data write-back home directory makes state invalid. Thisreply arrives home directory controller, always accept thereply, since preallocated buffer. directory controller sends back data requesting processor, always accept reply; cache updated, requesting cache controller notifies processor. ■The directory controller sends fetch/invalidate message node indicated owner. message arrives owner node, finds cache controller taken read write miss caused block replaced. Inthis case, cache controller already sent block home directorywith data write-back made data unavailable. Since exactly theeffect fetch/invalidate message, protocol operates correctly thiscase well. shown coherence mechanism operates correctly cache directory controller accept requests operation cache blocks outstanding operations progress, replies arealways accepted, requests NAKed forced retry. Likethe case snooping protocol, cache controller must able break ties,and always favoring instructions directory. ability toNAK requests allows implementation finite buffering avoiddeadlock. Implementing Directory Controller implement cache coherence scheme, cache controller must sameabilities needed snooping case, namely, capability handling requestsfor independent blocks awaiting response request local pro-cessor. incoming requests still processed order, one com-pleted beginning next. cache controller receive manyrequests short period time, NAK them, knowing directory subsequently regenerate request. directory must also multithreaded able handle requests mul- tiple blocks independently. situation somewhat different thecache controller handle incoming requests independent blocks, since direc-tory controller need begin processing one request earlier one stillunderway. directory controller cannot wait one complete servic-ing next request, since could lead deadlock. Instead, directory con-troller must reentrant ; is, must capable suspending executionI-40 ■Appendix Large-Scale Multiprocessors Scientific Applicationswhile waiting reply accepting another transaction. place must occur response read write misses, waiting response owner. leads three important observations: 1.The state controller need saved restored either fetch operation remote location fetch/invalidate outstanding. 2.The implementation bound number outstanding transactions handled directory simply NAKing read write miss requests thatcould cause number outstanding requests exceeded. 3.If instead returning data directory, owner node forwards data directly requester (as well returning directory), caneliminate need directory handle one outstandingrequest. motivation, addition reduction latency, reasonfor using forwarding style protocol. complexities forwarding protocols arise requests arrive closely spaced time. major remaining implementation difficulty handle NAKs. One alter- native processor keep track outstanding transactions itknows, NAK received, requested transaction was. alter-native bundle original request NAK, controller receiv-ing NAK determine original request was. every requestallocates slot receive reply NAK reply, NAKs always received. fact, buffer holding return slot request also hold information request, allowing processor reissue request itis NAKed. practice, great care required implement protocols correctly avoid deadlock. key ideas seen section —dealing nona- tomicity finite buffering —are critical ensuring correct implementation. Designers found formal informal verification techniques arehelpful ensuring implementations correct. I.8 Custom Cluster Approach: Blue Gene/L Blue Gene/L (BG/L) scalable message-passing supercomputer whose designoffers unprecedented computing density measured compute power per watt.By focusing power efficiency, BG/L also achieves unmatched throughput percubic foot. High computing density, combined cost-effective nodes exten- sive support RAS, allows BG/L efficiently scale large processor counts. BG/L distributed-memory, message-passing computer one quite different cluster-based, often throughput-oriented computers rely oncommodity technology processors, interconnect, and, sometimes, pack-aging system-level organization. BG/L uses special customized processingnode contains two processors (derived low-power, lower-clock-ratePowerPC 440 chips used embedded market), caches, interconnect logic.I.8 Custom Cluster Approach: Blue Gene/L ■I-41A complete computing node formed adding SDRAM chips, commodity semiconductor parts BG/L design. BG/L consists 64 K nodes organized 32 racks containing 1 K nodes 50 cubic feet. rack contains two double-sided boards 512nodes each. Due high density within board rack, 85% intercon-nect within single rack, greatly reducing complexity latency associatedwith connections racks. Furthermore, compact size rack, isenabled low power high density node, greatly improves effi-ciency, since interconnection network connections within single rack integrated single compute chip comprises node. Appendix F discusses main BL/G interconnect network, three- dimensional torus. four networks: Gigabit Ethernet, connected atdesignated I/O nodes; JTAG network used test; barrier network; globalcollective network. barrier network contains four independent channels andcan used performing global oror global across processors latency less 1.5 microseconds. global collective network connectsall processors tree used global operations. supports variety integer reductions directly, avoiding need involve processor, leading times large-scale reductions 10 100 times faster typicalsupercomputers. collective network also used broadcast singlevalue efficiently. Support collective network well torus includedin chip forms heart processing node. Blue Gene/L Computing Node BG/L node consists single processing chip several SDRAM chips.The BG/L processing chip, shown Figure I.18 , contains following: 1.Two PowerPC 440 CPUs, two-issue superscalar seven-stage pipeline speculative out-order issue capability, clocked modest(and power-saving) 700 MHz. CPU separate 32 KB cachesthat nonbblocking four outstanding misses. Cache coherencemust enforced software. CPU also contains pair floating-point coprocessors, FP register set capable issuing multiply-add clock cycle, supporting special SIMD instruction setcapability includes complex arithmetic using pair registers and128-bit operands. 2.Separate fully associative L2 caches, 2 KB data 128-byte block size, act essentially like prefetch buffers. L2 cache controllersrecognize streamed data access also handle prefetch L3 main mem-ory. low latency (11 cycles) provide high bandwidth (5 bytes perclock). L2 prefetch buffer supply 5.5 GB/sec L1 caches. 3.A 4 MB L3 cache implemented embedded DRAM. L2 buffer con- nected bus supplying 11 GB/sec bandwidth L3 cache.I-42 ■Appendix Large-Scale Multiprocessors Scientific Applications4.A memory bus supporting 256 512 MB DDR DRAMS providing 5.5 GB/sec memory bandwidth L3 cache. amount memory might seem rather modest node, given node contains two pro- cessors, two FP units. Indeed Amdahl ’s rule thumb (1 MB per 1 MIPS) assumption 25% peak performance would favor 2.7times memory per node. floating-point-intensive applications thecomputational need usually grows faster linear memory size, theupper limit 512 MB/node probably reasonable. 5.Support logic five interconnection networks. Byplacing logicother thanDRAMsintoa singlechip,BG/Lachieveshigher density, lower power, lower cost, making possible pack processing nodesextremely densely.The density interms allowsthe interconnection networkstobelow latency,highbandwidth, quite costeffective.The combination yieldsa supercom- puter scales cost-effectively, yielding order-of-magnitude improvement32K/32K L1256 11 GB/sec 2565.5 GB/sec 1024 22 GB/sec144 ECCL2 prefetch buffer128PPC 440 CPUShared L3 directory embedded DRAM Includes error correction control (ECC)Double-issue FPU 32K/32K L1 Ethernet Gbit Gigabit EthernetSnoop L2 prefetch buffer128 PPC 440 CPU Double-issue FPU JTAG access IEEE 1149.1 (JTAG)Torus 6 6 in, 1.4 GB/sec link5.5 GB/secCollective 3 3 in, 2.8 GB/sec linkGlobal interrupt/ lockbox 4 global barriers interruptsDDR control ECC 144-bit-wide DDR 256/ 512 MB2564 MB embedded DRAM L3 cache memory 25611 GB/sec 128 Figure I.18 BG/L processing node. unfilled boxes PowerPC processors added floating-point units. solid gray boxes network interfaces, theshaded lighter gray boxes part memory system, supplemented byDDR RAMS.I.8 Custom Cluster Approach: Blue Gene/L ■I-43in GFLOPs/watt approaches well significant improvements GFLOPS/ $for large-scale multiprocessors. example, BG/L 64 K nodes peak performance 360 TF uses 1.4 megawatts. achieve 360 TF peak using Power5+, power-efficient, high-end FP processor, would require 23,500 processors (the dual processor execute 8 FLOPs/clock 1.9 GHz). power requirement processors, without external cache, DRAM, interconnect, would 2.9 megawatts, double power entire BG/L system. Likewise, smaller die size BG/L node need DRAMs external chip produce significant cost savings versus node built using high-end multiprocessor. Figure I.19 shows photo 64K node BG/L. total size occupied 128K-processor multiprocessor com- parable occupied earlier multiprocessors 16K processors. I.9 Concluding Remarks landscape large-scale multiprocessors changed dramatically past five ten years. form clustering used largest-scale multiprocessors, calling “clusters ”ignores significant differ- ences architecture, implementation style, cost, performance. Bell Gray Figure I.19 64 K-processor Blue Gene/L system.I-44 ■Appendix Large-Scale Multiprocessors Scientific Applications[2002] discussed trend, arguing clusters dominate. Dongarra et al. [2005] agreed form clustering almost inevitable largestmultiprocessors, developed nuanced classification attempts dis-tinguish among variety different approaches. InFigure I.20 summarize range terminology used large-scale multiprocessors focus defining terms architecturaland implementation perspective. Figure I.21 shows hierarchical relationship different architecture approaches. Although conver- gence architectural approaches past 15 years, TOP500 list, reports 500 fastest computers world measured Linpack bench-mark, includes commodity clusters, customized clusters, Symmetric Multiproces-sors (SMPs), DSMs, constellations, well processors scalarand vector.Terminology Characteristics Examples MPP Originally referred class architectures characterized large numbers small, typically custom processors usually using SIMD style architecture.Connection Machines CM-2 SMP (symmetric multiprocessor)Shared-memory multiprocessors symmetric relationship memory; also called UMA (uniform memory access). Scalableversions architectures used multistage interconnection networks, typically configured 64 128 processors.SUN Sunfire, NEC Earth Simulator DSM (distributed shared memory)A class architectures support scalable shared memory distributed fashion. architectures available andwithout cache coherence typically support hundreds thousands processors.SGI Origin Altix, Cray T3E, Cray X1, IBM p5 590/5 Cluster class multiprocessors using message passing. individual nodes either commodities customized, likewise theinterconnect.See commodity custom clusters Commodity clusterA class clusters nodes truly commodities, typically headless workstations, motherboards, blade servers, connected SAN LAN usually accessible via I/O bus.“Beowulf ”and “homemade ”clusters Custom cluster cluster architecture nodes interconnect customized tightly integrated commoditycluster. Also called distributed memory message passingmultiprocessors.IBM Blue Gene, Cray XT3 Constellation Large-scale multiprocessors use clustering smaller-scale multiprocessors, typically DSM SMP architecture 32 processors.Larger SGI Origin/Altix, ASC Purple Figure I.20 classification large-scale multiprocessors. term MPP, original meaning described above, used recently, less precisely, refer large-scale multiprocessors. None commercial shipping multiprocessors true MPP original sense word, approach may make sense future. SMP DSM class includes multiprocessors vector support. termconstellation used different ways; usage seems intuitive precise [Dongarra et al. 2005].I.9 Concluding Remarks ■I-45Nonetheless, clearly emerging trends, see looking distribution types multiprocessors TOP500 list: 1.Clusters represent majority systems. lower development effort clusters clearly driving force making popular. high-end multiprocessor market grown sufficiently large support full-scale, highly customized designs dominant choice. 2.The majority clusters commodity clusters, often put together users, rather system vendor designing standard product. 3.Although commodity clusters dominate representation, top 25 entries list much varied include 9 custom clusters (primarilyinstances Blue Gene Cray XT3 systems), 2 constellations, 8 commodityclusters, 2 SMPs (one NEC Earth Simulator, nodeswith vector processors), 4 DSM multiprocessors. 4.Vector processors, dominated list, almost disappeared. 5.The IBM Blue Gene dominates top 10 systems, showing advantage approach uses commodity processor cores, customizes many otherfunctions balances performance, power, packaging density. 6.Architectural convergence driven market effects (lack growth, limited suppliers, etc.) clear-cut consensus best archi-tectural approaches.Larger multiprocessors Shared address space Symmetric shared memory (SMP) Examples: IBM eServer, SUN SunfireDistributed shared memory (DSM)Commodity clusters: Beowulf othersCustom cluster Uniform cluster: IBM Blue GeneCache coherent: ccNUMA: SGI Origin/Altix Constellation cluster DSMs SMPs SGI Altix, ASC PurpleNoncache coherent: Cray T3E, X1Distributed address space Figure I.21 space large-scale multiprocessors relation different classes.I-46 ■Appendix Large-Scale Multiprocessors Scientific ApplicationsSoftware, applications programming languages environments, remains big challenge parallel computing, 30 years ago, multiprocessors Illiac IV designed. combination easeof programming high parallel performance remains elusive. better pro-gress made front, convergence toward single programming model andunderlying architectural approach (remembering uniprocessors essen-tially one programming model one architectural approach!) slowor driven factors proven architectural superiority.I.9 Concluding Remarks ■I-47J.1 Introduction J-2 J.2 Basic Techniques Integer Arithmetic J-2 J.3 Floating Point J-13 J.4 Floating-Point Multiplication J-17 J.5 Floating-Point Addition J-21 J.6 Division Remainder J-27 J.7 Floating-Point Arithmetic J-32 J.8 Speeding Integer Addition J-37 J.9 Speeding Integer Multiplication Division J-44 J.10 Putting Together J-57 J.11 Fallacies Pitfalls J-62 J.12 Historical Perspective References J-63 Exercises J-67J Computer Arithmetic David Goldberg Xerox Palo Alto Research Center Fast drives Slow even Fast wrong. W. KahanJ.1 Introduction Although computer arithmetic sometimes viewed specialized part CPU design, important part. brought home Intel 1994 whentheir Pentium chip discovered bug divide algorithm. Thisfloating-point flaw resulted flurry bad publicity Intel also cost thema lot money. Intel took $300 million write-off cover cost replacing buggy chips. appendix, study basic floating-point algorithms, includ- ing division algorithm used Pentium. Although tremendous varietyof algorithms proposed use floating-point accelerators, actualimplementations usually based refinements variations basicalgorithms presented here. addition choosing algorithms addition, sub-traction, multiplication, division, computer architect must make otherchoices. precisions implemented? exceptions behandled? appendix give background making decisions. discussion floating point focus almost exclusively IEEE floating-point standard (IEEE 754) rapidly increasing acceptance.Although floating-point arithmetic involves manipulating exponents shiftingfractions, bulk time floating-point operations spent operating onfractions using integer algorithms (but necessarily sharing hardware thatimplements integer instructions). Thus, discussion floating point,we take detailed look integer algorithms. good references computer arithmetic, order least detailed, Chapter 3 ofPatterson Hennessy [2009] ;Chapter 7 ofHamacher, Vranesic, Zaky [1984]; Gosling [1980] ;a n Scott [1985] . J.2 Basic Techniques Integer Arithmetic Readers studied computer arithmetic find section review. Ripple-Carry Addition Adders usually implemented combining multiple copies simple com-ponents. natural components addition half adders andfull adders . half adder takes two bits aandbas input produces sum bit sand carry bit c outas output. Mathematically, s¼(a+b)m d2 ,a n cout¼b(a+b)/2c, bcis floor function. logic equations, s¼ab+aband cout¼ab, abmeans a^band a+bmeans a_b. half adder also called (2,2) adder, since takes two inputs produces two outputs. full adderJ-2 ■Appendix J Computer Arithmeticis (3,2) adder defined s¼(a+b+c)m d2 , cout¼b(a+b+c)/2c,o r logic equations s¼abc+abc+abc+abc J:2:1 cout¼ab+ac+bc J:2:2 principal problem constructing adder n-bit numbers smaller pieces propagating carries one piece next. obvious way solve ripple-carry adder , consisting nfull adders, illustrated Figure J.1 . (In figures appendix, least-significant bit always right.) inputs adder an/C01an/C02⋯a0 bn/C01bn/C02⋯b0, an/C01an/C02⋯a0 represents number an/C012n/C01+an/C022n/C02+⋯+a0.T h e ci+1output ith adder fed ci+1 input next adder (the ( i+1)-th adder) lower-order carry-in c0 set 0. Since low-order carry-in wired 0, low-order adder could half adder. Later, however, see setting low-order carry-in bit 1 useful performing subtraction. general, time circuit takes produce output proportional maximum number logic levels signal travels. However, deter-mining exact relationship logic levels timings highly technologydependent. Therefore, comparing adders simply compare numberof logic levels one. many levels ripple-carry adder? Ittakes two levels compute c 1from a0andb0. takes two levels com- pute c2from c1,a1,b1, on, cn. So, total 2 nlevels. Typical values nare 32 integer arithmetic 53 double-precision floating point. ripple-carry adder slowest adder, also cheapest. built withonly nsimple cells, connected simple, regular way. ripple-carry adder relatively slow compared designs discussed Section J.8, might wonder used all. technologieslike CMOS, even though ripple adders take time O( n), constant factor small. cases short ripple adders often used building blocks largeradders. bn–1an–1 sn–1Full adder cn–1sn–2cnan–2bn–2 Full adderb1a1 s1Full adder s0a0b0 Full adder c2 c10 Figure J.1 Ripple-carry adder, consisting nfull adders. carry-out one full adder connected carry-in adder next most-significant bit. Thecarries ripple least-significant bit (on right) most-significant bit (on left).J.2 Basic Techniques Integer Arithmetic ■J-3Radix-2 Multiplication Division simplest multiplier computes product two unsigned numbers, one bit time, illustrated Figure J.2(a) . numbers multiplied an/C01an/C02⋯a0 andbn/C01bn/C02⋯b0, placed registers B, respectively. Register P initially 0. multiply step two parts: Multiply Step (i)If least-significant bit 1, register B, containing bn/C01bn/C02⋯b0,i added P; otherwise, 00 ⋯00 added P. sum placed back P. (ii)Registers P shifted right, carry-out sum moved high-order bit P, low-order bit P moved register A,and rightmost bit A, used rest algorithm, beingshifted out. Carry-out P n nnShift P B 0A n + 1 n 1nShift(a) (b)1 B Figure J.2 Block diagram (a) multiplier (b) divider n-bit unsigned integers. multiplication step consists adding contents P either B 0 (dependingon low-order bit A), replacing P sum, shifting P one bit right. division step involves first shifting P one bit left, subtracting B P, and, difference nonnegative, putting P. difference nonnegative,the low-order bit set 1.J-4 ■Appendix J Computer ArithmeticAfter nsteps, product appears registers P A, holding lower-order bits. simplest divider also operates unsigned numbers produces quotient bits one time. hardware divider shown Figure J.2(b) .T compute a/b, put register, bin B register, 0 P register perform ndivide steps. divide step consists four parts: Divide Step (i)Shift register pair (P,A) one bit left. (ii)Subtract content register B (which bn/C01bn/C02⋯b0) register P, putting result back P. (iii)If result step 2 negative, set low-order bit 0, otherwise 1. (iv)If result step 2 negative, restore old value P adding contents register B back P. repeating process ntimes, register contain quotient, P register contain remainder. algorithm binary version thepaper-and-pencil method; numerical example illustrated Figure J.3(a) . Notice two block diagrams Figure J.2 similar. main difference register pair (P,A) shifts right multiplying left whendividing. allowing registers shift bidirectionally, hardwarecan shared multiplication division. division algorithm illustrated Figure J.3(a) called restoring, subtraction byields negative result, P register restored adding b back in. restoring algorithm variant skips restoring step andinstead works resulting negative numbers. step nonrestoring algorithm three parts: Nonrestoring P negative, Divide Step (i-a)Shift register pair (P,A) one bit left. (ii-a)Add contents register B P. Else, (i-b)Shift register pair (P,A) one bit left. (ii-b)Subtract contents register B P. (iii)If P negative, set low-order bit 0, otherwise set 1. repeating ntimes, quotient A. P nonnegative, remainder. Otherwise, needs restored (i.e., add b), remainder. numerical example given Figure J.3(b) . Since steps ( i-a) ( i- b) same, might tempted perform common step first, test sign P. ’t work, since sign bit lost shifting.J.2 Basic Techniques Integer Arithmetic ■J-5PA 00000 1110 Divide 14 ¼1110 2by 3¼112. B always contains 0011 2. 00001 110 step 1(i): shift. /C000011 step 1(ii): subtract. /C000010 110 0step 1(iii): result negative, set quotient bit 0. 00001 110 0step 1(iv): restore. 00011 10 0 step 2(i): shift. /C000011 step 2(ii): subtract. 00000 10 01 step 2(iii): result nonnegative, set quotient bit 1. 00001 0 01 step 3(i): shift. /C000011 step 3(ii): subtract. /C000010 0 010 step 3(iii): result negative, set quotient bit 0. 00001 0 010 step 3(iv): restore. 00010 010 step 4(i): shift. /C000011 step 4(ii): subtract. /C000001 0100 step 4(iii): result negative, set quotient bit 0. 00010 0100 step 4(iv): restore. quotient 0100 2and remainder 00010 2. (a) 00000 1110 Divide 14 ¼1110 2by 3¼112. B always contains 0011 2. 00001 110 step 1(i-b): shift. +11101 step 1(ii-b): subtract b (add two ’s complement). 11110 110 0step 1(iii): P negative, set quotient bit 0. 11101 10 0 step 2(i-a): shift. +00011 step 2(ii-a): add b. 00000 10 01 step 2(iii): P nonnegative, set quotient bit 1. 00001 0 01 step 3(i-b): shift. +11101 step 3(ii-b): subtract b. 11110 0 010 step 3(iii): P negative, set quotient bit 0. 11100 010 step 4(i-a): shift. +00011 step 4(ii-a): add b. 11111 0100 step 4(iii): P negative, set quotient bit 0. +00011 Remainder negative, final restore step. 00010 quotient 0100 2and remainder 00010 2. (b) Figure J.3 Numerical example (a) restoring division (b) nonrestoring division.J-6 ■Appendix J Computer ArithmeticThe explanation nonrestoring algorithm works this. Let rkbe contents (P,A) register pair step k, ignoring quotient bits (which sim- ply sharing unused bits register A). Figure J.3(a) , initially contains 14, r0¼14. end first step, r1¼28, on. restoring algorithm, part (i) computes 2 rkand part (ii) 2 rk/C02nb(2nbsince bis subtracted left half). 2 rk/C02nb/C210, algorithms end step identical values (P,A). 2rk/C02nb<0, restoring algorithm restores 2 rk, next step begins computing rres¼2(2rk)/C02nb. non-restoring algorithm, 2 rk/C02nb kept negative number, next step rnonres¼2(2rk/C02nb)+ 2nb¼4rk/C02nb¼rres. Thus (P,A) bits algorithms. Ifaandbare unsigned n-bit numbers, hence range 0 /C20a,b/C202n/C01, multiplier Figure J.2 work register P nbits long. However, division, P must extended n+1 bits order detect sign P. Thus adder must also n+1 bits. would anyone implement restoring division, uses hard- ware nonrestoring division (the control slightly different) involves extraaddition? fact, usual implementation restoring division ’t actually perform add step ( iv). Rather, sign resulting subtraction tested output adder, sum nonnegative loaded back intothe P register. final point, beginning divide, hardware must check see whether divisor 0. Signed Numbers four methods commonly used represent signed n-bit numbers: sign magnitude ,two ’s complement ,one ’s complement , biased . sign magni- tude system, high-order bit sign bit, low-order n/C01 bits magnitude number. two ’s complement system, number negative add 2n. one ’s complement, negative number obtained complementing bit (or, alternatively, number negative add to2 n/C01). three systems, nonnegative numbers represented usual way. biased system, nonnegative numbers usual rep- resentation. Instead, numbers represented first adding biasand encoding sum ordinary unsigned number. Thus, negative num-berkcan encoded long k+bias /C210. typical value bias 2 n/C01. Example Using 4-bit numbers ( n¼4), k¼3 (or binary, k¼0011 2), /C0kexpressed formats? Answer signed magnitude, leftmost bit k¼0011 2is sign bit, flip 1: /C0kis represented 1011 2. two ’s complement, k+1101 2¼2n¼16. /C0kis repre- sented 1101 2. one ’s complement, bits k¼0011 2are flipped, /C0k represented 1100 2. biased system, assuming bias 2n/C01¼8,kis represented k+bias ¼1011 2, and/C0kby/C0k+bias ¼0101 2.J.2 Basic Techniques Integer Arithmetic ■J-7The widely used system representing integers, two ’s complement, system use here. One reason popularity two ’s complement makes signed addition easy: Simply discard carry-out highorder bit.To add 5+ /C02, example, add 0101 2and 1110 2to obtain 0011 2, resulting correct value 3. useful formula value two ’s complement number an/C01an/C02⋯a1a0is /C0an/C012n/C01+an/C022n/C02+⋯+a121+a0 J:2:3 illustration formula, value 1101 2as 4-bit two ’s complement number /C01/C123+1/C122+0/C121+1/C120¼/C08+4+1 ¼/C03, confirming result example above. Overflow occurs result operation fit represen- tation used. example, unsigned numbers represented using 4 bits, 6 ¼0110 2and 11 ¼1011 2. sum (17) overflows binary equivalent (10001 2) ’t fit 4 bits. unsigned numbers, detecting over- flow easy; occurs exactly carry-out most-significant bit.For two ’s complement, things trickier: Overflow occurs exactly carry high-order bit different (to discarded) carry-out high-order bit. example 5+ /C02 above, 1 carried leftmost bit, avoiding overflow. Negating two ’s complement number involves complementing bit adding 1. instance, negate 0011 2, complement get 1100 2and add 1 get 1101 2. Thus, implement a/C0busing adder, simply feed aandb (where bis number obtained complementing bit b) adder set low-order, carry-in bit 1. explains rightmost adder inFigure J.1 full adder. Multiplying two ’s complement numbers quite simple adding them. obvious approach convert operands nonnegative, anunsigned multiplication, (if original operands opposite signs) negate result. Although conceptually simple, requires extra time hardware. better approach: Suppose multiplying atimes b using hardware shown Figure J.2(a) . Register loaded number a;B loaded b.Since content register B always b, use B binterchangeably. B potentially negative nonnegative, change needed convert unsigned multiplication algorithm two ’s com- plement one ensure P shifted, shifted arithmetically; is,the bit shifted high-order bit P sign bit P (rather carry-out addition). Note n-bit-wide adder adding n-bit two ’s complement numbers /C02 n/C01and 2n/C01/C01. Next, suppose ais negative. method handling case called Booth recoding . Booth recoding basic technique computer arithmetic play key role Section J.9 . algorithm page J-4 computes a/C2bby examining bits afrom least significant significant. example, a¼7¼0111 2,t h e ns e p( i) successively add B, add B, add B, add 0. Booth recoding “recodes ”the number 7 8 /C01¼1000 2/C00001 2¼1001, 1J-8 ■Appendix J Computer Arithmeticrepresents /C01. gives alternative way compute a/C2b, namely, successively subtract B, add 0, add 0, add B. complicated unsigned algo- rithm page J-4, since uses addition subtraction. advantage showsup negative values a. proper recoding, treat aas though unsigned. example, take a¼/C04¼1100 2. Think 1100 2as unsigned num- ber 12, recode 12 ¼16/C04¼10000 2/C00100 2¼10100. multiplication algorithm iterated ntimes ( n¼4 case), high-order digit ignored, end subtracting 0100 2¼4 times multiplier —exactly right answer. suggests multiplying using recoded form awill work equally well positive negative numbers. And, indeed, deal negative values a, required sometimes subtract bfrom P, instead adding either bor 0 P. precise rules: initial content an/C01⋯a0,t h e na tt h e ith multiply step low-order bit register ai, step ( i) multiplication algorithm becomes: I.Ifai¼0 ai/C01¼0, add 0 P. II.Ifai¼0 ai/C01¼1, add B P. III.Ifai¼1 ai/C01¼0, subtract B P. IV.Ifai¼1 ai/C01¼1, add 0 P. first step, i¼0, take ai/C01to 0. Example multiplying /C06 times /C05, sequence values (P,A) register pair? Answer SeeFigure J.4 . PA 0000 1010 Put /C06¼1010 2into A, /C05¼1011 2into B. 0000 1010 step 1(i): a0¼a/C01¼0, rule add 0. 0000 0101 step 1(ii): shift. +0101 step 2(i): a1¼1,a0¼0.Rule III says subtract b(or add/C0b¼/C01011 2¼0101 2). 0101 0101 0010 1010 step 2(ii): shift. + 1011 step 3(i): a2¼0,a1¼1. Rule II says add b(1011). 1101 10101110 1101 step 3(ii): shift. (Arithmetic shift —load 1 leftmost bit.) + 0101 step 4(i): 3¼1,a2¼0. Rule III says subtract b. 0011 11010001 1110 step 4(ii): shift. Final result 00011110 2¼30. Figure J.4 Numerical example Booth recoding. Multiplication a¼/C06b b¼/C05 get 30.J.2 Basic Techniques Integer Arithmetic ■J-9The four prior cases restated saying ith step add (ai/C01/C0ai)B P. observation, easy verify rules work, result additions Xn/C01 i¼0ba i/C01/C0ai ðÞ 2i¼b/C0an/C012n/C01+an/C022n/C02+…+a12+a0/C0/C1 +ba/C01 Using Equation J.2.3 (page J-8) together a/C01¼0, right-hand side seen value b/C2aas two ’s complement number. simplest way implement rules Booth recoding extend register one bit right new bit contain ai/C01. Unlike naive method inverting negative operands, technique ’t require extra steps special casing negative operands. slightly controllogic. multiplier shared divider, already capa-bility subtracting b, rather adding it. summarize, simple method handling two ’s complement multiplication pay attention sign P shifting right, save recently shifted-out bit use deciding whether add subtract bfrom P. Booth recoding usually best method designing multiplication hardware operates signed numbers. hardware ’t directly implement it, however, performing Booth recoding software microcode usually slowbecause conditional tests branches. hardware supports arithmeticshifts (so negative bis handled correctly), following method used. Treat multiplier aas unsigned number, perform first n/C01 multiply steps using algorithm page J-4. a<0 (in case 1 low-order bit register point), subtract bfrom P; otherwise ( a/C210), neither add subtract. either case, final shift (for total ofnshifts). works amounts multiplying bby /C0a n/C012n/C01+⋯+a12+a0, value an/C01⋯a0as two ’sc p l e e n number Equation J.2.3 . hardware ’t support arithmetic shift, converting operands nonnegative probably best approach. Two final remarks: good way test signed-multiply routine try/C02n/C01/C2/C02n/C01, since case produces 2 n/C01 bit result. Unlike multiplication, division usually performed hardware convertingthe operands nonnegative unsigned divide. divi-sion substantially slower (and less frequent) multiplication, extra timeused manipulate signs less impact multiplication. Systems Issues designing instruction set, number issues related integer arithmeticneed resolved. Several discussed here. First, done integer overflow? situation compli- cated fact detecting overflow differs depending whether operandsare signed unsigned integers. Consider signed arithmetic first. threeJ-10 ■Appendix J Computer Arithmeticapproaches: Set bit overflow, trap overflow, nothing overflow. last case, software check whether overflow occurred. convenient solution programmer enable bit. bit turnedon, overflow causes trap. turned off, overflow sets bit (or, alter-natively, two different add instructions). advantage approach isthat trapping nontrapping operations require one instruction. Fur-thermore, see Section J.7 , analogous IEEE floating-point standard handles floating-point overflow. Figure J.5 shows common machines treat overflow. unsigned addition? Notice none architectures Figure J.5 traps unsigned overflow. reason primary use unsigned arithmetic manipulating addresses. convenient ableto subtract unsigned address adding. example, n¼4, subtract 2 unsigned address 10 ¼1010 2by adding 14 ¼1110 2. generates overflow, would want trap generated. second issue concerns multiplication. result multiplying two n-bit numbers 2 n-bit result, multiplication return low-order n bits, signaling overflow result ’t fit nbits? argument favor n-bit result virtually high-level languages, multiplication oper- ation arguments integer variables result integer variableof type. Therefore, compilers ’t generate code utilizes double- precision result. argument favor 2 n-bit result used assembly language routine substantially speed multiplication multiple-precision integers (by factor 3). third issue concerns machines want execute one instruction every cycle. rarely practical perform multiplication division amount time addition register-register move takes. three possible approaches tothis problem. first single-cycle multiply-step instruction. might one step Booth algorithm. second approach integer multipli-cation floating-point unit part floating-point instruction set. Machine Trap signed overflow?Trap unsigned overflow?Set bit signed overflow?Set bit unsigned overflow? VAX enable Yes. Add sets V bit.Yes.Add sets C bit. IBM 370 enable Yes. Add sets cond code.Yes. Logical add sets cond code. Intel 8086No Yes. Add sets V bit.Yes.Add sets C bit. MIPS R3000Twoadd instructions; one always traps, never does.No No. Software must deduce sign operands result. SPARC Addcc sets V bit. Add not.Addcc sets C bit. Add not. Figure J.5 Summary various machines handle integer overflow. 8086 SPARC instruc- tion traps V bit set, cost trapping overflow one extra instruction.J.2 Basic Techniques Integer Arithmetic ■J-11(This DLX does.) third approach autonomous unit CPU multiplication. case, result either guaranteed deliv- ered fixed number cycles —and compiler charged waiting proper amount time —or interlock. comments apply division well. examples, original SPARC multiply-step instruction nodivide-step instruction, MIPS R3000 autonomous unit mul-tiplication division (newer versions SPARC architecture added integermultiply instruction). designers HP Precision Architecture espe-cially thorough job analyzing frequency operands multiplication division, based multiply divide steps accordingly. (See Magenheimer et al. [1988] details.) final issue involves computation integer division remainder negative numbers. example, /C05 DIV3 /C05MOD3? computing xDIVyandxMODy, negative values xoccur frequently enough worth careful consideration. (On hand, negative values yare quite rare.) built-in hardware instructions operations, corre-spond high-level languages specify. Unfortunately, agreement among existing programming languages. See Figure J.6 . One definition expressions stands clearly superior, namely, x DIVy¼bx/yc, 5 DIV3¼1 /C05DIV3¼/C02. MOD satisfy x¼(xDIVy)/C2y+xMOD y, xMOD y/C210. Thus, 5 MOD 3¼2, /C05MOD 3¼1. many advantages definition follows: 1.A calculation compute index hash table size Ncan use MODNand guaranteed produce valid index range 0 N/C01. 2.In graphics, converting one coordinate system another, “glitch ”near 0. example, convert value xexpressed system uses 100 dots per inch value yon bitmapped display 70 dots per inch, formula y¼(70/C2x)DIV 100 maps one two xcoordinates ycoordinate. DIV defined Pascal x/yrounded 0, 0 would three different points ( /C01, 0, 1) mapped it. 3.xMOD2kis performing bitwise ANDwith mask kbits, xDIV 2kis k-bit arithmetic right shift. Language Division Remainder FORTRAN /C05/3¼/C01 MOD(/C05, 3)¼/C02 Pascal /C05DIV3¼/C01 /C05MOD 3¼1 Ada /C05/3¼/C01 /C05MOD 3¼1 /C05REM3¼/C02 C /C05/3 undefined /C05% 3 undefined Modula-3 /C05DIV3¼/C02 /C05MOD 3¼1 Figure J.6 Examples integer division integer remainder various program- ming languages.J-12 ■Appendix J Computer ArithmeticFinally, potential pitfall worth mentioning concerns multiple-precision addi- tion. Many instruction sets offer variant add instruction adds three operands: two n-bit numbers together third single-bit number. third number carry previous addition. Since multiple-precision num-ber typically stored array, important able increment thearray pointer without destroying carry bit. J.3 Floating Point Many applications require numbers ’t integers. number ways nonintegers represented. One use fixed point ; is, use inte- ger arithmetic simply imagine binary point somewhere tothe right least-significant digit. Adding two numbers done integer add, whereas multiplication requires extra shifting. repre- sentations proposed involve storing logarithm number anddoing multiplication adding logarithms, using pair integers ( a,b)t represent fraction a/b.However, one noninteger representation gained widespread use, floating point. system, computer word divided two parts, exponent significand. example, exponentof/C03 significand 1.5 might represent number 1.5 /C22 /C03¼0.1875. advantages standardizing particular representation obvious. Numerical analysts build high-quality software libraries, computer designers develop techniques implementing high-performance hardware, hardwarevendors build standard accelerators. Given predominance thefloating-point representation, appears unlikely representation willcome widespread use. semantics floating-point instructions clear-cut seman- tics rest instruction set, past behavior floating-pointoperations varied considerably one computer family next. varia- tions involved things number bits allocated exponent significand, range exponents, rounding carried out, actionstaken exceptional conditions like underflow overflow. Computer architec-ture books used dispense advice deal details, butfortunately longer necessary. ’s computer industry rap- idly converging format specified IEEE standard 754-1985 (also inter-national standard, IEC 559). advantages using standard variant offloating point similar using floating point noninteger representations. IEEE arithmetic differs many previous arithmetics following major ways: 1.When rounding “halfway ”result nearest floating-point number, picks one even. 2.It includes special values NaN,∞, and/C0∞.J.3 Floating Point ■J-133.It uses denormal numbers represent result computations whose value less 1 :0/C22Emin. 4.It rounds nearest default, also three rounding modes. 5.It sophisticated facilities handling exceptions. elaborate (1), note operating two floating-point numbers, result usually number cannot exactly represented another floating-point number. example, floating-point system using base 10and two significant digits, 6.1 /C20.5¼3.05. needs rounded two digits. rounded 3.0 3.1? IEEE standard, halfway cases arerounded number whose low-order digit even. is, 3.05 rounds 3.0,not 3.1. standard actually four rounding modes . default round nearest , rounds ties even number explained. modes round toward 0, round toward+ ∞, round toward /C0∞. elaborate differences following sections. reading, see IEEE [1985], Cody et al. [1984] , Goldberg [1991] . Special Values Denormals Probably notable feature standard default computation continues face exceptional conditions, dividing 0 taking square root negative number. example, result taking square rootof negative number NaN (NotaNumber), bit pattern represent ordinary number. example NaNs might useful, consider thecode zero finder takes function Fas argument evaluates Fat various points determine zero it. zero finder accidentally probes out-side valid values F, Fmay well cause exception. Writing zero finder deals case highly language operating-system dependent, relies operating system reacts exceptions reaction mapped back programming language. IEEE arithmetic itis easy write zero finder handles situation runs many differentsystems. evaluation F, simply checks see whether Fhas returned NaN; so, knows probed outside domain F. IEEE arithmetic, input operation NaN, output NaN (e.g., 3+NaN ¼NaN). rule, writing floating-point subroutines accept NaN argument rarely requires special case checks. exam- ple, suppose arccos computed terms arctan, using formula arccos x¼2arctanﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ 1/C0xðÞ =1+xðÞp/C0/C1 . arctan handles argument NaN properly, arccos automatically so, too. ’s xis NaN, 1+x,1/C0x,( 1 + x)/(1/C0x), andﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ 1/C0xðÞ =1+xðÞp also NaNs. checking NaNs required. result ofﬃﬃﬃﬃﬃﬃ ﬃ /C01p NaN, result 1/0 NaN, + ∞, another special value. standard defines arithmetic infinities (there areJ-14 ■Appendix J Computer Arithmeticboth +∞ and/C0∞) using rules 1/ ∞¼0. formula arccos x¼2arctanﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ 1/C0xðÞ =1+xðÞp/C0/C1 illustrates infinity arithmetic used. Since arctan xasymptotically approaches π/2 xapproaches ∞, natural define arctan( ∞)¼π/2, case arccos( /C01) automatically com- puted correctly 2 arctan( ∞)¼π. final kind special values standard denormal numbers. many floating-point systems, Eminis smallest exponent, number less 1 :0/C2 2Emincannot represented, floating-point operation results number less simply flushed 0. IEEE standard, hand, num- bers less 1 :0/C22Eminare represented using significands less 1. called gradual underflow . Thus, numbers decrease magnitude 2Emin, gradually lose significance represented 0 theirsignificance shifted out. example, base 10 four significantfigures, let x¼1:234/C210 Emin. Then, x/10 rounded 0 :123/C210Emin, lost digit precision. Similarly x/100 rounds 0 :012/C210Emin, x/1000 0:001/C210Emin, x/10000 finally small enough rounded 0. Denor- mals make dealing small numbers predictable maintaining familiar properties x¼y,x/C0y¼0. example, flush-to-zero system (again base 10 four significant digits), x¼1:256/C210Eminandy¼1:234/C210Emin, x/C0y¼0:022/C210Emin, flushes zero. even though x6¼y, computed value x/C0y¼0. never happens gradual underflow. example, x/C0y¼0:022/C210Eminis denormal number, computation x/C0yis exact. Representation Floating-Point Numbers Let us consider represent single-precision numbers IEEE arithmetic. Single-precision numbers stored 32 bits: 1 sign, 8 exponent,and 23 fraction. exponent signed number represented using biasmethod (see subsection “Signed Numbers, ”page J-7) bias 127. term biased exponent refers unsigned number contained bits 1 8, andunbiased exponent (or exponent) means actual power 2 raised. fraction represents number less 1, significand floating-point number 1 plus fraction part. words, eis biased exponent (value exponent field) fis value fraction field, number represented 1. f/C22 e/C0127. Example single-precision number following 32-bit word represent? 1 10000001 01000000000000000000000 Answer Considered unsigned number, exponent field 129, making value exponent 129 /C0127¼2. fraction part .01 2¼.25, making significand 1.25. Thus, bit pattern represents number /C01.25/C222¼/C05.J.3 Floating Point ■J-15The fractional part floating-point number (.25 example above) must confused significand, 1 plus fractional part. lead-ing 1 significand 1. fdoes appear representation; is, leading bit implicit. performing arithmetic IEEE format numbers, fraction part usually unpacked , say implicit 1 made explicit. Figure J.7 summarizes parameters single (and other) precisions. shows exponents single precision range /C0126 127; accord- ingly, biased exponents range 1 254. biased exponents 0 and255 used represent special values. summarized Figure J.8 . biased exponent 255, zero fraction field represents infinity, nonzerofraction field represents NaN. Thus, entire family NaNs. thebiased exponent fraction field 0, number represented 0. implicit leading 1, ordinary numbers always significand greater equal 1. Thus, special convention required torepresent 0. Denormalized numbers implemented word zeroexponent field represent number 0 :f/C22 Emin. primary reason IEEE standard, like floating-point for- mats, uses biased exponents means nonnegative numbers ordered inthe way integers. is, magnitude floating-point numbers becompared using integer comparator. Another (related) advantage 0 repre- sented word 0s. downside biased exponents adding slightly awkward, requires bias subtracted sum. Single Single extended Double Double extended p(bits precision) 24 /C2132 53 /C2164 Emax 127 /C211023 1023 /C2116383 Emin /C0126 /C20/C01022 /C01022 /C20/C016382 Exponent bias 127 1023 Figure J.7 Format parameters IEEE 754 floating-point standard. first row gives number bits significand. blanks unspecified parameters. Exponent Fraction Represents e¼Emin/C01 f¼0 /C60 e¼Emin/C01 f6¼0 0:f/C22Emin Emin/C20e/C20Emax — 1.f/C22e e¼Emax+1 f¼0 /C6∞ e¼Emax+1 f6¼0 NaN Figure J.8 Representation special values. exponent number falls out- side range Emin/C20e/C20Emax, number special interpretation indicated table.J-16 ■Appendix J Computer ArithmeticJ.4 Floating-Point Multiplication simplest floating-point operation multiplication, discuss first. binary floating-point number xis represented significand exponent, x¼s/C22e. formula s1/C22e1/C0/C1 /C15s2/C22e2/C0/C1 ¼s1/C15s2 ðÞ /C2 2e1+e2 shows floating-point multiply algorithm several parts. first part mul- tiplies significands using ordinary integer multiplication. floating- point numbers stored sign magnitude form, multiplier need dealwith unsigned numbers (although seen Booth recoding handlessigned two ’s complement numbers painlessly). second part rounds result. significands unsigned p-bit numbers (e.g., p¼24 single precision), product many 2 pbits must rounded p-bit num- ber. third part computes new exponent. exponents stored witha bias, involves subtracting bias sum biased exponents. Example multiplication single-precision numbers 1 1000001 0000 …¼/C0 1/C223 0 1000001 1000 …¼1/C224 proceed binary? Answer unpacked, significands 1.0, product 1.0, result form: 1???????? 000… compute exponent, use formula: biased exp e1+e2 ðÞ ¼ biased exp e1ðÞ+ biased exp e2ðÞ /C0 bias Figure J.7 , bias 127 ¼01111111 2,s oi nt w ’s complement /C0127 10000001 2. Thus, biased exponent product 10000010 10000011 + 10000001 10000110 Since 134 decimal, represents exponent 134 /C0bias¼134/C0127, expected. interesting part floating-point multiplication rounding. different cases occur illustrated Figure J.9 . Since cases similar bases, figure uses human-friendly base 10, rather base 2.J.4 Floating-Point Multiplication ■J-17In figure, p¼3, final result must rounded three significant digits. three most-significant digits boldface. fourth most-significant digit (marked arrow) round digit, denoted r. round digit less 5, bold digits represent rounded result. round digit greater 5 (as part (a)), 1 must added least-significant bold digit. round digit exactly 5 (as part (b)), additional digits must examined decide truncation incrementing 1. necessary know digits past 5 nonzero. algorithm below, berecorded sticky bit . Comparing parts (a) (b) figure shows two possible positions round digit (relative least-significant digit theproduct). Case (c) illustrates that, adding 1 least-significant bold digit,there may carry-out. happens, final significand must 10.0. straightforward method handling rounding using multiplier Figure J.2 (page J-4) together extra sticky bit. pis number bits significand, A, B, P registers pbits wide. Multiply two significands obtain 2 p-bit product (P,A) registers (see Figure J.10 ). multiplication, first p/C02 times bit shifted register, sticky bit. used halfway cases. Let srepresent sticky bit,g(for guard) most-significant bit A, r(for round) second most- significant bit A. two cases: 1.The high-order bit P 0. Shift P left 1 bit, shifting gbit A. Shift- ing rest necessary. 2.The high-order bit P 1. Set s:¼s_randr:¼g, add 1 exponent. r¼0, P correctly rounded product. r¼1 s¼1, P+1 product (where P+1 mean adding 1 least-significant bit P).(a) 1.23 /C26.78 r¼9>5 round rounds 8.348.3394 " (b) 2.83 /C24.47 r¼5 following digit 6¼0 round rounds 1.27 /C2101 12.6501 " (c) 1.28 /C27.81 r¼6>5 round rounds 1.00 /C2101 09.9968 " Figure J.9 Examples rounding multiplication. Using base 10 p¼3, parts (a) (b) illustrate result multiplication either 2 p/C01o r2 pdigits; hence, position 1 added rounding (just left arrow) vary. Part (c) shows rounding cause carry-out.J-18 ■Appendix J Computer ArithmeticIfr¼1 s¼0, halfway case round according least- significant bit P. example, apply decimal version rules toFigure J.9(b) . multiplication, P ¼126 ¼501, g¼5,r¼0 s¼1. Since high-order digit P nonzero, case (2) applies r:¼g, r¼5, arrow indicates Figure J.9 . Since r¼5, could halfway case, s¼1 indicates result fact slightly 1/2, add 1 P obtain correctly rounded product. precise rules rounding depend rounding mode given Figure J.11 . Note P nonnegative, is, contains magnitude result. good discussion efficient ways implement rounding inSantoro, Bewick, Horowitz [1989] . Example binary p¼4, show multiplication algorithm computes product /C05/C210 four rounding modes. Answer binary, /C05i s/C01.010 2/C222and 10 ¼1.010 2/C223. Applying integer multipli- cation algorithm significands gives 01100100 2,s oP ¼0110 2,A¼0100 2, g¼0,r¼1, s¼0. high-order bit P 0, case (1) applies. Thus, P becomes 1100 2, since result negative, Figure J.11 gives: round /C0∞ 1101 2 add 1 since r_s¼1/0¼TRUE round to+ ∞ 1100 2 round 0 1100 2 round nearest 1100 2 add since r^p0¼1^0¼FALSE r^s¼1^0¼FALSE exponent 2+3 ¼5, result /C01.100 2/C225¼/C048, except round- ing /C0∞, case /C01.101 2/C225¼/C052.Product Case (1): x0 = 0 Shift needed Case (2): x0 = 1 Increment exponent Adjust binary point, add 1 exponent compensaternd sticky rnd sticky x2 x3 x4 x5 x0 . x1x1 . x 2 x3 x4 x5gx0 x1 . x2 x3 x4 x5gr sssPA Figure J.10 two cases floating-point multiply algorithm. top line shows contents P registers multiplying significands, p¼6. case (1), leading bit 0, P register must shifted. case (2), leading bit 1, shift required, exponent round sticky bits must adjusted. sticky bit logical bits marked s.J.4 Floating-Point Multiplication ■J-19Overflow occurs rounded result large represented. sin- gle precision, occurs result exponent 128 higher. e1and e2are two biased exponents, 1 /C20ei/C20254, exponent calculation e1+e2/C0127 gives numbers 1+1 /C0127 254+254 /C0127, /C0125 381. range numbers represented using 9 bits. one way detect overflow perform exponent calculations 9-bit adder (seeExercise J.12). Remember must check overflow rounding — example Figure J.9(c) shows make difference. Denormals Checking underflow somewhat complex denormals. sin- gle precision, result exponent less /C0126, necessarily indicate underflow, result might denormal number. example,the product (1 /C22 /C064) (1 /C22/C065)i s1/C22/C0129, and/C0129 legal exponent limit. result valid denormal number, namely, 0.125 /C22/C0126. general, unbiased exponent product dips /C0126, result- ing product must shifted right exponent incremented exponent reaches /C0126. process causes entire significand shifted out, underflow occurred. precise definition underflow somewhat subtle — seeSection J.7 details. one operands multiplication denormal, significand leading zeros, product significands also leadingzeros. exponent product less /C0126, result denormal, right-shift increment exponent before. exponent greater /C0126, result may normalized number. case, left-shift product (while decrementing exponent) either becomes normalized theexponent drops /C0126. Denormal numbers present major stumbling block implementing floating-point multiplication, becau se require performing variable shift multiplier, ’t otherwise needed. Thus, high- performance, floating-point multipliers often handle denormalizedRounding mode Sign result ≥0 Sign result <0 /C0∞ +1 r_s +∞ +1 r_s 0Nearest +1 r^p 0orr^s +1 r^p0orr^s Figure J.11 Rules implementing IEEE rounding modes. LetSbe magnitude preliminary result. Blanks mean pmost-significant bits Sare actual result bits. condition listed true, add 1 pth most-significant bit S. symbols rand srepresent round sticky bits, p0is pth most-significant bit S.J-20 ■Appendix J Computer Arithmeticnumbers, instead trap, letting softw handle them. practical codes frequently underflow, even working properly, programs run quite bit slower systems require denormals processed bya trap handler. far ’t mentioned deal operands zero. handled either testing operands beginning multiplication test-ing product afterward. test afterward, sure handle case 0 /C2∞ properly: results NaN, 0. detect result 0, set thebiased exponent 0. ’t forget sign. sign product XOR signs operands, even result 0. Precision Multiplication discussion integer multiplica tion, mentioned designers must decide whether deliver low-order word product entire prod-uct. similar issue arises floating-point multiplication, exactproduct rounded precision operands next higher precision. case integer multiplication, none standard high-level languages contains construct would generate “single times single gets double ”instruction. situation diff erent floating point. Many lan- guages allow assigning product f two single-precision variables double-precision one, constructi also exploited numerical algorithms. best-known case using iterative refinement solve linearsystems equations. J.5 Floating-Point Addition Typically, floating-point operation takes two inputs pbits precision returns p-bit result. ideal algorithm would compute first performing operation exactly, rounding result pbits (using current rounding mode). multiplication algorithm presented previous sectionfollows strategy. Even though hardware implementing IEEE arithmetic must return result ideal algorithm, ’t need actually perform ideal algorithm. addition, fact, better ways proceed. see this,consider examples. First, sum binary 6-bit numbers 1.10011 2and 1.10001 2/C22/C05: summands shifted exponent, 1:10011 +:0000110001 Using 6-bit adder (and discarding low-order bits second addend) gives 1:10011 +:00001 +1:10100J.5 Floating-Point Addition ■J-21The first discarded bit 1. ’t enough decide whether round up. rest discarded bits, 0001, need examined. Or, actually, need torecord whether bits nonzero, storing fact sticky bit asin multiplication algorithm. So, adding two p-bit numbers, p-bit adder sufficient, long first discarded bit (round) rest thebits (sticky) kept. Figure J.11 used determine roundup necessary, multiplication. example above, sticky 1, roundup necessary. final sum 1.10101 2. ’s another example: 1:11011 + :0101001 6-bit adder gives: 1:11011 + :01010 +1 0:00101 carry-out left, round bit ’t first discarded bit; rather, low-order bit sum (1). discarded bits, 01, OR’ed together make sticky. round sticky 1, high-order 6 bits sum, 10.0010 2, must rounded final answer 10.0011 2. Next, consider subtraction following example: 1:00000 /C0:00000101111 simplest way computing convert /C0.00000101111 2to two ’s complement form, difference becomes sum: 1:00000 +1:11111010001 Computing sum 6-bit adder gives: 1:00000 +1:11111 0:11111 top bits canceled, first discarded bit (the guard bit) needed fill least-significant bit sum, becomes 0.111110 2, second dis- carded bit becomes round bit. analogous case (1) multiplicationalgorithm (see page J-19). round bit 1 ’t enough decide whether round up. Instead, need remaining bits (0001) sticky bit. case, stickyis1,sothefinal resultmustberounded upto 0.111111. Thisexample shows subtraction causes most-significant bit cancel, one guard bit needed. Itis natural ask whether two guard bits needed case twomost- significant bits cancel. answer no, xandyare close top two bits x/C0ycancel, x/C0ywill exact, guard bits ’tn e e e da ta l l .J-22 ■Appendix J Computer ArithmeticTo summarize, addition complex multiplication because, depend- ing signs operands, may actually subtraction. addition, carry-out left, second example. subtraction, therecan cancellation, third example. case, position roundbit different. However, ’t need compute exact sum round. infer sum high-order pbits together round sticky bits. rest section devoted detailed discussion floatingpoint addition algorithm. Let 1anda2be two numbers added. notations ei andsiare used exponent significand addends ai. means floating-point inputs unpacked sihas explicit leading bit. add a1anda2, perform eight steps: 1.Ife1<e2, swap operands. ensures difference exponents satisfies d¼e1/C0e2/C210. Tentatively set exponent result e1. 2.If signs a1anda2differ, replace s2by two ’s complement. 3.Place s2in ap-bit register shift d¼e1/C0e2places right (shifting 1’si fs2was complemented previous step). bits shifted out, set g most-significant bit, set rto next most-significant bit, set sticky rest. 4.Compute preliminary significand S¼s1+s2by adding s1to p-bit register containing s2. signs a1anda2are different, most-significant bit 1, carry-out, Sis negative. Replace Swith two ’s complement. happen d¼0. 5.Shift Sas follows. signs a1anda2are carryout step 4, shift Sright one, filling high-order position 1 (the carry- out). Otherwise, shift left normalized. left-shifting, first shift fill low-order position gbit. that, shift zeros. Adjust exponent result accordingly. 6.Adjust rands.I fSwas shifted right step 5, set r:¼low-order bit Sbefore shifting s:¼gORrORs. shift, set r:¼g,s:¼rORs.I f single left shift, ’t change rands. two left shifts, r:¼0,s:¼0. (In last case, two shifts happen a1anda2have opposite signs exponent, case com- putation s1+s2in step 4 exact.) 7.Round Susing Figure J.11 ; namely, table entry nonempty, add 1 low-order bit S. rounding causes carry-out, shift Sright adjust expo- nent. significand result. 8.Compute sign result. a1anda2have sign, sign result. a1anda2have different signs, sign result depends a1ora2is negative, whether swap step 1, whether replaced two ’s complement step 4. See Figure J.12 .J.5 Floating-Point Addition ■J-23Example Use algorithm compute sum ( /C01.001 2/C22/C02)+(/C01.111 2/C220). Answer s1¼1.001, e1¼/C02,s2¼1.111, e2¼0 1.e1<e2,so swap. d¼2. Tentative exp ¼0. 2.Signs operands negative, ’t negate s2. 3.Shift s2(1.001 swap) right 2, giving s2¼.010, g¼0,r¼1,s¼0. 4.1:111 +:010 1ðÞ0:001 S¼0:001, carry /C0out: 5.Carry-out, shift Sright, S¼1.000, exp ¼exp+1, exp ¼1. 6.r¼low-order bit sum ¼1,s¼g_r_s¼0_1_0¼1. 7.rANDs¼TRUE,s oFigure J.11 says round up, S¼S+1 S¼1.001. 8.Both signs negative, sign result negative. Final answer: /C0S/C22exp¼1.001 2/C221. Example Use algorithm compute sum ( /C01.010 2)+1.100 2. Answer s1¼1.010, e1¼0,s2¼1.100, e2¼0 1.No swap, d¼0, tentative exp ¼0. 2.Signs differ, replace s2with 0.100. 3.d¼0, shift. r¼g¼s¼0. 4.1:010 +0:100 1:110 Signs different, most-significant bit 1, carry-out, must two ’s complement sum, giving S¼0:010:swap compl sign( a1) sign( a2) sign(result) Yes + /C0/C0 Yes /C0 ++ + /C0 + /C0 + /C0 Yes + /C0/C0 Yes /C0 ++ Figure J.12 Rules computing sign sum addends different signs. swap column refers swapping operands step 1, compl column refers performing two ’s complement step 4. Blanks “don ’tc r e . ”J-24 ■Appendix J Computer Arithmetic5.Shift left twice, S¼1.000, exp ¼exp/C02, exp ¼/C02. 6.Two left shifts, r¼g¼s¼0. 7.No addition required rounding. 8.Answer sign /C2S/C22expor sign /C21.000 /C22/C02. Get sign Figure J.12 . Since complement swap sign( a1)i s/C0, sign sum +. Thus, answer ¼1.000 2/C22/C02. Speeding Addition Let’s estimate long takes perform algorithm above. Step 2 may require addition, step 4 requires one two additions, step 7 may require addi-tion. takes Ttime units perform p-bit add (where p¼24 single preci- sion, 53 double), appears algorithm take least 4 Ttime units. pessimistic. step 4 requires two adds, 1anda2have exponent different signs, case difference exact, roundupis required step 7. Thus, three additions ever occur. Similarly,it appears variable shift may required step 3 step 5. ifje 1/C0e2j/C201, step 3 requires right shift one place, step 5 needs variable shift. And, je1/C0e2j>1, step 3 needs variable shift, step 5 require left shift one place. single variable shift performed. Still, algorithm requires three sequential adds, which, case 53-bit double-precision significand, rather time consuming. number techniques speed addition. One use pipelining. “Put- ting Together ”section gives examples commercial chips pipeline addition. Another method (used Intel 860 [ Kohn Fu 1989 ]) perform two additions parallel. explain reduces latency 3 TtoT. three cases consider. First, suppose operands sign. want combine addition operations steps 4 7. position high-order bit sum known ahead time, addition step 4 may may cause carry-out. possibilities areaccounted two adders. first adder assumes add step 4 willnot result carry-out. Thus, values randscan computed add actually done. randsindicate roundup necessary, first adder compute S¼s 1+s2+1, notation +1 means adding 1 position least-significant bit s1. done regular adder setting low- order carry-in bit 1. randsindicate roundup, adder computes S¼s1+s2 usual. One extra detail: r¼1,s¼0, also need know low- order bit sum, also computed advance quickly. Thesecond adder covers possibility carry-out. values r andsand position roundup 1 added different above, quickly computed advance. known whether therewill carry-out add actually done, ’t matter. adds parallel, one adder guaranteed reduce correct answer.J.5 Floating-Point Addition ■J-25The next case a1anda2have opposite signs exponent. sum a1+a2is exact case (no roundup necessary) sign ’t known add completed. ’t compute two ’s complement (which requires add) step 2, instead compute s1+s2+ 1 s1+s2+ 1 parallel. first sum result simultaneously complementing s1and computing sum, resulting s2/C0s1. second sum computes s1/C0s2. One nonnegative hence correct final answer. again, additionsare done one step using two adders operating parallel. last case, 1anda2have opposite signs different exponents, complex. je1/C0e2j>1, location leading bit difference one two locations, two cases addition. je1/C0e2j¼1, cancellation possible leading bit could almost anywhere. However,only leading bit difference position leading bit 1 could roundup necessary. one adder assumes roundup, assumes roundup. Thus, addition step 4 rounding step 7can combined. However, still problem addition step 2! eliminate addition, consider following diagram step 4: j__ __ p__ __j s11:xxxxxxx s2/C0 1yyzzzzz bits marked zare 0, high-order pbits S¼s1/C0s2can com- puted s1+s2+ 1. least one zbits 1, use s1+s2.S s1/C0s2can computed one addition. However, still ’t know gandrfor two ’s complement s2, needed rounding step 7. compute s1/C0s2and get proper gandrbits, combine steps 2 4 follows. ’t complement s2in step 2. Extend adder used computing Stwo bits right (call extended sum S0). preliminary sticky bit (computed step 3) 1, compute S0¼s0 1+s0 2, s10has two 0 bits tacked onto right, ands20has preliminary gandrappended. sticky bit 0, compute s0 1+s0 2+1 . two low-order bits S0have correct values gandr(the sticky bit already computed properly step 3). Finally, modification becombined modification combines addition steps 4 7to provide final result time T, time one addition. details need considered, discussed Santoro, Bewick, Horowitz [1989] Exercise J.17. Although Santoro paper aimed mul- tiplication, much discussion applies addition well. Also relevant isExercise J.19, contains alternative method adding signed magnitude numbers. Denormalized Numbers Unlike multiplication, addition little changes preceding description one inputs denormal number. must test see exponentfield 0. is, unpacking significand leading 1.J-26 ■Appendix J Computer ArithmeticBy setting biased exponent 1 unpacking denormal, algorithm works unchanged. deal denormalized outputs, step 5 must modified slightly. Shift normalized, exponent becomes Emin(that is, biased expo- nent becomes 1). exponent Eminand, rounding, high-order bit 1, result normalized number packed usual way,by omitting 1. If, hand, high-order bit 0, result denor-mal. result unpacked, exponent field must set 0. Section J.7 discusses exact rules detecting underflow. Incidentally, detecting overflow easy. happen step 5 involves shift right biased exponent point bumped 255in single precision (or 2047 double precision), occurs rounding. J.6 Division Remainder section, ’ll discuss floating-point division remainder. Iterative Division earlier discussed algorithm integer division. Converting floating- point division algorithm similar converting integer multiplication algo-rithm floating point. formula s1/C22e1ðÞ =s2/C22e2ðÞ ¼s1=s2ðÞ /C22e1/C0e2 shows divider computes s1/s2, final answer quotient multiplied 2e1/C0e2. Referring Figure J.2(b) ( p g eJ - 4 ) ,t h ea l g n e n fo p e r - ands slightly different integer division. Load s2into B s1into P. register needed hold operands. integer algorithm divi-sion (with one small change skipping first left shift) used,and result form q 0/C1q1⋯. round, simply compute two addi- tional quotient bits (guard round) use remainder sticky bit. Theguard digit necessary first quotient bit might 0. However, since numerator denominator n ormalized, possible two most-significant quotient bits 0. algorithm produces one quotient bit ineach step. different approach division converges quotient quadratic rather linear rate. actual machine uses algorithm dis-cussed Section J.10 . First, describe two main iterative algorithms, discuss pros cons iteration compared thedirect algorithms. general technique f constructing iterative algorithms, called Newton ’si e r n ,i ss h w ni n Figure J.13 . First, cast problem form finding zero function. Then, starting guess zero,approximate function tangent guess form new guess basedJ.6 Division Remainder ■J-27on tangent zero. xiis guess zero, tangent line equation: y/C0fxiðÞ ¼ f0xiðÞx/C0xi ðÞ equation zero x¼xi+1¼xi/C0fxiðÞ f0xiðÞJ:6:1 recast division finding zero function, consider f(x)¼x/C01/C0b. Since zero function 1/ b,applying Newton ’s iteration give iterative method computing 1/ bfrom b.U n g f0(x)¼/C01/x2,Equation J.6.1 becomes: xi+1¼xi/C01=xi/C0b /C01=x2 i¼xi+xi/C0x2 ib¼xi2/C0xib ðÞ J:6:2 Thus, could implement computation a/busing following method: 1.Scale bto lie range 1 /C20b<2 get approximate value 1/ b(call x0) using table lookup. 2.Iterate xi+1¼xi(2/C0xib) reaching xnthat accurate enough. 3.Compute axnand reverse scaling done step 1. details. many times step 2 iterated? say xiis accurate pbits means j(xi/C01/b)/(1/b)j¼2/C0p, simple alge- braic manipulation shows so, ( xi+1/C01/b)/(1/b)¼2/C02p. Thus, number correct bits doubles step. Newton ’s iteration self-correct- ingin sense making error xidoesn ’t really matter. is, treats xias guess 1/ band returns xi+1as improvement (roughly doubling digits). One thing would cause xito error rounding error. Morex xi+1xif(x) f(xi) Figure J.13 Newton ’s iteration zero finding. Ifxiis estimate zero f, xi+1is better estimate. compute x i+1, find intersection x-axis tangent line fatf(xi).J-28 ■Appendix J Computer Arithmeticimportantly, however, early iterations take advantage fact ’t expect many correct bits performing multiplication reduced pre- cision, thus gaining speed without sacrificing accuracy. Another application ofNewton ’s iteration discussed Exercise J.20. second iterative division method sometimes called Goldschmidt ’s algo- rithm . based idea compute a/b, multiply numer- ator denominator number rwith rb/C251. detail, let x 0¼a y0þ¼b. step compute xi+1¼rixiand yi+1¼riyi. quotient xi+1/yi+1¼xi/yi¼a/bis constant. pick riso yi!1, xi!a/b, xiconverge answer want. idea used compute functions. example, compute square root a, letx0¼aandy0¼a, step compute xi+1¼ri2xi,yi+1¼riyi. xi+1/yi+12¼xi/yi2¼1/a, riare chosen drive xi!1, yi!ﬃﬃﬃap. technique used compute square roots TI 8847. Returning Goldschmidt ’s division algorithm, set x0¼aandy0¼b, write b¼1/C0δ, jδj<1. pick r0¼1+δ, y1¼r0y0¼1/C0δ2. next pick r1¼1+δ2, y2¼r1y1¼1/C0δ4, on. Since jδj<1,yi!1. choice ri, xiwill computed xi+1¼rixi¼1+δ2i/C16/C17 xi¼ 1+ 1 /C0bðÞ2i/C16/C17 xi,o r xi+1¼a1+ 1 /C0bðÞ½/C138 1+ 1 /C0bðÞ2hi 1+ 1 /C0bðÞ4hi ⋯1+ 1 /C0bðÞ2ihi J:6:3 appear two problems algorithm. First, convergence slow bis near 1 (that is, δis near 0), and, second, formula ’t self- correcting —since quotient computed product independent terms, error one ’t get corrected. deal slow convergence, want compute a/b, look approximate inverse b(call b0), run algorithm ab0/bb0. converge rapidly since bb0/C251. deal self-correction problem, computation run bits extra precision compensate rounding errors. However, Gold- schmidt ’s algorithm weak form self-correction, precise value ridoes matter. Thus, first iterations, full pre- cision 1 /C0δ2iis needed choose rito truncation 1 + δ2i,w h c h may make iterations run faster without affecting speed convergence.Ifr iis truncated, yiis longer exactly 1 /C0δ2i. Thus, Equation J.6.3 longer used, easy organize computation notdepend precise value r i. changes, Goldschmidt ’s algorithm follows (the notes brackets show connection earlier formulas). 1.Scale aandbso 1 /C20b<2. 2.Look approximation 1/ b(call b0) table. 3.Setx0¼ab0andy0¼bb0.J.6 Division Remainder ■J-294.Iterate xiis close enough a/b: Loop r/C252/C0y ifyi¼1+δi,then r/C251/C0δi ½/C138 y¼y/C2ry i+1¼yi/C2r/C251/C0δi2/C2/C3 xi+1¼xi/C2rx i+1¼xi/C2r ½/C138 End loop two iteration methods related. Suppose Newton ’s method unroll iteration compute term xi+1directly terms b, instead recursively terms xi. carrying calculation (see Exercise J.22), discover xi+1¼x02/C0x0b ðÞ 1+x0b/C01 ðÞ2/C16i 1+x0b/C01 ðÞ4hi ⋯1+x0b/C01 ðÞ2ihi h formula similar Equation J.6.3. fact, identical aandbin J.6.3 replaced ax0,bx0, a¼1. Thus, iterations done infi- nite precision, two methods would yield exactly sequence xi. advantage iteration ’t require special divide hardware. Instead, use multiplier (which, however, requires extra control). Further,on step, delivers twice many digits previous step —unlike ordi- nary division, produces fixed number digits every step. two disadvantages inverting iteration. first IEEE standard requires division correctly rounded, iteration delivers result close correctly rounded answer. case Newton ’s iter- ation, computes 1/ binstead a/bdirectly, additional problem. Even 1/ bwere correctly rounded, guarantee a/bwill be. exam- ple decimal p¼2i sa¼13,b¼51. a/b¼.2549…, rounds .25. 1/ b¼.0196…, rounds .020, a/C2.020¼.26, 1. second disadvantage iteration give remainder. isespecially troublesome floating-point divide hardware used toperform integer division, since remainder operation present almost every high-level language. Traditional folklore held way get correctly rounded result iteration compute 1/ bto slightly 2 pbits, compute a/bto slightly 2 pbits, round pbits. However, faster way, apparently first implemented TI 8847. method, a/bis computed 6 extra bits precision, giving preliminary quotient q. comparing qb a(again 6 extra bits), possible quickly decide whether q correctly rounded whether needs bumped 1 least-significant place. algorithm explored Exercise J.21. One factor take account deciding division algorithms rel- ative speed division multiplication. Since division complex mul-tiplication, run slowly. common rule thumb divisionalgorithms try achieve speed one-third multiplication.J-30 ■Appendix J Computer ArithmeticOne argument favor rule real programs (such ver- sions spice) ratio division multiplication 1:3. Another place factor 3 arises standard iterative method computing squareroot. method involves one division per iteration, replaced oneusing three multiplications. discussed Exercise J.20. Floating-Point Remainder nonnegative integers, integer division remainder satisfy: a¼aDIV b ðÞ b+aREM b,0 /C20aREM b<b floating-point remainder xREMycan similarly defined x¼INT(x/y)y+xREM y. x/ybe converted integer? IEEE remainder function uses round-to-even rule. is, pick n¼INT(x/y) jx/y/C0nj/C201/2. two dif- ferent nsatisfy relation, pick even one. REMis defined x/C0yn. Unlike integers 0 /C20aREMb<b, floating-point numbers jxREMyj/C20y/2. Although defines REMprecisely, practical operational definition, ncan huge. single precision, ncould large 2127/ 2/C0126¼2253/C251076. natural way compute REMif direct division algorithm used. Proceed computing x/y.I fx¼s12e1andy¼s22e2and divider Figure J.2(b) (page J-4), load s1into P s2into B. e1/C0e2 division steps, P register hold number rof form x/C0ynsatisfying 0/C20r<y. Since IEEE remainder satisfies jREMj/C20y/2,REMis equal either r orr/C0y. necessary keep track last quotient bit produced, needed resolve halfway cases. Unfortunately, e1/C0e2can lot steps, floating-point units typically maximum amount time allowed tospend one instruction. Thus, usually possible implement REMdirectly. None chips discussed Section J.10 implements REM, could providing remainder-step instruction —this done Intel 8087 fam- ily. remainder step takes arguments two numbers xandy,and performs divide steps either remainder P nsteps performed, nis small number, number steps required division highest-supported precision. REMcan implemented software routine calls REMstep instruction b(e1/C0e2)/nctimes, initially using xas numerator replacing remainder previous REMstep. REMcan used computing trigonometric functions. simplify things, imagine working base 10 five significant figures, considercomputing sin x. Suppose x¼7. reduce π¼3.1416 com- pute sin(7) ¼sin(7/C02/C23.1416) ¼sin(0.7168) instead. But, suppose want compute sin(2.0 /C210 5). 2 /C2105/3.1416 ¼63661.8, five-place system comes 63662. Since multiplying 3.1416 times 63662 gives200000.5392, rounds 2.0000 /C210 5, argument reduction reduces 2/C2105to 0, even close correct. problem ourJ.6 Division Remainder ■J-31five-place system precision correct argument reduction. Suppose REMoperator. could compute 2 /C2105REM3.1416 get /C0.53920. However, still correct used 3.1416, approximation π. value 2 /C2105REM πis/C0.071513. Traditionally, two approaches computing periodic functions large arguments. first return error value xis large. second store πto large number places exact argument reduction. REMoperator much help either situations. third approach used math libraries, Berkeley UNIX 4.3bsd release. libraries, πis computed nearest floating-point number. Let ’s call machine π, denote π0. Then, computing sin x, reduce xusing xREMπ0. saw example, xREMπ0is quite different xREMπwhen xis large, computing sin xas sin( xREMπ0) give exact value sin x. However, computing trigonometric functions fash- ion property familiar identities (such sin2x+cos2x¼1) true within rounding errors. Thus, using REMtogether machine πprovides simple method computing trigonometric functions accurate small arguments still may useful large arguments. REMis used argument reduction, handy also returns low-order bits n(where xREMy¼x/C0ny). practical implemen- tation trigonometric functions reduce something smaller 2 π. example, might use π/2, exploiting identities sin( x/C0π/2)¼/C0cos x, sin( x/C0π)¼/C0sinx. low bits nare needed choose correct identity. J.7 Floating-Point Arithmetic leaving subject floating-point arithmetic, present additionaltopics. Fused Multiply-Add Probably common use floating-point units performing matrixoperations, frequent matrix operation multiplying matrix timesa matrix (or vector), boils computing inner product,x 1/C1y1+x2/C1y2+…+xn/C1yn. Computing requires series multiply-add combinations. Motivated this, IBM RS/6000 introduced single instruction computes ab+c, fused multiply-add . Although requires able read three operands single instruction, potential improving perfor- mance computing inner products. fused multiply-add computes ab+cexactly rounds. Although rounding increases accuracy inner products somewhat, isnot primary motivation. two main advantages rounding once. First,J-32 ■Appendix J Computer Arithmeticas saw previous sections, rounding expensive implement may require addition. rounding once, addition operation eliminated. Second, extra accuracy fused multiply-add used com-pute correctly rounded division square root availabledirectly hardware. Fused multiply-add also used implement efficientfloating-point multiple-precision packages. implementation correctly rounded division using fused multiply-add many details, main idea simple. Consider example fromSection J.6 (page J-30), computing a/bwith a¼13,b¼51. 1/ b rounds b 0¼.020, ab0rounds q0¼.26, correctly rounded quotient. Applying fused multiply-add twice correctly adjust result, via theformulas r¼a/C0bq0 q00¼q0+rb0 Computing two-digit accuracy, bq0¼51/C2.26 rounds 13, r¼a/C0bq0 would 0, giving adjustment. using fused multiply-add gives r¼a/C0bq0¼13/C0(51/C2.26)¼/C0.26, q00¼q0+rb0¼.26/C0.0052 ¼.2548, rounds correct quotient, .25. details found papers byMontoye, Hokenek, Runyon [1990] andMarkstein [1990] . Precisions standard specifies four precisions: single ,single extended ,double , double extended . properties precisions summarized Figure J.7 (page J- 16). Implementations required four precisions, encour-aged support either combination single single extended sin-gle, double, double extended. widespread use doubleprecision scientific computing, double precision almost always implemented. Thus, computer designer usually decide whether support double extended and, so, many bits have. Motorola 68882 Intel 387 coprocessors implement extended precision using smallest allowable size 80 bits (64 bits significand). However, manyof recently designed, high-performance floating-point chips imple-ment 80-bit extended precision. One reason 80-bit width extendedprecision awkward 64-bit buses registers. new architectures, suchas SPARC V8 PA-RISC, specify 128-bit extended (or quad ) precision. established de facto convention quad 15 bits exponent 113 bits significand. Although high-level languages provide access extended preci- sion, useful writers mathematical software. example, considerwriting library routine compute length vector ( x,y) plane, namely,ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ x 2+y2p .I fxis larger 2Emax=2, computing obvious way overflow. means either allowable exponent range subroutineJ.7 Floating-Point Arithmetic ■J-33will cut half complex algorithm using scaling employed. But, extended precision available, simple algorithm work. Computing length vector simple task, difficult tocome algorithm ’t overflow. However, com- plex problems extended precision means difference asimple, fast algorithm much complex one. One best examplesof binary-to-decimal conversion. efficient algorithm binary-to-decimal conversion makes essential use extended precision readablypresented Coonen [1984] . algorithm also briefly sketched Goldberg [1991] . Computing accurate values transcendental functions another example problem made much easier extended precision present. One important fact precision concerns double rounding .T oi l l u - r ei nd e c l ,s u p p et h tw ew n tt oc p u e1 . 9 /C20.66 single precision two digits, extended pr ecision three digits. exact result product 1.254. Rounded extended precision, result 1.25. Whenfurther rounded single precision, get 1.2. However, result 1.9 /C20.66 correctly rounded single precision 1.3. Thus, rounding twice may pro- duce result rounding once. Suppose want build hardware double-precision arithmetic . simulate single precision computing first double precision rounding single? example suggests ’t. However, double rounding always dangerous. fact, following rule true (this easy prove, buts e eE x e r c eJ . 2 5 ) . Ifxandyhave p-bit significands, x+y computed exactly rounded toqplaces, second rounding pplaces change answer q/C212p+2. true addition, also multiplication, division, square root. example above, q¼3 p¼2, qŠ2p+2 true. hand, IEEE arithmetic, double precision q¼53 p¼24, q¼53Š 2p+2¼50. Thus, single precision implemented computing double precision —that is, computing answer exactly rounding double — rounding single precision. Exceptions IEEE standard defines five exceptions: underflow, overflow, divide zero, inexact, invalid. default, exceptions occur, merely set aflag computation continues. flags sticky , meaning set remain set explicitly cleared. standard strongly encourages implementa- tions provide trap-enable bit exception. exception anenabled trap handler occurs, user trap handler called, value asso-ciated exception flag undefined. Section J.3 mentioned thatﬃﬃﬃﬃﬃﬃ ﬃ /C03p value NaN 1/0 ∞. examples operations raise exception.J-34 ■Appendix J Computer ArithmeticBy default, computingﬃﬃﬃﬃﬃﬃ ﬃ /C03p sets invalid flag returns value NaN. Similarly 1/0 sets divide-by-zero flag returns ∞. underflow, overflow, divide-by-zero exceptions found systems. invalid exception result operations asﬃﬃﬃﬃﬃﬃ ﬃ /C01p , 0/0, or∞/C0∞, ’t natural value floating-point num- ber /C6∞. inexact exception peculiar IEEE arithmetic occurs either result operation must rounded overflows. Infact, since 1/0 operation overflows deliver ∞, exception flags must consulted distinguish them. inexact exception unusual “exception, ”in really exceptional condition occurs frequently. Thus, enabling trap handler inexact exception willmost likely severe impact performance. Enabling trap handler ’t affect whether operation exceptional except case underflow. isdiscussed below. IEEE standard assumes trap occurs, possible identify operation trapped operands. machines pipelining multiplearithmetic units, exception occurs, may enough simply trap handler examine program counter. Hardware support may necessary identify exactly operation trapped. Another problem illustrated following program fragment. r1 = r2/r3 r2 = r4 + r5 two instructions might well executed parallel. divide traps, argument r2could already overwritten addition, especially since addition almost always faster division. Computer systems support trap- ping IEEE standard must provide way save value r2, either hardware compiler avoid situation first place. Thiskind problem peculiar floating point. sequence r1 = 0(r2) r2 = r3 would efficient execute r2 = r3 waiting memory. But, acces- sing0(r2) causes page fault, r2might longer available restarting instruction r1 = 0(r2) . One approach problem, used MIPS R3010, identify instructions may cause exception early instruction cycle. example, additioncan overflow one operands exponent E max, on. early check conservative: might flag operation ’t actually cause exception. However, false positives rare, technique haveexcellent performance. instruction tagged possibly exceptional, special code trap handler compute without destroying state. Remember problems occur trap handlers enabled. Otherwise, settingthe exception flags normal processing straightforward.J.7 Floating-Point Arithmetic ■J-35Underflow alluded several times fact detection underflow complex exceptions. IEEE standard specifies anunderflow trap handler enabled, sy stem must trap result denormal. hand, trap handlers disabled, underflow flag setonly loss accuracy —that is, result must rounded. rationale is, accuracy lost underflow, point setting awarning flag. trap handler enabled, user might trying sim-ulate flush-to-zero therefore notified whenever result dips 1 :0/C22 Emin. trap handler, underflow exception signaled result denormal inexact, definitions denormal andinexact subject multiple interpretations. Normally, inexact means wasar e u l tt h tc u l n ’t represented exactly rounded. Consider example (in base 2 floating-point system 3-bit significands) 1:11 2/C22/C02/C0/C1 /C21:112/C22Emin/C0/C1 ¼0:110001 2/C22Emin, round nearest effect. delivered result 0 :112/C22Emin, rounded, causing inexact signaled. correct also signal underflow? Gradual underflow loses significance exponent range bounded. exponent range wereunbounded, delivered result would 1 :10 2/C22Emin/C01, exactly answer obtained gradual underflow. fact denormalized numbers fewerbits significand normalized numbers therefore ’tm k ea n difference case. commentary standard [ Cody et al. 1984 ] encour- ages criterion setting underflow flag. is, beset whenever delivered result different would delivered system fraction size, large exponent range. However, owing difficulty implementing scheme, standard allows settingthe underflow flag whenever result denormal different infinitelyprecise result. two possible definitions means result denormal. Consider example 1.10 2/C22/C01multiplied 1 :102/C22Emin. exact product 0:1111/C22Emin. rounded result normal number 1 :002/C22Emin. underflow signaled? Signaling underflow means using rounding rule, result denormal rounding. signaling underflow means using rounding rule, result normalized rounding. IEEE standard provides choosing either rule;however, one chosen must used consistently operations. illustrate rules, consider floating-point addition. result addition (or subtraction) denormal, always exact. Thus, underflow flagnever needs set addition. ’s traps enabled exception raised. traps enabled, value underflow flag undefined, ’t need set. One final subtlety mentioned concerning underflow. underflow trap handler, result operation p-bit numbers causesJ-36 ■Appendix J Computer Arithmetican underflow denormal number p/C01 fewer bits precision. traps enabled, trap handler provided result operation rounded pbits exponent wrapped around. potential double-rounding problem. trap handler wants return denormal result, itcan’t round argument, might lead double-rounding error. Thus, trap handler must passed least one extra bit information tobe able deliver correctly rounded result. J.8 Speeding Integer Addition previous section showed many steps go implementing floating-pointoperations; however, floating-point operation eventually reduces integeroperation. Thus, increasing speed integer operations also lead fasterfloating point. Integer addition simplest operation important. Even programs ’t explicit arithmetic, addition must performed incre- ment program counter calculate addresses. Despite simplicity addition, ’t single best way perform high-speed addition. dis- cuss three techniques current use: carry-lookahead, carry-skip, andcarry-select. Carry-Lookahead Ann-bit adder combinational circuit. therefore written logic formula whose form sum products computed circuit twolevels logic. figure circuit looks like? FromEquation J.2.1 (page J-3) formula ith sum written as: si¼aibici+aibici+aibici+aibici J:8:1 ciis carry-in ith adder carry-out ( i/C01)-st adder. problem formula that, although know values ai andbi—they inputs circuit —we ’t know ci. goal write ciin terms aiand bi. accomplish this, first rewrite Equation J.2.2 (page J-3) as: ci¼gi/C01+pi/C01ci/C01,gi/C01¼ai/C01bi/C01,pi/C01¼ai/C01+bi/C01 J:8:2 reason symbols pandg:I fgi/C01is true, ciis certainly true, carry generated . Thus, gis generate. pi/C01is true, ci/C01is true, propagated toci. Start Equation J.8.1 use Equation J.8.2 replace ciwith gi/C01+pi/C01ci/C01. Then, use Equation J.8.2 i/C01 place replace ci/C01with ci/C02, on. gives result: ci¼gi/C01+pi/C01gi/C02+pi/C01pi/C02gi/C03+⋯+pi/C01pi/C02⋯p1g0+pi/C01pi/C02⋯p1p0c0 J:8:3J.8 Speeding Integer Addition ■J-37An adder computes carries using Equation J.8.3 called carry-lookahead adder , CLA. CLA requires one logic level form pandg, two levels form carries, two sum, grand total five logic levels. vast improvement 2 nlevels required ripple-carry adder. Unfortunately, evident Equation J.8.3 Figure J.14 , carry- lookahead adder nbits requires fan-in n+1 ORgate well rightmost ANDgate. Also, pn/C01signal must drive nANDgates. addition, rather irregular structure many long wires Figure J.14 make impractical build full carry-lookahead adder nis large. However, use carry-lookahead idea build adder log2nlogic levels (substantially fewer 2 nrequired ripplecarry adder) yet simple, regular structure. idea build p’s g’si n steps. already seen c1¼g0+c0p0 says carry-out 0th position ( c1) either carry gen- erated 0th position carry 0th position carry propagates. Similarly, c2¼G01+P01c0 G01means carry generated block consisting first two bits. P01means carry propagates block. PandGhave following logic equations: G01¼g1+p1g0 P01¼p1p0gn–1pn–1 cngn–2pn–2gn–3p1g0p0c0 cn= gn–1+ pn–1gn–2+ . . . + pn–1pn–2 . . . p1g0+ pn–1pn–2. . . p0c0 Figure J.14 Pure carry-lookahead circuit computing carry-out c nof n-bit adder.J-38 ■Appendix J Computer ArithmeticMore generally, jwith i<j,j+1<k, recursive relations: ck+1¼Gik+Pikci J:8:4 Gik¼Gj+1,k+Pj+1,kGij J:8:5 Pik¼PijPj+1,k J:8:6 Equation J.8.5 says carry generated block consisting bits kinclusive generated high-order part block ( j+1,k) generated low-order part block ( i,j) propagated high part. equations also hold i/C20j<kif set Gii¼gi andPii¼pi. Example Express P03andG03in terms p’s g’s. Answer Using Equation J.8.6 ,P03¼P01P23¼P00P11P22P33. Since Pii¼pi,P03¼p0p1p2p3. G03,Equation J.8.5 says G03¼G23+P23G01¼(G33+P33G22)+(P22P33) (G11+P11G00)¼g3+p3g2+p3p2g1+p3p2p1g0. preliminaries way, show design practical CLA. adder consists two parts. first part computes variousvalues PandGfrom p iandgi,u n g Equations J.8.5 andJ.8.6 ;t h es e c n dp r uses PandGvalues compute carries via Equation J.8.4 .T h ef r part design shown Figure J.15 . top diagram, input num- bers a7…a0andb7…b0are converted p’sa n g’s using cells type 1. various P’sa n G’s generated combining cells type 2 binary tree structure. second part design shown Figure J.16 .B yf e e n g c0in bottom tree, carry bits come top. cell mustknow pair ( P,G) values order conversion, value needs written inside cells. compare Figures J.15 andJ.16. one-to- one correspondence cells, value ( P,G) needed carry- generating cells exactly value known corresponding ( P,G)- generating cells. combined cell shown Figure J.17 . numbers added flow top downward tree, combining c 0 bottom flowing back tree form carries. Note one thing missing Figure J.17 : small piece extra logic compute c8for carry-out adder. bits CLA must pass log 2nlogic levels, compared 2nfor ripple-carry adder. substantial speed improvement, especially large n.Whereas ripple-carry adder ncells, however, CLA 2 n cells, although layout take nlognspace. point small investment size pays dramatic improvement speed. number technology-dependent modifications improve CLAs. example, node tree three inputs instead two, heightJ.8 Speeding Integer Addition ■J-39of tree decrease log 2nto log 3n.Of course, cells complex thus might operate slowly, negating advantage thedecreased height. technologies rippling works well, hybrid designmight better. illustrated Figure J.19 . Carries ripple adders11 11 11 111 2 2 22 2a7b7a6b6a5b5a4b4a3b3a2b2a1b1a0b0 p0g0p1g1g7p7 G6, 7P6, 7G4, 5P4, 5G2, 3P2, 3G0 ,1P0 ,1 G4, 7P4, 7G0, 3P0, 3 G0, 7P0, 7 gi = aibi pi = ai + bi Gi, k = Gj+1, k + Pj+1, k Gi, jPi, k = Pi, j Pj+1,kPi, jGi, jGj+1, kaibiPj+1, k2 2 2 Figure J.15 First part carry-lookahead tree. signals flow top bottom, various values Pand Gare computed. c7c6c5c4c3c2c1c0 p0 g0p2 g2 P0, 1 G0, 1p4 g4p6 g6 c6c4c2c0 c0c4 c0cj+1 = Gi j + Pi j cici Pi, j Gi, j cip4, 5 G4, 5 P0, 3 G0, 3 Figure J.16 Second part carry-lookahead tree. Signals flow bottom top, combining Pand Gto form carries.J-40 ■Appendix J Computer Arithmeticat top level, “B”boxes Figure J.17 . design faster time ripple four adders faster timeit takes traverse level “B”boxes. (To make pattern clear, Figure J.19 shows 16-bit adder, 8-bit adder Figure J.17 corresponds right half Figure J.19 .) Carry-Skip Adders Acarry-skip adder sits midway ripple-carry adder carry- lookahead adder, terms speed cost. (A carry-skip adder calleda CSA, name reserved carry-save adders.) motivation thisadder comes examining equations PandG. example, P03¼p0p1p2p3 G03¼g3+p3g2+p3p2g1+p3p2p1g0 Computing Pis much simpler computing G, carry-skip adder computes P’s. adder illustrated Figure J.18 . Carries begin ripplingAA AB Bs7 a7b7 c7AA AA AA BB B B ++Bs1 a1b1s0 a0b0 c6c5c4c3c2c1c0 c0 c0P0, 3G0, 3c4 c0siaibi si = ai pi = ai + bi gi = ai bi gipici Gi, kPi, kciciPijGijcj+1Pj+1,k Gj+1,k bi ci c2c4c6 B Figure J.17 Complete carry-lookahead tree adder. combination Figures J.15 J.16. numbers added enter top, flow bottom combine c0, flow back compute sum bits.J.8 Speeding Integer Addition ■J-41simultaneously block. block generates carry, carry-out ofa block willbetrue, eventhoughthe carry-intothe block may becorrect yet.Ifatthe start add operation carry-in block 0, spurious carry-outs generated. Thus, carry-out block thought weretheGsignal. carry-out least-significant block generated, feeds next block also fed gate Psignal next block. carry-out Psignals true, carry skips second block ready feed third block, on. carry-skip adder practical carry-in signals easily clearedat start operation —for example, precharging CMOS. analyze speed carry-skip adder, let ’s assume takes 1 time unit signal pass two logic levels. take ktime units carry ripple across block size k, take 1 time unit carry skip block. longest signal path carry-skip adder starts carry generated 0th position. adder nbits wide, takes ktime units ripple first block, n/k/C02 time units skip blocks, kmore ripple last block. sp ecific: 20-bit adder broken groups 4 bits, take 4+(20/4 /C02)+4 ¼11 time units perform ana3 b3 a2 b2 a1 b1 a0 b0 c4c0 P4, 7c8c12 P12, 15P8, 11a19a18b19b18 c16c20 Figure J.18 Carry-skip adder. 20-bit carry-skip adder ( n¼20) block 4 bits wide ( k¼4). c15c14c13 c12 P12, 15 P8, 15c8 c0P0, 7c8c4 c0c0 G0, 3P0, 3c1c2c3 C B BC C C B Figure J.19 Combination CLA ripple-carry adder. top row, carries ripple within group four boxes.J-42 ■Appendix J Computer Arithmeticadd. experimentation reveals efficient ways divide 20 bits blocks. example, consider five blocks least-significant 2 bits first block, next 5 bits second block, followed blocks size 6,5 ,a n d2 .T h e nt h ea dt ei sr e u c e dt o9t e units. illustrates important general principle. carry-skip adder, making interior blocks larger willspeed adder. fact, idea varying block sizes cansometimes speed adder designs well. large amountof rippling, carry-skip adder appr opriate technologies rippling fast. Carry-Select Adder Acarry-select adder works following principle: Two additions performed parallel, one assuming carry-in 0 assuming thecarry-in 1. carry-in finally known, correct sum (which beenprecomputed) simply selected. example design shown inFigure J.20 . 8-bit adder divided two halves, carry-out lower half used select sum bits upper half. blockis computing sum using rippling (a linear time algorithm), design inFigure J.20 twice fast 50% cost. However, note c 4signal must drive many muxes, may slow technologies. Instead divid- ing adder halves, could divided quarters still speedup.This illustrated Figure J.21 . takes ktime units block add k-bit numbers, takes 1 time unit compute mux input twocarry-out signals, optimal operation block 1 bit wider thanthe next, shown Figure J.21 . Therefore, carry-skip adder, best design involves variable-size blocks. summary section, asymptotic time space requirements different adders given Figure J.22 . (The times carry-skip c0 s0s1s2s3 c4 s4a4 b4 s5s6s710a3 b3 a2 b2 a1 b1 a0 b0a7 b7a4 b4 Figure J.20 Simple carry-select adder. time sum low-order 4 bits computed, high-order bits computed twice parallel: assuming c4¼0 assuming c4¼1.J.8 Speeding Integer Addition ■J-43carry-select come careful choice block size. See Exercise J.26 carry-skip adder.) different adders ’t thought disjoint choices, rather building blocks used constructing adder. util-ity different building blocks highly dependent technology used. example, carry-select adder works well signal drive many muxes, carry-skip adder attractive technologies signals canbe cleared start operation. Knowing asymptotic behavior ofadders useful understanding them, relying much behavior isa pitfall. reason asymptotic behavior important ngrows large. nfor adder bits precision, double precision today 20 years ago —about 53 bits. Although true computers get faster, computations get longer —and thus rounding error, turn requires precision —this effect grows slowly time. J.9 Speeding Integer Multiplication Division multiplication division algorithms presented Section J.2 fairly slow, producing 1 bit per cycle (although cycle might fraction CPUinstruction cycle time). section, discuss various techniques forhigher-performance multiplication division, including division algorithmused Pentium chip.c13c8 c4c0 s0s1s2s3 s4s5s6s7s8s910 0 1 s10s11s12s13c13c81 s14s15s16s17s180 Figure J.21 Carry-select adder. soon carry-out rightmost block known, used select sum bits. Adder Time Space Ripple 0( n)0 ( n) CLA 0(log n)0 ( nlogn) Carry-skip 0ﬃﬃﬃnpðÞ 0(n) Carry-select 0ﬃﬃﬃnpðÞ 0(n) Figure J.22 Asymptotic time space requirements four different types adders.J-44 ■Appendix J Computer ArithmeticShifting Zeros Although technique shifting zeros currently used much, instructive consider. distinguished fact execution time oper-and dependent. lack use primarily attributable failure offer enoughspeedup bit-at-a-time algorithms. addition, pipelining, synchronizationwith CPU, good compiler optimization difficult algorithms thatrun variable time. multiplication, idea behind shifting zeros toadd logic detects low-order bit register 0 (seeFigure J.2(a) page J-4) and, so, skips addition step proceeds directly shift step —hence term shifting zeros . shifting division? nonrestoring division, ALU oper- ation (either addition subtractio n) performed every step. appears opportunity skipping operation. think aboutdivision way: compute a/b, subtract multiples bfrom a,a n dt h e n report many subtractions done. stage subtraction pro-cess remainder must fit P register Figure J.2(b) (page J-4). case remainder small positive number, normally subtract b; suppose instead shifted remainder subtracted bthe next time. long remainder sufficiently small (its high-order bit 0),after shifting still would fit P register, information wouldbe lost. However, method require changing way keep trackof number times bhas subtracted a.This idea usually goes name SRT division , Sweeney, Robertson, Tocher, independently proposed algorithms nature. main extra complica-tion SRT division quotient bits cannot determined immediately sign P step, ordinary nonrestoring division. precisely, divide abybwhere aandbaren-bit numbers, load aandb B registers, respectively, Figure J.2 (page J-4). SRT Division 1.If B kleading zeros expressed using nbits, shift registers left kbits. 2.Fori¼0,n/C01, a)If top three bits P equal, set q i¼0 shift (P,A) one bit left. b)If top three bits P equal P negative, set qi¼/C01 (also written 1), shift (P,A) one bit left, add B. c)Otherwise set qi¼1, shift (P,A) one bit left, subtract B. End loop 3.If final remainder negative, correct remainder adding B, correct quotient subtracting 1 q0. Finally, remainder must shifted k bits right, kis initial shift.J.9 Speeding Integer Multiplication Division ■J-45A numerical example given Figure J.23 . Although discussing integer division, helps explaining algorithm imagine binary point left ofthe most-significant bit. changes Figure J.23 01000 2/0011 2to 0.1000 2/ .0011 2. Since binary point changed numerator denominator, quotient affected. (P,A) register pair holds remainder two ’s complement number. example, P contains 11110 2and ¼0, remain- der 1.1110 2¼/C01/8. ris value remainder, /C01/C20r<1. Given preliminaries, analyze SRT division algorithm. first step algorithm shifts bso b/C211/2. rule ALU operation perform this: /C01/4/C20r<1/4 (true whenever top three bits P equal), compute 2 rby shifting (P,A) left one bit; r<0( n dh e n c e r</C01/4, since otherwise would eliminated first condition), compute 2 r +bby shifting adding; r/C211/4 subtract bfrom 2 r.U n g b/C211/2, easy check rules keep /C01/2/C20r<1/2. nonrestoring division, jrj/C20b, need P n+1 bits wide. But, SRT division, bound ris tighter, namely, /C01/2/C20r<1/2. Thus, save bit elim- inating high-order bit P (and band adder). particular, test equality top three bits P becomes test two bits. algorithm might change slightly implementation SRT division. ALU operation, P register shifted many places necessary make either r/C211/4 r</C01/4. shifting kplaces, kquotient bits set equal zero once. reason SRT division sometimes described one thatkeeps remainder normalized jrj/C211/4.PA 00000 1000 Divide 8 ¼1000 3 ¼0011. B contains 0011. 00010 0000 Step 1: B two leading 0 s, shift left 2. B contains 1100. Step 2.1: Top three bits equal. case (a), 00100 000 0 setq0¼0 shift. Step 2.2: Top three bits equal P /C210 case (c), 01000 00 01 setq1¼1 shift. + 10100 Subtract B. 11100 00 01 Step 2.3: Top bits equal case (a), 11000 0 010 setq2¼0 shift. Step 2.4: Top three bits unequal case (b), 10000 0101 setq3¼/C01 shift. + 01100 Add B. 11100 Step 3. remainder negative restore subtract 1 q. + 01100 01000 Must undo shift step 1, right-shift 2 get true remainder. Remainder ¼10, quotient ¼0101/C01¼0010. Figure J.23 SRT division 1000 2/0011 2.The quotient bits shown bold, using notation 1 /C01.J-46 ■Appendix J Computer ArithmeticNotice value quotient bit computed given step based operation performed step (which turn depends result operation previous step). contrast nonrestoring division,where quotient bit computed ith step depends result oper- ation step. difference reflected fact finalremainder negative, last quotient bit must adjusted SRT division,but nonrestoring division. However, key fact quotient bitsin SRT division include 1. Although Figure J.23 shows quotient bits stored low-order bits A, actual implementation ’t ’t fit three values /C01, 0, 1 one bit. Furthermore, quo- tient must converted ordinary two ’s complement full adder. common way accumulate positive quotient bits one register thenegative quotient bits another, subtract two registers thebits known. one way write number termsof digits /C01, 0, 1, SRT division said use redundant quotient representation. differences SRT division ordinary nonrestoring division summarized follows: 1.ALU decision rule —In nonrestoring division, determined sign P; SRT, determined two most-significant bits P. 2.Final quotient —In nonrestoring division, immediate successive signs P; SRT, three quotient digits (1, 0, 1), final quotient must computed full n-bit adder. 3.Speed —SRT division faster operands produce zero quotient bits. simple version SRT division algorithm given offer enough speedup practical cases. However, later sectionwe study variants SRT division quite practical. Speeding Multiplication Single Adder mentioned before, shifting-over-zero techniques used much currenthardware. discuss methods widespread use. Methods thatincrease speed multiplication divided two classes: use asingle adder use multiple adders. Let ’s first discuss techniques use single adder. discussion addition noted that, carry propagation, practical perform addition two levels logic. Using cells ofFigure J.17 , adding two 64-bit numbers require trip seven cells compute P’s G’s seven compute carry bits, require least 28 logic levels. simple multiplier Figure J.2 page J-4, multiplication step passes adder. amount computation ineach step dramatically reduced using carry-save adders (CSAs). carry- save adder simply collection nindependent full adders. multiplier usingJ.9 Speeding Integer Multiplication Division ■J-47such adder illustrated Figure J.24 . circle marked “+”is single-bit full adder, box represents one bit register. addition operation results pair bits, stored sum carry parts P. Since add indepen- dent, two logic levels involved add —a vast improvement 28. operate multiplier Figure J.24 , load sum carry bits P zero perform first ALU operation. (If Booth recoding used, might asubtraction rather addition.) shift low-order sum bit P A, aswell shifting itself. n/C01 high-order bits P ’t need shifted next cycle sum bits fed next lower-order adder. Eachaddition step substantially increased speed, since add cell working independently others, carry propagated. two drawbacks carry-save adders. First, require hardware must copy register P hold carry outputsof adder. Second, last step, high-order word result mustbe fed ordinary adder combine sum carry parts. One way toaccomplish feeding output P adder used performthe addition operation. Multiplying carry-save adder sometimes calledredundant multiplication P represented using two registers. Since many ways represent P sum two registers, representation redundant. term carry-propagate adder (CPA) used denote adder CSA. propagate adder may propagate carries using ripples, carry-lookahead, method. Another way speed multiplication without using extra adders examine klow-order bits step, rather one bit. often called higher- radix multiplication . example, suppose k¼2. pair bits 00, add 0 P; 01, add B. 10, simply shift bone bit left adding P. Unfortunately, pair 11, appears would compute b+2b. avoided using higher-radix version Booth recoding. Imagine abase 4 number: digit 3 appears, change 1 add 1 next higher digit compensate. extra benefit using scheme like ordinaryBooth recoding, works negative well positive integers ( Section J.2 ).BAP Sum bitsCarry bits ciaici+1si biShift ++++++ + Figure J.24 Carry-save multiplier. circle represents (3,2) adder working indepen- dently. step, bit P needs shifted low-order sum bit.J-48 ■Appendix J Computer ArithmeticThe precise rules radix-4 Booth recoding given Figure J.25 . ith multiply step, two low-order bits register contain a2ianda2i+1. two bits, together bit shifted ( a2i/C01), used select multiple ofbthat must added P register. numerical example given Figure J.26 . Another name multiplication technique overlapping triplets, since looks 3 bits determine multiple bto use, whereas ordinary Booth recoding looks 2 bits. Besides complex control logic, overlapping triplets also requires P register 1 bit wider accommodate possibility 2 bor/C02bbeing added it. possible use radix-8 (or even higher) version Booth recod- ing. case, however, would necessary use multiple 3B poten-tial summand. Radix-8 multipliers normally compute 3B thebeginning multiplication operation. Low-order bits Last bit shifted 2i+1 2 2i/C01 Multiple 00 0 000 1 + b 01 0 + b 01 1 + 2 b 10 0 /C02b 10 1 /C0b 11 0 /C0b 11 1 0 Figure J.25 Multiples bto use radix-4 Booth recoding. example, two low-order bits register 1, last bit shifted register 0, correct multiple /C0b, obtained second-to-last row table. PA L 00000 1001 Multiply /C07¼1001 times /C05¼1011. B contains 1011. + 11011 Low-order bits 0, 1; L ¼0, add B. 11011 100111110 1110 0 Shift right two bits, shifting 1 left. + 01010 Low-order bits 1, 0; L ¼0, add /C02b. 01000 1110 000010 0011 1 Shift right two bits. Product 35 ¼0100011. Figure J.26 Multiplication 27 times 25 using radix-4 Booth recoding. column labeled L contains last bit shifted right end A.J.9 Speeding Integer Multiplication Division ■J-49Faster Multiplication Many Adders space many adders available, multiplication speed improved. Figure J.27 shows simple array multiplier multiplying two 5-bit numbers, using three CSAs one propagate adder. Part (a) block diagramof kind use throughout section. Parts (b) (c) show adder inmore detail. inputs adder shown (b); actual adders theirinterconnections shown (c). row adders (c) corresponds box (b) (c)b0 a1b0 a0 b0 b1 b2 b3 b4 b4 a1b4 a0 b0 a4 b0 b1 b2 Ab1 a4 p9p8p7p6p5p4p3p2p1p0(a)b4Ab3Ab2A b1A b0A CSA CSA CSA Propagate adder Figure J.27 array multiplier. 5-bit number multiplied b4b3b2b1b0. Part (a) shows block diagram, (b) shows inputs array, (c) expands arrayto show adders.J-50 ■Appendix J Computer Arithmetic(a). picture “twisted ”so bits significance column. actual implementation, array would likely laid square instead. array multiplier Figure J.27 performs number additions design Figure J.24 , latency dramatically different single carry-save adder. However, hardware Figure J.27 , multiplication pipelined, increasing total throughput. hand, although thislevel pipelining sometimes used array processors, used thesingle-chip, floating-point accelerators discussed Section J.10 . Pipelining dis- cussed general Appendix C Kogge [1981] context multipliers. Sometimes space budgeted chip arithmetic may hold array large enough multiply two double-precision numbers. case, populardesign use two-pass arrangement one shown Figure J.28 . first pass array “retires ”5 bits B. result first pass fed back top combined next three summands. Theresult second pass fed CPA. design, however, losesthe ability pipelined. arrays require many addition steps much cheaper arrangements Figures J.2 andJ.24, popular? First all, using array smaller latency using single adder —because array combinational circuit, signals flow directly without clocked. Although thetwo-pass adder Figure J.28 would normally still use clock, cycle time passing karrays less ktimes clock would needed designs like ones Figures J.2 orJ.24. Second, array amenable various schemes speedup. One shown Figure J.29 . idea design two adds proceed parallel or, put another way, stream passes half adders. Thus, runs almost twice CSA CPAb5A b2A b6A b3A b7A b4Ab1A b0A CSACSA Figure J.28 Multipass array multiplier. Multiplies two 8-bit numbers half hardware would used one-pass design like Figure J.27 . end second pass, bits flow CPA. inputs used first pass marked bold.J.9 Speeding Integer Multiplication Division ■J-51the speed multiplier Figure J.27 . even/odd multiplier popular VLSI regular structure. Arrays also speeded using asyn-chronous logic. One reasons multiplier Figure J.2 (page J-4) needs clock keep output adder feeding back input adder output fully stabilized. Thus, array Figure J.28 long enough signal propagate top bottom thetime takes first adder stabilize, may possible avoid clocks alto-gether. Williams et al. [1987] discussed design using idea, although dividers instead multipliers. techniques previous paragraph still multiply time 0( n), time reduced log nusing tree. simplest tree would combine pairs summands b 0A⋯bn/C01A, cutting number summands nton/2. n/2 numbers would added pairs again, reducing n/4, on, resulting single sum log nsteps. However, simple binary tree idea ’t map full (3,2) adders, reduce three inputs two rather reducing two inputs one. tree use full adders, known Wallace tree, shown Figure J.30 . computer arithmetic units built ofb2A b4A b3A b5Ab1A b0A CSA CSAb6A b7ACSA CSA CSA CPACSA Figure J.29 Even/odd array. first two adders work parallel. results fed third fourth adders, also work parallel, on.J-52 ■Appendix J Computer ArithmeticMSI parts, Wallace tree design choice high-speed multipliers. is, however, problem implementing VLSI. try fill inall adders paths Wallace tree Figure J.30 , discover nice, regular structure Figure J.27 . VLSI designers often chosen use log ndesigns binary tree mul- tiplier , discussed next. problem adding summands binary tree coming (2,1) adder combines two digits produces single-sum digit. ofcarries, ’t possible using binary notation, done representation. use signed-digit representation 1, 1, 0, used previously understand Booth ’s algorithm. representation two costs. First, takes 2 bits represent signed digit. Second, algo-rithm adding two signed-digit numbers iand biis complex requires examining aiai/C01ai/C02andbibi/C01bi/C02. Although means must look 2 bits back, binary addition might look arbitrary number bits backbecause carries. describe algorithm adding two signed-digit numbers follows. First, compute sum carry bits iandci+1using Figure J.31 . compute final sum si+ci. tables set final sum generate carry. Example sum signed-digit numbers 1 102and 001 2? Answer two low-order bits sum 0 + 1 ¼11, next pair sums 1+0¼01, high-order pair sums 1+0 ¼01, sum 1 1+0 10 + 0100 ¼1012.CSA CSA CSACSAb7Ab6Ab5A b4A b3A b2 b1 b0 CSA CSA Propagate adder Figure J.30 Wallace tree multiplier. example multiply tree computes product 0(log n) steps.J.9 Speeding Integer Multiplication Division ■J-53This, then, defines (2,1) adder. hand, use straightforward binary tree perform multiplication. first step adds b0A+b1A parallel b2A+b3A,…,bn/C02A+bn/C01A. next step adds results sums pairs, on. Although final sum must run carry-propagateadder convert signed-digit form two ’s complement, final add step necessary multiplier using CSAs. summarize, Wallace trees signed-digit trees log nmultipliers. Wallace tree uses fewer gates harder lay out. signed-digit tree hasa regular structure, requires 2 bits represent digit morecomplicated add logic. adders, possible combine different multiplytechniques. example, Booth recoding arrays combined. InFigure J.27 instead input b iA, could bibi/C01A. avoid compute multiple 3 b, use Booth recoding. Faster Division One Adder two techniques discussed speeding multiplication single adder carry-save adders higher-radix multiplication. However, isa difficulty trying utilize approaches speed nonrestoring divi-sion. adder Figure J.2(b) page J-4 replaced carry-save adder, P replaced two registers, one sum bits one carrybits (compare multiplier Figure J.24 ). end cycle, sign P uncertain (since P unevaluated sum two registers), yet sign P used compute quotient digit decide next ALU oper- ation. higher radix used, problem deciding value subtractfrom P. paper-and-pencil method, guess quotient digit. Inbinary division, two possibilities. able finesse prob-lem initially guessing one adjusting guess based sign P.This ’t work higher radices two possible quo- tient digits, rendering quotient selection potentially quite complicated: wouldhave compute multiples band compare P. carry-save technique higher-radix division made work use redundant quotient representation. Recall discussion SRTdivision (page J-45) allowing quotient digits /C01, 0, 1, often choice one pick. idea previous algorithm tochoose 0 whenever possible, meant ALU operation could be11 +1 001 +1 100 1x +0 11 011x +0 11 11+1 +0 10 00 f x/C210 y/C210 otherwiseifx/C210 y/C210 otherwise Figure J.31 Signed-digit addition table. leftmost sum shows comput- ing 1 + 1, sum bit 0 carry bit 1.J-54 ■Appendix J Computer Arithmeticskipped. carry-save division, idea that, remainder (which value (P,A) register pair) known exactly (being stored carry-save form), exact quotient digit also known. But, thanks redundant rep-resentation, remainder ’t known precisely order pick quotient digit. illustrated Figure J.32 , x-axis represents r i, remainder isteps. line labeled qi¼1 shows value ri+1would chose qi¼1, similarly lines qi¼0 qi¼/C01. choose value qi, long ri+1¼2ri/C0qibsatisfies jri+1j/C20b. allowable ranges shown right half Figure J.32 . shows ’t need know precise value riin order choose quotient digit qi. need know thatrlies interval small enough fit entirely within one overlapping bars shown right half Figure J.32 . basis using carry-save adders. Look high-order bits carry-save adder sum propagate adder. use approximationofr(together divisor, b) compute q i, usually means lookup table. technique works higher-radix division (whether carry-saveadder used). high-order bits P used index table gives one allowable quotient digits. design challenge building high-speed SRT divider figuring many bits P B need examined. example, suppose takea radix 4, use quotient digits 2, 1, 0, 1,2, propagate adder. many bits P B need examined? Deciding involves two steps. Forordinary radix-2 nonrestoring division, stage jrj/C20b, P buffer won’t overflow. But, radix 4, r i+1¼4ri/C0qibis computed stage, ri near b, 4 riwill near 4 b, even largest quotient digit bring r back range jri+1j/C20b. words, remainder might grow without bound. However, restricting jrij/C202b/3 makes easy check riwill stay bounded. figuring bound rimust satisfy, draw diagram Figure J.33 , analogous Figure J.32 . example, diagram showsb –b –bb –b 0qi = –1qi= 0 qi= 1 qi= –1 qi= 0 qi= 1 riri ri+1 = 2ri – qib Figure J.32 Quotient selection radix-2 division. x-axis represents ith remainder, quantity (P,A) register pair. y-axis shows value remainder one additional divide step. bar right-hand graph gives range rivalues permissible select associated value qi.J.9 Speeding Integer Multiplication Division ■J-55that riis (1/12) band (5/12) b, pick q¼1, on. Or, put another way, r/bis 1/12 5/12, pick q¼1. Suppose divider examines 5 bits P (including sign bit) 4 bits b(ignoring sign, since always nonnegative). interesting case high bits P 00011 xxx⋯, high bits bare 1001 xxx⋯. Imagine binary point left end register. Since truncated, r(the value P concatenated A) could value 0.0011 2to 0.0100 2,a n bcould value .1001 2to .1010 2. Thus, r/bcould small 0.0011 2/ .1010 2or large 0.0100 2/.1001 2, 0.0011 2/.1010 2¼3/10<1/3 would require quotient bit 1, 0.0100 2/.1001 2¼4/9>5/12 would require quo- tient bit 2. words, 5 bits P 4 bits baren’t enough pick quotient bit. turns 6 bits P 4 bits bare enough. verified writing simple program checks cases. output sucha program shown Figure J.34 . Example Using 8-bit registers, compute 149/5 using radix-4 SRT division. Answer Follow SRT algorithm page J-45, replace quotient selection rule step 2 one uses Figure J.34 . See Figure J.35 . Pentium uses radix-4 SRT division algorithm like one presented, except uses carry-save adder. Exercises J.34(c) J.35 explore detail. Although simple cases, SRT analyses proceed way. First compute range r i, plot riagainst ri+1to find quotient ranges, finally write program compute many bits necessary.(It sometimes also possible compute required number bits analytically.)Various details need considered building practical SRT divider.2b 3–2b 3 2b 35b 12b 3b 6b 120qi = –2 qi = –1 qi = 1 qi = 0 qi = 2 ri ri+1 = 4ri – qib qi = 2qi = 1 riqi = 0 qi = –2qi = –1–2b 3ri+1 Figure J.33 Quotient selection radix-4 division quotient digits 22,21, 0, 1, 2.J-56 ■Appendix J Computer ArithmeticFor example, quotient lookup table fairly regular structure, means usually cheaper encode PLA rather ROM. details SRT division, see Burgess Williams [1995] . J.10 Putting Together section, compare Weitek 3364, MIPS R3010, Texas Instruments 8847 (see Figures J.36 andJ.37). many ways, ideal chips compare. implement IEEE standard addition, subtraction,b Range P qb Range P q 8 /C012 /C07 /C021 2 /C018 /C010 /C02 8 /C06 /C03 /C011 2 /C010 /C04 /C01 8 /C021 0 1 2 /C043 0 8 2 5 1 12 3 9 18 6 11 2 12 9 17 29 /C014 /C08 /C021 3 /C019 /C011 /C02 9 /C07 /C03 /C011 3 /C010 /C04 /C01 9 /C032 0 1 3 /C043 0 9 2 6 1 13 3 9 19 7 1 32 1 31 01 8210 /C015 /C09 /C021 4 /C020 /C011 /C02 10 /C08 /C03 /C011 4 /C011 /C04 /C01 10 /C032 0 1 4 /C043 0 10 2 7 1 14 3 10 110 8 14 2 14 10 19 211 /C016 /C09 /C021 5 /C022 /C012 /C02 11 /C09 /C03 /C011 5 /C012 /C04 /C01 11 /C032 0 1 5 /C054 0 11 2 8 1 15 3 11 1 11 8 15 2 15 11 21 2 Figure J.34 Quotient digits radix-4 SRT division propagate adder. top row says high-order 4 bits bare 1000 2¼8, top 6 bits P 110100 2¼/C012 111001 2¼/C07, /C02 valid quotient digit.J.10 Putting Together ■J-57PA 000000000 10010101 Divide 149 5. B contains 00000101. 000010010 10100000 Step 1: B 5 leading 0s, shift left 5. B contains 10100000, use b¼10 section table. Step 2.1: Top 6 bits P 2, shift left 2. table, pick qto 0 1. Choose q0¼0.001001010 100000 0 Step 2.2: Top 6 bits P 9, shift left 2. q1¼2. 100101010 0000 02 + 011000000 Subtract 2 b. 111101010 0000 02 Step 2.3: Top bits ¼/C03, shift left 2. pick 0 /C01 forq, pick q2¼0.110101000 00 020 Step 2.4: Top bits ¼/C011, shift left 2. q3¼/C02. 010100000 0202 + 101000000 Add 2 b. 111100000 Step 3: Remainder negative, restore adding b subtract 1 q.+ 010100000 010000000 Answer: q¼0202/C01¼29 get remainder, undo shift step 1 remainder ¼010000000 >>5¼4. Figure J.35 Example radix-4 SRT division. Division 149 5. Features MIPS R3010 Weitek 3364 TI 8847 Clock cycle time (ns) 40 50 30 Size (mil2) 114,857 147,600 156,180 Transistors 75,000 165,000 180,000Pins 84 168 207Power (watts) 3.5 1.5 1.5Cycles/add 2 2 2Cycles/mult 5 2 3Cycles/divide 19 17 11Cycles/square root /C0 30 14 Figure J.36 Summary three floating-point chips discussed section. cycle times production parts available June 1989. cycle counts double-precision operations.J-58 ■Appendix J Computer ArithmeticFigure J.37 Chip layout TI 8847, MIPS R3010, Weitek 3364. left-hand columns photo- micrographs; right-hand columns show corresponding floor plans. (Continued)J.10 Putting Together ■J-59multiplication, division single chip. introduced 1988 run cycle time 40 nanoseconds. However, see, use quite different algorithms. Weitek chip well described Birman et al. [1990] , MIPS chip described less detail Rowen, Johnson, Ries [1988] , details TI chip found Darley et al. [1989] . three chips number things common. perform addition multiplication parallel, implement neither extended precision remainder step operation. (Recall Section J.6 easy implement IEEE remainder function software remainder step instruction available.) designers chips probably decided provide extended precision influential users run portable codes, ’t rely extended precision. However, seen, extended precision make faster simpler math libraries. summary three chips given Figure J.36 , note higher tran- sistor count generally leads smaller cycle counts. Comparing cycles/op num- bers needs done carefully, figures MIPS chip complete system (R3000/3010 pair), Weitek TI numbers stand-alone chips usually larger used complete system. MIPS chip fewest transistors three. reflected fact chip three pipelining hardware Figure J.37 (Continued)J-60 ■Appendix J Computer Arithmeticsquare root. Further, multiplication addition operations completely independent share carry-propagate adder performs final rounding (as well rounding logic). Addition R3010 uses mixture ripple, CLA, carry-select. carry-select adder used fashion Figure J.20 (page J-43). Within half, carries propagated using hybrid ripple-CLA scheme type indicatedinFigure J.19 (page J-42). However, tuned varying size block, rather fixed 4 bits (as Figure J.19 ). multiplier midway designs Figures J.2 (page J-4) J.27 (page J-50). array large enough output fed back input without clocked. Also, uses radix-4 Booth recodingand even/odd technique Figure J.29 (page J-52). R3010 divide multiply parallel (like Weitek chip unlike TI chip). divider isa radix-4 SRT method quotient digits /C02,/C01, 0, 1, 2, similar described Taylor [1985] . Double-precision division four times slower multiplication. R3010 shows chips using 0( n) multiplier, SRT divider operate fast enough keep reasonable ratio multiply divide. Weitek 3364 independent add, multiply, divide units. also uses radix-4 SRT division. However, add multiply operations Weitekchip pipelined. three addition stages (1) exponent compare, (2) addfollowed shift (or vice versa ), (3) final rounding. Stages (1) (3) take half-cycle, allowing whole operation done two cycles, eventhough three pipeline stages. multiplier uses array styleofFigure J.28 uses radix-8 Booth recoding, means must compute 3 times multiplier. three multiplier pipeline stages (1) compute 3 b, (2) pass array, (3) final carry-propagation add round. Single pre-cision passes array once, double precision twice. Like addition, thelatency two cycles. Weitek chip uses interesting addition algorithm. variant carry-skip adder pictured Figure J.18 (page J-42). However, P ij, log- ical many terms, computed rippling, performing one per ripple. Thus, carries propagate left within block, value Pijis prop- agating right within next block, block sizes chosen waves complete time. Unlike MIPS chip, 3364 hardware square root,which shares divide hardware. ratio double-precision multiply divide is2:17. large disparity multiply divide due fact multi-plication uses radix-8 Booth recoding, division uses radix-4 method. theMIPS R3010, multiplication division use radix. notable feature TI 8847 division iteration (using Goldschmidt algorithm discussed Section J.6 ). improves speed divi- sion (the ratio multiply divide 3:11), means multiplication divi- sion cannot done parallel two chips. Addition two-stagepipeline. Exponent compare, fraction shift, fraction addition done thefirst stage, normalization rounding second stage. Multiplication usesJ.10 Putting Together ■J-61a binary tree signed-digit adders three-stage pipeline. first stage passes array, retiring half bits; second stage passes array second time; third stage converts signed-digit form two ’s complement. Since one array, new multiply operation beinitiated every cycle. However, slowing clock, two passesthrough array made single cycle. case, new multiplicationcan initiated cycle. 8847 adder uses carry-select algorithm ratherthan carry-lookahead. mentioned Section J.6 , TI carries 60 bits pre- cision order correctly rounded division. three chips illustrate different trade-offs made designers sim- ilar constraints. One interesting things chips diversityof algorithms. uses different add algorithm, well different mul-tiply algorithm. fact, Booth recoding technique universallyused chips. J.11 Fallacies Pitfalls Fallacy Underflows rarely occur actual floating-point application code Although codes rarely underflow, actual codes underflow fre- quently. SDRWAVE [ Kahaner 1988 ], solves one-dimensional wave equa- tion, one example. program underflows quite frequently, even whenfunctioning properly. Measurements one machine show adding hardware support gradual underflow would cause SDRWAVE run 50% faster. Fallacy Conversions integer floating point rare fact, spice frequent divides. assumption conversions rare leads mistake SPARC version 8 instruction set, notprovide instruction move integer registers floating-point registers. Pitfall Don’t increase speed floating-point unit without increasing memory bandwidth typical use floating-point unit add two vectors produce third vector. vectors consist double-precision numbers, floating-pointadd use three operands 64 bits each, 24 bytes memory. memorybandwidth requirements even greater floating-point unit performaddition multiplication parallel (as do). Pitfall /C0x 0 /C0x fine point IEEE standard tripped designers. floating-point numbers use sign magnitude system, two zeros, +0 /C00. standard says 0 /C00¼+0, whereas /C0(0)¼/C00. Thus, /C0xis 0 /C0xwhen x¼0.J-62 ■Appendix J Computer ArithmeticJ.12 Historical Perspective References earliest computers used fixed point rather floating point. “Preliminary Discussion Logical Design Electronic Computing Instrument, ”Burks, Goldstine, von Neumann [1946] put like this: appear two major purposes “floating ”decimal point system arise fact number digits word constant fixed bydesign considerations particular machine. first purposes toretain sum product many significant digits possible second ofthese free human operator burden estimating inserting ap r b l e “scale factors ”—multiplicative constants serve keep numbers within limits machine. is, course, denying fact human time consumed arrang- ing introduction suitable scale factors. argue time soconsumed small percentage total time spend preparingan interesting problem machine. first advantage floating pointis, feel, somewhat illusory. order floating point, one mustwaste memory capacity could otherwise used carrying digits per word. would therefore seem us clear whether modest advan- tages floating binary point offset loss memory capacity theincreased complexity arithmetic control circuits. enables us see things perspective early computer designers, believed saving computer time memory important saving programmer time. original papers introducing Wallace tree, Booth recoding, SRT divi- sion, overlapped triplets, reprinted Swartzlander [1990] . good explanation early machine (the IBM 360/91) used pipelined Wallacetree, Booth recoding, iterative division Anderson et al. [1967] . discus- sion average time single-bit SRT division Freiman [1961] ; one interesting historical papers appear Swartzlander. standard book Mead Conway [1980] discouraged use CLAs cost effective VLSI. important paper Brent Kung [1982] helped combat view. example detailed layout CLAs found inNgai Irwin [1985] Weste Eshraghian [1993] , theoretical treatment given Leighton [1992] .Takagi, Yasuura, Yajima [1985] pro- vide detailed description signed-digit tree multiplier. ascendancy IEEE arithmetic, many different floating-point for- mats use. Three important ones used IBM 370, DEC VAX, Cray. brief summary older formats. VAX format closest IEEE standard. single-precision format (F format) like IEEEsingle precision hidden bit, 8 bits exponent, 23 bits fraction.However, sticky bit, causes round halfway cases upinstead even. VAX slightly different exponent range IEEEJ.12 Historical Perspective References ■J-63single: Eminis/C0128 rather /C0126 IEEE, Emaxis 126 instead 127. main differences VAX IEEE lack special values gradual underflow. VAX reserved operand, works like signalingNaN: traps whenever referenced. Originally, VAX ’s double precision (D format) also 8 bits exponent. However, small manyapplications, G format added; like IEEE standard, format 11 bitsof exponent. VAX also H format, 128 bits long. IBM 370 floating-point format uses base 16 rather base 2. means cannot use hidden bit. single precision, 7 bits exponent 24 bits (6 hex digits) fraction. Thus, largest representable number 16 27¼24/C227¼229, compared 228for IEEE. However, number normalized hexadec- imal sense needs nonzero leading digit. interpreted binary, thethree most-significant bits could zero. Thus, potentially fewer 24 bitsof significance. reason using higher base minimize amount ofshifting required adding floating-point numbers. However, less signifi-cant current machines, floating-point add time usually fixed indepen-dently operands. Another difference 370 arithmetic IEEE arithmetic 370 neither round digit sticky digit, effectively means truncates rather rounds. Thus, many computations, result willsystematically small. Unlike VAX IEEE arithmetic, every bit pattern isa valid number. Thus, library routines must establish conventions return incase errors. IBM FORTRAN library, example,ﬃﬃﬃﬃﬃﬃ ﬃ /C04p returns 2! Arithmetic Cray computers interesting driven motiva- tion highest possible floating-point performance. 15-bit exponentfield 48-bit fraction field. Addition Cray computers guard digit, multiplication even less accurate addition. Thinking multipli- cation sum pnumbers, 2 pbits long, Cray computers drop low-order bits summand. Thus, analyzing exact error characteristics mul-tiply operation easy. Reciprocals computed using iteration, divisionofabybis done multiplying atimes 1/ b. errors multiplication recip- rocation combine make last three bits divide operation unreliable. Atleast Cray computers serve keep numerical analysts toes! IEEE standardization process began 1977, inspired mainly W. Kahan based partly Kahan ’s work IBM 7094 University Toronto [Kahan 1968 ]. standardization process lengthy affair, gradual underflow causing controversy. (According Cleve Moler, visitors tothe United States advised sights missed Las Vegas,the Grand Canyon, IEEE standards committee meeting.) standardwas finally approved 1985. Intel 8087 first major commercial IEEEimplementation appeared 1981, standard finalized. con-tains features eliminated final standard, projective bits. According Kahan, length double-extended precision based could implemented 8087. Although IEEE standard based onany existing floating-point system, features present othersystem. example, CDC 6600 reserved special bit patterns INDEFINITEJ-64 ■Appendix J Computer Arithmeticand INFINITY, idea denormal numbers appears Goldberg [1967] well Kahan [1968] . Kahan awarded 1989 Turing prize recognition work floating point. Although floating point rarely attracts interest general press, news- papers filled stories floating-point division November 1994. Abug division algorithm used Intel ’s Pentium chips come light. discovered Thomas Nicely, math professor Lynchburg Collegein Virginia. Nicely found bug calculations involving reciprocals ofprime numbers. News Nicely ’s discovery first appeared press front page November 7 issue Electronic Engineering Times. Intel’s immediate response stonewall, asserting bug would affect theoreticalmathematicians. Intel told press, “This ’t even qualify errata …even ’re engineer, ’re going see this. ” pressure, Intel issued white paper, dated November 30, explain- ing ’t think bug significant. One arguments based fact pick two floating-point numbers random divideone other, chance resulting quotient error 1 9 billion. However, Intel neglected explain thought typical customer accessed floating-point numbers randomly. Pressure continued mount Intel. One sore point Intel known bug Nicely discovered it, decided make public.Finally, December 20, Intel announced would unconditionally replaceany Pentium chip used faulty algorithm would take unspe-cified charge earnings, turned $300 million. Pentium uses simple version SRT division discussed Section J.9 . bug introduced converted quotient lookup table PLA. Evidently elements table containing quotient digit 2 thatIntel thought would never accessed, optimized PLA design usingthis assumption. resulting PLA returned 0 rather 2 situations.However, entries really accessed, caused division bug.Even though effect faulty PLA cause 5 2048 table entriesto wrong, Pentium computes incorrect quotient 1 9 billiontimes random inputs. explored Exercise J.34. References Anderson, S.F., Earle, J.G., Goldschmidt, R.E., Powers, D.M., 1967. IBM System/360 Model 91: Floating-point execution unit. IBM J. Research Development 11, 34 –53. Reprinted Swartzlander [1990]. Good description early high-performance floating-point unit used pipelined Wallace tree multiplier iterative division. Bell, C.G., Newell, A., 1971. Computer Structures: Readings Examples. McGraw-Hill, New York.Birman, M., Samuels, A., Chu, G., Chuk, T., Hu, L., McLeod, J., Barnes, J., 1990. Developing WRL3170/3171 SPARC floating-point coprocessors. IEEE Micro 10 (1), 55 –64.These chips floating-point core Weitek 3364, paper fairly detailed description floating-point design.J.12 Historical Perspective References ■J-65Brent, R.P., Kung, H.T., 1982. regular layout parallel adders. IEEE Trans. Computers C-31, 260 –264. paper popularized CLAs VLSI. Burgess, N., Williams, T., 1995. Choices operand truncation SRT division algorithm. IEEE Trans. Computers 44, 7. Analyzes many bits divisor remainder need examined SRT division. Burks, A.W., Goldstine, H.H., von Neumann, J., 1946. Preliminary discussion logical design electronic computing instrument. In: Aspray, W., Burks, A. (Eds.), Papers John von Neumann. Report U.S. Army Ordnance Department, p. 1; also appears. MIT Press, Cambridge, Mass, pp. 97 –146. Tomash Publishers, Los Angeles, 1987. Cody, W.J., Coonen, J.T., Gay, D.M., Hanson, K., Hough, D., Kahan, W., Karpinski, R., Palmer, J., Ris, F.N., Stevenson, D., 1984. proposed radix- word-length-independent standard floating-point arithmetic. IEEE Micro 4 (4), 86 –100. Contains draft 854 standard, general 754. significance article contains commentary stan-dard, equally relevant 754. However, aware differencesbetween draft final standard. Coonen, J., 1984. Contributions proposed standard binary floating point arithmetic. Ph.D. thesis. University California –Berkeley. detailed discussion rounding modes used implement efficient binary decimal conversion. Darley, H.M., et al., 1989. Floating point/integer processor divide square root functions. U.S. Patent. 4 (878,190) October 31, 1989. Pretty readable patents go. Gives high-level view TI 8847 chip, ’t details division algorithm. Demmel, J.W., Li, X., 1994. Faster numerical algorithms via exception handling. IEEE Trans. Computers 43 (8), 983 –992. good discussion features unique IEEE floating point improve performance important software library. Freiman, C.V., 1961. Statistical analysis certain binary division algorithms. Proc. IRE 49 (1), 91 –103. Contains analysis performance shifting-over-zeros SRT division algorithm. Goldberg, D., 1991. every computer scientist know floating-point arithmetic. Computing Surveys 23 (1), 5 –48.Contains in-depth tutorial IEEE standard soft- ware point view. Goldberg, I.B., 1967. 27 bits enough 8-digit accuracy. Comm. ACM 10 (2), 105 –106. paper proposes using hidden bits gradual underflow. Gosling, J.B., 1980. Design Arithmetic Units Digital Computers. Springer-Verlag, New York. concise, well-written book, although focuses MSI designs. Hamacher, V.C., Vranesic, Z.G., Zaky, S.G., 1984. Computer Organization, 2nd ed. McGraw-Hill, New York. Introductory computer architecture book good chapter computer arithmetic. Hwang, K., 1979. Computer Arithmetic: Principles, Architecture, Design. Wiley, New York. book contains widest range topics computer arithmetic books. IEEE, 1985. IEEE standard binary floating-point arithmetic. SIGPLAN Notices 22 (2), 9 –25.IEEE 754 reprinted here. Kahan, W., 1968. 7094-II system support numerical analysis. SHARE Secretarial Distribution. SSD- 159. system many features incorporated IEEE floating-point standard. Kahaner, D.K., 1988. Benchmarks ‘real’programs. SIAM News.(November).. benchmark pre- sented article turns cause many underflows. Knuth, D., 1981. 2nd ed. Art Computer Programming.Vol. II. Addison-Wesley, Reading, Mass. section distribution floating-point numbers. Kogge, P., 1981. Architecture Pipelined Computers. McGraw-Hill, New York. brief dis- cussion pipelined multipliers. Kohn, L., Fu, S.-W., 1989. 1,000,000 transistor microprocessor. In: IEEE Int ’l. Solid-State Circuits Conf. Digest Technical Papers, pp. 54 –55.There several articles i860, one contains details floating-point algorithms. Koren, I., 1989. Computer Arithmetic Algorithms. Prentice Hall, Englewood Cliffs, N.J..Leighton, F.T., 1992. Introduction Parallel Algorithms Architectures: Arrays. Trees, Hypercubes, Morgan Kaufmann, San Francisco. excellent book, emphasis complexity anal- ysis algorithms. Section 1.2.1 nice discussion carry-lookahead addition tree.J-66 ■Appendix J Computer ArithmeticMagenheimer, D.J., Peters, L., Pettis, K.W., Zuras, D., 1988. Integer multiplication division HP Precision architecture. IEEE Trans. Computers 37 (8), 980 –990. Gives rationale integer- divide-step instructions Precision architecture. Markstein, P.W., 1990. Computation elementary functions IBM RISC System/6000 processor. IBM J. Research Development 34 (1), 111 –119. Explains use fused muliply-add compute correctly rounded division square root. Mead, C., Conway, L., 1980. Introduction VLSI Systems. Addison-Wesley, Reading, Mass. Montoye, R.K., Hokenek, E., Runyon, S.L., 1990. Design IBM RISC System/6000 floating-point execution. IBM J. Research Development 34 (1), 59 –70.Describes one implementation fused multiply-add. Ngai, T.-F., Irwin, M.J., 1985. Regular, area-time efficient carry-lookahead adders. In: Proc. Seventh IEEE Symposium Computer Arithmetic, pp. 9 –15.Describes CLA like Figure J.17, bits flow come back down. Patterson, D.A., Hennessy, J.L., 2009. Computer Organization Design: Hardware/Software Interface, 4th Edition Morgan Kaufmann, San Francisco. Chapter 3 gentler introduction first third appendix. Peng, V., Samudrala, S., Gavrielov, M., 1987. implementation shifters, multipliers, dividers VLSI floating point units. In: Proc. Eighth IEEE Symposium Computer Arithmetic, pp. 95 –102. Highly recommended survey different techniques actually used VLSI designs. Rowen, C., Johnson, M., Ries, P., 1988. MIPS R3010 floating-point coprocessor. IEEE Micro 53–62 (June). Santoro, M.R., Bewick, G., Horowitz, M.A., 1989. Rounding algorithms IEEE multipliers. In: Proc. Ninth IEEE Symposium Computer Arithmetic, pp. 176 –183. readable discussion efficiently implement rounding floating-point multiplication. Scott, N.R., 1985. Computer Number Systems Arithmetic. Prentice Hall, Englewood Cliffs, N.J.Swartzlander, E. (Ed.), 1990. Computer Arithmetic. IEEE Computer Society Press, Los Alamitos, Calif. collection historical papers two volumes. Takagi, N., Yasuura, H., Yajima, S., 1985. High-speed VLSI multiplication algorithm redundant binary addition tree. IEEE Trans. Computers C-34 (9), 789 –796. discussion binary tree signed multiplier basis design used TI 8847. Taylor, G.S., 1981. Compatible hardware division square root. In: Proc. Fifth IEEE Symposium Computer Arithmetic, May 18 –19, 1981. Ann Arbor, Mich, pp. 127 –134. Good discussion radix-4 SRT division algorithm. Taylor, G.S., 1985. Radix 16 SRT dividers overlapped quotient selection stages. In: Proc. Seventh IEEE Symposium Computer Arithmetic, June 4 –6, 1985, pp. 64 –71 Urbana, Ill.. Describes sophisticated high-radix division algorithm. Weste, N., Eshraghian, K., 1993. Principles CMOS VLSI Design: Systems Perspective, 2nd ed. Addison-Wesley, Reading, Mass. textbook section layouts various kinds adders. Williams, T.E., Horowitz, M., Alverson, R.L., Yang, T.S., 1987. self-timed chip division. In: Advanced Research VLSI, Proc. 1987 Stanford Conf. MIT Press, Cambridge, Mass. Describes divider tries get speed combinational design without using area would required one. Exercises J.1 [12]<J.2>Using nbits, largest smallest integer repre- sented two ’s complement system? J.2 [20/25] <J.2>In subsection “Signed Numbers ”(page J-7), stated two’s complement overflows carry high-order bit position dif- ferent carry-out position.Exercises ■J-67a.[20]<J.2>Give examples pairs integers four combinations carry-in carry-out. Verify rule stated above. b.[25]<J.2>Explain rule always true. J.3 [12]<J.2>Using 4-bit binary numbers, multiply /C08/C2/C08 using Booth recoding. J.4 [15]<J.2>Equations J.2.1 J.2.2 adding two n-bit numbers. Derive similar equations subtraction, borrow insteadof carry. J.5 [25]<J.2>On machine ’t detect integer overflow hardware, show would detect overflow signed addition operation software. J.6 [15/15/20] <J.3>Represent following numbers single-precision double-precision IEEE floating-point numbers: a.[15]<J.3>10. b.[15]<J.3>10.5. c.[20]<J.3>0.1. J.7 [12/12/12/12/12] <J.3>Below list floating-point numbers. single preci- sion, write number binary, decimal, give representation inIEEE arithmetic. a.[12]<J.3>The largest number less 1. b.[12]<J.3>The largest number. c.[12]<J.3>The smallest positive normalized number. d.[12]<J.3>The largest denormal number. e. [12]<J.3>The smallest positive number. J.8 [15]<J.3>Is ordering nonnegative floating-point numbers inte- gers denormalized numbers also considered? J.9 [20]<J.3>Write program prints bit patterns used represent floating-point numbers favorite computer. bit pattern usedfor NaN? J.10 [15]<J.4>Using p¼4, show binary floating-point multiply algorithm computes product 1.875 /C21.875. J.11 [12/10] <J.4>Concerning addition exponents floating-point multiply: a.[12]<J.4>What would hardware implements addition expo- nents look like? b.[10]<J.4>If bias single precision 129 instead 127, would addi- tion harder easier implement? J.12 [15/12] <J.4>In discussion overflow detection floating-point multipli- cation, stated (for single precision) detect overflowed expo-nent performing exponent addition 9-bit adder.J-68 ■Appendix J Computer Arithmetica.[15]<J.4>Give exact rule detecting overflow. b.[12]<J.4>Would overflow detection easier used 10-bit adder instead? J.13 [15/10] <J.4>Floating-point multiplication: a.[15]<J.4>Construct two single-precision floating-point numbers whose prod- uct ’t overflow final rounding step. b.[10]<J.4>Is rounding mode phenomenon cannot occur? J.14 [15]<J.4>Give example product denormal operand normal- ized output. large final shifting step? maximum possibleshift occur inputs double-precision numbers? J.15 [15]<J.5>Use floating-point addition algorithm page J-23 compute 1.010 2/C0.1001 2(in 4-bit precision). J.16 [10/15/20/20/20] <J.5>In certain situations, sure a+bis exactly representable floating-point number, is, rounding necessary. a.[10]<J.5>Ifa,bhave exponent different signs, explain +bis exact. used subsection “Speeding Addition ”on page J-25. b.[15]<J.5>Give example exponents differ 1, aandbhave dif- ferent signs, a+bis exact. c.[20]<J.5>Ifa/C21b/C210, top two bits acancel computing a/C0b, explain result exact (this fact mentioned page J-22). d.[20]<J.5>Ifa/C21b/C210, exponents differ 1, show a/C0bis exact unless high order bit a/C0bis position a(mentioned in“Speeding Addition, ”page J-25). e.[20]<J.5>If result a/C0bora+bis denormal, show result exact (mentioned subsection “Underflow, ”on page J-36). J.17 [15/20] <J.5>Fast floating-point addition (using parallel adders) p¼5. a.[15]<J.5>Step fast addition algorithm a+b,where a¼1.0111 2andb¼.11011 2. b.[20]<J.5>Suppose rounding mode toward+ ∞. complication arises example adder assumes carry-out? Suggesta solution. J.18 [12]<J.4, J.5 >How would use two parallel adders avoid final round-up addition floating-point multiplication? J.19 [30/10] <J.5>This problem presents way reduce number addition steps floating-point addition three two using single adder. a.[30]<J.5>LetAandBbe integers opposite signs, aandbtheir mag- nitudes. Show following rules manipulating unsigned numbers andbgives A+B.Exercises ■J-691.Complement one operands. 2.Use end-around carry add complemented operand (uncomplemented) one. 3.If carry-out, sign result sign associated uncomplemented operand. 4.Otherwise, carry-out, complement result, give sign complemented operand. b.[10]<J.5>Use show steps 2 4 floating-point addi- tion algorithm page J-23 performed using single addition. J.20 [20/15/20/15/20/15] <J.6>Iterative square root. a.[20]<J.6>Use Newton ’s method derive iterative algorithm square root. formula involve division. b.[15]<J.6>What fastest way think divide floating-point number 2? c.[20]<J.6>If division slow, iterative square root routine also slow. Use Newton ’s method f(x)¼1/x2/C0ato derive method ’t use divisions. d.[15]<J.6>Assume ratio division 2 : floating-point add : floating- point multiply 1:2:4. ratios multiplication time divide time makeseach iteration step method part (c) faster iteration themethod part (a)? e.[20]<J.6>When using method part (a), many bits need initial guess order get double-precision accuracy three iterations?(You may ignore rounding error.) f.[15]<J.6>Suppose spice runs TI 8847, spends 16.7% time square root routine (this percentage measured othermachines). Using values Figure J.36 assuming three iterations, much slower would spice run square root implemented software using method part(a)? J.21 [10/20/15/15/15] <J.6>Correctly rounded iterative division. Let aand bbe floating-point numbers p-bit significands ( p¼53 double precision). Let qbe exact quotient q¼a/b,1/C20q<2. Suppose qis result iteration process, qhas extra bits precision, 0 <q/C0q<2/C0p. following, important q<q, even qcan exactly represented floating-point number. a.[10]<J.6>Ifxis floating-point number, 1 /C20x<2, next rep- resentable number x? b.[20]<J.6>Show compute q0from q, q0hasp+1 bits precision andjq/C0q0j<2/C0p. c.[15]<J.6>Assuming round nearest, show correctly rounded quo- tient either q0,q0/C02/C0p,o rq0+2/C0p.J-70 ■Appendix J Computer Arithmeticd.[15]<J.6>Give rules computing correctly rounded quotient q0 based low-order bit q0and sign a/C0bq0. e.[15]<J.6>Solve part (c) three rounding modes. J.22 [15]<J.6>Verify formula page J-30. ( Hint:I f xn¼x02/C0x0b ðÞ /C2 Πi¼1,n1+ 1 /C0x0b ðÞ2ihi , 2 /C0xnb¼2/C0x0b2/C0x0b ðÞ Π1+ 1 /C0x0b ðÞ2ihi ¼ 2/C01/C01/C0x0b ðÞ2hi Π1+ 1 /C0x0b ðÞ2ihi .) J.23 [15]<J.7>Our example showed double rounding give different answer rounding used round-to-even rule. halfway cases arealways rounded up, double rounding still dangerous? J.24 [10/10/20/20] <J.7>Some cases italicized statement “Preci- sions ”subsection (page J-33) ’t hard demonstrate. a.[10]<J.7>What form must binary number rounding qbits followed rounding pbits gives different answer rounding directly pbits? b.[10]<J.7>Show multiplication p-bit numbers, rounding qbits followed rounding pbits rounding immediately pbits ifq/C212p. c.[20]<J.7>Ifaandbarep-bit numbers sign, show rounding a+btoqbits followed rounding pbits rounding immedi- ately pbits q/C212p+1. d.[20]<J.7>Do part (c) aandbhave opposite signs. J.25 [Discussion] <J.7>In MIPS approach exception handling, need test determining whether two floating-point operands could cause exception. fast also many false positives. comeup practical test? performance cost design depend onthe distribution floating-point numbers. discussed Knuth [1981] Hamming paper Swartzlander [1990] . J.26 [12/12/10] <J.8>Carry-skip adders. a.[12]<J.8>Assuming time proportional logic levels, long take n-bit adder divided (fixed) blocks length kbits perform addition? b.[12]<J.8>What value kgives fastest adder? c.[10]<J.8>Explain carry-skip adder takes time 0ﬃﬃﬃnpðÞ . J.27 [10/15/20] <J.8>Complete details block diagrams following adders. a.[10]<J.8>InFigure J.15 , show implement “1”and“2”boxes terms ANDand ORgates. b.[15]<J.8>InFigure J.19 , signals need flow adder cells top row “C”cells? Write logic equations “C”box. c.[20]<J.8>Show extend block diagram J.17 produce carry-out bit c8.Exercises ■J-71J.28 [15]<J.9>For ordinary Booth recoding, multiple bused ith step simply ai/C01/C0ai. find similar formula radix-4 Booth recoding (overlapped triplets)? J.29 [20]<J.9>Expand Figure J.29 fashion J.27, showing individual adders. J.30 [25]<J.9>Write analog Figure J.25 radix-8 Booth recoding. J.31 [18]<J.9>Suppose an/C01…a1a0andbn/C01…b1b0are added signed-digit adder illustrated example page J-53. Write formula ith bit sum, si, terms ai,ai/C01,ai/C02,bi,bi/C01,a n bi/C02. J.32 [15]<J.9>The text discussed radix-4 SRT division quotient digits /C02, /C01, 0, 1, 2. Suppose 3 /C03 also allowed quotient digits. relation replaces jrij/C202b/3? J.33 [25/20/30] <J.9>Concerning SRT division table, Figure J.34 : a.[25]<J.9>Write program generate results Figure J.34 . b.[20]<J.9>Note Figure J.34 certain symmetry respect pos- itive negative values P. find way exploit symmetry andonly store values positive P? c.[30]<J.9>Suppose carry-save adder used instead propagate adder. input quotient lookup table kbits divisor lbits remainder, remainder bits computed summing top lbits sum carry registers. kandl? Write program generate analog Figure J.34 . J.34 [12/12/12] <J.9, J.12 >The first several million Pentium chips produced flaw caused division sometimes return wrong result. Pentium usesa radix-4 SRT algorithm similar one illustrated example page J-56(but remainder stored carry-save format; see Exercise J.33(c)). Accord-ing Intel, bug due five incorrect entries quotient lookup table. a.[12]<J.9, J.12 >The bad entries quotient plus minus 2, instead quotient 0. redundancy, ’s conceivable algorithm could “recover ”from bad quotient digit later iterations. Show possible Pentium flaw. b.[12]<J.9, J.12 >Since operation floating-point divide rather integer divide, SRT division algorithm page J-45 must modified intwo ways. First, step 1 longer needed, since divisor already normal-ized. Second, first remainder may satisfy proper bound (jrj/C202b/3 Pentium; see page J-55). Show skipping first left shift step 2(a) SRT algorithm solve problem. c.[12]<J.9, J.12 >If faulty table entries indexed remainder could occur first divide step (when remainder divisor), ran-dom testing would quickly reveal bug. ’t happen. tell remainder values index faulty entries?J-72 ■Appendix J Computer ArithmeticJ.35 [12]<J.6, J.9 >The discussion remainder-step instruction assumed division done using bit-at-a-time algorithm. would change division implemented using higher-radix method? J.36 [25]<J.9>In array Figure J.28 , fact array pipelined exploited. come design feeds output bottom CSA bottom CSAs instead top one, run faster thearrangement Figure J.28 ?Exercises ■J-73K.1 Introduction K-2 K.2 Survey RISC Architectures Desktop, Server, Embedded Computers K-3 K.3 Intel 80x86 K-30 K.4 VAX Architecture K-50 K.5 IBM 360/370 Architecture Mainframe Computers K-69 K.6 Historical Perspective References K-75K Survey Instruction Set Architectures RISC: computer announced 1985. Steven Przybylski Designer Stanford MIPSK.1 Introduction appendix covers 10 instruction set architectures, remain vital part industry retired greener pastures. keepthem part show changes fashion instruction set architectureover time. start eight RISC architectures, using RISC V basis compar- ison. billions dollars computers shipped year ARM (includ- ing Thumb-2), MIPS (including microMIPS), Power, SPARC. ARM dominates PMD (including smart phones tablets) theembedded markets. 80x86 remains highest dollar-volume ISA, dominating desktop much server market. 80x86 get traction either embed-ded PMD markets, started lose ground server market. hasbeen extended ISA book, plans stopit soon. made transition 64-bit addressing, expect architecture around, although may play smaller role future past 30 years. VAX typifies ISA emphasis code size offering higher level machine language hopes better match programminglanguages. architects clearly expected implemented large amountsof microcode, made single chip pipelined implementations chal-lenging. successor Alpha, RISC architecture similar MIPS andRISC V, short life. vulnerable IBM 360/370 remains classic set standard many instruction sets follow. Among decisions architects made early1960s were: ■8-bit byte ■Byte addressing ■32-bit words ■32-bit single precision floating-point format + 64-bit double precision floating-point format ■32-bit general-purpose registers, separate 64-bit floating-point registers ■Binary compatibility across family computers different cost-performance ■Separation architecture implementation mentioned Chapter 2, IBM 370 extended virtualizable, lowest overhead virtual machine ISA. IBM 360/370remains foundation IBM mainframe business version hasextended 64 bits.K-2 ■Appendix K Survey Instruction Set ArchitecturesK.2 Survey RISC Architectures Desktop, Server, Embedded Computers Introduction cover two groups Reduced Instruction Set Computer (RISC) architectures section. first group desktop, server RISCs, PMD processors: ■Advanced RISC Machines ARMv8, AArch64, 64-bit ISA, ■MIPS64, version 6, recent 64-bit ISA, ■Power version 3.0, merges earlier IBM Power architecture thePowerPC architecture. ■RISC-V, specifically RV64G, 64-bit extension RISC-V. ■SPARCv9, 64-bit ISA. AsFigure K.1 shows architectures remarkably similar. two important historical RISC processors almost iden- tical list above: DEC Alpha processor, made Dig- ital Equipment Corporation 1992 2004 almost identical MIPS64. Hewlett-Packard ’s PA-RISC produced HP 1986 2005, replaced Itanium. PA-RISC closely related Power ISA,which emerged IBM Power design, descendant IBM 801. second group embedded RISCs designed lower-end applications: ■Advanced RISC Machines, Thumb-2: 32-bit instruction set 16-bit and32-bit instructions. architecture includes features ARMv7 ARMv8. ■microMIPS64: version MIPS64 instruction set 16-itinstructions, ■RISC-V Compressed extension (RV64GC), set 16-bit instructions addedto RV64G RV64GC microMIPS64 corresponding 32-bit versions: RV32GC microMIPS32. Since comparison base 32-bit 64-bit desktop server architec- ture examine differences among ISAs, discussion embed-ded architectures focuses 16-bit instructions. Figure K.2 shows embedded architectures also similar. three, 16-bit instructions ver- sions 32-bit instructions, typically restricted set registers.The idea reduce code size replacing common 32-bit instructions 16-bit versions.For RV32GC Thumb-2, including 16-bit instructions yields reduction incode size 0.73 code size using 32-bit ISA (either RV32Gor ARMv7).K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-3Figure K.1 Summary recent version five architectures desktop, server, PMD use (all earlier versions). Except number data address modes instruction set details, integer instruc- tion sets architectures similar. Contrast Figure K.29 . ARMv8, register 31 0 (like register 0 architectures), used load store, current stack pointer, special purposeregister. either think SP-based addressing different mode (which assembly mnemonics operate) simply register + offset addressing mode (which instruction encoded). Figure K.2 Summary three recent architectures embedded applications. three use 16-bit extensions base instruction set. Except number data address modes number instruction set details, integer instruction sets architectures similar. Contrast Figure K.29 . earlier 16-bit version MIPS instruction set, called MIPS16, created 1995 replaced microMIPS32 microMIPS64. firstThumb architecture 16-bit instructions created 1996. Thumb-2 built primarily ARMv7, 32-bit ARM instruction set; offers 16 registers. RISC-V also defines RV32E, 16 registers, includes 16-bit instructions, cannot floating point. appears implementations embedded appli-cations opt RV32C RV64GC.A key difference among three architectures structure base 32-bit ISA. case RV64GC, 32-bit instructions exactly RV64G. possible RISC V planned 16-it option thebeginning, branch addresses jump addresses specified 16-itboundaries. case microMIPS64, base ISA MIPS64, onechange: branch jump offsets interpreted 16-bit rather 32-bitaligned. (microMIPS also uses encoding space reserved MIPS64for user-defined instruction set extensions; extensions part thebase ISA.) Thumb-2 uses slightly different approach. 32-bit instructions Thumb- 2 mostly subset ARMv7; certain features dropped inARMv8 included (e.g., conditional execution instructions andthe ability write PC GPR). Thumb-2 also includes dozen instruc-tions introduced ARMv8, specifically bit field manipulation, additional systeminstructions, synchronization support. Thus, 32-bit instructions Thumb-2 constitute unique ISA. Earlier versions 16-bit instruction sets MIPS (MIPS16) ARM (Thumb), took approach creating separate mode, invoked procedure call, transfer control code segment employed 16-bit instructions. 16-bit instruction set complete intended user pro- grams code-size critical. One complication description older RISCs extended years. decided describe recent versions thearchitectures: ARMv8 (the 64-bit architecture AArch64), MIPS64 R6, Powerv3.0, RV64G, SPARC v9 desktop/server/PMD, 16-bit subset ISAs microMIPS64, RV64GC, Thumb-2. remaining sections proceed follows. discussing addressing modes instruction formats RISC architectures, present surveyof instructions five steps: ■Instructions found RV64G core, described Appendix A. ■Instructions found RV64G RV64GC found two ofthe architectures. describe organize functionality, e.g.instructions support extended integer arithmetic. ■Instruction groups unique ARM, MIPS, Power, SPARC, organized byfunction. ■Multimedia extensions desktop/server/PMD RISCs ■Digital signal-processing extensions embedded RISCs Although majority instructions architectures included, included every single instruction; especially true Powerand ARM ISAs, many instructions.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-5Addressing Modes Instruction Formats Figure K.3 shows data addressing modes supported desktop/server/ PMD architectures. Since all, ARM, one register always value0 used address modes, absolute address mode limited range besynthesized using register 0 base displacement addressing. (This registercan changed arithmetic-logical unit (ALU) operations PowerPC, isalways zero used address calculation.) Similarly, register indirectaddressing synthesized using displacement addressing offset 0.Simplified addressing modes one distinguishing feature RISC architectures. AsFigure K.4 shows, embedded architectures restrict registers accessed 16-bit instructions, typically 8 registers, mostinstructions, special instructions refer registers. Figure K.5 shows data addressing modes supported embedded architectures their16-bit instruction mode. versions load/store instructions restrict reg-isters used address calculations, well significantly shorten theimmediate fields, used displacements. References code normally PC-relative, although jump register indirect supported returning procedures, case statements, pointer func- tion calls. One variation PC-relative branch addresses often shifted left 2bits added PC desktop RISCs, thereby increasing thebranch distance. works length instructions desktop Figure K.3 Summary data addressing modes supported desktop architectures, B, H, W, indi- cate datatypes use addressing mode. Note ARM includes two different types address modes updates, one included Power.K-6 ■Appendix K Survey Instruction Set ArchitecturesRISCs 32 bits instructions must aligned 32-bit words memory. Embedded architectures RISC V (when extended) 16-bit-long instruc- tions usually shift PC-relative address 1 similar reasons. Figure K.6 shows important instruction formats desktop/server/ PMD RISC instructions. instruction set architecture uses four primaryinstruction formats, typically include 90 –98% instructions. register-register format used register-register ALU instructions, theALU immediate format used ALU instructions immediate operandand also loads stores. branch format used conditional branches,and jump/call format unconditional branches (jumps) procedures calls. number less frequently used instruction formats Figure K.6 leaves out. Figure K.7 summarizes desktop/server/PMD architectures. Unlike, 32-bit base architectures, 16-bit extensions (microMIPS64, RV64GC, Thumb-2) focused minimizing code. result, area larger number instruction formats, even though far fewer instructions.Figure K.4 Register encodings 16-bit subsets microMIPS64, RV64GC, Thumb-2, including core general purpose registers, special-purpose registers accessible instructions. Figure K.5 Summary data addressing modes supported embedded architectures. microMIPS64, RV64c, Thumb-2 show modes supported 16-bit instruction formats. stack pointer RV64GC micro-MIPS64 designed GPR; another version r31 Thumb-2. microMIPS64, global pointer register 30 used linkage convention point global variable data pool. Notice typically 8 registers accessible base registers (and see ALU sources destinations).K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-7Opcode Register ConstantARM MIPSPowerRISC-VSPARC ARM MIPS PowerRISC-V SPARC Arm MIPSPower RISC-V SPARC ARM MIPS Power RISC-V SPARCRegister-register Register-immediate Branch Jump/callOp631 25 20 15 10 4 0 0 51 02 52 13 31 25 31 25 200 51 02 0Opx5Rs15 Rd5Rs15Const5 Const21 Rs15Opx5/Rs25Const16 Opx6 Opx3 Opx11Rs15Const14Opx22 Opx2Rs25Rs15 Const19 Const26 Const24 Const21 Const30Const16 Const16Rs15Rd5 Rd5Rs15 Rs15Rd5 Rd5Opx6Rs151 Const13Rs15Rs25Rd5 Rd5Rs15 Rs15Rs25Opx11Opx6 Opx3Rs25Rs15 Opx80 Rd5Opx6Rs25Op6 Op6 Op6 Op6 Op6 Op2 Op2 Op2Op2 Op6 Op6 Op6 Op6 Op6 Op631 29 24 18 13 12 4 0 0 2131 81 42 92 13 01 21 81 92 13 0 1 21 51 02 92 13Rd5 Const26Rs25Const5 Op7Opx2/Const7Rd5 Const12Opx4 Op7Opx3Const12 Rs15 Const7Op7Const5 Rd5Op7 Figure K.6 Instruction formats desktop/server RISC architectures. four formats found five archi- tectures. (The superscript notation figure means width field bits.) Although register fields located similar pieces instruction, aware destination two source fields sometimes scram-bled. Op¼the main opcode, Opx ¼an opcode extension, Rd ¼the destination register, Rs1 ¼source register 1, Rs2 ¼source register 2, Const ¼a constant (used immediate, address, mask, sift amount). Although labels instruction formats tell various instructions encoded, variations. example, loads andstores, use ALU immediate form MIPS. RISC-V, loads use ALU immediate format, stores use branch format.K-8 ■Appendix K Survey Instruction Set ArchitecturesmicroMIPs64 RV64GC eight seven major formats, respectively, Thumb-2 15. Figure K.8 shows, involve varying number register operands (0 3), different immediate sizes, even different size register spec- ifiers, small number registers accessible instructions, fewer instructions able access 32 registers. Instructions similarities architecture allow simultaneous descriptions, starting withthe operations equivalent RISC-V 64-bit ISA.Figure K.7 instruction formats beyond four major formats previous figure. cases, formats similar one four core formats, register field used purposes. ThePower architecture also includes number formats vector operations.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-9ArchitectureOpcode main: extendedRegister speciﬁers x lengthImmediate ﬁeld length Typical instructions microMIPS64 6 none 10 Jumps 6 1x5 5 Register-register operation (32 registers) Load using SP base register; destination 6 1x3 7 Branches equal/not equal zero. Loads using GP. base. ,noitareporetsiger-retsigeR 3x2 4:6 rd/rs1, rs2; 8 registers 6:1 2x3 3 Register-register immediate, rd/rs1, rs2; 8 registers 6 2x3 4 Loads stores; 8 registers itareporetsiger-retsigeR 3x2 4:6 on, rd, rs1; 8 registers retsiger-retsigeR 5x2 6 operation; 32 registers. RV64GC 2:3 11 Jumps 2:3 1x3 7 Branch2:3 1x3 8 Immediate one source register. 2:3 1x5 6 Store using SP base. 2:3 1x5 6 ALU immediate load using SP base. noitareporetsiger-retsigeR 5x2 4:2 2:3 2x3 5 Loads stores using 8 registers. Thumb-2 3:2 2x3 5 Shift, move, load/store word/byte 3:2 1x3 8 immediates: add, subtract, move, compare 4:1 1x3 8 Load/store stack pointer base, Add SP PC, Load/store multiple dexedniretsigerdaoL 3x3 3:4 4:4 8 Conditional branch, system instruction tnereffid22:suoenallecsiM 21:4 instructions 12 formats (includes compare branch zero, pop/push registers, adjust stack pointer, reverse bytes, IF-THEN instruction). 5 1x3 8 Load relative PC hcnarblanoitidnocnU 11 5 tcartbus/ddA 3x3 1:6 6:3 1x4, 1x3 Special data processing gnissecorpatadlacigoL 3x2 4:6 tsniegnahcdnahcnarB 4x1 6:6 ruction set (ARM vs. Thumb) Figure K.8 Instruction formats 16-bit instructions microMIPS64, RV64GC, Thumb-2. instructions destination two sources, two register fields, instruction uses one registers source destination. Note extended opcode field (or function field) immediate field sometimes over-lap identical. RV64GC microMIPS64, formats shown; Thumb-2, Miscellaneous format includes 22 instructions 12 slightly different formats; use extended opcode field, instructions immediate register fields.K-10 ■Appendix K Survey Instruction Set ArchitecturesRV64G Core Instructions Almost every instruction found RV64G found architectures, Figures K.9 K.19 show. (For reference, definitions RISC-V instruc- tions found Section A.9.) Instructions listed four categories: data transfer ( Figure K.9 ); arithmetic, logical ( Figure K.10 ); control ( Figure K.11 Figure K.12 ); floating point ( Figure K.13 ). RV64G core instruction requires short sequence instructions architectures, instructions separated semicolons Figure K.9 Figure K.13 . (To avoid confusion, destination register always left- operand appendix, independent notation normally used witheach architecture.). Compare Conditional Branch Every architecture must scheme compare conditional branch, butdespite similarities, architectures found different wayto perform operation! Figure K.11 summarizes control instructions, Figure K.12 shows details conditional branches handled. SPARC uses traditional four condition code bits stored program status word: negative, zero, carry ,a n overflow . set arithmetic logical instruction; unlike earlier architectures, setting optional instruction. explicitoption leads fewer problems pipelined implementation. Although conditioncodes set side effect operation, explicit compares synthesizedwith subtract using r0as destination. SPARC conditional branches test con- dition codes determine possible unsigned signed relations. Floating pointuses separate condition codes encode EEE 754 conditions, requiring afloating-point compare instruction. Version 9 expanded SPARC branches four ways: separate set condition codes 64-bit operations; branch tests contents register branches value =, not=, <,<=,>=, <=0; three sets floating-point condition codes; branch instructions encode static branch prediction. Power also uses four condition codes: less than, greater than, equal ,a n sum- mary overflow , eight copies them. redundancy allows Power instructions use different condition codes without conflict, essentially givingPower eight extra 4-bit registers. eight condition codes target compare instruction, source conditional branch. integer instructions option bit behaves integer followedby compare zero sets first condition “register. ”Power also lets second “register ”be optionally set floating-point instructions. PowerPC provides logical operations among eight 4-bit condition code registers ( CRAND ,CROR, CRXOR, CRNAND, CRNOR, CREQV ), allowing complex conditions tested single branch. Finally, Power includes set branch count registers,that automatically decremented tested, used branch con- dition. also special instructions moving from/to condition register.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-11Figure K.9 Desktop RISC data transfer instructions equivalent RV64G core. sequence instructions syn- thesize RV64G instruction shown separated semicolons. MIPS Power instructions atomic opera- tions load conditionally store pair registers used implement RV64G atomic operations one intervening ALU instruction. SPARC instructions: compare-and-swap, swap, LDSTUB provide atomicupdates memory location used build RV64G instructions. Power3 instructions provide functionality, RV64G instructions, depending function field.K-12 ■Appendix K Survey Instruction Set ArchitecturesFigure K.10 Desktop RISC arithmetic/logical instructions equivalent RISC-V integer ISA. MIPS also provides instructions trap arithmetic overflow, synthesized architectures multiple instructions.Note “Arithmetic/logical ”category machines SPARC use separate instruction mnemonics indicate immediate operand; SPARC offers immediate versions instructions uses single mnemonic. (Of course, separate opcodes!)K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-13RISC-V MIPS similar. RISC-V uses compare branch full set arithmetic comparisons. MIPS also uses compare branch, comparisons limited equality tests zero. limited set con- ditions simplifies branch determination (since ALU operation requiredto test condition), cost sometimes requiring use set-on-less-thaninstruction ( SLT,SLTI, SLTU, SLTIU ), compares two operands set destination register 1 less 0 otherwise. Figure K.12 providesInstruction name ARMv8 MIPS64 PowerPC RISC-V SPARC v.9 Branch integer compareB.cond, CBZ, CBNZBEQ, BNE, B_Z (<, >, <=, >=) S***; BEZBC BEQ, BNE, BLT, BGE, BLTU, BGEU BR_Z, BPcc (<, >, <=, >=, =, not=) Branch floating-point compareB.cond BC1T, BC1FBC BEZ, BNZ FBPfcc (<, >, <=, >=, =,...) Jump, jump register B, BR J, JR B, BCLR, BCCTRJAL, JALR (with x0)BA, JMPL r0,... Call, call register BL, BLR JAL, JALRBL, BLA, BCLRL, BCCTRLJAL, JALR CALL, JMPL Trap SVC, HVC, SMCBREAK TW, TWI ECALL Ticc, SIR Return interrupt ERET JR; ERET RFI EBREAK DONE, RETRY, RETURN Figure K.11 Desktop RISC control instructions equivalent RV64G. ARMv8 MIPS64 PowerPC RISC-V SPARC v.9 Number condition code bits (integer FP)16 (8 + inverse)none 8 4 none 2 4 integer, 4 2 FP Basic compare instructions (integer FP)1 integer; 1 FP1 integer, 1 FP 4 integer, 2 FP 2 integer; 3 FP 1 FP Basic branch instructions (integer FP)1 2 integer, 1 FP 1 4 integer (used FP well)3 integer, 1 FP Compare register register/ constant branch— =, not= — =, =, >=, < — Compare register zero branch— =, not=, <, <=, >, >=— =, not=, <, <=, >, >==, not=, <, <=, >, >= Figure K.12 Summary five desktop RISC approaches conditional branches. Integer compare SPARC synthesized arithmetic instruction sets condition codes using r0as destination.K-14 ■Appendix K Survey Instruction Set Architecturesadditional details conditional branch. RISC-V floating point comparisons sets integer register 0 1, use conditional branches content.MIPS alsouses separate floating-point compare, sets floating point register 0 1,which tested floating-point conditional branch.Floating point (instruc- Multiply add; Negative multiply add: single, double Multiply subtract single, double, Negative multiply subtract: single, double Copy sign negative sign double single another FP register Replace sign bit XOR sign bits single double Maximum minimum single, double Classify floating point value single double Convert FP single double FP single double, integer single double, signed unsigned roundingtion formats) R-R R-R R-R R-R R-R Instruction name ARMv8 MIPS64 PowerPC RISC-V SPARC v.9 Add single, double FADD ADD.* FADD* FADD.* FADD* Subtract single, double FSUB SUB.* FSUB* FSUB.* FSUB* Multiply single, double FMUL MUL.* FMUL* FMUL.* FMUL* Divide single, double FDIV DIV.* FDIV* FDIV.* FDIV* Square root single, double FSQRT SQRT.* FSQRT* FSQRT.* FSQRT* FMADD, FNMADDMADD.* NMAD.*FMADD*, FNMADD*FMADD.* FNMADD.* FMSUB, FNMSUBMSUB.*, NMSUB.*FMSUB*, FNMSUB*FMSUB.*, FNMSUB.* FMOV, FNEGFMOV.*, FNEG.* FMOV*, FNEG*FSGNJ.*, FSGNJN.*FMOV*, FNEG* FABS FABS.* FABS* FSGNJX.* FABS*FMAX, FMINMAX.*, MIN.* FMAX.*, FMIN.* FCLASS.* CLASS.* Compare FCMP CMP.* FCMP* FCMP.* FCMP* FCVT CVT, CEIL, FLOORFCVT F*TO* Figure K.13 Desktop RISC floating-point instructions equivalent RV64G ISA empty entry meaning instruction unavailable. ARMv8 uses assembly mnemonic single double precision; reg- ister designator indicates precision. “*”is used abbreviation D. floating point compares con- ditions: equal, equal, less than, less-then equal provided. Moves operate directions from/to integer registers. Classify sets register based whether floating point quantity plus minus infinity, denorm, +//C00, etc.). sign-injection instructions take two operands, primarily used form floating point move, negate, absolute value, separate instructions ISAs.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-15ARM similar SPARC, provides four traditional condition codes optionally set. CMP subtracts one operand difference sets condition codes. Compare negative ( CMN)adds one operand other, sum sets condition codes. TST performs logical two oper- ands set condition codes overflow, TEQ uses exclusive set first three condition codes. Like SPARC, conditional version ARMbranch instruction tests condition codes determine possible unsigned andsigned relations. ARMv8 added bit-test instructions also compare andbranch zero. Floating point compares ARM, set integer condition codes, used B.cond instruction. AsFigure K.13 shows floating point support similar five architectures. RV64GC Core 16-bit Instructions Figures K.14 K.17 summarize data transfer, ALU, control instruc- tions three embedded processors: microMIPS64, RV64GC, Thumb-2. Since architectures based 32-bit 64-bit versions full archi- tecture, focus attention functionality implemented 16-bitinstructions. Since floating point optional, include it. Instruction namemicroMIPS64 rs1;rs2/dst; oﬀsetRV64GC rs1;rs2/dst; oﬀsetThumb-2 rs1;rs2/dst; oﬀset Load word 8;8;4 8;8;5 8;8;5 Load double word 8;8;5 Load word stack pointer base register 1;32;5 1;32;6 1;3;8 Load double word stack pointer base register 1;32;6 Store word 8;8;4 8;8;5 8;8;5 Store double word 8;8;5 Store word stack pointer base register 1;32;5 1;32;6 1;3;8 Store double stack pointer base register 1;32;6 Figure K.14 Embedded RISC data transfer instructions equivalent RV64GC 16-bit ISA; blank indicates instruction 16-bit instruction. Rather show instruction name, appropriate, show num- ber registers base register address calculation, followed number registers bethe destination load source store, finally, size immediate used address calculation. example: 8; 8; 5 load means 8 possible base registers, 8 possible destination registers load, 5-bit offset address calculation. store, 8; 8; 5, specifies source value storecomes one 8 registers. Remember Thumb-2 also 32-bit instructions (although full ARMv8 set)and RV64GC microMIPS64 full set 32-bit instructions RV64I MIPS64.K-16 ■Appendix K Survey Instruction Set ArchitecturesmicroMIPS64 RV64GC Thumb-2 sffotib-01 hcnarblanoitidnocnU et 11-bit offset 11-bit offset Unconditional branch link 11-bit offset 11-bit offset Unconditional branch register w/wo link 32 registers 32 registers Compare register zero (=/!=) branch 8 registers; 7-bit offset 8 registers; 8-bit offsetno: see caption Figure K.16 Summary three embedded RISC approaches conditional branches. blank indicates instruction exist. Thumb-2 uses 4 condition code bits; provides conditional branch tests the4-bit condition code branch offset 8 bits.Instruction Name/Function Load immediate etaidemmireppudaoL add immediate add immediate word (32 bits) & sign extend add immediate stack pointer add immediate stack pointer store reg. shift left/right logical citemhtirathgirtfihs immediate evom dda AND, OR, XORThumb-2 8;8 8;8;3 1;7 8;8;5 (shift amt.) 8;8;5 (shift amt.) 8;8 61;61 8;8;8 16;16 8;8 8;8;8microMIPS64 8;7 32;4 1;9 1;8;6 8;8;3 (shift amt.) 8;8;4 23;23 8;8;8 8;8 8;8;8 tcartbus add word, subtract word (32 bits) & sign extend RV64GC 32;6 6;23 32;6 32;6 1;6 (adds 16x imm.) 1;8;6 (adds 4x imm.) 8;6(shift amt.) fihs(6;8t . ) 8;6 23;23 23;23 8;8 8;8 8;8 Figure K.15 ALU instructions provided RV64GC equivalents, any, 16-bit instructions micro- MIPS64 Thumb-2. entry shows number register sources/destinations, followed size imme- diate field, exists instruction. add stack pointer scaled immediate instructions used adjusting stack pointer creating pointer location stack. Thumb, add two forms one three operands 8-register subset (Lo) one two operands 16-registers.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-17Instructions: Common Extensions beyond RV64G Figures K.15 K.18 list instructions found Figures K.9 K.13 four categories (data transfer, ALU, control. significantfloating point extension reciprocal instruction, MIPS64 Power support. Instructions put lists appear one standard architectures. Recall Figure K.3 page 6 showed address modes supported various instruction sets. three processors pro-vide address modes provided RV64G. loads stores usingthese additional address modes shown Figure K.17 , effectively additional data transfer instructions. means ARM 64 additional loadand store instructions, Power3 12, MIPS64 SPARVv9 eachhave 4. accelerate branches, modern processors use dynamic branch prediction (see Section 3.3). Many architectures earlier versions supported delayedbranches, although dropped largely eliminated later versionsFunction Deﬁnition ARMv8 MIPS64 PowerPC SPARC v.9 Load/store multiple registers Loads stores 2 registers Load pair, store pairLoad store multiple (<=31 registers), Cache manipulation prefetchModifies status cache line prefetchPrefetch CACHE, PREFETCHPrefetch Prefetch Figure K.17 Data transfer instructions found RISC-V core found two five desktop architectures. SPARC requires memory accesses aligned, architectures support unaligned access, albeit, often major performance penalties. architectures require alignment, mayuse slow mechanisms handle unaligned accesses.MIPS provides set instructions handle misaligned accesses: LDL LDR (load double left load double right instructions) work pair load misaligned word; corresponding store instructions perform inverse. Prefetch instruction causes cache prefetch, whileCACHE provides limited user control cache state. Name Deﬁnition ARMv8 MIPS64 PowerPC SPARC v.9 Delayed branches Delayed branches with/without cancellationBEQ, BNE, BGTZ, BLEZ, BCxEQZ, BCxNEZBPcc, A, FPBcc, Conditional trap Traps condition true TEQ, TNE, TGE, TLT, TGEU, TLTUTW, TD, TWI, TDITcc Figure K.18 Control instructions found RV64G core found two architectures. MIPS64 Release 6 nondelayed normal delayed branches, SPARC v.9 delayed branches can- cellation based static prediction.K-18 ■Appendix K Survey Instruction Set Architecturesof architecture, typically offering nondelayed version, preferred con- ditional branch. SPARC “annulling ”branch optimized form delayed branch executes instruction delay slot branch taken;otherwise, instruction annulled. means instruction target ofthe branch safely copied delay slot since executedif branch taken. restrictions target another branch andthat target known compile time. (SPARC also offers nondelayed jumpbecause unconditional branch annul bit set notexecute follow- ing instruction.). contrast differences among full ISAs, 16-bit subsets three embedded ISAs essentially significant differences thosedescribed earlier figures (e.g. size immediate fields, uses SP otherregisters, etc.). covered similarities, focus unique features architecture. first cover desktop/server RISCs, ordering bylength description unique features shortest longest, thenthe embedded RISCs. Instructions Unique MIPS64 R6 MIPS gone six generations instruction sets. Generations 1 –4 mostly added instructions. Release 6 eliminated many older instructions also providedsupport nondelayed branches misaligned data access. Figure K.19 summa- rizes unique instructions MIPS64 R6. Instruction class Instruction name(s) Function ALU Byte align Take pair registers extract word double word bytes. Used implement unaligned byte copies. Align Immediate PC Adds upper 16 bits PC immediate shifted left 16 bits puts result register; Us ed get PC-relative address. Bit swap Reverses bits byte register. No-op link Puts value PC+8 registerLogical Computes 2 registers Control transfer Branch Link conditional Compares register 0 branch condition true; places return address link register. Jump indexed, Jump link indexedAdds offset register get new PC, w/wo link address Figure K.19 Additional instructions provided MIPS64 R6. addition, several instructions supporting virtual machines, privileged.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-19Instructions Unique SPARC v.9 Several features unique SPARC. review major figures sum- marize small differences figure. Register Windows primary unique feature SPARC register windows, optimization forreducing register traffic procedure calls. Several banks registers used, witha new one allocated procedure call. Although could limit depth procedure calls, limitation avoided operating banks circular buffer. knee cost-performance curve seems six eight banks; programswith deeper call stacks, would need save restore registers memory. SPARC 2 32 windows, typically using 8 registers globals, locals, incoming parameters, outgoing parameters. (Given thateach window 16 unique registers, implementation SPARC asfew 40 physical registers many 520, although 128 136, sofar.) Rather tie window changes call return instructions, SPARC separate instructions SAVE andRESTORE .SAVE used “save”the caller ’s window pointing next window registers addition performing anadd instruction. trick source registers caller ’s window addition operation, destination register callee ’s window. SPARC compilers typically use instruction changing stack pointerto allocate local variables new stack frame. RESTORE inverse SAVE , bringing back caller ’s window acting add instruction, source registers callee ’s window destination register caller ’s window. automatically deallocates stack frame. Compilers also make use generating callee ’s final return value. danger register windows larger number registers could slow clock rate. case early implementations. TheSPARC architecture (with register windows) MIPS R2000 architecture(without) built several technologies since 1987. several genera-tions SPARC clock rate slower MIPS clock rate imple-mentations similar technologies, probably cache access times dominate register access times implementations. advent multiple issue, requires many register ports, register renaming reorderbuffers, register windows posed larger penalty.Register windows featureof original Berkeley RISC designs, inclusion SPARC inspiredby designs. Tensilica major architecture use todayemploys them, included RISC-V ISA. Fast Traps SPARCv9 includes support make traps fast. expands single level traps atleast four levels,allowing thewindow overflow underflow traphandlers beinter-rupted. extra levels mean handler need check page faults orK-20 ■Appendix K Survey Instruction Set Architecturesmisaligned stack pointers explicitly code, thereby making handler faster. Two new instructions added return multilevel handler: RETRY (which retries interrupted instruction) DONE (which not). support user-level traps, instruction RETURN return trap nonprivileged mode. Support LISP Smalltalk primary remaining arithmetic feature tagged addition subtraction. designers SPARC spent time thinking languages like LISP Smalltalk, influenced features SPARC already discussed: register windows, conditional trap instructions, calls 32-bit instructionaddresses, multi-word arithmetic (see Taylor et al. [1986] Ungar et al.[1984]). small amount support offered tagged data types operationsfor addition, subtraction, hence comparison. two least-significant bits indi-cate whether operand integer (coded 00), TADDcc andTSUBcc set overflow bit either operand tagged integer result toolarge. subsequent conditional branch trap instruction decide do. (If operands integers, software recovers operands, checks types operands, invokes correct operation based types.) turnsout misaligned memory access trap also put use tagged data,since loading pointer wrong tag invalid access.Figure K.20 shows types tag support. (a) Add, sub, compare integers(coded 00) (b) Loading via valid pointer (coded 11)00 (R5) 00 (R6) 00 (R7) 11 3(R4) 00 (Word address)TADDcc r7, r5, r6 LD rD, r4, –3+– – Figure K.20 SPARC uses two least-significant bits encode different data types tagged arithmetic instructions. (a) Integer arithmetic, takes single cycle long operands result integers. (b) misaligned trap usedto catch invalid memory accesses, trying use integer pointer. lan-guages paired data like LISP, offset –3 used access even word pair (CAR) +1 used odd word pair (CDR).K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-21Figure K.21 summarizes additional instructions mentioned well several others. Instructions Unique ARM Earlier versions ARM architecture (ARM v6 v7) number unusual features including conditional execution instructions, making PC general purpose register. features eliminated arrivalof ARMv8 (in 32-bit 64-bit ISA). remains, however, much ofthe complexity, least terms size instruction set. Figure K.3 page 6 shows, ARM addressing modes, including listed inthe table; remember addressing modes add dozens load/store instruc-tions compared RVG, even though listed table follows. AsFigure K.6 page 8 shows, ARMv8 also far largest number different instruction formats, reflects variety instructions, well different addressing modes, applicable loads stores notothers. ARMv8 ALU instructions allow second operand shifted operation completed. extends range immediates, operandshifting limited immediates. shift options shift left logical, shiftright logical, shift right arithmetic, rotate right. addition, Power3, mostALU instructions optionally set condition flags. Figure K.22 includes additional instructions, enumerate varieties (such optional setting condition flags); see caption detail. conditionalexecution instructions eliminated, ARMv8 provides number condi-tional instructions beyond conditional move conditional set, mentionedearlier.Instruction class Instruction name(s) Function Data transfer SA VE, RESTORE Save restore register window Nonfaulting load Version load instructions generate faults address exceptions; allows speculation loads. ALU Tagged add, Tagged subtract, without trap Perform tagged add/subtract, set condition codes, optionally trap. Control transfer Retry, Return, Done provide handling traps. Floating Point InstructionsFMOVcc Conditional move FP registers based integer FP condition codes. Figure K.21 Additional instructions provided SPARCv9. Although register windows far signif- icant distinction, require many instructions!K-22 ■Appendix K Survey Instruction Set ArchitecturesInstructions Unique Power3 Power3 result several generations IBM commercial RISC machines — IBM RT/PC, IBM Power1, IBM Power2, PowerPC development,undertaken primarily IBM Motorola. First, describe branch registersand support loop branches. Figure K.23 lists instructions pro- vided Power3.Instruction class Instruction name(s) Function Data transfer Load/Store Non-temporal pair Loads/stores pair registers indication cache data. Base + scaled offset addressing mode only. ALU Add Extended word/double word Add 2 registers left shifting second register operand extending it. Add shift; add immediate shiftAdds shift second operand. Address page Computes address page based PC (similar ADDUIPC, ADR ARMv8) AND, OR, XOR, XOR shifted registerLogical operation register shifted register. Bit field clear shifted Shift operand, invert another operand Conditional compare, immediate, negative, negative immediateIf condition true, set condition flags compare result, otherwise leave condition flags untouched. Conditional increment, invert, negateIf condition set destination increment/invert/negate source register elbuod,drowflah,drow,etyb:muskcehcCRCasetupmoC CRC Multiply add, subtract Integer multiply-add multiply-subtract Multiply negate Negate product two integers; word & double word Move immediate inverse Replace 16-bits register immediate, possibly shifted Reverse bit order Reverses order bits register Signed bit field move Move signed bit field; sign extend left; zero extend right Unsigned divide, multiple, multiply negate, multiply-add, multiply-subUnsigned versions basic instructions Control transfer CBNZ, CBZ Compare branch =/!= 0, indicating call return. TBNZ, TBZ Tests bit register =/!= 0, branch. Figure K.22 Additional instructions provided ARMv8, AArch64 instruction set. Unless noted instruction available word double word format, difference. ALU instructions optionally set condition codes; included separate instructions earlier tables.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-23Instruction class Instruction name(s) Function Data transfer LHBRX, LWBRX, LDBRX Loads halfword/word/double word reverses byte order. SHBRX, SWBRX, SDBRX Stores halfword/word/double word reverses byte order LDQ, STQ Load/store quadword register pair. retsigeranirebmunmodnaraetareneG NARD ULA CMPB CMPRBCompares individual bytes register sets another register byte byte. Compares byte (x) two bytes (y z) sets condition indicate value y<=x<=z. CRAND, CRNAND, CROR, CRNOR, CRXOR, CREQV , CORC, CRANDCLogical operations condition register. ZCMPEQB Compares byte (x) eight bytes another register sets condition indicate x = 8 bytes EXTSWSL Sign extend word shift leftPOPCNTB, POPCNTW POPCNTDCount number 1s byte place total another byte. Count number 1s word place total another word. Count number 1s double word. PRTYD, PRTYW Compute byte parity bytes word double word. BPERMD Permutes bits double word, producing permuted byte. CDTBCD, CDCBCD, ADDGCSInstructions convert from/to binary coded decimal (BCD) operate two BCD values Control transfer BA, BCA Branches absolute address, conditionally & unconditionally BCCTR, BCCTRL Conditional branch address count register, w/wo linking BCTSAR, BCTARL Conditional branch address Branch Target Address register, w/wo linking CLRBHRB, MFBHRBE Manipulate branch history rolling buffer. Floating Point InstructionsFRSQRTE Computes estimate reciprocal square root, FTDIV , FTSQRT Tests divide zero square negative number dnaoreztsniagaretsigertse LESF select one two operands move Decimal floating point operationsA series 48 instructions support decimal floating point. Figure K.23 Additional instructions provided Power3. Rotate instructions two forms: one sets con- dition register one not. set string instructions load 32 bytes arbitraryaddress set registers. instructions phased future implementations, hence justmention here.K-24 ■Appendix K Survey Instruction Set ArchitecturesBranch Registers: Link Counter Rather dedicate one 32 general-purpose registers save return address procedure call, Power3 puts address special register called thelink register. Since many procedures return without calling another pro- cedure, link ’t always saved away. Making return address special register makes return jump faster since hardware need gothrough register read pipeline stage return jumps. similar vein, Power3 count register used loops program iterates fixed number times. using special register branchhardware determine quickly whether branch based count register islikely branch, since value register known early execution cycle. Tests value count register branch instruction automat- ically decrement count register. Given count register link register already located hard- ware controls branches, one problems branch prediction isgetting target address early pipeline (see Appendix C), Power archi-tects decided make second use registers. Either register hold atarget address conditional branch. Thus, PowerPC supplements basic con-ditional branch two instructions get target address registers (BCLR ,BCCTR ).Figure K.23 shows several dozen instructions added; note extensive facility decimal floating point, well. Instructions: Multimedia Extensions Desktop/Server RISCs Support multimedia graphics operations developed several phases, beginning 1996 Intel MMX, MIPS MDMX, SPARC VIS. describedin Section 4.3, assume reader read, extensions allowed aregister treated multiple independent small integers (8 16 bits long) witharithmetic logical operations done parallel items register.These initial SIMD extensions, sometimes called packed SIMD, furtherdeveloped 2000 widening registers, partially totally separating general purpose floating pointer registers, adding support parallel floating point operations. RISC-V reserved extension suchpacked SIMD instructions, designers opted focus true vectorextension present. vector extension RV64V vector architecture,and, Section 4.3 points out, true vector instruction set considerably moregeneral, typically perform operations handled SIMD extensionsusing vector operations. Figure K.24 shows basic structure SIMD extensions ARM, MIPS, Power, SPARC. Note difference SIMD “vector registers ”are structured: repurposing floating point, extending floating point, addingadditional registers. key differences include support FP well integers,K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-25support 128-bit integers, provisions immediate fields operands inte- ger logical operations. Standard load store instructions used moving data SIMD registers memory special extensions handle moving less full SIMD register. SPARC VIS, one earliest ISA exten-sions graphics, much limited: add, subtract, multiply areincluded, FP support, limited instructions bit element oper-ations; include Figure K.24 going detail. Figure K.25 shows arithmetic instructions included SIMD exten- sions; appearing least two extensions included. MIPS SIMDincludes many instructions, Power 3 Vector-Scalar extension, cover. One frequent feature generally found general- purpose microprocessors saturating operations. Saturation means acalculation overflows result set largest positive number neg-ative number, rather modulo calculation two ’s complement arithmetic. Commonly found digital signal processors (see next subsection), sat-urating operations helpful routines filtering. Another common extensionare instructions accumulating values within single register; dot productinstruction maximum/minimum instructions typical examples. addition arithmetic instructions, common additions log- ical bitwise operations instructions version permutations andpacking elements SIMD registers. additions summarized inFigure K.26 , Lastly, three extensions support SIMD FP operations, summa- rized Figure K.27 .ARMv8 MIPS64 R6 Power v3.0 SPARCv9 Name ISA extension Advanced SIMD MIPS64 SIMD ArchitectureVector Facility VIS Date Current Version 2011 2012 2015 1995 Vector registers: # x size 32 x 128 bits 32 x 128 bits 32 x 128 bits 32 x 64 bits Use GP/FP registers independent setextend FP registers doubling widthextend FP registers doubling widthIndependent FP registers Integer data sizes 8, 16, 32, 64 8, 16, 32, 64 8, 16, 32, 64, 128 8,16, 32 FP data sizes 32, 64 32, 64 32 Immediates integer logical operations5 bits arithmetic 8 bits logical Figure K.24 Structure SIMD extensions intended multimedia support. addition vector facility, last row states whether SIMD instruction set supports immediates (e.g, add vector immediate vector immediate); entry states size immediates ISAs support them. Note fact imme-diate present encoded opcode space, could alternatively added next table additional instructions. Power 3 optional Vector-Scalar Extension. Vector-Scalar Extension defines set vector registers overlap FP normal vector registers, eliminating need move data back forth tothe vector registers. also supports double precision floating point operations.K-26 ■Appendix K Survey Instruction Set ArchitecturesInstruction category ARM Advanced SIMD MIPS SIMD Power Vector Facility Q,D2,W4,H8,B61 D2;W4;H8,B61 D2;W4,H8,B61 tcartbus/ddA Saturating add/sub 16B, 8H, 4W; 2 16B, 8H; 4W; 2 16B, 8H, 4W, 2 D, Q Absolute value difference 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; QAdjacent add & subtract (pairwise) 16B, 8H, 4W 16B, 8H, 4W 16B, 8H, 4W; 2 Q;D2;W4,H8,B61 D2;W4,H8,B61 egarevA Dot product add, dot product subtract 16B, 8H, 4W 16B, 8H, 4W 16B, 8H, 4W; 2 DDivide: signed, unsigned 16B, 8H, 4W 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q Multiply: signed, unsigned 16B, 8H, 4W 16B, 8H, 4W 16B, 8H, 4W; 2 Multiply add, multiply subtract 16B, 8H, 4W 16B, 8H, 4W 16B, 8H, 4W; 2 Maximum, signed & unsigned 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q Minimum, signed & unsigned 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q Q;D2;W4,H8 ,B61 D2;W4,H8,B61 dengisnu&dengis,oludoM Q;D2;W4,H8,B61 D2;W4,H8,B61 D2;W4,H8,B61 lauqeerapmoC Compare <, <=, signed, unsigned 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q Figure K.25 Summary arithmetic SIMD instructions. B stands byte (8 bits), H half word (16 bits), W word (32 bits), double word (64 bits), Q quad word (128 bits). Thus, 8B means operation 8 bytes asingle instruction. Note instructions –such adjacent add/subtract, multiply –produce results twice width inputs (e.g. multiply 16 bytes produces 8 halfword results). Dot product multiply accumulate. SPARC VIS instructions aimed primarily graphics structured accordingly. Instruction category ARM Advanced SIMD MIPS SIMD Power Vector Facility Shift right/left, logical, arithmetic 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q 16B, 8H, 4W; 2 D; Q Count leading trailing zeros 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q Q Q Q rox/ro/dna Bit insert & extract 16B, 8H, 4W; 2 16B, 8H, 4W; 2 16B, 8H, 4W; 2 D; Q ;W4,H8,B61 tnuocnoitalupoP 2 16B, 8H, 4W; 2 D; Q D2;W4,H8,B6 D2;W4,H8,B61 thgir/tfel,ddo/neveevaelretnI D2;W4,H8,B6 D2;W4,H8,B61 ddo/nevekcaP D2;W4,H8,B61 D2;W4,H8,B61 elffuhS D2;W4,H8,B61 D2;W4,H8,B61 TALPS Figure K.26 Summary logical, bitwise, permute, pack/unpack instructions, using format previous figure. single operand instruction applies entire register; logical operations difference.Interleave puts together elements (all even, odd, leftmost rightmost) two differentregisters create one value; used unpacking. Pack moves even odd elements two different registers leftmost rightmost halves result. Shuffle creates two registers based mask selects source item. SPLAT copies value item register.Instructions: Digital Signal-Processing Extensions Embedded RISCs Thumb2 microMIPS32 provide instructions DSP (Digital Signal Pro- cessing) multimedia operations. Thumb2, part core instruc-tion set; microMIPS32, part DSP extension. extensions,which encoded 32-bit instructions, less extensive multimediaand graphics support provided SIMD/Vector extensions MIPS64 orARMv8 (AArch64). Like comprehensive extensions, ones inThumb2 microMIPS32 also rely packed SIMD, use existing integer registers, small extension allow wide accumulator, operate integer data. RISC-V specified “P”extension support packed integer SIMD using floating point registers, time publica-tion, specification completed. DSP operations often include linear algebra functions operations convolutions; operations produce intermediate results largerthan inputs. Thumb2, handled set operations produce64-bit results using pair integer registers. microMIPS32 DSP, 4 64-bit accumulator registers, including Hi-Lo register, already exists integer multiply divid e. architectures provide parallel arithmetic using bytes, halfwords, wo rds, multimedia extensions ARMv8 MIPS64. addition, MIPS DSP extension handles fractionaldata, data heavily used DSP operations. Fractional data items asign bit remaining bits used represent fraction, providing arange values -1.0 0.9999 (in decimal). MIPS DSP supports twofractional data sizes Q15 Q31 one sign bit 15 31 bits fraction. Figure K.28 shows common operations using notation used Figure K.25 . Remember basic 32-bit instruction set provides additional functionality, including basic arithmetic, logical, bit manipulation.Figure K.27 Summary floating point, using format previous figure.K-28 ■Appendix K Survey Instruction Set ArchitecturesConcluding Remarks survey covers addressing modes, instruction formats, almost instructions found 8 RISC architectures. Although later sections concentrateon differences, would possible cover 8 architectures fewpages many similarities. fact, would guess than90% instructions executed architectures would found inFigures K.9 K.13 . contrast homogeneity, Figure K.29 gives sum- mary four architectures 1970s format similar shown Figure K.1 . (Since would impossible write single section style architectures, next three sections cover 80x86, VAX, IBM360/370.) history computing, never widespread agree-ment computer architecture since RISC ideas emerged inthe 1980s.PSD23SPIMorcim 2-bmuhT noitcnuF 51Q2,B4 H2,B4 tcartbuS/ddA 13Q,51Q2,B4 H2,B4 noitarutashtiwtcartbuS/ddA Add/Subtract Exchange (exchanges halfwords rt, adds first halfword subtracts second) optional saturation2H B4 )seulavehtmus(ddaybecudeR 13Q,51Q2 eulavetulosbA Precision reduce/increase (reduces increases precision value) 2B, Q15, 2Q15, Q31 H2,B4 noitarutaslanoitpohtiw,citemhtira&lacigol,thgir,tfel:stfihS 51Q2,H2,B2 H2 ylpitluM Multiply add/subtract (to GPR accumulator register MIPS) 2H 2Q15Complex multiplication step (2 multiplies addition/subtraction) 2H 2Q15 Multiply accumulate (by addition subtraction) 2H Q15, Q31 H,B stibetacilpeR Compare: =, <, <=, se H 2,B4 dleifnoitidnocst Pick (use condition bits choose bytes halfwords two operands) 4B, 2H H dnarepohcaemorfdrowflahagnisooh ckcaP Extract Q63 Move from/to WD rotalumucca Figure K.28 Summary two embedded RISC DSP operations, showing data types operation. blank indicates operation supported single instruction. Byte quantities usually unsigned. Complex multiplication step implements multiplication complex numbers component Q15 value. ARM usesits standard condition register, MIPS adds set condition bits part state DSP extension.K.2 Survey RISC Architectures Desktop, Server, Embedded Computers ■K-29K.3 Intel 80x86 Introduction MIPS vision single architect. pieces architecture fit nicely together whole architecture described succinctly. caseof 80x86: product several independent groups evolved archi-tecture 20 years, adding new features original instruction set mightadd clothing packed bag. important 80x86 milestones: ■1978—The Intel 8086 architecture announced assembly language – compatible extension then-successful Intel 8080, 8-bit microproces- sor. 8086 16-bit architecture, internal registers 16 bits wide. Whereas 8080 straightforward accumulator machine, 8086extended architecture additional registers. nearly every reg-ister dedicated use, 8086 falls somewhere accumulatormachine general-purpose register machine, fairly called anextended accumulator machine. ■1980—The Intel 8087 floating-point coprocessor announced. architec- ture extends 8086 60 floating-point instructions. architectsrejected extended accumulators go hybrid stacks registers,IBM 360/370 Intel 8086 Motorola 68000 DEC VAX Date announced 1964/1970 1978 1980 1977 Instruction size(s) (bits) 16, 32, 48 8, 16, 24, 32, 40, 48 16, 32, 48, 64, 80 8, 16, 24, 32, ... , 432 Addressing (size, model) 24 bits, flat/ 31 bits, flat4 + 16 bits, segmented24 bits, flat 32 bits, flat Data aligned? Yes 360/No 370 16-bit aligned Data addressing modes 2/3 5 9 =14 Protection Page None Optional Page Page size 2 KB & 4 KB — 0.25 32 KB 0.5 KB deppamyromeM deppamyromeM edocpO edocpO O/I Integer registers (size, model, number)16 GPR × 32 bits 8 dedicated data × 16 bits8 data 8 address × 32 bits15 GPR × 32 bits Separate floating-point registers4 × 64 bits Optional: 8 × 80 bits Optional: 8 × 80 bits 0 Floating-point format IBM (floating hexadecimal)IEEE 754 single, double, extendedIEEE 754 single, double, extendedDEC Figure K.29 Summary four 1970s architectures. Unlike architectures Figure K.1 , little agreement architectures category. (See Section K.3 details 80x86 Section K.4 description VAX.)K-30 ■Appendix K Survey Instruction Set Architecturesessentially extended stack architecture: complete stack instruction set supplemented limited set register-memory instructions. ■1982—The 80286 extended 8086 architecture increasing address space 24 bits, creating elaborate memory mapping protection model, adding instructions round instruction set manipulate protection model. important run 8086 pro-grams without change, 80286 offered real addressing mode make machine look like 8086. ■1985—The 80386 extended 80286 architecture 32 bits. addition 32-bit architecture 32-bit registers 32-bit address space, 80386added new addressing modes additional operations. added instructionsmake 80386 nearly general-purpose register machine. 80386 alsoadded paging support addition segmented addressing (see Chapter 2). Likethe 80286, 80386 mode execute 8086 programs without change. history illustrates impact “golden handcuffs ”of compatibility 80x86, existing software base step important jeopar-dize significant architectural changes. Fortunately, subsequent 80486 in1989, Pentium 1992, P6 1995 aimed higher performance, withonly four instructions added user-visible instruction set: three help withmultiprocessing plus conditional move instruction. Since 1997 Intel added hundreds instructions support multimedia operating many narrower data types within single clock (see Appendix A). SIMD vector instructions primarily used hand-coded libraries ordrivers rarely generated compilers. first extension, called MMX,appeared 1997. consists 57 instructions pack unpack multiplebytes, 16-bit words, 32-bit double words 64-bit registers performs shift,logical, integer arithmetic narrow data items parallel. supports bothsaturating nonsaturating arithmetic. MMX uses registers comprising thefloating-point stack hence new state operating systems save. 1999 Intel added another 70 instructions, labeled SSE, part Pentium III. primary changes add eight separate registers, double width 128bits, add single-precision floating-point data type. Hence, four 32-bitfloating-point operations performed parallel. improve memory perfor-mance, SSE included cache prefetch instructions plus streaming store instructionsthat bypass caches write directly memory. 2001, Intel added yet another 144 instructions, time labeled SSE2. new data type double-precision arithmetic, allows pairs 64-bit floating- point operations parallel. Almost 144 instructions versions existing MMX SSE instructions operate 64 bits data parallel.Not change enable multimedia operations, also gives com-piler different target floating-point operations unique stack architec-ture. Compilers choose use eight SSE registers floating-point registersas found RISC machines. change boosted performance thePentium 4, first microprocessor include SSE2 instructions. time ofK.3 Intel 80x86 ■K-31announcement, 1.5 GHz Pentium 4 1.24 times faster 1 GHz Pentium III SPECint2000(base), 1.88 times faster SPECfp2000(base). 2003 company Intel enhanced IA-32 architecture time. AMD announced set architectural extensions increase address space for32 64 bits. Similar transition 16- 32-bit address space 1985 withthe 80386, AMD64 widens registers 64 bits. also increases number ofregisters sixteen 16 128-bit registers support XMM, AMD ’s answer SSE2. Rather expand instruction set, primary change adding newmode called long mode redefines execution IA-32 instructions 64-bit addresses. address larger number registers, adds new prefix instructions. AMD64 still 32-bit mode backwards compatible thestandard Intel instruction set, allowing graceful transition 64-bit addres-sing HP/Intel Itanium. Intel later followed AMD ’s lead, making almost identical changes software run either 64-bit address versionof 80x86 without change. Whatever artistic failures 80x86, keep mind instances architectural family server desktop processor world. Nevertheless, checkered ancestry led architecture difficult explain impossible love. start explanation registers addressing modes, move integer operations, cover floating-point operations, conclude withan examination instruction encoding. 80x86 Registers Data Addressing Modes evolution instruction set seen registers 80x86(Figure K.30 ). Original registers shown black type, extensions 80386 shown lighter shade, coloring scheme followed subsequent fig-ures. 80386 basically extended 16-bit registers (except segment regis-ters) 32 bits, prefixing “E”to name indicate 32-bit version. arithmetic, logical, data transfer instructions two-operand instructions thatallow combinations shown Figure K.31 . explain addressing modes, need keep mind whether talk- ing 16-bit mode used 8086 80286 32-bit mode available 80386 successors. seven data memory addressingmodes supported ■Absolute ■Register indirect ■Based ■Indexed ■Based indexed displacement ■Based scaled indexed ■Based scaled indexed displacementK-32 ■Appendix K Survey Instruction Set ArchitecturesFPR 0 FPR 1 FPR 2 FPR 3 FPR 4 FPR 5 FPR 6 FPR 70 790 15 0 1587 31 GPR 0 Accumulator EAX AX AH AL GPR 3 Base addr. reg EBX BX BH BLGPR 1 Count reg: string, loop ECX CX CH CL GPR 2 Data reg: multiply, divide EDX DX DH DL GPR 6 ESI Index reg, string source ptr. SI Code segment ptr. CS Stack segment ptr. (top stack) SS Data segment ptr. DS Extra data segment ptr. ES Data segment ptr. 2 FS Data segment ptr. 3 GSGPR 7 EDI Index reg, string dest. ptr. DIGPR 5 EBP Base ptr. (for base stack seg.) BP PCGPR 4 ESP Stack ptr. SP EIP Instruction ptr. (PC) IP EFLAGS Condition codes FLAGS Top FP stack, FP condition codesStatus80x86, 80 x286 80x386, 80 x486, Pentium Figure K.30 80x86 evolved time, register set. original set shown black extended set gray. 8086 divided first four registers half could used either one 16-bit register two 8-bit registers. Starting 80386, top eight registers extended 32 bits could also used general-purpose registers. floating-point registers bottom 80 bits wide, althoughthey look like regular registers not. implement stack, top stack pointed status register. One operand must top stack, seven registers top stack.K.3 Intel 80x86 ■K-33Displacements 8 32 bits 32-bit mode, 8 16 bits 16-bit mode. count size address separate addressing mode, total 11addressing modes. Although memory operand use addressing mode, restric- tions registers used mode. section “80x86 Instruction Encoding ”on page K-11 gives full set restrictions registers, fol- lowing description addressing modes gives basic register options: ■Absolute —With 16-bit 32-bit displacement, depending mode. ■Register indirect —BX, SI, DI 16-bit mode EAX, ECX, EDX, EBX, ESI, andEDI 32-bit mode. ■Based mode 8-bit 16-bit/32-bit displacement —BP, BX, SI, andDI 16-bit mode EAX, ECX, EDX, EBX, ESI, andEDI 32-bit mode. displacement either 8 bits size address mode: 16 32 bits.(Intel gives two different names single addressing mode, based indexed , essentially identical combine them. book uses indexed addressing mean something different, explained next.) ■Indexed —The address sum two registers. allowable combinations areBX+SI, BX+DI, BP+SI, andBP+DI . mode called based indexed 8086. (The 32-bit mode uses different addressing mode get effect.) ■Based indexed 8- 16-bit displacement —The address sum dis- placement contents two registers. restrictions registersapply indexed mode. ■Base plus scaled indexed —This addressing mode next added 80386 available 32-bit mode. address calculation Base register + 2Scale/C2Index/C2registerSource/destination operand type Second source operand retsigeR etaidemmI yromeM retsigeR etaidemmIretsigeR retsigeR retsigeR yromeM yromeM Figure K.31 Instruction types arithmetic, logical, data transfer instruc- tions. 80x86 allows combinations shown. restriction absence memory-memory mode. Immediates may 8, 16, 32 bits length; registeris one 14 major registers Figure K.30 (not IP FLAGS).K-34 ■Appendix K Survey Instruction Set Architectureswhere Scale value 0, 1, 2, 3; Index register eight 32-bit general registers except ESP; Base register eight 32-bit general registers. ■Base plus scaled index 8- 32-bit displacement —The address sum displacement address calculated scaled mode immediatelyabove. restrictions registers apply. 80x86 uses Little Endian addressing. Ideally, would refer discussion 80x86 logical physical addresses Chapter 2, segmented address space prevents us hiding infor-mation. Figure K.32 shows memory mapping options generations 80x86 machines; Chapter 2 describes segmented protection scheme greaterdetail. assembly language programmer clearly must specify segment reg- ister used address, matter address mode used. Tosave space instructions, segment registers selected automatically depend- ing address register used. rules simple: References instruc- tions (IP) use code segment register (CS), references stack (BP SP)use stack segment register (SS), default segment register otherregisters data segment register (DS). next section explains canbe overridden. 80x86 Integer Operations 8086 provides support 8-bit ( byte) 16-bit (called word ) data types. data type distinctions apply register operations well memoryaccesses. 80386 adds 32-bit addresses data, called double words. Almost every operation works 8-bit data one longer data size. size isdetermined mode either 16 32 bits. Clearly programs want operate data three sizes, 80x86 architects provide convenient way specify version without expanding code size significantly. decided programs would dominatedby either 16- 32-bit data, made sense able set default largesize. default size set bit code segment register. override thedefault size, 8-bit prefix attached instruction tell machine use large size instruction. prefix solution borrowed 8086, allows multiple prefixes modify instruction behavior. three original prefixes override default seg- ment register, lock bus perform semaphore (see Chapter 5), repeat following instruction CX counts zero. last prefix intended tobe paired byte move instruction move variable number bytes. The80386 also added prefix override default address size.K.3 Intel 80x86 ■K-35The 80x86 integer operations divided four major classes: 1.Data movement instructions, including move, push, pop 2.Arithmetic logic instructions, including logical operations, test, shifts, integer decimal arithmetic operations 3.Control flow, including conditional branches unconditional jumps, calls, returns 4.String instructions, including string move string compareOffset Segment 16 32 32 32 32202020 10 1012 Physical addressPhysical addressLinear addressLogical address PagingSegmentationOffsetSegment 16 16 24 24Logical address Offset Segment 16 Physical address12 4 16 20Logical address Segmentationedom detcetorP edom laeR )68208( )6808( (80386, 80486, Pentium) Figure K.32 original segmented scheme 8086 shown left. 80x86 processors support style addressing, called real mode . simply takes contents segment register, shifts left 4 bits, adds 16-bit offset, forming 20-bit physical address. 80286 (center) used contents segment register select segment descriptor, includes 24-bit base address among items. added 16-bit offset form 24-bit physical address. 80386 successors (right) expand base address segment descriptor to32 bits also add optional paging layer segmentation. 32-bit linear address first formed segment offset, address divided two 10-bit fields 12-bit page offset. first 10-bit field selects entry first-level page table, entry used combination second 10-bit field toaccess second-level page table select upper 20 bits physical address. Prepending 20-bit address final 12-bit field gives 32-bit physical address. Paging turned off, redefining 32-bit linear address physical address. Note “flat ”80x86 address space comes simply loading value segment registers; is, ’t matter segment register selected.K-36 ■Appendix K Survey Instruction Set ArchitecturesFigure K.33 shows typical 80x86 instructions functions. data transfer, arithmetic, logic instructions unremarkable, except arithmetic logic instruction operations allow destination eithera register memory location. Control flow instructions must able address destinations another seg- ment. handled two types control flow instructions: “near”for intrasegment (within segment) “far”for intersegment (between segments) transfers. far jumps, must unconditional, two 16-bit quantities followthe opcode 16-bit mode. One used instruction pointer, loaded CS becomes new code segment. 32- bit mode first field expanded 32 bits match 32-bit programcounter (EIP). Calls returns work similarly —a far call pushes return instruction pointer return segment stack loads instruction pointerand code segment. far return pops instruction pointer codesegment stack. Programmers compiler writers must sure alwaysuse type call andreturn procedure —a near return work far call, vice versa . String instructions part 8080 ancestry 80x86 commonly executed programs. Figure K.34 lists integer 80x86 instructions. Many instructions available byte word formats.noitcnuF noitcurtsnI JE name JMP name IP name CALLF name, seg SP SP–2;M[SS:SP] IP+5;SP SP–2; PUSH SI SP SP–2;M[SS:SP] SI POP DI DI M[SS:SP];SP SP+2 ADD AX,#6765 AX AX+6765 SHL BX,1 BX BX 1..15## 0 TEST DX,#42 Set CC flags DX & 42 MOVSB M[ES:DI]8M[DS:SI];DI DI+1;SI SI+1MOVW BX,[DI+45] BX 16M[DS:DI+45]M[SS:SP] CS;IP name;CS seg; equal(CC) {IP name};IP–128 name IP+128 Figure K.33 typical 80x86 instructions functions. list frequent operations appears Figure K.34 . use abbreviation SR:X indicate forma- tion address segment register SRand offset X. effective address corre- sponding SR:X (SR<<4)+X. CALLF saves IPof next instruction current CSon stack.K.3 Intel 80x86 ■K-3780x86 Floating-Point Operations Intel provided stack architecture floating-point instructions: loads push numbers onto stack, operations find operands top two elements thestacks, stores pop elements stack, stack example inFigure A.31 page A-4 suggests.gninaeM noitcurtsnI Control Conditional unconditional branches JNZ, JZ Jump condition IP + 8-bit offset; JNE (for JNZ) JE (for JZ) alternative names JMP, JMPF Unconditional jump—8- 16-bit offset intrasegment (near) intersegment (far) versions CALL, CALLF Subroutine call—16-bit offset; return address pushed; near far versions RET, RETF Pops return address stack jumps it; near far versions LOOP Loop branch—decrement CX; jump IP + 8-bit displacement CX ¦ 0 Data transfer Move data registers register memory MOV Move two registers register memory PUSH Push source operand stack POP Pop operand stack top register LES Load ES one GPRs memory Arithmetic/logical Arithmetic logical operations using data registers memory ADD Add source destination; register-memory format SUB Subtract source destination; register-memory format CMP Compare source destination; register-memory format SHL Shift left SHR Shift logical right RCR Rotate right carry fill CBW Convert byte AL word AX TEST Logical source destination sets flags INC Increment destination; register-memory format DEC Decrement destination; register-memory format Logical OR; register-memory format XOR Exclusive OR; register-memory format String instructions Move string operands; length given repeat prefix MOVS Copies string source destination; may repeated LODS Loads byte word string register Figure K.34 typical operations 80x86. Many operations use register-memory format, either source destination may memory may register immediate operand.K-38 ■Appendix K Survey Instruction Set ArchitecturesIntel supplemented stack architecture instructions addressing modes allow architecture benefits register-memory model. addition finding operands top two elements stack, oneoperand memory one seven registers top stack. hybrid still restricted register-memory model, however, loads always move data top stack incrementing top stack pointerand stores move top stack memory. Intel uses notation STto indicate top stack, ST(i) represent ith register top stack. One novel feature architecture operands wider reg- ister stack stored memory, operations performed wide internal precision. Numbers automatically converted internal 80-bitformat load converted back appropriate size store. Memorydata 32-bit (single-precision) 64-bit (double-precision) floating-pointnumbers, called realby Intel. register-memory version instructions convert memory operand Intel 80-bit format performingthe operation. data transfer instructions also automatically convert 16- and32-bit integers reals, vice versa , integer loads stores. 80x86 floating-point operations divided four major classes: 1.Data movement instructions, including load, load constant, store 2.Arithmetic instructions, including add, subtract, multiply, divide, square root, absolute value 3.Comparison, including instructions send result integer CPU branch 4.Transcendental instructions, including sine, cosine, log, exponentiation Figure K.35 shows 60 floating-point operations. use curly brackets {}to show optional variations basic operations: {I} means integer version instruction, {P} means variation pop one operand stack operation, {R} means reverse sense operands operation. combinations provided. Hence, F{I}SUB{R}{P} represents instructions found 80x86: FSUB FISUBFSUBRFISUBRFSUBP FSUBRPK.3 Intel 80x86 ■K-39There pop reverse pop versions integer subtract instructions. Note get even combinations including operand modes operations. floating-point add options, ignoring integerand pop versions instruction: FADD operands stack, result replaces top stack. FADD ST(i) One source operand ith register top stack, result replaces top stack. FADD ST(i),ST One source operand top stack, result replaces ith register top stack. FADD mem32 One source operand 32-bit location memory, result replaces top stack. FADD mem64 One source operand 64-bit location memory, result replaces top stack. mentioned earlier SSE2 presents model IEEE floating-point registers. 80x86 Instruction Encoding Saving worst last, encoding instructions 8086 complex, many different instruction formats. Instructions may vary 1 byte, operands, 6 bytes, instruction contains 16-bit immediateerapmoC citemhtirA refsnartataD Transcendental F{I}LD mem/ST(i) F{I}ADD{P}mem/ST(i) F{I}COM{P}{P} FPATAN F{I}ST{P} mem/ST(i) F{I}SUB{R}{P}mem/ST(i) F{I}UCOM{P}{P} F2XM1 SOCF FSTSW AX/mem I}MUL{P}mem/ST(i){F IPDLF NATPF F{I}DIV{R}{P}mem/ST(i) 1DLF MERPF TRQSF ZDLF INSF SBAF X2LYF FRNDINT Figure K.35 floating-point instructions 80x86. first column shows data transfer instructions, move data memory one registers top stack. last three operations pushconstants stack: pi, 1.0, 0.0. second column contains arithmetic operations described above. Note last three operate top stack. third column compare instructions. Since special floating-point branch instructions, result compare must transferred integer CPU via theFSTSW instruction, either AX register memory, followed SAHF instruction set conditioncodes. floating-point comparison tested using integer branch instructions. final column gives higher-level floating-point operations.K-40 ■Appendix K Survey Instruction Set Architecturesand uses 16-bit displacement addressing. Prefix instructions increase 8086 instruc- tion length beyond obvious sizes. 80386 additions expand instruction size even further, Figure K.36 shows. displacement immediate fields 32 bits long, two moreprefixes possible, opcode 16 bits long, scaled index mode spec-ifier adds another 8 bits. maximum possible 80386 instruction 17 bytes long. Figure K.37 shows instruction format several example instruc- tions Figure K.33 . opcode byte usually contains bit saying whether operand byte wide larger size, 16 bits 32 bits depending mode. instructions, opcode may include addressing mode reg- ister; true many instructions form register register op immediate . instructions use “postbyte ”or extra opcode byte, labeled “mod, reg, r/m ”inFigure K.36 , contains addres- sing mode information. postbyte used many instructions thataddress memory. based scaled index uses second postbyte, labeled“sc, index, base ”inFigure K.36 . floating-point instructions encoded escape opcode 8086 postbyte address specifier. memory operations reserve 2 bits decide Seg. override Opcode mod, reg, r/m Disp8 Disp16 Disp24 Imm8 Imm16Disp32 Imm24 Imm32Opcode ext. sc, index, baseAddr. override Size overridePrefixes Address specifiers Displacement ImmediateOpcodeRepeat Lock Figure K.36 instruction format 8086 (black type) extensions 80386 (shaded type). Every field optional except opcode.K.3 Intel 80x86 ■K-41whether operand 32- 64-bit real 16- 32-bit integer. 2 bits used versions access memory decide whether stackshould popped operation whether top stack lower reg-ister get result. Alas, cannot separate restrictions registers encoding addressing modes 80x86. Hence, Figures K.38 andK.39 show encoding two postbyte address specifiers 16- 32-bit mode.JE a. JE PC + displacement rebmun tnemgeS FLLAC Offset b. CALLF c. MOV BX, [DI + 45] PUSH d. PUSH SI ADD w e. ADD AX, #6765 SHLr-r postbytev/w f. SHL BX, 1 g. TEST DX, #42Reg44 8 68 861 61 8 2 53 41 6 1 3 Constant 62 8 71 8 8Condition Displacement tnemecalpsiD w/d VOMr-m postbyte TEST Postbyte Immediate wReg Figure K.37 Typical 8086 instruction formats. encoding postbyte shown inFigure K.38 .Many instructions contain 1-bit field w, says whether oper- ation byte word. Fields form v/w d/w d-field v-field followed bythe w-field. d-field MOV used instructions may move memory shows direction move. field v SHL instruction indicates variable-length shift; variable-length shifts use register hold shift count. TheADD instruction shows typical optimized short encoding usable first operand AX. Overall instructions may vary 1 6 bytes length.K-42 ■Appendix K Survey Instruction Set Architecturesw = 1 mod = 0 mod = 1 mod = 2 reg w = 0 16b 32b r/m 16b 32b 16b 32b 16b 32b mod = 3 0AL AX EAX 0 1CL CX ECX 1 2DL DX EDX 2 3BL BX EBX 3 4AH SP ESP 4 SI+disp16 (sib)+disp8 " 5CH BP EBP 5 DI+disp8 EBP+disp8 DI+disp16 " 6DH SI ESI 6 BP+di sp8 ESI+disp8 " 7BH DI EDI 7aaddr=BX+SI addr=BX+DIaddr=BP+SIaddr=BP+SI addr=SI ddr=DI addr=disp16 addr=BX=EDX =EBX =(si)b =disp32 =ESI =EDI BX+disp8 EDI+disp8SI+disp8 BP+disp16 BX+disp16(sib)+disp32 EBP+disp32 ESI+disp32 EDI+disp32 "same =ECX=EAX addr addr addr addr mod= 0 mod= 0 mod= 0 mod= 0 reg + disp 8 + disp 8 + disp1 6 + disp3 2 field Figure K.38 encoding first address specifier 80x86, mod, reg, r/m. first four columns show encoding 3-bit reg field, depends w bit opcode whether machine 16- 32- bit mode. remaining columns explain mod r/m fields. meaning 3-bit r/m field depends value 2-bit mod field address size. Basically, registers used address calculation listed thesixth seventh columns, mod ¼0, mod¼1 adding 8-bit displacement mod ¼2 adding 16- 32-bit displacement, depending address mode. exceptions r/m ¼6 mod¼1 mod¼2 16-bit mode selects BPplus displacement; r/m ¼5 mod¼1 mod¼2 32-bit mode selects EBP plus displace- ment; r/m¼4 32-bit mode mod ¦3 (sib) means use scaled index mode shown Figure K.39 . mod¼3, r/m field indicates register, using encoding reg field combined w bit. esaB xednI XAE XAE 0 XCE XCE 1 XDE XDE 2 XBE XBE 3 PSE xednioN 4 23psid,0=domfI PBE 5 mod ¦ 0, EBP ISE ISE 6 IDE IDE 7 Figure K.39 Based plus scaled index mode address specifier found 80386. mode indicated (sib) notation Figure K.38 . Note mode expands list registers used modes: Register indirect using ESP comes Scale ¼0, Index¼4, Base¼4, base displacement EBP comes Scale ¼0, Index¼5, mod¼0. two-bit scale field used formula effective address: Base register + 2Scale/C2Index register.Putting Together: Measurements Instruction Set Usage section, present detailed measurements 80x86 compare measurements MIPS programs. facilitate comparisonsamong dynamic instruction set measurements, use subset SPEC92 pro-grams. 80x86 results taken 1994 using Sun Solaris FORTRAN andC compilers V2.0 executed 32-bit mode. compilers comparablein quality compilers used MIPS. Remember measurements depend benchmarks chosen compiler technology used. Although feel measurements section reasonably indicative usage architectures, programs may behave differently benchmarks here, different compilers mayyield different results. real instruction set study, architect would wantto much larger set benchmarks, spanning wide application range aspossible, consider operating system usage instruction set.Single-user benchmarks like measured necessarily behave inthe fashion operating system. start evaluation features 80x86 isolation, later compare instruction counts DLX. Measurements 80x86 Operand Addressing start addressing modes. Figure K.40 shows distribution operand types 80x86. measurements cover “second ”operand oper- ation; example, mov EAX, [45] counts single memory operand. types first operand counted, percentage register usage would increase factor 1.5. 80x86 memory operands divided respective addressing modes Figure K.41 . Probably biggest surprise popularity Integer average FP average %54 retsigeR %61 etaidemmI%22 %6 %27 %93 yromeM Figure K.40 Operand type distribution average five SPECint92 programs (compress, eqntott, espresso, gcc, li) average five SPECfp92 programs(doduc, ear, hydro2d, mdljdp2, su2cor).K-44 ■Appendix K Survey Instruction Set Architecturesaddressing modes added 80386, last four rows figure. account half memory accesses. Another surprise popu-larity direct addressing. machines, equivalent directaddressing mode rare. Perhaps segmented address space 80x86 makes direct addressing useful, since address relative base address segment register. addressing modes largely determine size Intel instructions. Figure K.42 shows distribution instruction sizes. average number bytes per instruction integer programs 2.8, standard deviation of1.5, 4.1 standard deviation 1.9 floating-point programs. dif-ference length arises partly differences addressing modes: Integerprograms rely shorter register indirect 8-bit displacement addres- sing modes, floating-point programs frequently use 80386 addres- sing modes longer 32-bit displacements. Given floating-point instructions aspects stacks reg- isters, used? Figure K.43 shows that, least compilers used measurement, stack model execution rarely followed. (See Section L.3for historical explanation observation.) Finally, Figures K.44 andK.45 show instruction mixes 10 SPEC92 programs. Comparative Operation Measurements Figures K.46 andK.47 show number instructions executed 10 programs 80x86 ratio instruction execution compared thatAddressing mode Integer average FP average %31 tceridniretsigeR %13 .psidtib-8+esaB %9 .psidtib-23+esaB %0 dexednI %0 .psidtib-8+dexednidesaB %0 .psidtib-23+dexednidesaB %22 dexednidelacs+esaB Base + scaled indexed + 8-bit disp. 0% Base + scaled indexed + 32-bit disp. 4%%3 %51 %52 %0 %0 %1 %7 8% 4% %73 %02 tceridtib-23 Figure K.41 Operand addressing mode distribution program. chart include addressing modes used branches control instructions.K.3 Intel 80x86 ■K-45for DLX: Numbers less 1.0 mean 80x86 executes fewer instructions DLX. instruction count surprisingly close DLX many integerprograms, would expect load-store instruction set architecture likeDLX execute instructions register-memory architecture like the80x86. floating-point programs always higher counts 80x86,doduc ear hydro2d mdljdp2 su2cor FP average Stack (2nd operand ST (1 Register (2nd operand ST(i), )) 1.1% 0.0% 0.0% 0.2% 0.6% 0.4% > 1) 17.3% 63.4% 14.2% 7.1% 30.7% 26.5% %1.37 %7.86 %7.29 %8.58 %6.63 %6.18 yromeMOption Figure K.43 percentage instructions floating-point operations (add, sub, mul, div) use three options specifying floating-point operand 80x86. three options (1) strict stack model implicit operands stack, (2) register version naming explicit operand one top two elements stack, (3) memory operand.Percenta ge instructions len gthInstruction lengths11 10 9 8 7 6 5 4 3 2 10% 1%0% 0%0% 0%0% 0% 4% 2%Floating-point average Integer average 8%39% 4% 6% 7% 5% 18% 25% 19% 40% 10% 14% 0% 20% 40% 60% Figure K.42 Averages histograms 80x86 instruction lengths five SPE- Cint92 programs five SPECfp92 programs, running 32-bit mode.K-46 ■Appendix K Survey Instruction Set Architecturespresumably due lack floating-point registers use stack architecture. Another question total amount data traffic 80x86 versus DLX, since 80x86 specify memory operands part operations DLXcan access via loads stores. Figures K.46 K.47 also show datareads, data writes, data read-modify-writes 10 programs. totalInstruction doduc ear hydro2d mdljdp2 su2cor FP average %02 %6.72 %6.72 %0.81 %5.6 %9.8 daoL %8 %8.7 %8.7 %5.11 %1.3 %4.21 erotS %01 %8.8 %8.8 %6.41 %6.6 %4.5 ddA %3 %4.2 %4.2 %3.3 %4.2 %0.1 buS Mul 0% Div 0% %2 %0.1 %0.1 %8.0 %1.5 %8.1 erapmoC Mov reg-reg 3.2% 0.1% 1.8% 2.3% 2.3% 2% %0 %5.1 %4.0 mmidaoL Cond. branch 5.4% 8.2% 5.1% 2.7% 2.7% 5% Uncond branch 0.8% 0.4% 1.3% 0.3% 0.3% 1% %0 %1.0 %1.0 %6.1 %5.0 llaC %0 %1.0 %1.0 %6.1 %5.0 tceridnipmj,nruteR %2 %5.2 %5.2 %5.4 %1.1 tfihS 0.8% 0.8% 0.7% 1.3% 1.3% 1% %0 %1.0 %1.0 %1.0 ( XOR, not, . . .) 0% %41 %6.21 %6.21 %1.9 %5.22 %1.41 PFdaoL %7 %6.6 %6.6 %1.4 %4.11 %6.8 PFerotS %5 %6.6 %6.6 %4.1 %1.6 %8.5 PFddA %3 %9.2 %9.2 %1.3 %7.2 %2.2 PFbuS %9 %0.21 %0.21 %1.4 %0.8 %9.8 PFluM %0 %2.0 %2.0 %8.0 %1.2 PFviD Compare FP 9.4% 6.9% 10.8% 0.5% 0.5% 5% Mov reg-reg FP 2.5% 0.8% 0.3% 0.8% 0.8% 1%Other (abs, sqrt, . . .) 3.9% 3.8% 4.1% 0.8% 0.8% 2% Figure K.44 80x86 instruction mix five SPECfp92 programs.K.3 Intel 80x86 ■K-47accesses ratio DLX memory access type shown bottom rows, read-modify-write counting one read one write. 80x86 performs two four times many data accesses DLX forfloating-point programs, 1.25 times many integer programs. Finally,Figure K.48 shows percentage instructions category 80x86 DLX.Instruction compress eqntott espresso gcc (cc1) li Int. average %22 %3.32 %9.42 %9.12 %5.81 %8.02 daoL %21 %7.81 %6.61 %3.8 %2.3 %8.31 erotS %8 %1.6 %6.7 %51.8 %8.8 %3.01 ddA %5 %6.3 %9.2 %5.3 %6.01 %0.7 buS %0 %1.0 luM Div 0% Compare 8.2% 27.7% 15.3% 13.5% 7.7% 16% Mov reg-reg 7.9% 0.6% 5.0% 4.2% 7.8% 4% %0 %4.0 %6.0 %2.0 %5.0 mmidaoL Cond. branch 15.5% 28.6% 18.9% 17.4% 15.4% 20% Uncond. branch 1.2% 0.2% 0.9% 2.2% 2.2% 1% %1 %2.3 %5.1 %7.0 %4.0 %5.0 llaC Return, jmp indirect 0.5% 0.4% 0.7% 1.5% 3.2% 1% %1 %7.1 %5.2 %8.3 tfihS 8.4% 1.0% 8.7% 4.5% 8.4% 6% %1 %4.0 %4.0 %7.2 %6.0 ( XOR %1 %1.0 %2.2 %9.0 )...,ton, Load FP 0% Store FP 0% Add FP 0% Sub FP 0% Mul FP 0% Div FP 0% Compare FP 0% Mov reg-reg FP 0% (abs, sqrt, . . .) 0% Figure K.45 80x86 instruction mix five SPECint92 programs.K-48 ■Appendix K Survey Instruction Set ArchitecturesConcluding Remarks Beauty eye beholder. Old Adage seen, “orthogonal ”is term found Intel architectural dictio- nary. fully understand registers addressing modes avail-able, need see encoding addressing modes sometimes theencoding instructions.compress eqntott espresso gcc (cc1) li Int. avg. Instructions executed 80x86 (millions) 2226 1203 2216 3770 5020 Instructions executed ratio DLX 0.61 1.74 0.85 0.96 0.98 1.03 Data reads 80x86 (millions) 589 229 622 1079 1459 Data writes 80x86 (millions) 311 39 191 661 981Data read-modify-writes 80x86 (millions) 26 1 129 48 48 Total data reads 80x86 (millions) 615 230 751 1127 1507 01.1 49.0 52.1 83.1 90.1 58.0 XLDotoitardaerataD Total data writes 80x86 (millions) 338 40 319 709 1029 51.3 02.1 52.1 93.2 62.9 76.1 XLDotoitaretirwataD Total data accesses 80x86 (millions) 953 269 1070 1836 2536 32.1 30. 1 52.1 85.1 52.1 30.1 XLDotoitarsseccaataD Figure K.46 Instructions executed data accesses 80x86 ratios compared DLX five SPECint92 programs. doduc ear hydro2d mdljdp2 su2cor FP average Instructions executed 80x86 (millions) 1223 15,220 13,342 6197 6197 Instructions executed ratio DLX 1.19 1.19 2.53 2.09 1.62 1.73Data reads 80x86 (millions) 515 6007 5501 3696 3643 Data writes 80x86 (millions) 260 2205 2085 892 892 Data read-modify-writes 80x86 (millions) 1 0 189 124 124Total data reads 80x86 (millions) 517 6007 5690 3820 3767 15.3 19.3 77.4 84.4 63.2 40.2 XLDotoitardaerataD Total data writes 80x86 (millions) 261 2205 2274 1015 1015 3 XLDotoitaretirwataD .68 33.25 38.74 16.74 9.35 20.35 Total data accesses 80x86 (millions) 778 8212 7965 4835 4782 53.4 74.4 37.5 99.5 41.3 04.2 XLDotoit arsseccaataD Figure K.47 Instructions executed data accesses five SPECfp92 programs 80x86 ratio DLX.K.3 Intel 80x86 ■K-49Some argue inelegance 80x86 instruction set unavoidable, price must paid rampant success architecture. reject notion. Obviously, successful architecture jettison features addedin previous implementations, time features may seen unde-sirable. awkwardness 80x86 began core 8086 instructionset exacerbated architecturally inconsistent expansions 8087,80286, 80386. counterexample IBM 360/370 architecture, much older 80x86. dominates mainframe market 80x86 dominates PC market. Due undoubtedly better base compatible enhancements, instruction set makes much sense 80x86 30 years itsfirst implementation. better worse, Intel 16-bit microprocessor years compet- itors’more elegant architectures, head start led selection 8086 CPU IBM PC. lacks style made quantity, makingthe 80x86 beautiful right perspective. saving grace 80x86 architectural components difficult implement, Intel demonstrated rapidly improving perfor- mance integer programs since 1978. High floating-point performance largerchallenge architecture. K.4 VAX Architecture VAX: successful minicomputer design industry history . . . VAX probably hacker ’s favorite machine . . . . Especially noted large, assembler-programmer-friendly instruction set —an asset became liability RISC revolution. Eric Raymond New Hacker ’s Dictionary (1991)Integer average FP average Category x86 DLX x86 DLX Total data transfer 34% 36% 28% 2% Total integer arithmetic 34% 31% 16% 12% Total control 24% 20% 6% 10%Total logical 8% 13% 3% 2% Total FP data transfer 0% 0% 22% 33% Total FP arithmetic 0% 0% 25% 41% Figure K.48 Percentage instructions executed category 80x86 DLX averages five SPECint92 SPECfp92 programs Figures K.46 K.47.K-50 ■Appendix K Survey Instruction Set ArchitecturesIntroduction enhance understanding instruction set architectures, chose VAX representative Complex Instruction Set Computer (CISC) differ- ent MIPS yet still easy understand. seeing two divergent styles,we confident able learn instruction sets own. time VAX designed, prevailing philosophy create instruction sets close programming languages order simplifycompilers. example, programming languages loops, instructionsets loop instructions. VAX architect William Strecker said (“VAX-11/780 —A Virtual Address Extension PDP-11 Family, ”AFIPS Proc., National Computer Conference, 1978): major goal VAX-11 instruction set provide effective compiler generated code. Four decisions helped realize goal: 1) regular andconsistent treatment operators . . . . 2) avoidance instructions unlikely generated compiler . . . . 3) Inclusions several forms common operators ....4 ) Replacement common instruction sequences single instructions . . . . Examples include procedure calling, multiway branching, loopcontrol, array subscript calculation. Recall DRAMs mid-1970s contained less 1/1000th capacity today ’s DRAMs, code space also critical. Hence, another prevailing phi- losophy minimize code size, de-emphasized fixed-lengthinstruction sets like MIPS. example, MIPS address fields always use 16 bits,even address small. contrast, VAX allows instructions bea variable number bytes, little wasted space address fields. Whole books written VAX, VAX extension cannot exhaustive. Hence, following sections describe itsaddressing modes instructions. show VAX instructions action, later sections show VAX assembly code two C procedures. general style contrast instructions MIPS code already familiar with. differing goals VAX MIPS led different architectures. VAX goals, simple compilers code density, led powerful addressingmodes, powerful instructions, efficient instruction encoding. MIPS goalswere high performance via pipelining, ease hardware implementation, com-patibility highly optimizing compilers. MIPS goals led simple instruc-tions, simple addressing modes, fixed-length instruction formats, large number registers. VAX Operands Addressing Modes VAX 32-bit architecture, 32-bit-wide addresses 32-bit-wide reg- isters. Yet, VAX supports many data sizes types, Figure K.49 shows. Unfortunately, VAX uses name “word”to refer 16-bit quantities; text, word means 32 bits. Figure K.49 shows conversion betweenK.4 VAX Architecture ■K-51the MIPS data type names VAX names. careful reading VAX instructions, refer names VAX data types. VAX provides sixteen 32-bit registers. VAX assembler uses notation r 0 ,r 1 ,...,r 1 5 refer registers, stick notation. Alas, 4 16 registers effectively claimed instruction setarchitecture. example, r14 stack pointer ( sp) r15 program counter ( pc). Hence, r15 cannot used general-purpose register, using r14 difficult interferes instructions manipulate stack. dedicated registers r12, used argument pointer ( ap), andr13, used frame pointer ( fp); purpose become clear later. (Like MIPS, VAX assembler accepts either register number registername.) VAX addressing modes include discussed Appendix A, MIPS addressing modes: register, displacement, immediate , PC-relative . Moreover, modes used jump addresses data addresses. ’s addressing modes. reduce code size, VAX three lengths addresses displacement addressing: 8-bit, 16-bit, 32-bitaddresses called, respectively, byte displacement, word displacement , long displacement addressing. Thus, address small possible also large necessary; large addresses need split, equiv-alent MIPS lui instruction (see Figure A.24 page A-37). still VAX addressing modes. Several deferred option, meaning object addressed address real object, requiring another memory access get operand. addressing mode iscalled indirect addressing machines. Thus, register deferred, autoincre- ment deferred , byte/word/long displacement deferred addressing modes choose from. example, using notation VAX assembler,Bits Data type MIPS name VAX name 08 Integer Byte Byte 16 Integer Half word Word 32 Integer Word Long word 32 Floating point Single precision F_floating64 Integer Double word Quad word 64 Floating point Double precision D_floating G_floating 8n Character string Character Character Figure K.49 VAX data types, lengths, names. first letter VAX type (b, w, l, f, q, d, g, c) often used complete instruction name. Examples move instructions include movb, movw, movl, movf, movq, movd, movg, andmovc3 . move instruction transfers operand data type indicated letterfollowing mov.K-52 ■Appendix K Survey Instruction Set Architecturesr1 means operand register 1 ( r1) means operand location memory pointed r1. yet another addressing mode. Indexed addressing automatically con- verts value index operand proper byte address add rest ofthe address. 32-bit word, needed multiply index 4-byte quantityby 4 adding base address. Indexed addressing, called scaled addres- singon computers, automatically multiplies index 4-byte quantity 4 part address calculation. cope plethora addressing options, VAX architecture separates specification addressing mode specification operation. Hence, opcode supplies operation number oper-ands, operand addressing mode specifier. Figure K.50 shows name, assembler notation, example, meaning, length addressspecifier. VAX style addressing means operation ’t know operands come from; VAX add instruction three operands registers,three operands memory, combination registers memory operands. Addressing mode name Syntax Example MeaningLength address speciﬁer bytes Literal #value # –1 )eulavdengistib-6(1 1– ehtfohtgnel+1 001 001# eulav# etaidemmI immediate 1 3r 3r nr retsigeR Register deferred (rn) (r3) Memory[r3] 1 Byte/word/long displacementDisplacement (rn) 100(r3) Memory[r3 + 100] 1 + length displacement Byte/word/long displacement deferred@displacement (rn) @100(r3) Memory[Memory [r3 + 100]] 1 + length displacement Indexed (scaled) Base mode [rx] (r3)[r4] Memory[r3 + r4 d] (where data size bytes)1 + length base addressing mode Autoincrement (rn)+ (r3)+ Memory[r3]; r3 = r3 + 1 Autodecrement – (rn) –(r3) r3 = r3 – d; Memory[r3] 1 Autoincrement deferred @(rn)+ @(r3)+ Memory[Memory[r3]]; r3 = r3 + d1 Figure K.50 Definition length VAX operand specifiers. length addressing mode 1 byte plus length displacement immediate field needed mode. Literal mode uses special 2-bit tag remaining 6 bits encode constant value. constant big, must use immediate addressing mode.Note length immediate operand dictated length data type indicated opcode, value immediate. symbol din last four modes represents length data bytes; dis 4 32-bit add.K.4 VAX Architecture ■K-53Example long following instruction? addl3 r1,737(r2),(r3)[r4] name addl3 means 32-bit add instruction three operands. Assume length VAX opcode 1 byte. Answer first operand specifier —r1—indicates register addressing 1 byte long. second operand specifier —737(r2) —indicates displacement addressing two parts: first part byte specifies word displacementaddressing mode base register (r2) ; second part 2-byte-long dis- placement (737) . third operand specifier —(r3)[r4] —also two parts: first byte specifies register deferred addressing mode ((r3)) , second byte specifies Index register use indexed addressing ([r4]). Thus, thetotal length instruction 1 + (1) + (1 + 2) + (1 + 1) ¼7 bytes. example instruction, show VAX destination operand left source operands right, show MIPS code. VAX assembler actually expects operands opposite order, felt would less con- fusing keep destination left machines. Obviously, left rightorientation arbitrary; requirement consistency. Elaboration PC 1 16 registers selected VAX addressing mode, 4 22 VAX addressing modes synthesized addressing modes. Using PC chosen register case, immediate addressing isreally autoincrement, PC-relative displacement, absolute autoincrementdeferred, relative deferred displacement deferred. Encoding VAX Instructions Given independence operations addressing modes, encoding instructions quite different MIPS. VAX instructions begin single byte opcode containing operation number operands. operands follow opcode. operand beginswith single byte, called address specifier , describes addressing mode operand. simple addressing mode, register addressing, thisbyte specifies register number well mode (see rightmost columninFigure K.50 ). cases, initial byte followed many bytes specify rest address information. specific example, let ’s show encoding add instruction example page K-24: addl3 r1,737(r2),(r3)[r4] Assume instruction starts location 201. Figure K.51 shows encoding. Note operands stored memory opposite order assembly code above. execution VAX instructionsK-54 ■Appendix K Survey Instruction Set Architecturesbegins fetching source operands, makes sense come first. Order important fixed-length instructions like MIPS, since source anddestination operands easily found within 32-bit word. first byte, location 201, opcode. next byte, location 202, specifier index mode using register r4. Like many specifiers, left 4 bits specifier give mode right 4 bits give register used mode. Since addl3 4-byte operation, r4will multiplied 4 added whatever address specified next. case register deferredaddressing using register r3. Thus, bytes 202 203 combined define third operand assembly code. following byte, address 204, specifier word displacement addres- sing using register r2as base register. specifier tells VAX fol- lowing two bytes, locations 205 206, contain 16-bit address added r2. final byte instruction gives destination operand, specifier selects register addressing using register r1. variability addressing means single VAX operation many different lengths; example, integer add varies 3 bytes 19 bytes.VAX implementations must decode first operand find sec-ond, implementors strongly tempted take 1 clock cycle decode eachoperand; thus, sophisticated instruction set architecture result higherclock cycles per instruction, even using simple addresses. VAX Operations keeping philosophy, VAX large number operations wellas large number addressing modes. review give flavor ofthe machine. Given power addressing modes, VAX move instruction performs several operations found machines. transfers data twoaddressable locations subsumes load, store, register-register moves, andByte address Contents byte Machine code 1c 3lddagniniatnocedocpO 102hex 44 ]4r[rofreificepsedomxednI 202hex 203 Register indirect mode specifier (r3) 63hex 204 Word displacement mode specifier using r2 base c2hex 1e 737tnatsnoctib-61ehT 502hex 20 602hex 15 1rrofreificepsedomretsigeR 702hex Figure K.51 encoding VAX instruction addl3 r1,737(r2),(r3)[r4], assuming starts address 201. satisfy curiosity, right column shows actual VAX encoding hexadecimal notation. Note 16-bit constant 737 tentakes 2 bytes.K.4 VAX Architecture ■K-55memory-memory moves special cases. first letter VAX data type (b, w, l, f, q, d, g, c Figure K.49 ) appended acronym mov determine size data. One special move, called move address , moves 32-bit address operand rather data. uses acronym mova . arithmetic operations MIPS also found VAX, two major differences. First, type data attached name. Thus, addb, addw, andaddl operate 8-bit, 16-bit, 32-bit data memory registers, respec- tively; MIPS single add instruction operates full 32-bit reg-ister. second difference reduce code size add instruction specifies number unique operands; MIPS always specifies three even one operand redundant. example, MIPS instruction add $1, $1, $2 takes 32 bits like MIPS instructions, VAX instruction addl2 r1, r2 usesr1for destination source, taking 24 bits: 8 bits opcode 8 bits two register specifiers. Number Operations show VAX instruction names formed: operationðÞ datatypeðÞ2 3/C18/C19 operation add works data types byte, word, long, float, double comes versions either 2 3 unique operands, following instructionsare found VAX: addb2 addw2 addl2 addf2 addd2 addb3 addw3 addl3 addf3 addd3 Accounting addressing modes (but ignoring register numbers immediate values) limiting byte, word, long, 30,000 ver-sions integer add VAX; MIPS 4! Another reason large number VAX instructions instructions either replace sequences instructions take fewer bytes represent sin- gle instruction. four examples (* means data type): VAX operation Example Meaning clr* clrl r3 r3 = 0 inc* incl r3 r3 = r3+1 dec* decl r3 r3 = r3 −1 push* pushl r3 sp = sp −4; Memory[sp] = r3;K-56 ■Appendix K Survey Instruction Set ArchitecturesThe push instruction last row exactly using move instruc- tion autodecrement addressing stack pointer: movl –(sp), r3 Brevity advantage pushl: 1 byte shorter since sp implied. Branches, Jumps, Procedure Calls VAX branch instructions related arithmetic instructions branch instructions rely condition codes . Condition codes set side effect operation, indicate whether result positive, negative, zero overflow occurred. instructions set VAX condition codes accordingto result; instructions without results, branches, not. VAX con-dition codes N (Negative), Z (Zero), V (oVerflow), C (Carry). alsoacompare instruction cmp* set condition codes subsequent branch. VAX branch instructions include conditions. Popular branch instruc- tions include beql(=), bneq( 6¼), blss( <), bleq(/C20), bgtr( >), andbgeq(/C21), would expect. also unconditional branches whose name determined size PC-relative offset. Thus, brb (branch byte ) 8-bit displacement, brw (branch word ) 16- bit displacement. final major category cover procedure call return instruc- tions. Unlike MIPS architecture, elaborate instructions take dozens ofclock cycles execute. next two sections show work, need toexplain purpose pointers associated stack manipulated calls andret.T h e stack pointer ,sp, like stack pointer MIPS; points top stack. argument pointer ,ap, points base list arguments parameters memory passed procedure. frame pointer ,fp, points base local variables procedure kept memory (thestack frame ). VAX call return instructions manipulate pointers maintain stack proper condition across procedure calls provide conve-nient base registers use accessing memory operands. shall see, calland return also save restore general-purpose registers well programcounter. Figure K.52 gives sampling VAX instruction set. Example Put Together: swap see programming VAX assembly language, translate two C procedures, swap andsort . C code swap reproduced Figure K.53 . next section covers sort . describe swap procedure three general steps assembly language programming: 1.Allocate registers program variables. 2.Produce code body procedure. 3.Preserve registers across procedure invocation.K.4 VAX Architecture ■K-57gninaemnoitcurtsnI elpmaxE epytnoitcurtsnI n byte, half-word, word, double-word operands; * data type mov* Move two operands word, extending zeros operand; data type last integer logical bytes, half words (16 bits), words (32 bits); * data type size data type sehcnarblanoitidnocnudnalanoitidnoC lortnoC branch equal l, branch greater equal nd; branch result second operand case selector ents stack (see “A Longer Example: sort ” page K-33) RTRAN-style parameter list return address (like MIPS jal)Data transfers Move data betwee Arithmetic/logical Operations Procedure Call/return procedure Floating point Floating-point operations D, F, G, H formats format floating numbers n D-format floating numbers F-format floating point e coefficients F format snoitarepolaicepS rehtO redundancy checkmovzb* Move byte half word mova* Move 32-bit address push* Push operand onto stack add*_ Add 2 3 operands cmp* Compare set condition codes tst* Compare zero set condition codes ash* Arithmetic shift clr* Clear cvtb* Sign-extend byte beql, bneq Branch equal, bleq, bgeq Branch less equa brb, brw Unconditional branch 8-bit 16-bit address jmp Jump using addressing mode specify target aobleq Add one opera case_ Jump based calls Call procedure argum callg Call procedure FO jsb Jump subroutine, saving ret Return procedure call addd_ Add double-precision D- subd_ Subtract double-precisio mulf_ Multiply single-precisi polyf Evaluate polynomial using tabl crc Calculate cyclic insque Insert queue entry queue Figure K.52 Classes VAX instructions examples. asterisk stands multiple data types: b, w, l, d, f, g, h, q. underline, addd_ , means 2-operand ( addd2 ) 3-operand ( addd3 ) forms instruction.K-58 ■Appendix K Survey Instruction Set ArchitecturesThe VAX code procedures based code produced VMS C compiler using optimization. Register Allocation swap contrast MIPS, VAX parameters normally allocated memory, step assembly language programming properly called “variable alloca- tion.”The standard VAX convention parameter passing use stack. two parameters, v[] andk, accessed using register ap, argument pointer: address 4(ap) corresponds v[] and8(ap) corresponds k. Remember byte addressing address sequential 4-byte words differs 4. variable temp , associate register r3. Code Body Procedure swap remaining lines C code swap temp = v[k]; v[k] = v[k + 1];v[k + 1] = temp; Since program uses v[] andkseveral times, make programs run faster VAX compiler first moves parameters registers: movl r2, 4(ap) ;r2 = v[] movl r1, 8(ap) ;r1 = k Note follow VAX convention using semicolon start comment; MIPS comment symbol # represents constant operand VAX assemblylanguage.swap(int v[], int k) { int temp; temp = v[k]; v[k] = v[k + 1]; v[k + 1] = temp;} Figure K.53 C procedure swaps two locations memory. procedure used sorting example next section.K.4 VAX Architecture ■K-59The VAX indexed addressing, use index k without converting byte address. VAX code straightforward: movl r3, (r2)[r1] ;r3 (temp) = v[k] addl3 r0, #1,8(ap) ;r0 = k + 1 movl (r2)[r1],(r2)[r0] ;v[k] = v[r0] (v[k + 1]) movl (r2)[r0],r3 ;v[k + 1] = r3 (temp) Unlike MIPS code, basically two loads two stores, key VAX code one memory-to-register move, one memory-to-memory move, oneregister-to-memory move. Note addl3 instruction shows flexibility VAX addressing modes: adds constant 1 memory operand andplaces result register. allocated storage written code perform operations procedure. missing item code preserves registers acrossthe routine calls swap . Preserving Registers across Procedure Invocation swap VAX pair instructions preserve registers, calls andret. example shows work. VAX C compiler uses form callee convention. Examining code above, see values registers r0, r1, r2, andr3must saved later restored. calls instruction expects 16-bit mask beginning procedure determine registers saved: bit set mask, register iis saved stack calls instruction. addition, calls saves mask stack allow return instruction (ret) restore proper registers. Thus, calls executed caller saving, callee sets call mask indicate saved. One operands calls gives number parameters passed, calls adjust pointers associated stack: argument pointer ( ap), frame pointer ( fp), stack pointer ( sp). course, calls also saves program counter procedure return! Thus, preserve four registers swap, add mask beginning procedure, letting calls instruction caller work: .word ^m<r0,r1,r2,r3 >;set bits mask 0,1,2,3 directive tells assembler place 16-bit constant proper bits set save registers r0through r3. return instruction undoes work calls . finished, ret sets stack pointer current frame pointer pop everything calls placed stack. Along way, restores register values saved calls , including marked mask old values fp, ap, andpc.K-60 ■Appendix K Survey Instruction Set ArchitecturesTo complete procedure swap , add one instruction: ret ;restore registers return Full Procedure swap ready whole routine. Figure K.54 identifies block code purpose procedure, MIPS code left VAX codeon right. example shows advantage scaled indexed addressing sophisticated call return instructions VAX reducing num- ber lines code. 17 lines MIPS assembly code became 8 lines VAXassembly code. also shows passing parameters memory results extramemory accesses. Keep mind number instructions executed per- formance; fallacy page K-38 makes point. Note VAX software follows convention treating registers r0andr1 temporaries saved across procedure call, VMS C compiler include registers r0andr1in register saving mask. Also, C compiler used r1instead 8(ap) addl3 instruction; examples inspire computer architects try write compilers! MIPS versus VAX Saving register Procedure body Restoring registersswap: addi $29,$29, –12 sw $2, 0($29) sw $15, 4($29)sw $16, 8($29)swap: .word ^m<r0,r1,r2,r3> muli $2, $5,4 add $2, $4,$2lw $15, 0($2) lw $16, 4($2) sw $16, 0($2)sw $15, 4($2)movl r2, 4(a) movl r1, 8(a) movl r3, (r2)[r1] addl3 r0, #1,8(ap) movl (r2)[r1],(r2)[r0]movl (r2)[r0],r3 lw $2, 0($29) lw $15, 4($29) lw $16, 8($29) addi $29,$29, 12 Procedure return ret $31jr Figure K.54 MIPS versus VAX assembly code procedure swap Figure K.53 page K-30.K.4 VAX Architecture ■K-61A Longer Example: sort show longer example sort procedure. Figure K.55 shows C ver- sion program. present procedure several steps, con-cluding side-by-side comparison MIPS code. Register Allocation sort two parameters procedure sort, v n, found stack loca- tions 4(ap) 8(ap), respectively. two local variables assigned regis-ters: r6 j r4. two parameters referenced frequently thecode, VMS C compiler copies address parameters registers upon entering procedure: moval r7,8(ap) ;move address n r7 moval r5,4(ap) ;move address v r5 would seem moving value operand register would useful address, bow decision VMS C com- piler. Apparently compiler cannot sure v n ’t overlap memory. Code Body sort Procedure procedure body consists two nested forloops call swap, includes parameters. Let ’s unwrap code outside middle. Outer Loop first translation step first loop: (i = 0; <n ;i=i+1 ){ Recall C statement three parts: initialization, loop test, iteration increment. takes one instruction initialize ito0, first part statement: clrl r6 ;i = 0 sort (int v[], int n) { int i, j;for (i = 0; < n; = + 1) {for (j = – 1; j >= 0 && v[ j] > v[j + 1]; j = j – 1) { swap(v,j); } } } Figure K.55 C procedure performs bubble sort array v.K-62 ■Appendix K Survey Instruction Set ArchitecturesIt also takes one instruction increment i, last part for: incl r6 ;i = + 1 loop exited i<nisfalse, said another way, exit loop i/C21n. test takes two instructions: for1tst: cmpl r6,(r7) ;compare r6 memory[r7] (i:n) bgeq exit1 ;go exit1 r6 /C21mem[r7] (i/C21n) Note cmpl sets condition codes use conditional branch instruction bgeq . bottom loop jumps back loop test: brb for1tst ;branch test outer loop exit1: skeleton code first loop clrl r6 ;i = 0 for1tst: cmpl r6,(r7) ;compare r6 memory[r7] (i:n) bgeq exit1 ;go exit1 r6 /C21mem[r7] (i/C21n) ...(body first loop)... incl r6 ;i =i+1 brb for1tst ;branch test outer loop exit1: Inner Loop second loop (j = –1; j>= 0 && v[j] >v[j + 1]; j = j –1) { initialization portion loop one instruction: subl3 r4,r6,#1 ;j = –1 decrement j also one instruction: decl r4 ;j = j –1 loop test two parts. exit loop either condition fails, first test must exit loop fails (j<0): for2tst:blss exit2 ;go exit2 r4 <0( j<0) Notice explicit comparison. lack comparison benefit condition codes, conditions set side effect prior instruc-tion. branch skips second condition test. second test exits v[j]>v[j + 1] false, exits v[j]/C20v[j + 1] . First load vand put j+1 registers: movl r3,(r5) ;r3 = Memory[r5] (r3 = v) addl3 r2,r4,#1 ;r2 = r4 + 1 (r2 = j + 1)K.4 VAX Architecture ■K-63Register indirect addressing used get operand pointed r5. index addressing mode means use indices without con- verting byte address, two instructions v[j]/C20v[j + 1] cmpl (r3)[r4],(r3)[r2] ;v[r4] : v[r2] (v[j]:v[j + 1]) bleq exit2 ;go exit2 v[j] /C20v[j + 1] bottom loop jumps back full loop test: brb for2tst # jump test inner loop Combining pieces, second loop looks like this: subl3 r4,r6, #1 ;j = –1 for2tst: blss exit2 ;go exit2 r4 <0( j<0) movl r3,(r5) ;r3 = Memory[r5] (r3 = v)addl3 r2,r4,#1 ;r2 = r4 + 1 (r2 = j + 1)cmpl (r3)[r4],(r3)[r2];v[r4] : v[r2]bleq exit2 ;go exit2 v[j] ð [j+1] ... (body second loop) ... decl r4 ;j = j –1 brb for2tst ;jump test inner loop exit2: Notice instruction blss (at top loop) testing condition codes based new value r4 (j) , set either subl3 entering loop decl bottom loop. Procedure Call next step body second loop: swap(v,j); Calling swap easy enough: calls #2,swap constant 2 indicates number parameters pushed stack. Passing Parameters C compiler passes variables stack, pass parameters swap two instructions: pushl (r5) ;first swap parameter v pushl r4 ;second swap parameter j Register indirect addressing used get operand first instruction. Preserving Registers across Procedure Invocation sort remaining code saving restoring registers using callee save convention. procedure uses registers r2through r7, add mask bits set:K-64 ■Appendix K Survey Instruction Set Architectures.word ^m<r2,r3,r4,r5,r6,r7 >; set mask registers 2-7 Since ret undo operations, tack end procedure. Full Procedure sort put pieces together Figure K.56 . make code easier fol- low, identify block code purpose procedureand list MIPS VAX code side side. example, 11 lines sortprocedure C become 44 lines MIPS assembly language 20 lines inVAX assembly language. biggest VAX advantages register saving andrestoring indexed addressing. Fallacies Pitfalls ability simplify means eliminate unnecessary necessary may speak. Hans Hoffman Search Real (1967) Fallacy possible design flawless architecture. architecture design involves trade-offs made context set hardware software technologies. time technologies likely change, decisions may correct one time later look like mistakes. exam- ple, 1975 VAX designers overemphasized importance code size effi-ciency underestimated important ease decoding pipelining wouldbe 10 years later. And, almost architectures eventually succumb lack ofsufficient address space. Avoiding problems long run, however, wouldprobably mean compromising efficiency architecture short run. Fallacy architecture flaws cannot successful. IBM 360 often criticized literature —the branches PC-relative, address small displacement addressing. Yet, machine beenan enormous success correctly handled several new problems. First, thearchitecture large amount address space. Second, byte addressed andhandles bytes well. Third, general-purpose register machine. Finally, sim-ple enough efficiently implemented across wide performance cost range. Intel 8086 provides even dramatic example. 8086 architecture widespread architecture existence today truly general- purpose register machine. Furthermore, segmented address space 8086 causes major problems programmers compiler writers. Never-theless, 8086 architecture —because selection microprocessor IBM PC —has enormously successful.K.4 VAX Architecture ■K-65MIPS versus VAX Saving registers sort: addi $29,$29, –36 sw $15, 0($29)sw $16, 4($29) sw $17, 8($29) sw $18,12($29)sw $19,16($29) sw $20,20($29) sw $24,24($29) sw $25,28($29) sw $31,32($29)sort: .word ^m<r2,r3,r4,r5,r6,r7> Procedure body Move parameters move $18, $4 move $20, $5moval r7,8(ap) moval r5,4(ap) Outer loop add $19, $0, $0 for1tst: slt $8, $19, $20 beq $8, $0, exit1clrl r6 for1tst: cmpl r6,(r7) bgeq exit1 $17, $19, –1addi poolrennI for2tst: slti $8, $17, 0 bne $8, $0, exit2 muli $15, $17, 4 add $16, $18, $15 lw $24, 0($16) lw $25, 4($16) slt $8, $25, $24 beq $8, $0, exit2for2tst:subl3 r4,r6,#1 blss exit2 movl r3,(r5) addl3 r2,r4,#1 cmpl (r3)[r4],(r3)[r2] bleq exit2 Pass parameters callmove $4, $18move $5, $17 jal swappushl (r5) pushl r4 calls #2,swap poolrennI Outer loop exit2: addi $19, $19, 1$17, $17, –1addi j for2tstdecl r4 brb for2tst j for1tstexit2: incl r6 brb for1tst Restoring registers exit1: lw $15,0($29) lw $16, 4($29) lw $17, 8($29) lw $18,12($29) lw $19,16($29) lw $20,20($29) lw $24,24($29) lw $25,28($29) lw $31,32($29) addi $29,$29, 36 Procedure return ret exit1: $31jr Figure K.56 MIPS32 versus VAX assembly version procedure sortinFigure K.55 page K-33.Fallacy architecture executes fewer instructions faster. Designers VAX machines performed quantitative comparison VAX MIPS implementations comparable organizations, VAX 8700 andthe MIPS M2000. Figure K.57 shows ratio number instructions exe- cuted ratio performance measured clock cycles. MIPS executes twice many instructions VAX MIPS M2000 almost three times performance VAX 8700. Concluding Remarks Virtual Address eXtension PDP-11 architecture …provides virtual address 4.3 gigabytes which, even given rapid improvement mem-ory technology, adequate far future. William Strecker “VAX-11/780 —A Virtual Address Extension PDP-11 Family, ” AFIPS Proc., National Computer Conference (1978) seen instruction sets vary quite dramatically, access operands operations performed single instruction. Figure K.58 compares instruction usage architectures two programs; even different architectures behave similarly use instruction classes.33.54 2.5 2 1.5 1 0.5 0 spice matrix nasa7 fppppInstructions executed Performance tomcatv doduc espresso eqntott liMIPS/VAX Number bits displacement Figure K.57 Ratio MIPS M2000 VAX 8700 instructions executed perfor- mance clock cycles using SPEC89 programs. average, MIPS executes little twice many instructions VAX, CPI VAX almost six times MIPS CPI, yielding almost threefold performance advantage. (Based data “Per- formance Architecture: Comparing RISC CISC Similar Hardware Orga-nization, ”by D. Bhandarkar D. Clark, Proc. Symp. Architectural Support Programming Languages Operating Systems IV , 1991.)K.4 VAX Architecture ■K-67A product time, VAX emphasis code density complex oper- ations addressing modes conflicts current emphasis easy decoding, simple operations addressing modes, pipelined performance. 600,000 sold, VAX architecture successful run. 1991, DEC made transition VAX Alpha. Orthogonality key VAX architecture; opcode independent addressing modes, independent data types even number ofunique operands. Thus, hundred operations expand hundreds thousands ofinstructions accounting data types, operand counts, addressing modes. Exercises K.1 [3]<K.4>The following VAX instruction decrements location pointed register r5: decl (r5) single MIPS instruction, cannot represented singleinstruction, shortest sequence MIPS instructions, performs sameoperation? lengths instructions machine? K.2 [5]<K.4>This exercise Exercise K.1, except VAX instruction clears location using autoincrement deferred addressing: clrl @(r5)+ K.3 [5]<K.4>This exercise Exercise K.1, except VAX instruction adds 1 register r5, placing sum back register r5, compares sum reg- ister r6, branches L1 r5 <r6: aoblss r6, r5, L1 # r5 = r5 + 1; (r5 <r6) goto L1 . K.4 [5]<K.4>Show single VAX instruction, minimal sequence instructions, C statement: = b + 100; Assume corresponds register r3 b corresponds register r4. K.5 [10]<K.4>Show single VAX instruction, minimal sequence instruc- tions, C statement: x[i + 1] = x[i] + c; Assume c corresponds register r3, register r4, x array 32-bitwords beginning memory location 4,000,000 ten.Program Machine BranchArithmetic/ logicalData transferFloating point Totals gcc V AX 30% 40% 19% 89% MIPS 24% 35% 27% 86% spice V AX 18% 23% 15% 23% 79% MIPS 04% 29% 35% 15% 83% Figure K.58 frequency instruction distribution two programs VAX MIPS.K-68 ■Appendix K Survey Instruction Set ArchitecturesK.5 IBM 360/370 Architecture Mainframe Computers Introduction term “computer architecture ”was coined IBM 1964 use IBM 360. Amdahl, Blaauw, Brooks [1964] used term refer programmer-visible portion instruction set. believed family machines architecture able run software. Although idea may seem obvious us today, quite novel time. IBM, even though theleading company industry, five different architectures 360. Thus,the notion company standardizing single architecture radical one. The360 designers hoped six different divisions IBM could brought together bydefining common architecture. definition architecture …the structure computer c h n el n g u g ep r g r e rm u understand write correct (timing independent) program machine. term “machine language programmer ”meant compatibility would hold, even assembly language, “timing independent ”allowed different implementations. IBM 360 introduced 1964 six models 25:1 performance ratio. Amdahl, Blaauw, Brooks [1964] discussed architecture IBM 360 concept permitting multiple object-code-compatible implementa- tions. notion instruction set architecture understand todaywas important aspect 360. architecture also introduced severalimportant innovations, wide use: 1.32-bit architecture 2.Byte-addressable memory 8-bit bytes 3.8-, 16-, 32-, 64-bit data sizes 4.32-bit single-precision 64-bit double-precision floating-point data 1971, IBM shipped first System/370 (models 155 165), included number significant extensions 360, discussed Case Padegs[1978], also discussed early history System/360. important addi-tion virtual memory, though virtual memory 370 ship 1972, whena virtual memory operating system ready. 1978, high-end 370 several hundred times faster low-end 360 shipped 10 years earlier. 1984, 24- bit addressing model built IBM 360 needed abandoned, 370-XA(eXtended Architecture) introduced. old 24-bit programs could sup-ported without change, several instructions could function mannerwhen extended 32-bit addressing model (31-bit addresses supported) becausethey would produce 31-bit addresses. Converting operating system, whichwas written mostly assembly language, doubt biggest task. Several studies IBM 360 instruction measurement made. Shustek ’s thesis [1978] best known complete study 360/370 architecture. made several observations instruction set complexity thatK.5 IBM 360/370 Architecture Mainframe Computers ■K-69were fully appreciated years later. Another important study 360 Toronto study Alexander Wortman [1975] done IBM 360 using 19 XPL programs. System/360 Instruction Set 360 instruction set shown following tables, organized instructiontype format. System/370 contains 15 additional user instructions. Integer/Logical Floating-Point R-R Instructions * indicates instruction floating point, may either (double pre-cision) E (single precision). noitpircseD noitcurtsnI retsigerlacigolddA ALR retsigerddA AR noitiddaPF A*R retsigerlacigolerapmoC CLR retsigererapmoC CR erapmocPF C*R retsigerediviD DR edividPF D*R evlahPF H*R retsigertnemelpmocdaoL LCR tnemelpmocdaoL LC*R retsigerevitagendaoL LNR evitagendaoL LN*R retsigerevitisopdaoL LPR evitisopdaoL LP*R retsigerdaoL LR retsigerPFdaoL L*R retsigertsetdnadaoL LTR retsigerPFtsetdnadaoL LT*R retsigerylpitluM MR ylpitlumPF M*R retsigerdnA NR retsigerrO ltcartbuS SLR ogical register retsigertcartbuS SR noitcartbusPF S*R retsigerroevisulcxE XRK-70 ■Appendix K Survey Instruction Set ArchitecturesBranches Status Setting R-R Instructions R-R format instructions either branch set system status; sev- eral privileged legal supervisor mode. noitpircseD noitcurtsnI BALR Branch link BCTR Branch count BCR Branch/condition ISK Insert key SPM Set program mask SSK Set storage key SVC Supervisor call Branches/Logical Floating-Point Instructions —RX Format RX format instructions. symbol “+”means either word oper- ation (and stands nothing) H (meaning half word); example, A+ stands two opcodes AandAH. “*”represents DorE, standing double- single-precision floating point. noitpircseD noitcurtsnI A+ Add A* FP add AL Add logical C+ Compare C* FP compare CL Compare logical ediviD D* FP divide L+ Load L* Load FP register M+ Multiply M* FP multiply dnA N rO S+ Subtract S* FP subtract SL Subtract logical ST+ Store ST* Store FP register roevisulcxE XK.5 IBM 360/370 Architecture Mainframe Computers ■K-71Branches Special Loads Stores —RX Format noitpircseD noitcurtsnI knil dna hcnarB BAL noitidnoc hcnarB BC tnuoc hcnarB BCT yranib-trevnoC CVB lamiced-trevnoC CVD etucexE EX retcarahc tresnI IC sserdda daoL LA retcarahc erotS STC RS SI Format Instructions RS SI format instructions. symbol “*”may A(arith- metic) L(logical). noitpircseD noitcurtsnI hgih/hcnarB BXH lauqe-wol/hcnarB BXLE igol erapmoC CLI cal immediate O/I tlaH HIO WSP daoL LPSW elpitlum daoL LM etaidemmi evoM MVI etaidemmi dnA NI etaidemmi rO OI tcerid daeR RDD O/I tratS SIO L/A tfel tfihS SL* L/A elbuod tfel tfihS SLD* L/A thgir tfihS SR* L/A elbuod thgir tfihS SRD* ksam metsys teS SSM elpitlum erotS STM lennahc tseT TCH O/I tseT TIO ksam rednu tseT TM tes-dna-tseT TS tcerid etirW WRD etaidemmi ro evisulcxE XIK-72 ■Appendix K Survey Instruction Set ArchitecturesSS Format Instructions add decimal string instructions. noitpircseD noitcurtsnI dekcapddA AP srahclacigolerapmoC CLC dekcaperapmoC CP dekcapediviD DP tidE ED kramdnatidE EDMK dekcapylpitluM MP retcarahcevoM MVC ciremunevoM MVN tesffohtiwevoM MVO enozevoM MVZ sretcarahcdnA NC sretcarahcrO OC r → decimal) etcarahC(kcaP PACK dekcaptcartbuS SP etalsnarT TR tsetdnaetalsnarT TRT kcapnU UNPK sretcarahcroevisulcxE XC dekcapddadnaoreZ ZAP 360 Detailed Measurements Figure K.59 shows frequency instruction usage four IBM 360 programs.K.5 IBM 360/370 Architecture Mainframe Computers ■K-73Instruction PLIC FORTGO PLIGO COBOLGO Average %61 %61 %5 %31 %23 lortnoC %51 %41 %5 %31 %82 BC, BCR %1 %2 %3 BAL, BALR %01 %12 %71 %3 A, AR %3 %7 %3 SR %2 %3 %6 SLL %2 %1 %1 %8 LA %2 %7 CLI NI 7% 2% %3 %0 %4 %4 %5 C %2 %3 %1 %3 TM %1 %2 MHArithmetic/logical 29% 35% 29% 9% 26% %33 %02 %65 %04 %71 refsnartataD %91 %91 %82 %32 %7 L, LR %5 %1 %61 %2 MVI %3 %7 %3 ST %2 %2 %7 LD %2 %2 %7 STD %1 %3 LPDR %1 %3 LH %1 %2 IC %0 %1 LTR %2 %7 tniopgnitaolF %1 %3 AD %1 %3 MDR %11 %04 %4 gnirts,lamiceD %3 %7 %4 MVC AP 11% 3% ZAP 9% 2% CVD 5% 1% MP 3% 1% CLC 3% 1% CP 2% 1% ED 1% 0% %88 %58 %09 %59 %28 latoT Figure K.59 Distribution instruction execution frequencies four 360 programs. instructions fre- quency execution greater 1.5% included. Immediate instructions, operate single byte, included section characterized operation, rather long character-string versions operation. comparison, average frequencies major instruction classes VAX 23% (con-trol), 28% (arithmetic), 29% (data transfer), 7% (floating point), 9% (decimal). again, 1% entry aver- age column occur entries constituent columns. programs compiler programming language PL-I runtime systems programming languages FORTRAN, PL/I, Cobol.K.6 Historical Perspective References Section L.4 (available online) features discussion evolution instruction sets includes references reading exploration related topics. Acknowledgments would like thank following people comments drafts survey:Professor Steven B. Furber, University Manchester; Dr. Dileep Bhandarkar,Intel Corporation; Dr. Earl Killian, Silicon Graphics/MIPS; Dr. HiokazuTakata, Mitsubishi Electric Corporation.Acknowledgments ■K-75L Advanced Concepts Address Translation Abhishek Bhattacharjee Appendix L available online https://www.elsevier.com/books/computer- architecture/hennessy/978-0-12-811905-1M.1 Introduction M-2 M.2 Early Development Computers (Chapter 1) M-2 M.3 Development Memory Hierarchy Protection (Chapter 2 Appendix B) M-9 M.4 Evolution Instruction Sets (Appendices A, J, K) M-17 M.5 Development Pipelining Instruction-Level Parallelism (Chapter 3 Appendices C H) M-27 M.6 Development SIMD Supercomputers, Vector Computers, Multimedia SIMD Instruction Extensions, GraphicalProcessor Units (Chapter 4) M-45 M.7 History Multiprocessors Parallel Processing (Chapter 5 Appendices F, G, I) M-55 M.8 Development Clusters (Chapter 6) M-74 M.9 Historical Perspectives References M-79 M.10 History Magnetic Storage, RAID, I/O Buses (Appendix D) M-84M Historical Perspectives References If…history…teaches us anything, man quest knowledge progress determined cannot deterred. John F. Kennedy Address Rice University (1962) cannot remember past condemned repeat it. George Santayana Life Reason (1905), Vol. 2, Chapter 3M.1 Introduction appendix provides historical background key ideas presented chapters. may trace development idea series ofmachines describe significant projects. interested examining theinitial development idea machine interested reading,references provided end section. Section M.2 starts us invention digital computer corre- sponds Chapter 1 .Section M.3 , memory hierarchy, corresponds Chapter 2 andAppendix B .Section M.4 , instruction set architecture, covers Appendices A, J, K. Section M.5 , pipelining instruction-level parallelism, corre- sponds Chapter 3 Appendices C H. Section M.6 , data-level paral- lelism vector, SIMD, GPU architectures, corresponds Chapter 4 . Section M.7 , multiprocessors parallel programming, covers Chapter 5 Appendices F, G, I. Section M.8 , development clusters, covers Chapter 6 . Finally, Section M.9 , I/O, corresponds Appendix D. M.2 Early Development Computers ( Chapter 1 ) historical section, discuss early development digital computers development performance measurement methodologies. First General-Purpose Electronic Computers J. Presper Eckert John Mauchly Moore School University ofPennsylvania built world ’s first fully operational electronic general-purpose computer. machine, called ENIAC (Electronic Numerical Integrator Calculator), funded U.S. Army became operational WorldWar II, publicly disclosed 1946. ENIAC used comput-ing artillery firing tables. machine enormous —100 feet long, 8 ½feet high, several feet wide. 20 ten-digit registers 2 feet long.In total, 18,000 vacuum tubes. Although size three orders magnitude bigger size average machines built today, five orders magnitude slower, add taking 200 microseconds. ENIAC provided conditional jumps programmable, clearly distinguished earlier calculators.Programming done manually plugging cables setting switchesand required half hour whole day. Data provided punchedcards. ENIAC limited primarily small amount storage tediousprogramming. 1944, John von Neumann attracted ENIAC project. group wanted improve way programs entered discussed storing programs numbers; von Neumann helped crystallize ideas wrote memo propos- ing stored-program computer called EDVAC (Electronic Discrete VariableM-2 ■Appendix Historical Perspectives ReferencesAutomatic Computer). Herman Goldstine distributed memo put von Neu- mann ’s name it, much dismay Eckert Mauchly, whose names omitted. memo served basis commonly used term von Neumann computer . Several early inventors computer field believe term gives much credit von Neumann, conceptualized wrote theideas, little engineers, Eckert Mauchly, worked themachines. Like historians, authors (winners 2000 IEEE vonNeumann Medal) believe three individuals played key role developingthe stored-program computer. Von Neumann ’s role writing ideas, gen- eralizing them, thinking programming aspects critical trans- ferring ideas wider audience. 1946, Maurice Wilkes Cambridge University visited Moore School attend latter part series lectures developments electronic com-puters. returned Cambridge, Wilkes decided embark projectto build stored-program computer named EDSAC (Electronic Delay StorageAutomatic Calculator). (The EDSAC used mercury delay lines memory;hence, phrase “delay storage ”in name.) EDSAC became operational 1949 world ’s first full-scale, operational, stored-program computer [Wilkes, Wheeler, Gill 1951; Wilkes 1985, 1995]. (A small prototype calledthe Mark I, built University Manchester ran 1948, mightbe called first operational stored-program machine.) EDSAC anaccumulator-based architecture. style instruction set architecture remainedpopular early 1970s. ( Appendix starts brief summary EDSAC instruction set.) 1947, Mauchly took time help found Association Computing Machinery. served ACM ’s first vice-president second president. year, Eckert Mauchly applied patent electronic computers.The dean Moore School, demanding patent turned theuniversity, may helped Eckert Mauchly conclude leave.Their departure crippled EDVAC project, become operationaluntil 1952. Goldstine left join von Neumann Institute Advanced Study Princeton 1946. Together Arthur Burks, issued report based 1944 memo [Burks, Goldstine, von Neumann 1946]. paper led IAS machine built Julian Bigelow Princeton ’s Institute Advanced Study. total 1024 40-bit words roughly 10 times faster ENIAC. Thegroup thought uses machine, published set reports, encouragedvisitors. reports visitors inspired development number newcomputers, including first IBM computer, 701, based theIAS machine. paper Burks, Goldstine, von Neumann incrediblefor period. Reading today, would never guess landmark paper written 50 years ago, architectural concepts seen modern computers discussed (e.g., see quote beginning Chapter 2 ). time period ENIAC, Howard Aiken designing electro- mechanical computer called Mark-I Harvard. Mark-I built teamM.2 Early Development Computers ■M-3of engineers IBM. followed Mark-I relay machine, Mark-II, pair vacuum tube machines, Mark-III Mark-IV. Mark-III Mark-IV built first stored-program machines. theyhad separate memories instructions data, machines regarded asreactionary advocates stored-program computers. term Harvard architecture coined describe type machine. Though clearly different original sense, term used today apply machines singlemain memory separate instruction data caches. Whirlwind project [Redmond Smith 1980] began MIT 1947 aimed applications real-time radar signal processing. Although led several inventions, overwhelming innovation creation magnetic corememory, first reliable inexpensive memory technology. Whirlwind had2048 16-bit words magnetic core. Magnetic cores served main memorytechnology nearly 30 years. Important Special-Purpose Machines World War II, major computing efforts Great Britain UnitedStates focused special-purpose code-breaking computers. work GreatBritain aimed decrypting messages encoded German Enigma codingmachine. work, occurred location called Bletchley Park, led twoimportant machines. first, electromechanical machine, conceived AlanTuring, called BOMB [see Good Metropolis, Howlett, Rota 1980]. Thesecond, much larger electronic machine, conceived designed Newman Flowers, called COLOSSUS [see Randall Metropolis, Howlett, Rota 1980]. highly specialized cryptanalysis machines, played vitalrole war providing ability read coded messages, especially thosesent U-boats. work Bletchley Park highly classified (indeed, someof still classified), direct impact development ENIAC, EDSAC,and computers difficult trace, certainly indirect effect inadvancing technology gaining understanding issues. Similar work special-purpose computers cryptanalysis went United States. direct descendent effort company Engineer- ing Research Associates (ERA) [see Thomash Metropolis, Howlett, Rota1980], founded war attempt commercialize keyideas. ERA built several machines sold secret government agencies,and eventually purchased Sperry-Rand, earlier purchased theEckert Mauchly Computer Corporation. Another early set machines deserves credit group special- purpose machines built Konrad Zuse Germany late 1930s early 1940s [see Bauer Zuse Metropolis, Howlett, Rota 1980]. addition producing operating machine, Zuse first implement floating point,which von Neumann claimed unnecessary! early machines used amechanical store smaller electromechanical solutions theM-4 ■Appendix Historical Perspectives Referencestime. last machine electromechanical but, war, never completed. important early contributor development electronic computers John Atanasoff, built small-scale electronic computer early 1940s [Ata-nasoff 1940]. machine, designed Iowa State University, special-purposecomputer (called ABC, Atanasoff Berry Computer) nevercompletely operational. Mauchly briefly visited Atanasoff built ENIAC,and several Atanasoff ’s ideas (e.g., using binary representation) likely influenced Mauchly. presence Atanasoff machine, delays filing ENIAC pat- ents (the work classified, patents could filed war), distribution von Neumann ’s EDVAC paper used break Eckert – Mauchly patent [Larson 1973]. Though controversy still rages Atanasoff ’s role, Eckert Mauchly usually given credit building first working,general-purpose, electronic computer [Stern 1980]. Atanasoff, however, demon-strated several important innovations included later computers. Atanasoffdeserves much credit work, might fairly given credit world ’s first special-purpose electronic computer possibly influencing Eckert Mauchly. Commercial Developments December 1947, Eckert Mauchly formed Eckert-Mauchly Computer Corporation. first machine, BINAC, built Northrop wasshown August 1949. financial difficulties, Eckert-Mauchly Com-puter Corporation acquired Remington-Rand, later called Sperry-Rand.Sperry-Rand merged Eckert-Mauchly acquisition, ERA, tabulatingbusiness form dedicated computer division, called UNIVAC. UNIVAC deliv-ered first computer, UNIVAC I, June 1951. UNIVAC sold for$250,000 first successful commercial computer —48 systems built! Today, early machine, along many fascinating pieces com- puter lore, seen Computer History Museum Mountain View,California. places early computing systems visited includethe Deutsches Museum Munich Smithsonian Institution Washington,D.C., well numerous online virtual museums. IBM, earlier punched card office automation busi- ness, ’t start building computers 1950. first IBM computer, IBM 701 based von Neumann ’s IAS machine, shipped 1952 eventually sold 19 units [see Hurd Metropolis, Howlett, Rota 1980]. early 1950s, many people pessimistic future computers, believing themarket opportunities “highly specialized ”machines quite lim- ited. Nonetheless, IBM quickly became successful computer company.Their focus reliability customer- market-driven strategies key.Although 701 702 modest successes, IBM ’s follow-up machines, 650, 704, 705 (delivered 1954 1955) significant successes,each selling 132 1800 computers.M.2 Early Development Computers ■M-5Several books describing early days computing written pioneers [Goldstine 1972; Wilkes 1985, 1995], well Metropolis, Howlett, Rota [1980], collection recollections early pioneers. arenumerous independent histories, often built around people involved [Slater1987], well journal, Annals History Computing , devoted history computing. Development Quantitative Performance Measures: Successes Failures earliest days computing, designers set performance goals —ENIAC 1000 times faster Harvard Mark-I, IBM Stretch (7030) tobe 100 times faster fastest machine existence. ’t clear, though, performance measured. looking back overthe years, consistent theme generation computers obsoletesthe performance evaluation techniques prior generation. original measure performance time perform individual oper- ation, addition. Since instructions took execution time, thetiming one gave insight others. execution times instructions ina machine became diverse, however, time one operation longeruseful comparisons. take differences account, instruction mix calculated measuring relative frequency instructions computeracross many programs. Gibson mix [Gibson 1970] early popularinstruction mix. Multiplying time instruction times weight mix gave user average instruction execution time . (If measured clock cycles, average instruction execution time average cycles per instruc-tion.) Since instruction sets similar, accurate comparison thanadd times. average instruction execution time, then, small stepto MIPS (as seen, one inverse other). MIPS virtueof easy layperson understand. CPUs became sophisticated relied memory hierarchies pipelining, longer single execution time per instruction; MIPS could calculated mix manual. next step benchmarking using kernels synthetic programs. Curnow Wichmann [1976] created theWhetstone synthetic program measuring scientific programs written Algol60. program converted FORTRAN widely used character-ize scientific program performance. effort similar goals Whetstone, theLivermore FORTRAN Kernels, made McMahon [1986] researchers atLawrence Livermore Laboratory attempt establish benchmark super-computers. kernels, however, consisted loops real programs. became clear using MIPS compare architectures different instruction sets would work, notion relative MIPS created. Whenthe VAX-11/780 ready announcement 1977, DEC ran small benchmarksthat also run IBM 370/158. IBM marketing referred 370/158 aM-6 ■Appendix Historical Perspectives References1 MIPS computer, and, programs ran speed, DEC market- ing called VAX-11/780 1 MIPS computer. Relative MIPS machine defined based reference machine as: MIPS M¼Performance Performance reference/C2MIPS reference popularity VAX-11/780 made popular reference machine relative MIPS, especially since relative MIPS 1 MIPS computer easy calculate: Ifa machine five times faster VAX-11/780, benchmark ratingwould 5 relative MIPS. 1 MIPS rating unquestioned 4 years, untilJoel Emer DEC measured VAX-11/780 time-sharing load. foundthat VAX-11/780 native MIPS rating 0.5. Subsequent VAXes ran 3native MIPS benchmarks therefore called 6 MIPS machines ran six times faster VAX-11/780. early 1980s, term MIPS almost universally used mean relative MIPS. 1970s 1980s marked growth supercomputer industry, defined high performance floating-point-intensive programs. Averageinstruction time MIPS clearly inappropriate metrics industry,hence invention MFLOPS (millions floating-point operations persecond), effectively measured inverse execution time bench-mark. Unfortunately, customers quickly forget program used rating, marketing groups decided start quoting peak MFLOPS supercomputer performance wars. SPEC (System Performance Evaluation Cooperative) founded late 1980s try improve state benchmarking make valid basisfor comparison. group initially focused workstations servers theUNIX marketplace, remain primary focus benchmarks today.The first release SPEC benchmarks, called SPEC89, substantialimprovement use realistic benchmarks. SPEC2006 still dominates processor benchmarks almost two decades later. References Amdahl, G. M. [1967]. “Validity single processor approach achieving large scale computing capabilities, ”Proc. AFIPS Spring Joint Computer Conf. , April 18 –20, 1967, Atlantic City, N.J., 483 –485. Atanasoff, J. V. [1940]. “Computing machine solution large systems linear equations, ”Internal Report, Iowa State University, Ames. Azizi, O., Mahesri, A., Lee, B. C., Patel, S. J., & Horowitz, M. [2010]. Energy- performance tradeoffs processor architecture circuit design: marginalcost analysis. Proc. International Symposium Computer Architecture, 26-36. Bell, C. G. [1984]. “The mini micro industries, ”IEEE Computer 17:10 (Octo- ber), 14 –30. Bell, C. G., J. C. Mudge, J. E. McNamara [1978]. DEC View Computer Engineering , Digital Press, Bedford, Mass.M.2 Early Development Computers ■M-7Burks, A. W., H. H. Goldstine, J. von Neumann [1946]. “Preliminary discussion logical design electronic computing instrument, ”Report U.S. Army Ordnance Department, p. 1; also appears Papers John von Neumann , W. Aspray A. Burks, eds., MIT Press, Cambridge, Mass., Tomash Publishers, Los Angeles, Calif., 1987, 97 –146. Curnow, H. J., B. A. Wichmann [1976]. “A synthetic benchmark, ”The Computer J. 19:1, 43 –49. Dally, William J., “High Performance Hardware Machine Learning, ”Cadence Embedded Neural Network Summit, February 9, 2016. http://ip.cadence. com/uploads/presentations/1000AM_Dally_Cadence_ENN.pdf Flemming, P. J., J. J. Wallace [1986]. “How lie statistics: cor- rect way summarize benchmarks results, ”Communications ACM 29:3 (March), 218 –221. Fuller, S. H., W. E. Burr [1977]. “Measurement evaluation alternative computer architectures, ”Computer 10:10 (October), 24 –35. Gibson, J. C. [1970]. “The Gibson mix, ”Rep. TR. 00.2043, IBM Systems Devel- opment Division, Poughkeepsie, N.Y. (research done 1959). Goldstine, H. H. [1972]. Computer: Pascal von Neumann , Princeton University Press, Princeton, N.J. Gray, J., C. van Ingen [2005]. Empirical Measurements Disk Failure Rates Error Rates , MSR-TR-2005-166, Microsoft Research, Redmond, Wash. Jain, R. [1991]. Art Computer Systems Performance Analysis: Techniques Experimental Design, Measurement, Simulation, Modeling , Wiley, New York. Kembel, R. [2000]. “Fibre Channel: comprehensive introduction, ”Internet Week (April). Larson, E. R. [1973]. “Findings fact, conclusions law, order judg- ment, ”File No. 4-67, Civ. 138, Honeywell v. Sperry-Rand Illinois Scientific Development , U.S. District Court State Minnesota, Fourth Division (October 19). Lubeck, O., J. Moore, R. Mendez [1985]. “A benchmark comparison three supercomputers: Fujitsu VP-200, Hitachi S810/20, Cray X-MP/2, ” Computer 18:12 (December), 10 –24. Landstrom, B. [2014]. “The Cost Downtime, ”http://www.interxion.com/blogs/ 2014/07/the-cost-of-downtime/ McMahon, F. M. [1986]. Livermore FORTRAN Kernels: Computer Test Numerical Performance Range , Tech. Rep. UCRL-55745, Lawrence Livermore National Laboratory, University California, Livermore. Metropolis, N., J. Howlett, G. C. Rota, eds. [1980]. History Computing Twentieth Century , Academic Press, New York. Mukherjee S. S., C. Weaver, J. S. Emer, S. K. Reinhardt, T. M. Austin [2003]. “Measuring architectural vulnerability factors, ”IEEE Micro 23:6, 70 –75. Oliker, L., A. Canning, J. Carter, J. Shalf, S. Ethier [2004]. “Scientific com- putations modern parallel vector systems, ”Proc. ACM/IEEE Conf. Supercomputing , November 6 –12, 2004, Pittsburgh, Penn., 10.M-8 ■Appendix Historical Perspectives ReferencesPatterson, D. [2004]. “Latency lags bandwidth, ”Communications ACM 47:10 (October), 71 –75. Redmond, K. C., T. M. Smith [1980]. Project Whirlwind —The History Pioneer Computer , Digital Press, Boston. Shurkin, J. [1984]. Engines Mind: History Computer , W. W. Norton, New York. Slater, R. [1987]. Portraits Silicon , MIT Press, Cambridge, Mass. Smith, J. E. [1988]. “Characterizing computer performance single number, ” Communications ACM 31:10 (October), 1202 –1206. SPEC. [1989]. SPEC Benchmark Suite Release 1.0 (October 2). SPEC. [1994]. SPEC Newsletter (June). Stern, N. [1980]. “Who invented first electronic digital computer? ”Annals History Computing 2:4 (October), 375 –376. Touma, W. R. [1993]. Dynamics Computer Industry: Modeling Supply Workstations Components , Kluwer Academic, Boston. Weicker, R. P. [1984]. “Dhrystone: synthetic systems programming bench- mark, ”Communications ACM 27:10 (October), 1013 –1030. Wilkes, M. V. [1985]. Memoirs Computer Pioneer , MIT Press, Cambridge, Mass. Wilkes, M. V. [1995]. Computing Perspectives , Morgan Kaufmann, San Francisco. Wilkes, M. V., D. J. Wheeler, S. Gill [1951]. Preparation Programs Electronic Digital Computer , Addison-Wesley, Cambridge, Mass. M.3 Development Memory Hierarchy Protection (Chapter 2 Appendix B ) Although pioneers computing knew need memory hierarchy coined term, automatic management two levels first proposedby Kilburn et al. [1962]. demonstrated Atlas computer theUniversity Manchester. computer appeared year IBM 360was announced. Although IBM planned introduction next genera-tion (System/370), operating system TSS challenge 1970.Virtual memory announced 370 family 1972, com-puter term translation lookaside buffer coined [Case Padegs 1978]. computers today without virtual memory supercom- puters, embedded processors, older personal computers. Atlas IBM 360 provided protection pages, GE 645 first system provide paged segmentation. earlier Burroughs computers provided virtual memory us ing segmentation, similar seg- mented address scheme Intel 8086. 80286, first 80x86 havethe protection mechanisms described Appendix C , inspired Multics protection software ran GE 645. time, computersM.3 Development Memory Hierarchy Protection ■M-9evolved elaborate mechanisms. elaborate mechanism capa- bilities , attracted greatest interest late 1970s early 1980s [Fabry 1974; Wulf, Levin, Harbison 1981]. Wilkes [1982], one earlyworkers capabilities, say: Anyone concerned implementation type described [capability system], tried explain one others, likely feel complexity got hand. particularly disappointing theattractive idea capabilities tickets freely handed aroundhas become lost …. Compared conventional computer system, inevitably cost met providing system domains protection small andfrequently changed. cost manifest terms additional hardware,decreased runtime speed, increased memory occupancy. present open question whether, adoption capability approach, cost reduced reasonable proportions. [p. 112] Today little interest capabilities either operating systems computer architecture communities, despite growing interest protection andsecurity. Bell Strecker [1976] reflected PDP-11 identified small address space architectural mistake difficult recover from. timeof creation PDP-11, core memories increasing slow rate. addition, competition 100 minicomputer companies meant DEC might cost-competitive product every address go throughthe 16-bit data path twice, hence architect ’s decision add 4 address bits found predecessor PDP-11. architects IBM 360 aware importance address size planned architecture extend 32 bits address. 24 bits usedin IBM 360, however, low-end 360 models would evenslower larger addresses 1964. Unfortunately, architects ’t reveal plans software people, programmers stored extra information upper 8 “unused ”address bits foiled expansion effort. (Apple made similar mistake 20 years later 24-bit address Motorola 68000, whichrequired procedure later determine “32-bit clean ”programs Macintosh later 68000s used full 32-bit virtual address.) Virtually every computersince check make sure unused bits stay unused trap bitshave wrong value. mentioned text, system virtual machines pioneered IBM part investigation virtual memory. IBM ’s first computer virtual memory IBM 360/67, introduced 1967. IBM researchers wrote theprogram CP-67 created illusion several independent 360 computers.They wrote interactive, single-user operating system called CMS thatran virtual machines. CP-67 led product VM/370, todayIBM sells z/VM mainframe computers [Meyer Seawright 1970;Van Vleck 2005].M-10 ■Appendix Historical Perspectives ReferencesA years Atlas paper, Wilkes published first paper describing concept cache [1965]: use discussed fast core memory of, say, 32,000 words slave slower core memory of, say, one million words way practical casesthe effective access time nearer fast memory slow memory. [p. 270] two-page paper describes direct-mapped cache. Although first publication caches, first implementation probably direct-mappedinstruction cache built University Cambridge. based tunnel diodememory, fastest form memory available time. Wilkes stated G.Scarott suggested idea cache memory. Subsequent publication, IBM started project led first com- mercial computer cache, IBM 360/85 [Liptay 1968]. Gibson [1967] described measure program behavior memory traffic well miss rate showed miss rate varies programs. Using sample 20 pro-grams (each 3 million references!), Gibson also relied average memoryaccess time compare systems without caches. precedent morethan 40 years old, yet many used miss rates early 1990s. Conti, Gibson, Pitkowsky [1968] described resulting performance 360/85. 360/91 outperforms 360/85 3 11 programsin paper, even though 360/85 slower clock cycle time (80 ns versus 60 ns), less memory interleaving (4 versus 16), slower main memory (1.04 microsecond versus 0.75 microsecond). paper also first use termcache . Others soon expanded cache literature. Strecker [1976] published first comparative cache design paper examining caches PDP-11. Smith [1982]later published thorough survey paper used terms spatial locality temporal locality ; paper served reference many computer designers. Although studies relied simulations, Clark [1983] used hardware monitor record cache misses VAX-11/780 several days. Clarkand Emer [1985] later compared simulations hardware measurements fortranslations. Hill [1987] proposed three C ’s used Appendix B explain cache misses. Jouppi [1998] retrospectively said Hill ’s three C ’s model led directly invention victim cache take advantage faster direct-mappedcaches yet avoid cost conflict misses. Sugumar Abraham [1993] argued baseline cache three C ’s model use optimal replacement; would eliminate anomalies least recently used(LRU)-based miss classification allow conflict misses broken intothose caused mapping caused nonoptimal replacementalgorithm. One first papers nonblocking caches Kroft [1981]. Kroft [1998] later explained first design computer cache atM.3 Development Memory Hierarchy Protection ■M-11Control Data Corporation, using old concepts new mechanisms hit upon idea allowing two-ported cache continue service accesses miss. Baer Wang [1988] one first examinations multilevel inclu- sion property. Wang, Baer, Levy [1989] produced early paper per-formance evaluation multilevel caches. Later, Jouppi Wilton [1994]proposed multilevel exclusion multilevel caches chip. addition victim caches, Jouppi [1990] also examined prefetching via streaming buffers. work extended Farkas, Jouppi, Chow [1995] streaming buffers work well nonblocking loads speculative exe- cution in-order processors, later Farkas et al. [1997] showed that, out-of-order processors tolerate unpredictable latency better, still benefit.They also refined memory bandwidth demands stream buffers. Proceedings Symposium Architectural Support Compilers Operating Systems (ASPLOS) International Computer Architecture Sym-posium (ISCA) 1990s filled papers caches. (In fact, somewags claimed ISCA really stood International Cache Architecture Symposium.) Chapter 2 relies measurements SPEC2000 benchmarks collected Cantin Hill [2001]. several papers used Chapter 2 cited captions figures use data: Agarwal Pudar[1993]; Barroso, Gharachorloo, Bugnion [1998]; Farkas Jouppi [1994];Jouppi [1990]; Lam, Rothberg, Wolf [1991]; Lebeck Wood [1994];McCalpin [2005]; Mowry, Lam, Gupta [1992]; Torrellas, Gupta, andHennessy [1992]. References Agarwal, A. [1987]. “Analysis Cache Performance Operating Systems Multiprogramming, ”Ph.D. thesis, Tech. Rep. No. CSL-TR-87-332, Stanford University, Palo Alto, Calif. Agarwal, A., S. D. Pudar [1993]. “Column-associative caches: technique reducing miss rate direct-mapped caches, ”20th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 16 –19, 1993, San Diego, Calif. (Computer Architecture News 21:2 (May), 179 –190). Baer, J.-L., W.-H. Wang [1988]. “On inclusion property multi-level cache hierarchies, ”Proc. 15th Annual Int ’l. Symposium Computer Architec- ture (ISCA) , May 30 –June 2, 1988, Honolulu, Hawaii, 73 –80. Barham, P., B. Dragovic, K. Fraser, S. Hand, T. Harris, A. Ho, R. Neugebauer [2003]. “Xen art virtualization, ”Proc. 19th ACM Symposium Operating Systems Principles , October 19 –22, 2003, Bolton Landing, N.Y. Barroso, L. A., K. Gharachorloo, E. Bugnion [1998]. “Memory system char- acterization commercial workloads, ”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 3 –14.M-12 ■Appendix Historical Perspectives ReferencesBell, C. G., W. D. Strecker [1976]. “Computer structures: learned PDP-11? ”Proc. Third Annual Int ’l. Symposium Computer Architecture (ISCA) , January 19 –21, 1976, Tampa, Fla., 1 –14. Bhandarkar, D. P. [1995]. Alpha Architecture Implementations , Digital Press, Newton, Mass. Borg, A., R. E. Kessler, D. W. Wall [1990]. “Generation analysis long address traces, ”Proc. 17th Annual Int ’l. Symposium Computer Archi- tecture (ISCA) , May 28 –31, 1990, Seattle, Wash., 270 –279. Cantin, J. F., M. D. Hill [2001]. “Cache performance selected SPEC CPU2000 benchmarks, ” http://www.cs.wisc.edu/multifacet/misc/ spec2000cache-data/ . Cantin, J., M. Hill [2003]. “Cache performance SPEC CPU2000 bench- marks, version 3.0, ”http://www.cs.wisc.edu/multifacet/misc/spec2000cache- data/index.html . Case, R. P., A. Padegs [1978]. “The architecture IBM System/370, ” Communications ACM 21:1, 73 –96. Also appears D. P. Siewiorek, C. G. Bell, A. Newell, Computer Structures: Principles Examples , McGraw-Hill, New York, 1982, 830 –855. Clark, B., T. Deshane, E. Dow, S. Evanchik, M. Finlayson, J. Herne, J. Neefe Matthews [2004]. “Xen art repeated research, ”Proc. USENIX Annual Technical Conf. , June 27 –July 2, 2004, Boston, 1135 –1144. Clark, D. W. [1983]. “Cache performance VAX-11/780, ”ACM Trans. Computer Systems 1:1, 24 –37. Clark, D. W., J. S. Emer [1985]. “Performance VAX-11/780 translation buffer: Simulation measurement, ”ACM Trans. Computer Systems 3:1 (February), 31 –62. Compaq Computer Corporation. [1999]. Compiler Writer ’s Guide Alpha 21264 , Order Number EC-RJ66A-TE, June. Conti, C., D. H. Gibson, S. H. Pitkowsky [1968]. “Structural aspects System/360 Model 85. Part I. General organization, ”IBM Systems J. 7:1, 2 –14. Crawford, J., P. Gelsinger [1988]. Programming 80386 , Sybex, Alameda, Calif. Cvetanovic, Z., R. E. Kessler [2000]. “Performance analysis Alpha 21264-based Compaq ES40 system, ”Proc. 27th Annual Int ’l. Sympo- sium Computer Architecture (ISCA) , June 10 –14, 2000, Vancouver, Canada, 192–202. Fabry, R. S. [1974]. “Capability based addressing, ”Communications ACM 17:7 (July), 403 –412. Farkas, K. I., P. Chow, N. P. Jouppi, Z. Vranesic [1997]. “Memory-system design considerations dynamically-scheduled processors, ”Proc. 24th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo., 133 –143. Farkas, K. I., N. P. Jouppi [1994]. “Complexity/performance trade-offs non-blocking loads, ”Proc. 21st Annual Int ’l. Symposium Computer Archi- tecture (ISCA) , April 18 –21, 1994, Chicago.M.3 Development Memory Hierarchy Protection ■M-13Farkas, K. I., N. P. Jouppi, P. Chow [1995]. “How useful non-blocking loads, stream buffers speculative execution multiple issue processors? ” Proc. First IEEE Symposium High-Performance Computer Architecture , January 22 –25, 1995, Raleigh, N.C., 78 –89. Gao, Q. S. [1993]. “The Chinese remainder theorem prime memory system, ”20th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 16 –19, 1993, San Diego, Calif. ( Computer Architecture News 21:2 (May), 337 –340). Gee, J. D., M. D. Hill, D. N. Pnevmatikatos, A. J. Smith [1993]. “Cache per- formance SPEC92 benchmark suite, ”IEEE Micro 13:4 (August), 17 –27. Gibson, D. H. [1967]. “Considerations block-oriented systems design, ”AFIPS Conf. Proc. 30, 75 –80. Handy, J. [1993]. Cache Memory Book , Academic Press, Boston. Heald, R., K. Aingaran, C. Amir, M. Ang, M. Boland, A. Das, P. Dixit, G. Goulds- berry, J. Hart, T. Horel, W.-J. Hsu, J. Kaku, C. Kim, S. Kim, F. Klass, H. Kwan,R. Lo, H. McIntyre, A. Mehta, D. Murata, S. Nguyen, Y.-P. Pai, S. Patel, K.Shin, K. Tam, S. Vishwanthaiah, J. Wu, G. Yee, H. [2000]. “Imple- mentation third-generation SPARC V9 64-b microprocessor, ”ISSCC Digest Technical Papers , 412 –413 slide supplement. Hill, M. D. [1987]. “Aspects Cache Memory Instruction Buffer Perfor- mance, ”Ph.D. thesis, Tech. Rep. UCB/CSD 87/381, Computer Science Divi- sion, University California, Berkeley. Hill, M. D. [1988]. “A case direct mapped caches, ”Computer 21:12 (Decem- ber), 25 –40. Horel, T., G. Lauterbach [1999]. “UltraSPARC-III: Designing third- generation 64-bit performance, ”IEEE Micro 19:3 (May –June), 73 –85. Hughes, C. J., P. Kaul, S. V. Adve, R. Jain, C. Park, J. Srinivasan [2001]. “Variability execution multimedia applications implications architecture, ”Proc. 28th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 30 –July 4, 2001, Goteborg, Sweden, 254 – 265. IEEE. [2005]. “Intel virtualization technology, computer, ”IEEE Computer Society 38:5 (May), 48 –56. Jouppi, N. P. [1990]. “Improving direct-mapped cache performance addi- tion small fully-associative cache prefetch buffers, ”Proc. 17th Annual Int’l. Symposium Computer Architecture (ISCA) , May 28 –31, 1990, Seattle, Wash., 364 –373. Jouppi, N. P. [1998]. “Retrospective: Improving direct-mapped cache performance addition small fully-associative cache prefetch buffers, ”in G. S. Sohi, ed., 25 Years International Symposia Computer Architec- ture (Selected Papers) , ACM, New York, 71 –73. Jouppi, N. P., S. J. E. Wilton [1994]. “Trade-offs two-level on-chip cach- ing,”Proc. 21st Annual Int ’l. Symposium Computer Architecture (ISCA) , April 18 –21, 1994, Chicago, 34 –45. Kessler, R. E. [1999]. “The Alpha 21264 microprocessor, ”IEEE Micro 19:2 (March/April), 24 –36.M-14 ■Appendix Historical Perspectives ReferencesKilburn, T., D. B. G. Edwards, M. J. Lanigan, F. H. Sumner [1962]. “One-level storage system, ”IRE Trans. Electronic Computers EC-11 (April) 223 –235. Also appears D. P. Siewiorek, C. G. Bell, A. Newell, Computer Structures: Principles Examples , McGraw-Hill, New York, 1982, 135 –148. Kroft, D. [1981]. “Lockup-free instruction fetch/prefetch cache organization, ” Proc. Eighth Annual Int ’l. Symposium Computer Architecture (ISCA) , May 12 –14, 1981, Minneapolis, Minn., 81 –87. Kroft, D. [1998]. “Retrospective: Lockup-free instruction fetch/prefetch cache organization, ”in G. S. Sohi, ed., 25 Years International Symposia Computer Architecture (Selected Papers) , ACM, New York, 20 –21. Kunimatsu, A., N. Ide, T. Sato, Y. Endo, H. Murakami, T. Kamei, M. Hirano, F. Ishihara, H. Tago, M. Oka, A. Ohba, T. Yutaka, T. Okada, M. Suzuoki[2000]. “Vector unit architecture emotion synthesis, ”IEEE Micro 20:2 (March –April), 40 –47. Lam, M. S., E. E. Rothberg, M. E. Wolf [1991]. “The cache performance optimizations blocked algorithms, ”Proc. Fourth Int ’l. Conf. Archi- tectural Support Programming Languages Operating Systems (ASPLOS) , April 8 –11, 1991, Santa Clara, Calif. ( SIGPLAN Notices 26:4 (April), 63 –74). Lebeck, A. R., D. A. Wood [1994]. “Cache profiling SPEC bench- marks: case study, ”Computer 27:10 (October), 15 –26. Liptay, J. S. [1968]. “Structural aspects System/360 Model 85. Part II. cache, ”IBM Systems J. 7:1, 15 –21. Luk, C.-K., T. C Mowry [1999]. “Automatic compiler-inserted prefetching pointer-based applications, ”IEEE Trans. Computers , 48:2 (February), 134 – 141. McCalpin, J. D. [2005]. “STREAM: Sustainable Memory Bandwidth High Per- formance Computers, ” www.cs.virginia.edu/stream/ . McFarling, S. [1989]. “Program optimization instruction caches, ”Proc. Third Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , April 3 –6, 1989, Boston, 183 –191. Menon, A., J. Renato Santos, Y. Turner, G. Janakiraman, W. Zwaenepoel [2005]. “Diagnosing performance overheads xen virtual machine environ- ment, ”Proc. First ACM/USENIX Int ’l. Conf. Virtual Execution Environ- ments , June 11 –12, 2005, Chicago, 13 –23. Meyer, R. A., L. H. Seawright [1970]. “A virtual machine time sharing sys- tem,”IBM Systems J. 9:3, 199 –218. Mowry, T. C., S. Lam, A. Gupta [1992]. “Design evaluation compiler algorithm prefetching, ”Proc. Fifth Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 12 –15, 1992, Boston ( SIGPLAN Notices 27:9 (September), 62 –73). Oka, M., M. Suzuoki [1999]. “Designing programming emotion engine, ”IEEE Micro 19:6 (November –December), 20 –28. Pabst, T. [2000]. “Performance Showdown 133 MHz FSB —The Best Platform Coppermine, ”www6.tomshardware.com/mainboard/00q1/000302/ .M.3 Development Memory Hierarchy Protection ■M-15Palacharla, S., R. E. Kessler [1994]. “Evaluating stream buffers secondary cache replacement, ”Proc. 21st Annual Int ’l. Symposium Computer Architec- ture (ISCA) , April 18 –21, 1994, Chicago, 24 –33. Przybylski, S. A. [1990]. Cache Design: Performance-Directed Approach , Morgan Kaufmann, San Francisco. Przybylski, S. A., M. Horowitz, J. L. Hennessy [1988]. “Performance trade- offs cache design, ”Proc. 15th Annual Int ’l. Symposium Computer Archi- tecture (ISCA) , May 30 –June 2, 1988, Honolulu, Hawaii, 290 –298. Reinman, G., N. P. Jouppi. [1999]. “Extensions CACTI. ” Robin, J., C. Irvine [2000]. “Analysis Intel Pentium ’s ability support secure virtual machine monitor, ”Proc. USENIX Security Symposium , August 14–17, 2000, Denver, Colo. Saavedra-Barrera, R. H. [1992]. “CPU Performance Evaluation Execution Time Prediction Using Narrow Spectrum Benchmarking, ”Ph.D. dissertation, University California, Berkeley. Samples, A. D., P. N. Hilfinger [1988]. Code Reorganization Instruction Caches , Tech. Rep. UCB/CSD 88/447, University California, Berkeley. Sites, R. L. (ed.) [1992]. Alpha Architecture Reference Manual , Digital Press, Burlington, Mass. Skadron, K., D. W. Clark [1997]. “Design issues tradeoffs write buffers, ”Proc. Third Int ’l. Symposium High-Performance Computer Archi- tecture , February 1 –5, 1997, San Antonio, Tex., 144 –155. Smith, A. J. [1982]. “Cache memories, ”Computing Surveys 14:3 (September), 473–530. Smith, J. E., J. R. Goodman [1983]. “A study instruction cache organiza- tions replacement policies, ”Proc. 10th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1982, Stockholm, Sweden, 132 –137. Stokes, J. [2000]. “Sound Vision: Technical Overview Emotion Engine, ”http://arstechnica.com/hardware/reviews/2000/02/ee.ars . Strecker, W. D. [1976]. “Cache memories PDP-11? ”Proc. Third Annual Int’l. Symposium Computer Architecture (ISCA) , January 19 –21, 1976, Tampa, Fla., 155 –158. Sugumar, R. A., S. G. Abraham [1993]. “Efficient simulation caches optimal replacement applications miss characterization, ”Proc. ACM SIGMETRICS Conf. Measurement Modeling Computer Systems , May 17 –21, 1993, Santa Clara, Calif., 24 –35. Tarjan, D., S. Thoziyoor, N. Jouppi [2006]. CACTI 4.0. Technical Report HPL-2006-86, HP Laboratories. Torrellas, J., A. Gupta, J. Hennessy [1992]. “Characterizing caching synchronization performance multiprocessor operating system, ”Proc. Fifth Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , October 12 –15, 1992, Boston ( SIGPLAN Notices 27:9 (September), 162 –174). Van Vleck, T. [2005]. “The IBM 360/67 CP/CMS, ”http://www.multicians. org/thvv/360-67.html .M-16 ■Appendix Historical Perspectives ReferencesWang, W.-H., J.-L. Baer, H. M. Levy [1989]. “Organization performance two-level virtual-real cache hierarchy, ”Proc. 16th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 28 –June 1, 1989, Jerusalem, 140 –148. Wilkes, M. [1965]. “Slave memories dynamic storage allocation, ”IEEE Trans. Electronic Computers EC-14:2 (April), 270 –271. Wilkes, M. V. [1982]. “Hardware support memory protection: Capability implementations, ”Proc. Symposium Architectural Support Program- ming Languages Operating Systems (ASPLOS) , March 1 –3, 1982, Palo Alto, Calif., 107 –116. Wulf, W. A., R. Levin, S. P. Harbison [1981]. Hydra/C.mmp: Experimental Computer System , McGraw-Hill, New York. M.4 Evolution Instruction Sets (Appendices A, J, K) One ’s eyebrows rise whenever future architecture developed stack- register-oriented instruction set. Meyers [1978, p. 20] earliest computers, including UNIVAC I, EDSAC, IAS computers, accumulator-based computers. simplicity type ofcomputer made natural choice hardware resources con-strained. first general-purpose register computer Pegasus, built byFerranti, Ltd., 1956. Pegasus eight general-purpose registers, R0 always zero. Block transfers loaded eight registers drum memory. Stack Architectures 1963, Burroughs delivered B5000. B5000 perhaps first com-puter seriously consider software hardware-software trade-offs. Bartonand designers Burroughs made B5000 stack architecture (as describedin Barton [1961]). Designed support high-level languages ALGOL, thisstack architecture used operating system (MCP) written high-level lan-guage. B5000 also first computer U.S. manufacturer support virtual memory. B6500, introduced 1968 (and discussed Hauck Dent [1968]), added hardware-managed activation records. B5000 andB6500, top two elements stack kept processor restof stack kept memory. stack architecture yielded good code density,but provided two high-speed storage locations. authors orig-inal IBM 360 paper [Amdahl, Blaauw, Brooks 1964] original PDP-11paper [Bell et al. 1970] argued stack organization. cited threemajor points arguments stacks:M.4 Evolution Instruction Sets ■M-17■Performance derived fast registers, way used. ■The stack organization limiting requires many swap copy operations. ■The stack bottom, placed slower memory aperformance loss. Stack-based hardware fell favor late 1970s and, except Intel 80x86 floating-point architecture, essentially disappeared; example, except forthe 80x86, none computers listed SPEC report uses stack. 1990s, however, stack architectures received shot arm success Java Virtual Machine (JVM). JVM software interpreter foran intermediate language produced Java compilers, called Java bytecodes [Lindholm Yellin 1999]. purpose interpreter provide softwarecompatibility across many platforms, hope “write once, run every- where. ”Although slowdown factor 10 due interpretation, times compatibility important performance, whendownloading Java “applet ”into Internet browser. Although proposed hardware directly execute JVM instruc- tions (see McGhan ’Connor [1998]), thus far none proposals significant commercially. hope instead just-in-time (JIT) Java compilers —which compile runtime native instruction set computer running Java program —will overcome performance penalty interpretation. popularity Java also led compilers compile directly native hardware instruction sets, bypassing illusion theJava bytecodes. Computer Architecture Defined IBM coined term computer architecture early 1960s. Amdahl, Blaauw, Brooks [1964] used term refer programmer-visible portion theIBM 360 instruction set. believed family computers architecture able run software. Although idea may seem obvious us today, quite novel time. IBM, although lead-ing company industry, five different architectures 360; thus,the notion company standardizing single architecture radical one.The 360 designers hoped defining common architecture would bring sixdifferent divisions IBM together. definition architecture …the structure computer machine language programmer must understand write correct (timing independent) program machine. term machine language programmer meant compatibility would hold, even machine language, timing independent allowed different implemen- tations. architecture blazed path binary compatibility, othershave followed.M-18 ■Appendix Historical Perspectives ReferencesThe IBM 360 first computer sell large quantities byte addressing using 8-bit bytes general-purpose registers. 360 also register-memory limited memory-memory instructions. Appendix K summa-rizes instruction set. 1964, Control Data delivered first supercomputer, CDC 6600. Thornton [1964] discussed, he, Cray, 6600 designers amongthe first explore pipelining depth. 6600 first general-purpose,load-store computer. 1960s, designers 6600 realized needto simplify architecture sake efficient pipelining. Microprocessor minicomputer designers largely neglected interaction architectural simplicity implementation 1970s, returned 1980s. High-Level Language Computer Architecture late 1960s early 1970s, people realized software costs growingfaster hardware costs. McKeeman [1967] argued compilers operatingsystems getting big complex taking long develop.Because inferior compilers memory limitations computers, mostsystems programs time still written assembly language. Manyresearchers proposed alleviating software crisis creating powerful, software-oriented architectures. Tanenbaum [1978] studied properties high-level languages. Like researchers, found programs sim-ple. argued architectures designed mind theyshould optimize program size ease compilation. Tanenbaum proposed astack computer frequency-encoded instruction formats accomplish thesegoals; however, observed, program size translate directlyto cost-performance, stack computers faded shortly work. Strecker ’s article [1978] discusses architects DEC responded designing VAX architecture. VAX designed simplify compilation high-level languages. Compiler writers complainedabout lack complete orthogonality PDP-11. VAX architecturewas designed highly orthogonal allow mapping high-levellanguage statement single VAX instruction. Additionally, VAXdesigners tried optimize code size compiled programs often toolarge available memories. Appendix K summarizes instruction set. VAX-11/780 first computer announced VAX series. one successful —and heavily studied —computers ever built. cornerstone DEC ’s strategy single architecture, VAX, running sin- gle operating system, VMS. strategy worked well 10 years. largenumber papers reporting instruction mixes, implementation measurements, andanalysis VAX makes ideal case study [Clark Levy 1982; Wiecek1982]. Bhandarkar Clark [1991] gave quantitative analysis disadvan-tages VAX versus RISC computer, essentially technical explanation forthe demise VAX.M.4 Evolution Instruction Sets ■M-19While VAX designed, radical approach, called high-level language computer architecture (HLLCA), advocated research community. movement aimed eliminate gap high-level lan-guages computer hardware —what Gagliardi [1973] called “semantic gap”—by bringing hardware “up ”the level programming language. Meyers [1982] provided good summary arguments history high-level language computer architecture projects. HLLCA never significantcommercial impact. increase memory size computers eliminated codesize problems arising high-level languages enabled operating systems written high-level languages. combination simpler architectures together software offered greater performance flexibility lowercost lower complexity. Reduced Instruction Set Computers early 1980s, direction computer architecture began swing away fromproviding high-level hardware support languages. Ditzel Patterson [1980]analyzed difficulties encountered high-level language architecturesand argued answer lay simpler architectures. another paper [Pattersonand Ditzel 1980], authors first discussed idea Reduced Instruction Set Computers (RISCs) presented argument simpler architectures. Clark Strecker [1980], VAX architects, rebutted proposal. simple load-store computers MIPS commonly called RISC architectures. roots RISC architectures go back computers like the6600, Thornton, Cray, others recognized importance instructionset simplicity building fast computer. Cray continued tradition keepingcomputers simple CRAY-1. Commercial RISCs built primarily thework three research projects: Berkeley RISC processor, IBM 801, Stanford MIPS processor. architectures attracted enormous industrial interest claims performance advantage anywhere fromtwo five times computers using technology. Begun 1975, IBM project first start last become public. IBM computer designed 24-bit ECL minicomputer, theuniversity projects MOS-based, 32-bit microprocessors. John Cocke isconsidered father 801 design. received Eckert –Mauchly Turing awards recognition contribution. Radin [1982] described high- lights 801 architecture. 801 experimental project never designed product. fact, keep costs complexity, computerwas built 24-bit registers. 1980, Patterson colleagues Berkeley began project give architectural approach name (see Patterson Ditzel [1980]). Theybuilt two computers called RISC-I RISC-II. IBM project notwidely known discussed, role played Berkeley group promoting theRISC approach critical acceptance technology. also built one ofM-20 ■Appendix Historical Perspectives Referencesthe first instruction caches support hybrid-format RISCs (see Patterson et al. [1983]). supported 16-bit 32-bit instructions memory 32 bits cache. Berkeley group went build RISC computers targeted towardSmalltalk, described Ungar et al. [1984], LISP, described Tayloret al. [1986]. 1981, Hennessy colleagues Stanford published description Stanford MIPS computer. Efficient pipelining compiler-assisted scheduling ofthe pipeline important aspects original MIPS design. MIPS stoodfor Microprocessor without Interlocked Pipeline Stages, reflecting lack hardware stall pipeline, compiler would handle dependencies. early RISC computers —the 801, RISC-II, MIPS —had much common. university projects interested designing simple computerthat could built VLSI within university environment. three computersused simple load-store architecture fixed-format 32-bit instructions, andemphasized efficient pipelining. Patterson [1985] described three computersand basic design principles come characterize RISC com-puter is, Hennessy [1984] provided another view ideas, well issues VLSI processor design. 1985, Hennessy published explanation RISC performance advan- tage traced roots substantially lower CPI —under 2 RISC processor 10 VAX-11/780 (though identical workloads). paper byEmer Clark [1984] characterizing VAX-11/780 performance instrumentalin helping RISC researchers understand source performance advan-tage seen computers. Since university projects finished up, 1983 –1984 time frame, technology widely embraced industry. Many manufacturers early computers (those made 1986) claimed products RISCcomputers. claims, however, often born marketing ambitionthan engineering reality. 1986, computer industry began announce processors based tech- nology explored three RISC research projects. Moussouris et al. [1986]described MIPS R2000 integer processor, Kane ’s book [1986] provides complete description architecture. Hewlett-Packard converted existing minicomputer line RISC architectures; Lee [1989] described HP Precision Architecture. IBM never directly turned 801 product. Instead, ideaswere adopted new, low-end architecture incorporated IBMRT-PC described collection papers [Waters 1986]. 1990, IBMannounced new RISC architecture (the RS 6000), first superscalarRISC processor. 1987, Sun Microsystems began delivering computers basedon SPARC architecture, derivative Berkeley RISC-II processor; SPARCis described Garner et al. [1988]. PowerPC joined forces Apple, IBM, Motorola. Appendix K summarizes several RISC architectures. help resolve RISC versus traditional design debate, designers VAX processors later performed quantitative comparison VAX RISC proces-sor implementations comparable organizations. choices theM.4 Evolution Instruction Sets ■M-21VAX 8700 MIPS M2000. differing goals VAX MIPS led different architectures. VAX goals, simple compilers code density, led powerful addressing modes, powerful instructions, efficient instructionencoding, registers. MIPS goals high performance via pipelin-ing, ease hardware implementation, compatibility highly optimizingcompilers. goals led simple instructions, simple addressing modes,fixed-length instruction formats, large number registers. Figure M.1 shows ratio number instructions executed, ratio CPIs, ratio performance measured clock cycles. Since organiza- tions similar, clock cycle times assumed same. MIPS executes twice many instructions VAX, CPI VAX aboutsix times larger MIPS. Hence, MIPS M2000 almostthree times performance VAX 8700. Furthermore, much less hardwareis needed build MIPS processor VAX processor. cost-performance gap reason company used make VAX intro-duced MIPS-based product dropped VAX completely andswitched Alpha, quite similar MIPS. Bell Strecker [1998] summarized debate inside company. Today, DEC, second largest computer company major success minicomputer industry, existsonly remnants within HP Intel. 0.00.51.01.52.02.53.03.54.0 li eqntottespressodoductomcatvfpppp nasa7 matrix spicePerformance ratio Instructions executed ratio CPI ratio SPEC89 benchmarksMIPS/VAX Figure M.1 Ratio MIPS M2000 VAX 8700 instructions executed performance clock cycles using SPEC89 programs. average, MIPS executes little twice many instructions VAX, CPI VAX almost six times MIPS CPI, yielding almost threefold performance advantage. (Based data fromBhandarkar Clark [1991].)M-22 ■Appendix Historical Perspectives ReferencesLooking back, one Complex Instruction Set Computer (CISC) instruction set survived RISC/CISC debate, one binary compatibility PC software. volume chips high PC industry sufficientrevenue stream pay extra design costs —and sufficient resources due Moore ’s law —to build microprocessors translate CISC RISC inter- nally. Whatever loss efficiency occurred (due longer pipeline stages big-ger die size accommodate translation chip) overcome theenormous volume ability dedicate IC processing lines specifically thisproduct. Interestingly, Intel also concluded future 80x86 line doubt- ful. created IA-64 architecture support 64-bit addressing tomove RISC-style instruction set. embodiment IA-64 (see Hucket al. [2000]) architecture Itanium-1 Itanium-2 mixed suc-cess. Although high performance achieved floating-point applica-tions, integer performance never impressive. addition, Itaniumimplementations large transistor count die size powerhungry. complexity IA-64 instruction set, standing least partial conflict RISC philosophy, doubt contributed area power inefficiency. AMD decided instead stretch architecture 32-bit address 64-bit address, much Intel done 80386 stretched 16-bitaddress 32-bit address. Intel later followed AMD ’s example. end, tremendous marketplace advantage 80x86 presence much evenfor Intel, owner legacy, overcome! References Alexander, W. G., D. B. Wortman [1975]. “Static dynamic characteristics XPL programs, ”IEEE Computer 8:11 (November), 41 –46. Amdahl, G. M., G. A. Blaauw, F. P. Brooks, Jr. [1964]. “Architecture IBM System 360, ”IBM J. Research Development 8:2 (April), 87 –101. Barton, R. S. [1961]. “A new approach functional design computer, ” Proc. Western Joint Computer Conf. , May 9 –11, 1961, Los Angeles, Calif., 393–396. Bell, G., R. Cady, H. McFarland, B. DeLagi, J. ’Laughlin, R. Noonan, W. Wulf [1970]. “A new architecture mini-computers: DEC PDP-11, ” Proc. AFIPS SJCC , May 5 –7, 1970, Atlantic City, N.J., 657 –675. Bell, G., W. D. Strecker [1998]. “Computer structures: learned PDP-11? ”in G. S. Sohi, ed., 25 Years International Symposia Computer Architecture (Selected Papers) , ACM, New York, 138 –151. Bhandarkar, D. P. [1995]. Alpha Architecture Implementations , Digital Press, Newton, Mass. Bhandarkar, D., D. W. Clark [1991]. “Performance architecture: Com- paring RISC CISC similar hardware organizations, ”Proc. FourthM.4 Evolution Instruction Sets ■M-23Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , April 8 –11, 1991, Palo Alto, Calif., 310 –319. Bier, J. [1997]. “The evolution DSP processors, ”paper presented University California, Berkeley, November 14. Boddie, J. R. [2000]. “History DSPs, ”www.lucent.com/micro/dsp/dsphist.html . Case, R. P., A. Padegs [1978]. “The architecture IBM System/370, ” Communications ACM 21:1, 73 –96. Chow, F. C. [1983]. “A Portable Machine-Independent Global Optimizer — Design Measurements, ”Ph.D. thesis, Stanford University, Palo Alto, Calif. Clark, D., H. Levy [1982]. “Measurement analysis instruction set use VAX-11/780, ”Proc. Ninth Annual Int ’l. Symposium Computer Architec- ture (ISCA) , April 26 –29, 1982, Austin, Tex., 9 –17. Clark, D., W. D. Strecker [1980]. “Comments ‘the case reduced instruction set computer, ’”Computer Architecture News 8:6 (October), 34–38. Crawford, J., P. Gelsinger [1988]. Programming 80386 , Sybex Books, Alameda, Calif. Darcy, J. D., D. Gay [1996]. “FLECKmarks: Measuring floating point performance using full IEEE compliant arithmetic benchmark, ”CS 252 class project, University California, Berkeley (see http://www.sonic.net/ /C24jddarcy/ Research/fleckmrk.pdf ). Digital Semiconductor. [1996]. Alpha Architecture Handbook, Version 3 , Digital Press, Maynard, Mass. Ditzel, D. R., D. A. Patterson [1980]. “Retrospective high-level language computer architecture, ”Proc. Seventh Annual Int ’l. Symposium Computer Architecture (ISCA) , May 6 –8, 1980, La Baule, France, 97 –104. Emer, J. S., D. W. Clark [1984]. “A characterization processor performance VAX-11/780, ”Proc. 11th Annual Int ’l. Symposium Computer Archi- tecture (ISCA) , June 5 –7, 1984, Ann Arbor, Mich., 301 –310. Furber, S. B. [2000]. ARM system-on-chip architecture . Addison-Wesley, Boston, Mass. Gagliardi, U. O. [1973]. “Report workshop 4 —software-related advances computer hardware, ”Proc. Symposium High Cost Software , Septem- ber 17 –19, 1973, Monterey, Calif., 99 –120. Game, M., A. Booker [1999]. “CodePack code compression PowerPC pro- cessors, ”MicroNews , 5:1. Garner, R., A. Agarwal, F. Briggs, E. Brown, D. Hough, B. Joy, S. Kleiman, S. Muchnick, M. Namjoo, D. Patterson, J. Pendleton, R. Tuck [1988]. “Scal- able processor architecture (SPARC), ”Proc. IEEE COMPCON , February 29 – March 4, 1988, San Francisco, 278 –283. Hauck, E. A., B. A. Dent [1968]. “Burroughs ’B6500/B7500 stack mecha- nism, ”Proc. AFIPS SJCC , April 30 –May 2, 1968, Atlantic City, N.J., 245 –251. Hennessy, J. [1984]. “VLSI processor architecture, ”IEEE Trans. Computers C- 33:11 (December), 1221 –1246.M-24 ■Appendix Historical Perspectives ReferencesHennessy, J. [1985]. “VLSI RISC processors, ”VLSI Systems Design 6:10 (Octo- ber), 22 –32. Hennessy, J., N. Jouppi, F. Baskett, J. Gill [1981]. “MIPS: VLSI processor architecture, ”inCMU Conference VLSI Systems Computations , Com- puter Science Press, Rockville, Md. Hewlett-Packard. [1994]. PA-RISC 2.0 Architecture Reference Manual , 3rd ed., Hewlett-Packard, Palo Alto, Calif. Hitachi. [1997]. SuperH RISC Engine SH7700 Series Programming Manual , Hita- chi, Santa Clara, Calif. Huck, J. et al. [2000]. “Introducing IA-64 Architecture ”IEEE Micro , 20:5, (September –October), 12 –23. IBM. [1994]. PowerPC Architecture , Morgan Kaufmann, San Francisco. Intel. [2001]. “Using MMX instructions convert RGB YUV color conver- sion, ” cedar.intel.com/cgi-bin/ids.dll/content/content.jsp?cntKey ¼Legacy:: irtm_AP548_9996&cntType ¼IDS_EDITORIAL . Kahan, J. [1990]. “On advantage 8087 ’s stack, ”unpublished course notes, Computer Science Division, University California, Berkeley. Kane, G. [1986]. MIPS R2000 RISC Architecture , Prentice Hall, Englewood Cliffs, N.J. Kane, G. [1996]. PA-RISC 2.0 Architecture , Prentice Hall, Upper Saddle River, N.J. Kane, G., J. Heinrich [1992]. MIPS RISC Architecture , Prentice Hall, Engle- wood Cliffs, N.J. Kissell, K. D. [1997]. “MIPS16: High-density embedded market, ”Proc. Real Time Systems ’97, June 15, 1997, Las Vegas, Nev. Kozyrakis, C. [2000]. “Vector IRAM: media-oriented vector processor embedded DRAM, ”paper presented Hot Chips 12, August 13 –15, 2000, Palo Alto, Calif, 13 –15. Lee, R. [1989]. “Precision architecture, ”Computer 22:1 (January), 78 –91. Levy, H., R. Eckhouse [1989]. Computer Programming Architecture: VAX, Digital Press, Boston. Lindholm, T., F. Yellin [1999]. Java Virtual Machine Specification , 2nd ed., Addison-Wesley, Reading, Mass. Lunde, A. [1977]. “Empirical evaluation features instruction set proces- sor architecture, ”Communications ACM 20:3 (March), 143 –152. Magenheimer, D. J., L. Peters, K. W. Pettis, D. Zuras [1988]. “Integer multi- plication division HP precision architecture, ”IEEE Trans. Com- puters 37:8, 980 –990. McGhan, H., M. ’Connor [1998]. “PicoJava: direct execution engine Java bytecode, ”Computer 31:10 (October), 22 –30. McKeeman, W. M. [1967]. “Language directed computer design, ”Proc. AFIPS Fall Joint Computer Conf. , November 14 –16, 1967, Washington, D.C., 413 – 417. Meyers, G. J. [1978]. “The evaluation expressions storage-to-storage archi- tecture, ”Computer Architecture News 7:3 (October), 20 –23.M.4 Evolution Instruction Sets ■M-25Meyers, G. J. [1982]. Advances Computer Architecture , 2nd ed., Wiley, New York. MIPS. [1997]. MIPS16 Application Specific Extension Product Description . Mitsubishi. [1996]. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family Software Manual , Mitsubishi, Cypress, Calif. Morse, S., B. Ravenal, S. Mazor, W. Pohlman [1980]. “Intel microproces- sors—8080 8086, ”Computer 13:10 (October). Moussouris, J., L. Crudele, D. Freitas, C. Hansen, E. Hudson, S. Przybylski, T. Riordan, C. Rowen [1986]. “A CMOS RISC processor integrated system functions, ”Proc. IEEE COMPCON , March 3 –6, 1986, San Francisco, 191. Muchnick, S. S. [1988]. “Optimizing compilers SPARC, ”Sun Technology 1:3 (Summer), 64 –77. Palmer, J., S. Morse [1984]. 8087 Primer , John Wiley & Sons, New York, 93. Patterson, D. [1985]. “Reduced instruction set computers, ”Communications ACM 28:1 (January), 8 –21. Patterson, D. A., D. R. Ditzel [1980]. “The case reduced instruction set computer, ”Computer Architecture News 8:6 (October), 25 –33. Patterson, D. A., P. Garrison, M. Hill, D. Lioupis, C. Nyberg, T. Sippel, K. Van Dyke [1983]. “Architecture VLSI instruction cache RISC, ”10th Annual Int ’l. Conf. Computer Architecture Conf. Proc. , June 13 –16, 1983, Stockholm, Sweden, 108 –116. Radin, G. [1982]. “The 801 minicomputer, ”Proc. Symposium Architectural Sup- port Programming Languages Operating Systems (ASPLOS) , March 1 – 3, 1982, Palo Alto, Calif., 39 –47. Riemens, A., K. A. Vissers, R. J. Schutten, F. W. Sijstermans, G. J. Hekstra, G. D. La Hei [1999]. “Trimedia CPU64 application domain benchmark suite, ” Proc. IEEE Int ’l. Conf. Computer Design: VLSI Computers Processors (ICCD ’99), October 10 –13, 1999, Austin, Tex., 580 –585. Ropers, A., H. W. Lollman, J. Wellhausen [1999]. DSPstone: Texas Instru- ments TMS320C54x , Tech. Rep. Nr. IB 315 1999/9-ISS-Version 0.9, Aachen University Technology, Aaachen, Germany ( www.ert.rwth-aachen.de/ Projekte/Tools/coal/dspstone_c54x/index.html ). Shustek, L. J. [1978]. “Analysis Performance Computer Instruction Sets, ” Ph.D. dissertation, Stanford University, Palo Alto, Calif. Silicon Graphics. [1996]. MIPS V Instruction Set (seehttp://www.sgi.com/MIPS/ arch/ISA5/#MIPSV_indx ). Sites, R. L., R. Witek, eds. [1995]. Alpha Architecture Reference Manual , 2nd ed., Digital Press, Newton, Mass. Strauss, W. [1998]. “DSP Strategies 2002, ”www.usadata.com/market_research/ spr_05/spr_r127-005.htm . Strecker, W. D. [1978]. “VAX-11/780: virtual address extension PDP-11 family, ”Proc. AFIPS National Computer Conf. , June 5 –8, 1978, Anaheim, Calif., 47, 967 –980.M-26 ■Appendix Historical Perspectives ReferencesSun Microsystems. [1989]. SPARC Architectural Manual , Version 8, Part No. 800-1399-09, Sun Microsystems, Santa Clara, Calif. Tanenbaum, A. S. [1978]. “Implications structured programming machine architecture, ”Communications ACM 21:3 (March), 237 –246. Taylor, G., P. Hilfinger, J. Larus, D. Patterson, B. Zorn [1986]. “Evaluation SPUR LISP architecture, ”Proc. 13th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –5, 1986, Tokyo. Texas Instruments [2000]. “History innovation: 1980s, ”www.ti.com/corp/docs/ company/history/1980s.shtml . Thornton, J. E. [1964]. “Parallel operation Control Data 6600, ”Proc. AFIPS Fall Joint Computer Conf., Part II , October 27 –29, 1964, San Francisco, 26, 33 –40. Ungar, D., R. Blau, P. Foley, D. Samples, D. Patterson [1984]. “Architecture SOAR: Smalltalk RISC, ”Proc. 11th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1984, Ann Arbor, Mich., 188 –197. van Eijndhoven, J. T. J., F. W. Sijstermans, K. A. Vissers, E. J. D. Pol, M. I. A. Tromp, P. Struik, R. H. J. Bloks, P. van der Wolf, A. D. Pimentel, H. P. E.Vranken [1999]. “Trimedia CPU64 architecture, ”Proc. IEEE Int ’l. Conf. Computer Design: VLSI Computers Processors (ICCD ’99), October 10–13, 1999, Austin, Tex., 586 –592. Wakerly, J. [1989]. Microcomputer Architecture Programming , Wiley, New York. Waters, F. (ed.) [1986]. IBM RT Personal Computer Technology , SA 23-1057, IBM, Austin, Tex. Weaver, D. L., T. Germond [1994]. SPARC Architectural Manual , Version 9, Prentice Hall, Englewood Cliffs, N.J. Weiss, S., J. E. Smith [1994]. Power PowerPC , Morgan Kaufmann, San Francisco. Wiecek, C. [1982]. “A case study VAX 11 instruction set usage compiler execution, ”Proc. Symposium Architectural Support Programming Lan- guages Operating Systems (ASPLOS) , March 1 –3, 1982, Palo Alto, Calif., 177–184. Wulf, W. [1981]. “Compilers computer architecture, ”Computer 14:7 (July), 41–47. M.5 Development Pipelining Instruction-Level Parallelism ( Chapter 3 Appendices C H) Early Pipelined CPUs first general-purpose pipelined processor considered Stretch, IBM 7030. Stretch followed IBM 704 goal 100 times faster thanthe 704. goal stretch state art time, hence theM.5 Development Pipelining Instruction-Level Parallelism ■M-27nickname. plan obtain factor 1.6 overlapping fetch, decode, execute, using four-stage pipeline. Bloch [1959] Bucholtz [1962] described design engineering trade-offs, including use ALU bypasses. series general pipelining descriptions appeared late 1970s early 1980s provided terminology described basic tech-niques used simple pipelines. surveys include Keller [1975], Ramamoorthyand Li [1977], Chen [1980], well Kogge [1981], whose book devotedentirely pipelining. Davidson colleagues [1971, 1975] developed theconcept pipeline reservation tables design methodology multicycle pipe- lines feedback (also described Kogge [1981]). Many designers use varia- tion concepts, either designing pipelines creating software toschedule them. RISC processors originally designed ease implementation pipelining mind. Several early RISC papers, published early1980s, attempt quantify performance advantages simplification ininstruction set. best analysis, however, comparison VAX MIPSimplementation published Bhandarkar Clark 1991, 10 years first published RISC papers (see Figure M.1 ). 10 years arguments implementation benefits RISC, paper convinced even skepticaldesigners advantages RISC instruction set architecture. J. E. Smith colleagues written number papers examining instruction issue, exception handling, pipeline depth high-speed scalarCPUs. Kunkel Smith [1986] evaluated impact pipeline overhead anddependences choice optimal pipeline depth; also provided excel-lent discussion latch design impact pipelining. Smith Pleszkun [1988] evaluated variety techniques preserving precise exceptions. Weiss Smith [1984] evaluated variety hardware pipeline scheduling instruc-tion issue techniques. MIPS R4000 one first deeply pipelined microprocessors described Killian [1991] Heinrich [1993]. initial Alpha implemen-tation (the 21064) similar instruction set similar integer pipeline struc-ture, pipelining floating-point unit. Introduction Dynamic Scheduling 1964, CDC delivered first CDC 6600. CDC 6600 unique manyways. addition introducing scoreboarding, CDC 6600 first pro-cessor make extensive use multiple functional units. also peripheralprocessors used multithreading. interaction pipelining andinstruction set design understood, simple, load-store instruction set used promote pipelining. CDC 6600 also used advanced packaging technology. Thornton [1964] described pipeline I/O processor architecture,including concept out-of-order instruction execution. Thornton ’s book [1970] provides excellent description entire processor, technologyM-28 ■Appendix Historical Perspectives Referencesto architecture, includes foreword Cray. (Unfortunately, book currently print.) CDC 6600 also instruction scheduler FORTRAN compilers, described Thorlin [1967]. IBM 360 Model 91: Landmark Computer IBM 360/91 introduced many new concepts, including tagging data, reg-ister renaming, dynamic detection memory hazards, generalized forward-ing. Tomasulo ’s algorithm described 1967 paper. Anderson, Sparacio, Tomasulo [1967] described aspects processor, including theuse branch prediction. Many ideas 360/91 faded use nearly25 years broadly resurrected 1990s. Unfortunately, 360/91 successful, handful sold. complexity design made late market allowed Model 85, first IBMprocessor cache, outperform 91. Branch-Prediction Schemes 2-bit dynamic hardware branch-prediction scheme described J. E.Smith [1981]. Ditzel McLellan [1987] described novel branch-target bufferfor CRISP, implements branch folding. correlating predictor exam-ine described Pan, So, Rameh [1992]. Yeh Patt [1992, 1993] gen- eralized correlation idea described multilevel predictors use branch histories branch, similar local history predictor used the21264. McFarling ’s tournament prediction scheme, refers com- bined predictor, described 1993 technical report. variety ofmore recent papers branch prediction based variations multileveland correlating predictor ideas. Kaeli Emma [1991] described return addressprediction, Evers et al. [1998] provided in-depth analysis multilevel pre-dictors. data shown Chapter 3 Skadron et al. [1999]. several schemes prediction may offer additional benefit beyond tour- nament predictors. Eden Mudge [1998] Jimenez Lin [2002] havedescribed approaches. Development Multiple-Issue Processors IBM pioneering work multiple issue. 1960s, project called ACS wasunder way California. included multiple-issue concepts, proposal dynamicscheduling (although simpler mechanism Tomasulo ’s scheme, used backup registers), fetching branch paths. project originally started new architecture follow Stretch surpass CDC 6600/6800. ACS started New York moved California, later changed S/360 com-patible, eventually canceled. John Cocke one intellectual forcesbehind team included number IBM veterans younger contributors,M.5 Development Pipelining Instruction-Level Parallelism ■M-29many went important roles IBM elsewhere: Jack Ber- tram, Ed Sussenguth, Gene Amdahl, Herb Schorr, Fran Allen, Lynn Conway, Phil Dauber, among others. compiler team published many theirideas great influence outside IBM, architecture ideas widelydisseminated time. complete accessible documentation thisimportant project www.cs.clemson.edu/ /C24mark/acs.html , includes inter- views ACS veterans pointers sources. Sussenguth [1999] agood overview ACS. early multiple-issue processors actually reached market followed LIW VLIW design approach. Charlesworth [1981] reported Floating Point Systems AP-120B, one first wide-instruction processorscontaining multiple operations per instruction. Floating Point Systems applied theconcept software pipelining compiler handwriting assemblylanguage libraries use processor efficiently. processor anattached processor, many difficulties implementing multiple issue ingeneral-purpose processors (for example, virtual memory exception handling)could ignored. One interesting approaches used early VLIW processors, AP-120B i860, idea pipeline organization requires operationsto “pushed ”a functional unit results caught end pipeline. processors, operations advance another operationpushes behind (in sequence). Furthermore, instruction specifies thedestination instruction issued earlier pushed pipelinewhen new operation pushed in. approach advantage itdoes specify result destination operation first issues result register actually written. separation eliminates need detect write write (WAW) write read (WAR) hazards hardware.The disadvantage increases code size since no-ops may needed pushresults dependence operation still pipeline andno operations type immediately needed. Instead “push-and- catch ”approach used two processors, almost designers chosen useself-draining pipelines specify destination issuing instruction issued instruction complete without action. advantages code density simplifications code generation seem outweigh advan- tages unusual structure. Several research projects introduced form multiple issue mid- 1980s. example, Stanford MIPS processor ability place twooperations single instruction, although capability dropped com-mercial variants architecture, primarily performance reasons. Alongwith colleagues Yale, Fisher [1983] proposed creating processor witha wide instruction (512 bits) named type processor VLIW. Code generated processor using trace scheduling, Fisher [1981] developed originally generating horizontal microcode. imple-mentation trace scheduling Yale processor described Fisher et al.[1984] Ellis [1986].M-30 ■Appendix Historical Perspectives ReferencesAlthough IBM canceled ACS, active research area continued 1980s. 10 years ACS canceled, John Cocke made new proposal superscalar processor dynamically made issue decisions; heand Tilak Agerwala described key ideas several talks mid-1980sand coined term superscalar . called design America; described Agerwala Cocke [1987]. IBM Power1 architecture (the RS/6000 line)is based ideas (see Bakoglu et al. [1989]). J. E. Smith [1984] colleagues Wisconsin proposed decoupled approach included multiple issue limited dynamic pipeline scheduling. key feature processor use queues maintain order among class instructions (such memory references) allowing slip behind orahead another class instructions. Astronautics ZS-1 described Smithet al. [1987] embodies approach queues connect load-store unit andthe operation units. Power2 design uses queues similar fashion. J. E. Smith[1989] also described advantages dynamic scheduling compared thatapproach static scheduling. concept speculation roots original 360/91, per- formed limited form speculation. approach used recent processors combines dynamic scheduling techniques 360/91 buffer allowin-order commit. Smith Pleszkun [1988] explored use buffering tomaintain precise interrupts described concept reorder buffer. Sohi[1990] described adding renaming dynamic scheduling, making possibleto use mechanism speculation. Patt colleagues early propo-nents aggressive reordering speculation. focused checkpoint andrestart mechanisms pioneered approach called HPSm, also extension Tomasulo ’s algorithm [Hwu Patt 1986]. use speculation technique multiple-issue processors evalu- ated Smith, Johnson, Horowitz [1989] using reorder buffer technique;their goal study available ILP nonscientific code using speculation andmultiple issue. subsequent book, Johnson [1990] described design aspeculative superscalar processor. Johnson later led AMD K-5 design, oneof first speculative superscalars. parallel superscalar developments, commercial interest VLIW approaches also increased. Multiflow processor (see Colwell et al. [1987]) based concepts developed Yale, although many important refine-ments made increase practicality approach. Among wasa control-lable store buffer provided support form speculation.Although 100 Multiflow processors sold, variety problems,including difficulties introducing new instruction set small companyand competition commercial RISC microprocessors changed theeconomics mini-computer market, led failure Multiflow company. Around time Multiflow, Cydrome founded build VLIW- style processor (see Rau et al. [1989]), also unsuccessful commercially.Dehnert, Hsu, Bratt [1989] explained architecture performance theM.5 Development Pipelining Instruction-Level Parallelism ■M-31Cydrome Cydra 5, processor wide-instruction word provides dynamic register renaming additional support software pipelining. Cydra 5 unique blend hardware software, including conditional instructions andregister rotation, aimed extracting ILP. Cydrome relied hardware thanthe Multiflow processor achieved competitive performance primarily onvector-style codes. end, Cydrome suffered problems similar thoseof Multiflow commercial success. Multiflow Cydrome,although unsuccessful commercial entities, produced number people withextensive experience exploiting ILP well advanced compiler technology; many people gone incorporate experience pieces technology newer processors. Fisher Rau [1993] edited comprehen-sive collection papers covering hardware software two impor-tant processors. Rau also developed scheduling technique called polycyclic scheduling , basis software-pipelining schemes (see Rau, Glaeser, andPicard [1982]). Rau ’s work built earlier work Davidson colleagues design optimal hardware schedulers pipelined processors. his- torical LIW processors included Apollo DN 10000 Intel i860, could dual-issue FP integer operations. Compiler Technology Hardware Support Scheduling Loop-level parallelism dependence analysis developed primarily D.Kuck colleagues University Illinois 1970s. also coined commonly used terminology antidependence andoutput dependence developed several standard dependence tests, including GCD Banerjeetests. latter test named Uptal Banerjee comes variety fla-vors. Recent work dependence analysis focused using variety exacttests ending linear programming algorithm called Fourier –Motzkin. D. Maydan W. Pugh showed sequences exact tests prac-tical solution. area uncovering scheduling ILP, much early work con- nected development VLIW processors, described earlier. Lam [1988] developed algorithms software pipelining evaluated use Warp,a wide-instruction-word processor designed special-purpose applications.Weiss Smith [1987] compared software pipelining versus loop unrolling astechniques scheduling code pipelined processor. Rau [1994] developedmodulo scheduling deal issues software-pipelining loops simul-taneously handling register allocation. Support speculative code scheduling explored variety contexts, including several processors provided mode exceptions ignored, allowing aggressive scheduling loads (e.g., MIPS TFP pro-cessor [Hsu 1994]). Several groups explored ideas aggressive hardwaresupport speculative code scheduling. example, Smith, Horowitz, LamM-32 ■Appendix Historical Perspectives References[1992] created concept called boosting contains hardware facility sup- porting speculation provides checking recovery mechanism, similar IA-64 Crusoe. sentinel scheduling idea, also similarto speculate-and-check approach used Crusoe IA-64 architec-tures, developed jointly researchers University Illinois HPLaboratories (see Mahlke et al. [1992]). early 1990s, Wen-Mei Hwu colleagues University Illi- nois developed compiler framework, called IMPACT (see Chang et al. [1991]),for exploring interaction multiple-issue architectures compiler technology. project led several important ideas, including superblock scheduling (see Hwu et al. [1993]), extensive use profiling guiding varietyof optimizations (e.g., procedure inlining), use special buffer (similarto ALAT program-controlled store buffer) compile-aided memory con-flict detection (see Gallagher et al. [1994]). also explored performancetrade-offs partial full support predication Mahlke et al. [1995]. early RISC processors delayed branches, scheme inspired microprogramming, several studies compile time branch prediction inspired delayed branch mechanisms. McFarling Hennessy [1986] quantitative comparison variety compile time runtime branch-prediction schemes. Fisher Freudenberger [1992] evaluated range compiletime branch-prediction schemes using metric distance mispredic-tions. Ball Larus [1993] Calder et al. [1997] described static predictionschemes using collected program behavior. EPIC IA-64 Development roots EPIC approach lie earlier attempts build LIW VLIWmachines —especially Cydrome Multiflow —and long history compiler work continued companies failed HP, University Illinois, elsewhere. Insights gained work led designers HP propose VLIW-style, 64-bit architecture follow HP PA RISC architecture.Intel looking new architecture replace x86 (now called IA-32)architecture provide 64-bit capability. 1995, formed partnershipto design new architecture, IA-64 (see Huck et al. [2000]), build processorsbased it. Itanium (see Sharangpani Arora [2000]) first processor.In 2002, Intel introduced second-generation IA-64 design, Itanium 2(see McNairy Soltis [2003] McCormick Knies [2002]). Studies ILP Ideas Increase ILP series early papers, including Tjaden Flynn [1970] Riseman andFoster [1972], concluded small amounts parallelism could availableat instruction level without investing enormous amount hardware. Thesepapers dampened appeal multiple instruction issue 10 years.M.5 Development Pipelining Instruction-Level Parallelism ■M-33Nicolau Fisher [1984] published paper based work trace sched- uling asserted presence large amounts potential ILP scientific programs. Since many studies available ILP. studies criticized presume level hardware supportand compiler technology. Nonetheless, studies useful set expectationsas well understand sources limitations. Wall participated sev-eral studies, including Jouppi Wall [1989] Wall [1991, 1993].Although early studies criticized conservative (e.g., ’t include speculation), last study far ambitious study ILP date basis data Section 3.10 . Sohi Vajapeyam [1989] provided measurements available parallelism wide-instruction-word processors.Smith, Johnson, Horowitz [1989] also used speculative superscalar processorto study ILP limits. time study, anticipated processorthey specified upper bound reasonable designs. Recent upcomingprocessors, however, likely least ambitious processor.Skadron et al. [1999] examined performance trade-offs limitations processor comparable aggressive processors 2005, concluding larger window sizes make sense without significant improvements onbranch prediction integer programs. Lam Wilson [1992] looked limitations imposed speculation showed additional gains possible allowing processors speculate inmultiple directions, requires one PC. (Such schemes cannotexceed perfect speculation accomplishes, help close gap betweenrealistic prediction schemes perfect prediction.) Wall ’s 1993 study includes limited evaluation approach (up eight branches explored). Going Beyond Data Flow Limit One approach explored literature use value pre- diction. Value prediction allow speculation based data values. havebeen number studies use value prediction. Lipasti Shen publishedtwo papers 1996 evaluating concept value prediction potentialimpact ILP exploitation. Calder, Reinman, Tullsen [1999] explored ideaof selective value prediction. Sodani Sohi [1997] approached prob-lem viewpoint reusing values produced instructions. Moshovos et al. [1997] showed deciding speculate values, tracking whether speculation accurate past, important achievingperformance gains value speculation. Moshovos Sohi [1997] Chrysosand Emer [1998] focused predicting memory dependences using infor-mation eliminate dependence memory. González González[1998], Babbay Mendelson [1998], Calder, Reinman, Tullsen[1999] recent studies use value prediction. area currentlyhighly active, new results published every conference.M-34 ■Appendix Historical Perspectives ReferencesRecent Advanced Microprocessors years 1994 1995 saw announcement wide superscalar processors (three issues per clock) every major processor vendor: Intel Pentium Pro Pen-tium II (these processors share core pipeline architecture, described Col-wellandSteck[1995]);AMDK-5,K-6,andAthlon;SunUltraSPARC(seeLauterbachand Horel [1999]); Alpha21164 (see Edmondson et al. [1995]) 21264 (see Kessler[1999]); MIPS R10000 R12000 (see Yeager [1996]); PowerPC 603, 604, 620(see Diep, Nelson, Shen [1995]); HP 8000 (Kumar [1997]). latter part oft hede ca de( 19 9 6 –2000) sawsecondgenerationsof manyof processors (Pentium III, AMD Athlon, Alpha 21264, among others). second generation, although similar issue rate, could sustain lower CPI provided much higher clock rates.All included dynamic scheduling, almost universally supported speculation.In practice, many factors, including implementation technology, memory hier-archy, skill designers, type applications benchmarked, play arole determining approach best. period 2000 2005 dominated three trends among superscalar processors: introduction higher clock rates achieved deeper pipelin- ing (e.g., Pentium 4; see Hinton et al. [2001]), introduction multithread- ing IBM Power 4 Intel Pentium 4 Extreme, beginningof movement multicore IBM Power 4, AMD Opteron (see Keltcheret al. [2003]), recently Intel (see Douglas [2005]). Multithreading Simultaneous Multithreading concept multithreading dates back one earliest transistorized com- puters, TX-2. TX-2 also famous computer Ivan Suth- erland created Sketchpad, first computer graphics system. TX-2 built atMIT’s Lincoln Laboratory became operational 1959. used multiple threads support fast context switching handle I/O functions. Clark [1957]described basic architecture, Forgie [1957] described I/O architecture.Multithreading also used CDC 6600, fine-grained multithread-ing scheme interleaved scheduling among threads used architectureof I/O processors. HEP processor, pipelined multiprocessor designed Denelcor shipped 1982, used fine-grained multithreading hide pipe- line latency well hide latency large memory shared among theprocessors. HEP cache, hiding memory latency wascritical. Burton Smith, one primary architects, described HEP architec-ture 1978 paper, Jordan [1983] published performance evaluation. TheTERA processor extends multithreading ideas described Alversonet al. 1992 paper. Niagara multithreading approach similar thoseof HEP TERA systems, although Niagara employs caches reducing need thread-based latency hiding. late 1980s early 1990s, researchers explored concept coarse- grained multithreading (also called block multithreading ) way tolerateM.5 Development Pipelining Instruction-Level Parallelism ■M-35latency, especially multiprocessor environments. SPARCLE processor Alewife system used scheme, switching threads whenever highlatency exceptional event, long cache miss, occurred. Agarwal et al. describedSPARCLE 1993 paper. IBM Pulsar processor uses similar ideas. early 1990s, several research groups arrived two key insights. First, realized fine-grained multithreading needed get max-imum performance benefit, since coarse-grained approach, overhead ofthread switching thread start-up (e.g., filling pipeline newthread) negated much performance advantage (see Laudon, Gupta, Horowitz [1994]). Second, several groups realized effectively use large numbers functional units would require ILP thread-level parallelism(TLP). insights led several architectures used combinations multi-threading multiple issue. Wolfe Shen [1991] described architecturecalled XIMD statically interleaves threads scheduled VLIW processor.Hirata et al. [1992] described proposed processor media use combines astatic superscalar pipeline support multithreading; reported speed-ups combining forms parallelism. Keckler Dally [1992] com- bined static scheduling ILP dynamic scheduling threads processor multiple functional units. question balance allocation offunctional units ILP TLP schedule two forms par-allelism remained open. became clear mid-1990s dynamically scheduled supersca- lars would delivered shortly, several research groups proposed using thedynamic scheduling capability mix instructions several threads thefly. Yamamoto et al. [1994] appear published first proposal, though simulation results multithreaded superscalar architecture use simplistic assumptions. work quickly followed Tullsen, Eggers,and Levy [1995], provided first realistic simulation assessment coinedthe term simultaneous multithreading. Subsequent work group together industrial coauthors addressed many open questions aboutSMT. example, Tullsen et al. [1996] addressed questions challengesof scheduling ILP versus TLP. Lo et al. [1997] provided extensive discussion ofthe SMT concept evaluation performance potential, Lo et. al. [1998] evaluated database performance SMT processor. Tuck Tullsen [2003] reviewed performance SMT Pentium 4. IBM Power4 introduced multithreading (see Tendler et al. [2002]), Power5 used simultaneous multithreading. Mathis et al. [2005] exploredthe performance SMT Power5, Sinharoy et al. [2005] describedthe system architecture. References Agarwal, A., J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. ’Souza, M. Parkin [1993]. “Sparcle: evolutionary processor design large-scale mul- tiprocessors, ”IEEE Micro 13 (June), 48 –61.M-36 ■Appendix Historical Perspectives ReferencesAgerwala, T., J. Cocke [1987]. High Performance Reduced Instruction Set Processors , Tech. Rep. RC12434, IBM Thomas Watson Research Center, Yorktown Heights, N.Y. Alverson, G., R. Alverson, D. Callahan, B. Koblenz, A. Porterfield, B. Smith [1992]. “Exploiting heterogeneous parallelism multithreaded multiproces- sor,”Proc. ACM/IEEE Conf. Supercomputing , November 16 –20, 1992, Minneapolis, Minn., 188 –197. Anderson, D. W., F. J. Sparacio, R. M. Tomasulo [1967]. “The IBM 360 Model 91: Processor philosophy instruction handling, ”IBM J. Research Development 11:1 (January), 8 –24. Austin, T. M., G. Sohi [1992]. “Dynamic dependency analysis ordinary pro- grams, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 342 –351. Babbay, F., A. Mendelson [1998]. “Using value prediction increase power speculative execution hardware, ”ACM Trans. Computer Systems 16:3 (August), 234 –270. Bakoglu, H. B., G. F. Grohoski, L. E. Thatcher, J. A. Kaeli, C. R. Moore, D. P. Tattle, W. E. Male, W. R. Hardell, D. A. Hicks, M. Nguyen Phu, R. K. Montoye, W. T. Glover, S. Dhawan [1989]. “IBM second-generation RISC processor organization, ”Proc. IEEE Int ’l. Conf. Computer Design , October, Rye Brook, N.Y., 138 –142. Ball, T., J. Larus [1993]. “Branch prediction free, ”Proc. ACM SIG- PLAN ’93 Conference Programming Language Design Implementation (PLDI) , June 23 –25, 1993, Albuquerque, N.M., 300 –313. Bhandarkar, D., D. W. Clark [1991]. “Performance architecture: Com- paring RISC CISC similar hardware organizations, ”Proc. Fourth Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , April 8 –11, 1991, Palo Alto, Calif., 310 –319. Bhandarkar, D., J. Ding [1997]. “Performance characterization Pentium Pro processor, ”Proc. Third Int ’l. Symposium High Performance Computer Architecture , February 1 –5, 1997, San Antonio, Tex., 288 –297. Bloch, E. [1959]. “The engineering design Stretch computer, ”Proc. Eastern Joint Computer Conf. , December 1 –3, 1959, Boston, Mass., 48 –59. Bucholtz, W. [1962]. Planning Computer System: Project Stretch , McGraw-Hill, New York. Calder, B., D. Grunwald, M. Jones, D. Lindsay, J. Martin, M. Mozer, B. Zorn [1997]. “Evidence-based static branch prediction using machine learning, ” ACM Trans. Program. Lang. Syst. 19:1, 188 –222. Calder, B., G. Reinman, D. M. Tullsen [1999]. “Selective value prediction, ” Proc. 26th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 2 –4, 1999, Atlanta, Ga. Chang, P. P., S. A. Mahlke, W. Y. Chen, N. J. Warter, W. W. Hwu [1991]. “IMPACT: architectural framework multiple-instruction-issue processors, ”Proc. 18th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 27 –30, 1991, Toronto, Canada, 266 –275.M.5 Development Pipelining Instruction-Level Parallelism ■M-37Charlesworth, A. E. [1981]. “An approach scientific array processing: archi- tecture design AP-120B/FPS-164 family, ”Computer 14:9 (September), 18–27. Chen, T. C. [1980]. “Overlap parallel processing, ”inIntroduction Com- puter Architecture , H. Stone, ed., Science Research Associates, Chicago, 427–486. Chrysos, G. Z., J. S. Emer [1998]. “Memory dependence prediction using store sets,”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 142 –153. Clark, D. W. [1987]. “Pipelining performance VAX 8800 processor, ” Proc. Second Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 5 –8, 1987, Palo Alto, Calif., 173 –177. Clark, W. A. [1957]. “The Lincoln TX-2 computer development, ”Proc. Western Joint Computer Conference , February 26 –28, 1957, Los Angeles, 143 –145. Colwell, R. P., R. Steck [1995]. “A 0.6 μm BiCMOS processor dynamic execution. ”Proc. IEEE Int ’l. Symposium Solid State Circuits (ISSCC) , February 15 –17, 1995, San Francisco, 176 –177. Colwell, R. P., R. P. Nix, J. J. ’Donnell, D. B. Papworth, P. K. Rodman [1987]. “A VLIW architecture trace scheduling compiler, ”Proc. Second Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , October 5 –8, 1987, Palo Alto, Calif., 180 –192. Cvetanovic, Z., R. E. Kessler [2000]. “Performance analysis Alpha 21264-based Compaq ES40 system, ”27th Annual Int ’l. Symposium Com- puter Architecture (ISCA) , June 10 –14, 2000, Vancouver, Canada, 192 –202. Davidson, E. S. [1971]. “The design control pipelined function generators, ” Proc. IEEE Conf. Systems, Networks, Computers , January 19 –21, 1971, Oaxtepec, Mexico, 19 –21. Davidson, E. S., A. T. Thomas, L. E. Shar, J. H. Patel [1975]. “Effective con- trol pipelined processors, ”Proc. IEEE COMPCON , February 25 –27, 1975, San Francisco, 181 –184. Dehnert, J. C., P. Y.-T. Hsu, J. P. Bratt [1989]. “Overlapped loop support Cydra 5, ”Proc. Third Int ’l. Conf. Architectural Support Program- ming Languages Operating Systems (ASPLOS) , April 3 –6, 1989, Boston, Mass., 26 –39. Diep, T. A., C. Nelson, J. P. Shen [1995]. “Performance evaluation PowerPC 620 microarchitecture, ”Proc. 22nd Annual Int ’l. Symposium Com- puter Architecture (ISCA) , June 22 –24, 1995, Santa Margherita, Italy. Ditzel, D. R., H. R. McLellan [1987]. “Branch folding CRISP micro- processor: Reducing branch delay zero, ”Proc. 14th Annual Int ’l. Sympo- sium Computer Architecture (ISCA) , June 2 –5, 1987, Pittsburgh, Penn., 2 –7. Douglas, J. [2005]. “Intel 8xx series Paxville Xeon-MP Microprocessors, ” paper presented Hot Chips 17, August 14 –16, 2005, Stanford University, Palo Alto, Calif. Eden, A., T. Mudge [1998]. “The YAGS branch prediction scheme, ”Proc. 31st Annual ACM/IEEE Int ’l. Symposium Microarchitecture , November 30–December 2, 1998, Dallas, Tex., 69 –80.M-38 ■Appendix Historical Perspectives ReferencesEdmondson, J. H., P. I. Rubinfield, R. Preston, V. Rajagopalan [1995]. “Superscalar instruction execution 21164 Alpha microprocessor, ”IEEE Micro 15:2, 33 –43. Ellis, J. R. [1986]. Bulldog: Compiler VLIW Architectures , MIT Press, Cambridge, Mass. Emer, J. S., D. W. Clark [1984]. “A characterization processor performance VAX-11/780, ”Proc. 11th Annual Int ’l. Symposium Computer Archi- tecture (ISCA) , June 5 –7, 1984, Ann Arbor, Mich., 301 –310. Evers, M., S. J. Patel, R. S. Chappell, Y. N. Patt [1998]. “An analysis cor- relation predictability: makes two-level branch predictors work, ” Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3–14, 1998, Barcelona, Spain, 52 –61. Fisher, J. A. [1981]. “Trace scheduling: technique global microcode compac- tion,”IEEE Trans. Computers 30:7 (July), 478 –490. Fisher, J. A. [1983]. “Very long instruction word architectures ELI-512, ”10th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1982, Stockholm, Sweden, 140 –150. Fisher, J. A., S. M. Freudenberger [1992]. “Predicting conditional branches previous runs program, ”Proc. Fifth Int ’l. Conf. Architectural Sup- port Programming Languages Operating Systems (ASPLOS) , October 12–15, 1992, Boston, 85 –95. Fisher, J. A., B. R. Rau [1993]. Journal Supercomputing , January (special issue). Fisher, J. A., J. R. Ellis, J. C. Ruttenberg, A. Nicolau [1984]. “Parallel proces- sing: smart compiler dumb processor, ”Proc. SIGPLAN Conf. Com- piler Construction , June 17 –22, 1984, Montreal, Canada, 11 –16. Forgie, J. W. [1957]. “The Lincoln TX-2 input-output system, ”Proc. Western Joint Computer Conference , February 26 –28, 1957, Los Angeles, 156 –160. Foster, C. C., E. M. Riseman [1972]. “Percolation code enhance parallel dispatching execution, ”IEEE Trans. Computers C-21:12 (December), 1411 –1415. Gallagher, D. M., W. Y. Chen, S. A. Mahlke, J. C. Gyllenhaal, W.W. Hwu [1994]. “Dynamic memory disambiguation using memory conflict buffer, ” Proc. Sixth Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 4 –7, Santa Jose, Calif., 183 –193. González, J., A. González [1998]. “Limits instruction level parallelism data speculation, ”Proc. Vector Parallel Processing (VECPAR) Conf. , June 21–23, 1998, Porto, Portugal, 585 –598. Heinrich, J. [1993]. MIPS R4000 User ’s Manual , Prentice Hall, Englewood Cliffs, N.J. Hinton, G., D. Sager, M. Upton, D. Boggs, D. Carmean, A. Kyker, P. Roussel [2001]. “The microarchitecture Pentium 4 processor, ”Intel Technology Journal , February. Hirata, H., K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, T. Nishizawa [1992]. “An elementary processor architecture simultaneous instruction issuing multiple threads, ”Proc. 19th Annual Int ’l. SymposiumM.5 Development Pipelining Instruction-Level Parallelism ■M-39on Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 136–145. Hopkins, M. [2000]. “A critical look IA-64: Massive resources, massive ILP, deliver? ”Microprocessor Report , February. Hsu, P. [1994]. “Designing TFP microprocessor, ”IEEE Micro 18:2 (April), 2333. Huck, J. et al. [2000]. “Introducing IA-64 Architecture ”IEEE Micro , 20:5 (September –October), 12 –23. Hwu, W.-M., Y. Patt [1986]. “HPSm, high performance restricted data flow architecture minimum functionality, ”13th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –5, 1986, Tokyo, 297 –307. Hwu, W. W., S. A. Mahlke, W. Y. Chen, P. P. Chang, N. J. Warter, R. A. Bring- mann, R. O. Ouellette, R. E. Hank, T. Kiyohara, G. E. Haab, J. G. Holm, D.M. Lavery [1993]. “The superblock: effective technique VLIW superscalar compilation, ”J. Supercomputing 7:1, 2 (March), 229 –248. IBM. [1990]. “The IBM RISC System/6000 processor ”(collection papers), IBM J. Research Development 34:1 (January). Jimenez, D. A., C. Lin [2002]. “Neural methods dynamic branch prediction, ”ACM Trans. Computer Sys 20:4 (November), 369 –397. Johnson, M. [1990]. Superscalar Microprocessor Design , Prentice Hall, Engle- wood Cliffs, N.J. Jordan, H. F. [1983]. “Performance measurements HEP —a pipelined MIMD computer, ”Proc. 10th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1982, Stockholm, Sweden, 207 –212. Jouppi, N. P., D. W. Wall [1989]. “Available instruction-level parallelism superscalar superpipelined processors, ”Proc. Third Int ’l. Conf. Archi- tectural Support Programming Languages Operating Systems (ASPLOS) , April 3 –6, 1989, Boston, 272 –282. Kaeli, D. R., P. G. Emma [1991]. “Branch history table prediction moving target branches due subroutine returns, ”Proc. 18th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 27 –30, 1991, Toronto, Canada, 34 –42. Keckler, S. W., W. J. Dally [1992]. “Processor coupling: Integrating compile time runtime scheduling parallelism, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 202 –213. Keller, R. M. [1975]. “Look-ahead processors, ”ACM Computing Surveys 7:4 (December), 177 –195. Keltcher, C. N., K. J. McGrath, A. Ahmed, P. Conway [2003]. “The AMD Opteron processor multiprocessor servers, ”IEEE Micro 23:2 (March – April), 66 –76. Kessler, R. [1999]. “The Alpha 21264 microprocessor, ”IEEE Micro 19:2 (March/ April) 24 –36. Killian, E. [1991]. “MIPS R4000 technical overview –64 bits/100 MHz bust, ” Hot Chips III Symposium Record , August 26 –27, 1991, Stanford University, Palo Alto, Calif., 1.6 –1.19.M-40 ■Appendix Historical Perspectives ReferencesKogge, P. M. [1981]. Architecture Pipelined Computers , McGraw-Hill, New York. Kumar, A. [1997]. “The HP PA-8000 RISC CPU, ”IEEE Micro 17:2 (March/ April). Kunkel, S. R., J. E. Smith [1986]. “Optimal pipelining supercomputers, ” Proc. 13th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2–5, 1986, Tokyo, 404 –414. Lam, M. [1988]. “Software pipelining: effective scheduling technique VLIW processors, ”SIGPLAN Conf. Programming Language Design Implementation , June 22 –24, 1988, Atlanta, Ga., 318 –328. Lam, M. S., R. P. Wilson [1992]. “Limits control flow parallelism, ” Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19–21, 1992, Gold Coast, Australia, 46 –57. Laudon, J., A. Gupta, M. Horowitz [1994]. “Interleaving: multithreading technique targeting multiprocessors workstations, ”Proc. Sixth Int ’l. Conf. Architectural Support Programming Languages Operating Systems(ASPLOS) , October 4 –7, San Jose, Calif., 308 –318. Lauterbach, G., T. Horel [1999]. “UltraSPARC-III: Designing third generation 64-bit performance, ”IEEE Micro 19:3 (May/June). Lipasti, M. H., J. P. Shen [1996]. “Exceeding dataflow limit via value pre- diction, ”Proc. 29th Int ’l. Symposium Microarchitecture , December 2 –4, 1996, Paris, France. Lipasti, M. H., C. B. Wilkerson, J. P. Shen [1996]. “Value locality load value prediction, ”Proc. Seventh Conf. Architectural Support Program- ming Languages Operating Systems (ASPLOS) , October 1 –5, 1996, Cambridge, Mass., 138 –147. Lo, J., L. Barroso, S. Eggers, K. Gharachorloo, H. Levy, S. Parekh [1998]. “An analysis database workload performance simultaneous multithreaded pro-cessors, ”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 39 –50. Lo, J., S. Eggers, J. Emer, H. Levy, R. Stamm, D. Tullsen [1997]. “Converting thread-level parallelism instruction-level parallelism via simultaneous multithreading, ”ACM Trans. Computer Systems 15:2 (August), 322 –354. Mahlke, S. A., W. Y. Chen, W.-M. Hwu, B. R. Rau, M. S. Schlansker [1992]. “Sentinel scheduling VLIW superscalar processors, ”Proc. Fifth Int ’l. Conf. Architectural Support Programming Languages OperatingSystems (ASPLOS) , October 12 –15, 1992, Boston, 238 –247. Mahlke, S. A., R. E. Hank, J. E. McCormick, D. I. August, W. W. Hwu [1995]. “A comparison full partial predicated execution support ILP proces- sors,”Proc. 22nd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 22 –24, 1995, Santa Margherita, Italy, 138 –149. Mathis, H. M., A. E. Mercias, J. D. McCalpin, R. J. Eickemeyer, S. R. Kunkel [2005]. “Characterization multithreading (SMT) efficiency Power5, ” IBM J. Research Development , 49:4/5 (July/September), 555 –564.M.5 Development Pipelining Instruction-Level Parallelism ■M-41McCormick, J., A. Knies [2002]. “A brief analysis SPEC CPU2000 benchmarks Intel Itanium 2 processor, ”paper presented Hot Chips 14, August 18 –20, 2002, Stanford University, Palo Alto, Calif. McFarling, S. [1993]. Combining Branch Predictors , WRL Technical Note TN-36, Digital Western Research Laboratory, Palo Alto, Calif. McFarling, S., J. Hennessy [1986]. “Reducing cost branches, ”Proc. 13th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –5, 1986, Tokyo, 396 –403. McNairy, C., D. Soltis [2003]. “Itanium 2 processor microarchitecture, ”IEEE Micro 23:2 (March –April), 44 –55. Moshovos, A., G. S. Sohi [1997]. “Streamlining inter-operation memory communication via data dependence prediction, ”Proc. 30th Annual Int ’l. Symposium Microarchitecture , December 1 –3, Research Triangle Park, N.C., 235–245. Moshovos, A., S. Breach, T. N. Vijaykumar, G. S. Sohi [1997]. “Dynamic speculation synchronization data dependences, ”Proc. 24th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo. Nicolau, A., J. A. Fisher [1984]. “Measuring parallelism available long instruction word architectures, ”IEEE Trans. Computers C-33:11 (November), 968 –976. Pan, S.-T., K. So, J. T. Rameh [1992]. “Improving accuracy dynamic branch prediction using branch correlation, ”Proc. Fifth Int ’l. Conf. Archi- tectural Support Programming Languages Operating Systems(ASPLOS) , October 12 –15, 1992, Boston, 76 –84. Postiff, M.A., D. A. Greene, G. S. Tyson, T. N. Mudge [1999]. “The limits instruction level parallelism SPEC95 applications, ”Computer Architecture News 27:1 (March), 31 –40. Ramamoorthy, C. V., H. F. Li [1977]. “ Pipeline architecture, ”ACM Computing Surveys 9:1 (March), 61 –102. Rau, B. R. [1994]. “Iterative modulo scheduling: algorithm software pipe- lining loops, ”Proc. 27th Annual Int ’l. Symposium Microarchitecture , November 30 –December 2, 1994, San Jose, Calif., 63 –74. Rau, B. R., C. D. Glaeser, R. L. Picard [1982]. “Efficient code generation horizontal architectures: Compiler techniques architectural support, ”Proc. Ninth Annual Int ’l. Symposium Computer Architecture (ISCA) , April 26 –29, 1982, Austin, Tex., 131 –139. Rau, B. R., D. W. L. Yen, W. Yen, R. A. Towle [1989]. “The Cydra 5 depart- mental supercomputer: Design philosophies, decisions, trade-offs, ”IEEE Computers 22:1 (January), 12 –34. Riseman, E. M., C. C. Foster [1972]. “Percolation code enhance paralleled dispatching execution, ”IEEE Trans. Computers C-21:12 (December), 1411 –1415. Rymarczyk, J. [1982]. “Coding guidelines pipelined processors, ”Proc. Sym- posium Architectural Support Programming Languages Operating Sys-tems (ASPLOS) , March 1 –3, 1982, Palo Alto, Calif., 12 –19.M-42 ■Appendix Historical Perspectives ReferencesSharangpani, H., K. Arora [2000]. “Itanium Processor Microarchitecture, ” IEEE Micro , 20:5 (September –October), 24 –43. Sinharoy, B., R. N. Koala, J. M. Tendler, R. J. Eickemeyer, J. B. Joyner [2005]. “POWER5 system microarchitecture, ”IBM J. Research Devel- opment , 49:4 –5, 505 –521. Sites, R. [1979]. Instruction Ordering CRAY-1 Computer , Tech. Rep. 78-CS-023, Dept. Computer Science, University California, San Diego. Skadron, K., P. S. Ahuja, M. Martonosi, D. W. Clark [1999]. “Branch prediction, instruction-window size, cache size: Performance tradeoffs simulation techniques, ”IEEE Trans. Computers , 48:11 (November). Smith, A., J. Lee [1984]. “Branch prediction strategies branch-target buffer design, ”Computer 17:1 (January), 6 –22. Smith, B. J. [1978]. “A pipelined, shared resource MIMD computer, ”Proc. Int ’l. Conf. Parallel Processing (ICPP) , August, Bellaire, Mich., 6 –8. Smith, J. E. [1981]. “A study branch prediction strategies, ”Proc. Eighth Annual Int’l. Symposium Computer Architecture (ISCA) , May 12 –14, 1981, Minne- apolis, Minn., 135 –148. Smith, J. E. [1984]. “Decoupled access/execute computer architectures, ”ACM Trans. Computer Systems 2:4 (November), 289 –308. Smith, J. E. [1989]. “Dynamic instruction scheduling Astronautics ZS-1, ” Computer 22:7 (July), 21 –35. Smith, J. E., A. R. Pleszkun [1988]. “Implementing precise interrupts pipelined processors, ”IEEE Trans. Computers 37:5 (May), 562 –573. (This paper based earlier paper appeared Proc. 12th Annual Int’l. Symposium Computer Architecture (ISCA) , June 17 –19, 1985, Boston, Mass.) Smith, J. E., G. E. Dermer, B. D. Vanderwarn, S. D. Klinger, C. M. Rozewski, D. L. Fowler, K. R. Scidmore, J. P. Laudon [1987]. “The ZS-1 central pro- cessor, ”Proc. Second Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 5 –8, 1987, Palo Alto, Calif., 199 –204. Smith, M. D., M. Horowitz, M. S. Lam [1992]. “Efficient superscalar perfor- mance boosting, ”Proc. Fifth Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 12 –15, 1992, Boston, 248 –259. Smith, M. D., M. Johnson, M. A. Horowitz [1989]. “Limits multiple instruction issue, ”Proc. Third Int ’l. Conf. Architectural Support Pro- gramming Languages Operating Systems (ASPLOS) , April 3 –6, 1989, Bos- ton, 290 –302. Sodani, A., G. Sohi [1997]. “Dynamic instruction reuse, ”Proc. 24th Annual Int’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo. Sohi, G. S. [1990]. “Instruction issue logic high-performance, interruptible, multiple functional unit, pipelined computers, ”IEEE Trans. Computers 39:3 (March), 349 –359.M.5 Development Pipelining Instruction-Level Parallelism ■M-43Sohi, G. S., S. Vajapeyam [1989]. “Tradeoffs instruction format design horizontal architectures, ”Proc. Third Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , April 3 –6, 1989, Boston, 15 –25. Sussenguth, E. [1999]. “IBM’s ACS-1 Machine, ”IEEE Computer 22:11 (November). Tendler, J. M., J. S. Dodson, J. S. Fields, Jr., H. Le, B. Sinharoy [2002]. “Power4 system microarchitecture, ”IBM J. Research Development , 46:1, 5 –26. Thorlin, J. F. [1967]. “Code generation PIE (parallel instruction execution) computers, ”Proc. Spring Joint Computer Conf. , April 18 –20, 1967, Atlantic City, N.J., 27. Thornton, J. E. [1964]. “Parallel operation Control Data 6600, ”Proc. AFIPS Fall Joint Computer Conf., Part II , October 27 –29, 1964, San Francisco, 26, 33–40. Thornton, J. E. [1970]. Design Computer, Control Data 6600 , Scott, Fores- man, Glenview, Ill. Tjaden, G. S., M. J. Flynn [1970]. “Detection parallel execution independent instructions, ”IEEE Trans. Computers C-19:10 (October), 889–895. Tomasulo, R. M. [1967]. “An efficient algorithm exploiting multiple arithmetic units, ”IBM J. Research Development 11:1 (January), 25 –33. Tuck, N., D. Tullsen [2003]. “Initial observations simultaneous multi- threading Pentium 4 processor, ”Proc. 12th Int. Conf. Parallel Architectures Compilation Techniques (PACT ’03), September 27 –October 1, New Orleans, La., 26 –34. Tullsen, D. M., S. J. Eggers, H. M. Levy [1995]. “Simultaneous multithread- ing: Maximizing on-chip parallelism, ”Proc. 22nd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 22 –24, 1995, Santa Margherita, Italy, 392 – 403. Tullsen, D. M., S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, R. L. Stamm [1996]. “Exploiting choice: Instruction fetch issue implementable simultaneous multithreading processor, ”Proc. 23rd Annual Int ’l. Symposium Computer Architecture (ISCA) , May 22 –24, 1996, Philadelphia, Penn., 191–202. Wall, D. W. [1991]. “Limits instruction-level parallelism, ”Proc. Fourth Int ’l. Conf. Architectural Support Programming Languages OperatingSystems (ASPLOS) , April 8 –11, 1991, Palo Alto, Calif., 248 –259. Wall, D. W. [1993]. Limits Instruction-Level Parallelism , Research Rep. 93/6, Western Research Laboratory, Digital Equipment Corp., Palo Alto, Calif. Weiss, S., J. E. Smith [1984]. “Instruction issue logic pipelined supercom- puters, ”Proc. 11th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1984, Ann Arbor, Mich., 110 –118. Weiss, S., J. E. Smith [1987]. “A study scalar compilation techniques pipelined supercomputers, ”Proc. Second Int ’l. Conf. Architectural SupportM-44 ■Appendix Historical Perspectives Referencesfor Programming Languages Operating Systems (ASPLOS) , October 5 –8, 1987, Palo Alto, Calif., 105 –109. Wilson, R. P., M. S. Lam [1995]. “Efficient context-sensitive pointer analysis C programs, ”Proc. ACM SIGPLAN ’95 Conf. Programming Language Design Implementation , June 18 –21, 1995, La Jolla, Calif., 1 –12. Wolfe, A., J. P. Shen [1991]. “A variable instruction stream extension VLIW architecture, ”Proc. Fourth Int ’l. Conf. Architectural Support Pro- gramming Languages Operating Systems (ASPLOS) , April 8 –11, 1991, Palo Alto, Calif., 2 –14. Yamamoto, W., M. J. Serrano, A. R. Talcott, R. C. Wood, M. Nemirosky [1994]. “Performance estimation multistreamed, superscalar processors, ” Proc. 27th Annual Hawaii Int ’l. Conf. System Sciences , January 4 –7, 1994, Maui, 195 –204. Yeager, K. [1996]. “The MIPS R10000 superscalar microprocessor, ”IEEE Micro 16:2 (April), 28 –40. Yeh, T., Y. N. Patt [1992]. “Alternative implementations two-level adaptive branch prediction, ”Proc. 19th Annual Int ’l. Symposium Computer Architec- ture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 124 –134. Yeh, T., Y. N. Patt [1993]. “A comparison dynamic branch predictors use two levels branch history, ”Proc. 20th Annual Int ’l. Symposium Com- puter Architecture (ISCA) , May 16 –19, 1993, San Diego, Calif., 257 –266. M.6 Development SIMD Supercomputers, Vector Computers, Multimedia SIMD Instruction Extensions,and Graphical Processor Units ( Chapter 4 ) historical section, start perhaps infamous supercomputer, Illiac IV, representative early SIMD (Single Instruction, MultipleData) architectures move perhaps famous supercomputer,the Cray-1, representative vector architectures. next step Multimedia SIMD Extensions, got name part due advertising campaign involving “Bunny People, ”a disco-dancing set workers cleansuits semiconductor fabrication line. conclude history GPUs, isnot quite colorful. SIMD Supercomputers cost general multiprocessor is, however, high design options considered would decrease cost without seriously degrad- ing power efficiency system. options consist recentralizing oneof three major components. …Centralizing [control unit] gives rise basic organization [an] …array processor Illiac IV. Bouknight et al. [1972]M.6 Development SIMD Supercomputers, Vector Computers, Multimedia ■M-45…with Iliac IV, programming machine difficult architecture probably well suited applications trying run. key idea think good match Iliac IVbetween applications architecture. David Kuck Software designer Illiac IV early pioneer parallel software David Kuck oral history conducted 1991 Andrew Goldstein, IEEE History Center, New Brunswick, N.J. SIMD model one earliest models parallel computing, dating back first large-scale multiprocessor, Illiac IV. Rather pipelining datacomputation vector architectures, machines array functionalunits; hence, might considered array processors. earliest ideas SIMD-style computers Unger [1958] Slot- nick, Borck, McReynolds [1962]. Slotnick ’s Solomon design formed basis Illiac IV, perhaps infamous supercomputer projects. Although successful pushing several technologies proved useful later projects, failed computer. Costs escalated $8 million estimate 1966 $31 million 1972, despite construction quarter planned multiprocessor. (In 2011 dollars, increase $54M $152M.) Actual performance best 15 MFLOPS versus initial predictions 1000MFLOPS full system [Hord 1982]. Delivered NASA Ames Researchin 1972, computer required three years engineering wasusable. events slowed investigation SIMD, Danny Hillis [1985] resus- citated style Connection Machine, 65,536 1-bit processors. basic trade-off SIMD multiprocessors performance processor ver- sus number processors. SIMD supercomputers 1980s emphasized largedegree parallelism performance individual processors. Connec-tion Multiprocessor 2, example, offered 65,536 single-bit-wide processors,while Illiac IV planned 64 64-bit processors. Massively parallel SIMD mul-tiprocessors relied interconnection communication networks exchangedata processing elements. resurrected 1980s, first Thinking Machines MasPar, SIMD model faded away supercomputers two main reasons.First, inflexible. number important problems data parallel,and architecture scale competitive fashion; is, small-scaleSIMD multiprocessors often worse cost-performance compared ofthe alternatives. Second, SIMD could take advantage tremendous per-formance cost advantages SISD (Single Instruction, Single Data) micropro-cessor technology 1980s, doubling performance every 18 months. Instead leveraging low-cost technology, designers SIMD multiprocessors build custom processors multiprocessors.M-46 ■Appendix Historical Perspectives ReferencesVector Computers I’m certainly inventing vector processors. three kinds know existing today. represented Illiac-IV, (CDC) Star processor, andthe TI (ASC) processor. three pioneering processors. …One problems pioneer always make mistakes never, never wantto pioneer. ’s always best come second look mistakes pioneers made. Seymour Cray Public lecture Lawrence Livermore Laboratories introduction Cray-1 (1976) first vector processors Control Data Corporation (CDC) STAR-100 (see Hintz Tate [1972]) Texas Instruments ASC (see Watson [1972]),both announced 1972. memory-memory vector processors. hadrelatively slow scalar units —the STAR used units scalars vectors —making scalar pipeline extremely deep. processors high start-up overhead worked vectors several hundred several thousandelements. crossover scalar vector could 50 elements. appears enough attention paid role Amdahl ’s law two processors. Seymour Cray, worked 6600 7600 CDC, founded Cray Research introduced Cray-1 1976 (see Russell [1978]). Cray-1 useda vector-register architecture lower start-up overhead significantly reducememory bandwidth requirements. also efficient support non-unit strideand invented chaining. importantly, Cray-1 fastest scalar proces-sor world time. matching good scalar vector performance probably significant factor making Cray-1 success. customers bought processor primarily outstanding scalar performance.Many subsequent vector processors based architecture firstcommercially successful vector processor. Baskett Keller [1977] provided agood evaluation Cray-1. 1981, CDC started shipping CYBER 205 (see Lincoln [1982]). 205 basic architecture STAR offered improved performance allaround well expandability vector unit four lanes, multiple functional units wide load-store pipe provided multiple words per clock. peak performance CYBER 205 greatly exceeded perfor-mance Cray-1; however, real programs, performance difference wasmuch smaller. 1983, Cray Research shipped first Cray X-MP (see Chen [1983]). improved clock rate (9.5 ns versus 12.5 ns Cray-1), better chainingsupport (allowing vector operations RAW dependencies operate paral-lel), multiple memory pipelines, processor maintained Cray Research lead supercomputers. Cray-2, completely new design configurable four processors, introduced later. major feature Cray-2 useM.6 Development SIMD Supercomputers, Vector Computers, Multimedia ■M-47of DRAM, made possible large memories time. first Cray-2, 256M word (64-bit words) memory, contained memory total Cray machines shipped point! Cray-2 muchfaster clock X-MP, also much deeper pipelines; however, lackedchaining, enormous memory latency, one memory pipe per pro-cessor. general, Cray-2 faster Cray X-MP problems thatrequired large main memory. year, processor vendors Japan entered supercomputer mar- ketplace. First Fujitsu VP100 VP200 (see Miura Uchida [1983]), later came Hitachi S810 NEC SX/2 (see Watanabe [1987]). processors proved close Cray X-MP performance. general, thesethree processors much higher peak performance Cray X-MP. How-ever, large start-up overhead, typical performance often lowerthan Cray X-MP. Cray X-MP favored multiple-processorapproach, first offering two-processor version later four-processor version.In contrast, three Japanese processors expandable vector capabilities. 1988, Cray Research introduced Cray Y-MP —a bigger faster ver- sion X-MP. Y-MP allowed eight processors lowered cycle time 6 ns. full complement eight processors, Y-MP generallythe fastest supercomputer, though single-processor Japanese supercomputerscould faster one-processor Y-MP. late 1989, Cray Research splitinto two companies, aimed building high-end processors available theearly 1990s. Seymour Cray headed spin-off, Cray Computer Corporation, untilits demise 1995. initial processor, Cray-3, implemented ingallium arsenide, unable develop reliable cost-effective implementation technology. Shortly tragic death car accident 1996, Seymour Cray started yet another company develop high-performancesystems time using commodity components. Cray Research focused C90, new high-end processor 16 processors clock rate 240 MHz. processor delivered 1991.In 1993, Cray Research introduced first highly parallel processor, theT3D, employing 2048 Digital Alpha21064 microprocessors. 1995, theyannounced availability new low-end vector machine, J90, high-end machine, T90. T90 much like C90, clock twice fast (460 MHz), using three-dimensional packaging opticalclock distribution. 1995, Cray Research acquired Silicon Graphics. 1998, released SV1 system, grafted considerably faster CMOS processors onto J90memory system. also added data cache vectors CPU help meet theincreased memory bandwidth demands. Silicon Graphics sold Cray Research toTera Computer 2000, joint company renamed Cray Inc. Japanese supercomputer makers continued evolve designs. 2001, NEC SX/5 generally held fastest available vector super-computer, 16 lanes clocking 312 MHz 16 processors sharingthe memory. NEC SX/6, released 2001, first commercialsingle-chip vector microprocessor, integrating out-of-order quad-issueM-48 ■Appendix Historical Perspectives Referencessuperscalar processor, scalar instruction data caches, eight-lane vector unit single die [Kitagawa et al. 2003]. Earth Simulator constructed 640 nodes connected full crossbar, node comprises eight SX-6vector microprocessors sharing local memory. SX-8, released 2004,reduces number lanes four increases vector clock rate 2GHz. scalar unit runs slower 1 GHz clock rate, common pattern vectormachines lack hazards simplifies use deeper pipelines thevector unit. 2002, Cray Inc. released X1 based completely new vector ISA. X1 SSP processor chip integrates out-of-order superscalar scalar caches running 400 MHz two-lane vector unit running 800 MHz. fourSSP chips ganged together form MSP, resulting peak vector perfor-mance 12.8 GFLOPS competitive contemporary NEC SX machines.The X1E enhancement, delivered 2004, raises clock rates 565 1130MHz, respectively. Many ideas borrowed Cray T3E design,which MIMD (Multiple Instruction, Multiple Data) computer uses off-the-shelf microprocessors. X1 new instruction set larger number reg- isters memory distributed locally processor shared address space. out-of-order scalar unit vector units decoupled, scalarunit get ahead vector unit. Vectors become shorter data areblocked utilize MSP caches, good match eight-lane vec-tor unit. handle shorter vectors, processor two vector lanescan work different loop. Cray X2 announced 2007, may prove last Cray vec- tor architecture built, ’s difficult justify investment new silicon given size market. processor 1.3 GHz clock rate 8 vector lanes processor peak performance 42 GFLOP/sec single precision. Itincludes L1 L2 caches. node 4-way SMP 128 GBytesof DRAM, maximum size 8K nodes. NEC SX-9 16 processors per node, processor 8 lanes running 3.2 GHz. announced 2008. peak double pre-cision vector performance 102 GFLOP/sec. 16 processor SMP have1024 GBytes DRAM. maximum size 512 nodes. basis modern vectorizing compiler technology notion data dependence developed Kuck colleagues [1974] University ofIllinois. Padua Wolfe [1986] gave good overview vectorizing compilertechnology. Multimedia SIMD Instruction Extensions could computer hardware company …possibly common disco dancing. lot, one goes advertisement campaign released bythe world ’s largest microprocessor company …Intel, 1997. IBS Center Management Research “Dancing Way Towards Leadership, ”2002M.6 Development SIMD Supercomputers, Vector Computers, Multimedia ■M-49Going history books, 1957 TX-2 partitioned ALUs support media time, ideas faded away rediscovered 30 years later personal computer era. Since every desktop microprocessor definition hasits graphical displays, transistor budgets increased inevitable thatsupport would added graphics operations. Many graphics systems use 8 bitsto represent 3 primary colors plus 8 bits transparency pixel.The addition speakers microphones teleconferencing video gamessuggested support sound well. Audio samples need 8 bits pre-cision, 16 bits sufficient. Every microprocessor special support bytes half words take less space stored memory, due infrequency arithmetic oper-ations data sizes typical integer programs, little supportbeyond data transfers. Intel i860 justified graphical acceleratorwithin company. architects recognized many graphics audio appli-cations would perform operation vectors data [Atkins 1991;Kohn 1989]. Although vector unit beyond transistor budget i860in 1989, partitioning carry chains within 64-bit ALU, could perform simultaneous operations short vectors eight 8-bit operands, four 16-bit oper- ands, two 32-bit operands. cost partitioned ALUs small.Applications lend support include MPEG (video), videogames (3D graphics), digital photography, teleconferencing (audio imageprocessing). Like virus, time multimedia support spread nearly every desktop microprocessor. HP first successful desktop RISC includesuch support, soon every manufacturer take idea 1990s. extensions originally called subword parallelism orvector . Since Intel marketing used SIMD describe MMX extension 80x86announced 1996, became popular name, due part successful tele-vision advertising campaign involving disco dancers wearing clothing modeledafter cleansuits worn semiconductor fabrication lines. Graphical Processor Units It’s almost three years since GPU computing broke mainstream HPC introduction NVIDIA ’s CUDA API September 2007. Adoption technology since proceeded surprisingly strong steadypace. Many organizations began small pilot projects year twoago moved enterprise deployment, GPU accelerated machinesare represented TOP500 list starting position two. relativelyrapid adoption CUDA community known rapid adoption much anything noteworthy signal. Contrary accepted wisdom GPU computing difficult, believe success thus far signals nomore complicated good CPU programming. Further, clearly andsuccinctly expresses parallelism large class problems leading codeM-50 ■Appendix Historical Perspectives Referencesthat easier maintain, scalable better positioned map future many-core architectures. Vincent Natol “Kudos CUDA, ”HPCwire (2010) 3D graphics pipeline hardware evolved large expensive systems early 1980s small workstations PC accelerators mid- late1990s. period, three major transitions occurred: ■Performance-leading graphics subsystems declined price $50,000 $200. ■Performance increased 50 million pixels per second 1 billion pixels per second 100,000 vertices per second 10 million vertices per second. ■Native hardware capabilities evolved wireframe (polygon outlines) toflat-shaded (constant color) filled polygons, smooth-shaded (interpolated color) filled polygons, full-scene anti-aliasing texture mapping rudimentary multitexturing. Scalable GPUs Scalability attractive feature graphics systems beginning.Workstation graphics systems gave customers choice pixel horse-power vary-ing number pixel processor circuit boards installed. Prior mid-1990s PCgraphics scaling almost nonexistent. one option —the VGA controller. 3D-capable accelerators appeared, market room range offerings. 3dfx introduced multiboard scaling original SLI (Scan Line Interleave) Voodoo2, held performance crown time (1998). Also in1998, NVIDIA introduced distinct products variants single architecture withRiva TNT Ultra (high-performance) Vanta (low-cost), first speed binning andpackaging, separate chip designs (GeForce 2 GTS GeForce 2 MX). Atpresent, given architecture generation, four five separate GPU chip designs areneeded cover range desktop PC performance price points. addition,there separate segments notebook workstation systems. acquiring 3dfx, NVIDIA continued multi-GPU SLI concept 2004, starting GeForce 6800 —providing multi-GPU scalability transparently programmer user. Functional behavioris identical across scaling range; one application rununchanged implementation architectural family. Graphics Pipelines Early graphics hardware configurable, programmable applica-tion developer. generation, incremental improvements offered;however, developers growing sophisticated asking newfeatures could reasonably offered built-in fixed functions. NVIDIAM.6 Development SIMD Supercomputers, Vector Computers, Multimedia ■M-51GeForce 3, described Lindholm et al. [2001], took first step toward true gen- eral shader programmability. exposed application developer private internal instruction set floating-point vertex engine. coin-cided release Microsoft ’s DirectX 8 OpenGL ’s vertex shader exten- sions. Later GPUs, time DirectX 9, extended general programmability andfloating-point capability pixel fragment stage made texture available atthe vertex stage. ATI Radeon 9700, introduced 2002, featured program-mable 24-bit floating-point pixel fragment processor programmed DirectX 9and OpenGL. GeForce FX added 32-bit floating-point pixel processors. part general trend toward unifying functionality different stages, least far application programmer concerned. NVIDIA ’s GeForce 6800 7800 series built separate processor designs separate hard-ware dedicated vertex fragment processing. XBox 360 intro-duced early unified processor GPU 2005, allowing vertex pixel shaders toexecute processor. GPGPU: Intermediate Step DirectX 9-capable GPUs became available, researchers took notice theraw performance growth path GPUs began explore use GPUs tosolve complex parallel problems. DirectX 9 GPUs designed match features required graphics API. access computational resources, programmer cast problem native graphics operations.For example, run many simultaneous instances pixel shader, triangle hadto issued GPU (with clipping rectangle shape wasdesired). Shaders means perform arbitrary scatter operations tomemory. way write result memory emit pixel colorvalue configure framebuffer operation stage write (or blend, desired)the result two-dimensional framebuffer. Furthermore, way get result one pass computation next write parallel results pixel framebuffer, use framebuffer texture map input tothe pixel fragment shader next stage computation. Mapping generalcomputations GPU era quite awkward. Nevertheless, intrepidresearchers demonstrated handful useful applications painstaking efforts.This field called “GPGPU ”for general-purpose computing GPUs. GPU Computing developing Tesla architecture GeForce 8800, NVIDIA realized potential usefulness would much greater programmers could think GPU processor. NVIDIA selected programming approach program- mers would explicitly declare data-parallel aspects workload. DirectX 10 generation, NVIDIA already begun work highef- ficiency floating-point integer processor could run variety ofM-52 ■Appendix Historical Perspectives Referencessimultaneous workloads support logical graphics pipeline. processor designed take advantage common case groups threads executing code path. NVIDIA added memory load store instructions inte-ger byte addressing support requirements compiled C programs. intro-duced thread block (cooperative thread array), grid thread blocks, barriersynchronization dispatch manage highly parallel computing work. Atomicmemory operations added. NVIDIA developed CUDA C/C++ compiler,libraries, runtime software enable programmers readily access newdata-parallel computation model develop applications. create vendor-neutral GPU programming language, large number com- panies creating compilers OpenCL language, many fea-tures CUDA runs many platforms. 2011, performance ismuch higher write CUDA code GPUs write OpenCL code. AMD ’s acquisition ATI, second leading GPU vendor, suggests spread GPU computing. AMD Fusion architecture, announced editionwas finished, initial merger traditional GPUs traditionalCPUs. NVIDIA also announced Project Denver, combines ARM scalar processor NVIDIA GPUs single address space. systems shipped, interesting learn tightly integrated theimpact integration performance energy data parallel graphicsapplications. References SIMD Supercomputers Bouknight, W. J., S. A. Deneberg, D. E. McIntyre, J. M. Randall, A. H. Sameh, D. L. Slotnick [1972]. “The Illiac IV system, ”Proc. IEEE 60:4, 369 –379. Also appears D. P. Siewiorek, C. G. Bell, A. Newell, Computer Structures: Principles Examples , McGraw-Hill, New York, 1982, 306 –316. Hillis, W. D. [1985]. Connection Multiprocessor , MIT Press, Cambridge, Mass. Hord, R. M. [1982]. Illiac-IV, First Supercomputer , Computer Science Press, Rockville, Md. Slotnick, D. L., W. C. Borck, R. C. McReynolds [1962]. “The Solomon com- puter, ”Proc. AFIPS Fall Joint Computer Conf. , December 4 –6, 1962, Philadelphia, Penn., 97 –107. Unger, S. H. [1958]. “A computer oriented towards spatial problems, ”Proc. Insti- tute Radio Engineers 46:10 (October), 1744 –1750. Vector Architecture Asanovic, K. [1998]. “Vector Microprocessors, ”Ph.D. thesis, Computer Science Division, University California, Berkeley. Baskett, F., T. W. Keller [1977]. “An Evaluation Cray-1 Processor, ”in High Speed Computer Algorithm Organization , D. J. Kuck, D. H. Lawrie, A. H. Sameh, eds., Academic Press, San Diego, Calif., 71 –84.M.6 Development SIMD Supercomputers, Vector Computers, Multimedia ■M-53Chen, S. [1983]. “Large-scale high-speed multiprocessor system scientific applications, ”Proc. NATO Advanced Research Workshop High Speed Com- puting , June 20 –22, Julich, West Germany. Also K. Hwang, ed., “Superpro- cessors: Design applications, ”IEEE , August, 59 –73, 1984. Flynn, M. J. [1966]. “Very high-speed computing systems, ”Proc. IEEE 54:12 (December), 1901 –1909. Gebis, J. Patterson, D. [2007]. “Embracing extending 20th-century instruction set architectures, ”IEEE Computer , 40:4 (April), 68 –75. Hintz, R. G., D. P. Tate [1972]. “Control data STAR-100 processor design, ” Proc. IEEE COMPCON , September 12 –14, 1972, San Francisco, 1 –4. Kitagawa, K., S. Tagaya, Y. Hagihara, Y. Kanoh [2003]. “A hardware over- view SX-6 SX-7 supercomputer, ”NEC Research Development Jour- nal44:1 (January), 2 –7. Kozyrakis, C., D. Patterson [2002]. “Vector vs. superscalar VLIW archi- tectures embedded multimedia benchmarks, ”Proc. 35th Annual Intl. Sym- posium Microarchitecture (MICRO) , November 18 –22, 2002, Istanbul, Turkey. Kuck, D., P. P. Budnik, S.-C. Chen, D. H. Lawrie, R. A. Towle, R. E. Strebendt, E. W. Davis, Jr., J. Han, P. W. Kraska, Y. Muraoka [1974]. “Measurements parallelism ordinary Fortran programs, ”Computer 7:1 (January), 37 –46. Lincoln, N. R. [1982]. “Technology design trade offs creation mod- ern supercomputer, ”IEEE Trans. Computers C-31:5 (May), 363 –376. Miura, K., K. Uchida [1983]. “FACOM vector processing system: VP100/ 200,”Proc. NATO Advanced Research Workshop High Speed Computing , June 20 –22, Julich, West Germany. Also K. Hwang, ed., “Superprocessors: Design applications, ”IEEE , August, 59 –73, 1984. Padua, D., M. Wolfe [1986]. “Advanced compiler optimizations supercom- puters, ”Communications ACM 29:12 (December), 1184 –1201. Russell, R. M. [1978]. “The Cray-1 processor system, ”Communications ACM 21:1 (January), 63 –72. Vajapeyam, S. [1991]. “Instruction-Level Characterization Cray Y-MP Pro- cessor, ”Ph.D. thesis, Computer Sciences Department, University Wiscon- sin–Madison. Watanabe, T. [1987]. “Architecture performance NEC supercomputer SX system, ”Parallel Computing 5, 247 –255. Watson, W. J. [1972]. “The TI ASC —a highly modular flexible super processor architecture, ”Proc. AFIPS Fall Joint Computer Conf. , December 5–7, 1972, Anaheim, Calif., 221 –228. Multimedia SIMD Atkins, M. [1991]. “Performance i860 Microprocessor, ”IEEE Micro , 11:5 (September), 24 –27, 72 –78. Kohn, L., N. Margulis [1989]. “Introducing Intel i860 64-Bit Microproces- sor,”IEEE Micro , 9:4 (July), 15 –30.M-54 ■Appendix Historical Perspectives ReferencesGPU Akeley, K., T. Jermoluk [1988]. “High-performance polygon rendering, ” Proc. SIGGRAPH 88 , August 1 –5, 1988, Atlanta, Ga., 239 –46. Hillis, W. D., G. L. Steele [1986]. “Data parallel algorithms, ”Communications ACM 29:12 (December), 1170 –1183 ( http://doi.acm.org/10.1145/7902. 7903 ). IEEE 754-2008 Working Group. [2006]. DRAFT Standard Floating-Point Arithmetic , 754-2008 ( https://doi.org/10.1109/IEEESTD.2008.4610935 ). Lee, W. V., et al. [2010]. “Debunking 100X GPU vs. CPU myth: evaluation throughput computing CPU GPU, ”Proc. ISCA ’10, June 19 –23, 2010, Saint-Malo, France. Lindholm, E., M. J. Kligard, H. Moreton [2001]. user-programmable vertex engine. SIGGRAPH ’01:Proceedings 28th annual conference Com- puter graphics interactive techniques , 149 –158. Moore, G. E. [1965]. “Cramming components onto integrated circuits, ”Elec- tronics 38:8 (April 19), 114 –117. Williams, S., A. Waterman, D. Patterson [2009]. “Roofline: insightful visual performance model multicore architectures, ”Communications ACM , 52:4 (April), 65 –76. M.7 History Multiprocessors Parallel Processing (Chapter 5 Appendices F ,G, I) tremendous amount history multiprocessors; section, divide discussion time period architecture. start SIMDapproach Illiac IV. turn short discussion earlyexperimental multiprocessors progress discussion greatdebates parallel processing. Next discuss historical roots presentmultiprocessors conclude discussing recent advances. SIMD Computers: Attractive Idea, Many Attempts, Lasting Successes cost general multiprocessor is, however, high design options considered would decrease cost without seriously degrading power efficiency system. options consist recen-tralizing one three major components. …Centralizing [control unit] gives rise basic organization [an] …array processor Illiac IV. Bouknight et al. [1972]M.7 History Multiprocessors Parallel Processing ■M-55The SIMD model one earliest models parallel computing, dating back first large-scale multiprocessor, Illiac IV. key idea mul- tiprocessor, recent SIMD multiprocessors, single instructionthat operates many data items once, using many functional units. earliest ideas SIMD-style computers Unger [1958] Slot- nick, Borck, McReynolds [1962]. Slotnick ’s Solomon design formed basis Illiac IV, perhaps infamous supercomputer projects.Although successful pushing several technologies proved useful laterprojects, failed computer. Costs escalated $8 million estimate 1966 $31 million 1972, despite construction quarter planned multiprocessor. Actual performance best 15 MFLOPS versus initial predic-tions 1000 MFLOPS full system [Hord 1982]. Delivered NASA AmesResearch 1972, computer took three years engineering wasusable. events slowed investigation SIMD, Danny Hillis [1985] resus-citated style Connection Machine, 65,636 1-bit processors. Real SIMD computers need mixture SISD SIMD instructions. SISD host computer perform operations branches address calculations need parallel operation. SIMD instructions broadcast execution units, set registers.For flexibility, individual execution units disabled SIMD instruc-tion. addition, massively parallel SIMD multiprocessors rely interconnectionor communication networks exchange data processing elements. SIMD works best dealing arrays loops; hence, opportu- nity massive parallelism SIMD must massive amounts data, data parallelism . SIMD weakest casestatements, eachexecution unit must perform different operation data, depending data has. execution units wrong data disabled proper units continue. sit-uations essentially run 1/ nth performance, nis number cases. basic trade-off SIMD multiprocessors performance processor versus number processors. Recent multiprocessors emphasize large degreeof parallelism performance individual processors. ConnectionMultiprocessor 2, example, offered 65,536 single-bit-wide processors, whilethe Illiac IV 64 64-bit processors. resurrected 1980s, first Thinking Machines MasPar, SIMD model put bed general-purpose mul-tiprocessor architecture, two main reasons. First, inflexible. numberof important problems cannot use style multiprocessor, architec-ture scale competitive fashion; is, small-scale SIMDmultiprocessors often worse cost-performance compared thealternatives. Second, SIMD cannot take advantage tremendous performanceand cost advantages microprocessor technology. Instead leveraging low- cost technology, designers SIMD multiprocessors must build custom processors multiprocessors. Although SIMD computers departed scene general-purpose alternatives, style architecture continue role special-purposeM-56 ■Appendix Historical Perspectives Referencesdesigns. Many special-purpose tasks highly data parallel require limited set functional units. Thus, designers build support certain operations, well hardwired interconnection paths among functional units. organi-zations often called array processors , useful tasks image signal processing. Early Experiments difficult distinguish first MIMD multiprocessor. Surprisingly, firstcomputer Eckert-Mauchly Corporation, example, duplicate unitsto improve availability. Holland [1959] gave early arguments multiple proces-sors. Two best-documented multiprocessor projects undertaken the1970s Carnegie Mellon University. first C.mmp [Wulf Bell1972; Wulf Harbison 1978], consisted 16 PDP-11s connected acrossbar switch 16 memory units. among first multiprocessors withmore processors, shared-memory programming model. Much focus research C.mmp project software, especially OS area. later multiprocessor, Cm* [Swan et al. 1977], cluster-basedmultiprocessor distributed memory nonuniform access time. Theabsence caches long remote access latency made data placement critical.This multiprocessor number application experiments well described byGehringer, Siewiorek, Segall [1987]. Many ideas multiproces-sors would reused 1980s microprocessor made much cheaperto build multiprocessors. Great Debates Parallel Processing turning away conventional organization came middle 1960s,when law diminishing returns began take effect effort increasethe operational speed computer. …Electronic circuits ultimately limited speed operation speed light …and many circuits already operating nanosecond range. Bouknight et al. [1972] …sequential computers approaching fundamental physical limit potential computational power. limit speed light … Angel L. DeCegama Technology Parallel Processing, Vol. (1989) …today ’s multiprocessors …are nearing impasse technologies approach speed light. Even components sequential processor could made work fast, best could expected millioninstructions per second. David Mitchell Transputer: Time (1989)M.7 History Multiprocessors Parallel Processing ■M-57The quotes give classic arguments abandoning current form computing, Amdahl [1967] gave classic reply support continued focus IBM 360 architecture. Arguments advantages parallel exe-cution traced back 19th century [Menabrea 1842]! Yet, effective-ness multiprocessor reducing latency individual important programs isstill explored. Aside debates advantages limitationsof parallelism, several hot debates focused build multiprocessors. It’s hard predict future, yet 1989 Gordon Bell made two predictions 1995. included predictions first edition book, out- come completely unclear. discuss section, together assessment accuracy prediction. first computer capable sustaining teraFLOPS —one million MFLOPS —would constructed 1995, using either multicomputer 4K 32K nodes Connection Multiprocessor several million processing ele-ments [Bell 1989]. put prediction perspective, year Gordon BellPrize acknowledges advances parallelism, including fastest real program(highest MFLOPS). 1989, winner used eight-processor Cray Y-MP run 1680 MFLOPS. basis numbers, multiprocessors pro- grams would improved factor 3.6 year fastestprogram achieve 1 TFLOPS 1995. 1999, first Gordon Bell prize winnercrossed 1 TFLOPS bar. Using 5832-processor IBM RS/6000 SST systemdesigned specially Livermore Laboratories, achieved 1.18 TFLOPS ona shock-wave simulation. ratio represents year-to-year improvement of1.93, still quite impressive. become recognized since 1990s that, although may technology build TFLOPS multiprocessor, clear machine cost effective, except perhaps specialized critically importantapplications related national security. estimated 1990 achieve 1TFLOPS would require machine 5000 processors would costabout $100 million. 5832-processor IBM system Livermore cost $110 mil- lion. might expected, improvements performance individual micro-processors cost performance directly affect cost performance oflarge-scale multiprocessors, 5000-processor system cost 5000 times price desktop system using processor. Since time, much faster multiprocessors built, major improvements increas-ingly come processors past five years, rather fundamentalbreakthroughs parallel architecture. second Bell prediction concerned number data streams supercom- puters shipped 1995. Danny Hillis believed that, although supercomputers witha small number data streams may best sellers, biggest multiprocessorswould multiprocessors many data streams, would perform bulk computations. Bell bet Hillis last quarter calendar year 1995 sustained MFLOPS would shipped multiprocessors using fewdata streams ( /C20100) rather many data streams ( /C211000). bet concernedM-58 ■Appendix Historical Perspectives Referencesonly supercomputers, defined multiprocessors costing $1 million used scientific applications. Sustained MFLOPS defined bet number floating-point operations per month , availability multiprocessors affects rating. 1989, bet made, totally unclear would win. 1995, survey current publicly known supercomputers showed sixmultiprocessors existence world 1000 data streams,so Bell ’s prediction clear winner. fact, 1995, much smaller microprocessor-based multiprocessors ( /C2020 processors) becoming domi- nant. 1995, survey 500 highest-performance multiprocessors use (based Linpack ratings), called TOP500, showed largest numberof multiprocessors bus-based shared-memory multiprocessors! 2005,various clusters multicomputers played large role. example, top25 systems, 11 custom clusters, IBM Blue Gene system theCray XT3; 10 clusters shared-memory multiprocessors (both using distrib-uted centralized memory); remaining 4 clusters built using PCswith off-the-shelf interconnect. Recent Advances Developments primary exception parallel vector multiprocessors (see AppendixG) recently IBM Blue Gene design, recent MIMDcomputers built off-the-shelf microprocessors using bus log- ically central memory interconnection network distributed memory. number experimental multiprocessors built 1980s refined andenhanced concepts form basis many today ’s multiprocessors. Development Bus-Based Coherent Multiprocessors Although large mainframes built multiple processors 1960s 1970s, multiprocessors become highly successful 1980s. Bell[1985] suggested key smaller size microprocessor allowed memory bus replace interconnection network hardware portable operating systems meant multiprocessor projects longer requiredthe invention new operating system. paper, Bell defined terms mul- tiprocessor andmulticomputer set stage two different approaches building larger scale multiprocessors. first bus-based multiprocessor snooping caches Synapse N+1 described Frank [1984]. Goodman [1983] wrote one first papersto describe snooping caches. late 1980s saw introduction many commer- cial bus-based, snooping cache architectures, including Silicon Graphics 4D/240 [Baskett, Jermoluk, Solomon 1988], Encore Multimax [Wilson1987], Sequent Symmetry [Lovett Thakkar 1988]. mid-1980sM.7 History Multiprocessors Parallel Processing ■M-59saw explosion development alternative coherence protocols, Archibald Baer [1986] provided good survey analysis, well refer-ences original papers. Figure M.2 summarizes several snooping cache coher- ence protocols shows multiprocessors used using thatprotocol. early 1990s saw beginning expansion systems use wide, high-speed buses (the SGI Challenge system used 256-bit,packet-oriented bus supporting 8 processor boards 32 processors) later use multiple buses crossbar interconnects —for example, Sun SPARCCenter Enterprise systems (Charlesworth [1998] discussed theinterconnect architecture multiprocessors). 2001, Sun Enterpriseservers represented primary example large-scale ( >16 processors), symmet- ric multiprocessors active use. Today, bus-based machines offer fouror processors switches, alternative designs used eight more. Toward Large-Scale Multiprocessors effort build large-scale multiprocessors, two different directions explored: message-passing multicomputers scalable shared-memory multipro- cessors. Although many attempts build mesh hypercube-connected multiprocessors, one first multiprocessors successfully bringtogether pieces Cosmic Cube built Caltech [Seitz 1985]. intro-duced important advances routing interconnect technology substantiallyNameProtocol type Memory write policy Unique feature Multiprocessors using Write OnceWrite invalidateWrite-back first writeFirst snooping protocol described literature Synapse N+1Write invalidateWrite-back Explicit state memory ownerSynapse multiprocessors; first cache-coherent multiprocessorsavailable Berkeley (MOESI)Write invalidateWrite-back Owned shared state Berkeley SPUR multiprocessor; Sun Enterprise servers Illinois (MESI)Write invalidateWrite-back Clean private state; supply data cache aclean copySGI Power Challenge series “Firefly ” Write broadcastWrite-back private, write sharedMemory updated broadcast current multiprocessors; SPARCCenter 2000 closest Figure M.2 Five snooping protocols summarized. Archibald Baer [1986] use names describe five protocols, Eggers [1989] summarizes similarities differences shown figure. Firefly protocol named experimental DEC Firefly multiprocessor, appeared. alternative names protocolsare based states support: ¼Modified, E ¼Exclusive (private clean), ¼Shared, ¼Invalid, ¼Owner (shared dirty).M-60 ■Appendix Historical Perspectives Referencesreduced cost interconnect, helped make multicomputer viable. Intel iPSC 860, hypercube-connected collection i860s, based ideas. recent multiprocessors, Intel Paragon, used networkswith lower dimensionality higher individual links. Paragon also employeda separate i860 communications controller node, although number ofusers found better use i860 processors computation well ascommunication. Thinking Multiprocessors CM-5 made use off-the-shelfmicroprocessors fat tree interconnect (see Appendix F). provided user-level access communication channel, thus significantly improving commu- nication latency. 1995, two multiprocessors represented state art message-passing multicomputers. Early attempts building scalable shared-memory multiprocessor include IBM RP3 [Pfister et al. 1985], NYU Ultracomputer [Elder et al. 1985; Schwartz1980], University Illinois Cedar project [Gajksi et al. 1983], BBNButterfly Monarch [BBN Laboratories 1986; Rettberg et al. 1990]. Thesemultiprocessors provided variations nonuniform distributed-memory modeland, hence, distributed shared-memory (DSM) multiprocessors, support cache coherence, substantially complicated programming. RP3 Ultracomputer projects explored new ideas synchronization(fetch-and-operate) well idea combining references network.In four multiprocessors, interconnect networks turned morecostly processing nodes, raising problems smaller versions themultiprocessor. Cray T3D/E (see Arpaci et al. [1995] evaluation ofthe T3D Scott [1996] description T3E enhancements) builds theseideas, using noncoherent shared address space building advances interconnect technology developed multicomputer domain (see Scott Thorson [1996]). Extending shared-memory model scalable cache coherence done combining number ideas. Directory-based techniques cache coherencewere actually known snooping cache techniques. fact, first cachecoherence protocols actually used directories, described Tang [1976] andimplemented IBM 3081. Censier Feautrier [1978] described directorycoherence scheme tags memory. idea distributing directories memories obtain scalable implementation cache coherence first described Agarwal et al. [1988] served basis Stanford DASHmultiprocessor (see Lenoski et al. [1990, 1992]), first operationalcache-coherent DSM multiprocessor. DASH “plump ”node cc-NUMA machine used four-processor SMPs nodes, interconnecting astyle similar Wildfire using scalable two-dimensional gridrather crossbar interconnect. Kendall Square Research KSR-1 [Burkhardt et al. 1992] first com- mercial implementation scalable coherent shared memory. extended basic DSM approach implement concept called cache-only memory architecture (COMA), makes main memory cache. KSR-1, memory blockscould replicated main memories node hardware support toM.7 History Multiprocessors Parallel Processing ■M-61handle additional coherence requirements replicated blocks. (The KSR-1 strictly pure COMA migrate home location data item always kept copy home. Essentially, implemented replication.)Many research proposals [Falsafi Wood 1997; Hagersten, Landin, Har-idi 1992; Saulsbury et al. 1995; Stenstr €om, Joe, Gupta 1992] COMA-style architectures similar approaches reduce burden nonuniform memoryaccess migration [Chandra et al. 1994; Soundararajan et al. 1998] devel-oped, commercial implementations. Convex Exemplar implemented scalable coherent shared memory using two-level architecture: lowest level, eight-processor modules built using crossbar. ring connect 32 modules, total 256processors (see Thekkath et al. [1997] evaluation). Laudon Lenoski[1997] described SGI Origin, first delivered 1996 closelybased original Stanford DASH machine, although including number ofinnovations scalability ease programming. Origin uses bit vectorfor directory structure, either 16 32 bits long. bit representsa node, consists two processors; coarse bit vector representation allows bit represent 8 nodes total 1024 processors. Galles [1996] described, high-performance fat hypercube used global interconnect.Hristea, Lenoski, Keen [1997] provided thorough evaluation theperformance Origin memory system. Several research prototypes undertaken explore scalable coherence without multithreading. include MIT Alewife machine[Agarwal et al. 1995] Stanford FLASH multiprocessor [Gibson et al.2000; Kuskin et al. 1994]. Clusters Clusters probably “invented ”in 1960s customers could fit work one computer needed backup machine case failureof primary machine [Pfister 1998]. Tandem introduced 16-node cluster in1975. Digital followed VAX clusters, introduced 1984. originallyindependent computers shared I/O devices, requiring distributed operatingsystem coordinate activity. Soon communication links com-puters, part computers could geographically distributed toincrease availability case disaster single site. Users log onto cluster unaware machine running on. DEC (now HP) sold 25,000 clusters 1993. early companies Tandem (nowHP) IBM (still IBM). Today, virtually every company cluster products.Most products aimed availability, performance scaling asecondary benefit. Scientific computing clusters emerged competitor MPPs. 1993, Beowulf project started goal fulfilling NASA ’s desire 1 GFLOPS computer $50,000. 1994, 16-node cluster built off-the-shelfM-62 ■Appendix Historical Perspectives ReferencesPCs using 80486s achieved goal [Bell Gray 2001]. emphasis led variety software interfaces make easier submit, coordinate, debug large programs large number independent programs. Efforts made reduce latency communication clusters well increase bandwidth, several research projects worked problem.(One commercial result low-latency research VI interface standard,which embraced Infiniband, discussed below.) Low latency thenproved useful applications. example, 1997 cluster 100 Ultra-SPARC desktop computers University California –Berkeley, connected 160 MB/sec per link Myrinet switches, used set world records database sort—sorting 8.6 GB data originally disk 1 minute —and cracking encrypted message —taking 3.5 hours decipher 40-bit DES key. research project, called Network Workstations [Anderson, Culler, Patterson 1995], also developed Inktomi search engine, led startupcompany name. Google followed example Inktomi buildsearch engines clusters desktop computers rather large-scale SMPs, whichwas strategy leading search engine Alta Vista Google overtook [Brin Page 1998]. 2011, nearly Internet services rely clusters serve millions customers. Clusters also popular scientists. One reason low cost, individual scientists small groups cluster dedicated programs.Such clusters get results faster waiting long job queues sharedMPPs supercomputer centers, stretch weeks. interested inlearning more, Pfister [1998] wrote entertaining book clusters. Recent Trends Large-Scale Multiprocessors mid- late 1990s, became clear hoped growth market forultralarge-scale parallel computing unlikely occur. Without marketgrowth, became increasingly clear high-end parallel computing marketcould support costs highly customized hardware software designed small market. Perhaps important trend come observa- tion clustering would used reach highest levels performance.There four general classes large-scale multiprocessors: ■Clusters integrate standard desktop motherboards using interconnectiontechnology Myrinet Infiniband. ■Multicomputers built standard microprocessors configured proces-sing elements connected custom interconnect. include CrayXT3 (which used earlier version Cray interconnect simple clusterarchitecture) IBM Blue Gene (more unique machine momentarily). ■Clusters small-scale shared-memory computers, possibly vectorsupport, includes Earth Simulator (which journalavailable online).M.7 History Multiprocessors Parallel Processing ■M-63■Large-scale shared-memory multiprocessors, Cray X1 [Dunigan et al. 2005] SGI Origin Altix systems. SGI systems also configured clusters provide 512 processors, although onlymessage passing supported across clusters. IBM Blue Gene interesting designs since rationale par- allels underlying causes recent trend toward multicore uniprocessorarchitectures. Blue Gene started research project within IBM aimed pro-tein sequencing folding problem. Blue Gene designers observed power becoming increasing concern large-scale multiprocessors performance/watt processors embedded space much betterthat high-end uniprocessor space. parallelism route highperformance, start efficient building block simply havemore them? Thus, Blue Gene constructed using custom chip includes embedded PowerPC microprocessor offering half performance high-end PowerPC,but much smaller fraction area power. allows system func- tions, including global interconnect, integrated onto die. result highly replicable efficient building block, allowing Blue Gene toreach much larger processor counts efficiently. Instead using stand-alonemicroprocessors standard desktop boards building blocks, Blue Gene usesprocessor cores. doubt approach provides much greaterefficiency. Whether market support cost customized design andspecial software remains open question. 2006, Blue Gene processor Lawrence Livermore 32K processors (and scheduled go 65K late 2005) holds factor 2.6 lead Linpack performance third-place system consisting 20 SGI Altix 512-processorsystems interconnected Infiniband cluster. Blue Gene ’s predecessor experimental machine, QCDOD, pio- neered concept machine using lower-power embedded microprocessorand tightly integrated interconnect drive cost power consumptionof node. Developments Synchronization Consistency Models wide variety synchronization primitives proposed shared-memory multiprocessors. Mellor-Crummey Scott [1991] provided over-view issues well efficient implementations important primitives,such locks barriers. extensive bibliography supplies references otherimportant contributions, including developments spin locks, queuing locks, andbarriers. Lamport [1979] introduced concept sequential consistency correct execution parallel programs means. Dubois, Scheurich, Briggs [1988] introduced idea weak ordering (originally 1986). 1990, Adveand Hill provided better definition weak ordering also defined conceptof data-race-free; conference, Gharachorloo colleagues [1990]introduced release consistency provided first data performance ofM-64 ■Appendix Historical Perspectives Referencesrelaxed consistency models. relaxed consistency models widely adopted microprocessor architectures, including Sun SPARC, Alpha, IA-64. Adve Gharachorloo [1996] provided excellent tutorial mem-ory consistency differences among models. References concept using virtual memory implement shared address space amongdistinct machines pioneered Kai Li ’s Ivy system 1988. sub- sequent papers exploring hardware support issues, software mechanisms, pro-gramming issues. Amza et al. [1996] described system built workstations usinga new consistency model, Kontothanassis et al. [1997] described software shared-memory scheme using remote writes, Erlichson et al. [1996] described useof shared virtual memory build large-scale multiprocessors using SMPs nodes. Thereisanalmostunboundedamountofinformationonmultiprocessorsandmul- ticomputers: Conferences, journal papers, even books seem appear faster thanany single person absorb ideas. doubt many papers go unno-ticed—not unlike past. major architecture conferences contain papers multiprocessors. annual conference, Supercomputing XY(where XandYare last two digits year), brings together users, architects, software developers,and vendors, proceedings published book, CD-ROM, online (seewww.scXY.org ) form. Two major journals, Journal Parallel Distributed Com- puting IEEE Transactions Parallel Distributed Systems ,c n n papers aspects parallel processing. Several books focusing parallel pro-cessing included following references, Culler, Singh, Gupta [1999]being recent, large-scale effort. years, Eugene Miya NASA AmesResearch Center collected online bibliography parallel-processing papers.The bibliography, contains 35,000 entries, available online atliinwww.ira.uka.de/bibliography/Parallel/Eugene/index.html . addition documenting discovery concepts used practice, references also provide descriptions many ideas explored found wanting, well ideas whose time yet come. Given themove toward multicore multiprocessors future high-performancecomputer architecture, expect many new approaches explored inthe years ahead. manage solve hardware softwareproblems key using multiprocessing past 40 years! References Adve, S. V., K. Gharachorloo [1996]. “Shared memory consistency models: tutorial, ”IEEE Computer 29:12 (December), 66 –76. Adve, S. V., M. D. Hill [1990]. “Weak ordering —a new definition, ”Proc. 17th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 28 –31, 1990, Seattle, Wash., 2 –14.M.7 History Multiprocessors Parallel Processing ■M-65Agarwal, A., R. Bianchini, D. Chaiken, K. Johnson, D. Kranz [1995]. “The MIT Alewife machine: Architecture performance, ”22nd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 22 –24, 1995, Santa Mar- gherita, Italy, 2 –13. Agarwal, A., J. L. Hennessy, R. Simoni, M. A. Horowitz [1988]. “An evaluation directory schemes cache coherence, ”Proc. 15th Annual Int ’l. Symposium Computer Architecture , May 30 –June 2, 1988, Honolulu, Hawaii, 280 –289. Agarwal, A., J. Kubiatowicz, D. Kranz, B.-H. Lim, D. Yeung, G. ’Souza, M. Parkin [1993]. “Sparcle: evolutionary processor design large-scale multiprocessors, ”IEEE Micro 13 (June), 48 –61. Alles, A. [1995]. “ATM Internetworking, ”White Paper (May), Cisco Systems, Inc., San Jose, Calif. ( www.cisco.com/warp/public/614/12.html ). Almasi, G. S., A. Gottlieb [1989]. Highly Parallel Computing , Benjamin/ Cummings, Redwood City, Calif. Alverson, G., R. Alverson, D. Callahan, B. Koblenz, A. Porterfield, B. Smith [1992]. “Exploiting heterogeneous parallelism multithreaded multiproces- sor,”Proc. ACM/IEEE Conf. Supercomputing , November 16 –20, 1992, Min- neapolis, Minn., 188 –197. Amdahl, G. M. [1967]. “Validity single processor approach achieving large scale computing capabilities, ”Proc. AFIPS Spring Joint Computer Conf. , April 18 –20, 1967, Atlantic City, N.J., 483 –485. Amza C., A. L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, W. Yu, W. Zwaenepoel [1996]. “Treadmarks: Shared memory computing networks workstations, ”IEEE Computer 29:2 (February), 18 –28. Anderson, T. E., D. E. Culler, D. Patterson [1995]. “A case (networks workstations), ”IEEE Micro 15:1 (February), 54 –64. Ang, B., D. Chiou, D. Rosenband, M. Ehrlich, L. Rudolph, Arvind [1998]. “StarT-Voyager: flexible platform exploring scalable SMP issues, ”Proc. ACM/IEEE Conf. Supercomputing , November 7 –13, 1998, Orlando, FL. Archibald, J., J.-L. Baer [1986]. “Cache coherence protocols: Evaluation using multiprocessor simulation model, ”ACM Trans. Computer Systems 4:4 (November), 273 –298. Arpaci, R. H., D. E. Culler, A. Krishnamurthy, S. G. Steinberg, K. Yelick [1995]. “Empirical evaluation CRAY-T3D: compiler perspective, ” Proc. 22nd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 22–24, 1995, Santa Margherita, Italy. Baer, J.-L., W.-H. Wang [1988]. “On inclusion properties multi-level cache hierarchies, ”Proc. 15th Annual Int ’l. Symposium Computer Architec- ture, May 30 –June 2, 1988, Honolulu, Hawaii, 73 –80. Balakrishnan, H. V., N. Padmanabhan, S. Seshan, R. H. Katz [1997]. “A com- parison mechanisms improving TCP performance wireless links, ” IEEE/ACM Trans. Networking 5:6 (December), 756 –769. Barroso, L. A., K. Gharachorloo, E. Bugnion [1998]. “Memory system char- acterization commercial workloads, ”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 3 –14.M-66 ■Appendix Historical Perspectives ReferencesBaskett, F., T. Jermoluk, D. Solomon [1988]. “The 4D-MP graphics super- workstation: Computing+graphics ¼40 MIPS +40 MFLOPS 10,000 lighted polygons per second, ”Proc. IEEE COMPCON , February 29 –March 4, 1988, San Francisco, 468 –471. BBN Laboratories. [1986]. Butterfly Parallel Processor Overview , Tech. Rep. 6148, BBN Laboratories, Cambridge, Mass. Bell, C. G. [1985]. “Multis: new class multiprocessor computers, ”Science 228 (April 26), 462 –467. Bell, C. G. [1989]. “The future high performance computers science engi- neering, ”Communications ACM 32:9 (September), 1091 –1101. Bell, C. G., J. Gray [2001]. Crays, Clusters Centers , Tech. Rep. MSR-TR- 2001-76, Microsoft Research, Redmond, Wash. Bell, C. G., J. Gray [2002]. “What ’s next high performance computing, ” CACM , 45:2 (February), 91 –95. Bouknight, W. J., S. A. Deneberg, D. E. McIntyre, J. M. Randall, A. H. Sameh, D. L. Slotnick [1972]. “The Illiac IV system, ”Proc. IEEE 60:4, 369 –379. Also appears D. P. Siewiorek, C. G. Bell, A. Newell, Computer Structures: Principles Examples , McGraw-Hill, New York, 1982, 306 –316. Brain, M. [2000]. Inside Digital Cell Phone, www.howstuffworks.com/inside- cell-phone.htm . Brewer, E. A., B. C. Kuszmaul [1994]. “How get good performance CM-5 data network, ”Proc. Eighth Int ’l. Parallel Processing Symposium (IPPS) , April 26 –29, 1994, Cancun, Mexico. Brin, S., L. Page [1998]. “The anatomy large-scale hypertextual Web search engine, ”Proc. 7th Int ’l. World Wide Web Conf. , April 14 –18, 1998, Brisbane, Queensland, Australia, 107 –117. Burkhardt III, H., S. Frank, B. Knobe, J. Rothnie [1992]. Overview KSR1 Computer System , Tech. Rep. KSR-TR-9202001, Kendall Square Research, Boston. Censier, L., P. Feautrier [1978]. “A new solution coherence problems multicache systems, ”IEEE Trans. Computers C-27:12 (December), 1112 –1118. Chandra, R., S. Devine, B. Verghese, A. Gupta, M. Rosenblum [1994]. “Scheduling page migration multiprocessor compute servers, ”Proc. Sixth Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 4 –7, 1994, San Jose, Calif., 12 –24. Charlesworth, A. [1998]. “Starfire: Extending SMP envelope, ”IEEE Micro 18:1 (January/February), 39 –49. Clark, W. A. [1957]. “The Lincoln TX-2 computer development, ”Proc. Western Joint Computer Conference , February 26 –28, 1957, Los Angeles, 143 –145. Comer, D. [1993]. Internetworking TCP/IP , 2nd ed., Prentice Hall, Engle- wood Cliffs, N.J. Culler, D. E., J. P. Singh, A. Gupta [1999]. Parallel Computer Architecture: Hardware/Software Approach , Morgan Kaufmann, San Francisco. Dally, W. J., C. I. Seitz [1986]. “The torus routing chip, ”Distributed Comput- ing1:4, 187 –196.M.7 History Multiprocessors Parallel Processing ■M-67Davie, B. S., L. L. Peterson, D. Clark [1999]. Computer Networks: Systems Approach , 2nd ed., Morgan Kaufmann, San Francisco. Desurvire, E. [1992]. “Lightwave communications: fifth generation, ” Scientific American (International Edition) 266:1 (January), 96 –103. Dongarra, J., T. Sterling, H. Simon, E. Strohmaier [2005]. “High-performance computing: Clusters, constellations, MPPs, future directions, ”Computing Science & Engineering , 7:2 (March/April), 51 –59. Dubois, M., C. Scheurich, F. Briggs [1988]. “Synchronization, coherence, event ordering, ”IEEE Computer 21:2 (February), 9 –21. Dunigan, W., K. Vetter, K. White, P. Worley [2005]. “Performance evaluation Cray X1 distributed shared memory architecture, ”IEEE Micro , January/ February, 30 –40. Eggers, S. [1989]. “Simulation Analysis Data Sharing Shared Memory Mul- tiprocessors, ”Ph.D. thesis, Computer Science Division, University Califor- nia, Berkeley. Elder, J., A. Gottlieb, C. K. Kruskal, K. P. McAuliffe, L. Randolph, M. Snir, P. Teller, J. Wilson [1985]. “Issues related MIMD shared-memory computers: NYU Ultracomputer approach, ”Proc. 12th Annual Int ’l. Symposium Computer Architecture (ISCA) ,J u n e1 7 –19, 1985, Boston, Mass., 126 –135. Erlichson, A., N. Nuckolls, G. Chesson, J. L. Hennessy [1996]. “SoftFLASH: Analyzing performance clustered distributed virtual shared memory, ” Proc. Seventh Int ’l. Conf. Architectural Support Programming Lan- guages Operating Systems (ASPLOS) , October 1 –5, 1996, Cambridge, Mass., 210 –220. Falsafi, B., D. A. Wood [1997]. “Reactive NUMA: design unifying S- COMA CC-NUMA, ”Proc. 24th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo., 229 –240. Flynn, M. J. [1966]. “Very high-speed computing systems, ”Proc. IEEE 54:12 (December), 1901 –1909. Forgie, J. W. [1957]. “The Lincoln TX-2 input-output system, ”Proc. Western Joint Computer Conference , February 26 –28, 1957, Los Angeles, 156 –160. Frank, S. J. [1984]. “Tightly coupled multiprocessor systems speed memory access time, ”Electronics 57:1 (January), 164 –169. Gajski, D., D. Kuck, D. Lawrie, A. Sameh [1983]. “CEDAR —a large scale multiprocessor, ”Proc. Int ’l. Conf. Parallel Processing (ICPP) , August, Columbus, Ohio, 524 –529. Galles, M. [1996]. “Scalable pipelined interconnect distributed endpoint rout- ing: SGI SPIDER chip, ”Proc. IEEE HOT Interconnects ’96, August 15 – 17, 1996, Stanford University, Palo Alto, Calif. Gehringer, E. F., D. P. Siewiorek, Z. Segall [1987]. Parallel Processing: Cm* Experience , Digital Press, Bedford, Mass. Gharachorloo, K., A. Gupta, J. L. Hennessy [1992]. “Hiding memory latency using dynamic scheduling shared-memory multiprocessors, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia.M-68 ■Appendix Historical Perspectives ReferencesGharachorloo, K., D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, J. L. Hen- nessy [1990]. “Memory consistency event ordering scalable shared- memory multiprocessors, ”Proc. 17th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 28 –31, 1990, Seattle, Wash., 15 –26. Gibson, J., R. Kunz, D. Ofelt, M. Horowitz, J. Hennessy, M. Heinrich [2000]. “FLASH vs. (simulated) FLASH: Closing simulation loop, ”Proc. Ninth Int’l. Conf. Architectural Support Programming Languages Operat- ing Systems (ASPLOS) , November 12 –15, Cambridge, Mass., 49 –58. Goodman, J. R. [1983]. “Using cache memory reduce processor memory traf- fic,”Proc. 10th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1982, Stockholm, Sweden, 124 –131. Goralski, W. [1997]. SONET: Guide Synchronous Optical Network , McGraw- Hill, New York. Grice, C., M. Kanellos [2000]. “Cell phone industry crossroads: Go high low? ”CNET News ( August 31), technews.netscape.com/news/0-1004-201- 2518386-0.html?tag ¼st.ne.1002.tgif.sf . Groe, J. B., L. E. Larson [2000]. CDMA Mobile Radio Design , Artech House, Boston. Hagersten E., M. Koster [1998]. “WildFire: scalable path SMPs, ”Proc. Fifth Int ’l. Symposium High-Performance Computer Architecture , January 9–12, 1999, Orlando, Fla. Hagersten, E., A. Landin, S. Haridi [1992]. “DDM —a cache-only memory architecture, ”IEEE Computer 25:9 (September), 44 –54. Hill, M. D. [1998]. “Multiprocessors support simple memory consistency models, ”IEEE Computer 31:8 (August), 28 –34. Hillis, W. D. [1985]. Connection Multiprocessor , MIT Press, Cambridge, Mass. Hirata, H., K. Kimura, S. Nagamine, Y. Mochizuki, A. Nishimura, Y. Nakase, T. Nishizawa [1992]. “An elementary processor architecture simultaneous instruction issuing multiple threads, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 136–145. Hockney, R. W., C. R. Jesshope [1988]. Parallel Computers 2: Architectures, Programming Algorithms , Adam Hilger, Ltd., Bristol, England. Holland, J. H. [1959]. “A universal computer capable executing arbitrary number subprograms simultaneously, ”Proc. East Joint Computer Conf. 16, 108 –113. Hord, R. M. [1982]. Illiac-IV, First Supercomputer , Computer Science Press, Rockville, Md. Hristea, C., D. Lenoski, J. Keen [1997]. “Measuring memory hierarchy per- formance cache-coherent multiprocessors using micro benchmarks, ” Proc. ACM/IEEE Conf. Supercomputing , November 15 –21, 1997, San Jose, Calif. Hwang, K. [1993]. Advanced Computer Architecture Parallel Programming , McGraw-Hill, New York.M.7 History Multiprocessors Parallel Processing ■M-69IBM. [2005]. “Blue Gene, ”IBM J. Research Development , 49:2/3 (special issue). Infiniband Trade Association. [2001]. InfiniBand Architecture Specifications Release 1.0.a, www.infinibandta.org . Jordan, H. F. [1983]. “Performance measurements HEP —a pipelined MIMD computer, ”Proc. 10th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 5 –7, 1982, Stockholm, Sweden, 207 –212. Kahn, R. E. [1972]. “Resource-sharing computer communication networks, ”Proc. IEEE 60:11 (November), 1397 –1407. Keckler, S. W., W. J. Dally [1992]. “Processor coupling: Integrating compile time runtime scheduling parallelism, ”Proc. 19th Annual Int ’l. Sympo- sium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Austra- lia, 202 –213. Kontothanassis, L., G. Hunt, R. Stets, N. Hardavellas, M. Cierniak, S. Parthasarathy, W. Meira, S. Dwarkadas, M. Scott [1997]. “VM-based shared memory low-latency, remotememory-access networks, ”Proc. 24th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo. Kurose, J. F., K. W. Ross [2001]. Computer Networking: Top-Down Approach Featuring Internet , Addison-Wesley, Boston. Kuskin, J., D. Ofelt, M. Heinrich, J. Heinlein, R. Simoni, K. Gharachorloo, J. Cha- pin, D. Nakahira, J. Baxter, M. Horowitz, A. Gupta, M. Rosenblum, J. L. Hen-nessy [1994]. “The Stanford FLASH multiprocessor, ”Proc. 21st Annual Int ’l. Symposium Computer Architecture (ISCA) , April 18 –21, 1994, Chicago. Lamport, L. [1979]. “How make multiprocessor computer correctly exe- cutes multiprocess programs, ”IEEE Trans. Computers C-28:9 (September), 241–248. Laudon, J., A. Gupta, M. Horowitz [1994]. “Interleaving: multithreading technique targeting multiprocessors workstations, ”Proc. Sixth Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 4 –7, 1994, San Jose, Calif., 308 –318. Laudon, J., D. Lenoski [1997]. “The SGI Origin: ccNUMA highly scalable server, ”Proc. 24th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 2 –4, 1997, Denver, Colo., 241 –251. Lenoski, D., J. Laudon, K. Gharachorloo, A. Gupta, J. L. Hennessy [1990]. “The Stanford DASH multiprocessor, ”Proc. 17th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 28 –31, 1990, Seattle, Wash., 148 –159. Lenoski, D., J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. L. Hennessy, M. A. Horowitz, M. Lam [1992]. “The Stanford DASH multiprocessor, ” IEEE Computer 25:3 (March), 63 –79. Li, K. [1988]. “IVY: shared virtual memory system parallel computing, ” Proc. Int ’l. Conf. Parallel Processing (ICCP) , August, Pennsylvania State University, University Park, Penn. Lo, J., L. Barroso, S. Eggers, K. Gharachorloo, H. Levy, S. Parekh [1998]. “An analysis database workload performance simultaneous multithreadedM-70 ■Appendix Historical Perspectives Referencesprocessors, ”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 39 –50. Lo, J., S. Eggers, J. Emer, H. Levy, R. Stamm, D. Tullsen [1997]. “Converting thread-level parallelism instruction-level parallelism via simultaneous mul-tithreading, ”ACM Trans. Computer Systems 15:2 (August), 322 –354. Lovett, T., S. Thakkar [1988]. “The Symmetry multiprocessor system, ”Proc. Int’l. Conf. Parallel Processing (ICCP) , August, Pennsylvania State University, University Park, Penn., 303 –310. Mellor-Crummey, J. M., M. L. Scott [1991]. “Algorithms scalable synchro- nization shared-memory multiprocessors, ”ACM Trans. Computer Systems 9:1 (February), 21 –65. Menabrea, L. F. [1842]. “Sketch analytical engine invented Charles Bab- bage, ”Bibliothèque Universelle de Genève , 82 (October). Metcalfe, R. M. [1993]. “Computer/network interface design: Lessons Arpa- net Ethernet. ”IEEE J. Selected Areas Communications 11:2 (February), 173 –180. Metcalfe, R. M., D. R. Boggs [1976]. “Ethernet: Distributed packet switching local computer networks, ”Communications ACM 19:7 (July), 395 –404. Mitchell, D. [1989]. “The Transputer: time now, ”Computer Design (RISC suppl.), 40 –41. Miya, E. N. [1985]. “Multiprocessor/distributed processing bibliography, ”Com- puter Architecture News 13:1, 27 –29. National Research Council. [1997]. Evolution Untethered Communications , Computer Science Telecommunications Board, National Academy Press,Washington, D.C. Nikhil, R. S., G. M. Papadopoulos, Arvind [1992]. “*T: multithreaded mas- sively parallel architecture, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 – 21, 1992, Gold Coast, Australia, 156 –167. Noordergraaf, L., R. van der Pas [1999]. “Performance experiences Sun ’s WildFire prototype, ”Proc. ACM/IEEE Conf. Supercomputing , November 13–19, 1999, Portland, Ore. Partridge, C. [1994]. Gigabit Networking , Addison-Wesley, Reading, Mass. Pfister, G. F. [1998]. Search Clusters , 2nd ed., Prentice Hall, Upper Saddle River, N.J. Pfister, G. F., W. C. Brantley, D. A. George, S. L. Harvey, W. J. Kleinfekder, K. P. McAuliffe, E. A. Melton, V. A. Norton, J. Weiss [1985]. “The IBM research parallel processor prototype (RP3): Introduction architecture, ”Proc. 12th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 17 –19, 1985, Boston, Mass., 764 –771. Reinhardt, S. K., J. R. Larus, D. A. Wood [1994]. “Tempest Typhoon: User-level shared memory, ”Proc. 21st Annual Int ’l. Symposium Computer Architecture (ISCA) , April 18 –21, 1994, Chicago, 325 –336. Rettberg, R. D., W. R. Crowther, P. P. Carvey, R. S. Towlinson [1990]. “The Monarch parallel processor hardware design, ”IEEE Computer 23:4 (April), 18 –30.M.7 History Multiprocessors Parallel Processing ■M-71Rosenblum, M., S. A. Herrod, E. Witchel, A. Gupta [1995]. “Complete com- puter simulation: SimOS approach, ”inIEEE Parallel Distributed Technology (now called Concurrency ) 4:3, 34 –43. Saltzer, J. H., D. P. Reed, D. D. Clark [1984]. “End-to-end arguments sys- tem design, ”ACM Trans. Computer Systems 2:4 (November), 277 –288. Satran, J., D. Smith, K. Meth, C. Sapuntzakis, M. Wakeley, P. Von Stamwitz, R. Haagens, E. Zeidner, L. Dalle Ore, Y. Klein [2001]. “iSCSI, ”IPS Working Group IETF, Internet draft www.ietf.org/internet-drafts/draft-ietf-ips-iscsi- 07.txt . Saulsbury, A., T. Wilkinson, J. Carter, A. Landin [1995]. “An argument Simple COMA, ”Proc. First IEEE Symposium High-Performance Computer Architectures , January 22 –25, 1995, Raleigh, N.C., 276 –285. Schwartz, J. T. [1980]. “Ultracomputers, ”ACM Trans. Programming Languages Systems 4:2, 484 –521. Scott, S. L. [1996]. “Synchronization communication T3E multiproces- sor,”Seventh Int ’l. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS) , October 1 –5, 1996, Cambridge, Mass., 26 – 36. Scott, S. L., G. M. Thorson [1996]. “The Cray T3E network: Adaptive routing high-performance 3D torus, ”Proc. IEEE HOT Interconnects ’96, August 15–17, 1996, Stanford University, Palo Alto, Calif., 14 –156. Seitz, C. L. [1985]. “The Cosmic Cube (concurrent computing), ”Communications ACM 28:1 (January), 22 –33. Singh, J. P., J. L. Hennessy, A. Gupta [1993]. “Scaling parallel programs multiprocessors: Methodology examples, ”Computer 26:7 (July), 22 –33. Slotnick, D. L., W. C. Borck, R. C. McReynolds [1962]. “The Solomon com- puter, ”Proc. AFIPS Fall Joint Computer Conf. , December 4 –6, 1962, Philadelphia, Penn., 97 –107. Smith, B. J. [1978]. “A pipelined, shared resource MIMD computer, ”Proc. Int ’l. Conf. Parallel Processing (ICPP) , August, Bellaire, Mich., 6 –8. Soundararajan, V., M. Heinrich, B. Verghese, K. Gharachorloo, A. Gupta, J. L. Hennessy [1998]. “Flexible use memory replication/migration cache-coherent DSM multiprocessors, ”Proc. 25th Annual Int ’l. Symposium Computer Architecture (ISCA) , July 3 –14, 1998, Barcelona, Spain, 342 –355. Spurgeon, C. [2001]. “Charles Spurgeon ’s Ethernet Web site, ”www.host.ots. utexas.edu/ethernet/ethernet-home.html . Stenstr €om, P., T. Joe, A. Gupta [1992]. “Comparative performance evaluation cachecoherent NUMA COMA architectures, ”Proc. 19th Annual Int ’l. Symposium Computer Architecture (ISCA) , May 19 –21, 1992, Gold Coast, Australia, 80 –91. Sterling, T. [2001]. Beowulf PC Cluster Computing Windows Beowulf PC Cluster Computing Linux , MIT Press, Cambridge, Mass. Stevens, W. R. [1994 –1996]. TCP/IP Illustrated (three volumes), Addison- Wesley, Reading, Mass. Stone, H. [1991]. High Performance Computers , Addison-Wesley, New York.M-72 ■Appendix Historical Perspectives ReferencesSwan, R. J., A. Bechtolsheim, K. W. Lai, J. K. Ousterhout [1977]. “The imple- mentation Cm* multi-microprocessor, ”Proc. AFIPS National Computing Conf. , June 13 –16, 1977, Dallas, Tex., 645 –654. Swan, R. J., S. H. Fuller, D. P. Siewiorek [1977]. “Cm* —a modular, multi- microprocessor, ”Proc. AFIPS National Computing Conf. , June 13 –16, 1977, Dallas, Tex., 637 –644. Tanenbaum, A. S. [1988]. Computer Networks , 2nd ed., Prentice Hall, Englewood Cliffs, N.J. Tang, C. K. [1976]. “Cache design tightly coupled multiprocessor system, ” Proc. AFIPS National Computer Conf. ,J u n e7 –10, 1976, New York, 749 –753. Thacker, C. P., E. M. McCreight, B. W. Lampson, R. F. Sproull, D. R. Boggs [1982]. “Alto: personal computer, ”in D. P. Siewiorek, C. G. Bell, A. Newell, eds., Computer Structures: Principles Examples , McGraw-Hill, New York, 549 –572. Thekkath, R., A. P. Singh, J. P. Singh, S. John, J. L. Hennessy [1997]. “An evaluation commercial CC-NUMA architecture —the CONVEX Exemplar SPP1200, ”Proc. 11th Int ’l. Parallel Processing Symposium (IPPS) , April 1 –7, 1997, Geneva, Switzerland. Tullsen, D. M., S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, R. L. Stamm [1996]. “Exploiting choice: Instruction fetch issue implementable simultaneous multithreading processor, ”Proc. 23rd Annual Int ’l. Symposium Computer Architecture (ISCA) , May 22 –24, 1996, Philadelphia, Penn., 191–202. Tullsen, D. M., S. J. Eggers, H. M. Levy [1995]. “Simultaneous multithread- ing: Maximizing on-chip parallelism, ”Proc. 22nd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 22 –24, 1995, Santa Margherita, Italy, 392 – 403. Unger, S. H. [1958]. “A computer oriented towards spatial problems, ”Proc. Insti- tute Radio Engineers 46:10 (October), 1744 –1750. Walrand, J. [1991]. Communication Networks: First Course , Aksen Associates: Irwin, Homewood, Ill. Wilson, A. W., Jr. [1987]. “Hierarchical cache/bus architecture shared-memory multiprocessors, ”Proc. 14th Annual Int ’l. Symposium Computer Architec- ture (ISCA) , June 2 –5, 1987, Pittsburgh, Penn., 244 –252. Wolfe, A., J. P. Shen [1991]. “A variable instruction stream extension VLIW architecture. ”Proc. Fourth Int ’l. Conf. Architectural Support Pro- gramming Languages Operating Systems (ASPLOS) , April 8 –11, 1991, Palo Alto, Calif., 2 –14. Wood, D. A., M. D. Hill [1995]. “Cost-effective parallel computing, ”IEEE Computer 28:2 (February), 69 –72. Wulf, W., C. G. Bell [1972]. “C.mmp —A multi-mini-processor, ”Proc. AFIPS Fall Joint Computer Conf. , December 5 –7, 1972, Anaheim, Calif., 765 –777. Wulf, W., S. P. Harbison [1978]. “Reflections pool processors —an experience report C.mmp/Hydra, ”Proc. AFIPS National Computing Conf. June 5 –8, 1978, Anaheim, Calif., 939 –951.M.7 History Multiprocessors Parallel Processing ■M-73Yamamoto, W., M. J. Serrano, A. R. Talcott, R. C. Wood, M. Nemirosky [1994]. “Performance estimation multistreamed, superscalar processors, ”Proc. 27th Hawaii Int ’l. Conf. System Sciences ,J n u r y4 –7, 1994, Wailea, 195 –204. M.8 Development Clusters ( Chapter 6 ) section, cover development clusters foundation warehouse-scale computers (WSCs) utility computing. (Readers interested learning start Barroso H €olzle [2009] blog postings talks James Hamilton http://perspectives.mvdirona.com .) Clusters, Forerunner WSCs Clusters probably “invented ”in 1960s customers could fit work one computer needed backup machine case failureof primary machine [Pfister 1998]. Tandem introduced 16-node cluster in1975. Digital followed VAX clusters, introduced 1984. originallyindependent computers shared I/O devices, requiring distributed operating system coordinate activity. Soon communication links com- puters, part computers could geographically distributed increaseavailability case disaster single site. Users log onto cluster areunaware machine running on. DEC (now HP) sold than25,000 clusters 1993. early companies Tandem (now HP) andIBM (still IBM). Today, virtually every company cluster products. theseproducts aimed availability, performance scaling secondary benefit. Scientific computing clusters emerged competitor MPPs. 1993, Beowulf project started goal fulfilling NASA ’s desire 1 GFLOPS computer $50,000. 1994, 16-node cluster built off-the-shelf PCs using 80486s achieved goal [Bell Gray 2001]. emphasis ledto variety software interfaces make easier submit, coordinate, debuglarge programs large number independent programs. Efforts made reduce latency communication clusters well increase bandwidth, several research projects worked problem. (Onecommercial result low-latency research VI interface standard, embraced Infiniband, discussed below.) Low latency proved use- ful applications. example, 1997 cluster 100 UltraSPARC desk-top computers University California –Berkeley, connected 160 MB/sec per link Myrinet switches, used set world records database sort —sorting 8.6 GB data originally disk 1 minute —and cracking encrypted mes- sage—taking 3.5 hours decipher 40-bit DES key. research project, called Network Workstations [Anderson, Culler, Patterson 1995], also developed Inktomi search engine, led start-upM-74 ■Appendix Historical Perspectives Referencescompany name. Eric Brewer led Inktomi effort Berkeley company demonstrate use commodity hardware build com- puting infrastructure Internet services. Using standardized networks within arack PC servers gave Inktomi better scalability. contrast, strategy ofthe prior leading search engine Alta Vista build large-scale SMPs.Compared high-performance computing work clusters, emphasiswas relatively large number low-cost nodes clear programmingmodel. Hence, project Inktomi considered foundation ofWSCs Cloud Computing. Google followed example Inktomi technology took leading search engine mantle Inktomi Inktomi taken Alta Vista [Brin Page 1998]. (Google ’s initial innovation search quality; WSC innovations came much later.) many years now,all Internet services relied cluster technology serve millions ofcustomers. Utility Computing, Forerunner Cloud Computing stated text, earliest version utility computing timesharing.Although timesharing faded away time creation smaller andcheaper personal computers, last decade many less fullysuccessful attempts resuscitate utility computing. Sun began selling time SunCloud $1 per hour 2000, HP offered Utility Data Center 2001, Intel tried selling time internal supercomputers early 2000s. Although theywere commercially available, customers used them. related topic grid computing , originally invented scien- tific programs could run across geographically distributed computing facilities.At time, questioned wisdom goal, setting aside difficult itwould achieve. Grid computing tended require large systems runningvery large programs, using multiple datacenters tasks. Single applicationsdid really run well geographically distributed, given long latenciesinherent long distance. first step eventually led conventionsfor data access, grid computing community develop APIs useful beyond high-performance computing community, cloud comput- ing effort shares little code history grid computing. Armbrust et al [2009] argued that, Internet service companies solved operational problems work large scale, significant economies scalethat uncovered brought costs smaller datacenters.Amazon recognized cost advantage true Amazon beable make profit selling service. 2006, Amazon announced ElasticCloud Computing (EC2) $0.10 per hour per instance. subsequent popularity EC2 led Internet companies offer cloud computing services, Google App Engine Microsoft Azure, albeit higher abstraction levels thanthe x86 virtual machines Amazon Web Services. Hence, current popularityof pay-as-you go computing ’t someone recently came idea;M.8 Development Clusters ■M-75it’s technology business models aligned companies make money offering service many people want use. Time tell whether many successful utility computing models whether theindustry converge around single standard. certainly interestingto watch. Containers fall 2003, many people thinking using containers holdservers. Brewster Kahle, director founder Internet Archive, gave talksabout could fit whole archive single 40-foot container. interestwas making copies Archive distributing around world ensure itssurvivability, thereby avoiding fate Library Alexandria wasdestroyed fire 48 B.C.E. People working Kahle wrote white paperbased talk November 2003 get detail container design would look like. year, engineers Google also looking building datacenters using containers submitted patent aspects December 2003. Thefirst container datacenter delivered January 2005, Google receivedthe patent October 2007. Google publicly revealed use containers inApril 2009. Greg Papadopolous Sun Microsystems Danny Hillis Applied Minds heard Kahle ’s talk designed product called Sun Modular Datacenter debuted October 2006. (The project code name Black Box, term many people still use.) half-length (20-foot) container could hold 280 servers. Thisproduct release combined Microsoft ’s announcement building datacenter designed hold 220 40-foot containers inspired many compa-nies offer containers servers designed placed them. nice turn events, 2009 Internet Archive migrated data Sun Modular Datacenter. copy Internet Archive New Library ofAlexandria Egypt, near site original library. References Anderson, T. E., D. E. Culler, D. Patterson [1995]. “A case (net- works workstations), ”IEEE Micro 15:1 (February), 54 –64. Apache Software Foundation. [2011]. Apache Hadoop project, http://hadoop. apache.org . Armbrust, M., A. Fox, R. Griffith, A.D. Joseph, R. Katz, A. Konwinski, G. Lee, D. Patterson, A. Rabkin, I. Stoica, M. Zaharia [2009]. Clouds: Berkeley View Cloud Computing , Tech. Rep. UCB/EECS-2009-28, Univer- sity California, Berkeley ( http://www.eecs.berkeley.edu/Pubs/TechRpts/ 2009/EECS-2009-28.html ).M-76 ■Appendix Historical Perspectives ReferencesBarroso, L. A. [2010]. “Warehouse scale computing [keynote address], ”Proc. ACM SIG-MOD , June 8 –10, 2010, Indianapolis, Ind. Barroso, L. A., U. H €olzle [2007]. “The case energy-proportional comput- ing,”IEEE Computer 40:12 (December), 33 –37. Barroso, L.A., U. H €olzle [2009]. “The datacenter computer: introduc- tion design warehouse-scale machines, ”in M. D. Hill, ed., Synthesis Lectures Computer Architecture , Morgan & Claypool, San Rafael, Calif. Barroso, L.A., Clidaras, J. H €olzle, U., 2013. datacenter computer:An introduction design warehouse-scale machines . Synthesis lectures computer architecture, 8(3), pp.1 –154. Barroso, L.A., Marty, M., Patterson, D., Ranganathan, P. 2017. Attack Killer Microseconds. Communications ACM , 56(2). Bell, C. G., J. Gray [2002]. “What ’s next high performance computing, ” Communications ACM 45:2 (February), 91 –95. Brady, J.T., 1986. theory productivity creative process. IEEE Computer Graphics Applications, 6(5), pp.25 –34. Brin, S., L. Page [1998]. “The anatomy large-scale hypertextual Web search engine, ”Proc. 7th Int ’l. World Wide Web Conf. , April 14 –18, 1998, Bris- bane, Queensland, Australia, 107 –117. Carter, J., K. Rajamani [2010]. “Designing energy-efficient servers data centers, ”IEEE Computer 43:7 (July), 76 –78. Chang, F., J. Dean, S. Ghemawat, W. C. Hsieh, D. A. Wallach, M. Burrows, T. Chandra, A. Fikes, R. E. Gruber [2006]. “Bigtable: distributed storage system structured data, ”inProc. Operating Systems Design Implemen- tation (OSDI ’06), November 6 –8, 2006, Seattle, Wash. Chang, J., J. Meza, P. Ranganathan, C. Bash, A. Shah [2010]. “Green server design: Beyond operational energy sustainability, ”Workshop Power Aware Computing Systems (HotPower ’10), October 4 –6, 2010, Vancou- ver, British Columbia. Clark, J., 2014 Five Numbers Illustrate Mind-Bending Size Amazon ’s Cloud, Bloomberg, https://www.bloomberg.com/news/2014-11-14/5- numbersthat-illustrate-the-mind-bending-size-of-amazon-s-cloud.html . Clidaras, J., C. Johnson, B. Felderman [2010]. Private communication. Climate Savers Computing. [2007]. Efficiency specs, http://www. climatesaverscomputing.org/ . Clos, C., 1953. Study Non-Blocking Switching Networks. Bell Labs Techni- cal Journal , 32(2), pp.406-424. Dean, J. [2009]. “Designs, lessons advice building large distributed sys- tems [keynote address], ”Proc. 3rd ACM SIGOPS International Workshop Large Scale Distributed Systems Middleware, Co-located 22ndACM Symposium Operating Systems Principles (SOSP 2009) , October 10–11, 2009, Big Sky, Mont. Dean, J. Barroso, L.A., 2013. tail scale. Communications ACM , 56(2), pp.74-80.M.8 Development Clusters ■M-77Dean, J., S. Ghemawat [2004]. “MapReduce: Simplified data processing large clusters. ”InProc. Operating Systems Design Implementation (OSDI ’04), December 6 –8, 2004, San Francisco, 137 –150. Dean, J., S. Ghemawat [2008]. “MapReduce: simplified data processing large clusters, ”Communications ACM 51:1, 107 –113. DeCandia, G., D. Hastorun, M. Jampani, G. Kakulapati, A. Lakshman, A. Pilchin, S. Sivasubramanian, P. Vosshall, W. Vogels [2007]. “Dynamo: Amazon ’s highly available key-value store, ”inProc. 21st ACM Symposium Operating Systems Principles , October 14 –17, 2007, Stevenson, Wash. Doherty, W.J. Thadhani, A.J., 1982. economic value rapid response time. IBM Report. Fan, X., W. Weber, L. A. Barroso [2007]. “Power provisioning warehouse-sized computer, ”inProc. 34th Annual Int ’l. Symposium Com- puter Architecture (ISCA) , June 9 –13, 2007, San Diego, Calif. A. Fikes, “Storage architecture challenges, ”in Google Faculty Summit, 2010. Ghemawat, S., H. Gobioff, S.-T. Leung (2003). “The Google file system, ”in Proc. 19th ACM Symposium Operating Systems Principles , October 19 –22, 2003, Lake George, N.Y. Greenberg, A., N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz, P. Patel, S. Sengupta [2009]. “VL2: scalable flexible data center network, ”inProc. SIGCOMM , August 17 –21, Barcelona, Spain. González, A.and Day, M. April 27, 2016, “Amazon, Microsoft invest billions computing shifts cloud, ”The Seattle Times .http://www.seattletimes.com/ business/technology/amazon-microsoft-invest-billions-as-computing-shifts-to-cloud/ Hamilton, J. [2009]. “Data center networks way, ”Stanford Clean Slate CTO Summit, October 23, 2009, http://mvdirona.com/jrh/TalksAndPapers/ JamesHamilton_CleanSlateCTO2009.pdf . Hamilton, J. [2010]. “Cloud computing economies scale, ”Proc. AWS Workshop Genomics & Cloud Computing , June 8, 2010, Seattle, Wash. ( http:// mvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf ). Hamilton, J., 2014. AWS Innovation Scale, AWS Re-invent conference. https:// www.youtube.com/watch?v ¼JIQETrFC_SQ Hamilton, J., May 2015. Return Cloud, http://perspectives.mvdirona. com/2015/05/the-return-to-the-cloud/ / Hamilton, J., April 2017. Many Data Centers Needed World-Wide, http:// perspectives.mvdirona.com/2017/04/how-many-data-centers-needed-worldwide/ H€olzle, U. [2010]. “Brawny cores still beat wimpy cores, time, ”IEEE Micro , July/August. Kanev, S., Darago, J.P., Hazelwood, K., Ranganathan, P., Moseley, T., Wei, G.Y. Brooks, D., 2015, June. Profiling warehouse-scale computer. ACM/ IEEE 42nd Annual International Symposium Computer Architecture(ISCA) .M-78 ■Appendix Historical Perspectives ReferencesLang, W., J. M. Patel, S. Shankar [2010]. “Wimpy node clusters: non-wimpy workloads? ”Proc. Sixth Int ’l. Workshop Data Management New Hardware , June 7, 2010, Indianapolis, Ind. Lim, K., P. Ranganathan, J. Chang, C. Patel, T. Mudge, S. Reinhardt [2008]. “Understanding designing new system architectures emerging warehouse-computing environments, ”Proc. 35th Annual Int ’l. Symposium Computer Architecture (ISCA) , June 21 –25, 2008, Beijing, China. Narayanan, D., E. Thereska, A. Donnelly, S. Elnikety, A. Rowstron [2009]. “Migrating server storage SSDs: Analysis trade-offs, ”Proc. 4th ACM European Conf. Computer Systems , April 1 –3, 2009, Nuremberg, Germany. Pfister, G. F. [1998]. Search Clusters , 2nd ed., Prentice Hall, Upper Saddle River, N.J. Pinheiro, E., W.-D. Weber, L. A. Barroso [2007]. “Failure trends large disk drive population, ”Proc. 5th USENIX Conference File Storage Technologies (FAST ’07), February 13 –16, 2007, San Jose, Calif. Ranganathan, P., P. Leech, D. Irwin, J. Chase [2006]. “Ensemble-level power management dense blade servers, ”Proc. 33rd Annual Int ’l. Symposium Computer Architecture (ISCA) , June 17 –21, 2006, Boston, Mass., 66 –77. Reddi, V. J., B. C. Lee, T. Chilimbi, K. Vaid [2010]. “Web search using mobile cores: Quantifying mitigating price efficiency, ”Proc. 37th Annual Int’l. Symposium Computer Architecture (ISCA) , June 19 –23, 2010, Saint- Malo, France. Schroeder, B., G. A. Gibson [2007]. “Understanding failures petascale com- puters, ”Journal Physics: Conference Series 78 , 188 –198. Schroeder, B., E. Pinheiro, W.-D. Weber [2009]. “DRAM errors wild: large-scale field study, ”Proc. Eleventh Int ’l. Joint Conf. Measurement Modeling Computer Systems (SIGMETRICS) , June 15 –19, 2009, Seattle, Wash. Schurman, E. J. Brutlag [2009]. “The User Business Impact Server Delays, ”Proc. Velocity: Web Performance Operations Conf. , June 22 – 24, 2009, San Jose, Calif. Tezzaron Semiconductor. [2004]. “Soft Errors Electronic Memory —A White Paper , Tezzaron Semiconductor, Naperville, Ill. ( http://www.tezzaron.com/ about/papers/soft_errors_1_1_secure.pdf ). Vahdat, A., M. Al-Fares, N. Farrington, R. N. Mysore, G. Porter, S. Radhak- rishnan [2010]. “Scale-out networking data center, ”IEEE Micro July/ August 2010. M.9 Historical Perspectives References architects experiment DSAs, knowing architecture history may help.There likely older architecture ideas unsuccessful general-purposecomputing could nevertheless make eminent sense domain-specific archi-tectures. all, probably things well, either might matchM.9 Historical Perspectives References ■M-79your domain, or, conversely, domain might omit features challenges architectures. example, Illiac IV (Barnes et al., 1968) 1960s FPS 120a (Charlesworth, 1981) 1970s two-dimensional arrays processing elements, proper ancestors theTPU Paintbox. Similarly, VLIW architectures Multiflow (Rauand Fisher, 1993) Itanium (Sharangpani Arora, 2000) commer-cial successes general-purpose computing, Paintbox erraticdata cache misses, unpredictable branches, large code footprint diffi-cult VLIW architectures. Two survey articles document custom neural network ASICs go back least 25 years (Ienne et al., 1996; Asanovi /C19c, 2002). example, CNAPS chips contained 64 SIMD array 16-bit 8-bit multipliers, several CNAPS chipscould connected together sequencer (Hammerstrom, 1990). TheSynapse-1 system based custom systolic multiply-accumulate chip calledthe MA-16, performed sixteen 16-bit multiplications time (Ramacheret al., 1991). system concatenated MA-16 chips custom hardwareto activation functions. Twenty-five SPERT-II workstations, accelerated T0 custom ASIC, deployed starting 1995 NN training inference speech recog-nition (Asanovi /C19c et al., 1998). 40-MHz T0 added vector instructions MIPS instruction set architecture. eight-lane vector unit could produce upto sixteen 32-bit arithmetic results per clock cycle based 8-bit 16-bit inputs,making 25 times faster inference 20 times faster training SPARC-20 workstation. found 16 bits insufficient training, usedtwo 16-bit words instead, doubled training time. overcome draw- back, introduced “bunches ”(batches) 32 –1000 data sets reduce time spent updating weights, made faster training one word butno batches. use phrase Image Processing Unit Paintbox identify emerg- ing class processor, first use term. earliest use canfind 1999, Sony Playstation put name chip basicallyan MPEG2 decoder (Sony/Toshiba, 1999). 2006, Freescale used IPU namepart i.MX31 Applications Processor, closer generic way interpret (Freescale part i.MX31 Applications Processor, 2006). References Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., 2016. Tensor-flow: large-scale machine learning heterogeneous distributed systems. arXiv preprintarXiv:1603.04467. Adolf, R., Rama, S., Reagen, B., Wei, G.Y., Brooks, D., 2016. Fathom: reference workloads modern deep learning methods. In: IEEE International Sympo-sium Workload Characterization (IISWC). Amodei, D., et al., 2015. Deep speech 2: end-to-end speech recognition English mandarin, arXiv:1512.02595.M-80 ■Appendix Historical Perspectives ReferencesAsanovi /C19c, K., 2002. Programmable neurocomputing. In: Arbib, M.A. (Ed.), Handbook Brain Theory Neural Networks, second ed. MIT Press, Cam- bridge, MA. ISBN: 0-262-01197-2. https://people.eecs.berkeley.edu/ /C24krste/ papers/neurocomputing.pdf . Asanovi /C19c, K., Beck, A., Johnson, J., Wawrzynek, J., Kingsbury, B., Morgan, N., 1998. Training neural networks Spert-II. In: Sundararajan, N., Saratchan-dran, P. (Eds.), Parallel Architectures Artificial Networks: Paradigmsand Implementations. IEEE Computer Society Press. ISBN: 0-8186-8399-6.(Chapter 11) https://people.eecs.berkeley.edu/ /C24krste/papers/annbook.pdf . Bachrach, J., Vo, H., Richards, B., Lee, Y., Waterman, A., Avi žienis, R., Wawrzynek, J., Asanovi /C19c, K., 2012. Chisel: constructing hardware Scala embedded language. In: Proceedings 49th Annual Design AutomationConference, pp. 1216 –1225. Barnes, G.H., Brown, R.M., Kato, M., Kuck, D.J., Slotnick, D.L., Stokes, R., 1968. ILLIAC IV computer. IEEE Trans. Comput. 100 (8), 746 –757. Bhattacharya, S., Lane, N.D., 2016. Sparsification separation deep learning layers constrained resource inference wearables. In: Proceedings 14th ACM Conference Embedded Network Sensor Systems CD-ROM, pp. 176 –189. Brunhaver, J., 2014. PhD thesis. Stanford.Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski, T., Brown, S.D., Anderson, J.H., 2013. LegUp: open-source high-level synthesistool FPGA-based processor/accelerator systems. ACM Trans. Embed. Com-put. Syst. 13 (2). Canny, J., et al., 2015. Machine learning limit. In: IEEE International Con- ference Big Data. Caulfield, A.M., Chung, E.S., Putnam, A., Haselman, H.A.J.F.M., Humphrey, S.H.M., Daniel, P.K.J.Y.K., Ovtcharov, L.T.M.K., Lanka, M.P.L.W.S.,Burger, D.C.D., 2016. cloud-scale acceleration architecture. In: MICROConference. Charlesworth, A.E., 1981. approach scientific array processing: architec- tural design AP-120B/FPS-164 family. Computer 9, 18 –27. Clark, J., October 26, 2015. Google Turning Lucrative Web Search AI Machines. Bloomberg Technology, www.bloomberg.com . Dally, W.J., 2002. Computer architecture interconnect. In: Proceed- ings 8th International Symposium High Performance ComputerArchitecture. Freescale part i.MX31 Applications Processor, 2006. http://cache.freescale. com/files/32bit/doc/white_paper/IMX31MULTIWP.pdf . Galal, S., Shacham, O., Brunhaver II, J.S., Pu, J., Vassiliev, A., Horowitz, M., 2013. FPU generator design space exploration. In: 21st IEEE Symposium Computer Arithmetic (ARITH). Hameed, R., Qadeer, W., Wachs, M., Azizi, O., Solomatnikov, A., Lee, B.C., Richardson, S., Kozyrakis, C., Horowitz, M., 2010. Understanding sources ofinefficiency general-purpose chips. ACM SIGARCH Comput. Architect.News 38 (3), 37 –47.M.9 Historical Perspectives References ■M-81Hammerstrom, D., 1990. VLSI architecture high-performance, low-cost, on- chip learning. In: IJCNN International Joint Conference Neural Networks. He, K., Zhang, X., Ren, S., Sun, J., 2016. Identity mappings deep residual net- works. Also arXiv preprint arXiv:1603.05027. Huang, M., Wu, D., Yu, C.H., Fang, Z., Interlandi, M., Condie, T., Cong, J., 2016. Programming runtime support blaze FPGA accelerator deploymentat datacenter scale. In: Proceedings Seventh ACM Symposium CloudComputing. ACM, pp. 456 –469. Iandola, F., 2016. Exploring Design Space Deep Convolutional Neural Net- works Large Scale (Ph.D. dissertation). UC Berkeley. Ienne, P., Cornu, T., Kuhn, G., 1996. Special-purpose digital hardware neural networks: architectural survey. J. VLSI Signal Process. Syst. Signal ImageVideo Technol. 13 (1). Jouppi, N., 2016. Google supercharges machine learning tasks TPU custom chip. https://cloudplatform.googleblog.com . Jouppi, N., Young, C., Patil, N., Patterson, D., Agrawal, G., et al., 2017. Datacenter performance analysis matrix processing unit. In: 44th International Sympo- sium Computer Architecture. Karpathy, A., et al., 2014. Large-scale video classification convolutional neu- ral networks. CVPR. Krizhevsky, A., Sutskever, I., Hinton, G., 2012. Imagenet classification deep convolutional neural networks. Adv. Neural Inf. Process. Syst. Kung, H.T., Leiserson, C.E., 1980. Algorithms VLSI processor arrays. Intro- duction VLSI systems. Lee, Y., Waterman, A., Cook, H., Zimmer, B., Keller, B., Puggelli, A., Kwak, J., Jevtic, R., Bailey, S., Blagojevic, M., Chiu, P.-F., Avizienis, R., Richards, B., Bachrach, J., Patterson, D., Alon, E., Nikolic, B., Asanovic, K., 2016. agileapproach building RISC-V microprocessors. IEEE Micro 36 (2), 8 –20. Lewis-Kraus, G., 2016. Great A.I. Awakening. New York Times Magazine.Nielsen, M., 2016. Neural Networks Deep Learning. http:// neuralnetworksanddeeplearning.com/ . Nvidia, 2016. Tesla GPU Accelerators Servers. http://www.nvidia.com/object/ teslaservers.html . Olofsson, A., 2011. Debunking myth $100 ASIC. EE Times. http:// www.eetimes.com/author.asp?section_id ¼36&doc_id ¼1266014 . Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015a. Accelerating deep convolutional neural networks using specialized hardware.Microsoft Research Whitepaper. https://www.microsoft.com/en-us/research/ publication/accelerating-deepconvolutional-neural-networks-using-specialized-hardware/ . Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015b. Toward accelerating deep learning scale using specialized hardware datacenter. In: 2015 IEEE Hot Chips 27 Symposium.M-82 ■Appendix Historical Perspectives ReferencesPatterson, D., Nikoli /C19c, B., 7/25/2015, Agile Design Hardware, Parts I, II, III. EE Times, http://www.eetimes.com/author.asp?doc_id ¼1327239 . Patterson, D.A., Ditzel, D.R., 1980. case reduced instruction set com- puter. ACM SIGARCH Comput. Architect. News 8 (6), 25 –33. Prabhakar, R., Koeplinger, D., Brown, K.J., Lee, H., De Sa, C., Kozyrakis, C., Olukotun, K., 2016. Generating configurable hardware parallel patterns.In: Proceedingsofthe Twenty-FirstInternationalConference onArchitectural Sup-port Programming Languages Operating Systems. ACM, pp. 651 –665. Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2014. reconfigurable fabric foraccelerating large-scale datacenter services. In: 41st International Symposiumon Computer Architecture. Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck,S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2015. reconfigurable fabric accelerating large-scale datacenter services. IEEE Micro. 35 (3). Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck,S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S.,Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2016. reconfigurable fabric foraccelerating large-scale datacenter services. Commun. ACM. Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., Horowitz, M.A., 2015. Convolution engine: balancing efficiency & flexibility special- ized computing. Commun. ACM 58 (4). Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., Amarasinghe, S., 2013. Halide: language compiler optimizing parallelism, locality, andrecomputation image processing pipelines. ACM SIGPLAN Not. 48 (6),519–530. Ramacher, U., Beichter, J., Raab, W., Anlauf, J., Bruels, N., Hachmann, A., Wesseling, M., 1991. Design 1st generation neurocomputer. VLSI Design Neural Networks. Springer, USA. Rau, B.R., Fisher, J.A., 1993. Instruction-level parallelism. J. Supercomput. 235, Springer Science & Business Media. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., 2015. Imagenet large scalevisual recognition challenge. Int. J. Comput. Vis. 115 (3). Sergio Guadarrama, 2015. BVLC googlenet. https://github.com/BVLC/caffe/tree/ master/models/bvlc_googlenet . Shao, Y.S., Brooks, D., 2015. Research infrastructures hardware accelerators. Synth. Lect. Comput. Architect. 10 (4), 1 –99.M.9 Historical Perspectives References ■M-83Sharangpani, H., Arora, K., 2000. Itanium processor microarchitecture. IEEE Micro 20 (5), 24 –43. Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman,S., 2016. Mastering game Go deep neural networks tree search.Nature 529 (7587). Smith, J.E., 1982. Decoupled access/execute computer architectures. In: Proceed- ings 11th International Symposium Computer Architecture. Sony/Toshiba, 1999. ‘Emotion Engine ’in PS2 ( “IPU basically MPEG2 decoder…”).http://www.cpu-collection.de/?l0 ¼co&l1¼Sony&l2 ¼Emotion +Engine ,http://arstechnica.com/gadgets/2000/02/ee/3/ . Steinberg, D., 2015. Full-Chip Simulations, Keys Success. In: Proceedings Synopsys Users Group (SNUG) Silicon Valley 2015. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper convolutions. In: Pro-ceedings IEEE Conference Computer Vision Pattern Recognition. TensorFlow Tutorials, 2016. https://www.tensorflow.org/versions/r0.12/tutorials/ index.html . Tung, L., 2016. Google Translate: ‘This landmark update biggest single leap 10 years ’, ZDNet. http://www.zdnet.com/article/google-translate-this- landmarkupdate-is-our-biggest-single-leap-in-10years/ . Vanhoucke, V., Senior, A., Mao, M.Z., 2011. Improving speed neural net- works CPUs. https://static.googleusercontent.com/media/research.google. com/en//pubs/archive/37631.pdf . Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O.,Corrado, G., Hughes, M., Dean, J., 2016. Google ’s Neural Machine Translation System: Bridging Gap Human Machine Translation. http:// arxiv.org/abs/1609.08144 . M.10 History Magnetic Storage, RAID, I/O Buses (Appendix D) Mass storage term used imply unit capacity excess one million alphanumeric characters … Hoagland [1963] variety storage I/O issues leads varied history rest story. (Smotherman [1989] explored history I/O depth.) Thissection discusses magnetic storage, RAID, I/O buses controllers.Jain [1991] Lazowska et al. [1984] books interested learningmore queuing theory.M-84 ■Appendix Historical Perspectives ReferencesMagnetic Storage Magnetic recording invented record sound, 1941 magnetic tape able compete storage devices. success ENIAC in1947 led push use tapes record digital information. Reels mag-netic tapes dominated removable storage 1970s. 1980s, IBM3480 cartridge became de facto standard, least mainframes. transfer 3 MB/sec reading 18 tracks parallel. capacity 200 MB 1/2-inch tape. 9840 cartridge, used StorageTek Powder-Horn, transfersat 10 MB/sec stores 20,000 MB. device records tracks zigzag fashion rather longitudinally, head reverses direction follow track. technique called serpentine recording . Another 1/2-inch tape Digital Linear Tape; DLT7000 stores 35,000 MB transfers 5 MB/sec. Itscompetitor helical scan, rotates head get increased recordingdensity. 2001, 8-mm helical-scan tapes contain 20,000 MB transferat 3 MB/sec. Whatever density cost, serial nature tapes cre-ates appetite storage devices random access. 1953, Reynold B. Johnson IBM picked staff 15 scientists goal building radically faster random access storage system tape. goal storage equivalent 50,000 standard IBM punch cards tofetch data single second. Johnson ’s disk drive design simple untried: magnetic read/write sensors would float thousandthsof inch continuously rotating disk. Twenty-four months later teamemerged functional prototype. weighed 1 ton occupied 300cubic feet space. RAMAC-350 (Random Access Method AccountingControl) used 50 platters 24 inches diameter, rotated 1200 RPM, total capacity 5 MB access time 1 second. Starting RAMAC, IBM maintained leadership disk industry, storage headquarters San Jose, California, Johnson ’s team work. Many future leaders competing disk manufacturers started theircareers IBM, many disk companies located near San Jose. Although RAMAC contained first disk, major breakthrough magnetic recording found later disks air-bearing read/write heads, wherethe head would ride cushion air created fast-moving disk surface. cushion meant head could follow imperfections surface yet close surface. Subsequent advances come largely fromimproved quality components higher precision. 2001, heads flew 2 to3 microinches surface, whereas RAMAC drive 1000microinches away. Moving-head disks quickly became dominant high-speed magnetic stor- age, although high cost meant magnetic tape continued usedextensively 1970s. next important development hard disks removable hard disk drive developed IBM 1962; made possible share expensive drive electronics helped disks overtake tapes pre-ferred storage medium. IBM 1311 disk 1962 areal density 50,000M.10 History Magnetic Storage, RAID, I/O Buses ■M-85bits per square inch cost $800 per megabyte. IBM also invented floppy disk drive 1970, originally hold microcode IBM 370 series. Floppy disks became popular PC 10 years later. second major disk breakthrough so-called Winchester disk design 1973. Winchester disks benefited two related properties. First, inte-grated circuits lowered costs CPUs also disk controllers andthe electronics control disk arms. Reductions cost disk electronicsmade unnecessary share electronics thus made nonremovable diskseconomical. Since disk fixed could sealed enclosure, environmental control problems greatly reduced. Sealing system allowed heads fly closer surface, turn enabled increasesin areal density. first sealed disk IBM shipped two spindles, eachwith 30 MB disk; moniker “30-30 ”for disk led name Winchester. (America ’s popular sporting rifle, Winchester 94, nicknamed “30-30 ”after caliber cartridge.) Winchester disks grew rapidly popu- larity 1980s, completely replacing removable disks middle thatdecade. time, cost electronics control disk meant media removable. mentioned Appendix D, DRAMs started close areal density gap appeared catching disk storage, internal meetings IBM calledinto question future disk drives. Disk designers concluded disks mustimprove 60% per year forestall DRAM threat, contrast historical29% per year. essential enabler magnetoresistive heads, giantmagnetoresistive heads enabling current densities. competition,the gap time density record achieved lab disk shipped density closed considerably. personal computer created market small form factor (SFF) disk drives, since 14-inch disk drives used mainframes bigger thanthe PC. 2006, 3.5-inch drive market leader, although smaller2.5-inch drive required laptop computers significant sales volume. Itremains seen whether handheld devices iPODs video cameras,which require even smaller disks, remain significant sales volume. Forexample, 1.8-inch drives developed early 1990s palmtop com- puters, market chose Flash instead 1.8-inch drives disappeared. RAID SFF hard disks PCs 1980s led group Berkeley propose redun- dant arrays inexpensive disks (RAID). group worked reducedinstruction set computer effort expected much faster CPUs become avail- able. asked: could done small disks accompanied PCs? could done area I/O keep much faster pro-cessors? argued replace one mainframe drive 50 small drives gainmuch greater performance many independent arms. many smallM-86 ■Appendix Historical Perspectives Referencesdrives even offered savings power consumption floor space. downside many disks much lower mean time failure (MTTF). Hence, reasoned advantages redundant disks rotating parity addresshow get greater performance many small drives yet reliability highas single mainframe disk. problem experienced explaining ideas researchers heard disk arrays form redundancy, ’t understand Berkeley proposal. Hence, first RAID paper [Patterson, Gibson,and Katz 1987] case arrays SFF disk drives also something tutorial classification existing work disk arrays. Mirroring (RAID 1) long used fault-tolerant computers sold Tandem.Thinking Machines arrays 32 data disks 7 check disks using ECCfor correction (RAID 2) 1987, Honeywell Bull RAID 2 product evenearlier. Also, disk arrays single parity disk used scientific com-puters time frame (RAID 3). paper described single paritydisk support sector accesses (RAID 4) rotated parity (RAID 5). Chenet al. [1994] surveyed original RAID ideas, commercial products, recent developments. Unknown Berkeley group, engineers IBM working AS/400 computer also came rotated parity give greater reliability collectionof large disks. IBM filed patent RAID 5 Berkeley group wrote theirpaper. Patents RAID 1, RAID 2, RAID 3 several companies predatethe IBM RAID 5 patent, led plenty courtroom action. Berkeley paper written World Wide Web, captured imagination many engineers, copies faxed around world. One engineer Seagate received seven copies paper friends customers. EMC supplier DRAM boards IBM computers, butaround 1988 new policies IBM made nearly impossible EMC con-tinue sell IBM memory boards. Apparently, Berkeley paper also crossed thedesks EMC executives, decided go market dominated byIBM disk storage products instead. paper advocated, model usemany small drives compete mainframe drives, EMC announced aRAID product 1990. relied mirroring (RAID 1) reliability; RAID 5 products came much later EMC. next year, Micropolis offered RAID 3 product, Compaq offered RAID 4 product, Data General, IBM, NCRoffered RAID 5 products. RAID ideas soon spread rest workstation server industry. article explaining RAID Byte magazine (see Anderson [1990]) led RAID products offered desktop PCs, something surprise theBerkeley group. focused performance good availability, buthigher availability attractive PC market. Another surprise cost disk arrays. redundant power sup- plies fans, ability “hot swap ”a disk drive, RAID hardware controller itself, redundant disks, on, first disk arrays cost many times costof disks. Perhaps result, “inexpensive ”in RAID morphed intoM.10 History Magnetic Storage, RAID, I/O Buses ■M-87“independent. ”Many marketing departments technical writers today know RAID “redundant arrays independent disks. ” EMC transformation successful; 2006, EMC leading supplier storage systems, NetApp leading supplier Network-Attached Storage systems. RAID $30 billion industry 2006, 80% non-PC drive sales found RAIDs. recognition role, in1999 Garth Gibson, Randy Katz, David Patterson received IEEE ReynoldB. Johnson Information Storage Award “for development Redundant Arrays Inexpensive Disks (RAID). ” I/O Buses Controllers ubiquitous microprocessor inspired personal computers 1970s also trend late 1980s 1990s moving controller functionsinto I/O devices. I/O devices continued trend moving controllers intothe devices themselves. devices called intelligent devices , bus standards (e.g., SCSI) created specifically them. Intelligent devices relax timing constraints handling many low-level tasks andqueuing results. example, many SCSI-compatible disk drives include atrack buffer disk itself, supporting read ahead connect/disconnect. Thus,on SCSI string disks seeking others loading track bufferwhile one transferring data buffer SCSI bus. controller inthe original RAMAC, built vacuum tubes, needed move head overthe desired track, wait data pass head, transfer data calculated parity. SCSI, stands small computer systems interface ,i sa n example one company inventing bus generously encouraging com-panies build devices would plug it. Shugart created bus, originallycalled SASI. later standardized IEEE. several candidates successor SCSI, cur- rent leading contender Fibre Channel Arbitrated Loop (FC-AL). SCSIcommittee continues increase clock rate bus, giving standard anew life, SCSI lasting much longer proposed successors. creation serial interfaces SCSI ( “Serial Attach SCSI ”) ATA (“Serial ATA ”), may long lives. Perhaps first multivendor bus PDP-11 Unibus 1970 DEC. Alas, open-door policy buses contrast companies proprietarybuses using patented interfaces, thereby preventing competition plug-compatible vendors. Making bus proprietary also raises costs lowers thenumber available I/O devices plug it, since devices must havean interface designed bus. PCI bus pushed Intel represented return open, standard I/O buses inside computers. immediate successor PCI-X, Infiniband development 2000. standardizedby multicompany trade associations.M-88 ■Appendix Historical Perspectives ReferencesThe machines RAMAC era gave us I/O interrupts well storage devices. first machine extend interrupts detecting arithmetic abnor- malities detecting asynchronous I/O events credited NBS DYSEAC in1954 [Leiner Alexander 1954]. following year, first machine withDMA operational, IBM SAGE. today ’s DMA has, SAGE address counters performed block transfers parallel CPUoperations. early IBM 360s pioneered many ideas use I/O systems today. 360 first commercial machine make heavy use DMA, introduced notion I/O programs could interpreted device. Chaining I/O programs important feature. concept channels intro-duced 360 corresponds I/O bus today. Myer Sutherland [1968] wrote classic paper trade-off complex- ity performance I/O controllers. Borrowing religious concept the“wheel reincarnation, ”they eventually noticed caught loop continuously increasing power I/O processor needed ownsimpler coprocessor. quote Appendix captures cautionary tale. IBM mainframe I/O channels, I/O processors, thought inspiration Infiniband, processors Host ChannelAdaptor cards. References Anderson, D. [2003]. “You ’t know jack disks, ”Queue 1:4 (June), 20–30. Anderson, D., J. Dykes, E. Riedel [2003]. “SCSI vs. ATA —more interface, ”Proc. 2nd USENIX Conf. File Storage Technology (FAST ’03), March 31 –April 2, 2003, San Francisco. Anderson, M. H. [1990]. “Strength (and safety) numbers (RAID, disk storage technology), ”Byte 15:13 (December), 337 –339. Anon. et al. [1985]. Measure Transaction Processing Power , Tandem Tech. Rep. TR 85.2. Also appeared Datamation , 31:7 (April), 112 –118. Bashe, C. J., W. Buchholz, G. V. Hawkins, J. L. Ingram, N. Rochester [1981]. “The architecture IBM ’s early computers, ”IBM J. Research Develop- ment 25:5 (September), 363 –375. Bashe, C. J., L. R. Johnson, J. H. Palmer, E. W. Pugh [1986]. IBM’s Early Computers , MIT Press, Cambridge, Mass. Blaum, M., J. Brady, J. Bruck, J. Menon [1994]. “EVENODD: optimal scheme tolerating double disk failures RAID architectures, ”Proc. 21st Annual Int ’l. Symposium Computer Architecture (ISCA) , April 18 –21, 1994, Chicago, 245 –254. Blaum, M., J. Brady, J. Bruck, J. Menon [1995]. “EVENODD: optimal scheme tolerating double disk failures RAID architectures, ”IEEE Trans. Computers 44:2 (February), 192 –202.M.10 History Magnetic Storage, RAID, I/O Buses ■M-89Blaum, M., J. Brady, J., Bruck, J. Menon, A. Vardy [2001]. “The EVENODD code generalization, ”in H. Jin, T. Cortes, R. Buyya, eds., High Per- formance Mass Storage Parallel I/O: Technologies Applications , IEEE & Wiley Press, New York, 187 –208. Blaum, M., J. Bruck, A. Vardy [1996]. “MDS array codes independent parity symbols, ”IEEE Trans. Information Theory , IT-42 (March), 529–542. Brady, J. T. [1986]. “A theory productivity creative process, ”IEEE CG&A (May), 25 –34. Brown, A., D. A. Patterson [2000]. “Towards maintainability, availability, growth benchmarks: case study software RAID systems. ”Proc. 2000 USE- NIX Annual Technical Conf. , June 18 –23, San Diego, Calif. Bucher, I. V., A. H. Hayes [1980]. “I/O performance measurement Cray-1 CDC 7000 computers, ”Proc. Computer Performance Evaluation Users Group, 16th Meeting , October 20 –23, 1980, Orlando, Fl., 245 –254. Chen, P. M., G. A. Gibson, R. H. Katz, D. A. Patterson [1990]. “An evaluation redundant arrays inexpensive disks using Amdahl 5890, ”Proc. ACM SIGMETRICS Conf. Measurement Modeling Computer Systems , May 22–25, 1990, Boulder, Colo. Chen, P. M., E. K. Lee [1995]. “Striping RAID level 5 disk array, ”Proc. ACM SIGMETRICS Conf. Measurement Modeling ComputerSystems , May 15 –19, 1995, Ottawa, Canada, 136 –145. Chen, P. M., E. K. Lee, G. A. Gibson, R. H. Katz, D. A. Patterson [1994]. “RAID: High-performance, reliable secondary storage, ”ACM Computing Sur- veys 26:2 (June), 145 –188. Corbett, P., B. English, A. Goel, T. Grcanac, S. Kleiman, J. Leong, S. Sankar [2004]. “Row-diagonal parity double disk failure correction, ”Proc. 3rd USENIX Conf. File Storage Technology (FAST ’04), March 31 –April 2, 2004, San Francisco. Denehy, T. E., J. Bent, F. I. Popovici, A. C. Arpaci-Dusseau, R. H. Arpaci- Dusseau [2004]. “Deconstructing storage arrays, ”Proc. 11th Int ’l. Conf. Architectural Support Programming Languages Operating Systems(ASPLOS) , October 7 –13, 2004, Boston, Mass., 59 –71. Doherty, W. J., R. P. Kelisky [1979]. “Managing VM/CMS systems user effectiveness, ”IBM Systems J. 18:1, 143 –166. Douceur, J. R., W. J. Bolosky [1999]. “A large scale study file-system con- tents, ”Proc. ACM SIGMETRICS Conf. Measurement Modeling Com- puter Systems , May 1 –9, 1999, Atlanta, Ga., 59 –69. Enriquez, P. [2001]. “What happened dial tone? study FCC service disruption reports, ”poster, Richard Tapia Symposium Celebration Diversity Computing , October 18 –20, 2001, Houston, Tex. Friesenborg, S. E., R. J. Wicks [1985]. DASD Expectations: 3380, 3380- 23, MVS/XA , Tech. Bulletin GG22-9363-02, IBM Washington Systems Center, Gaithersburg, Md.M-90 ■Appendix Historical Perspectives ReferencesGibson, G. A. [1992]. Redundant Disk Arrays: Reliable, Parallel Secondary Stor- age, ACM Distinguished Dissertation Series, MIT Press, Cambridge, Mass. Goldstein, S. [1987]. Storage Performance —An Eight Year Outlook , Tech. Rep. TR 03.308-1, IBM Santa Teresa Laboratory, San Jose, Calif. Gray, J. [1990]. “A census Tandem system availability 1985 1990, ”IEEE Trans. Reliability , 39:4 (October), 409 –418. Gray, J. (ed.) [1993]. Benchmark Handbook Database Transaction Processing Systems , 2nd ed., Morgan Kaufmann, San Francisco. Gray, J., A. Reuter [1993]. Transaction Processing: Concepts Tech- niques , Morgan Kaufmann, San Francisco. Gray, J., D. P. Siewiorek [1991]. “High-availability computer systems. ”Com- puter 24:9 (September), 39 –48. Gray, J., C. van Ingen [2005]. Empirical Measurements Disk Failure Rates Error Rates ,”MSR-TR-2005-166, Microsoft Research, Redmond, Wash. Gurumurthi, S., A. Sivasubramaniam, V. Natarajan [2005]. Disk Drive Road- map Thermal Perspective: Case Dynamic Thermal Management,Proceedings International Symposium Computer Architecture (ISCA) , June, 38 –49. Henly, M., B. McNutt [1989]. DASD I/O Characteristics: Comparison MVS VM , Tech. Rep. TR 02.1550, IBM General Products Division, San Jose, Calif. Hewlett-Packard. [1998]. “HP’s‘5NINES:5MINUTES ’vision extends leadership re-defines high availability mission-critical environments, ”February 10, www.future.enterprisecomputing.hp.com/ia64/news/5nines_vision_pr.html . Hoagland, A. S. [1963]. Digital Magnetic Recording , Wiley, New York. Hospodor, A. D., A. S. Hoagland [1993]. “The changing nature disk con- trollers. ”Proc. IEEE 81:4 (April), 586 –594. IBM. [1982]. Economic Value Rapid Response Time , GE20-0752-0, IBM, White Plains, N.Y., 11 –82. Imprimis. [1989]. Imprimis Product Specification, 97209 Sabre Disk Drive IPI-2 Interface 1.2 GB , Document No. 64402302, Imprimis, Dallas, Tex. Jain, R. [1991]. Art Computer Systems Performance Analysis: Techniques Experimental Design, Measurement, Simulation, Modeling , Wiley, New York. Katz, R. H., D. A. Patterson, G. A. Gibson [1989]. “Disk system architectures high performance computing, ”Proc. IEEE 77:12 (December), 1842 – 1858. Kim, M. Y. [1986]. “Synchronized disk interleaving, ”IEEE Trans. Computers C-35:11 (November), 978 –988. Kuhn, D. R. [1997]. “Sources failure public switched telephone network, ” IEEE Computer 30:4 (April), 31 –36. Lambright, D. [2000]. “Experiences measuring reliability cache-based storage system, ”Proc. First Workshop Industrial Experiences Sys- tems Software (WIESS 2000), Co-Located 4th Symposium Operating Systems Design Implementation (OSDI) , October 22, 2000, San Diego, Calif.M.10 History Magnetic Storage, RAID, I/O Buses ■M-91Laprie, J.-C. [1985]. “Dependable computing fault tolerance: Concepts terminology, ”Proc. 15th Annual Int ’l. Symposium Fault-Tolerant Comput- ing, June 19 –21, 1985, Ann Arbor, Mich., 2 –11. Lazowska, E. D., J. Zahorjan, G. S. Graham, K. C. Sevcik [1984]. Quantitative System Performance: Computer System Analysis Using Queueing NetworkModels , Prentice Hall, Englewood Cliffs, N.J. (Although print, avail- able online www.cs.washington.edu/homes/lazowska/qsp/ .) Leiner, A. L. [1954]. “System specifications DYSEAC, ”J. ACM 1:2 (April), 57 –81. Leiner, A. L., S. N. Alexander [1954]. “System organization DYSEAC, ” IRE Trans. Electronic Computers EC-3:1 (March), 1 –10. Maberly, N. C. [1966]. Mastering Speed Reading , New American Library, New York. Major, J. B. [1989]. “Are queuing models within grasp unwashed? ” Proc. Int ’l. Conf. Management Performance Evaluation Computer Systems , December 11 –15, 1989, Reno, Nev., 831 –839. Mueller, M., L. C. Alves, W. Fischer, M. L. Fair, I. Modi [1999]. “RAS strat- egy IBM S/390 G5 G6, ”IBM J. Research Development , 43:5 –6 (September –November), 875 –888. Murphy, B., T. Gent [1995]. “Measuring system software reliability using automated data collection process, ”Quality Reliability Engineering International , 11:5 (September –October), 341 –353. Myer, T. H., I. E. Sutherland [1968]. “On design display processors, ” Communications ACM , 11:6 (June), 410 –414. National Storage Industry Consortium. [1998]. “Tape Roadmap, ”www.nsic.org . Nelson, V. P. [1990]. “Fault-tolerant computing: Fundamental concepts, ”Com- puter 23:7 (July), 19 –25. Nyberg, C. R., T. Barclay, Z. Cvetanovic, J. Gray, D. Lomet [1994]. “Alpha- Sort: RISC machine sort, ”Proc. ACM SIGMOD , May 24 –27, 1994, Minneapolis, Minn. Okada, S., S. Okada, Y. Matsuda, T. Yamada, A. Kobayashi [1999]. “System chip digital still camera, ”IEEE Trans. Consumer Electron- ics45:3 (August), 584 –590. Patterson, D. A., G. A. Gibson, R. H. Katz [1987]. Case Redundant Arrays Inexpensive Disks (RAID) , Tech. Rep. UCB/CSD 87/391, University California, Berkeley. Also appeared Proc. ACM SIGMOD , June 1 –3, 1988, Chicago, 109 –116. Pavan, P., R. Bez, P. Olivo, E. Zanoni [1997]. “Flash memory cells —an over- view, ”Proc. IEEE 85:8 (August), 1248 –1271. Robinson, B., L. Blount [1986]. VM/HPO 3880-23 Performance Results , IBM Tech. Bulletin GG66-0247-00, IBM Washington Systems Center, Gaithersburg, Md. Salem, K., H. Garcia-Molina [1986]. “Disk striping, ”Proc. 2nd Int ’l. IEEE Conf. Data Engineering , February 5 –7, 1986, Washington, D.C., 249 –259.M-92 ■Appendix Historical Perspectives ReferencesScranton, R. A., D. A. Thompson, D. W. Hunter [1983]. Access Time Myth , Tech. Rep. RC 10197 (45223), IBM, Yorktown Heights, N.Y. Seagate. [2000]. Seagate Cheetah 73 Family: ST173404LW/LWV/LC/LCV Prod- uct Manual , Vol. 1, Seagate, Scotts Valley, Calif. ( www.seagate.com/support/ disc/manuals/scsi/29478b.pdf ). Smotherman, M. [1989]. “A sequencing-based taxonomy I/O systems review historical machines, ”Computer Architecture News 17:5 (September), 5–15. Reprinted Computer Architecture Readings , M. D. Hill, N. P. Jouppi, G. S. Sohi, eds., Morgan Kaufmann, San Francisco, 1999, 451 –461. Talagala, N. [2000]. “Characterizing Large Storage Systems: Error Behavior Performance Benchmarks, ”Ph.D. dissertation, Computer Science Division, University California, Berkeley. Talagala, N., D. Patterson [1999]. Analysis Error Behavior Large Storage System , Tech. Report UCB//CSD-99-1042, Computer Science Divi- sion, University California, Berkeley. Talagala, N., R. Arpaci-Dusseau, D. Patterson [2000]. Micro-Benchmark Based Extraction Local Global Disk Characteristics , CSD-99-1063, Computer Science Division, University California, Berkeley. Talagala, N., S. Asami, D. Patterson, R. Futernick, D. Hart [2000]. “The art massive storage: case study Web image archive, ”IEEE Computer (November), 22 –28. Thadhani, A. J. [1981]. “Interactive user productivity, ”IBM Systems J. 20:4, 407 – 423.M.10 History Magnetic Storage, RAID, I/O Buses ■M-93References Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M., Ghemawat, S., Irving, G., Isard, M., Kudlur, M., 2016. TensorFlow: System Large-Scale Machine Learning. In: OSDI (November), vol. 16, pp. 265 –283. Adolf, R., Rama, S., Reagen, B., Wei, G.Y., Brooks, D., 2016. Fathom: reference workloads modern deep learning methods. In: IEEE International Symposium Workload Characterization (IISWC). Adve, S.V., Gharachorloo, K., 1996. Shared memory consistency models: tutorial. IEEE Comput. 29 (12), 66 –76. Adve, S.V., Hill, M.D., 1990. Weak ordering: new definition. In: Proceedings 17th Annual International Symposium Computer Architecture (ISCA), May 28 –31, 1990, Seattle, Washington, pp. 2 –14. Agarwal, A., 1987. Analysis Cache Performance Operating Systems Multipro- gramming (Ph.D. thesis). Tech. Rep. No. CSL-TR-87-332. Stanford University, Palo Alto, CA. Agarwal, A., 1991. Limits interconnection network performance. IEEE Trans. Parallel Distrib. Syst. 2 (4), 398 –412. Agarwal, A., Pudar, S.D., 1993. Column-associative caches: technique reducing miss rate direct-mapped caches. In: 20th Annual International Symposium Com- puter Architecture (ISCA), May 16 –19, 1993, San Diego, California. Also appears Computer Architecture News 21:2 (May), 179 –190, 1993. Agarwal, A., Hennessy, J.L., Simoni, R., Horowitz, M.A., 1988. evaluation directory schemes cache coherence. In: Proceedings 15th International Symposium Computer Architecture (June), pp. 280 –289. Agarwal, A., Kubiatowicz, J., Kranz, D., Lim, B.-H., Yeung, D., ’Souza, G., Parkin, M., 1993. Sparcle: evolutionary processor design large-scale multiprocessors. IEEE Micro 13, 48 –61. Agarwal, A., Bianchini, R., Chaiken, D., Johnson, K., Kranz, D., 1995. MIT Alewife machine: architecture performance. In: International Symposium Computer Architecture (Denver, CO), June, 2 –13. Agerwala, T., Cocke, J., 1987. High Performance Reduced Instruction Set Processors. IBM Tech. Rep. RC12434, IBM, Armonk, NY. Akeley, K., Jermoluk, T., 1988. High-performance polygon rendering. In: Proceedings 15th Annual Conference Computer Graphics Interactive Techniques (SIGGRAPH 1988), August 1 –5, 1988, Atlanta, GA, pp. 239 –246. Alexander, W.G., Wortman, D.B., 1975. Static dynamic characteristics XPL programs. IEEE Comput. 8 (11), 41 –46. Alles, A., 1995. ATM Internetworking. White Paper (May). Cisco Systems, Inc., San Jose, CA. www.cisco.com/warp/public/614/12.html . Alliant, 1987. Alliant FX/Series: Product Summary. Alliant Computer Systems Corp, Acton, MA. Almasi, G.S., Gottlieb, A., 1989. Highly Parallel Computing. Benjamin/Cummings, Redwood City, CA. R-1Alverson, G., Alverson, R., Callahan, D., Koblenz, B., Porterfield, A., Smith, B., 1992. Exploiting heterogeneous parallelism multithreaded multiprocessor. In: Proceedings ACM/IEEE Conference Supercomputing, November 16 –20, 1992, Minneapolis, MN, pp. 188 –197. Amdahl, G.M., 1967. Validity single processor approach achieving large scale computing capabilities. In: Proceedings AFIPS Spring Joint Computer Conference, April 18 –20, 1967, Atlantic City, NJ, pp. 483 –485. Amdahl, G.M., Blaauw, G.A., Brooks Jr., F.P., 1964. Architecture IBM System 360. IBM J. Res. Dev. 8 (2), 87 –101. Amodei, D., Ananthanarayanan, S., Anubhai, R., Bai, J., Battenberg, E., Case, C., Casper, J., Catanzaro, B., Cheng, Q., Chen, G., Chen, J., 2016. Deep speech 2: End- to-end speech recognition english mandarin. In: International Conference Machine Learning (June), pp. 173 –182. Amza, C., Cox, A.L., Dwarkadas, S., Keleher, P., Lu, H., Rajamony, R., Yu, W., Zwaenepoel, W., 1996. Treadmarks: shared memory computing networks work- stations. IEEE Comput. 29 (2), 18 –28. Anderson, M.H., 1990. Strength (and safety) numbers (RAID, disk storage technology). Byte 15 (13), 337 –339. Anderson, D., 2003. ’t know jack disks. Queue 1 (4), 20 –30. Anderson, D.W., Sparacio, F.J., Tomasulo, R.M., 1967. IBM 360 Model 91: processor philosophy instruction handling. IBM J. Res. Dev. 11 (1), 8 –24. Anderson, T.E., Culler, D.E., Patterson, D., 1995. case (networks worksta- tions). IEEE Micro 15 (1), 54 –64. Anderson, D., Dykes, J., Riedel, E., 2003. SCSI vs. ATA —more interface. In: Proceedings 2nd USENIX Conference File Storage Technology (FAST ’03), March 31 –April 2. Ang, B., Chiou, D., Rosenband, D., Ehrlich, M., Rudolph, L., Arvind, A., 1998. StarT- Voyager: flexible platform exploring scalable SMP issues. In: Proceedings ACM/IEEE Conference Supercomputing, November 7 –13, 1998, Orlando, FL. Anjan, K.V., Pinkston, T.M., 1995. efficient, fully-adaptive deadlock recovery scheme: Disha. In: Proceedings 22nd Annual International Symposium Computer Archi- tecture (ISCA), June 22 –24, 1995, Santa Margherita, Italy. Anon. et al., 1985. Measure Transaction Processing Power. Tandem Tech. Rep. TR85.2. Also appears Datamation 31:7 (April), 112 –118, 1985. Apache Hadoop, 2011. http://hadoop.apache.org . Archibald, J., Baer, J.-L., 1986. Cache coherence protocols: evaluation using multiproces- sor simulation model. ACM Trans. Comput. Syst. 4 (4), 273 –298. Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patter- son, D., Rabkin, A., Stoica, I., Zaharia, M., 2009. Clouds: Berkeley View Cloud Computing, Tech. Rep. UCB/EECS-2009-28, University California, Berke- ley.http://www.eecs.berkeley.edu/Pubs/TechRpts/2009/EECS-2009-28.html . Armbrust, M., Fox, A., Griffith, R., Joseph, A.D., Katz, R., Konwinski, A., Lee, G., Patterson, D., Rabkin, A., Stoica, I., Zaharia, M., 2010. view cloud computing. Commun. ACM. 53 (4), 50 –58. Arpaci, R.H., Culler, D.E., Krishnamurthy, A., Steinberg, S.G., Yelick, K., 1995. Empirical evaluation CRAY-T3D: compiler perspective. In: 22nd Annual International Symposium Computer Architecture (ISCA), June 22 –24, 1995, Santa Margherita, Italy. Asanovic, K., 1998. Vector Microprocessors (Ph.D. thesis). Computer Science Division, University California, Berkeley. Asanovi /C19c, K., 2002. Programmable neurocomputing. In: Arbib, M.A. (Ed.), Handbook Brain Theory Neural Networks, second ed. MIT Press, Cambridge, MA. ISBN: 0-262-01197-2. https://people.eecs.berkeley.edu/ /C24krste/papers/neurocomputing.pdf .R-2 ■ReferencesAsanovi /C19c, K., Beck, A., Johnson, J., Wawrzynek, J., Kingsbury, B., Morgan, N., 1998. Training neural networks Spert-II. In: Sundararajan, N., Saratchandran, P. (Eds.), Parallel Architectures Artificial Networks: Paradigms Implementations. IEEE Computer Society Press, California, USA. ISBN: 0-8186-8399-6. (Chapter 11) https://people.eecs.berkeley.edu/ /C24krste/papers/annbook.pdf . Associated Press, 2005. Gap Inc. shuts two Internet stores major overhaul. USA- TODAY.com, August 8, 2005. Atanasoff, J.V., 1940. Computing Machine Solution Large Systems Linear Equations. Internal Report. Iowa State University, Ames. Atkins, M., 1991. Performance i860 microprocessor. IEEE Micro 11 (5), 24 –27. 72–78. Austin, T.M., Sohi, G., 1992. Dynamic dependency analysis ordinary programs. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 342 –351. Azizi, O., Mahesri, A., Lee, B.C., Patel, S.J., Horowitz, M., 2010. Energy-performance tradeoffs processor architecture circuit design: marginal cost analysis. In: Proceedings International Symposium Computer Architecture, pp. 26 –36. Babbay, F., Mendelson, A., 1998. Using value prediction increase power specu- lative execution hardware. ACM Trans. Comput. Syst. 16 (3), 234 –270. Bachrach, J., Vo, H., Richards, B., Lee, Y., Waterman, A., Avi žienis, R., Wawrzynek, J., Asanovi /C19c, K., 2012. Chisel: constructing hardware Scala embedded language. In: Proceedings 49th Annual Design Automation Conference, pp. 1216 –1225. Baer, J.-L., Wang, W.-H., 1988. inclusion property multi-level cache hierarchies. In: Proceedings 15th Annual International Symposium Computer Architecture, May 30 –June 2, 1988, Honolulu, Hawaii, pp. 73 –80. Bailey, D.H., Barszcz, E., Barton, J.T., Browning, D.S., Carter, R.L., Dagum, L., Fatoohi, R.A., Frederickson, P.O., Lasinski, T.A., Schreiber, R.S., Simon, H.D., Venkatakrishnan, V., Weeratunga, S.K., 1991. NAS parallel benchmarks. Int. J. Supercomput. Appl. 5, 63 –73. Bakoglu, H.B., Grohoski, G.F., Thatcher, L.E., Kaeli, J.A., Moore, C.R., Tattle, D.P., Male, W.E., Hardell, W.R., Hicks, D.A., Nguyen Phu, M., Montoye, R.K., Glover, W.T., Dhawan, S., 1989. IBM second-generation RISC processor organization. In: Proceedings IEEE International Conference Computer Design, September 30–October 4, 1989, Rye, NY, pp. 138 –142. Balakrishnan, H., Padmanabhan, V.N., Seshan, S., Katz, R.H., 1997. comparison mechanisms improving TCP performance wireless links. IEEE/ACM Trans. Netw. 5 (6), 756 –769. Ball, T., Larus, J., 1993. Branch prediction free. In: Proceedings ACM SIGPLAN ’93 Conference Programming Language Design Implementation (PLDI), June 23–25, 1993, Albuquerque, NM, pp. 30 0 –313. Banerjee, U., 1979. Speedup Ordinary Programs (Ph.D. thesis). Department Computer Science, University Illinois Urbana-Champaign. Barham, P., Dragovic, B., Fraser, K., Hand, S., Harris, T., Ho, A., Neugebauer, R., 2003. Xen art virtualization. In: Proceedings 19th ACM Symposium Oper- ating Systems Principles, October 19 –22, 2003, Bolton Landing, NY. Barnes, G.H., Brown, R.M., Kato, M., Kuck, D.J., Slotnick, D.L., Stokes, R., 1968. ILLIAC IV computer. IEEE Trans. Comput. 100 (8), 746 –757. Barroso, L.A., 2010. Warehouse scale computing [keynote address]. In: Proceedings ACM SIGMOD, June 8 –10, 2010, Indianapolis, IN. Barroso, L.A., H €olzle, U., 2007. case energy-proportional computing. IEEE Comput. 40 (12), 33 –37. Barroso, L.A., H €olzle, U., 2009. Datacenter Computer: Introduction Design Warehouse-Scale Machines. Morgan & Claypool, San Rafael, CA.References ■R-3Barroso, L.A., Gharachorloo, K., Bugnion, E., 1998. Memory system characterization commercial workloads. In: Proceedings 25th Annual International Symposium Computer Architecture (ISCA), July 3 –14, 1998, Barcelona, Spain, pp. 3 –14. Barroso, L.A., Clidaras, J., H €olzle, U., 2013. datacenter computer: introduction design warehouse-scale machines. Synth. Lect. Comput. Architect. 8 (3), 1 –154. Barroso, L.A., Marty, M., Patterson, D., Ranganathan, P., 2017. Attack killer micro- seconds. Commun. ACM 56(2). Barton, R.S., 1961. new approach functional design computer. In: Proceedings Western Joint Computer Conference, May 9 –11, 1961, Los Angeles, CA, pp. 393 –396. Bashe, C.J., Buchholz, W., Hawkins, G.V., Ingram, J.L., Rochester, N., 1981. architec- ture IBM ’s early computers. IBM J. Res. Dev. 25 (5), 363 –375. Bashe, C.J., Johnson, L.R., Palmer, J.H., Pugh, E.W., 1986. IBM ’s Early Computers. MIT Press, Cambridge, MA. Baskett, F., Keller, T.W., 1977. evaluation Cray-1 processor. In: Kuck, D.J., Lawrie, D.H., Sameh, A.H. (Eds.), High Speed Computer Algorithm Organization. Academic Press, San Diego, pp. 71 –84. Baskett, F., Jermoluk, T., Solomon, D., 1988. 4D-MP graphics superworkstation: Computing + graphics ¼40 MIPS + 40 MFLOPS 10,000 lighted polygons per sec- ond. In: Proceedings IEEE COMPCON, February 29 –March 4, 1988, San Francisco, pp. 468 –471. BBN Laboratories, 1986. Butterfly Parallel Processor Overview, Tech. Rep. 6148. BBN Laboratories, Cambridge, MA. Bell, C.G., 1984. mini micro industries. IEEE Comput. 17 (10), 14 –30. Bell, C.G., 1985. Multis: new class multiprocessor computers. Science 228 (6), 462 –467. Bell, C.G., 1989. future high performance computers science engineering. Commun. ACM 32 (9), 1091 –1101. Bell, G., Gray, J., 2001. Crays, Clusters Centers, Tech. Rep. MSR-TR-2001-76. Micro- soft Research, Redmond, WA. Bell, C.G., Gray, J., 2002. ’s next high performance computing? CACM 45 (2), 91–95. Bell, C.G., Newell, A., 1971. Computer Structures: Readings Examples. McGraw-Hill, New York. Bell, C.G., Strecker, W.D., 1976. Computer structures: learned PDP- 11? In: Third Annual International Symposium Computer Architecture (ISCA), Jan- uary 19 –21, 1976, Tampa, FL, pp. 1 –14. Bell, C.G., Strecker, W.D., 1998. Computer structures: learned PDP- 11? In: 25 Years International Symposia Computer Architecture (Selected Papers), ACM, New York, pp. 138 –151. Bell, C.G., Cady, R., McFarland, H., DeLagi, B., ’Laughlin, J., Noonan, R., Wulf, W., 1970. new architecture mini-computers: DEC PDP-11. In: Proceedings AFIPS Spring Joint Computer Conference, May 5 –May 7, 1970, Atlantic City, NJ, pp. 657 –675. Bell, C.G., Mudge, J.C., McNamara, J.E., 1978. DEC View Computer Engineering. Digital Press, Bedford, MA. Benes, V.E., 1962. Rearrangeable three stage connecting networks. Bell Syst. Tech. J. 41, 1481 –1492. Bertozzi, D., Jalabert, A., Murali, S., Tamhankar, R., Stergiou, S., Benini, L., De Micheli, G., 2005. NoC synthesis flow customized domain specific multiprocessor systems-on-chip. IEEE Trans. Parallel Distrib. Syst. 16 (2), 113 –130. Bhandarkar, D.P., 1995. Alpha Architecture Implementations. Digital Press, Newton, MA.R-4 ■ReferencesBhandarkar, D.P., Clark, D.W., 1991. Performance architecture: comparing RISC CISC similar hardware organizations. In: Proceedings Fourth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 8 –11, 1991, Palo Alto, CA, pp. 310 –319. Bhandarkar, D.P., Ding, J., 1997. Performance characterization Pentium Pro proces- sor. In: Proceedings Third International Symposium High-Performance Computer Architecture, February 1 –February 5, 1997, San Antonio, TX, pp. 288 –297. Bhattacharya, S., Lane, N.D., 2016. Sparsification separation deep learning layers constrained resource inference wearables. In: Proceedings 14th ACM Confer- ence Embedded Network Sensor Systems CD-ROM, pp. 176 –189. Bhuyan, L.N., Agrawal, D.P., 1984. Generalized hypercube hyperbus structures computer network. IEEE Trans. Comput. 32 (4), 322 –333. Bienia, C., Kumar, S., Jaswinder, P.S., Li, K., 2008. Parsec Benchmark Suite: Charac- terization Architectural Implications, Tech. Rep. TR-811-08. Princeton University, Princeton, NJ. Bier, J., 1997. evolution DSP processors. In: Presentation University California, Berkeley, November 14. Bird, S., Phansalkar, A., John, L.K., Mericas, A., Indukuru, R., 2007. Characterization performance SPEC CPU benchmarks Intel ’s Core Microarchitecture based pro- cessor. In: Proceedings 2007 SPEC Benchmark Workshop, January 21, 2007, Austin, TX. Birman, M., Samuels, A., Chu, G., Chuk, T., Hu, L., McLeod, J., Barnes, J., 1990. Devel- oping WRL3170/3171 SPARC floating-point coprocessors. IEEE Micro 10 (1), 55–64. Blackburn, M., Garner, R., Hoffman, C., Khan, A.M., McKinley, K.S., Bentzur, R., Diwan, A., Feinberg, D., Frampton, D., Guyer, S.Z., Hirzel, M., Hosking, A., Jump, M., Lee, H., Moss, J.E.B., Phansalkar, A., Stefanovic, D., VanDrunen, T., von Dincklage, D., Wiedermann, B., 2006. DaCapo benchmarks: Java benchmark- ing development analysis. In: ACM SIGPLAN Conference Object-Oriented Programming, Systems, Languages, Applications (OOPSLA), October 22 –26, 2006, pp. 169 –190. Blaum, M., Brady, J., Bruck, J., Menon, J., 1994. EVENODD: optimal scheme tol- erating double disk failures RAID architectures. In: Proceedings 21st Annual Inter- national Symposium Computer Architecture (ISCA), April 18 –21, 1994, Chicago, IL, pp. 245 –254. Blaum, M., Brady, J., Bruck, J., Menon, J., 1995. EVENODD: optimal scheme tolerating double disk failures RAID architectures. IEEE Trans. Comput. 44 (2), 192–202. Blaum, M., Bruck, J., Vardy, A., 1996. MDS array codes independent parity symbols. IEEE Trans. Inf. Theory 42, 529 –542. Blaum, M., Brady, J., Bruck, J., Menon, J., Vardy, A., 2001. EVENODD code generalization. In: Jin, H., Cortes, T., Buyya, R. (Eds.), High Performance Mass Storage Parallel I/O: Technologies Applications. Wiley-IEEE, New York, pp. 187 –208. Bloch, E., 1959. engineering design Stretch computer. In: 1959 Proceedings Eastern Joint Computer Conference, December 1 –3, 1959, Boston, MA, pp. 48 –59. Boddie, J.R., 2000. History DSPs, www.lucent.com/micro/dsp/dsphist.html . Boggs, D., Baktha, A., Hawkins, J., Marr, D.T., Miller, J.A., Roussel, P., et al., 2004. Microarchitecture Intel Pentium 4 processor 90 nm technology. Intel Technol. J. 8 (1), 7 –23. Bolt, K.M., 2005. Amazon sees sales rise, profit fall. Seattle Post-Intelligencer. http:// seattlepi.nwsource.com/business/245943_techearns26.html .References ■R-5Bordawekar, R., Bondhugula, U., Rao, R., 2010. Believe not!: multi-core CPUs match GPU performance FLOP-intensive application! In: 19th International Conference Parallel Architecture Compilation Techniques (PACT 2010). Vienna, Austria, September 11 –15, 2010, pp. 537 –538. Borg, A., Kessler, R.E., Wall, D.W., 1990. Generation analysis long address traces. In: 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 270 –279. Bouknight, W.J., Deneberg, S.A., McIntyre, D.E., Randall, J.M., Sameh, A.H., Slotnick, D.L., 1972. Illiac IV system. Proc. IEEE 60 (4), 369 –379. Also appears Siewiorek, D.P., Bell, C.G., Newell, A. 1982. Computer Structures: Principles Examples. McGraw-Hill, New York, pp. 306 –316. Brady, J.T., 1986. theory productivity creative process. IEEE Comput. Graph. Appl. 6 (5), 25 –34. Brain, M., 2000. Inside Digital Cell Phone. www.howstuffworks.com/-inside-cellphone. htm. Brandt, M., Brooks, J., Cahir, M., Hewitt, T., Lopez-Pineda, E., Sandness, D., 2000. Benchmarker ’s Guide Cray SV1 Systems. Cray Inc., Seattle, WA. Brent, R.P., Kung, H.T., 1982. regular layout parallel adders. IEEE Trans. Comput. C-31, 260 –264. Brewer, E.A., Kuszmaul, B.C., 1994. get good performance CM-5 data network. In: Proceedings Eighth International Parallel Processing Symposium, April 26–27, 1994, Cancun, Mexico. Brin, S., Page, L., 1998. anatomy large-scale hypertextual Web search engine. In: Proceedings 7th International World Wide Web Conference, April 14 –18, 1998, Brisbane, Queensland, Australia, pp. 107 –117. Brown, A., Patterson, D.A., 2000. Towards maintainability, availability, growth bench- marks: case study software RAID systems. In: Proceedings 2000 USENIX Annual Technical Conference, June 18 –23, 2000, San Diego, CA. Brunhaver, J.S., 2015. Design optimization stencil engine (Ph.D. dissertation). Stanford University. Bucher, I.Y., 1983. computational speed supercomputers. In: Proceedings Inter- national Conference Measuring Modeling Computer Systems (SIGMETRICS 1983), August 29 –31, 1983, Minneapolis, MN, pp. 151 –165. Bucher, I.V., Hayes, A.H., 1980. I/O performance measurement Cray-1 CDC 7000 computers. In: Proceedings Computer Performance Evaluation Users Group, 16th Meeting, NBS 500-65, pp. 245 –254. Bucholtz, W., 1962. Planning Computer System: Project Stretch. McGraw-Hill, New York. Burgess, N., Williams, T., 1995. Choices operand truncation SRT division algo- rithm. IEEE Trans. Comput. 44 (7), 933 –938. Burkhardt III, H., Frank, S., Knobe, B., Rothnie, J., 1992. Overview KSR1 Computer System, Tech. Rep. KSR-TR-9202001. Kendall Square Research, Boston, MA. Burks, A.W., Goldstine, H.H., von Neumann, J., 1946. Preliminary discussion logical design electronic computing instrument. Report U.S. Army Ordnance Department, p. 1; also appears Papers John von Neumann, Aspray, W., Burks, A. (Eds.), MIT Press, Cambridge, MA, Tomash Publishers, Los Angeles, CA, 1987, pp. 97 –146. Calder, B., Grunwald, D., Jones, M., Lindsay, D., Martin, J., Mozer, M., Zorn, B., 1997. Evidence-based static branch prediction using machine learning. ACM Trans. Program. Lang. Syst. 19 (1), 188 –222.R-6 ■ReferencesCalder, B., Reinman, G., Tullsen, D.M., 1999. Selective value prediction. In: Proceedings 26th Annual International Symposium Computer Architecture (ISCA), May 2 –4, 1999, Atlanta, GA. Callahan, D., Dongarra, J., Levine, D., 1988. Vectorizing compilers: test suite results. In: Proceedings ACM/IEEE Conference Supercomputing, November 12 –17, 1988, Orland, FL, pp. 98 –105. Canis, A., Choi, J., Aldham, M., Zhang, V., Kammoona, A., Czajkowski, T., Brown, S.D., Anderson, J.H., 2013. LegUp: open-source high-level synthesis tool FPGA-based processor/accelerator systems. ACM Trans. Embed. Comput. Syst. 13(2). Canny, J., et al., 2015. Machine learning limit. In: IEEE International Conference Big Data. Cantin, J.F., Hill, M.D., 2001. Cache performance selected SPEC CPU2000 bench- marks. www.jfred.org/cache-data.html . Cantin, J.F., Hill, M.D., 2003. Cache performance SPEC CPU2000 benchmarks, version 3.0.www.cs.wisc.edu/multifacet/misc/spec2000cache-data/index.html . Carles, S., 2005. Amazon reports record Xmas season, top game picks. Gamasutra, December 27. http://www.gamasutra.com/php-bin/news_index.php?story ¼7630 . Carter, J., Rajamani, K., 2010. Designing energy-efficient servers data centers. IEEE Comput. 43 (7), 76 –78. Case, R.P., Padegs, A., 1978. architecture IBM System/370. Commun. ACM 21 (1), 73 –96. Also appears Siewiorek, D.P., Bell, C.G., Newell, A., 1982. Computer Structures: Principles Examples. McGraw-Hill, New York, pp. 830 –855. Caulfield, A.M., Chung, E.S., Putnam, A., Haselman, H.A.J.F.M., Humphrey, S.H.M., Daniel, P.K.J.Y.K., Ovtcharov, L.T.M.K., Lanka, M.P.L.W.S., Burger, D.C.D., 2016. cloud-scale acceleration architecture. In: MICRO Conference. Censier, L., Feautrier, P., 1978. new solution coherence problems multicache systems. IEEE Trans. Comput. C-27 (12), 1112 –1118. Chandra, R., Devine, S., Verghese, B., Gupta, A., Rosenblum, M., 1994. Scheduling page migration multiprocessor compute servers. In: Sixth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 4 –7, 1994, San Jose, CA, pp. 12 –24. Chang, P.P., Mahlke, S.A., Chen, W.Y., Warter, N.J., Hwu, W.W., 1991. IMPACT: architectural framework multiple-instruction-issue processors. In: 18th Annual International Symposium Computer Architecture (ISCA), May 27 –30, 1991, Toronto, Canada, pp. 266 –275. Chang, F., Dean, J., Ghemawat, S., Hsieh, W.C., Wallach, D.A., Burrows, M., Chandra, T., Fikes, A., Gruber, R.E., 2006. Bigtable: distributed storage system structured data. In: Proceedings 7th USENIX Symposium Operating Systems Design Imple-mentation (OSDI ’06), November 6 –8, 2006, Seattle, WA. Chang, J., Meza, J., Ranganathan, P., Bash, C., Shah, A., 2010. Green server design: beyond operational energy sustainability. In: Proceedings Workshop Power Aware Com- puting Systems (HotPower ’10), October 3, 2010, Vancouver, British Columbia. Charlesworth, A.E., 1981. approach scientific array processing: architectural design AP-120B/FPS-164 family. Computer 9, 18 –27. Charlesworth, A., 1998. Starfire: extending SMP envelope. IEEE Micro 18 (1), 39 –49. Chen, T.C., 1980. Overlap parallel processing. In: Stone, H. (Ed.), Introduction Computer Architecture. Science Research Associates, Chicago, pp. 427 –486. Chen, S., 1983. Large-scale high-speed multiprocessor system scientific applica- tions. In: Proceedings NATO Advanced Research Workshop High-Speed Com- puting, June 20 –22, 1983, J €ulich, West Germany. Also appears Hwang, K. (Ed.), 1984. Superprocessors: design applications, IEEE (August), 602 –609.References ■R-7Chen, P.M., Lee, E.K., 1995. Striping RAID level 5 disk array. In: Proceedings ACM SIGMETRICS Conference Measurement Modeling Computer Systems, May 15–19, 1995, Ottawa, Canada, pp. 136 –145. Chen, P.M., Gibson, G.A., Katz, R.H., Patterson, D.A., 1990. evaluation redundant arrays inexpensive disks using Amdahl 5890. In: Proceedings ACM SIG-METRICS Conference Measurement Modeling Computer Systems, May 22–25, 1990, Boulder, CO. Chen, P.M., Lee, E.K., Gibson, G.A., Katz, R.H., Patterson, D.A., 1994. RAID: high- performance, reliable secondary storage. ACM Comput. Surv. 26 (2), 145 –188. Chow, F.C., 1983. Portable Machine-Independent Global Optimizer —Design Measurements (Ph.D. thesis). Stanford University, Palo Alto, CA. Chrysos, G.Z., Emer, J.S., 1998. Memory dependence prediction using store sets. In: Proceedings 25th Annual International Symposium Computer Architecture (ISCA), July 3 –14, 1998, Barcelona, Spain, pp. 142 –153. Clark, W.A., 1957. Lincoln TX-2 computer development. In: Proceedings Western Joint Computer Conference, February 26 –28, 1957, Los Angeles, pp. 143 –145. Clark, D.W., 1983. Cache performance VAX-11/780. ACM Trans. Comput. Syst. 1 (1), 24 –37. Clark, D.W., 1987. Pipelining performance VAX 8800 processor. In: Proceedings Second International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 5 –8, 1987, Palo Alto, CA, pp. 173 –177. Clark, J., 2014. Five Numbers Illustrate Mind-Bending Size Amazon's Cloud. Bloomberg. https://www.bloomberg.com/news/2014-11-14/5-numbersthat-illustrate- the-mind-bending-size-of-amazon-s-cloud.html . Clark, J., October 26, 2015. Google Turning Lucrative Web Search AI Machines. Bloomberg Technology, www.bloomberg.com . Clark, D.W., Emer, J.S., 1985. Performance VAX-11/780 translation buffer: simula- tion measurement. ACM Trans. Comput. Syst. 3 (1), 31 –62. Clark, D., Levy, H., 1982. Measurement analysis instruction set use VAX-11/ 780. In: Proceedings Ninth Annual International Symposium Computer Architec- ture (ISCA), April 26 –29, 1982, Austin, TX, pp. 9 –17. Clark, D., Strecker, W.D., 1980. Comments ‘the case reduced instruction set com- puter ’. Comput. Architect. News 8 (6), 34 –38. Clark, B., Deshane, T., Dow, E., Evanchik, S., Finlayson, M., Herne, J., Neefe Matthews, J., 2004. Xen art repeated research. In: Proceedings USENIX Annual Tech- nical Conference, June 27 –July 2, 2004, pp. 135 –144. Clidaras, J., Johnson, C., Felderman, B., 2010. Private communication. Climate Savers Computing Initiative, 2007. Efficiency Specs. http://www.climatesavers computing.org/ . Clos, C., 1953. study non-blocking switching networks. Bell Syst. Tech. J. 32 (2), 406–424. Cloud, Bloomberg, n.d. https://www.bloomberg.com/news/2014-11-14/5-numbersthat- illustrate-the-mind-bending-size-of-amazon-s-cloud.html . Cody, W.J., Coonen, J.T., Gay, D.M., Hanson, K., Hough, D., Kahan, W., Karpinski, R., Palmer, J., Ris, F.N., Stevenson, D., 1984. proposed radix- word-length indepen- dent standard floating-point arithmetic. IEEE Micro 4 (4), 86 –100. Colwell, R.P., Steck, R., 1995. 0.6 μm BiCMOS processor dynamic execution. In: Proceedings IEEE International Symposium Solid State Circuits (ISSCC), February 15 –17, 1995, San Francisco, pp. 176 –177. Colwell, R.P., Nix, R.P., ’Donnel, J.J., Papworth, D.B., Rodman, P.K., 1987. VLIW architecture trace scheduling compiler. In: Proceedings Second InternationalR-8 ■ReferencesConference Architectural Support Programming Languages Operating Systems (ASPLOS), October 5 –8, 1987, Palo Alto, CA, pp. 180 –192. Comer, D., 1993. Internetworking TCP/IP, second ed. Prentice Hall, Englewood Cliffs, NJ. Compaq Computer Corporation, 1999. Compiler Writer ’s Guide Alpha 21264, Order Number EC-RJ66A-TE, June, www1.support.compaq.com/alpha-tools/-documentation/ current/21264_EV67/ec-rj66a-te_comp_writ_gde_for_alpha21264.pdf . Conti, C., Gibson, D.H., Pitkowsky, S.H., 1968. Structural aspects System/360 Model 85. Part I. General organization. IBM Syst. J. 7 (1), 2 –14. Coonen, J., 1984. Contributions Proposed Standard Binary Floating-Point Arithmetic (Ph.D. thesis). University California, Berkeley. Corbett, P., English, B., Goel, A., Grcanac, T., Kleiman, S., Leong, J., Sankar, S., 2004. Row-diagonal parity double disk failure correction. In: Proceedings 3rd USENIX Conference File Storage Technology (FAST ’04), March 31 –April 2, 2004, San Francisco. Crawford, J., Gelsinger, P., 1988. Programming 80386. Sybex Books, Alameda, CA. Culler, D.E., Singh, J.P., Gupta, A., 1999. Parallel Computer Architecture: Hardware/ Software Approach. Morgan Kaufmann, San Francisco. Curnow, H.J., Wichmann, B.A., 1976. synthetic benchmark. Comput. J. 19 (1), 43 –49. Cvetanovic, Z., Kessler, R.E., 2000. Performance analysis Alpha 21264-based Compaq ES40 system. In: Proceedings 27th Annual International Symposium Computer Architecture (ISCA), June 10 –14, 2000, Vancouver, Canada, pp. 192 –202. Dally, W.J., 1990. Performance analysis k-ary n-cube interconnection networks. IEEE Trans. Comput. 39 (6), 775 –785. Dally, W.J., 1992. Virtual channel flow control. IEEE Trans. Parallel Distrib. Syst. 3 (2), 194–205. Dally, W.J., 1999. Interconnect limited VLSI architecture. In: Proceedings Interna- tional Interconnect Technology Conference, May 24 –26, 1999, San Francisco. Dally, W.J., 2002. Computer architecture interconnect. In: Proceedings 8th International Symposium High Performance Computer Architecture. Dally, W.J., 2016. High Performance Hardware Machine Learning. Cadence Embedded Neural Network Summit, February 9, 2016. http://ip.cadence.com/uploads/ presentations/1000AM_Dally_Cadence_ENN.pdf . Dally, W.J., Seitz, C.I., 1986. torus routing chip. Distrib. Comput. 1 (4), 187 –196. Dally, W.J., Towles, B., 2001. Route packets, wires: on-chip interconnection networks. In: Proceedings 38th Design Automation Conference, June 18 –22, 2001, Las Vegas. Dally, W.J., Towles, B., 2003. Principles Practices Interconnection Networks. Morgan Kaufmann, San Francisco. Darcy, J.D., Gay, D., 1996. FLECKmarks: measuring floating point performance using full IEEE compliant arithmetic benchmark. CS 252 class project, University California, Berkeley. See http.CS.Berkeley.EDU/ /C24darcy/Projects/cs252/ . Darley, H.M., et al., 1989. Floating Point/Integer Processor Divide Square Root Functions, U.S. Patent 4,878,190, October 31. Davidson, E.S., 1971. design control pipelined function generators. In: Proceedings IEEE Conference Systems, Networks, Computers, January 19–21, 1971, Oaxtepec, Mexico, pp. 19 –21. Davidson, E.S., Thomas, A.T., Shar, L.E., Patel, J.H., 1975. Effective control pipelined processors. In: Proceedings IEEE COMPCON, February 25 –27, 1975, San Francisco, pp. 181 –184. Davie, B.S., Peterson, L.L., Clark, D., 1999. Computer Networks: Systems Approach, second ed. Morgan Kaufmann, San Francisco.References ■R-9Dean, J., 2009. Designs, lessons advice building large distributed systems [key- note address]. In: Proceedings 3rd ACM SIGOPS International Workshop Large-Scale Distributed Systems Middleware, Co-located 22nd ACM Symposium Operating Systems Principles, October 11 –14, 2009, Big Sky, Mont. Dean, J., Barroso, L.A., 2013. tail scale. Commun. ACM 56 (2), 74 –80. Dean, J., Ghemawat, S., 2004. MapReduce: simplified data processing large clusters. In: Proceedings Operating Systems Design Implementation (OSDI), December 6–8, 2004, San Francisco, CA, pp. 137 –150. Dean, J., Ghemawat, S., 2008. MapReduce: simplified data processing large clusters. Commun. ACM 51 (1), 107 –113. DeCandia, G., Hastorun, D., Jampani, M., Kakulapati, G., Lakshman, A., Pilchin, A., Sivasubramanian, S., Vosshall, P., Vogels, W., 2007. Dynamo: Amazon ’s highly avail- able key-value store. In: Proceedings 21st ACM Symposium Operating Systems Principles, October 14 –17, 2007, Stevenson, WA. Dehnert, J.C., Hsu, P.Y.-T., Bratt, J.P., 1989. Overlapped loop support Cydra 5. In: Proceedings Third International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 3 –6, 1989, Boston, MA, pp. 26 –39. Demmel, J.W., Li, X., 1994. Faster numerical algorithms via exception handling. IEEE Trans. Comput. 43 (8), 983 –992. Denehy, T.E., Bent, J., Popovici, F.I., Arpaci-Dusseau, A.C., Arpaci-Dusseau, R.H., 2004. Deconstructing storage arrays. In: Proceedings 11th International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 7 –13, 2004, Boston, MA, pp. 59 –71. Desurvire, E., 1992. Lightwave communications: fifth generation. Sci. Am. (Int. Ed.) 266 (1), 96 –103. Diep, T.A., Nelson, C., Shen, J.P., 1995. Performance evaluation PowerPC 620 microarchitecture. In: Proceedings 22nd Annual International Symposium Computer Architecture (ISCA), June 22 –24, 1995, Santa Margherita, Italy. Digital Semiconductor, 1996. Alpha Architecture Handbook, Version 3. Digital Press, Maynard, MA. Ditzel, D.R., McLellan, H.R., 1987. Branch folding CRISP microprocessor: reducing branch delay zero. In: Proceedings 14th Annual International Symposium Computer Architecture (ISCA), June 2 –5, 1987, Pittsburgh, PA, pp. 2 –7. Ditzel, D.R., Patterson, D.A., 1980. Retrospective high-level language computer archi- tecture. In: Proceedings Seventh Annual International Symposium Computer Architecture (ISCA), May 6 –8, 1980, La Baule, France, pp. 97 –104. Doherty, W.J., Kelisky, R.P., 1979. Managing VM/CMS systems user effectiveness. IBM Syst. J. 18 (1), 143 –166. Doherty, W.J., Thadhani, A.J., 1982. economic value rapid response time. IBM Report. Dongarra, J.J., 1986. survey high performance processors. In: Proceedings IEEE COMPCON, March 3 –6, 1986, San Francisco, pp. 8 –11. Dongarra, J.J., Luszczek, P., Petitet, A., 2003. LINPACK benchmark: past, present future. Concurr. Comput. Pract. Exp. 15 (9), 803 –820. Dongarra, J., Sterling, T., Simon, H., Strohmai er, E., 2005. High-performance computing: clus- ters, constellations, MPPs, future directions. Comput. Sci. Eng. 7 (2), 51 –59. Douceur, J.R., Bolosky, W.J., 1999. large scale study file-system contents. In: Proceedings ACM SIGMETRICS Conference Measurement Modeling Computer Systems, May 1 –9, 1999, Atlanta, GA, pp. 59 –69. Douglas, J., 2005. Intel 8xx series Paxville Xeon-MP microprocessors. In: Paper Presented Hot Chips 17, August 14 –16, 2005, Stanford University, Palo Alto, CA.R-10 ■ReferencesDuato, J., 1993. new theory deadlock-free adaptive routing wormhole networks. IEEE Trans. Parallel Distrib. Syst. 4 (12), 1320 –1331. Duato, J., Pinkston, T.M., 2001. general theory deadlock-free adaptive routing using mixed set resources. IEEE Trans. Parallel Distrib. Syst. 12 (12), 1219 –1235. Duato, J., Yalamanchili, S., Ni, L., 2003. Interconnection Networks: Engineering Approach, 2nd printing Morgan Kaufmann, San Francisco. Duato, J., Johnson, I., Flich, J., Naven, F., Garcia, P., Nachiondo, T., 2005a. new scalable cost-effective congestion management strategy lossless multistage interconnec- tion networks. In: Proceedings 11th International Symposium High-Performance Computer Architecture, February 12 –16, 2005, San Francisco. Duato, J., Lysne, O., Pang, R., Pinkston, T.M., 2005b. Part I: theory deadlock-free dynamic reconfiguration interconnection networks. IEEE Trans. Parallel Distrib. Syst. 16 (5), 412 –427. Dubois, M., Scheurich, C., Briggs, F., 1988. Synchronization, coherence, event order- ing. IEEE Comput. 21 (2), 9 –21. Dunigan, W., Vetter, K., White, K., Worley, P., 2005. Performance evaluation Cray X1 distributed shared memory architecture. IEEE Micro, 30 –40. Eden, A., Mudge, T., 1998. YAGS branch prediction scheme. In: Proceedings 31st Annual ACM/IEEE International Symposium Microarchitecture, November 30–December 2, 1998, Dallas, TX, pp. 69 –80. Edmondson, J.H., Rubinfield, P.I., Preston, R., Rajagopalan, V., 1995. Superscalar instruc- tion execution 21164 Alpha microprocessor. IEEE Micro 15 (2), 33 –43. Eggers, S., 1989. Simulation Analysis Data Sharing Shared Memory Multiprocessors (Ph.D. thesis). University California, Berkeley. Elder, J., Gottlieb, A., Kruskal, C.K., McAuliffe, K.P., Randolph, L., Snir, M., Teller, P., Wilson, J., 1985. Issues related MIMD shared-memory computers: NYU ultra- computer approach. In: Proceedings 12th Annual International Symposium Com- puter Architecture (ISCA), June 17 –19, 1985, Boston, MA, pp. 126 –135. Ellis, J.R., 1986. Bulldog: Compiler VLIW Architectures. MIT Press, Cambridge, MA. Emer, J.S., Clark, D.W., 1984. characterization processor performance VAX-11/ 780. In: Proceedings 11th Annual International Symposium Computer Architec- ture (ISCA), June 5 –7, 1984, Ann Arbor, MI, pp. 301 –310. Enriquez, P., 2001. happened dial tone? study FCC service disruption reports. In: Poster, Richard Tapia Symposium Celebration Diversity Computing, October 18 –20, Houston, TX. Erlichson, A., Nuckolls, N., Chesson, G., Hennessy, J.L., 1996. SoftFLASH: analyzing performance clustered distributed virtual shared memory. In: Proceedings Seventh International Conference Architectural Support Programming Languages andOperating Systems (ASPLOS), October 1 –5, 1996, Cambridge, MA, pp. 210 –220. Esmaeilzadeh, H., Cao, T., Xi, Y., Blackburn, S.M., McKinley, K.S., 2011. Looking back language hardware revolution: measured power, performance, scaling. In: Proceedings 16th International Conference Architectural Support Program- ming Languages Operating Systems (ASPLOS), March 5 –11, 2011, Newport Beach, CA. Esmaeilzadeh, H., Blem, E., St Amant, R., Sankaralingam, K., Burger, D., 2012. Power lim- itations dark silicon challenge future multicore. ACM Trans. Comput. Syst. 30 (3), 115 –138. Evers, M., Patel, S.J., Chappell, R.S., Patt, Y.N., 1998. analysis correlation predictability: makes two-level branch predictors work. In: Proceedings 25th Annual International Symposium Computer Architecture (ISCA), July 3 –14, 1998, Barcelona, Spain, pp. 52 –61. Fabry, R.S., 1974. Capability based addressing. Commun. ACM 17 (7), 403 –412.References ■R-11Falsafi, B., Wood, D.A., 1997. Reactive NUMA: design unifying S-COMA CC- NUMA. In: Proceedings 24th Annual International Symposium Computer Archi- tecture (ISCA), June 2 –4, 1997, Denver, CO, pp. 229 –240. Fan, X., Weber, W., Barroso, L.A., 2007. Power provisioning warehouse-sized com- puter. In: Proceedings 34th Annual International Symposium Computer Architec-ture (ISCA), June 9 –13, 2007, San Diego, CA. Farkas, K.I., Jouppi, N.P., 1994. Complexity/performance trade-offs non-blocking loads. In: Proceedings 21st Annual International Symposium Computer Architec- ture (ISCA), April 18 –21, 1994, Chicago. Farkas, K.I., Jouppi, N.P., Chow, P., 1995. useful non-blocking loads, stream buffers speculative execution multiple issue processors? In: Proceedings First IEEE Symposium High-Performance Computer Architecture, January 22 –25, 1995, Raleigh, NC, pp. 78 –89. Farkas, K.I., Chow, P., Jouppi, N.P., Vranesic, Z., 1997. Memory-system design consider- ations dynamically-scheduled processors. In: Proceedings 24th Annual Interna- tional Symposium Computer Architecture (ISCA), June 2 –4, 1997, Denver, CO, pp. 133 –143. Fazio, D., 1987. ’s really much fun building supercomputer simply invent- ing one. In: Proceedings IEEE COMPCON, February 23 –27, 1987, San Francisco, pp. 102 –105. Fikes, A., 2010. Storage architecture challenges. In: Google Faculty Summit. Fisher, J.A., 1981. Trace scheduling: technique global microcode compaction. IEEE Trans. Comput. 30 (7), 478 –490. Fisher, J.A., 1983. long instruction word architectures ELI-512. In: 10th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1982, Stock- holm, Sweden, pp. 140 –150. Fisher, J.A., Freudenberger, S.M., 1992. Predicting conditional branches previous runs program. In: Proceedings Fifth International Conference Architectural Sup- port Programming Languages Operating Systems (ASPLOS), October 12 –15, 1992, Boston, MA, pp. 85 –95. Fisher, J.A., Rau, B.R., 1993. J. Supercomput., January (special issue). Fisher, J.A., Ellis, J.R., Ruttenberg, J.C., Nicolau, A., 1984. Parallel processing: smart compiler dumb processor. In: Proceedings SIGPLAN Conference Compiler Construction, June 17 –22, 1984, Montreal, Canada, pp. 11 –16. Flemming, P.J., Wallace, J.J., 1986. lie statistics: correct way sum- marize benchmarks results. Commun. ACM 29 (3), 218 –221. Flynn, M.J., 1966. high-speed computing systems. Proc. IEEE 54 (12), 1901 –1909. Forgie, J.W., 1957. Lincoln TX-2 input-output system. In: Proceedings Western Joint Computer Conference (February), Institute Radio Engineers, Los Angeles, pp. 156 –160. Foster, C.C., Riseman, E.M., 1972. Percolation code enhance parallel dispatching execution. IEEE Trans. Comput. C-21 (12), 1411 –1415. Frank, S.J., 1984. Tightly coupled multiprocessor systems speed memory access time. Elec- tronics 57 (1), 164 –169. Freescale part i.MX31 Applications Processor, 2006. http://cache.freescale.com/files/ 32bit/doc/white_paper/IMX31MULTIWP.pdf . Freiman, C.V., 1961. Statistical analysis certain binary division algorithms. Proc. IRE 49 (1), 91 –103. Friesenborg, S.E., Wicks, R.J., 1985. DASD Expectations: 3380, 3380-23, MVS/XA, Tech. Bulletin GG22-9363-02. IBM Washington Systems Center, Gaithers- burg, MD.R-12 ■ReferencesFuller, S.H., Burr, W.E., 1977. Measurement evaluation alternative computer archi- tectures. Computer 10 (10), 24 –35. Furber, S.B., 1996. ARM System Architecture. Addison-Wesley, Harlow, England. www. cs.man.ac.uk/amulet/publications/books/ARMsysArch . Gagliardi, U.O., 1973. Report workshop 4 —software-related advances computer hard- ware. In: Proceedings Symposium High Cost Software, September 17 –19, 1973, Monterey, CA, pp. 99 –120. Gajski, D., Kuck, D., Lawrie, D., Sameh, A., 1983. CEDAR —a large scale multiprocessor. In: Proceedings International Conference Parallel Processing (ICPP), August, Columbus, Ohio, pp. 524 –529. Galal, S., Shacham, O., Brunhaver II, J.S., Pu, J., Vassiliev, A., Horowitz, M., 2013. FPU generator design space exploration. In: 21st IEEE Symposium Computer Arith- metic (ARITH). Gallagher, D.M., Chen, W.Y., Mahlke, S.A., Gyllenhaal, J.C., Hwu, W.W., 1994. Dynamic memory disambiguation using memory conflict buffer. In: Proceedings Sixth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 4 –7, Santa Jose, CA, pp. 183 –193. Galles, M., 1996. Scalable pipelined interconnect distributed endpoint routing: SGI SPIDER chip. In: Proceedings IEEE HOT Interconnects ’96, August 15 –17, 1996, Stanford University, Palo Alto, CA. Game, M., Booker, A., 1999. CodePack code compression PowerPC processors. Micro- News. 5 (1). www.chips.ibm.com/micronews/vol5_no1/codepack.html . Gao, Q.S., 1993. Chinese remainder theorem prime memory system. In: 20th Annual International Symposium Computer Architecture (ISCA), May 16 –19, 1993, San Diego, CA (Computer Architecture News 21:2 (May), pp. 337 –340. Gap, 2005. Gap Inc. Reports Third Quarter Earnings. http://gapinc.com/public/documents/ PR_Q405EarningsFeb2306.pdf . Gap, 2006. Gap Inc. Reports Fourth Quarter Full Year Earnings. http://-gapinc.com/ public/documents/Q32005PressRelease_Final22.pdf . Garner, R., Agarwal, A., Briggs, F., Brown, E., Hough, D., Joy, B., Kleiman, S., Muchnick, S., Namjoo, M., Patterson, D., Pendleton, J., Tuck, R., 1988. Scalable pro- cessor architecture (SPARC). In: Proceedings IEEE COMPCON, February 29–March 4, 1988, San Francisco, pp. 278 –283. Gebis, J., Patterson, D., 2007. Embracing extending 20th-century instruction set archi- tectures. IEEE Comput. 40 (4), 68 –75. Gee, J.D., Hill, M.D., Pnevmatikatos, D.N., Smith, A.J., 1993. Cache performance SPEC92 benchmark suite. IEEE Micro 13 (4), 17 –27. Gehringer, E.F., Siewiorek, D.P., Segall, Z., 1987. Parallel Processing: Cm* Experi- ence. Digital Press, Bedford, MA. Gharachorloo, K., Lenoski, D., Laudon, J., Gibbons, P., Gupta, A., Hennessy, J.L., 1990. Memory consistency event ordering scalable shared-memory multiprocessors. In: Proceedings 17th Annual International Symposium Computer Architecture (ISCA), May 28 –31, 1990, Seattle, WA, pp. 15 –26. Gharachorloo, K., Gupta, A., Hennessy, J.L., 1992. Hiding memory latency using dynamic scheduling shared-memory multiprocessors. In: Proceedings 19th Annual Interna- tional Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia. Ghemawat, S., Gobioff, H., Leung, S.-T., 2003. Google file system. In: Proceedings 19th ACM Symposium Operating Systems Principles, October 19 –22, 2003, Bolton Landing, NY.References ■R-13Gibson, D.H., 1967. Considerations block-oriented systems design. AFIPS Conf. Proc. 30, 75 –80. Gibson, J.C., 1970. Gibson mix, Rep. TR. 00.2043. IBM Systems Development Divi- sion, Poughkeepsie, NY (research done 1959). Gibson, G.A., 1992. In: Redundant Disk Arrays: Reliable, Parallel Secondary Storage. ACM Distinguished Dissertation Series, MIT Press, Cambridge, MA. Gibson, J., Kunz, R., Ofelt, D., Horowitz, M., Hennessy, J., Heinrich, M., 2000. FLASH vs. (simulated) FLASH: Closing simulation loop. In: Proceedings Ninth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), November 12 –15, Cambridge, MA, pp. 49 –58. Glass, C.J., Ni, L.M., 1992. Turn Model adaptive routing. In: 19th Annual Interna- tional Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia. Goldberg, I.B., 1967. 27 bits enough 8-digit accuracy. Commun. ACM 10 (2), 105–106. Goldberg, D., 1991. every computer scientist know floating-point arith- metic. Comput. Surv. 23 (1), 5 –48. Goldstein, S., 1987. Storage Performance —An Eight Year Outlook, Tech. Rep. TR 03.308- 1. Santa Teresa Laboratory, IBM Santa Teresa Laboratory, San Jose, CA. Goldstine, H.H., 1972. Computer: Pascal von Neumann. Princeton University Press, Princeton, NJ. González, A., Day, M., 2016. Amazon, Microsoft invest billions computing shifts cloud. Seattle Times. http://www.seattletimes.com/business/technology/amazon- microsoft-invest-billions-as-computing-shifts-to-cloud/ . González, J., González, A., 1998. Limits instruction level parallelism data specula- tion. In: Proceedings Vector Parallel Processing (VECPAR) Conference, June 21–23, 1998, Porto, Portugal, pp. 585 –598. Goodman, J.R., 1983. Using cache memory reduce processor memory traffic. In: Proceedings 10th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1982, Stockholm, Sweden, pp. 124 –131. Goralski, W., 1997. SONET: Guide Synchronous Optical Network. McGraw-Hill, New York. Gosling, J.B., 1980. Design Arithmetic Units Digital Computers. Springer-Verlag, New York. Gray, J., 1990. census Tandem system availability 1985 1990. IEEE Trans. Reliab. 39 (4), 409 –418. Gray, J. (Ed.), 1993. Benchmark Handbook Database Transaction Processing Systems, second ed. Morgan Kaufmann, San Francisco. Gray, J., 2006. Sort benchmark home page. http://sortbenchmark.org/ . Gray, J., Reuter, A., 1993. Transaction Processing: Concepts Techniques. Morgan Kaufmann, San Francisco. Gray, J., Siewiorek, D.P., 1991. High-availability computer systems. Computer 24 (9), 39–48. Gray, J., van Ingen, C., 2005. Empirical Measurements Disk Failure Rates Error Rates, MSR-TR-2005-166. Microsoft Research, Redmond, WA. Greenberg, A., Jain, N., Kandula, S., Kim, C., Lahiri, P., Maltz, D., Patel, P., Sengupta, S., 2009. VL2: scalable flexible data center network. In: Proceedings ACM SIG- COMM, August 17 –21, 2009, Barcelona, Spain. Grice, C., Kanellos, M., 2000. Cell phone industry crossroads: go high low? CNET News.technews.netscape.com/news/0-1004-201-2518386-0.html?tag ¼st.ne.1002.tgif.sf. Groe, J.B., Larson, L.E., 2000. CDMA Mobile Radio Design. Artech House, Boston.R-14 ■ReferencesGunther, K.D., 1981. Prevention deadlocks packet-switched data transport systems. IEEE Trans. Commun. 29 (4), 512 –524. Hagersten, E., Koster, M., 1998. WildFire: scalable path SMPs. In: Proceedings Fifth International Symposium High-Performance Computer Architecture, January 9–12, 1999, Orlando, FL. Hagersten, E., Landin, A., Haridi, S., 1992. DDM —a cache-only memory architecture. IEEE Comput. 25 (9), 44 –54. Hamacher, V.C., Vranesic, Z.G., Zaky, S.G., 1984. Computer Organization, second ed. McGraw-Hill, New York. Hameed, R., Qadeer, W., Wachs, M., Azizi, O., Solomatnikov, A., Lee, B.C., Richardson, S., Kozyrakis, C., Horowitz, M., 2010. Understanding sources ineffi- ciency general-purpose chips. ACM SIGARCH Comput. Architect. News 38 (3), 37–47. Hamilton, J., 2009. Data center networks way. In: Paper Presented Stanford Clean Slate CTO Summit, October 23, 2009. http://mvdirona.com/jrh/- TalksAndPapers/JamesHamilton_CleanSlateCTO2009.pdf . Hamilton, J., 2010. Cloud computing economies scale. In: Paper Presented AWS Workshop Genomics Cloud Computing, June 8, 2010, Seattle, WA. http:// mvdirona.com/jrh/TalksAndPapers/JamesHamilton_GenomicsCloud20100608.pdf . Hamilton, J., 2014. AWS Innovation Scale, AWS Re-invent conference. https://www. youtube.com/watch?v ¼JIQETrFC_SQ . Hamilton, J., 2015. Return Cloud. http://perspectives.mvdirona.com/2015/05/ the-return-to-the-cloud// . Hamilton, J., 2017. Many Data Centers Needed World-Wide. http://perspectives. mvdirona.com/2017/04/how-many-data-centers-needed-worldwide/ . Hammerstrom, D., 1990. VLSI architecture high-performance, low-cost, on-chip learning. In: IJCNN International Joint Conference Neural Networks. Handy, J., 1993. Cache Memory Book. Academic Press, Boston. Hauck, E.A., Dent, B.A., 1968. Burroughs ’B6500/B7500 stack mechanism. In: Proceedings AFIPS Spring Joint Computer Conference, April 30 –May 2, 1968, Atlantic City, NJ, pp. 245 –251. He, K., Zhang, X., Ren, S., Sun, J., 2016. Identity mappings deep residual networks. Also arXiv preprint arXiv:1603.05027. H e l ,R . ,A n g r n ,K . ,A r ,C . ,A n g ,M . ,B l n ,M . ,D ,A . ,D x ,P . , Gouldsberry, G., Hart, J., Horel, T., Hsu, W.-J., Kaku, J., Kim, C., Kim, S., K l ,F . ,K w n ,H . ,L ,R . ,M c n r e ,H . ,M e h ,A . ,M u r ,D . ,N g u e n ,S . , P ,Y . - P . ,P e l ,S . ,S h n ,K . ,T ,K . ,V h w n h h ,S . ,W u ,J . ,Y e e ,G . , You, H., 2000. Implementation third-generation SPARC V9 64-b microprocessor.In: ISSCC Digest Technical Papers, pp. 412 –413. Heinrich, J., 1993. MIPS R4000 User ’s Manual. Prentice Hall, Englewood Cliffs, NJ. Henly, M., McNutt, B., 1989. DASD I/O Characteristics: Comparison MVS VM, Tech. Rep. TR 02.1550 (May). IBM General Products Division, San Jose, CA. Hennessy, J., 1984. VLSI processor architecture. IEEE Trans. Comput. C-33 (11), 1221 –1246. Hennessy, J., 1985. VLSI RISC processors. VLSI Syst. Des. 6 (10), 22 –32. Hennessy, J., Jouppi, N., Baskett, F., Gill, J., 1981. MIPS: VLSI processor architecture. In: CMU Conference VLSI Systems Computations. Computer Science Press, Rockville, MD. Hewlett-Packard, 1994. PA-RISC 2.0 Architecture Reference Manual, third ed. Hewlett- Packard, Palo Alto, CA. Hewlett-Packard, 1998. HP ’s‘5NINES:5MINUTES ’Vision Extends Leadership Redefines High Availability Mission-Critical Environments. www.future. enterprisecomputing.hp.com/ia64/news/5nines_vision_pr.html .References ■R-15Hill, M.D., 1987. Aspects Cache Memory Instruction Buffer Performance (Ph.D. thesis). Tech. Rep. UCB/CSD 87/381. Computer Science Division, University California, Berkeley. Hill, M.D., 1988. case direct mapped caches. Computer 21 (12), 25 –40. Hill, M.D., 1998. Multiprocessors support simple memory consistency models. IEEE Comput. 31 (8), 28 –34. Hillis, W.D., 1985. Connection Multiprocessor. MIT Press, Cambridge, MA. Hillis, W.D., Steele, G.L., 1986. Data parallel algorithms. Commun. ACM 29 (12), 1170 –1183. Hinton, G., Sager, D., Upton, M., Boggs, D., Carmean, D., Kyker, A., Roussel, P., 2001. microarchitecture Pentium 4 processor. Intel Technol. J. Hintz, R.G., Tate, D.P., 1972. Control data STAR-100 processor design. In: Proceedings IEEE COMPCON, September 12 –14, 1972, San Francisco, pp. 1 –4. Hirata, H., Kimura, K., Nagamine, S., Mochizuki, Y., Nishimura, A., Nakase, Y., Nishizawa, T., 1992. elementary processor architecture simultaneous instruction issuing multiple threads. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 136 –145. Hitachi, 1997. SuperH RISC Engine SH7700 Series Programming Manual. Hitachi, Santa Clara, CA. www.halsp.hitachi.com/tech_prod/ . Ho, R., Mai, K.W., Horowitz, M.A., 2001. future wires. In: Proc. IEEE, 89. 4, pp. 490 –504. Hoagland, A.S., 1963. Digital Magnetic Recording. Wiley, New York. Hockney, R.W., Jesshope, C.R., 1988. Parallel Computers 2: Architectures, Programming Algorithms. Adam Hilger, Ltd., Bristol, England. Holland, J.H., 1959. universal computer capable executing arbitrary number subprograms simultaneously. Proc. East Joint Comput. Conf. 16, 108 –113. Holt, R.C., 1972. deadlock properties computer systems. ACM Comput. Surv. 4 (3), 179 –196. H€olzle, U., 2010. Brawny cores still beat wimpy cores, time. IEEE Micro 30, 4 (July/August). Hopkins, M., 2000. critical look IA-64: massive resources, massive ILP, deliver? Microprocessor Rep. February. Hord, R.M., 1982. Illiac-IV, First Supercomputer. Computer Science Press, Rockville, MD. Horel, T., Lauterbach, G., 1999. UltraSPARC-III: designing third-generation 64-bit perfor- mance. IEEE Micro 19 (3), 73 –85. Hospodor, A.D., Hoagland, A.S., et al., 1993. changing nature disk controllers. Proc. IEEE 81 (4), 586 –594. Hristea, C., Lenoski, D., Keen, J., 1997. Measuring memory hierarchy performance cache-coherent multiprocessors using micro benchmarks. In: Proceedings ACM/ IEEE Conference Supercomputing, November 16 –21, 1997, San Jose, CA. Hsu, P., 1994. Designing TFP microprocessor. IEEE Micro 18(2). Huang, M., Wu, D., Yu, C.H., Fang, Z., Interlandi, M., Condie, T., Cong, J., 2016. Program- ming runtime support blaze FPGA accelerator deployment datacenter scale. In: Proceedings Seventh ACM Symposium Cloud Computing. ACM, pp. 456 –469. Huck, J., et al., 2000. Introducing IA-64 Architecture. IEEE Micro 20 (5), 12 –23. Hughes, C.J., Kaul, P., Adve, S.V., Jain, R., Park, C., Srinivasan, J., 2001. Variability execution multimedia applications implications architecture. In: ProceedingsR-16 ■Referencesof 28th Annual International Symposium Computer Architecture (ISCA), June 30–July 4, 2001, Goteborg, Sweden, pp. 254 –265. Hwang, K., 1979. Computer Arithmetic: Principles, Architecture, Design. Wiley, New York. Hwang, K., 1993. Advanced Computer Architecture Parallel Programming. McGraw-Hill, New York. Hwu, W.-M., Patt, Y., 1986. HPSm, high performance restricted data flow architecture minimum functionality. In: Proceedings 13th Annual International Sympo- sium Computer Architecture (ISCA), June 2 –5, 1986, Tokyo, pp. 297 –307. Hwu, W.W., Mahlke, S.A., Chen, W.Y., Chang, P.P., Warter, N.J., Bringmann, R.A., Ouellette, R.O., Hank, R.E., Kiyohara, T., Haab, G.E., Holm, J.G., Lavery, D.M., 1993. superblock: effective technique VLIW superscalar compilation. J. Supercomput. 7 (1), 229 –248. Iandola, F., 2016. Exploring Design Space Deep Convolutional Neural Networks Large Scale (Ph.D. dissertation). UC Berkeley. IBM, 1982. Economic Value Rapid Response Time, GE20-0752-0. IBM, White Plains, NY, pp. 11 –82. IBM, 1990. IBM RISC System/6000 processor. IBM J. Res. Dev. 34(1). IBM, 1994. PowerPC Architecture. Morgan Kaufmann, San Francisco. IBM, 2005. Blue Gene. IBM J. Res. Dev. 49 (2/3) (Special issue). IEEE, 1985. IEEE standard binary floating-point arithmetic. SIGPLAN Notices 22 (2), 9–25. IEEE, 2005. Intel virtualization technology, computer. IEEE Comput. Soc. 38 (5), 48–56. IEEE 754-2008 Working Group, 2006. DRAFT Standard Floating-Point Arithmetic 754-2008, https://doi.org/10.1109/IEEESTD.2008.4610935 . Ienne, P., Cornu, T., Kuhn, G., 1996. Special-purpose digital hardware neural networks: architectural survey. J. VLSI Signal Process. Syst. Signal Image Video Technol. 13(1). Imprimis Product Specification, 97209 Sabre Disk Drive IPI-2 Interface 1.2 GB, Document No. 64402302, Imprimis, Dallas, TX. InfiniBand Trade Association, 2001. InfiniBand Architecture Specifications Release 1.0.a. www.infinibandta.org . Inoue, K., Ishihara, T., Murakami, K., 1999. Way-predicting set-associative cache high performance low energy consumption. In: Proc. 1999 International Symposium Low Power Electronics Design, ACM, pp. 273 –275. Intel, 2001. Using MMX Instructions Convert RGB YUV Color Conversion. cedar. intel.com/cgi-bin/ids.dll/content/content.jsp?cntKey ¼Legacy::irtm_AP548_9996& cntType ¼IDS_EDITORIAL . Internet Retailer, 2005. Gap launches new site —after two weeks downtime. Inter- net Retailer. http://www.internetretailer.com/2005/09/28/the-gap-launches-a-new-site- after-two-weeks-of-downtime . Jain, R., 1991. Art Computer Systems Performance Analysis: Techniques Experimental Design, Measurement, Simulation, Modeling. Wiley, New York. Jantsch, A., Tenhunen, H. (Eds.), 2003. Networks Chips. Kluwer Academic Publishers, Netherlands. Jimenez, D.A., Lin, C., 2001. Dynamic branch prediction perceptrons. In: Proceedings 7th International Symposium High-Performance Computer Architecture (HPCA '01). IEEE, Washington, DC, pp. 197 –206. Jimenez, D.A., Lin, C., 2002. Neural methods dynamic branch prediction. ACM Trans. Comput. Syst. 20 (4), 369 –397.References ■R-17Johnson, M., 1990. Superscalar Microprocessor Design. Prentice Hall, Englewood Cliffs, NJ. Jordan, H.F., 1983. Performance measurements HEP —a pipelined MIMD computer. In: Proceedings 10th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1982, Stockholm, Sweden, pp. 207 –212. Jordan, K.E., 1987. Performance comparison large-scale scientific processors: scalar main- frames, mainframes vector facilities, supercomputers. Computer 20 (3), 10 –23. Jouppi, N.P., 1990. Improving direct-mapped cache performance addition small fully-associative cache prefetch buffers. In: Proceedings 17th Annual Interna- tional Symposium Computer Architecture (ISCA), May 28 –31, 1990, Seattle, WA, pp. 364 –373. Jouppi,N.P.,1998.Retrospective:Improvingdirect-mappedcacheperformancebytheaddition small fully-associative cache prefetch buffers. In: 25 Years International Symposia Computer Architecture (Selected Papers). ACM, New York, pp. 71 –73. Jouppi, N., 2016. Google supercharges machine learning tasks TPU custom chip. https://cloudplatform.googleblog.com . Jouppi, N.P., Wall, D.W., 1989. Available instruction-level parallelism super-scalar superpipelined processors. In: Proceedings Third International Conference Archi- tectural Support Programming Languages Operating Systems (ASPLOS), April 3–6, 1989, Boston, pp. 272 –282. Jouppi, N.P., Wilton, S.J.E., 1994. Trade-offs two-level on-chip caching. In: Proceedings 21st Annual International Symposium Computer Architecture (ISCA), April 18 – 21, 1994, Chicago, pp. 34 –45. Jouppi, N., Young, C., Patil, N., Patterson, D., Agrawal, G., et al., 2017. Datacenter perfor- mance analysis matrix processing unit. In: 44th International Symposium Computer Architecture. Kaeli, D.R., Emma, P.G., 1991. Branch history table prediction moving target branches due subroutine returns. In: Proceedings 18th Annual International Symposium Computer Architecture (ISCA), May 27 –30, 1991, Toronto, Canada, pp. 34 –42. Kahan, W., 1968. 7094-II system support numerical analysis, SHARE Secretarial Distribution SSD-159. Department Computer Science, University Toronto. Kahan, J., 1990. advantage 8087 ’s stack, unpublished course notes. Computer Science Division, University California, Berkeley. Kahaner, D.K., 1988. Benchmarks ‘real’programs. SIAM News. November. Kahn, R.E., 1972. Resource-sharing computer communication networks. Proc. IEEE 60 (11), 1397 –1407. Kane, G., 1986. MIPS R2000 RISC Architecture. Prentice Hall, Englewood Cliffs, NJ. Kane, G., 1996. PA-RISC 2.0 Architecture. Prentice Hall, Upper Saddle River, NJ.Kane, G., Heinrich, J., 1992. MIPS RISC Architecture. Prentice Hall, Englewood Cliffs, NJ. Kanev, S., Darago, J.P., Hazelwood, K., Ranganathan, P., Moseley, T., Wei, G.Y., Brooks, D., 2015. Profiling warehouse-scale computer. In: ACM/IEEE 42nd Annual International Symposium Computer Architecture (ISCA). Karpathy, A., et al., 2014. Large-scale video classification convolutional neural networks. CVPR. Katz, R.H., Patterson, D.A., Gibson, G.A., 1989. Disk system architectures high perfor- mance computing. Proc. IEEE 77 (12), 1842 –1858. Keckler, S.W., Dally, W.J., 1992. Processor coupling: integrating compile time runtime scheduling parallelism. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 202 –213. Keller, R.M., 1975. Look-ahead processors. ACM Comput. Surv. 7 (4), 177 –195.R-18 ■ReferencesKeltcher, C.N., McGrath, K.J., Ahmed, A., Conway, P., 2003. AMD Opteron processor multiprocessor servers. IEEE Micro 23 (2), 66 –76. Kembel, R., 2000. Fibre channel: comprehensive introduction. Internet Week. April. Kermani, P., Kleinrock, L., 1979. Virtual cut-through: new computer communication switching technique. Comput. Netw. 3, 267 –286. Kessler, R., 1999. Alpha 21264 microprocessor. IEEE Micro 19 (2), 24 –36. Kilburn, T., Edwards, D.B.G., Lanigan, M.J., Sumner, F.H., 1962. One-level storage sys- tem. IRE Trans. Electron. Comput. EC-11, 223 –235. Also appears Siewiorek, D.P., Bell, C.G., Newell, A. 1982. Computer Structures: Principles Examples. McGraw- Hill, New York. pp. 135 –148. Killian, E., 1991. MIPS R4000 technical overview –64 bits/100 MHz bust. In: Hot Chips III Symposium Record, August 26 –27, 1991, Stanford University, Palo Alto, CA. pp. 1.6 –1.19. Kim, M.Y., 1986. Synchronized disk interleaving. IEEE Trans. Comput. 35 (11), 978–988. Kim, K., 2005. Technology sub-50nm DRAM NAND flash manufacturing. In: Elec- tron Devices Meeting Technical Digest (December), pp. 323 –326. Kissell, K.D., 1997. MIPS16: High-density embedded market. In: Proceedings Real Time Systems ’97, June 15, 1997, Las Vegas, Nev. www.sgi.com/MIPS/arch/ MIPS16/MIPS16.whitepaper.pdf . Kitagawa, K., Tagaya, S., Hagihara, Y., Kanoh, Y., 2003. hardware overview SX-6 SX-7 supercomputer. NEC Res. Dev. J. 44 (1), 2 –7. Knuth, D., 1981. second ed. Art Computer Programming, vol. II. Addison-Wesley, Reading, MA. Kogge, P.M., 1981. Architecture Pipelined Computers. McGraw-Hill, New York. Kohn, L., Fu, S.-W., 1989. 1,000,000 transistor microprocessor. In: Proceedings IEEE International Symposium Solid State Circuits (ISSCC), February 15 –17, 1989, New York, pp. 54 –55. Kohn, L., Margulis, N., 1989. Introducing Intel i860 64-Bit Microprocessor. IEEE Micro 9 (4), 15 –30. Kontothanassis, L., Hunt, G., Stets, R., Hardavellas, N., Cierniak, M., Parthasarathy, S., Meira, W., Dwarkadas, S., Scott, M., 1997. VM-based shared memory low-latency, remote-memory-access networks. In: Proceedings 24th Annual International Symposium Computer Architecture (ISCA), June 2 –4, 1997, Denver, CO. Koren, I., 1989. Computer Arithmetic Algorithms. Prentice Hall, Englewood Cliffs, NJ. Kozyrakis, C., 2000. Vector IRAM: media-oriented vector processor embedded DRAM. In: Paper Presented Hot Chips 12, August 13 –15, 2000, Palo Alto, CA, pp. 13 –15. Kozyrakis, C., Patterson, D., 2002. Vector vs. superscalar VLIW architectures embedded multimedia benchmarks. In: Proceedings 35th Annual International Symposium Microarchitecture (MICRO-35), November 18 –22, 2002, Istanbul, Turkey. Krizhevsky, A., Sutskever, I., Hinton, G., 2012. Imagenet classification deep convolu- tional neural networks. Adv. Neural Inf. Process. Syst. Kroft, D., 1981. Lockup-free instruction fetch/prefetch cache organization. In: Proceedings Eighth Annual International Symposium Computer Architecture (ISCA), May 12–14, 1981, Minneapolis, MN, pp. 81 –87. Kroft, D., 1998. Retrospective: lockup-free instruction fetch/prefetch cache organization. In: 25 Years International Symposia Computer Architecture (Selected Papers), ACM, New York, pp. 20 –21.References ■R-19Kuck, D., Budnik, P.P., Chen, S.-C., Lawrie, D.H., Towle, R.A., Strebendt, R.E., Davis Jr., E.W., Han, J., Kraska, P.W., Muraoka, Y., 1974. Measurements parallelism ordinary FORTRAN programs. Computer 7 (1), 37 –46. Kuhn, D.R., 1997. Sources failure public switched telephone network. IEEE Comput. 30 (4), 31 –36. Kumar, A., 1997. HP PA-8000 RISC CPU. IEEE Micro 17 (2), 27 –32. Kung, H.T., Leiserson, C.E., 1980. Algorithms VLSI processor arrays. Introduction VLSI systems. Kunimatsu, A., Ide, N., Sato, T., Endo, Y., Murakami, H., Kamei, T., Hirano, M., Ishihara, F., Tago, H., Oka, M., Ohba, A., Yutaka, T., Okada, T., Suzuoki, M., 2000. Vector unit architecture emotion synthesis. IEEE Micro 20 (2), 40 –47. Kunkel, S.R., Smith, J.E., 1986. Optimal pipelining supercomputers. In: Proceedings 13th Annual International Symposium Computer Architecture (ISCA), June 2 –5, 1986, Tokyo, pp. 404 –414. Kurose, J.F., Ross, K.W., 2001. Computer Networking: Top-Down Approach Featuring Internet. Addison-Wesley, Boston. Kuskin, J., Ofelt, D., Heinrich, M., Heinlein, J., Simoni, R., Gharachorloo, K., Chapin, J., Nakahira, D., Baxter, J., Horowitz, M., Gupta, A., Rosenblum, M., Hennessy, J.L., 1994. Stanford FLASH multiprocessor. In: Proceedings 21st Annual International Symposium Computer Architecture (ISCA), April 18 –21, 1994, Chicago. Lam, M., 1988. Software pipelining: effective scheduling technique VLIW proces- sors. In: SIGPLAN Conference Programming Language Design Implementa- tion, June 22 –24, 1988, Atlanta, GA, pp. 318 –328. Lam, M.S., Wilson, R.P., 1992. Limits control flow parallelism. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19–21, 1992, Gold Coast, Australia, pp. 46 –57. Lam, M.S., Rothberg, E.E., Wolf, M.E., 1991. cache performance optimizations blocked algorithms. In: Proceedings Fourth International Conference Architec- tural Support Programming Languages Operating Systems (ASPLOS), April 8–11, 1991, Santa Clara, CA. (SIGPLAN Notices 26:4 (April), 63 –74). Lambright, D., 2000. Experiences measuring reliability cache-based storage system. In: Proceedings First Workshop Industrial Experiences Systems Software (WIESS 2000), Co-Located 4th Symposium Operating Systems Design Implementation (OSDI), October 22, 2000, San Diego, CA. Lamport, L., 1979. make multiprocessor computer correctly executes multi- process programs. IEEE Trans. Comput. C-28 (9), 241 –248. Landstrom, B., 2014. Cost Downtime. http://www.interxion.com/blogs/2014/07/the- cost-of-downtime/ . Lang, W., Patel, J.M., Shankar, S., 2010. Wimpy node clusters: non-wimpy workloads? In: Proceedings Sixth International Workshop Data Management New Hardware (DaMoN), June 7, Indianapolis, IN. Laprie, J.-C., 1985. Dependable computing fault tolerance: concepts terminology. In: Proceedings 15th Annual International Symposium Fault-Tolerant Computing, June 19 –21, 1985, Ann Arbor, Mich, pp. 2 –11. Larabel, M., 2016. Google Looks Open StreamExecutor Make GPGPU Program- ming Easier. Phoronix, March 10. https://www.phoronix.com/ . Larson, E.R., 1973. Findings fact, conclusions law, order judgment, File No. 4-67, Civ. 138, Honeywell v. Sperry-Rand Illinois Scientific Develop- ment, U.S. District Court State Minnesota, Fourth Division (October 19). Laudon, J., Lenoski, D., 1997. SGI Origin: ccNUMA highly scalable server. In: Proceedings 24th Annual International Symposium Computer Architecture (ISCA), June 2 –4, 1997, Denver, CO, pp. 241 –251.R-20 ■ReferencesLaudon, J., Gupta, A., Horowitz, M., 1994. Interleaving: multithreading technique target- ing multiprocessors workstations. In: Proceedings Sixth International Confer- ence Architectural Support Programming Languages Operating Systems (ASPLOS), October 4 –7, San Jose, CA, pp. 308 –318. Lauterbach, G., Horel, T., 1999. UltraSPARC-III: designing third generation 64-bit perfor- mance. IEEE Micro 19, 3 (May/June). Lazowska, E.D., Zahorjan, J., Graham, G.S., Sevcik, K.C., 1984. Quantitative System Performance: Computer System Analysis Using Queueing Network Models. Prentice Hall, Englewood Cliffs, NJ (Although print, available online www.cs. washington.edu/homes/lazowska/qsp/ ). Lebeck, A.R., Wood, D.A., 1994. Cache profiling SPEC benchmarks: case study. Computer 27 (10), 15 –26. Lee, R., 1989. Precision architecture. Computer 22 (1), 78 –91. Lee, W.V., et al., 2010. Debunking 100X GPU vs. CPU myth: evaluation throughput computing CPU GPU. In: Proceedings 37th Annual Interna- tional Symposium Computer Architecture (ISCA), June 19 –23, 2010, Saint- Malo, France. Lee, Y., Waterman, A., Cook, H., Zimmer, B., Keller, B., Puggelli, A., Kwak, J., Jevtic, R., Bailey, S., Blagojevic, M., Chiu, P.-F., Avizienis, R., Richards, B., Bachrach, J., Patterson, D., Alon, E., Nikolic, B., Asanovic, K., 2016. agile approach building RISC-V microprocessors. IEEE Micro 36 (2), 8 –20. Leighton, F.T., 1992. Introduction Parallel Algorithms Architectures: Arrays, Trees, Hypercubes. Morgan Kaufmann, San Francisco. Leiner, A.L., 1954. System specifications DYSEAC. J. ACM 1 (2), 57 –81. Leiner, A.L., Alexander, S.N., 1954. System organization DYSEAC. IRE Trans. Electron. Comput. 3 (1), 1 –10. Leiserson, C.E., 1985. Fat trees: universal networks hardware-efficient supercomputing. IEEE Trans. Comput. C-34 (10), 892 –901. Lenoski, D., Laudon, J., Gharachorloo, K., Gupta, A., Hennessy, J.L., 1990. Stanford DASH multiprocessor. In: Proceedings 17th Annual International Symposium Computer Architecture (ISCA), May 28 –31, 1990, Seattle, WA, pp. 148 –159. Lenoski, D., Laudon, J., Gharachorloo, K., Weber, W.-D., Gupta, A., Hennessy, J.L., Horowitz, M.A., Lam, M., 1992. Stanford DASH multiprocessor. IEEE Comput. 25 (3), 63 –79. Levy, H., Eckhouse, R., 1989. Computer Programming Architecture: VAX. Digital Press, Boston. Lewis-Kraus, G., 2016. Great A.I. Awakening. New York Times Magazine.. Li, K., 1988. IVY: shared virtual memory system parallel computing. In: Proceedings 1988 International Conference Parallel Processing. Pennsylvania State University Press, University Park, PA. Li, S., Chen, K., Brockman, J.B., Jouppi, N., 2011. Performance Impacts Non-blocking Caches Out-of-order Processors. HP Labs Tech Report HPL-2011-65 (full text avail- able http://Library.hp.com/techpubs/2011/Hpl-2011-65.html) . Lim, K., Ranganathan, P., Chang, J., Patel, C., Mudge, T., Reinhardt, S., 2008. Understand- ing designing new system architectures emerging warehouse-computing environments. In: Proceedings 35th Annual International Symposium Computer Architecture (ISCA), June 21 –25, 2008, Beijing, China. Lincoln, N.R., 1982. Technology design trade offs creation modern super- computer. IEEE Trans. Comput. C-31 (5), 363 –376. Lindholm, T., Yellin, F., 1999. Java Virtual Machine Specification, 2nd ed. Addi- son-Wesley, Reading, (Also available online java.sun.com/docs/books/ vmspec/).References ■R-21Lipasti, M.H., Shen, J.P., 1996. Exceeding dataflow limit via value prediction. In: Proceedings 29th International Symposium Microarchitecture, December 2–4, 1996, Paris, France. Lipasti, M.H., Wilkerson, C.B., Shen, J.P., 1996. Value locality load value prediction. In: Proceedings Seventh Conference Architectural Support Programming Lan-guages Operating Systems (ASPLOS), October 1 –5, 1996, Cambridge, MA, pp. 138 –147. Liptay, J.S., 1968. Structural aspects System/360 Model 85, Part II: cache. IBM Syst. J. 7 (1), 15 –21. Lo, J., Eggers, S., Emer, J., Levy, H., Stamm, R., Tullsen, D., 1997. Converting thread-level parallelism instruction-level parallelism via simultaneous multithreading. ACM Trans. Comput. Syst. 15 (2), 322 –354. Lo, J., Barroso, L., Eggers, S., Gharachorloo, K., Levy, H., Parekh, S., 1998. analysis database workload performance simultaneous multithreaded processors. In: Proceedings 25th Annual International Symposium Computer Architecture (ISCA), July 3 –14, 1998, Barcelona, Spain, pp. 39 –50. Lo, D., Cheng, L., Govindaraju, R., Barroso, L.A., Kozyrakis, C., 2014. Towards energy proportionality large-scale latency-critical workloads. In: ACM/IEEE 41st Annual International Symposium Computer Architecture (ISCA). Loh, G.H., Hill, M.D., 2011. Efficiently enabling conventional block sizes large die-stacked DRAM caches. In: Proc. 44th Annual IEEE/ACM International Symposium Microarchitecture, ACM, pp. 454 –464. Lovett, T., Thakkar, S., 1988. symmetry multiprocessor system. In: Proceedings 1988 International Conference Parallel Processing, University Park, PA, pp. 303 –310. Lubeck, O., Moore, J., Mendez, R., 1985. benchmark comparison three supercom- puters: Fujitsu VP-200, Hitachi S810/20, Cray X-MP/2. Computer 18 (12), 10–24. Luk, C.-K., Mowry, T.C., 1999. Automatic compiler-inserted prefetching pointer-based applications. IEEE Trans. Comput. 48 (2), 134 –141. Lunde, A., 1977. Empirical evaluation features instruction set processor archi- tecture. Commun. ACM 20 (3), 143 –152. Luszczek, P., Dongarra, J.J., Koester, D., Rabenseifner, R., Lucas, B., Kepner, J., McCalpin, J., Bailey, D., Takahashi, D., 2005. Introduction HPC challenge benchmark suite. Lawrence Berkeley National Laboratory, Paper LBNL-57493 (April 25), repositories. cdlib.org/lbnl/LBNL-57493. Maberly, N.C., 1966. Mastering Speed Reading. New American Library, New York. Magenheimer, D.J., Peters, L., Pettis, K.W., Zuras, D., 1988. Integer multiplication division HP precision architecture. IEEE Trans. Comput. 37 (8), 980 –990. Mahlke, S.A., Chen, W.Y., Hwu, W.-M., Rau, B.R., Schlansker, M.S., 1992. Sentinel scheduling VLIW superscalar processors. In: Proceedings Fifth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 12 –15, 1992, Boston, pp. 238 –247. Mahlke, S.A., Hank, R.E., McCormick, J.E., August, D.I., Hwu, W.W., 1995. comparison full partial predicated execution support ILP processors. In: Proceedings 22nd Annual International Symposium Computer Architecture (ISCA), June 22 –24, 1995, Santa Margherita, Italy, pp. 138 –149. Major, J.B., 1989. queuing models within grasp unwashed? In: Proceedings International Conference Management Performance Evaluation Computer Systems, December 11 –15, 1989, Reno, Nev, pp. 831 –839. Markstein, P.W., 1990. Computation elementary functions IBM RISC System/ 6000 processor. IBM J. Res. Dev. 34 (1), 111 –119.R-22 ■ReferencesMathis, H.M., Mercias, A.E., McCalpin, J.D., Eickemeyer, R.J., Kunkel, S.R., 2005. Char- acterization multithreading (SMT) efficiency Power5. IBM J. Res. Dev. 49 (4/5), 555 –564. McCalpin, J., 2005. STREAM: Sustainable Memory Bandwidth High Performance Computers. www.cs.virginia.edu/stream/ . McCalpin, J., Bailey, D., Takahashi, D., 2005. Introduction HPC Challenge Bench- mark Suite, Paper LBNL-57493. Lawrence Berkeley National Laboratory, University California, Berkeley, repositories.cdlib.org/lbnl/LBNL-57493. McCormick, J., Knies, A., 2002. brief analysis SPEC CPU2000 benchmarks Intel Itanium 2 processor. In: Paper Presented Hot Chips 14, August 18 –20, 2002, Stanford University, Palo Alto, CA. McFarling, S., 1989. Program optimization instruction caches. In: Proceedings Third International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 3 –6, 1989, Boston, pp. 183 –191. McFarling, S., 1993. Combining Branch Predictors, WRL Technical Note TN-36, Digital Western Research Laboratory, Palo Alto, CA. McFarling, S., Hennessy, J., 1986. Reducing cost branches. In: Proceedings 13th Annual International Symposium Computer Architecture (ISCA), June 2 –5, 1986, Tokyo, pp. 396 –403. McGhan, H., ’Connor, M., 1998. PicoJava: direct execution engine Java bytecode. Computer 31 (10), 22 –30. McKeeman, W.M., 1967. Language directed computer design. In: Proceedings AFIPS Fall Joint Computer Conference, November 14 –16, 1967, Washington, DC, pp. 413 –417. McMahon, F.M., 1986. Livermore FORTRAN Kernels: Computer Test Numerical Performance Range, Tech. Rep. UCRL-55745. Lawrence Livermore National Labora- tory, University California, Livermore. McNairy, C., Soltis, D., 2003. Itanium 2 processor microarchitecture. IEEE Micro 23 (2), 44–55. Mead, C., Conway, L., 1980. Introduction VLSI Systems. Addison-Wesley, Reading, MA. Mellor-Crummey, J.M., Scott, M.L., 1991. Algorithms scalable synchronization shared-memory multiprocessors. ACM Trans. Comput. Syst. 9 (1), 21 –65. Menabrea, L.F., 1842. Sketch analytical engine invented Charles Babbage. Bibliothèque Universelle de Genève. 82. Menon, A., Renato Santos, J., Turner, Y., Janakiraman, G., Zwaenepoel, W., 2005. Diag- nosing performance overheads xen virtual machine environment. In: Proceedings First ACM/USENIX International Conference Virtual Execution Environments, June 11 –12, 2005, Chicago, pp. 13 –23. Merlin, P.M., Schweitzer, P.J., 1980. Deadlock avoidance store-and-forward networks. Part I. Store-and-forward deadlock. IEEE Trans. Commun. 28 (3), 345 –354. Metcalfe, R.M., 1993. Computer/network interface design: lessons Arpanet Ether- net. IEEE J. Sel. Area. Commun. 11 (2), 173 –180. Metcalfe, R.M., Boggs, D.R., 1976. Ethernet: distributed packet switching local computer networks. Commun. ACM 19 (7), 395 –404. Metropolis, N., Howlett, J., Rota, G.C. (Eds.), 1980. History Computing Twentieth Century. Academic Press, New York. Meyer, R.A., Seawright, L.H., 1970. virtual machine time sharing system. IBM Syst. J. 9 (3), 199 –218. Meyers, G.J., 1978. evaluation expressions storage-to-storage architecture. Comput. Architect. News 7 (3), 20 –23. Meyers, G.J., 1982. Advances Computer Architecture, second ed. Wiley, New York. Micron, 2004. Calculating Memory System Power DDR2. http://download.micron.com/ pdf/pubs/designline/dl1Q04.pdf .References ■R-23Micron, 2006. Micron System-Power Calculator. http://www.micron.com/-systemcalc . MIPS, 1997. MIPS16 Application Specific Extension Product Description. www.sgi.com/ MIPS/arch/MIPS16/mips16.pdf . Miranker, G.S., Rubenstein, J., Sanguinetti, J., 1988. Squeezing Cray-class supercomputer single-user package. In: Proceedings IEEE COMPCON, February 29 –March 4, 1988, San Francisco, pp. 452 –456. Mitchell, D., 1989. transputer: time now. Comput. Des. (RISC suppl.) 40 –41. Mitsubishi, 1996. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family Software Manual. Mitsubishi, Cypress, CA. Miura, K., Uchida, K., 1983. FACOM vector processing system: VP100/200. In: Proceedings NATO Advanced Research Workshop High-Speed Computing, June 20 –22, 1983, J €ulich, West Germany. Also appears Hwang, K. (Ed.), 1984. Superprocessors: Design Applications. IEEE (August), pp. 59 –73. Miya, E.N., 1985. Multiprocessor/distributed processing bibliography. Comput. Architect. News 13 (1), 27 –29. Money, M.S.N., 2005. Amazon Shares Tumble Rally Fizzles. http://moneycentral. msn.com/content/CNBCTV/Articles/Dispatches/P133695.asp . Montoye, R.K., Hokenek, E., Runyon, S.L., 1990. Design IBM RISC System/6000 floating-point execution. IBM J. Res. Dev. 34 (1), 59 –70. Moore, G.E., 1965. Cramming components onto integrated circuits. Electronics 38 (8), 114–117. Moore, B., Padegs, A., Smith, R., Bucholz, W., 1987. Concepts System/370 vector architecture. In: 14th Annual International Symposium Computer Architecture (ISCA), June 2 –5, 1987, Pittsburgh, PA, pp. 282 –292. Morgan, T., 2014. rare peek massive scale AWS. Enterprise Tech. https://www. enterprisetech.com/2014/11/14/rare-peek-massive-scale-aws/ . Morgan, T., 2016. long AWS keep climbing steep growth curve? https://www. nextplatform.com/2016/02/01/how-long-can-aws-keep-climbingits-steep-growth-curve/. Morse, S., Ravenal, B., Mazor, S., Pohlman, W., 1980. Intel microprocessors —8080 8086. Computer 13, 10. Moshovos, A., Sohi, G.S., 1997. Streamlining inter-operation memory communication via data dependence prediction. In: Proceedings 30th Annual International Symposium Microarchitecture, December 1 –3, Research Triangle Park, NC, pp. 235 –245. Moshovos, A., Breach, S., Vijaykumar, T.N., Sohi, G.S., 1997. Dynamic speculation synchronization data dependences. In: 24th Annual International Symposium Computer Architecture (ISCA), June 2 –4, 1997, Denver, CO. Moussouris, J., Crudele, L., Freitas, D., Hansen, C., Hudson, E., Przybylski, S., Riordan, T., Rowen, C., 1986. CMOS RISC processor integrated system functions. In: Proceedings IEEE COMPCON, March 3 –6, 1986, San Francisco, p. 191. Mowry, T.C., Lam, S., Gupta, A., 1992. Design evaluation compiler algorithm prefetching. In: Proceedings Fifth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 12 –15, 1992, Boston (SIGPLAN Notices 27:9 (September), pp. 62 –73. Muchnick, S.S., 1988. Optimizing compilers SPARC. Sun Technol. 1 (3), 64 –77. Mueller, M., Alves, L.C., Fischer, W., Fair, M.L., Modi, I., 1999. RAS strategy IBM S/ 390 G5 G6. IBM J. Res. Dev. 43 (5-6), 875 –888. Mukherjee, S.S., Weaver, C., Emer, J.S., Reinhardt, S.K., Austin, T.M., 2003. Measuring architectural vulnerability factors. IEEE Micro 23 (6), 70 –75. Murphy, B., Gent, T., 1995. Measuring system software reliability using automated data collection process. Qual. Reliab. Eng. Int. 11 (5), 341 –353.R-24 ■ReferencesMyer, T.H., Sutherland, I.E., 1968. design display processors. Commun. ACM 11 (6), 410 –414. Narayanan, D., Thereska, E., Donnelly, A., Elnikety, S., Rowstron, A., 2009. Migrating server storage SSDs: analysis trade-offs. In: Proceedings 4th ACM European Conference Computer Systems, April 1 –3, 2009, Nuremberg, Germany. National Research Council, 1997. Evolution Untethered Communications. Computer Science Telecommunications Board, National Academy Press, Washington, DC. National Storage Industry Consortium, 1998. Tape Roadmap. www.nsic.org . Nelson, V.P., 1990. Fault-tolerant computing: fundamental concepts. Computer 23 (7), 19 –25. Ngai, T.-F., Irwin, M.J., 1985. Regular, area-time efficient carry-lookahead adders. In: Proceedings Seventh IEEE Symposium Computer Arithmetic, June 4 –6, 1985, University Illinois, Urbana, pp. 9 –15. Nicolau, A., Fisher, J.A., 1984. Measuring parallelism available long instruction word architectures. IEEE Trans. Comput. C33 (11), 968 –976. Nielsen, M., 2016. Neural Networks Deep Learning. http://neuralnetwork sanddeeplearning.com/ . Nikhil, R.S., Papadopoulos, G.M., Arvind, 1992. *T: multithreaded massively parallel architecture. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 156 –167. Noordergraaf, L., van der Pas, R., 1999. Performance experiences Sun ’s WildFire prototype. In: Proceedings ACM/IEEE Conference Supercomputing, November 13–19, 1999, Portland, Ore. Nvidia, 2016. Tesla GPU Accelerators Servers. http:// www.nvidia.com/object/tesla- servers.html . Nyberg, C.R., Barclay, T., Cvetanovic, Z., Gray, J., Lomet, D., 1994. AlphaSort: RISC machine sort. In: Proceedings ACM SIGMOD, May 24 –27, 1994, Minneapolis, Minn. Oka, M., Suzuoki, M., 1999. Designing programming emotion engine. IEEE Micro 19 (6), 20 –28. Okada, S., Okada, S., Matsuda, Y., Yamada, T., Kobayashi, A., 1999. System chip digital still camera. IEEE Trans. Consum. Electron. 45 (3), 584 –590. Oliker, L., Canning, A., Carter, J., Shalf, J., Ethier, S., 2004. Scientific computations modern parallel vector systems. In: Proceedings ACM/IEEE Conference Supercomputing, November 6 –12, 2004, Pittsburgh, Penn, p. 10. Olofsson, A., 2011. Debunking myth $100M ASIC. EE Times. http://www. eetimes.com/author.asp?section_id ¼36&doc_id ¼1266014 . Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015a. Accel- erating deep convolutional neural networks using specialized hardware. Microsoft Research Whitepaper. https://www.microsoft.com/en-us/res earch/publication/accelerating-deep- convolutional-neural-network s-using-specialized-hardware/ . Ovtcharov, K., Ruwase, O., Kim, J.Y., Fowers, J., Strauss, K., Chung, E.S., 2015b. Toward accelerating deep learning scale using specialized hardware datacenter. In: 2015 IEEE Hot Chips 27 Symposium. Pabst, T., 2000. Performance Showdown 133 MHz FSB —The Best Platform Coppermine. www6.tomshardware.com/mainboard/00q1/000302/ . Padua, D., Wolfe, M., 1986. Advanced compiler optimizations supercomputers. Commun. ACM 29 (12), 1184 –1201. Palacharla, S., Kessler, R.E., 1994. Evaluating stream buffers secondary cache replace- ment. In: Proceedings 21st Annual International Symposium Computer Architec- ture (ISCA), April 18 –21, 1994, Chicago, pp. 24 –33. Palmer, J., Morse, S., 1984. 8087 Primer. John Wiley & Sons, New York, p. 93.References ■R-25Pan, S.-T., So, K., Rameh, J.T., 1992. Improving accuracy dynamic branch prediction using branch correlation. In: Proceedings Fifth International Conference Architec- tural Support Programming Languages Operating Systems (ASPLOS), October 12–15, 1992, Boston, pp. 76 –84. Partridge, C., 1994. Gigabit Networking. Addison-Wesley, Reading, MA.Patterson, D., 1985. Reduced instruction set computers. Commun. ACM 28 (1), 8 –21. Patterson, D., 2004. Latency lags bandwidth. Commun. ACM 47 (10), 71 –75. Patterson, D.A., Ditzel, D.R., 1980. case reduced instruction set computer. ACM SIGARCH Comput. Architect. News 8 (6), 25 –33. Patterson, D.A., Hennessy, J.L., 2004. Computer Organization Design: Hardware/ Software Interface, third ed. Morgan Kaufmann, San Francisco. Patterson, D., Nikoli /C19c, B., 7/25/2015, Agile Design Hardware, Parts I, II, III. EE Times, http://www.eetimes.com/author.asp?doc_id ¼1327239 . Patterson, D.A., Garrison, P., Hill, M., Lioupis, D., Nyberg, C., Sippel, T., Van Dyke, K., 1983. Architecture VLSI instruction cache RISC. In: 10th Annual International Conference Computer Architecture Conf. Proc., June 13 –16, 1983, Stockholm, Sweden, pp. 108 –116. Patterson, D.A., Gibson, G.A., Katz, R.H., 1987. Case Redundant Arrays Inexpen- sive Disks (RAID), Tech. Rep. UCB/CSD 87/391, University California, Berkeley. Also appeared Proc. ACM SIGMOD, June 1 –3, 1988, Chicago, pp. 109 –116. Pavan, P., Bez, R., Olivo, P., Zanoni, E., 1997. Flash memory cells —an overview. Proc. IEEE 85 (8), 1248 –1271. Peh, L.S., Dally, W.J., 2001. delay model speculative architecture pipe-lined routers. In: Proceedings 7th International Symposium High-Performance Com- puter Architecture, January 22 –24, 2001, Monterrey, Mexico. Peng, V., Samudrala, S., Gavrielov, M., 1987. implementation shifters, multi- pliers, dividers VLSI floating point units. In: Proceedings 8th IEEE Sympo- sium Computer Arithmetic, May 19 –21, 1987, Como, Italy, pp. 95 –102. Pfister, G.F., 1998. Search Clusters, second ed. Prentice Hall, Upper Saddle River, NJ. Pfister, G.F., Brantley, W.C., George, D.A., Harvey, S.L., Kleinfekder, W.J., McAuliffe, K.P., Melton, E.A., Norton, V.A., Weiss, J., 1985. IBM research parallel processor prototype (RP3): introduction architecture. In: Proceedings 12th Annual International Symposium Computer Architecture (ISCA), June 17–19, 1985, Boston, MA, pp. 764 –771. Pinheiro, E., Weber, W.D., Barroso, L.A., 2007. Failure trends large disk drive popu- lation. In: Proceedings 5th USENIX Conference File Storage Technologies (FAST ’07), February 13 –16, 2007, San Jose, CA. Pinkston, T.M., 2004. Deadlock characterization resolution interconnection net- works. In: Zhu, M.C., Fanti, M.P. (Eds.), Deadlock Resolution Computer-Integrated Systems. CRC Press, Boca Raton, FL, pp. 445 –492. Pinkston, T.M., Shin, J., 2005. Trends toward on-chip networked microsystems. Int. J. High Perform. Comput. Netw. 3 (1), 3 –18. Pinkston, T.M., Warnakulasuriya, S., 1997. deadlocks interconnection networks. In: 24th Annual International Symposium Computer Architecture (ISCA), June 2–4, 1997, Denver, CO. Pinkston, T.M., Benner, A., Krause, M., Robinson, I., Sterling, T., 2003. InfiniBand: ‘de facto ’future standard system local area networks scalable replacement PCI buses? ”. Cluster Comput. 6 (2), 95 –104 (Special issue communication archi- tecture clusters). Postiff, M.A., Greene, D.A., Tyson, G.S., Mudge, T.N., 1999. limits instruction level parallelism SPEC95 applications. Comput. Architect. News 27 (1), 31 –40.R-26 ■ReferencesPrabhakar, R., Koeplinger, D., Brown, K.J., Lee, H., De Sa, C., Kozyrakis, C., Olukotun, K., 2016. Generating configurable hardware parallel patterns. In: Proceedings Twenty-First International Conference Architectural Support Programming Lan- guages Operating Systems. ACM, pp. 651 –665. Prakash, T.K., Peng, L., 2008. Performance characterization spec cpu2006 benchmarks intel core 2 duo processor. ISAST Trans. Comput. Softw. Eng. 2 (1), 36 –41. Przybylski, S.A., 1990. Cache Design: Performance-Directed Approach. Morgan Kauf- mann, San Francisco. Przybylski, S.A., Horowitz, M., Hennessy, J.L., 1988. Performance trade-offs cache design. In: 15th Annual International Symposium Computer Architecture, May 30–June 2, 1988, Honolulu, Hawaii, pp. 290 –298. Puente, V., Beivide, R., Gregorio, J.A., Prellezo, J.M., Duato, J., Izu, C., 1999. Adaptive bubble router: design improve performance torus networks. In: Proceedings 28th International Conference Parallel Processing, September 21 –24, 1999, Aizu-Wakamatsu, Fukushima, Japan. Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2014. reconfigurable fabric accelerating large- scale datacenter services. In: 41st International Symposium Computer Architecture. Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2015. reconfigurable fabric accel- erating large-scale datacenter services. IEEE Micro. 35(3). Putnam, A., Caulfield, A.M., Chung, E.S., Chiou, D., Constantinides, K., Demme, J., Esmaeilzadeh, H., Fowers, J., Gopal, G.P., Gray, J., Haselman, M., Hauck, S., Heil, S., Hormati, A., Kim, J.-Y., Lanka, S., Larus, J., Peterson, E., Pope, S., Smith, A., Thong, J., Xiao, P.Y., Burger, D., 2016. reconfigurable fabric accelerating large-scale datacenter services. Commun. ACM. 59 (11), 114 –122. Qadeer, W., Hameed, R., Shacham, O., Venkatesan, P., Kozyrakis, C., Horowitz, M.A., 2015. Convolution engine: balancing efficiency & flexibility specialized computing. Commun. ACM 58(4). Qureshi, M.K., Loh, G.H., 2012. Fundamental latency trade-off architecting dram caches: Outperforming impractical sram-tags simple practical design. In: Proc. 2012 45th Annual IEEE/ACM International Symposium Microarchitecture, IEEE Com- puter Society, pp. 235 –246. Radin, G., 1982. 801 minicomputer. In: Proceedings Symposium Architectural Support Programming Languages Operating Systems (ASPLOS), March 1–3, 1982, Palo Alto, CA, pp. 39 –47. Ragan-Kelley, J., Barnes, C., Adams, A., Paris, S., Durand, F., Amarasinghe, S., 2013. Halide: language compiler optimizing parallelism, locality, recomputation image processing pipelines. ACM SIGPLAN Not. 48 (6), 519 –530. Ramacher, U., Beichter, J., Raab, W., Anlauf, J., Bruels, N., Hachmann, A., Wesseling, M., 1991. Design 1st generation neurocomputer. VLSI Design Neural Networks. Springer, USA. Ramamoorthy, C.V., Li, H.F., 1977. Pipeline architecture. ACM Comput. Surv. 9 (1), 61–102. Ranganathan, P., Leech, P., Irwin, D., Chase, J., 2006. Ensemble-level power management dense blade servers. In: Proceedings 33rd Annual International Symposium Computer Architecture (ISCA), June 17 –21, 2006, Boston, MA, pp. 66 –77.References ■R-27Rau, B.R., 1994. Iterative modulo scheduling: algorithm software pipelining loops. In: Proceedings 27th Annual International Symposium Microarchitecture, November 30 –December 2, 1994, San Jose, CA, pp. 63 –74. Rau, B.R., Fisher, J.A., 1993. Instruction-level parallelism. J. Supercomput. 235, Springer Science & Business Media. Rau, B.R., Glaeser, C.D., Picard, R.L., 1982. Efficient code generation horizontal architectures: compiler techniques architectural support. In: Proceedings Ninth Annual International Symposium Computer Architecture (ISCA), April 26 –29, 1982, Austin, TX, pp. 131 –139. Rau, B.R., Yen, D.W.L., Yen, W., Towle, R.A., 1989. Cydra 5 departmental supercomputer: design philosophies, decisions, trade-offs. IEEE Comput. 22 (1), 12–34. Reddi, V.J., Lee, B.C., Chilimbi, T., Vaid, K., 2010. Web search using mobile cores: quantifying mitigating price efficiency. In: Proceedings 37th Annual Inter- national Symposium Computer Architecture (ISCA), June 19 –23, 2010, Saint-Malo, France. Redmond, K.C., Smith, T.M., 1980. Project Whirlwind —The History Pioneer Computer. Digital Press, Boston. Reinhardt, S.K., Larus, J.R., Wood, D.A., 1994. Tempest typhoon: user-level shared memory. In: 21st Annual International Symposium Computer Architecture (ISCA), April 18 –21, 1994, Chicago, pp. 325 –336. Reinman, G., Jouppi, N.P., 1999. Extensions CACTI. research.compaq.com/wrl/people/ jouppi/CACTI.html . Rettberg, R.D., Crowther, W.R., Carvey, P.P., Towlinson, R.S., 1990. Monarch parallel processor hardware design. IEEE Comput. 23 (4), 18 –30. Riemens, A., Vissers, K.A., Schutten, R.J., Sijstermans, F.W., Hekstra, G.J., La Hei, G.D., 1999. Trimedia CPU64 application domain benchmark suite. In: Proceedings IEEE International Conference Computer Design: VLSI Computers Proces- sors (ICCD ’99), October 10 –13, 1999, Austin, TX, pp. 580 –585. Riseman, E.M., Foster, C.C., 1972. Percolation code enhance parallel dispatching execution. IEEE Trans. Comput. C-21 (12), 1411 –1415. Robin, J., Irvine, C., 2000. Analysis Intel Pentium ’s ability support secure virtual machine monitor. In: Proceedings USENIX Security Symposium, August 14 –17, 2000, Denver, CO. Robinson, B., Blount, L., 1986. VM/HPO 3880-23 Performance Results, IBM Tech. Bulletin GG66-0247-00. IBM Washington Systems Center, Gaithersburg, MD. Ropers, A., Lollman, H.W., Wellhausen, J., 1999. DSPstone: Texas Instruments TMS320C54x, Tech. Rep. IB 315 1999/9-ISS-Version 0.9. Aachen University Tech-nology, Aachen, Germany ( www.ert.rwth-aachen.de/Projekte/Tools/coal/dspstone_ c54x/index.html ). Rosenblum, M., Herrod, S.A., Witchel, E., Gupta, A., 1995. Complete computer simulation: SimOS approach. IEEE Parallel Distrib. Technol. 4 (3), 34 –43. Rowen, C., Johnson, M., Ries, P., 1988. MIPS R3010 floating-point coprocessor. IEEE Micro 8 (3), 53 –62. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A.C., 2015. Imagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115(3). Russell, R.M., 1978. Cray-1 processor system. Commun. ACM 21 (1), 63 –72. Rymarczyk, J., 1982. Coding guidelines pipelined processors. In: Proceeding Sym- posium Architectural Support Programming Languages Operating Systems (ASPLOS), March 1 –3, 1982, Palo Alto, CA, pp. 12 –19.R-28 ■ReferencesSaavedra-Barrera, R.H., 1992. CPU Performance Evaluation Execution Time Predic- tion Using Narrow Spectrum Benchmarking (Ph.D. dissertation). University Califor- nia, Berkeley. Salem, K., Garcia-Molina, H., 1986. Disk striping. In: Proceedings 2nd International IEEE Conference Data Engineering, February 5 –7, 1986, Washington, DC, pp. 249 –259. Saltzer, J.H., Reed, D.P., Clark, D.D., 1984. End-to-end arguments system design. ACM Trans. Comput. Syst. 2 (4), 277 –288. Samples, A.D., Hilfinger, P.N., 1988. Code Reorganization Instruction Caches, Tech. Rep. UCB/CSD 88/447, University California, Berkeley. Santoro, M.R., Bewick, G., Horowitz, M.A., 1989. Rounding algorithms IEEE multi- pliers. In: Proceedings Ninth IEEE Symposium Computer Arithmetic, September 6–8, Santa Monica, CA, pp. 176 –183. Satran, J., Smith, D., Meth, K., Sapuntzakis, C., Wakeley, M., Von Stamwitz, P., Haagens, R., Zeidner, E., Dalle Ore, L., Klein, Y., 2001. “iSCSI, ”IPS Working Group IETF, Internet draft. www.ietf.org/internet-drafts/draft-ietf-ips-iscsi-07.txt . Saulsbury, A., Wilkinson, T., Carter, J., Landin, A., 1995. argument simple COMA. In: Proceedings First IEEE Symposium High-Performance Computer Architec- tures, January 22 –25, 1995, Raleigh, NC, pp. 276 –285. Schneck, P.B., 1987. Superprocessor Architecture. Kluwer Academic Publishers, Norwell, MA. Schroeder, B., Gibson, G.A., 2007. Understanding failures petascale computers. J. Phys. Conf. Ser. 78 (1), 188 –198. Schroeder, B., Pinheiro, E., Weber, W.-D., 2009. DRAM errors wild: large-scale field study. In: Proceedings Eleventh International Joint Conference Measure- ment Modeling Computer Systems (SIGMETRICS), June 15 –19, 2009, Seat- tle, WA. Schurman, E., Brutlag, J., 2009. user business impact server delays. In: Proceedings Velocity: Web Performance Operations Conference, June 22 – 24, 2009, San Jose, CA. Schwartz, J.T., 1980. Ultracomputers. ACM Trans. Program. Lang. Syst. 4 (2), 484 –521. Scott, N.R., 1985. Computer Number Systems Arithmetic. Prentice Hall, Englewood Cliffs, NJ. Scott, S.L., 1996. Synchronization communication T3E multiprocessor. In: Seventh International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 1 –5, 1996, Cambridge, MA. Scott, S.L., Goodman, J., 1994. impact pipelined channels k-aryn-cube networks. IEEE Trans. Parallel Distrib. Syst. 5 (1), 1 –16. Scott, S.L., Thorson, G.M., 1996. Cray T3E network: adaptive routing high per- formance 3D torus. In: Proceedings IEEE HOT Interconnects ’96, August 15 –17, 1996, Stanford University, Palo Alto, CA, pp. 14 –156. Scranton, R.A., Thompson, D.A., Hunter, D.W., 1983. Access Time Myth. Tech. Rep. RC 10197 (45223). IBM, Yorktown Heights, NY. Seagate, 2000. Seagate Cheetah 73 Family: ST173404LW/LWV/LC/LCV Product Manual, vol. 1. Seagate, Scotts Valley, CA. www.seagate.com/support/disc/manuals/scsi/ 29478b.pdf . Seitz, C.L., 1985. Cosmic Cube (concurrent computing). Commun. ACM 28 (1), 22 –33. Senior, J.M., 1993. Optical Fiber Communications: Principles Practice, second ed. Prentice Hall, Hertfordshire, UK. Sergio Guadarrama, 2015. BVLC googlenet. https://github.com/BVLC/caffe/tree/ master/ models/bvlc_googlenet .References ■R-29Seznec, A., Michaud, P., 2006. case (partially) TAgged GEometric history length branch prediction. J. Instruction Level Parallel. 8, 1 –23. Shao, Y.S., Brooks, D., 2015. Research infrastructures hardware accelerators. Synth. Lect. Comput. Architect. 10 (4), 1 –99. Sharangpani, H., Arora, K., 2000. Itanium processor microarchitecture. IEEE Micro 20 (5), 24–43. Shurkin, J., 1984. Engines Mind: History Computer. W.W. Norton, New York. Shustek, L.J., 1978. Analysis Performance Computer Instruction Sets (Ph.D. disser- tation). Stanford University, Palo Alto, CA. Silicon Graphics, 1996. MIPS V Instruction Set. http://www.sgi.com/MIPS/arch/ISA5/ #MIPSV_indx . Silver, D., Huang, A., Maddison, C.J., Guez, A., Sifre, L., Van Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., 2016. Mastering game Go deep neural networks tree search. Nature 529(7587). Singh, J.P., Hennessy, J.L., Gupta, A., 1993. Scaling parallel programs multiprocessors: methodology examples. In: Computer, 2. 7, pp. 22 –33. Singh, A., Ong, J., Agarwal, A., Anderson, G., Armistead, A., Bannon, R., Boving, S., Desai, G., Felderman, B., Germano, P., Kanagala, A., Provost, J., Simmons, J., Eiichi Tanda, E., Wanderer, J., H €olzle, U., Stuart, S., Vahdat, A., 2015. Jupiter rising: decade CLOS topologies centralized control Google ’s datacenter network. ACM SIG- COMM Comput. Commun. Rev. 45 (4), 183 –197. Sinharoy, B., Koala, R.N., Tendler, J.M., Eickemeyer, R.J., Joyner, J.B., 2005. POWER5 system microarchitecture. IBM J. Res. Dev. 49 (4 –5), 505 –521. Sites, R., 1979. Instruction Ordering CRAY-1 Computer, Tech. Rep. 78-CS-023. Dept. Computer Science, University California, San Diego. Sites, R.L. (Ed.), 1992. Alpha Architecture Reference Manual. Digital Press, Burlington, MA. Sites, R.L., Witek, R. (Eds.), 1995. Alpha Architecture Reference Manual, second ed. Dig- ital Press, Newton, MA. Skadron, K., Clark, D.W., 1997. Design issues tradeoffs write buffers. In: Proceedings Third International Symposium High-Performance Computer Architecture, February 1 –5, 1997, San Antonio, TX, pp. 144 –155. Skadron, K., Ahuja, P.S., Martonosi, M., Clark, D.W., 1999. Branch prediction, instruction- window size, cache size: performance tradeoffs simulation techniques. IEEE Trans. Comput. 48(11). Slater, R., 1987. Portraits Silicon. MIT Press, Cambridge, MA.Slotnick, D.L., Borck, W.C., McReynolds, R.C., 1962. Solomon computer. In: Proceedings AFIPS Fall Joint Computer Conference, December 4 –6, 1962, Philadelphia, PA, pp. 97 –107. Smith, B.J., 1978. pipelined, shared resource MIMD computer. In: Proceedings Inter- national Conference Parallel Processing (ICPP), August, Bellaire, MI, pp. 6 –8. Smith, B.J., 1981a. Architecture applications HEP multiprocessor system. Real Time Signal Process. IV 298, 241 –248. Smith, J.E., 1981b. study branch prediction strategies. In: Proceedings Eighth Annual International Symposium Computer Architecture (ISCA), May 12 –14, 1981, Minneapolis, MN, pp. 135 –148. Smith, A.J., 1982a. Cache memories. Comput. Surv., 14, 3, pp. 473 –530. Smith, J.E., 1982b. Decoupled access/execute computer architectures. In: Proceedings 11th International Symposium Computer Architecture.R-30 ■ReferencesSmith, J.E., 1984. Decoupled access/execute computer architectures. ACM Trans. Comput. Syst. 2 (4), 289 –308. Smith, J.E., 1988. Characterizing computer performance single number. Commun. ACM 31 (10), 1202 –1206. Smith, J.E., 1989. Dynamic instruction scheduling Astronautics ZS-1. Computer 22 (7), 21 –35. Smith, J.E., Goodman, J.R., 1983. study instruction cache organizations replace- ment policies. In: Proceedings 10th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1982, Stockholm, Sweden, pp. 132 –137. Smith, A., Lee, J., 1984. Branch prediction strategies branch-target buffer design. Com- puter 17 (1), 6 –22. Smith, J.E., Pleszkun, A.R., 1988. Implementing precise interrupts pipelined processors. IEEE Trans. Comput. 37 (5), 562 –573. (This paper based earlier paper appeared Proceedings 12th Annual International Symposium Computer Architecture (ISCA), June 17 –19, 1985, Boston, MA. Smith, J.E., Dermer, G.E., Vanderwarn, B.D., Klinger, S.D., Rozewski, C.M., Fowler, D.L., Scidmore, K.R., Laudon, J.P., 1987. ZS-1 central processor. In: Proceedings Second International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 5 –8, 1987, Palo Alto, CA, pp. 199 –204. Smith, M.D., Johnson, M., Horowitz, M.A., 1989. Limits multiple instruction issue. In: Proceedings Third International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 3 –6, 1989, Boston, pp. 290 –302. Smith, M.D., Horowitz, M., Lam, M.S., 1992. Efficient superscalar performance boosting. In: Proceedings Fifth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 12 –15, 1992, Boston, pp. 248 –259. Smotherman, M., 1989. sequencing-based taxonomy I/O systems review historical machines. Comput. Architect. News 17 (5), 5 –15. Reprinted Computer Architecture Readings, Hill, M.D., Jouppi, N.P., Sohi, G.S. (Eds.), 1999. Morgan Kaufmann, San Francisco, pp. 451 –461. Sodani, A., Sohi, G., 1997. Dynamic instruction reuse. In: Proceedings 24th Annual Inter- national Symposium Computer Architecture (ISCA), June 2 –4, 1997, Denver, CO. Sohi, G.S., 1990. Instruction issue logic high-performance, interruptible, multiple functional unit, pipelined computers. IEEE Trans. Comput. 39 (3), 349 –359. Sohi, G.S., Vajapeyam, S., 1989. Tradeoffs instruction format design horizontal archi- tectures. In: Proceedings Third International Conference Architectural Support forProgramming Languages Operating Systems (ASPLOS), April 3 –6, 1989, Boston, pp. 15 –25. Sony/Toshiba, 1999. ‘Emotion Engine ’in PS2 ( “IPU basically MPEG2 decoder …”). http://www.cpu-collection.de/?l0 ¼co&l1¼Sony&l2 ¼Emotion+Engine http://arstechnica. com/gadgets/2000/02/ee/3/ . Soundararajan, V., Heinrich, M., Verghese, B., Gharachorloo, K., Gupta, A., Hennessy, J.L., 1998. Flexible use memory replication/migration cache- coherent DSM multiprocessors. In: Proceedings 25th Annual International Sympo- sium Computer Architecture (ISCA), July 3 –14, 1998, Barcelona, Spain, pp. 342 –355. SPEC, 1989. SPEC Benchmark Suite Release 1.0 (October 2). SPEC, 1994. SPEC Newsletter (June).References ■R-31Sporer, M., Moss, F.H., Mathais, C.J., 1988. introduction architecture Stellar Graphics supercomputer. In: Proceedings IEEE COMPCON, February 29 –March 4, 1988, San Francisco, p. 464. Spurgeon, C., 2001. Charles Spurgeon ’s Ethernet Web Site. www.host.ots.utexas.edu/ ethernet/ethernet-home.html . Steinberg, D., 2015. Full-Chip Simulations, Keys Success. In: Proceedings Synopsys Users Group (SNUG) Silicon Valley 2015. Stenstr €om, P., Joe, T., Gupta, A., 1992. Comparative performance evaluation cache- coherent NUMA COMA architectures. In: Proceedings 19th Annual Interna- tional Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 80 –91. Sterling, T., 2001. Beowulf PC Cluster Computing Windows Beowulf PC Cluster Computing Linux. MIT Press, Cambridge, MA. Stern, N., 1980. invented first electronic digital computer? Ann. Hist. Comput. 2 (4), 375 –376. Stevens, W.R., 1994 –1996. TCP/IP Illustrated (three volumes). Addison-Wesley, Reading, MA. Stokes, J., 2000. Sound Vision: Technical Overview Emotion Engine. arstechnica.com/reviews/1q00/playstation2/ee-1.html . Stone, H., 1991. High Performance Computers. Addison-Wesley, New York. Strauss, W., 1998. DSP Strategies 2002. www.usadata.com/market_research/spr_05/ spr_r127-005.htm . Strecker, W.D., 1976. Cache memories PDP-11? In: Proceedings Third Annual International Symposium Computer Architecture (ISCA), January 19 –21, 1976, Tampa, FL, pp. 155 –158. Strecker, W.D., 1978. VAX-11/780: virtual address extension PDP-11 family. In: Proceedings AFIPS National Computer Conference, June 5 –8, 1978, Anaheim, CA. vol. 47, pp. 967 –980. Sugumar, R.A., Abraham, S.G., 1993. Efficient simulation caches optimal replace- ment applications miss characterization. In: Proceedings ACM SIG- METRICS Conference Measurement Modeling Computer Systems, May 17–21, 1993, Santa Clara, CA, pp. 24 –35. Sun Microsystems, 1989. SPARC Architectural Manual, Version 8, Part No. 8001399- 09. Sun Microsystems, Santa Clara, CA. Sussenguth, E., 1999. IBM ’s ACS-1 machine. IEEE Comput. 22, 11. Swan, R.J., Bechtolsheim, A., Lai, K.W., Ousterhout, J.K., 1977a. implementation Cm* multi-microprocessor. In: Proceedings AFIPS National Computing Confer- ence, June 13 –16, 1977, Dallas, TX, pp. 645 –654. Swan, R.J., Fuller, S.H., Siewiorek, D.P., 1977b. Cm* —a modular, multi-microprocessor. In: Proceedings AFIPS National Computing Conference, June 13 –16, 1977, Dallas, TX, pp. 637 –644. Swartzlander, E. (Ed.), 1990. Computer Arithmetic. IEEE Computer Society Press, Los Alamitos, CA. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., Rabinovich, A., 2015. Going deeper convolutions. In: Proceedings IEEE Conference Computer Vision Pattern Recognition. Takagi, N., Yasuura, H., Yajima, S., 1985. High-speed VLSI multiplication algorithm redundant binary addition tree. IEEE Trans. Comput. C-34 (9), 789 –796. Talagala, N., 2000. Characterizing Large Storage Systems: Error Behavior Performance Benchmarks (Ph.D. dissertation). Computer Science Division, University California, Berkeley.R-32 ■ReferencesTalagala, N., Patterson, D., 1999. Analysis Error Behavior Large Storage System, Tech. Report UCB//CSD-99-1042. Computer Science Division, University Califor- nia, Berkeley. Talagala, N., Arpaci-Dusseau, R., Patterson, D., 2000a. Micro-Benchmark Based Extraction Local Global Disk Characteristics, CSD-99-1063. Computer Science Division,University California, Berkeley. Talagala, N., Asami, S., Patterson, D., Futernick, R., Hart, D., 2000b. art massive storage: case study Web image archive. Computer 33 (11), 22 –28. Tamir, Y., Frazier, G., 1992. Dynamically-allocated multi-queue buffers VLSI commu- nication switches. IEEE Trans. Comput. 41 (6), 725 –734. Tanenbaum, A.S., 1978. Implications structured programming machine architecture. Commun. ACM 21 (3), 237 –246. Tanenbaum, A.S., 1988. Computer Networks, second ed. Prentice Hall, Englewood Cliffs, NJ. Tang, C.K., 1976. Cache design tightly coupled multiprocessor system. In: Proceedings AFIPS National Computer Conference, June 7 –10, 1976, New York, pp. 749 –753. Tanqueray, D., 2002. Cray X1 supercomputer road map. In: Proceedings 13th Daresbury Machine Evaluation Workshop, December 11 –12, 2002, Daresbury Labora- tories, Daresbury, Cheshire, UK. Tarjan, D., Thoziyoor, S., Jouppi, N., 2005. HPL Technical Report CACTI 4.0. www. hpl.hp.com/techeports/2006/HPL ¼2006+86.html . Taylor, G.S., 1981. Compatible hardware division square root. In: Proceedings 5th IEEE Symposium Computer Arithmetic, May 18 –19, 1981, University Mich- igan, Ann Arbor, MI, pp. 127 –134. Taylor, G.S., 1985. Radix 16 SRT dividers overlapped quotient selection stages. In: Proceedings Seventh IEEE Symposium Computer Arithmetic, June 4 –6, 1985, University Illinois, Urbana, IL, pp. 64 –71. Taylor, G., Hilfinger, P., Larus, J., Patterson, D., Zorn, B., 1986. Evaluation SPUR LISP architecture. In: Proceedings 13th Annual International Symposium Com- puter Architecture (ISCA), June 2 –5, 1986, Tokyo. Taylor, M.B., Lee, W., Amarasinghe, S.P., Agarwal, A., 2005. Scalar operand networks. IEEE Trans. Parallel Distrib. Syst. 16 (2), 145 –162. Tendler, J.M., Dodson, J.S., Fields Jr., J.S., Le, H., Sinharoy, B., 2002. Power4 system microarchitecture. IBM J. Res. Dev. 46 (1), 5 –26. TensorFlow Tutorials, 2016. https://www.tensorflow.org/versions/r0.12/tutorials/index.html . Texas Instruments, 2000. History Innovation: 1980s. www.ti.com/corp/docs/company/ history/1980s.shtml . Tezzaron Semiconductor, 2004. Soft Errors Electronic Memory, White Paper. Tezzaron Semiconductor, Naperville, IL http://www.tezzaron.com/about/papers/soft_errors_1_ 1_secure.pdf . Thacker, C.P., McCreight, E.M., Lampson, B.W., Sproull, R.F., Boggs, D.R., 1982. Alto: personal computer. In: Siewiorek, D.P., Bell, C.G., Newell, A. (Eds.), Computer Struc- tures: Principles Examples. McGraw-Hill, New York, pp. 549 –572. Thadhani, A.J., 1981. Interactive user productivity. IBM Syst. J. 20 (4), 407 –423. Thekkath, R., Singh, A.P., Singh, J.P., John, S., Hennessy, J.L., 1997. evaluation commercial CC-NUMA architecture —the CONVEX Exemplar SPP1200. In: Proceedings 11th International Parallel Processing Symposium (IPPS), April 1–7, 1997, Geneva, Switzerland. Thorlin, J.F., 1967. Code generation PIE (parallel instruction execution) computers. In: Proceedings Spring Joint Computer Conference, April 18 –20, 1967, Atlantic City, NJ, p. 27.References ■R-33Thornton, J.E., 1964. Parallel operation Control Data 6600. In: Proceedings AFIPSFall Joint Computer Conference, Part II, October 27 –29, 1964, San Francisco. 26, pp. 33 –40. Thornton, J.E., 1970. Design Computer, Control Data 6600. Scott Foresman, Glenview, IL. Tjaden, G.S., Flynn, M.J., 1970. Detection parallel execution independent instruc- tions. IEEE Trans. Comput. C-19 (10), 889 –895. Tomasulo, R.M., 1967. efficient algorithm exploiting multiple arithmetic units. IBM J. Res. Dev. 11 (1), 25 –33. Torrellas, J., Gupta, A., Hennessy, J., 1992. Characterizing caching synchroniza- tion performance multiprocessor operating system. In: Proceedings Fifth Inter- national Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 12 –15, 1992, Boston (SIGPLAN Notices 27:9 (September), pp. 162 –174. Touma, W.R., 1993. Dynamics Computer Industry: Modeling Supply Workstations Components. Kluwer Academic, Boston. Tuck, N., Tullsen, D., 2003. Initial observations simultaneous multithreading Pentium 4 processor. In: Proceedings 12th International Conference Parallel Architectures Compilation Techniques (PACT ’03), September 27 –October 1, 2003, New Orleans, LA, pp. 26 –34. Tullsen, D.M., Eggers, S.J., Levy, H.M., 1995. Simultaneous multithreading: Maximizing on-chip parallelism. In: Proceedings 22nd Annual International Symposium Computer Architecture (ISCA), June 22 –24, 1995, Santa Margherita, Italy, pp. 392 –403. Tullsen, D.M., Eggers, S.J., Emer, J.S., Levy, H.M., Lo, J.L., Stamm, R.L., 1996. Exploiting choice: instruction fetch issue implementable simultaneous multithreading processor. In: Proceedings 23rd Annual International Symposium Computer Architecture (ISCA), May 22 –24, 1996, Philadelphia, PA, pp. 191 –202. Tung, L., 2016. Google Translate: ‘This landmark update biggest single leap 10 years ’, ZDNet. http://www.zdnet.com/article/google-translate-this-landmark- update-is-our-biggest-single-leap-in-10years/ . Ungar, D., Blau, R., Foley, P., Samples, D., Patterson, D., 1984. Architecture SOAR: Smalltalk RISC. In: Proceedings 11th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1984, Ann Arbor, MI, pp. 188 –197. Unger, S.H., 1958. computer oriented towards spatial problems. Proc. Inst. Radio Eng. 46 (10), 1744 –1750. Vahdat, A., Al-Fares, M., Farrington, N., Niranjan Mysore, R., Porter, G., Radhakrishnan, S., 2010. Scale-out networking data center. IEEE Micro 30 (4), 29 –41. Vaidya, A.S., Sivasubramaniam, A., Das, C.R., 1997. Performance benefits virtual chan- nels adaptive routing: application-driven study. In: Proceedings ACM/IEEE Conference Supercomputing, November 16 –21, 1997, San Jose, CA. Vajapeyam, S., 1991. Instruction-Level Characterization Cray Y-MP Processor (Ph.D. thesis). Computer Sciences Department, University Wisconsin-Madison. van Eijndhoven, J.T.J., Sijstermans, F.W., Vissers, K.A., Pol, E.J.D., Tromp, M.I.A., Struik, P., Bloks, R.H.J., van der Wolf, P., Pimentel, A.D., Vranken, H.P.E., 1999. Trimedia CPU64 architecture. In: Proceedings IEEE International Conference Computer Design: VLSI Computers Processors (ICCD ’99), October 10 –13, 1999, Austin, TX, pp. 586 –592. Van Vleck, T., 2005. IBM 360/67 CP/CMS. http://www.multicians.org/thvv/ 360-67.html . Vanhoucke, V., Senior, A., Mao, M.Z., 2011. Improving speed neural networks CPUs. https://static.googleusercontent.com/media/research.google.com/en//pubs/ archive/37631.pdf .R-34 ■Referencesvon Eicken, T., Culler, D.E., Goldstein, S.C., Schauser, K.E., 1992. Active messages: mechanism integrated communication computation. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia. Waingold, E., Taylor, M., Srikrishna, D., Sarkar, V., Lee, W., Lee, V., Kim, J., Frank, M., Finch, P., Barua, R., Babb, J., Amarasinghe, S., Agarwal, A., 1997. Baring soft- ware: raw machines. IEEE Comput. 30, 86 –93. Wakerly, J., 1989. Microcomputer Architecture Programming. Wiley, New York. Wall, D.W., 1991. Limits instruction-level parallelism. In: Proceedings Fourth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 8 –11, 1991, Palo Alto, CA, pp. 248 –259. Wall, D.W., 1993. Limits Instruction-Level Parallelism, Research Rep. 93/6, Western Research Laboratory. Digital Equipment Corp., Palo Alto, CA. Walrand, J., 1991. Communication Networks: First Course. Aksen Associates/Irwin, Homewood, IL. Wang, W.-H., Baer, J.-L., Levy, H.M., 1989. Organization performance two- level virtual-real cache hierarchy. In: Proceedings 16th Annual International Symposium Computer Architecture (ISCA), May 28 –June 1, 1989, Jerusalem, pp. 140 –148. Watanabe, T., 1987. Architecture performance NEC supercomputer SX system. Parallel Comput. 5, 247 –255. Waters, F. (Ed.), 1986. IBM RT Personal Computer Technology, SA 23-1057. IBM, Austin, TX. Watson, W.J., 1972. TI ASC —a highly modular flexible super processor architec- ture. In: Proceedings AFIPS Fall Joint Computer Conference, December 5 –7, 1972, Anaheim, CA, pp. 221 –228. Weaver, D.L., Germond, T., 1994. SPARC Architectural Manual, Version 9. Prentice Hall, Englewood Cliffs, NJ. Weicker, R.P., 1984. Dhrystone: synthetic systems programming benchmark. Commun. ACM 27 (10), 1013 –1030. Weiss, S., Smith, J.E., 1984. Instruction issue logic pipelined supercomputers. In: Proceedings 11th Annual International Symposium Computer Architecture (ISCA), June 5 –7, 1984, Ann Arbor, MI, pp. 110 –118. Weiss, S., Smith, J.E., 1987. study scalar compilation techniques pipelined super- computers. In: Proceedings Second International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), October 5–8, 1987, Palo Alto, CA, pp. 105 –109. Weiss, S., Smith, J.E., 1994. Power PowerPC. Morgan Kaufmann, San Francisco.Wendel, D., Kalla, R., Friedrich, J., Kahle, J., Leenstra, J., Lichtenau, C., Sinharoy, B., Starke, W., Zyuban, V., 2010. Power7 processor SoC. In: Proceedings International Conference IC Design Technology, June 2 –4, 2010, Grenoble, France, pp. 71 –73. Weste, N., Eshraghian, K., 1993. Principles CMOS VLSI Design: Systems Perspective, 2nd ed. Addison-Wesley, Reading, MA. Wiecek, C., 1982. case study VAX 11 instruction set usage compiler execution. In: Proceedings Symposium Architectural Support Programming Languages Operating Systems (ASPLOS), March 1 –3, 1982, Palo Alto, CA, pp. 177 –184. Wilkes, M., 1965. Slave memories dynamic storage allocation. IEEE Trans. Electron. Comput. EC-14 (2), 270 –271. Wilkes, M.V., 1982. Hardware support memory protection: capability implementa- tions. In: Proceedings Symposium Architectural Support Programming Languages Operating Systems (ASPLOS), March 1 –3, 1982, Palo Alto, CA, pp. 107 –116.References ■R-35Wilkes, M.V., 1985. Memoirs Computer Pioneer. MIT Press, Cambridge, MA. Wilkes, M.V., 1995. Computing Perspectives. Morgan Kaufmann, San Francisco. Wilkes, M.V., Wheeler, D.J., Gill, S., 1951. Preparation Programs Electronic Digital Computer. Addison-Wesley, Cambridge, MA. Williams, T.E., Horowitz, M., Alverson, R.L., Yang, T.S., 1987. self-timed chip divi- sion. In: Losleben, P. (Ed.), 1987 Stanford Conference Advanced Research VLSI. MIT Press, Cambridge, MA. Williams, S., Waterman, A., Patterson, D., 2009. Roofline: insightful visual performance model multicore architectures. Commun. ACM 52 (4), 65 –76. Wilson Jr., A.W., 1987. Hierarchical cache/bus architecture shared-memory multipro- cessors. In: Proceedings 14th Annual International Symposium Computer Architecture (ISCA), June 2 –5, 1987, Pittsburgh, PA, pp. 244 –252. Wilson, R.P., Lam, M.S., 1995. Efficient context-sensitive pointer analysis C programs. In: Proceedings ACM SIGPLAN ’95 Conference Programming Language Design Implementation, June 18 –21, 1995, La Jolla, CA, pp. 1 –12. Wolfe, A., Shen, J.P., 1991. variable instruction stream extension VLIW architec- ture. In: Proceedings Fourth International Conference Architectural Support Programming Languages Operating Systems (ASPLOS), April 8 –11, 1991, Palo Alto, CA, pp. 2 –14. Wood, D.A., Hill, M.D., 1995. Cost-effective parallel computing. IEEE Comput. 28 (2), 69 –72. Wu, Y., Schuster, M., Chen, Z., Le, Q., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K., Klingner, J., Shah, A., Johnson, M., Liu, X., Kaiser, Ł., Gouws, S., Kato, Y., Kudo, T., Kazawa, H., Stevens, K., Kurian, G., Patil, N., Wang, W., Young, C., Smith, J., Riesa, J., Rudnick, A., Vinyals, O., Corrado, G., Hughes, M., Dean, J., 2016. Google ’s Neural Machine Translation System: Bridging Gap Human Machine Translation. http://arxiv.org/abs/1609.08144 . Wulf, W., 1981. Compilers computer architecture. Computer 14 (7), 41 –47. Wulf, W., Bell, C.G., 1972. C.mmp —a multi-mini-processor. In: Proceedings AFIPS Fall Joint Computer Conference, December 5 –7, 1972, Anaheim, CA, pp. 765 –777. Wulf, W., Harbison, S.P., 1978. Reflections pool processors —an experience report C.mmp/Hydra. In: Proceedings AFIPS National Computing Conference, June 5 – 8, 1978, Anaheim, CA, pp. 939 –951. Wulf, W.A., McKee, S.A., 1995. Hitting memory wall: implications obvious. ACM SIGARCH Comput. Architect. News 23 (1), 20 –24. Wulf, W.A., Levin, R., Harbison, S.P., 1981. Hydra/C.mmp: Experimental Computer System. McGraw-Hill, New York. Yamamoto, W., Serrano, M.J., Talcott, A.R., Wood, R.C., Nemirosky, M., 1994. Perfor- mance estimation multistreamed, superscalar processors. In: Proceedings 27thAnnual Hawaii International Conference System Sciences, January 4 –7, 1994, Maui, pp. 195 –204. Yang, Y., Mason, G., 1991. Nonblocking broadcast switching networks. IEEE Trans. Comput. 40 (9), 1005 –1015. Yeager, K., 1996. MIPS R10000 superscalar microprocessor. IEEE Micro 16 (2), 28 –40. Yeh, T., Patt, Y.N., 1993a. Alternative implementations two-level adaptive branch prediction. In: Proceedings 19th Annual International Symposium Computer Architecture (ISCA), May 19 –21, 1992, Gold Coast, Australia, pp. 124 –134. Yeh, T., Patt, Y.N., 1993b. comparison dynamic branch predictors use two levels branch history. In: Proceedings 20th Annual International Symposium Com- puter Architecture (ISCA), May 16 –19, 1993, San Diego, CA, pp. 257 –266.R-36 ■ReferencesIndex Page references bold represent figures, tables boxes. Absolute addressing mode, K-34 Accelerated Strategic Computing Initiative (ASCI) ASCI Red, F-104 –105 ASCI White, F-71, F-105 Access 1/Access 2 stages, TI 320C55 DSP, E-7 Access bit, B-52 Access time. See also Average memory access time (AMAT) DRAM/magnetic disk, D-3 DSM, 372 –373 memory hierarchy design, 85 slowdown causes, B-3, B-3 SMPs, 371 Access time gap, D-3 Accumulator, 557 –558 architecture, A-3 extended, A-3 Acknowledgment, packets, F-17 ACM. SeeAssociation Computing Machinery (ACM) ACS project, M-29 –30 Activation hardware, 557 –558 Ada language, integer division/ remainder, J-12 Adaptive routing definition, F-47 –48 vs.deterministic routing, F-53 –56 network fault tolerance, F-98 overhead, F-97Adders carry-lookahead, J-37 –41 chip comparison, J-61 full, J-2 –3,J-3 half, J-2 –3 integer division speedup, J-54 –57 integer multiplication speedup even/odd array, J-52 many adders, J-50 –54,J-50 multipass array multiplier, J-51 signed-digit addition table, J-54 single adder, J-47 –49,J-48–49 Wallace tree, J-53 radix-2 division, J-55 radix-4 division, J-56 radix-4 SRT division, J-57 ripple-carry, J-3, J-3 time/space requirements, J-44 Addition operations chip comparison, J-61 floating point denormals, J-26 –27 overview, J-21 –25 rules, J-24 speedup, J-25 –26 integer, speedup carry-lookahead, J-37 –41 carry-lookahead circuit, J-38 carry-lookahead tree, J-40 carry-lookahead tree adder, J-41 carry-select adder, J-43 –44, J-43–44 carry-skip adder, J-41 –43,J-42 overview, J-37 ripply-carry addition, J-2 –3,J-3 Address aliasing prediction, 239 –240Address fault, B-42 Addressing modes absolute, K-34 based indexed addressing, K-34 base plus scaled indexed, K-34 control flow instructions, A-17 –18 data addressing modes, K-32 –35 displacement, A-11 –12 instruction formats, K-6 –9 instruction set architecture, 13 memory addressing, A-8 –11,A-10 register indirect, K-34 RISC-V, A-36 Address offset, B-55 –56 Address space global, B-52 local, B-52 memory hierarchy, B-57 shared memory, 373 virtual memory, B-12, B-40, B-41 , B-44, B-55 Address specifier, A-21, K-54 Address stage, TI 320C55 DSP, E-7 Address trace, B-4 Address translation, B-42 AMD64 paged virtual memory, B-55 indexing, B-36 –40, 83 Opteron data TLB, B-47 translation lookaside buffers, B-37, B-46, B-47 virtual memory, B-46, B-47 , 120 Advanced load address table (ALAT) IA-64 ISA, H-40 vector sparse matrices, G-12 –13 Advanced loads, IA-64 ISA, H-40 I-1Advanced mobile phone service (AMPS), cell phones, E-25 Advanced Research Project Agency (ARPA), F-102 –103 Advanced RISC Machine (ARM), 12 architecture, K-22 GPU computing history, M-53 Advanced Simulation Computing (ASC) program, F-106 Advanced Switching Interconnect (ASI), F-107 Advanced Switching SAN, F-71 Advanced Technology Attachment (ATA) disks Berkeley ’s Tertiary Disk project, D-12 –13 disk power, D-5 disk storage, D-4 historical background, M-88 RAID 6, D-8 –9 Advanced vector extensions (AVX), 282, 305, 306 Affine, loop-level parallelism dependences, H-6 rounding rule, J-36Aggregate bandwidth definition, F-13 effective bandwidth calculations, F-18–19 shared- vs.switched-media networks, F-23, F-25 switched-media networks, F-24 –25 switch microarchitecture, F-56 AI.SeeArtificial intelligence (AI) Aiken, Howard, M-3 –4 ALAT. SeeAdvanced load address table (ALAT) Alewife machine, M-62 ALGOL, M-17 –18 Aliases, address translation, B-38 Allen, Fran, M-29 –30 Alliant processors, G-26 Alloy cache, 115 Alloyed predictors, 184 Alpha 21164 cache hierarchy, characteristics, 395 L1 caches, 395 AlphaServer, 395 –396 AlphaServer 4100, 395AltaVista search, cluster history, M-63, M-74 –75 ALUs. SeeArithmetic-logical units (ALUs) AMAT. SeeAverage memory access time (AMAT) Amazon Dynamo key-value storage system, 485–486 Elastic Computer Cloud, 491 –492 Simple Storage Service, 491 –492 warehouse-scale computers, 10 Amazon Elastic Computer Cloud (EC2), utility computing, M-75 –76 Amazon Web Services (AWS) availability zones, 497 –501,497 cloud computing, 491 –497 EC2 computer unit, 493–494 growth, 500 guarantee service, 492 low cost, 492 reliance open source software, 492 virtual machines, 491 Xen virtual machine, 126 Amdahl, Gene, M-29 –30 Amdahl ’s Law, 5 computer design principles, 49 –52 computer system power consumption case study, 69–71 execution time, 50 multicore scaling, 436, 438, 442 parallel processing calculations, 373–377 pitfall, 61 software overhead, F-96 speedup, 374 –375 VMIPS Linpack, G-18 AMD Athlon 64, Itanium 2 comparison, H-43 AMD Fusion, M-53 AMD K-5, M-35 AMD Opteron, 387 –388 address translation, B-38 data cache example, B-12 –15,B-13 implementation, 391 microprocessor, 27 misses per instruction, B-15 NetApp FAS6000 filer, D-42paged virtual memory example, B-54 –57 vs.Pentium protection, B-57 processors, 403 TLB address translation, B-47 tournament predictors, 185 –187 AMD processors GPU computing history, M-53 power consumption, F-89 recent advances, M-35 RISC history, M-23 Amortization overhead, D-64 –67 Ample parallelism, 467 Andreessen, Marc, F-102 Andrew benchmark, 399 Annual failure rate, 62 Antenna, radio receiver, E-23 Antialiasing, B-38 Antidependence compiler history, M-32definition, 172 finding, H-7 –8 register renaming, 196 Apollo DN 10000, M-32 Application layer, F-84 Applied Minds, M-76 Arbitration algorithm collision detection, F-23 –24 commercial interconnection networks, F-57 interconnection networks, F-21 –22, F-49–51 network impact, F-52 –53 SAN characteristics, F-76 –77 switched-media networks, F-24 –25 switch microarchitecture, F-56 pipelining, F-65 –66 system area network, F-104 –105 Architecturally visible registers, 234 Architectural Support Compilers Operating Systems (ASPLOS), M-12 Architecture. See also Computer architecture; Instruction set architecture (ISA) compiler writer, A-30 –31 microarchitecture, 266 –273 Areal density, D-2 Argument pointer, K-57 Arithmetic intensity, 307 –308I-2 ■IndexArithmetic-logical units (ALUs) data forwarding, C-36 –37 data hazards stall minimization, C-14 –17 DSP media extensions, E-10 effective address cycle, C-5 IA-64 instructions, H-35 integer division, J-54 integer multiplication, J-48 integer operations, C-46 –48 integer shifting zeros, J-45 latency, C-46 –48 load interlocks, C-35 micro-op fusion, 254 MIPS R4000 pipeline, C-59 multicycle implementation, C-29 operation, C-27 –28 pipeline branch issues, C-35 –36 RISC classic pipeline, C-8 RISC instruction set, C-5 RISC pipeline, C-31 –32, C-35 TX-2, M-50 ARM. SeeAdvanced RISC Machine (ARM) ARM AMBA, OCNs, F-3 ARM Cortex-A53 characteristics, 259 clock cycles per instruction, 251–252,252 data miss rate, 132 memory hierarchy design, 129–131,130 misprediction rate, 250 multiple-issue processors, 247 –252 pipeline performance, 249, 250–252 virtual address, physical data blocks, 131 ARMv8, K-4,K-9, 13,K-15 , K-16, K-22 ARPANET, F-102 Array FFT kernel, I-7 ocean application, I-9 –10 recurrences, H-12 Array multiplier example, J-50 integers, J-50 multipass system, J-51 Artificial intelligence (AI), 546 ASC Purple, F-71, F-105ASPLOS. SeeArchitectural Support Compilers Operating Systems(ASPLOS) Assembly language, 2 Association Computing Machinery (ACM), M-3 Associativity. See also Set associativity Opteron data cache, B-13 –14,B-13 sizes and, B-10 Astronautics ZS-1, M-31 Asynchronous events, exception, C-39 Asynchronous I/O, D-35 –36 Asynchronous Transfer Mode (ATM) interconnection networks, F-102 –103 LAN, F-93 –94 packet format, F-79 total time statistics, F-94 VOQs, F-60 –61 WANs, F-4, F-84 –85, F-102 –103 ATA disks. SeeAdvanced Technology Attachment (ATA) disks Atanasoff Berry Computer (ABC), M-5 Atanasoff, John, M-5 ATI Radeon 9700, M-51 –52 Atlas computer, M-9 ATM system, TP benchmarks, D-18 Atom 230, 258, 259 Atomic exchange, 413 Atomic instructions, barrier synchronization, I-14 Atomic operations, 386 Attributes field, B-52 Autoincrement deferred addressing, K-52 –53 Autonet, F-49 Autonomous instruction fetch units, 127 Availability computer systems, D-43 I/O system design/evaluation, D-36 –37 Average instruction execution time, M-6 Average memory access time (AMAT) block size calculations, B-26 –28, B-28 cache optimizations, B-22, B-26 –28 cache performance, B-15 –17,B-22 memory hierarchy design, 82miss rate, B-29 –30,B-30 out-of-order computer, B-21 processor performance, B-17 –20 using miss rates, B-30 Average reception factor centralized switched networks, F-33 multi-device interconnection networks, F-26 –27 AWS. SeeAmazon Web Services (AWS) B Back-off time, shared-media networks, F-103 Backpressure, congestion management, F-69 Backpropagation, 548 Backside bus, 377 Balanced systems, D-64 –67 Balanced tree, MINs nonblicking, F-34 Bandwidth. See also Cache bandwidth; Throughput arbitration, F-49 –50 bisection, F-39 –40, F-93, 478 cache miss, B-2 communication mechanism, I-3 compute, 350 congestion management, F-68 Cray Research T3D, F-91 definition, F-13 disparity, F-29 FP arithmetic, J-62 gap, disk storage, D-3 instruction fetch, 228 –232, 229–230 latency effective, F-25 –30 log-log plot, 21 memory, 350, 356 network performance topology, F-41 latency, 20 point-to-point links switches, D-34 shared- vs.switched-media networks, F-23, F-25 snooping, 389 –390 two-device networks, F-13 –20 vector load/store units, 298 –299Index ■I-3Banerjee, Uptal, M-32 Bank busy time, vector memory systems, G-9 Banked memory, 346 vector architectures, G-10 Barcelona Supercomputer Center, F-80 Barnes characteristics, I-8 –9 distributed-memory multiprocessor, I-32 symmetric shared-memory multiprocessors, I-21–22,I-23, I-25 –26 Barnes-Hut n-body algorithm, I-8 –9 Barriers Cray X1, G-23 fetch-and-increment, I-20 –21 large-scale multiprocessor synchronization, I-20–21 large-scale multiprocessor, synchronization, I-13–16,I-14,I-16, I-19, I-20 Based indexed addressing mode, K-34 Base field, B-52 Base plus scaled indexed addressing mode, K-34 –35 Base station, E-22 –23 Batches, DNNs, 556 Batch processing workloads, 467 Bay Area Research Network (BARRNet), F-83 BBN Butterfly, M-61 BBN Monarch, M-61 rounding rule, J-36 Benchmarks. See also Thread Block; specific benchmark desktop benchmarks, 41 –43 distribution data accesses by, A-14 EEMBC, E-12, E-12 embedded applications basic considerations, E-12 power consumption efficiency, E-13, E-13 –14 fallacy, 61 performance measurement, 40–45response time restrictions, D-18 sorting case study, D-64 –67 suite, 41 Bene ŝtopology, F-33, F-34 Berkeley ’s Tertiary Disk project failures components, D-12 overview, D-12 –13 system log, D-43 Berners-Lee, Tim, F-102 Bertram, Jack, M-29 –30 Best-case lower bounds, F-26 Best-case upper bounds, F-26 instructions exception, C-39, C-45 Biased exponent, J-15 –16 Bidirectional multistage interconnection networks, F-33 –34 Bidirectional rings, F-36 Big Endian byte order, A-7 interconnection networks, F-12 BINAC, M-5 Binary code compatibility, embedded systems, E-15 Binary-coded decimal, A-14Binary-to-decimal conversion, FP precisions, J-33 –34 Bing search engine negative impact, 486 WSCs, 485 Bisection bandwidth, 478 network cost constraint, F-93 network performance topology, F-93 NEWS communication, F-42 topology, F-39 Bisection traffic fraction, F-41 –42 Bit error rate (BER), wireless networks, E-21 Bit rot, case study, D-61 –64 Bit selection, B-8 Black box network basic concept, F-5 –6 effective bandwidth, F-18 performance, F-13 switched-media networks, F-24 –25 switched network topologies, F-41 Block. See also Cache block addressing, B-7 –9 cache optimization, 107 –109centralized switched networks, F-33 definition, B-2 disk array deconstruction, D-51 –54 disk deconstruction case study, D-48 –50 factor, 108 global code scheduling, H-15 –16 head-of-line, F-59 –60 identification memory hierarchy, B-8 –9 virtual memory, B-44 –45 LU kernel, I-8 memory hierarchy, 81multithreading, M-35 –36 network performance topology, F-41 offset, B-8 –9 block identification, B-8 –9 cache optimization, B-38 Opteron data cache, B-13 , B-14 placement memory hierarchy, B-7 –8,B-7 virtual memory, B-44 RAID performance prediction, D-57 –59 replacement memory hierarchy, B-9 –10 virtual memory, B-45 size, miss rate and, B-26 –28, B-27 TI TMS320C55 DSP, E-8 Blocked floating point arithmetic, DSP, E-6 Block servers, vs.filers, D-34 –35 Block transfer engine (BLT), F-91 Boggs, David, F-103 BOMB, M-4 Booth recoding, J-8 –10,J-9, J-17 integer multiplication, J-49 Bose-Einstein formula, 34 Bounds checking, B-52 Branch(es) completion cycle, C-28 delayed, C-20, C-20 folding, 231 history table, C-23 RISC instruction set, C-5 VAX, K-57 WCET, E-4I-4 ■IndexBranch byte, K-57 Branch hazards, C-18 –22 penalty reduction, C-19 –20 pipeline issues, C-35 –37 scheme performance, C-21 –22, C-22 simple scheme examples, C-21 –22 Branch penalty branch-target buffers, 230 –231, 231 prediction schemes, C-21 reduction, C-19 –20 Branch prediction accuracy, C-25 –26 buffers, C-23 –25,C-24 –26 correlation, 182 –184 cost reduction, C-22, 182 –191 dynamic, C-23 –25 early schemes, M-29 instruction-level parallelism correlating branch predictors, 182–184 Intel Core i7, 190 –191 specialized branch prediction, 232–234 tagged hybrid predictors, 188–190,188,190 tournament predictors, 184–188,186 integrated, 233 static, C-22, C-23 trace scheduling, H-19 Branch registers, IA-64, H-34 Branch stalls, C-61, C-64 Branch-target address branch hazards, C-38 pipeline branch issues, C-35 –36 RISC instruction set, C-5 Branch-target buffers branch penalty, 230 –231,231 handling instruction with, 230 instruction fetch bandwidth, 228–232,229–230 program counter, 228 –229,229 Branch-target cache, 228 Branch word, K-57 Brewer, Eric, M-74 –75 Bridges, F-82, F-83 Bubbles, F-47 –48,F-54 Buckets, D-26 Buffered crossbar switch, F-66Buffered wormhole switching, F-52 Buffers. See also Branch-target buffers branch-prediction, C-23 –25, C-24 –26 DSM multiprocessor cache coherence, I-38 –40 integrated instruction fetch units, 234 Intel Core i7, 256 interconnection networks, F-10–11 network interface functions, F-7 organizations, F-59 –61 translation lookaside, B-37, B-46, B-47 write, B-11, B-14 Bundles example calculations, H-35 –36 IA-64, H-34 –35,H-37 Itanium 2, H-41 Burks, Arthur, M-3 Burroughs B5000, M-17 –18 Bus-based coherent multiprocessors, M-59 –60 Buses barrier synchronization, I-16 I/O bus replacements, D-34 large-scale multiprocessor synchronization, I-12–13 NEWS communication, F-42 scientific workloads symmetric shared-memory multiprocessors, I-25 Sony PlayStation 2 Emotion Engine, E-18 vs.switched networks, F-2 switch microarchitecture, F-56 Bypassing, C-14 SAN example, F-77 Byte displacement addressing, K-52 Byte/word/long displacement deferred addressing, K-52 –53 C Cache(s). See also Memory hierarchy AMD Opteron example, B-12 –15, B-13 ,B-15 benefits, 350 block frames memory, B-7 concept, M-11definition, B-2 embedded systems, E-4 Itanium 2, H-42 –43 parameters, B-42 Sony PlayStation 2 Emotion Engine, E-18 vector processors, G-25 virtual memory, B-42 –43, B-42 , B-48 –49,B-48 Cache bandwidth block addressing, 100 increasing, 94 multibanked caches, 99 –100 nonblocking caches, 100 –104 pitfall, 143 Cache block cache coherence protocol, 382 –383, 384–385 definition, B-2 miss rate reduction, B-26 –28 shared state, 386 symmetric shared-memory multiprocessors, I-22, I-25–26,I-25 write strategy, B-10 –12 Cache coherence atomic operations, 386 cached data, 128 –129 Cray X1, G-22 definition, 377 –379 directory-based ( seeDirectory- based cache coherence) enforcement, 379 –380 example protocol, 383 –387,384 extensions, 388 implementing locks using, 414–417,416 large-scale multiprocessors, I-34–36 deadlock buffering, I-38–40 directory controller, I-40 –41 DSM implementation, I-36–37 large-scale multiprocessors history, M-61 mechanism, 384 multiprocessor, 377 –379, 387 nonatomic operations, 386 problem, 377, 378 program order, 378 –379Index ■I-5Cache coherence (Continued) snooping ( seeSnooping cache coherence) state diagram, 385,387 Cache hit, B-2 example calculation, B-5 Opteron data cache, B-14 Cache miss block replacement, B-10 definition, B-2 distributed-memory multiprocessor, I-32 interconnection network, F-91–92 large-scale multiprocessors, I-34–35 WCET, E-4 Cache-only memory architecture (COMA), M-61 –62 Cache optimization, B-22 –40 advancement, 117 cache misses, 112 –113 case study, 148 –164 compiler-controlled prefetching, 111–114 compiler optimizations, 107 –109 critical word first, 104 –105 early restart, 104 –105 energy consumption, 97 fallacy, 142 floating-point programs, 101 –102 hardware prefetching, 109 –111 HBM packaging, 114 –117 hit time reduction, B-36 –40, 95–98 impact, B-40 , 148 –150 miss categories, B-23 –25 miss penalty reduction read misses vs.writes, B-35 –36 via multilevel caches, B-30 –35 miss rate reduction via associativity, B-28 –30 via block size, B-26 –28,B-27 via cache size, B-28 multibanked caches, 99 –100 nonblocking caches, 100 –104 pipelined access, 99 –100 power reduction, 95 –98 way prediction, 98 –99 write buffer merging, 105 –106,106Cache organization block placement, B-7 –8,B-7 Opteron data cache, B-12 –15, B-13 Cache performance, B-3 –6, B-15 –16 average memory access time, B-17 –20 cache optimizations impact on, B-40 equations, B-22 example calculation, B-16 –17 miss penalty, B-20 –21 out-of-order execution, B-20 –21 Cache prefetch, 111 Cache size, B-13 miss rate vs.,B-24 –25, B-28, B-33 , B-37 scientific workloads distributed-memory multiprocessors, I-29–31 symmetric shared-memory multiprocessors, I-22–24,I-24 virtually addressed, B-37 Caching locks, 415CACTI energy consumption, 97 first-level caches, 95 –98,96 Callee/caller saving, A-19 –20 Call gate, B-53 , B-54 Canonical form, B-55 Capabilities, protection schemes, M-9 Capacity misses cache size, B-24 definition, B-23 memory hierarchy design, 81 scientific workloads symmetric shared-memory multiprocessors, I-22, I-24, I-24 Capital expenditures (CAPEX), 36, 486–490 Carrier sensing, F-23 –24 Carrier signal, wireless networks, E-21 Carry-in, carry-skip adder, J-41 –42 Carry-lookahead adder (CLA) chip comparison, J-61 circuit, J-38 early computer arithmetic, J-63example calculations, J-39 integer addition speedup, J-37–41 ripple-carry adder, J-42 tree,J-40–41 Carry-out carry-lookahead circuit, J-38 floating-point addition speedup, J-25 Carry-propagate adder (CPA) integer multiplication, J-48, J-51 multipass array multiplier, J-51 Carry-save adder (CSA) integer division, J-54 –55 integer multiplication, J-47 –48, J-48 Carry-select adder characteristics, J-43 –44 chip comparison, J-61 example, J-43–44 Carry-skip adder (CSA) characteristics, J-41 –43 example, J-42,J-44 Case statements, A-17 Catapult board design, 568 CNNs on, 570 –572,571–572 evaluating, 601 –602 guidelines, 577 –579 implementation architecture, 568–569 search acceleration on, 573 –574 software, 569 version 1 deployment, 574 version 2 deployment, 575 –577, 576–577 C/C++ language dependence analysis, H-6 GPU computing history, M-52 –53 integer division/remainder, J-12 CDB. SeeCommon data bus (CDB) Cedar project, M-61 Cell, Barnes-Hut n-body algorithm, I-9 Cell phones block diagram, E-23 embedded system case study characteristics, E-22 –24 Nokia circuit board, E-24 overview, E-20 radio receiver, E-23I-6 ■Indexstandards evolution, E-25 wireless networks, E-21 –22 Flash memory, D-3 –4 Nokia circuit board, E-24 wireless communication challenges, E-21 wireless networks, E-21 –22 Centralized shared-memory multiprocessor, 371, 377 cache coherence protocol, 377 –379, 378,384 enforcement, 379 –380 example protocol, 383 –387, 384 extensions, 388 state diagram, 385,387 invalidate protocol implementation, 382–383 local memory, 377 SMP snooping limitations, 389–392 snooping coherence protocols, 380, 381 example protocol, 383 –387, 384 implementation, 392 –393 invalidate protocol, 381 limitations, 389 –392 maintenance, 380 –381 structure, 372 Centralized switched networks, F-31–35,F-31 Centrally buffered switch, F-56 Central processing unit (CPU) average memory access time, B-18 –20 DNN GPUs vs., 595 –602 early pipelined versions, M-27 –28 execution time, B-3, B-5, B-22 GPU computing history, M-53 performance measurement history, M-6 Sony PlayStation 2 Emotion Engine, E-17 –18 time, 39 TI TMS320C55 DSP, E-8 vector memory systems, G-10 Cerf, Vint, F-102 CFM. SeeCurrent frame pointer (CFM)Chaining convoys, DAXPY code, G-16 vector processor performance, G-11 –12,G-12 Channels, cell phones, E-24 Charge-coupled device (CCD), Sanyo VPC-SX500 digital camera, E-19 Checksum dirty bits, D-61 –64 packet format, F-7 Chime, 291 vector chaining, G-11 –12 vector execution time, G-4 vector performance, G-2 –4 Chip-crossing wire delay, F-3, F-74, F-108 Chip fabrication cost, 67 –68 Chipkill, 94 Choke packets, F-69 –70 Chunk disk array deconstruction, D-51 –54 Shear algorithm, D-51 –54 CIFS. SeeCommon Internet File System (CIFS) Circuit switching, F-51 Circulating water system (CWS), 483 CISC. SeeComplex Instruction Set Computer (CISC) CLA. SeeCarry-lookahead adder (CLA) Clean block, B-11 Clock cycle floating-point operations, C-50 pipeline scheduling, 177 –179 RISC classic pipeline, C-6 RISC exception, C-42 –43 RISC implementation, C-30 switch microarchitecture pipelining, F-66 vector architectures, G-4 Clock cycles per instruction (CPI), 53, 559 ARM Cortex-A53, 251 –252,252 branch scheme, C-21 –22,C-22 cache behavior impact, B-19 –20 cache hit calculation, B-5 calculation, 375 –376 clock rate, 261 data hazards requiring stalls, C-17instruction-level parallelism, 168–169 Intel Core i7 6700, 256, 257 microprocessor advances, M-35 pipelined processor, 168 –169 pipeline stalls, C-11 pipelining concept, C-3 processor performance equation, 52 RISC history, M-22 SPEC92 benchmarks, C-64 SPECCPUint2006 benchmarks, 256,257 stalls, C-64 Clock cycle time, 53 associativity, B-29 –30 cache optimization, B-19 –20 cache performance, B-3 pipeline performance, C-11 pipelining, C-3 RISC implementation, C-30 shared- vs.switched-media networks, F-25 Clock rate, 261, 261 microprocessor advances, M-35 Clock skew, C-8 –10 Clos network, F-33, F-34, 510 –511, 510–511 Cloud computing advantages, 490 AWS ( seeAmazon Web Services (AWS)) economies scale, 491 fallacy, 514 providers, 518 utility computing history, M-75 –76 Clusters, 9 –10, 369, 478 characteristics, I-45 containers, M-76 Cray X1, G-22 history background, M-62 –65 IBM Blue Gene/L, I-41 –44, I-43–44 interconnection network domains, F-3 large-scale multiprocessors, I-6 large-scale multiprocessor trends, M-63 –64 power consumption, F-89 utility computing, M-75 –76 WSC forerunners, M-74 –75 Cm*, M-57Index ■I-7C.mmp, M-57 CMOS cell phone, E-24 first vector computers, M-48 ripply-carry addition, J-3 scaling, 442 –443 vector processors, G-25 –27 CNN. SeeConvolutional neural network (CNN) Coarse-grained multithreading definition, 243 –244 superscalar processor, 245 Cocke, John, M-20, M-29 –31 Code division multiple access (CDMA), cell phones, E-25 Code scheduling example, H-16 parallelism, H-15 –23 superblock scheduling, H-21 –23, H-22 trace scheduling, H-19 –21,H-20 Coefficient variance, D-27 Coerced exception, C-39 Coherence. SeeCache coherence Coherence misses, I-22, 82, 393Cold aisles, 506, 507 Cold-start misses, B-23 Collision detection, shared-media networks, F-23 –24 Collision misses, B-23 Collision, shared-media networks, F-23–24 Collocation sites, interconnection networks, F-89 COLOSSUS, M-4 Column access strobe (CAS), 85 –86 Column major order, 107 COMA. SeeCache-only memory architecture (COMA) Combining tree, large-scale multiprocessor synchronization, I-18 Command queue depth, vs.disk throughput, D-4 Commercial interconnection networks congestion management, F-68–70 connectivity, F-67 cross-company interoperability, F-67–68DECstation 5000 reboots, F-73 fault tolerance, F-70 –72 Commit stage, 211 Commoditization, cost, 30 –31 Commodity cluster, characteristics, I-45 Common data bus (CDB), 197, 201 performance, 207 reservation stations register tags, 202 write result, 211 Common Internet File System (CIFS), D-35 NetApp FAS6000 filer, D-41 –42 Communication bandwidth, I-3 Communication latency, I-3 hiding, I-4 Communication mechanism, 375 –376 internetworking, F-85 –89 large-scale multiprocessors advantages, I-4 –6 metrics, I-3 –4 network interfaces, F-7 –8 NEWS communication, F-42 –44 Communication protocol, F-8 Compare-select-store unit (CSSU), TI TMS320C55 DSP, E-8 Compiler(s) constants, A-31 definition, C-65 interaction, A-27 –30 multimedia instructions, A-31 –32 phase, 399 primitives, A-30 regularity, A-30 role, A-24 –33 structure, A-25 –26 trade-offs, A-30 writer, A-30 –31 Compiler-controlled prefetching, 111–114 Compiler optimization caches, 107 –109, 148 –164 instruction-level parallelism, 176–182 memory consistency, 422 Compiler scheduling, hardware support, M-32 –33 Compiler speculation, hardware supportexample calculations, H-29 memory references, H-32 overview, H-27 preserving exception behavior, H-28 –32 Compiler techniques Cray X1, G-21 –22 dependence analysis, H-7 –8 global code scheduling, H-17 –18 vectorization, G-12 –14 vector sparse matrices, G-12 Complex Instruction Set Computer (CISC), K-51 RISC history, M-23 Compulsory misses, B-23 cache size, B-24 memory hierarchy design, 81 Computation-to-communication ratios parallel programs, I-10 –12 scaling, I-11 Compute bandwidth, 350 Compute-optimized processors, F-92 Computer architecture definition, 11 –12, M-18 –19 floating-point addition, rule, J-24 functional requirements, 17 –18,18 goals, 17 –18 high-level language, M-19 –20 instruction set architecture, 12 –17 limits energy, 28 –29 warehouse-scale computers, 477–482 Computer arithmetic chip comparison, J-57 –61, J-58–60 floating point denormals, J-14 –15 exceptions, J-34 –35 fused multiply-add, J-32 –33 IEEE 754, J-16 iterative division, J-27 –31 memory bandwidth, J-62 number representation, J-15 –16 overview, J-13 –14 precisions, J-33 –34 remainder, J-31 –32 special values, J-14 –15,J-16 underflow, J-36 –37, J-62 floating-point addition denormals, J-26 –27 overview, J-21 –25I-8 ■Indexrules, J-24 speedup, J-25 –26 floating-point multiplication denormals, J-20 –21 examples, J-19 overview, J-17 –20 rounding, J-18, J-19 integer addition speedup carry-lookahead, J-37 –41 carry-lookahead circuit, J-38 carry-lookahead tree, J-40 carry-lookahead tree adder, J-41 carry-select adder, J-43 –44, J-43–44 carry-skip adder, J-41 –43,J-42 overview, J-37 integer arithmetic language comparison, J-12 overflow, J-11 Radix-2 multiplication/division, J-4–7,J-4 restoring/nonrestoring division, J-5,J-6 ripply-carry addition, J-2 –3 signed numbers, J-7 –10 systems issues, J-10 –13 integer division radix-2 division, J-55 radix-4 division, J-56 radix-4 SRT division, J-57 single adder, J-54 –57 SRT division, J-45 –47,J-46, J-55–57 integer-FP conversions, J-62 integer multiplication array multiplier, J-50 Booth recoding, J-49 even/odd array, J-52 many adders, J-50 –54,J-50 multipass array multiplier, J-51 signed-digit addition table, J-54 single adder, J-47 –49, J-48–49 Wallace tree, J-53 integer multiplication/division, shifting zeros, J-45 overview, J-2 rounding modes, J-14, J-17 –20, J-18,J-20Computer chip fabrication, Cray X1E, G-24 Computer classes clusters, 9 –10 desktop computing, 8 embedded computers, 6 –7 internet things, 6 –7 parallel architectures, 10 –11 parallelism, 10 –11 personal mobile device, 7 –8 servers, 8 –9 system characteristics, E-4 warehouse-scale computers, 9 –10 Computer clusters, 470 Computer design principles Amdahl ’s law, 49 –52 common case, 49 locality, 48 –49 parallelism, 48 processor performance equation, 52–55 Computer room air-conditioning (CRAC), 482 Computer technology, improvements, 2–6 Compute tiles, OCNs, F-3Compute Unified Device Architecture (CUDA), 311 CUDA Thread, 311 GPU programming, 320 SIMD instructions, 325 –326 GPU computing history, M-52 –53 Computing efficiently, low utilization, 468 Conditional branching global code scheduling, H-16, H-16 graphics processing units, 323–326 options, A-18 –19,A-19 static branch prediction, C-22 Conditional instructions example calculations, H-23 –24 exposing parallelism, H-23 –27 limitations, H-26 –27 Condition codes, K-11, C-44, K-57 Conflict misses cache optimizations, B-23 cache size, B-24 memory hierarchy design, 82 Congestion control, F-68, F-70 Congestion management, F-68 –70Connectedness, F-30, F-48 Connection Machine CM-5, F-96 Connection Multiprocessor 2, M-46, M-56 Consistency. SeeMemory consistency Constellation, characteristics, I-45 Containers, cluster history, M-76 Contention delay, F-26 Context switching, B-49, 119 Control bits, messages, F-7 Control Data Corporation (CDC) CDC 6600, C-66 –67 computer architecture definition, M-19 early computer arithmetic, J-64–65 first dynamic scheduling, M-28 –29 multiple-issue processor development, M-28 –29 multithreading history, M-35 RISC history, M-28 –29 first vector computers, M-47 STAR-100, first vector computers, M-47 STAR processor, G-26 Control dependences conditional instructions, H-24 global code scheduling, H-16 hardware-based speculation, 208 instruction-level parallelism, 174–176 maintenance, 175 Control flow instructions, 14 addressing modes for, A-17 –18 classes, A-17 compilers role, A-24 –33 structure, A-25 –26,A-25 conditional branch options, A-18 –19,A-19 conditional instructions, H-27 procedure invocation options, A-19 –20 RISC-V, A-39 –40 types, A-16 Control hazards, C-11 Controllers, historical background, M-88 –89 Convex Exemplar, M-62 Convex processors, G-26Index ■I-9Convolutional neural network (CNN), 550–552,551 Catapult, 570 –572,571 processing element of, 572 Convolution, DSP, E-5 Convoy, 290 –292 chained, DAXPY code, G-16 DAXPY VMIPS, G-20 –21 strip-mined loop, G-5 vector starting times, G-4 Conway, Lynn, M-29 –30 Cooling systems, 483 Cooling towers, 508 Copper wiring Ethernet, F-82 interconnection networks, F-9 –10 Copy propagation, H-10 Core, 17 Core i7, 346 –353 Core plus ASIC, embedded systems, E-3 Cortex-A53 ARM, 129 –131,130 performance, 132 Cosmic Cube, M-60 –61 Cost branch prediction, C-22 disk storage, D-2 DRAM/magnetic disk, D-3 interconnecting node calculations, F-32 –33 Internet Archive Cluster, D-38 –40 I/O system design/evaluation, D-36 magnetic storage history, M-85 –86 SIMD supercomputer development, M-45 Cost-performance, 467 DSAs, 600 –601,601 extensive pipelining, C-70 IBM eServer p5 multiprocessor, 440,441 sorting case study, D-64 –67 Cost trends cost-sensitive designs, 29 integrated circuit, 31 –35 manufacturing vs.operation, 36 vs.price, 35 time, volume, commoditization, 30 –31 Counter register, K-25CPA. SeeCarry-propagate adder (CPA) CPI. SeeClock cycles per instruction (CPI) CP-67 program, M-10 CPU. SeeCentral processing unit (CPU) Cray-1 first vector computers, M-47 –48 pipeline depths, G-4 RISC history, M-20 vector performance measures, G-16 Cray-2 DRAM, G-25 first vector computers, M-47 –48 tailgating, G-20 –21 Cray-3, G-27 Cray-4, G-27 Cray C90 first vector computers, M-48 vector performance calculations, G-8 Cray J90, M-48 Cray Research programmers, 303 Cray Research T3D, F-91 Cray, Seymour, G-25, G-27, M-47 –48 Cray supercomputers, early computer arithmetic, J-63 –64 Cray T90, 299 Cray T3D, M-61, F-104 –105 Cray T3E, M-49, M-61, F-71, F-98 –99 Cray X, 370 Cray X1, M-64 cluster history, M-64 first vector computers, M-49 MSP module, G-22 , G-23 –24 overview, G-21 –23 Cray X1E, G-24, F-91, F-95 Cray X2, first vector computers, M-49 Cray X-MP, M-47 –48 Cray XT3, M-59, M-63 Cray XT3 SeaStar, F-67 Cray Y-MP first vector computers, M-48 parallel processing debates, M-58 Create vector index instruction (CVI), G-13 Credit-based control flow, F-10, F-18, F-69, F-75 CRISP, M-29Critical path global code scheduling, H-16 trace scheduling, H-19 –21,H-20 Critical word first, cache optimization, 104–105 Crossbars, F-31, F-31 Convex Exemplar, M-62 Crossbar switch centralized switched networks, F-31 interconnecting node calculations, F-32 –33 Cross-company interoperability, F-67–68 Crusoe, M-32 –33 Cryptanalysis, M-4 CSA. SeeCarry-save adder (CSA) CUDA. SeeCompute Unified Device Architecture (CUDA) Current frame pointer (CFM), H-33 –34 Custom cluster characteristics, I-45 IBM Blue Gene/L, I-41 –44, I-43–44 Cut-through packet switching, F-51 –53 CVI. SeeCreate vector index instruction (CVI) CYBER 205, M-47 vector processor history, G-26 –27 Cycle time. See also Clock cycle time CPI calculation, 375 –376 memory hierarchy design, 85 Cyclic redundancy check (CRC) IBM Blue Gene/L 3D torus network, F-76 network interface, F-8 Cydrome, M-31 –33 DAG. SeeDirected acyclic graph (DAG) Dark silicon, 28 DASH multiprocessor, M-61 Data addressing modes, K-32 –35 Data cache cache performance, B-16 –17 TLB, B-46 Data cache miss applications vs.OS,B-59 Opteron, B-12 –15,B-13I-10 ■Indexsizes associativities, B-10 writes, B-10 Datacenters, containers, M-76 Data dependences conditional instructions, H-24 definition, 170 –172 example calculations, H-3 –4 instruction-level parallelism, 170–172 maintenance, 175 Data fetch (DF), C-58 –59 Data flow control dependence, 174 –176 execution, hardware-based speculation, 209 global code scheduling, H-17 –18 limit, M-34 Data hazards definition, C-11 dynamic scheduling, 191 –201 instruction set complications, C-45 microarchitectural techniques case study, 266 –273 pipelined execution instructions, C-13 program order, 173 –174 stall minimization forwarding, C-14 –15,C-15 –16 stall requirements, C-16 –17 types, C-12 Data integration (DI), 44 Data-level parallelism (DLP), 5, 10 –11 computer design principles, 48 cross-cutting issues banked memory graphics memory, 346 energy DLP, 345 strided accesses TLB misses, 346 energy and, 345 graphics processing units conditional branching, 323–326 multimedia SIMD computers vs., 335 NVIDIA computational structures, 313 –320 NVIDIA GPU instruction set architecture, 320 –323 NVIDIA GPU memory structures, 326 –328,327Pascal GPU architecture, 328–331 programming, 310 –313 quick guide, 314 vector architectures vs., 331–334,332 loop-level parallelism analysis, 337 –339 CUDA/NVIDIA term, 337–338 dependent computations, 344–345 finding dependences, 339 –344 SIMD Multimedia Extensions, 304–310 vector architecture, 282 execution time, 290 –293 vs.graphics processing units, 331–334 memory banks, 298 –299 multidimensional arrays, 299–301 multiple lanes, 293 –294 predicate registers, 296 –298 processor example, 288 –290 programming, 302 –304 RV64V extension, 283 –287, 284 sparse matrices, 301 –302 vector-length registers, 294–296 WSCs, 467 Data link layer definition, F-84 interconnection networks, F-10 Data parallelism, M-56 Data-race-free, 419 Data races, 419 Data transfers cache miss rate calculations, B-16 RISC-V, A-36 Data trunks, C-70 Data types, dependence analysis, H-10 Dauber, Phil, M-29 –30 DAXPY loop chained convoys, G-16 enhanced VMIPS, G-19 –21 vector performance measures, G-16 VMIPS, G-19 –21 calculations, G-18 Linpack, G-18 peak performance, G-17D-cache, way prediction, 98 –99 DDR. SeeDouble data rate (DDR) DDR3 memory systems, 153 –155 Deadlock, F-45 –46, 386 avoidance, F-46 large-scale multiprocessor cache coherence, I-34 –35, I-38–40 recovery, F-46 Dead time, vector pipeline, G-8, G-8 DEC Alpha processor, K-3 Decoder, radio receiver, E-23 Decode stage, TI 320C55 DSP, E-7 DEC PDP-11, address space, B-57 –58 DECstation 5000, F-73 DEC VAX address space, B-57 –58 cluster history, M-62, M-74 computer architecture definition, M-19 early computer arithmetic, J-63–64 early pipelined CPUs, M-28 failures, D-13 –15 integer overflow, J-11 RISC history, M-20 DEC VAX-11/780, M-6 –7, M-11, M-19 DEC VAX 8700 vs.MIPS M2000, M-22 RISC history, M-22 Dedicated link network black box network, F-5 –6 effective bandwidth, F-18 example, F-6 Deep neural networks (DNNs) acceleration, 606 –613 activation, 546 applications, 547,595 batches, 556 convolutional neural network, 550–552 CPUs GPUs vs., 595 –602 multilayer perceptron, 549 –550 neurons of, 546 –547 performance summary, 603 quantization, 556 recurrent neural network, 553–555 training set sizes/time, 548Index ■I-11Deep neural networks (DNNs) (Continued) training vs.inference, 547 –549 weights/parameters, 546 Defect tolerance, 67 –68 Delayed branch behavior, C-20 compiler history, M-33 definition, C-20 Dell PowerEdge servers, 55 –58,56 Dell Poweredge Thunderbird, F-80 Demand access, memory hierarchy design, 138 Demodulator, radio receiver, E-23 Dennard scaling, 4 –5, 58, 368 –369, 442 Denormals, J-14 –15, J-20 –21 floating point addition, J-26 –27 floating-point underflow, J-36 Dense matrix multiplication, LU kernel, I-8 Density-optimized processors, vs. SPEC-optimized, F-89 Dependability benchmark examples, D-21 –23 definition, D-10 –11 disk operators, D-13 –15 integrated circuits, 36 –38 Internet Archive Cluster, D-38 –40 memory systems, 93 –94 via redundancy, 467 Dependence analysis basic approach, H-5 example calculations, H-7 limitations, H-8 –9 Dependence distance, loop-carried dependences, H-6 Dependences control, 174 –176 data, 170 –172 finding, H-6 –10 loop-level parallelism, H-3 name, 172 –173 sparse matrices, G-12 –13 types, 170 –171 Dependent computations, H-10 –12, 344–345 Descriptor privilege level (DPL), B-53 Descriptor tables, B-52 Design faults, D-11 Desktop benchmarks, 41 –43Desktop computers interconnection networks, F-72 multimedia support, E-11 RAID history, M-87 RISC architectures survey for, K-3–29 system characteristics, E-4 Desktop computing, A-2, 8 Desktop/server RISC architectures, instruction formats for, K-8 Destination offset, IA-32 segment, B-53 Deterministic routing algorithm, F-46–47 DF. SeeData fetch (DF) Dies, 31 embedded systems, E-15 Intel Core i7 microprocessor, 32 RISC-V, 33 yield, 33 –34 Digital Alpha conditional instructions, H-27 Digital Alpha 21064, M-48 processors MAX, multimedia support, E-11 recent advances, M-35 synchronization history, M-64 –65 Digital Equipment Vax, 2 Digital Linear Tape, M-85 Digital signal processor (DSP) cell phones, E-23 –24,E-23 definition, E-3 desktop multimedia support, E-11 embedded RISCs, K-28 examples characteristics, E-6 media extensions, E-10 –11 overview, E-5 –7 TI TMS320C55, E-6–7, E-7 –8 TI TMS320C6x, E-8 –10 TI TMS320C64x, E-9 TI TMS320C6x instruction packet, E-10 Dimension-order routing (DOR), F-46–47 Direct attached disks, D-35 Directed acyclic graph (DAG), 582 Direct-mapped cache, B-7, B-8 address translation, B-38 early work, M-11 memory hierarchy, B-48 ,8 1Direct memory access (DMA) historical background, M-89 network interface functions, F-7 Sanyo VPC-SX500 digital camera, E-19 Sony PlayStation 2 Emotion Engine, E-18 TI TMS320C55 DSP, E-8 zero-copy protocols, F-95 Direct networks, F-35, F-37, F-96 Directory-based cache coherence, 380, 391–392 case study, 451 –452 home node, 406 large-scale multiprocessors history, M-61 local node, 406 operations, 406 protocol example, 408 –412 remote node, 406 –407 state transition diagram, 408, 409–410 Directory-based multiprocessor characteristics, I-31 scientific workloads, I-26, I-29 synchronization, I-16, I-19 –20 Directory controller, cache coherence, I-40–41 Directory protocol, 404 DirectX 9, M-51 –52 DirectX 10 generation, M-52 –53 Dirty bit, B-11, B-46, D-61 –64 Dirty block, B-11, B-36 Discrete cosine transform, DSP, E-5 Disk arrays deconstruction case study, D-51 –54 RAID 6, D-8 –9 RAID levels, D-6 –10 Disk layout, RAID performance prediction, D-57 –59 Disk power, D-5 Disk storage, D-2 –10, D-48 –50 Disk system performance milestones, 22 subsystem, failure rates of, 51–52 workload measurements, 400 Dispatch stage, 266 –273 Displacement addressing, K-52 Displacement-style addressing mode, A-11 –12I-12 ■IndexDisplay lists, Sony PlayStation 2 Emotion Engine, E-17 Distributed routing, F-49 Distributed shared memory (DSM), 371, 373 access time, 372 –373 architecture, 373 characteristics, I-45 directory-based cache coherence, 404–412,405 disadvantages, 372 –373 multicore processor, 373,405,452 Distributed shared-memory multiprocessors cache coherence implementation, I-36–37 scientific application performance, I-26–32,I-28–32 Distributed switched networks, F-35–40 Divide operations chip comparison, J-61 floating-point iterative, J-27 –31 integer shifting zeros, J-45 integers, speedup radix-2 division, J-55 radix-4 division, J-56 radix-4 SRT division, J-57 single adder, J-54 –57 SRT division, J-45 –47,J-46, J-55–57 language comparison, J-12 n-bit unsigned integers, J-4 Radix-2, J-4 –7,J-4 restoring/nonrestoring, J-5, J-6 SRT division, J-45 –47,J-46 DLP. SeeData-level parallelism (DLP) DLX, integer arithmetic, J-11 –12 DNNs. SeeDeep neural networks (DNNs) Domain-specific architectures (DSAs), 5 architecture renaissance, 605 –606 cost-performance, 600 –601 CPUs GPUs vs.DNN accelerators, 595 –602 custom chip, 602 deep neural networks activation, 546 applications, 547 batches, 556convolutional neural network, 550–552 multilayer perceptron, 549 –550 neurons of, 546 –547 quantization, 556 recurrent neural network, 553–555 training set sizes/time, 548 training vs.inference, 547 –549 weights/parameters, 546 designing, 604 guidelines for, 543 –544,543 heterogeneity, 592 –594 Intel Crest, 579 ISPs, 580 –582 Microsoft Catapult board design, 568 CNNs on, 570 –572,571–572 evaluating, 601 –602 guidelines, 577 –579 implementation architecture, 568 –569 search acceleration on, 573 –574 software, 569 version 1 deployment, 574 version 2 deployment, 575 –577, 576–577 open instruction set, 594 performance counters, 603 performance/watt, 600 –601 Pixel Visual Core architecture philosophy, 583–584 evaluating, 601 –602 example, 588 floor plan, 592 Halo, 584 –585 implementation, 590 –591 instruction set architecture, 587–588 line buffers in, 590 processing element, 588 –589 processor, 585 –587 software, 582 two-dimensional array, 586 two-dimensional line buffers, 589–590 response time, 596 –600 rooflines, 596 –600 system chip, 592 –594 systolic array, 561TCO, 600 –601 tensor processing unit architecture, 557 –558 block diagram, 558 case study, 606 –617 die,562 guidelines, 566 –567 implementation, 560 –563 improving, 564 –566 instruction set architecture, 559 microarchitecture, 559 –560 origin, 557 printed circuit board, 563 software, 563 TensorFlow program, 564 throughput, 596 –600 Double data rate (DDR), 87, 399 IBM Blue Gene/L, I-43 InfiniBand, F-81 Double-extended floating-point arithmetic, J-33 –34 Double failures, RAID reconstruction, D-55 –57 Double-precision floating point, C-63 , 329 chip comparison, J-58 DSP media extensions, E-10 Double rounding FP precisions, J-34 FP underflow, J-36 –37 Double words, A-7, A-8,A-14 , K-35, 300 DPL. SeeDescriptor privilege level (DPL) DRAM. SeeDynamic random-access memory (DRAM) DRDRAM, Sony PlayStation 2, E-16 Driver domains, Xen virtual machine, 126 DSAs. SeeDomain-specific architectures (DSAs) DSM. SeeDistributed shared memory (DSM) DSP. SeeDigital signal processor (DSP) Dual inline memory modules (DIMMs), F-74, 89 Dynamically allocatable multi-queues (DAMQs), F-56 Dynamically shared libraries, A-18Index ■I-13Dynamic branch prediction, C-23 –25 Dynamic energy, 25 Dynamic network reconfiguration, F-71–73 Dynamic power, 80 Dynamic programming feature (DPF), 577 Dynamic random-access memory (DRAM) arithmetic operations energy cost,29 clock rates, bandwidth, names, 89 cost vs.access time, D-3 Cray X1, G-22 dependability, 516 die stacking, 91 disk storage, D-3 embedded benchmarks, E-13 errors faults, D-11 first vector computers, M-47 –49 IBM Blue Gene/L, I-43 –44 internal organization, 86 magnetic storage history, M-86 memory hierarchy design, 85 –87 memory performance improvement, 87 –90 PlayStation 2, E-16 , E-17 price pressures, 34 semiconductor, 19 stacked/embedded, 91 timing parameters, 153 –155 vector memory systems, G-9 –10 vector processor, G-9 –10, G-25 Dynamic register typing, 287 Dynamic scheduling advantages, 191 –192 data hazards, 191 –201 definition, C-65 –66 first use, M-28 –29 out-of-order execution, 193–194 scoreboard, C-66 –70, C-68 Tomasulo ’s algorithm, 195 –204 loop-based example, 204 –208, 206 steps, 205 unoptimized code, C-70 Dynamic voltage-frequency scaling (DVFS), 27E Early restart, cache optimization, 104–105 Earth Simulator, M-48 –49, M-63 ECC. SeeError-Correcting Code (ECC) Eckert, J. Presper, M-2 –5, M-20 Eckert-Mauchly Computer Corporation, M-5, M-57 ECL minicomputer, M-20 EEMBC. SeeElectronic Design News Embedded Microprocessor Benchmark Consortium (EEMBC) Effective address, A-8 –9 RISC classic pipeline, C-8 RISC instruction set, C-5 simple RISC implementation, C-27 TLB, B-49 Effective bandwidth definition, F-13 example calculations, F-18 –19 vs.packet size, F-19 two-device networks, F-13 –20 Efficiency factor, F-53, F-55 –56 Eight-way set associativity cache optimization, B-28 –29 conflict misses, B-23 data cache misses, B-10 Elapsed time, 39 Electronically erasable programmable read-only memory (EEPROM), 92 Electronic Design News Embedded Microprocessor Benchmark Consortium (EEMBC), 41 benchmark classes, E-12 kernel suites, E-12 power consumption efficiency metrics, E-13, E-13 –14 Electronic Discrete Variable Automatic Computer (EDVAC), M-2–3 Electronic Numerical Integrator Calculator (ENIAC), M-2–3, M-85 Embedded applications, A-2 Embedded computer, 6 –7RISC architectures survey for, K-3–29 Embedded DRAM, 91 Embedded multiprocessors, characteristics, E-14 –15 Embedded systems benchmarks basic considerations, E-12 power consumption efficiency, E-13, E-13 –14 cell phone case study block diagram, E-23 characteristics, E-22 –24 Nokia circuit board, E-24 overview, E-20 radio receiver, E-23 standards evolution, E-25 wireless networks, E-21 –22 characteristics, E-4 digital signal processor cell phones, E-23 –24,E-23 definition, E-3 desktop multimedia support, E-11 examples characteristics, E-6 media extensions, E-10 –11 overview, E-5 –7 TI TMS320C55, E-6–7, E-7 –8 TI TMS320C6x, E-8 –10 TI TMS320C64x, E-9 TI TMS320C6x instruction packet, E-10 EEMBC benchmark suite, E-12 overview, E-2 performance, E-13 –14 real-time processing, E-3 –4 Sanyo digital cameras, SOC, E-20 Sanyo VPC-SX500 digital camera case study, E-19 Sony PlayStation 2 case study, E-15 –18 block diagram, E-16 organization, E-18 EMC, M-87 –88 Emotion Engine organization modes, E-18 Sony PlayStation 2 case study, E-15 –18 empowerTel Networks, MXP processor, E-14 –15I-14 ■IndexEnclaves, instruction set extensions, 125 Encore Multimax, M-59 –60 End-to-end flow control, F-69 Energy DLP, 345 limits of, 28 –29 within microprocessor, 25 –28 proportionality, 503 systems perspective, 23 –24 Energy efficiency, 434 –437, 467. See also Power consumption embedded benchmarks, E-13 Engineering Research Associates (ERA), M-4 –5 ENIAC. SeeElectronic Numerical Integrator Calculator (ENIAC) Enigma coding machine, M-4 Entry time, transactions, D-16, D-17 Environmental faults, storage systems, D-11 EPIC approach historical background, M-33 IA-64, H-33 E-24 RF. SeeRegister fetch (RF) Error correcting codes (ECCs), 93–94 disk storage, D-11 hardware dependability, D-15 RAID 2, D-6 Error handling, interconnection networks, F-9 –12 Errors, definition, D-10 Escape resource set, F-47 –48 ETA processor, G-26 –27 Ethernet, 478 bandwidth, F-82, F-93 LANs, F-4, F-82 –84, F-103 –104 packet format, F-79 shared-media networks, F-23 –24 shared- vs.switched-media networks, F-23 switch vs.NIC, F-90 system area networks, F-76 –77 total time statistics, F-94 WAN, F-84 –85 Eugene, Miya, M-65 European Center Particle Research (CERN), F-102Even/odd array example, J-52 integer multiplication, J-51 –52 EVEN-ODD scheme development, D-10 Exception arithmetic-logical units, C-5 categories, C-40 control dependence, 174 –175 floating-point, C-41 –42 floating-point arithmetic, J-34 –35 imprecise, 194 memory protection, 175 precise, C-41 –44 preservation via hardward support, H-28 –32 RISC V, C-42 –43,C-42 stopping/restarting, C-41 –42 types requirements, C-38 –41, C-40 unexpected sequences, C-70 Execute step Itanium 2, H-42 TI 320C55 DSP, E-7 Execution, C-69, 198, 211 Execution address cycle (EX) data hazards requiring stalls, C-18 data hazards stall minimization, C-14 dynamic scheduling pipelines, C-66 exception stopping/restarting, C-41 floating point pipeline, C-46 longer latency pipelines, C-51 MIPS R4000 pipeline, C-58 –59 pipeline branch issues, C-35 –36 RISC exception, C-42 –43, C-43 RISC instruction set, C-5, C-6 RISC pipeline, C-32 –35 simple RISC implementation, C-27 Execution time, 39 Amdahl ’s law, 50 application/OS misses, B-59 cache performance, B-3 central processing unit, B-3, B-5, B-22 components, 400 multiprocessor, 438 multiprogrammed parallel “make ” workload, 400 pipelining performance, C-3, C-8–10second-level cache size, B-32, B-34 stall time, B-21 vector length, G-7 Expand-down field, B-53 Explicit parallelism, H-34 –37 Explicit unit-stride, 333 Exponential back-off large-scale multiprocessor synchronization, I-17–18 spin lock, I-17 Exponential distribution, D-27 Extended accumulator, A-3, K-30 Extended stack architecture, K-30 F Fabrication cost, 67 –68 Fabrication yield, 67 –68 Failure. See also Mean time failures (MTBF); Mean time failure (MTTF) Berkeley ’s Tertiary Disk project, D-12 definition, D-10 dependability, 37 –38 dirty bits, D-61 –64 RAID reconstruction, D-55 –57 row-diagonal parity, D-9 rates disk subsystem, 51 –52 storage system, D-6 –10 components, D-43 Tertiary Disk, D-13 Failures time (FIT), 37 False sharing, 393 –394,398 Fast Fourier transformation (FFT) characteristics, I-7 distributed-memory multiprocessor, I-32 example calculations, I-27 –29 symmetric shared-memory multiprocessors, I-22, I-23, I-25 –26 Fat trees, F-34, F-38 Fault, 111 definition, D-10 dependability benchmarks, D-21 programming mistakes, D-11 Tandem Computers, D-13 Fault detection, 64 Fault-induced deadlock, F-45 –46Index ■I-15Fault tolerance, F-70 –72, F-98 dependability benchmarks, D-21 RAID, D-7 Fault-tolerant routing, F-70 –71, F-98–99 FC. SeeFibre Channel (FC) FC-AL. SeeFibre Channel Arbitrated Loop (FC-AL) Feature Extraction, 573, 574 Feature functional unit (FFU), 576 Feature maps, two-dimensional, 550 Feature size, 21 FEC. SeeForward error correction (FEC) Federal Communications Commission (FCC), D-15 FENCE RISC V, 420 –422 Fetch-and-increment, 413 –414 large-scale multiprocessor synchronization, I-20 –21 sense-reversing barrier, I-21 Fetch stage, TI 320C55 DSP, E-7 FFT. SeeFast Fourier transformation (FFT) Fibre Channel (FC), F-106 file system benchmarking, D-20 NetApp FAS6000 filer, D-41 –42 Fibre Channel Arbitrated Loop (FC- AL), M-88, F-106 block servers vs.filers, D-35 Fibre Channel Switched (FC-SW), F-106 Filers vs.block servers, D-34 –35 NetApp FAS6000 filer, D-41 –43 servers, SPEC benchmarking, D-20 –21 Filters, radio receiver, E-23 Fine-grained multithreading, 243 –244 Fingerprint, storage system, D-48 Finite-state machine, F-49, F-56 –58 Firmware, network interfaces, F-7 –8 First-in first-out (FIFO), B-9, B-10 , 197 definition, D-26 First-level caches cache optimization, B-30 –35 hit time/power reduction, 95 –98 interconnection network, F-74 Itanium 2, H-41 memory hierarchy, B-48 –49,B-48 parameter ranges, B-42First-reference misses, B-23 Fixed-field decoding, C-5 Fixed length, 14 Fixed-point arithmetic, DSP, E-5 –6 Flash memory disk storage, D-3 –4 embedded benchmarks, E-13 memory hierarchy design, 92 –93 technology trends, 19 FLASH multiprocessor, M-62 Flexible chaining, 290 –291 vector processor, G-11 Flex point, 579 Floating-point (FP) operations, K-38 –40 addition denormals, J-26 –27 overview, J-21 –25 rules, J-24 speedup, J-25 –26 chip comparison, J-58 CPI,C-64 data dependences, 171 denormals, J-14 –15, J-20 –21, J-26–27 double-precision, C-63 DSP media extensions, E-10 –11 early computer arithmetic, J-64 –65 exceptions, J-34 –35, C-41 –42 fused multiply-add, J-32 –33 IEEE 754, J-16 integer conversions, J-62 Itanium 2, H-41 iterative division, J-27 –31 latency, C-61 , C-63, 177 memory bandwidth, J-62 micro-op fusion, 254 MIPS R4000 pipeline, C-60 –61, C-60 mispeculation, 239 multiplication denormals, J-20 –21 examples, J-19 overview, J-17 –20 rounding, J-18, J-19 multiplication precision, J-21 multiply add operation, C-62 number representation, J-15 –16 overflow, J-11 overview, J-13 –14 performance, 308pipeline scheduling, 178 precisions, J-33 –34 programs, 101 –102 register file, C-50 remainder, J-31 –32 result stalls, C-61, C-64 RISC exception, C-43 RISC multicycle operations, C-45 –55 RISC pipeline, C-45 –55,C-47 –48, C-57 RISC-V, A-40 –41 special values, J-14 –15,J-16 square root, 51 static branch prediction, C-23 structural stalls, C-61, C-64 Tomasulo ’s algorithm, 198 underflow, J-36 –37, J-62 vector chaining, G-11 Floating-point registers (FPRs) IA-64, H-34 IBM Blue Gene/L, I-42 Floating Point Systems AP-120B, M-30 Floppy disks, M-85 –86 Flow-balanced state, D-24 Flow control arbitration, F-22 interconnection networks, F-9 –12 Fluent, F-80 –81 Flush, branch penalty reduction, C-19 Forget gate, 553 Form factor, interconnection networks, F-9–10 FORTRAN compiler vectorization, G-14, G-15 dependence analysis, H-6 integer division/remainder, J-12 performance measurement history, M-6 Forward error correction (FEC), E-5 –7 Forwarding, C-14 arithmetic-logical units, C-36 –37 data hazards stall minimization, C-14 –15,C-15 –16 load instruction, C-17 longer latency pipelines, C-49 –52 table, F-56 –58 Forward path, cell phones, E-24 Fourier-Motzkin algorithm, M-32 Fourier transform, DSP, E-5I-16 ■IndexFour-way set associativity, B-23 FPGA, 568 –569 Catapult, 567 Feature Extraction, 574 FP operations. SeeFloating-point (FP) operations Fragmentation problem, 114 Frame pointer, K-57 Free-form expressions, 573 –574 Freeze, branch penalty reduction, C-19 Frequency modulation (FM), wireless neworks, E-21 Front-end stage, Itanium 2, H-42 FU. SeeFunctional unit (FU) Fujitsu Primergy BX3000 blade server, F-89 Fujitsu SPARC64 X+, 389, 426, 429 feature, 427 performance, 429 –431,432 Fujitsu VP100, M-48 Fujitsu VP200, M-48 Full access dimension-order routing, F-47 –48 interconnection network topology, F-30 Full adders, J-2 –3,J-3 Full-duplex mode, F-23 Fully associative cache, B-7 –9, B-12, 81 Fully connected layer, 549 Fully connected topology, F-35 –36, F-35 –36 Functional hazards, 266 –273 Functional unit (FU), C-46 execution slots, superscalar processors, 244 –245,244 Itanium 2, H-41 –43 latency, C-47 , 177 OCNs, F-3 Function pointers, A-18 Fused multiply-add, floating point, J-32–33 Future file approach, C-54 G Gates, 553 Gateways, Ethernet, F-83 Gather-scatter, A-31 –32, 301 –302, 352 sparse matrices, G-13 –14 GE 645, M-9 –10 GeForce 8800, M-52General-Purpose Computing GPUs (GPGPU), M-52 General-purpose electronic computers, M-2–4 General-purpose registers (GPRs) architectures, A-3 IA-64, H-38 Geometric mean, 46 Gibson mix, M-6 Global address space, B-52 Global code scheduling example, H-16 parallelism, H-15 –23 superblock scheduling, H-21 –23, H-22 trace scheduling, H-19 –21,H-20 Global common subexpression elimination, A-26 Global data area, A-29 Global Environment Network Innovation (GENI), F-102 Global miss rate, B-31 Global optimizations, A-26 Global Positioning System, CDMA, E-25 Global predictors, 184 –188 Global scheduling algorithms, 219 –220 Global system mobile communication (GSM), cell phones, E-25 Goldschmidt ’s division algorithm, J-29–30 Goldstine, Herman, M-2 –3 Google clusters history, M-63 containers, M-76 Google App Engine, M-75 –76 Google Clusters, 94 power consumption, F-89 Google File System (GFS), 474 Google Translate, 4, 7, 40 –45 Google WSCs airflow, 506 availability zones, 498 cooling, 506 –508 generators, 505 networking, 510 –511 network switches, 502 network traffic, 501 on-site substation, 504power distribution, 504 –506 power utilization efficiency of, 485 racks, 509 –510,509,512 servers, 505, 512 –513,513 switch gear, 505 transformers, 505 Gordon Bell Prize, M-58 GPGPU. SeeGeneral-Purpose Computing GPUs (GPGPU) GPRs. SeeGeneral-purpose registers (GPRs) Gradual underflow, J-15, J-36 Grain size, 370 Grant phase, arbitration, F-49 –50 Graph coloring, A-27 Graphical Processor Units (GPUs) computing history, M-52 –53 historical background, M-50 –51 scalable, M-51 Graphics data RAMs (GDRAMs), 90 Graphics-intensive benchmarks, 41 Graphics memory, 346 Graphics pipelines, M-51 –52 Graphics processing unit (GPU), 10 conditional branching, 323 –326 DNN CPUs vs., 595 –602 embedded vs.server, 346 –353 fallacy, 353 multimedia SIMD MIMD vs., 347–353 multimedia SIMD computers vs., 335 NVIDIA computational structures, 313–320 NVIDIA GPU instruction set architecture, 320 –323 NVIDIA GPU memory structures, 326–328,327 Pascal GPU architecture, 328 –331 programming, 310 –313 quick guide, 314 vector architectures vs., 331 –334, 332 vector kernel implementation, 357–359 Graphics synchronous DRAMs (GSDRAMs), 90 Graphics Synthesizer, Sony PlayStation 2, E-16 –18, E-16Index ■I-17Greatest common divisor (GCD), 342–343 test, loop-level parallelism dependences, H-7 Grid computing, M-75 Grid mapping, 315, 316 Grid topology, F-36 –38 Gshare predictors tagged hybrid vs.,190 2-bit predictor, 184, 186, 262 Guest domains, 126 Guest virtual machine, 121 H Half adders, J-2 –3 Half-duplex mode, F-23 Half-precision floating-point arithmetic, 329 Halo, 584 –585 HAMR, 19 Handshaking, interconnection networks, F-10 Hard cores, Cortex-A53, 130 Hard errors, memory hierarchy design, 93 Hard real-time systems, definition, E-3–4 Hardware, 17 compiler scheduling support, M-32 –33 compiler speculation support memory references, H-32 overview, H-27 preserving exception behavior, H-28 –32 designing, 17 –18 exposing parallelism, H-23 –27 faults, D-11 interconnection networks, F-8pipeline hazard detection, C-34 Hardware-based speculation, 208 –217 data flow execution, 209 definition, 208 disadvantage, 241 instruction execution step, 211 –212 key ideas, 208 reorder buffer, 209 –212, 214 –215 vs.software speculation, 240 –241 write result, 217 Hardware prefetching, 109 –111, 148–164Hardware primitives, 412 –414 large-scale multiprocessor synchronization, I-18–21 Harvard architecture, M-3 –4 Hazards. See also Data hazards control hazards, C-11 data ( seeData hazards) definition, C-10 –11 detection, hardware, C-34 functional, 266 –273 instruction set complications, C-45 longer latency pipelines, C-49 –52 read write, C-12 –14 structural ( see Structural hazards) write read, C-12 write write, C-12 Header messages, F-7 packet format, F-7 switch microarchitecture pipelining, F-64 TCP/IP, F-87 –89 Head-of-line (HOL) blocking, F-59–61,F-60 Heap, A-29HEP processor, M-35 Heterogeneity, DSAs, 592 –594 Heterogeneous architecture, 282 Hewlett-Packard AlphaServer, F-104 –105 Hewlett-Packard PA-RISC EPIC approach, M-33 floating-point precisions, J-33 MAX2, multimedia support, E-11 Hewlett-Packard RISC microprocessors, G-26 Hewlett Packard server, WSCs, 476 Hewlett-Packard ’s PA-RISC, K-3 Hidden layers, 546 –547 High bandwidth memory (HBM), 346 cache optimization, 114 –117 memory hierarchy design, 91 Pascal GPU architecture, 329 Higher-radix division, J-54 –55 Higher-radix multiplication, integer, J-48 High-level language computer architecture (HLLCA), M-20 High-level optimizations, A-26Highly parallel memory systems, 150–153 High-order functions, A-18 High-performance computing (HPC), 466 vector processor history, G-27 –28 High Performance Fortran (HPF) — programs, 422 High-speed chip-to-chip interconnect, 329 Hillis, Danny, M-46, M-56, M-58 –59, M-76 Histogram, D-26 History file approach, C-54 Hitachi S810, M-48 Hit time, B-15 –16 address translation, 83 first-level caches, 95 –98 latency, 115 memory hierarchy design, 82 reducing, 94 reduction, B-36 –40 way prediction, 98 –99 HLLCA. SeeHigh-level language computer architecture(HLLCA) Home node, 406 Hop count, F-30 Hops, F-36, F-40 Host NVIDIA GPU memory structures, 327 virtual machine, 121 Host channel adapters (HCAs), F-90 historical background, M-89 Hot aisles, 506, 506 Hot swapping, F-71 –73 HPC. SeeHigh-performance computing (HPC) HP Precision Architecture, integer arithmetic, J-11 –12 HP ProLiant BL10e G2 blade server, F-89 HPSm, M-31 Hybrid predictors, 184 Hypercube networks, F-44, F-96 HyperTransport, F-67 NetApp FAS6000 filer, D-42 Hypervisor. SeeVirtual machine monitor (VMM)I-18 ■IndexI IAS machine, M-3, M-5 IBM BlueGene, 370 Chipkill, 94 cluster history, M-62, M-74 computer history, M-5 early VM work, M-10 IBM 360, address space, B-58 IBM 370 architecture, 124 magnetic storage, M-85 –86 multiple-issue processor development, M-29 –30 RAID history, M-87 IBM 360 architects, M-10computer architecture definition, M-18 I/O bus history, M-89 memory hierarchy development, M-9–10 parallel processing debates, M-58 IBM 360/85, M-11, M-29 IBM 360/91 early computer arithmetic, J-63 history, M-29 speculation concept origins, M-31 IBM 370 early computer arithmetic, J-63–64 integer overflow, J-11 vector processor history, G-27 IBM 370/158, M-6 –7 IBM 650, M-5 IBM 701, M-5 IBM 702, M-5 IBM 704, M-5, M-27 –28 IBM 705, M-5 IBM 801, M-20 IBM 3081, M-61 IBM 7030, M-27 –28 IBM 360/370 architecture, K-69 –70 branches special loads stores —RX format, K-72 branches status setting R-R instructions, K-71 branches/logical floating-point instructions —RX format, K-71 definition, K-69360 detailed measurements, K-70 –74 historical perspective references, K-75 integer/logical floating-point R- R instructions, K-70 measurements, K-70 –74 RS SI format instructions, K-72 SS format instructions, K-73 IBM AS/400, M-87 IBM Blue Gene/L, F-4, I-44 cluster history, M-64 computing node, I-42 –44,I-43 custom cluster, I-41 –44,I-43–44 deterministic vs.adaptive routing, F-53 –56 parallel processing debates, M-59 system area network, F-76 –77 3D torus network, F-39 IBM 3840 cartridge, M-85 IBM 9840 cartridge, M-85 IBM CoreConnect cross-company interoperability, F-68 OCNs, F-3 IBM eServer p5 multiprocessor benchmarks, 440 cost-performance, 440, 441 IBM Federation network interfaces, F-18 IBM Power 1, M-31 IBM Power 2, M-31 IBM Power 4 multithreading history, M-36 recent advances, M-35 IBM Power 5, 424 Itanium 2 comparison, H-43 multithreading history, M-36 IBM Power 8, 371, 389 –390, 426 design, 429 feature, 427 on-chip organizations, 428 performance, 431 –432,432 IBM Power processors branch-prediction buffer, C-25 characteristics, 265 IBM Pulsar processor, M-35 –36 IBM RP3, M-61 IBM RS/6000, M-58 IBM RT-PC, M-21 IBM SAGE, M-89IBM Stretch, M-6 IBM 3090 Vector Facility, G-27 IBM zSeries, G-27 IC.SeeInstruction count (IC) I-cache, way prediction, 98 –99 ID.SeeInstruction decode (ID) Ideal pipeline CPI, 169 IDE disks, Berkeley ’s Tertiary Disk project, D-12 Idle Control Register (ICR), TI TMS320C55 DSP, E-8 Idle domains, TI TMS320C55 DSP, E-8 IEEE arithmetic floating point, J-13 –14 addition, J-21 –27 exceptions, J-34 –35 multiplication, J-17 –21 remainder, J-31 –32 underflow, J-36 –37 historical background, J-63 –65 iterative division, J-30 NaN, J-14 rounding modes, J-20 single-precision numbers, J-15 –x vs. 0–x, J-62 IEEE 754 floating-point standard, J-16 IEEE 1394, Sony PlayStation 2 Emotion Engine case study, E-15 IEEE standard 802.3 (Ethernet), F-82 LAN history, F-82 cycle. SeeInstruction fetch (IF) cycle Illiac IV, M-45, M-55, F-104 ILP. SeeInstruction-level parallelism (ILP) Image processing units (IPUs), 580–582 Image signal processors (ISPs), 580–582 hardwired predecessors of, 580–581 interconnection of, 582 Immediate addressing, K-52 IMPACT, M-33 Implicit unit stride, 333 Imprecise exceptions, 194 Inclusion, 383 drawback, B-35 implementation, 423 –424 L1 caches, 423 –424Index ■I-19Inclusion (Continued) L2 caches, 423 –424 L3 caches, 424 memory hierarchy history, M-12 multilevel, B-34, 423 property, 78 Indexed addressing, K-34, K-53 Indexes address translation during, B-36 –40 Opteron data cache, B-13 recurrences, H-12 Index field, B-8 –9 Index vector, 301 –302 Indirect addressing, K-52 Indirect jumps, branch prediction, 232–234 Indirect networks, F-32 –33 Inexact exception floating-point arithmetic, J-35 floating-point underflow, J-36 InfiniBand, F-62, F-68, F-77 –81,F-79 cluster history, M-64 Infinite population model, D-30 Initiation rate, 290 Inktomi, M-63, M-74 –75 In-order commit, speculation concept origins, M-31 In-order execution average memory access time, B-18 cache miss, B-2 IBM Power processors, 265 Input buffered switch, F-56 Input gate, 553 Input-output buffered switch, F-56, F-58,F-65 Instruction cache AMD Opteron example, B-15, B-15 application/OS misses, B-59 branch prediction, C-24 TI TMS320C55 DSP, E-8 Instruction commit, C-43 –44, 209, 235 Instruction count (IC), B-4, 53 cache performance, B-15 –16 processor performance equation, 52 RISC history, M-23 Instruction decode (ID) branch hazards, C-18 data hazards requiring stalls, C-18 dynamic scheduling, C-66, 193 longer latency pipelines, C-50 –51MIPS R4000 pipeline, C-56 pipeline branch issues, C-35 –36 RISC classic pipeline, C-8 RISC instruction set, C-5, C-6 RISC pipeline, C-32 –34,C-35 simple RISC implementation, C-27 Instruction delivery, 228 –240 stage, Itanium 2, H-42 Instruction fetch (IF), 253 bandwidth, 228 –232,229–230 cycle ARM Cortex-A53, 249 –250 branch hazards, C-18 branch-prediction buffer, C-24 data hazards requiring stalls, C-18 exception stopping/restarting, C-41 MIPS R4000 pipeline, C-56 RISC exception, C-42 –43, C-43 RISC instruction set, C-4, C-6 RISC pipeline, C-31 –33 simple RISC implementation, C-27 units, integrated, 233 –234 Instruction formats addressing modes and, K-6 –9 high-level language computer architecture, M-20 IA-64 ISA, H-34 –38,H-39 Instruction groups, IA-64, H-34 Instruction issue, C-33 –34 Itanium 2, H-41 –43 Instruction-level parallelism (ILP), 5, 10, 368, 370 aggressive compiler-based approaches, 168 approaches, 168 branch prediction correlating branch predictors, 182–184 Intel Core i7, 190 –191 specialized, 232 –234 tagged hybrid predictors, 188–190,188,190 tournament predictors, 184–188,186 branch-prediction buffer, C-25, C-25clock cycles per instruction, 168–169 compiler scheduling, M-32 compiler techniques, 176 –182 concepts, 169 –170 control dependences, 174 –176 data dependences, 170 –172 data flow limit, M-34 data hazards, 173 –174 definition, 168 dynamic scheduling, 222 –227 advantages, 191 –192 data hazards, 191 –201 out-of-order execution, 193–194 Tomasulo ’s algorithm, 195–208,205–206 early studies, M-33 –34 exploitation methods, H-21 –23 exploitation of, 2 exploitation statically, H-2 exposing hardware support, H-23 –27 IA-64, H-32 loop unrolling, 177 –182 microarchitectural techniques case study, 266 –273 multiple-issue processors, M-31, 218–227 advantages, 221 –222 challenges, 182, 221 –222 characteristics, 219 dynamically scheduled processor, 222, 224 EPIC approach, 221 microarchitectural techniques case study, 266 –273 speculation, 223 superscalar, 218, 223 VLIW approach, 218 –222,220 multithreading history, M-36 name dependences, 172 –173 pipeline scheduling, 177 –182 scaling, 442 speculation, 222 –227 address aliasing prediction, 239–240 advanced techniques, 228 –240 advantages, 237 –238 challenge issues per clock, 236–237I-20 ■Indexcontrol dependence, 175 –176 disadvantages, 238 energy efficiency, 238 –239 exception handling, 199 execution, 241 hardware vs.software, 240 –241 microarchitectural techniques case study, 266 –273 multiple branches, 238 register renaming vs.ROB, 234–236 static scheduling, 218 –222 TI 320C6x DSP, E-8 Instruction path length, 52 Instruction prefetch, 234 Instruction register (IR) RISC pipeline, C-31 –32 simple RISC implementation, C-27 Instruction set architecture (ISA), 12–17.See also Intel 80x86 processors; Reduced Instruction Set Computer (RISC) byte-addressed computers, A-8 changes, A-46 –47 classes, A-4 classifying, A-3 –6 class of, 12 complications, C-43 –45 computer architecture definition, M-18 –19 Cray X1, G-21 –22 cross-cutting issues, 126 –127 encoding, 14, A-21 –24,A-22 fallacies pitfalls, A-42 –55 first vector computers, M-49 general-purpose register computers, A-6 high-level language computer architecture, M-20 IA-64 instruction formats, H-34 –37, H-39 instructions, H-35 –37 instruction set basics, H-38 overview, H-32 –40 predication speculation, H-38 –40 register model, H-33 –34 memory addressing, A-7 –13 memory total operands, A-5MIPS RISC history, M-20 –23,M-22 stack architectures, M-17 –18 operands, A-4, A-13 –15 operations in, A-15 –16 optimizations impact performance, A-27 performance energy efficiency, 258 Pixel Visual Core, 587 –588 register allocation, A-27 RISC-V, A-34 TPU, 559 virtual machine, 120 –121 virtual machine, 122 –123 Instruction set, extension, 124 –125 Instructions per clock (IPC), 52, 169 Integer arithmetic addition speedup carry-lookahead, J-37 –41 carry-lookahead circuit, J-38 carry-lookahead tree, J-40 carry-lookahead tree adder, J-41 carry-select adder, J-43 –44, J-43–44 carry-skip adder, J-41 –43,J-42 overview, J-37 division radix-2 division, J-55 radix-4 division, J-56 radix-4 SRT division, J-57 single adder, J-54 –57 SRT division, J-45 –47,J-46, J-55–57 FP conversions, J-62 language comparison, J-12 multiplication array multiplier, J-50 Booth recoding, J-49 even/odd array, J-52 many adders, J-50 –54,J-50 multipass array multiplier, J-51 signed-digit addition table, J-54 single adder, J-47 –49, J-48–49 Wallace tree, J-53 multiplication/division, shifting zeros, J-45overflow, J-11 Radix-2 multiplication/division, J-4–7,J-4 restoring/nonrestoring division, J-5, J-6 ripply-carry addition, J-2 –3 signed numbers, J-7 –10 SRT division, J-45 –47,J-46 systems issues, J-10 –13 Integer operations ARM Cortex-A53, 249 data dependences, 171 Itanium 2, H-41 mispeculation, 239 RISC pipeline, C-45 –55 stalls, C-55 static branch prediction, C-22, C-23 Integer registers, IA-64, H-33 –34 Integrated branch prediction, 233 Integrated circuits basics, cell phones, E-24, E-24 cost of, 31 –35 dependability, 36 –38 logic technology, 19 power energy in, 23 –29 Intel 80286, M-9 –10 Intel Core i7, 100 branch prediction, 190 –191 buffers queues, 256 hardware prefetching, 110 Intel Core i7 920 characteristics, 259 clock cycles per instruction, 256, 257 misprediction rate, 192 relative performance energy efficiency, 260 Intel Core i7 6700, 55 –56 clock cycles per instruction, 256, 257 memory hierarchy design, 133–142,134–135 misprediction rate, 192 multiple-issue processors, 247, 252–257 performance, 138 –142, 255 –257 pipeline structure, 254 Intel Core i7 microprocessor die,32 fallacy, 61 floorplan, 32Index ■I-21Intel Core i7 920 multicore computer, 309 Intel Crest, 579, 580 Intel 8087, floating point remainder, J-31 Intel Haswell CPU Roofline, 599 Intel i7, 388, 395 Intel i7 920 performance energy efficiency, 434–437 simultaneous multithreading, 246 Intel i860, M-30, M-32, M-50, M-60 –61 Intel IA-32 architecture call gate, B-53 , B-54 descriptor table, B-52 instruction set complications, C-45 OCNs, F-3 segment descriptor, B-52, B-53 segmented virtual memory, B-51 –54 Intel IA-64 architecture compiler scheduling history, M-32 –33 conditional instructions, H-27 explicit parallelism, H-34 –37 historical background, M-33 ISA instruction formats, H-34 –37, H-39 instructions, H-35 –37 instruction set basics, H-38 overview, H-32 –40 predication speculation, H-38 –40 Itanium 2 processor instruction latency, H-41 overview, H-40 –41 performance, H-43, H-43 parallelism exploitation statically, H-2 register model, H-33 –34 RISC history, M-23 software pipelining, H-14 –15 synchronization history, M-64 –65 Intel iPSC 860, M-60 –61 Intel Itanium, 168 instruction-level parallelism, 261–262 sparse matrices, G-13 speculation, 241Intel Itanium 2 IA-64 functional units instruction issue, H-41 –43 instruction latency, H-41 overview, H-40 –41 performance, H-43, H-43 Intellectual property, DSAs, 593 Intelligent devices, historical background, M-88 Intel Paragon, M-60 –61, F-96 Intel Pentium 4, 110, 261 –262 Extreme, M-35 Itanium 2 comparison, H-43 multithreading history, M-36 Intel Pentium II, M-35 Intel Pentium III, power consumption, F-89 Intel Pentium M, F-89 Intel Pentium MMX, multimedia support, E-11 Intel Pentium Pro, M-35 Intel Pentium processors early computer arithmetic, J-65 vs.Opteron memory protection, B-57 segmented virtual memory, B-51 –54 Intel processor, 261 instruction set extensions, 125 multiple processors, 5 power consumption, F-89 Intel Teraflops processors, OCNs, F-3 Intel Thunder Tiger 4 QsNetII, F-67 Intel 80x86 comparative operation measurements, K-45 –48 floating-point operations, K-38 –40 instruction encoding, K-40 –43 integer operations, K-35 –37 measurements instruction set usage, K-44 –48 operand addressing, K-44 –45 processors address space, B-58 integer overflow, J-11 memory hierarchy development, M-9 –10 protection structure, B-50 registers data addressing modes, K-32 –35SPECint92 programs, K-44 ,K-46 , K-48 –50 Intel x86, conditional instructions, H-27 Intel Xeon, F-80, 354, 387 Intel Xeon E7, 426 Interactive workloads, 467 Interarrival times, D-30 Interconnection networks adaptive routing, F-97 adaptive routing fault tolerance, F-98 arbitration, F-49 –51 basic characteristics, F-2, F-21 bisection bandwidth, F-93 commercial congestion management, F-68–70 connectivity, F-67 cross-company interoperability, F-67–68 DECstation 5000 reboots, F-73 fault tolerance, F-70 –72 communication bandwidth, I-3 compute-optimized processors vs. receiver overhead, F-92 definition, F-2 density- vs.SPEC-optimized processors, F-89 device example, F-3 direct vs.high-dimensional, F-96 domains, F-3 –4,F-3 Ethernet, F-82 –84 examples, F-73 –85 HOL blocking, F-59 –61 IBM Blue Gene/L, I-43 InfiniBand, F-77 –81 LAN, F-82 –84 link bandwidth, F-94 memory hierarchy interface, F-91–92 mesh network routing, F-47 MIN vs.direct network costs, F-96 multi-device connections basic considerations, F-20 –21 effective bandwidth vs.nodes, F-29 latency vs.nodes, F-28 performance characterization, F-25–30I-22 ■Indexshared-media networks, F-23 –24 shared- vs.switched-media networks, F-23, F-25 switched-media networks, F-24–25 topology, routing, arbitration, switching, F-21 –22 OCN, F-27 –29 protection, F-91 routing, F-22, F-44 –56 routing/arbitration/switching impact, F-21 –22 SAN characteristics, F-27 –29 software overhead, F-96 storage area networks, F-106 –108 switching, F-51 –52 switch microarchitecture, F-56 –66 switch vs.NIC, F-90 system area networks, F-104 –106 system/storage area network, F-77–81 top-level architecture, F-75 topology, F-30 –44 two-device interconnections basic considerations, F-6 effective bandwidth vs.packet size,F-19 example, F-6 interface functions, F-6 –9 performance, F-13 –20 structure functions, F-9 –12 virtual channels throughput, F-47–48 WAN, F-84 –85 wormhole switching performance, F-52 zero-copy protocols, F-95 Intermittent faults, D-11 Internal fragmentation, B-47 International Computer Architecture Symposium (ISCA), M-12 International Mobile Telephony 2000 (IMT-2000), cell phone standards, E-25 International Technology Roadmap Semiconductors (ITRS), 58–59,59 Internet Archive Cluster, D-36 –41 containers, M-76 Internet Things (IoT), 6 –7Internet Protocol (IP), F-85 –89 cores, OCNs, F-3 routers, VOQs, F-31, F-103 Internetworking, F-2, F-84, F-85 –89 Interprocedural analysis, H-10 Interprocessor communication, large- scale multiprocessors, I-3–6 Interrupt Enable (IE) flag, 127 Invalidate protocol, 380 example, 385, 385 implementation, 382 –383 snooping coherence, 381 Invalid exception, floating-point arithmetic, J-35 Inverted page table, B-44 –45 I/O bandwidth, D-15 –16 I/O benchmarks, response time restrictions, D-18 I/O-bound, 121, 123 –124 I/O bus historical background, M-88 –89 point-to-point replacement, D-34 Sony PlayStation 2 Emotion Engine case study, E-15 I/O cache coherency, 128 –129 I/O devices address translation, B-38 historical background, M-88 –89 performance, D-15 –23 SANs, F-4 shared-media networks, F-23 switched networks, F-2 switch vs.NIC, F-90 write strategy, B-11 I/O interfaces, storage area network history, F-107 I/O network, F-67 I/O processor (IOP) first dynamic scheduling, M-28 –29 Sony PlayStation 2 Emotion Engine case study, E-15 I/O subsystems design, D-59 –61 interconnection network speed, F-92 vs.NIC, F-95 zero-copy protocols, F-95 I/O systems asynchronous, D-35 –36 black box, D-24 dirty bits, D-61 –64multithreading history, M-35 queing theory, D-23 queue calculations, D-29 random variable distribution, D-26 IP block, DSAs, 593IPC. SeeInstructions per clock (IPC) IPoIB, F-81 IR.SeeInstruction register (IR) ISA. SeeInstruction set architecture (ISA) iSCSI NetApp FAS6000 filer, D-41 –42 storage area network, F-106 –107 ISPs. SeeImage signal processors (ISPs) Issue logic, 236 Issue stage ID pipe stage, 194 instruction step, 197, 211 Iterative division, floating point, J-27–31 J Java benchmark, 435 –437,435,437 Java language, dependence analysis, H-10 Java Virtual Machine (JVM), early stack architectures, M-18 Johnson, Reynold B., M-85 Jumps, VAX, K-57 Just-in-time (JIT), M-18 K Kahle, Brewster, M-76 Kahn, Robert, F-102 k-ary n-cubes, F-38 –39 Kendall Square Research KSR-1, M-61 –62 Kernels EEMBC benchmarks, E-12 FFT, I-7 FORTRAN, compiler vectorization, G-15 LU, I-8 process, 40 DAG of, 582 Driver, 563 segmented virtual memory, B-51Index ■I-23Kernels (Continued) throughput computing, 350 vector kernel implementation, 357–359 virtual memory, 119 L LabVIEW, embedded benchmarks, E-13 Lampson, Butler, F-103 Large-scale multiprocessors cache coherence implementation, I-34–35 deadlock buffering, I-38 –40 directory controller, I-40 –41 DSM multiprocessor, I-36 –37 characteristics, I-45 cluster history, M-63 –64 example calculations, I-12 –13 historical background, M-60 –62 IBM Blue Gene/L, I-41 –44, I-43–44 interprocessor communication, I-3 –6 parallel programming, I-2 scientific applications, I-6 –12 distributed-memory multiprocessors, I-26–32,I-28–32 parallel processing, I-33 –34 symmetric shared-memory multiprocessor, I-21 –26, I-23–26 space relation classes, I-46 synchronization mechanisms, I-17 –21 performance, I-12 –16 Latency, 20. See also Response time ALU, C-46 –48 bandwidth, F-25 –30 barrier synchronization, I-16 cache miss, B-2 cluster history, M-74 communication mechanism, I-3 definition, D-15 –16, C-46 –48 deterministic vs.adaptive routing, F-53 –56 distributed-memory multiprocessors, I-30, I-32 Flash memory, D-3 FP operations, C-61 , C-63, 177functional units, C-47 hazards forwarding, C-49 –52 interconnection networks, F-13 –20 Itanium 2 instructions, H-41 microarchitectural techniques case study, 266 –273 OCNs vs.SANs, F-28 out-of-order execution, B-20 –21 packets, F-13, F-13 performance trends, 20, 21 snooping cache coherence, 447, 448 Sony PlayStation 2 Emotion Engine, E-17 throughput vs.response time, D-16 utility computing, M-75 vector memory systems, G-9 vector start-up, G-8 Latency-hiding techniques, 418 L1 cache. See also First-level caches address translation, B-46 Alpha 21164, 395 ARM Cortex-A53, 251 –252 data cache size, 402 first-level caches, 95 –98 inclusion, 423 –424 Intel i7, 395 memory hierarchy, B-39 , B-48 –49, B-48 miss rate, 402 Opteron memory, B-57 L2 cache. See also Second-level caches Alpha 21164, 395 ARM Cortex-A53, 251 –252 cache optimization, B-34 , B-35 IBM Blue Gene/L, I-42 inclusion, 423 –424 Intel i7, 395 memory hierarchy, B-39 , B-48 –49, B-48 ,B-57 memory system, 241 L3 cache. See also Third-level caches Alpha 21164, 395 IBM Blue Gene/L, I-42 IBM Power8, 371 IBM Power processors, 265 inclusion, 424 Intel i7, 395 memory access cycle shift, 396–397,397 miss rate, 397 –399,398 snoop bandwidth, 390Learning curve, 30 Least common ancestor (LCA), F-48–49 Least recently used (LRU) AMD Opteron data cache, B-14 block replacement, B-9 –10,B-10 memory hierarchy history, M-11 virtual memory block replacement, B-45 Limit field, B-52 Linear speedup, multiprocessor, 438–439,440 Line Buffer Pool (LBP), 590 Line locking, embedded systems, E-4 Line, memory hierarchy, 81 Link injection bandwidth calculation, F-17 interconnection networks, F-26 –27 Link pipelining, F-16 –17 Link reception bandwidth, calculation, F-17 Link register, K-25 Linpack benchmark cluster history, M-64 parallel processing debates, M-59 VMIPS performance, G-17 –19 Linux operating system, RAID benchmarks, D-22 Liquid crystal display (LCD), Sanyo VPC-SX500 digital camera, E-19 LISP, K-21 –22 RISC history, M-20 –21 Little Endian byte, A-7 interconnection networks, F-12 Little ’s Law, 328 definition, D-24 server utilization calculation, D-29 Livelock, network routing, F-45 –46 Liveness, control dependences, 176 Livermore Fortran Kernels, M-6 Load instruction control dependence, 175 data hazards requiring stalls, C-17, C-17 RISC instruction set, C-5 Load interlock, C-33 –34,C-35 Load memory data (LMD), C-28 –29 Load reserved, 413 –414, 416 Loads instruction, 199I-24 ■IndexLoad stalls, C-61, C-64 Load-store architecture, A-3 Load-store instruction set architecture, C-5, 12, C-28 RISC history, M-20 Local address space, B-52 Local area networks (LANs) characteristics, F-4 cross-company interoperability, F-67–68 effective bandwidth, F-18 –19 Ethernet as, F-82 fault tolerance calculations, F-72 InfiniBand, F-77 –78 interconnection network domain relationship, F-4, F-5 latency effective bandwidth, F-27–29 offload engines, F-8 packet latency, F-13 –16,F-13 shared-media networks, F-23 time flight, F-14 Local memory centralized shared-memory architectures, 377 multiprocessor architecture, 371–373 NVIDIA GPU memory structures, 326–327 Local miss rate, B-31 Local node, 406 Local optimizations, A-26 Local predictors, 184 –188 Local scheduling techniques, 219 –220 Local state, 377 Location counts, WSCs, 468 Locks caching, 415 large-scale multiprocessor synchronization, I-18 –21 spin, 414 –416 using coherence, 414 –417,416 Lockup-free cache, 100 –104 Logical units, storage systems, D-34 Logical volumes, D-34 Long displacement addressing, K-52 Long Instruction Word (LIW) EPIC approach, M-33 multiple-issue processor development, M-30, M-32Long mode, K-32 Long short-term memory (LSTM) cells, 553, 554–555 Loop branches prediction, 232 –234 Loop-carried dependence, 289, 312, 337–339 dependence distance, H-6 example calculations, H-4 –5 loop-level parallelism, H-3 recurrence form, H-5 Loop interchange, cache optimization, 107 Loop-level parallelism, H-2 –12 analysis, 337 –339 CUDA/NVIDIA term, 337–338 definition, 169 –170 dependent computations, 344 –345 detection enhancement, H-2–12 dependence analysis, H-6 –10 dependent computation elimination, H-10 –12 finding dependences, 339 –344 history, M-32 –33 SIMD, 170 Loop stream detection, 254Loop unrolling limitation, 181 scheduling, 181 –182 software pipelining, H-12 –15, H-13 ,H-15 Lossless networks definition, F-12 switch buffer organizations, F-59–60 Lossy networks, definition, F-12 LRU. SeeLeast recently used (LRU) LU kernel characteristics, I-8 distributed-memory multiprocessor, I-32 symmetric shared-memory multiprocessors, I-22, I-23, I-25 –26 MAC. SeeMultiply-accumulate (MAC) Machine language programmer, M-18 Machine learning, 546, 573 Machine memory, virtual machine, 123Macro-op fusion, 253 Magnetic disk technology, 19 Magnetic storage access time, D-3 cost vs.access time, D-3 historical background, M-85 –86 Mail servers, D-20 –21 Main memory, B-2 –3, 377, 400 block identification, B-44 –45 block placement, B-44 cache function, B-2 vector processor, G-25 vs.virtual memory, B-41 write strategy, B-45 –46 MapReduce AWS, 495, 495 WSCs, 471 –476,472 Mark-I, M-3 –4, M-6 Mark-II, M-3 –4 Mark-III, M-3 –4 Mark-IV, M-3 –4 MasPar, M-46, M-56 Massively parallel processors (MPPs) characteristics, I-45 cluster history, M-62 –63, M-74 system area network, F-104 Matrix300 kernel, prediction buffer, C-25 Matrix multiplication, LU kernel, I-8 Matrix multiply unit, 557 –558,558,562 Mauchly, John, M-2 –3, M-5, M-20 Maximum transfer unit, network interfaces, F-8 McCreight, Ed, F-103 McFarling ’s gshare predictor, 184 MCP operating system, M-17 –18 Mean time failures (MTBF), 37 Mean time failure (MTTF) computer system power consumption case study, 69–71 dependability, 37 –38 benchmarks, D-21 disk arrays, D-6 fallacy, 62 I/O subsystem design, D-59 –61 RAID, M-86 –87 RAID reconstruction, D-55 –57 TB-80 cluster, D-41 WSCs, 468Index ■I-25Mean time repair (MTTR), 37 dependability benchmarks, D-21 disk arrays, D-6 RAID 6, D-8 –9 RAID reconstruction, D-55 –56 Mean time data loss (MTDL), D-55 –57 Media extensions, DSPs, E-10 –11 Media, interconnection networks, F-9–12 Mellanox MHEA28-XT, F-80 Memory access ALUs, data forwarding, C-36 –37 cache hit calculation, B-5 Cray Research T3D, F-91, F-91 cross-cutting issues, 127 –128 data hazards stall minimization, C-14, C-16 distributed-memory multiprocessor, I-32 exception stopping/restarting, C-41 instruction set complications, C-43 –44 integrated instruction fetch units, 234 longer latency pipelines, C-51MIPS R4000 pipeline, C-59 multicycle FP operations, C-52 RISC classic pipeline, C-8 RISC exception, C-42 –43, C-43 RISC instruction set, C-5, C-6 RISC pipeline, C-32 –35,C-36 simple RISC implementation, C-28 vector architectures, G-10 Memory addressing, 13 addressing modes, A-8 –11,A-10 compiler-based speculation, H-32 displacement addressing mode, A-11 –12 immediate/literal, A-12, A-12 interpreting, A-7 –8 vector architectures, G-10 Memory bandwidth, 350, 356 Memory banks. See also Banked memory example, 301 vector architecture, 298 –299 vector systems, G-9 –11 Memory bus (M-bus), 377 interconnection networks, F-91–92Memory consistency, 377 –378, 417– 422 case study, 456 –458 compiler optimization, 422 development models, M-64 –65 programmer ’s view, 418 –419 relaxed consistency models, 419–422,421 sequential consistency, 417 speculation hide latency, 422–423 Memory-constrained scaling, I-33 –34 Memory hierarchy, F-91 –92 address space, B-57 block identification, B-8 –9 block placement, B-7 –8,B-7 block replacement, B-9, B-10 cache optimizations, B-22 –40,B-40 hit time reduction, B-36 –40 miss categories, B-23 –25 miss penalty reduction, B-30 –36 miss rate reduction, B-26 –30, B-27 cache performance, B-3 –6, B-15 –16,B-40 average memory access time, B-17 –20 equations, B-22 example calculation, B-16 –17 miss penalty, B-20 –21 out-of-order execution, B-20 –21 development, M-9 –12 levels slow down, B-3 Opteron data cache example, B-12 –15,B-13 ,B-15 Opteron L1/L2, B-57 OS page size, B-58 questions, B-6 –12 terminology, B-2 virtual address L2 cache, B-39 virtual memory, B-2 –3, B-40 –49 address space, B-12, B-41 , B-44, B-55 address translation, B-46, B-47 caches and, B-42 –43,B-42 , B-48 –49,B-48 classes, B-43 Intel Pentium vs.AMD Opteron, B-57paged example, B-54 –57 page size selection, B-46 –47 parameter ranges, B-42 Pentium vs.Opteron protection, B-57 protection, B-49 –50 questions, B-44 –46 segmented example, B-51 –54 write strategy, B-45 –46 WSCs, 479 –482,479 Memory hierarchy design ARM Cortex-A53, 129 –131,130 basics of, 81 –84 cache optimization advancement, 117 cache misses, 112 –113 compiler-controlled prefetching, 111 –114 compiler optimizations, 107–109 critical word first, 104 –105 early restart, 104 –105 energy consumption, 97 floating-point programs, 101–102 hardware prefetching, 109 –111 HBM packaging, 114 –117 hit time/power reduction, 95 –98 multibanked caches, 99 –100 nonblocking caches, 100 –104 pipelined access, 99 –100 way prediction, 98 –99 write buffer merging, 105 –106, 106 Cortex-A53 performance, 132 C program evaluation, 151 cross-cutting issues autonomous instruction fetch units, 127 case study, 150 –153 coherency cached data, 128–129 protection, virtualization, instruction set architecture, 126 –127 special instruction caches, 128 speculation memory access, 127–128 Intel Core i7 6700, 133 –142,134 personal mobile device, 79 pitfall, 143I-26 ■Indextechnology optimizations dependability, 93 –94 DRAM technology, 85 –87 Flash memory, 92 –93 GDRAMs, 90 phase-change memory technology, 93 SRAM technology, 85 stacked/embedded DRAMs, 91 synchronous DRAM, 87 –90 virtual machine hardware management, 121 impact virtual memory, 123–124 instruction set architecture for, 122–123 protection via, 120 –122 software management, 121 virtual machine monitor instruction set extension, 124–125 requirements, 122 Xen virtual machine, 126 virtual memory instruction set extension, 124–125 protection via, 119 –120 virtual machines impact on, 123–124 Memory latency, 85 Memoryless, D-27 –28 Memory mapping, B-52 Memory-memory architecture, A-3 Memory protection exception, 175 Pentium vs.Opteron, B-57 processes, B-49 –50 safe calls, B-54 segmented virtual memory, B-51 –54 Memory stall cycles average memory access time, B-18 definition, B-3 –4,B-22 miss rate calculation, B-6 out-of-order execution, B-20 Memory system, 377 –378 coherency, 378 Intel Core i7 6700 pipeline structure, 254 multiprocessor architecture, 369, 371–373page size changes, B-58 speculative execution, 241 vector architectures, G-9 –11 vector chaining, G-11 virtual ( seeVirtual memory) Memristor, 93 Mesh interface unit (MIU), F-74 Mesh network, F-43, F-47 Mesh topology, F-74 MESIF protocol, 388 MESI protocol, 388, 449 Message ID, packet header, F-8, F-17 Message-passing communication advantages, I-5 –6 historical background, M-60 –61 Message Passing Interface (MPI) function, F-8 InfiniBand, F-80 –81 lack shared-memory multiprocessors, I-5 Message-passing protocols, 373 Messages adaptive routing, F-64 interconnection networks, F-6–9 zero-copy protocols, F-95 MFLOPS. SeeMillions floating- point operations per second (MFLOPS) Microarchitecture, 17, 272 case study, 266 –273 Cray X1, G-21 –22 OCNs, F-3 TPU, 559 –560 Microbenchmarks disk array deconstruction, D-51 –54 disk deconstruction, D-48 –50 Microinstructions, complications, C-45 MicroMIPS64, K-3 16-bit instruction, K-7,K-10 ,K-17 register encodings, K-7 Micro-op decode, 253 Micro-op fusion, ALU, 254 Microprocessor AMD Opteron, 27 clock rate, 26 design typical case, 27 nothing well, 27 energy power within, 25 –28growth rate, 2, 3 inside disks, D-4 performance milestones, 22 recent advances, M-35 VAX 11/780, 3 Microprocessor without Interlocked Pipeline Stages (MIPS) early pipelined CPUs, M-28 multiple-issue processor, M-30 performance measurement history, M-6–7 RISC history, M-20 Microsoft availability zones, 499 containers, M-76 Microsoft Azure, M-75 –76 Microsoft Catapult Bing search engine, 573board design, 568 CNNs on, 570 –572,571–572 evaluating, 601 –602 guidelines, 577 –579 implementation architecture, 568–569 search acceleration on, 573 –574 software, 569version 1 deployment, 574 version 2 deployment, 575 –577, 576–577 Microsoft ’s DirectX 8, M-51 –52 Microsoft Windows, RAID benchmarks, D-22 Microsoft XBox, M-51 –52 Migration, 379 Million instructions per second (MIPS) conditional instructions, H-27 embedded systems, E-15 MIPS16, K-4, K-5 MIPS M2000, M-22, M-22 MIPS M2000 vs.VAX 8700, M-22 MIPS64 R6, K-19, K-19 MIPS R2000, M-21 MIPS R10000, 423 MIPS R4000 pipeline, C-55 –64 floating-point operations, C-60 –61,C-60 performance, C-61 –64 RISC instruction set, C-3 –4 Sony PlayStation 2 Emotion Engine, E-17Index ■I-27Millions floating-point operations per second (MFLOPS) early performance measures, M-7 parallel processing debates, M-58 –59 SIMD computer history, M-56 SIMD supercomputer development, M-46 vector performance measures, G-15 –16 Minibatches, DNNs, 556 Minicomputers, 4 MIPS. SeeMillion instructions per second (MIPS) MIPS R3000 integer division/remainder, J-11–12 integer overflow, J-11 MIPS R3010 arithmetic functions, J-57 –61 chip comparison, J-58 chip layout, J-59–60 floating-point exceptions, J-35 MIPS R4000, early pipelined CPUs, M-28 Misprediction rate ARM Cortex-A53, 250 branch-prediction buffers, C-25 profile-based predictor, C-23 SPEC89 benchmark, C-26 SPEC89 vs.predictor size, 187 static branch prediction, C-22, C-23 tagged hybrid vs.gshare predictors, 190 Misses per instruction advantage, B-6 application/OS statistics, B-59 block size, 397 –399,398 cache performance, B-5 –6 memory hierarchy design, 82 Miss penalty, B-20 –21 cache optimization, B-30 –36, 105–106,106 compiler-controlled prefetching, 111–114 critical word first, 104 –105 early restart, 104 –105 hardware prefetching, 109 –111 memory hierarchy design, 82 multilevel caches reducing, 83 reducing, 95reduction via multilevel caches, B-30 –35 write buffers, 83 Miss rate AMD Opteron example, B-15 average memory access time, B-29 –30,B-30 bigger caches reducing, 83 cache optimization associativity, B-28 –30 block size, B-26 –28,B-27 cache size, B-24 –25, B-28, B-33 ,B-37 virtually addressed cache size,B-37 cache performance, B-16 –17 cache size, B-24 –25, B-28, B-33 ,B-37 compiler-controlled prefetching, 111–114 data, B-16 data cache size, 402 early IBM computers, M-11 example calculations, B-6 formula, B-16 global, B-31hardware prefetching, 109 –111 higher associativity reducing, 83 instruction, B-16 larger blocks reducing, 82 L1 caches, 402 L3 caches, 397 –399,398 local, B-31 measurement, B-4 –5 memory hierarchy, 81 memory stall clock cycles, B-4 reducing, 95 scientific workloads distributed-memory multiprocessors, I-28–32 symmetric shared-memory multiprocessors, I-22, I-23–25 total distribution, B-25 unified, B-16 Miss Status Handling Registers (MSHRs), 104 MIT Raw, F-78 Mixed cache, B-15 Mixer, radio receiver, E-23M/M/1 model, D-30, D-32, D-57 M/M/2 model, D-57 Modified, shared, invalid (MSI) protocol, 388 Modified state, 383 –384, 406 large-scale multiprocessor cache coherence, I-35 –36 Modula-3, integer division/remainder, J-12 Module availability, 37 Module reliability, 37 MOESI, 388 Moore ’s Law, 19 interconnection networks, F-74 pitfall, 58 point-to-point links switches, D-34 RISC history, M-23 semiconductor manufacturing, 4 switch size, F-29 transistors, 5, 540 Motion JPEG encoder, Sanyo VPC- SX500 digital camera, E-19 Motorola 68882, floating-point precisions, J-33 Motorola 68000, memory protection, M-10 Move address, K-55 –56 MPEG multimedia SIMD extensions history, M-50 Sanyo VPC-SX500 digital camera, E-19 Sony PlayStation 2 Emotion Engine, E-17 MPPs. SeeMassively parallel processors (MPPs) MTTF. SeeMean time failure (MTTF) MTTR. SeeMean time repair (MTTR) Multibanked caches, 99 –100 Multichip modules, OCNs, F-3 Multicomputers, 370 cluster history, M-65 definition, M-59 historical background, M-64 –65 Multicore processor, 17, 369, 371 –372, 382, 408 approaches, 389I-28 ■Indexarchitecture, 430 coherence, 387 Cray X1E, G-24 development, 404 DSM, 373,405,452 Intel i7 performance energy efficiency, 434 –437 multiprogrammed workload, 426–432 OCN, F-101 performance, 426 –437,432 point-to-point, 446 scaling, 432, 442 –444 single chip, 382, 391, 446 –451 SMT, 436 –437 Multics protection software, M-9 –10 Multicycle operations, RISC pipeline, C-45 –49 FP pipeline performance, C-55 hazards forwarding, C-49 –52 maintaining precise exceptions, C-53 –55 Multidimensional arrays, 299 –301 Multiflow processor, M-31 –33 Multigrid methods, ocean application, I-9–10 Multilayer perceptrons (MLPs), 549 –550 Multilevel caches centralized shared-memory architectures, 377 memory hierarchy history, M-12 miss penalty reduction, B-30 –35, B-33 write process, B-11 Multilevel exclusion, B-35 Multilevel inclusion, B-34, 423 memory hierarchy history, M-12 Multimedia applications, desktop processor support, E-11 Multimedia Extensions (MMX), K-31, M-49 –50 Multimedia instruction sets, 10 Multimedia SIMD extensions, M-49 –50 DSPs, E-11 Multimode fiber, interconnection networks, F-9 –10 Multipass array multiplier, J-51 Multiple instruction streams, multiple data streams (MIMD), 11, 282, 369 –370, 438–439early computers, M-57 first vector computers, M-49 multimedia SIMD GPU vs., 347–353 Multiple instruction streams, single data stream (MISD), 11 Multiple-issue processors advantages, 221 –222 challenges, 182, 221 –222 characteristics, 219 dynamically scheduled processor, 222, 224 early development, M-29 –32 EPIC approach, 221 instruction-level parallelism, 218–227 microarchitectural techniques case study, 266 –273 speculation, 223 superscalar, 218, 223 VLIW approach, 218 –222,220 Multiple lanes technique vector architecture, 293 –294,294 vector performance, G-7 –9 Multiple-precision addition, J-13 Multiply-accumulate (MAC), 589 DSP, E-5 TI TMS320C55 DSP, E-8 Multiply operations chip comparison, J-61 floating point denormals, J-20 –21 examples, J-19 multiplication, J-17 –21 overview, J-17 –20 precision, J-21 rounding, J-18, J-19 integer arithmetic array multiplier, J-50 Booth recoding, J-49 even/odd array, J-52 issues, J-11 many adders, J-50 –54,J-50 multipass array multiplier, J-51 n-bit unsigned integers, J-4 Radix-2, J-4 –7 signed-digit addition table, J-54 single adder, J-47 –49, J-48–49 Wallace tree, J-53 integer shifting zeros, J-45Multiprocessor application, 373 –374 architecture issues approach, 370–373 bus-based coherent, M-59 –60 cache coherence, 377 –379 cluster history, M-62 –65 coining term, M-59 definition, 369 early computers, M-57 early machines, M-57 embedded systems, E-14 –15 execution time, 438 factors, 368 fallacy, 60 Intel, 5 large-scale ( seeLarge-scale multiprocessors) linear speedup, 438 –439,440 parallel processing challenges, 373–377 parallel processing debates, M-57 –59 performance gains, 424 –426 processor performance, 371 –372 recent advances developments, M-59 –60 shared-memory ( seeShared- memory multiprocessors (SMPs)) SIMD computers, M-55 –57 synchronization consistency models, M-64 –65 Xeon E7 MP scalability, 433 –434 Multiprogramming, 369 virtual memory, B-49, 119 workload, 399 –404, 426 –432 Multistage interconnection networks (MINs) bidirectional, F-34 crossbar switch calculations, F-32 –33 vs.direct network costs, F-96 topology, F-32 Multistage switch fabrics, F-31 Multi-Streaming Processor (MSP) Cray X1, G-21 –24,G-22 Cray X1E, G-24 first vector computers, M-49 Multithreaded SIMD Processor, 311, 315,317Index ■I-29Multithreading, 369 hardware approaches, 243 –244 historical background, M-35 –36 instruction-level parallelism, 242–247 parallel benchmarks, 247 performance gains, 424 –426 simultaneous ( seeSimultaneous multithreading (SMT)) speedup from, 248 superscalar processors, 245 –247 MVAPICH, F-81, F-81 M-way set associative, B-8 MXP processor, components, E-14 –15 Myrinet SAN, F-49, M-63, M-74, F-90, F-90 N NAK. SeeNegative acknowledge (NAK) Name dependences instruction-level parallelism, 172–173 register renaming, 196 Nameplate power rating, 482 NaN. SeeNot Number (NaN) NASA Ames Research Center, M-46, M-65 NAS parallel benchmarks, vector processor history, G-27 –28 National Science Foundation, F-102 Natural parallelism embedded systems, E-15 n-bit adder, carry-lookahead, J-38 n-bit number representation, J-7 –10 n-bit unsigned integers division, J-4 N-body algorithms, Barnes application, I-8–9 NBS DYSEAC, M-89 N-cube topology, F-36 –38 NEC SX/2, M-48 NEC SX/5, M-48 –49 NEC SX/6, M-48 –49 NEC SX-8, M-48 –49 NEC SX-9, M-49 first vector computers, M-49 NEC VR 4122, embedded benchmarks, E-13 Negative acknowledge (NAK) cache coherence, I-39directory controller, I-41 DSM multiprocessor cache coherence, I-37 Negative-first routing, F-48 Nested page tables, 146 Netscape, F-102 Network Appliance (NetApp), D-9, D-41 –43 Network-attached storage (NAS), M-88, 478 Network bandwidth, interconnection network, F-18 Network-Based Computer Laboratory (Ohio State), F-80 –81 Network buffers, network interfaces, F-8 Network fabric, F-24 –25 Network File System (NFS) benchmarking, D-20, D-20 block servers vs.filers, D-35 interconnection networks, F-93 –94 TCP/IP, F-86 Networking Google WSCs, 510 –511 performance milestones, 22 Network injection bandwidth interconnection network, F-19 –20 multi-device interconnection networks, F-26 Network interface fault tolerance, F-67 functions, F-6 message composition/processing, F-6–9 Network interface card (NIC) functions, F-8 I/O subsystem, F-95 vs.switches, F-90, F-90 zero-copy protocols, F-95 Network I/O, 467 Network layer, F-84 Network nodes, F-23, F-35, F-36 distributed switched networks, F-35 Network Workstations, M-63, M-74 –75 Network chip (NoC), F-3 Network ports, F-30 Network protocol layer, F-10 Network reception bandwidth, F-19–20 Network reconfiguration, F-70 –71Network technology, 20 personal computers, F-2 Newton ’s iteration, J-27 –30,J-28 Nicely, Thomas, J-65 Nodes communication bandwidth, I-3 direct network topology, F-37 distributed switched networks, F-35–40 IBM Blue Gene/L, I-42 –44,I-43 IBM Blue Gene/L 3D torus network, F-76 network topology performance costs, F-40 points-to analysis, H-9 Nokia cell phone, circuit board, E-24, E-24 Nonatomic operations, cache coherence, 386 Nonbinding prefetch, 111 Nonblocking caches cache bandwidth, 100 –104 case study, 148 –164 effectiveness, 102 implementing, 103 –104 memory hierarchy history, M-11 –12 Nonblocking crossbar, F-33 Nonfaulting prefetches, 111 Nonoptimal replacement algorithm, M-11 Non-overlapped latency, B-20 Nonrecurring engineering (NRE) costs, 542 Nonrestoring division, J-5, J-6 Nonuniform cache access (NUCA), 371, 390, 426 Nonuniform memory access (NUMA), 372–373, 391, 426 –429 large-scale multiprocessors history, M-61 Non-unit strides, 300 vector processor, G-25 North-last routing, F-48 Number (NaN), J-14 –16, J-21, J-34–35 Notifications, interconnection networks, F-10 project, M-74 –75 No-write allocate, B-11 –12 NSFNET, F-102I-30 ■IndexNTSC/PAL encoder, Sanyo VPC- SX500 digital camera, E-19 NUMA. SeeNonuniform memory access (NUMA) NVIDIA GeForce, M-51 –52 NVIDIA K80 GPU die Roofline, 599 NVIDIA system computational structures, 313 –320 GPU computational structures, 313–320 GPU computing history, M-52 –53 GPU instruction set architecture, 320–323 GPU memory structures, 326 –328, 327 graphics pipeline history, M-51 –52 instruction set architecture, 320–323 NVIDIA P100, 354 scalable GPUs, M-51 Tegra Parker system vs.Core i7, 346–353 N-way set associative, B-8 conflict misses, B-23 memory hierarchy, 81TLB, B-49 NYU Ultracomputer, M-61 Occupancy, communication bandwidth, I-3 Ocean application characteristics, I-9 –10 distributed-memory multiprocessor, I-30,I-32 example calculations, I-11 –12 miss rates, I-28 symmetric shared-memory multiprocessors, I-23 Offline reconstruction, RAID, D-55 Offload engines network interfaces, F-8 TCP/IP reliance, F-100 Offset, B-8 –9 address, B-55 –56 block identification, B-8 –9 cache optimization, B-38 destination, IA-32 segment, B-53 IA-32 segment, B-53 Opteron data cache, B-13 , B-14page, B-38 sign-extended, C-5 virtual memory, B-43 –44, B-46, B-55 –56 word, C-28 OLTP. SeeOnline transaction processing (OLTP) Omega, F-31, F-32 –33 On-chip memory, embedded systems, E-4 On-chip networks (OCNs) basic considerations, F-3 commercial implementations, F-73–74 commercial interconnection networks, F-63 cross-company interoperability, F-68 effective bandwidth, F-19 interconnection network domain relationship, F-4, F-5 latency vs.nodes, F-28 packet latency, F-13, F-14 –16 time flight, F-14 topology, F-30 wormhole switching, F-52 One’s complement, J-7 –8 One-way set associativity, conflict misses, B-23 Online reconstruction, RAID, D-55 Online transaction processing (OLTP), 44, 395 –396,396, 401 storage system benchmarks, D-18 Opcode, A-21 OpenGL, M-51 –53 Open instruction set, DSAs, 594 Open Systems Interconnect (OSI) Ethernet, F-86 layers, F-84 Operand delivery stage, Itanium 2, H-42 Operands DSP, E-6 instruction set architecture, A-4, A-13 –15 read, C-68 dynamic scheduling pipelines, C-66 ID pipe stage, 194 TMS320C55 DSP, E-6 type size, A-13 –15Operating systems (general) address translation, B-38 communication performance, F-8 disk access scheduling, D-44 memory protection performance, B-58 miss statistics, B-59 page size, B-58 segmented virtual memory, B-54 storage systems, D-35 –36 vendor-independent, 2 workload, 399 –404 Operational costs count, 468 Operational expenditures (OPEX), 36, 486–490,488 Operation faults, D-11 Operations, 14 See also specific types operations atomic, 386 instruction set, A-15 –16,A-15 Operator dependability, disks, D-13 –15 Opteron Data Cache, B-11 Optical media, interconnection networks, F-9 –10 Oracle database, miss statistics, B-59 Ordering, deadlock, F-47 –48 Organizations, 17 block placement, B-7 –8,B-7 buffer, F-59 –61 data dependences, 172 designing, 17 –18 dynamic random-access memory (DRAM), 86 on-chip IBM Power8, 428 Xeon E7, 428 Opteron data cache, B-12 –15,B-13 Sony PlayStation Emotion Engine, E-18 SPEC benchmarks, 433 –434 Out-of-order completion definition, C-53 dynamic scheduling pipelines, C-66 Out-of-order execution cache miss, B-2 dynamic scheduling, 193 –194 dynamic scheduling pipelines, C-66 memory hierarchy, B-2 microarchitectural techniques case study, 266 –273Index ■I-31Out-of-order execution (Continued) miss penalty, B-20 –21 Tomasulo ’s scheme, 208 Out-of-order processors, memory hierarchy history, M-12 Output buffered switch, F-56, F-58, F-61, F-65 Output dependence, 173 compiler history, M-32 finding, H-7 –8 Output gate, 553 Overclocking, 28 Overflow, integer arithmetic, J-8, J-10–11,J-11 Overhead Amdahl ’s law, F-96 calculating, F-94 communication latency, I-4 interconnection networks, F-27 –29 OCNs vs.SANs, F-28 processor, G-4 software, F-96 sorting case study, D-64 –67 time flight, F-14 Overlapping triplets, integer multiplication, J-49 Oversubscription, 478 P Packed decimal, A-14Packets ATM, F-79 bidirectional rings, F-36 centralized switched networks, F-33 discarding, F-69 effective bandwidth vs.packet size, F-19 format example, F-7 InfiniBand, F-79 latency issues, F-13, F-13 lossless vs.lossy networks, F-12 network interfaces, F-8 network routing, F-22 switching, F-51 switch microarchitecture, F-56 –59 pipelining, F-64 –66 TI TMS320C6x DSP, E-10 topology, F-21 –22 transport, interconnection networks, F-9 –12Page(s) coloring, B-38 definition, B-43 vs.segments, B-43 Paged segments, B-43 –44,B-43 Paged virtual memory, B-54 –57 Page fault definition, B-2 –3, B-42, B-45 exception stopping/restarting, C-41 Page offset definition, B-38 virtual memory, B-43 –44, B-46 Page size, B-56 operating systems and, B-58 selection, B-46 –47 virtual memory, B-46 –47 Page table entry (PTE) definition, B-44, 136 fields in, B-52 Page tables AMD64 paged virtual memory, B-55 descriptor tables as, B-52 main memory block, B-44 –45 nested, 146 protection processes, B-50segmented virtual memory, B-51 –52 shadow, 123 size, B-47 virtual address physical address, B-45 Paging support, 330 Paired single operations, DSP media extensions, E-10 –11 Palt, definition, B-2 –3 Papadopolous, Greg, M-76 Parallel architectures, classes of, 10–11 Parallelism challenges, 373 –377 classes of, 10 –11 computer design principles, 48 dependence analysis, H-8 –9 Ethernet, F-74, F-82 –83 exploitation statically, H-2 exposing hardware support, H-23 –27 global code scheduling, H-15 –23, H-16 IA-64 instruction format, H-34 –37ILP ( seeInstruction-level parallelism (ILP)) request-level, 369 software pipelining, H-12 –15 superblock scheduling, H-21 –23, H-22 taking advantage of, 48 thread-level ( seeThread-level parallelism (TLP)) trace scheduling, H-19 –21,H-20 Parallel memory systems, highly, 150–153 Parallel processing, 369 Parallel processors areas debate, M-57 –59 bus-based coherent multiprocessors, M-59 –60 cluster history, M-62 –65 large-scale multiprocessors history, M-60 –62 recent advances developments, M-59 –60 scientific applications, I-33 –34 SIMD computers history, M-55 –57 synchronization consistency models, M-64 –65 virtual memory history, M-65 Parallel programming computation communication, I-10–12 large-scale multiprocessors, I-2 Parallel Thread Execution (PTX), 320–323,322 Paravirtualization, 126 PA-RISC, K-3 Parity, dirty bits, D-61 –64 PARSEC benchmark simultaneous multithreading, 246without SMT, 435 –437,435,437 Partial disk failure, dirty bits, D-61 –64 Partial store order (PSO), 420, 421, 457 Partitioned add operation, DSP media extensions, E-10 Partitioning, 480 –482 Pascal GPU architecture, 328 –331,329 full-chip block diagram, 318 SIMD Processor, 330 Pascal programs, integer division/ remainder, J-12I-32 ■IndexPattern, disk array deconstruction, D-51 Payload messages, F-6 packet format, F-7 p-bits, J-21 –23, J-25, J-36 –37 PC. SeeProgram counter (PC) PCI bus, historical background, M-88 PCI-Express (PCIe), storage area network, F-29 PCI-X, M-88 storage area network, F-29 PCI-X 2.0, F-67 PCMCIA slot, Sony PlayStation 2 Emotion Engine case study, E-15 PC-relative addressing, A-9, K-52 PDP-11, M-10 –11, M-19, M-57, M-88 Peak performance Cray X1E, G-24 DAXPY VMIPS, G-21 fallacy, 63, 63 vector architectures, 355 VMIPS DAXPY, G-17 Peer-to-peer, wireless networks, E-22 Pegasus, M-17PennySort competition, D-66 Pentium, K-31 –32 Perfect Club benchmarks vectorization, 303, 303 vector processor history, G-27 –28 Perfect-shuffle exchange, F-32 Performability, RAID reconstruction, D-55 –57 Performance. See also Cost- performance; Peak performance benchmarks, 40 –45 branch scheme, C-21 –22,C-22 cache ( seeCache performance) common data bus, 207 deep neural networks, 603 dirty bits, D-61 –64 disk array deconstruction, D-51 –54 disk deconstruction, D-48 –50 DSAs, 600 –601,601 embedded computers, E-13 –14 Fujitsu SPARC64 X+, 429 –431, 432 IBM Power8, 431 –432,432 instruction set architecture, 258Intel Core i7 920, 434 –437 Intel Core i7 6700, 138 –142, 255 –257 Internet Archive Cluster, D-36 –41 interprocessor communication, I-3 I/O devices, D-15 –23 I/O subsystem design, D-59 –61 I/O system design/evaluation, D-36 –41 Itanium 2, H-43, H-43 large-scale multiprocessors, scientific application distributed-memory multiprocessors, I-26–32,I-28–32 parallel processors, I-33 –34 symmetric shared-memory multiprocessor, I-21 –26, I-23–26 synchronization, I-12 –16 latency, 20, 21 measuring, 39 –47 microprocessor, 22 multicore processor, 426 –437,432 quantitative measures, M-6 –7 reporting, 39 –47 sorting case study, D-64 –67 summarizing, 39 –47 symmetric shared-memory multiprocessor, I-21 –26, I-23–26 trends, 20 vector processor, G-2 –9 chaining, G-11 –12,G-12 DAXPY VMIPS, G-19 –21 sparse matrices, G-12 –14 start-up multiple lanes, G-7 –9 unchaining, G-12 VMIPS Linpack, G-17 –19 Permanent failure, commercial interconnection networks, F-70 Permanent faults, D-11, 93 Personal computers, 4, 6 LANs, F-4 networks, F-2 PCIe, F-67 Personal mobile device (PMD), A-2, 7–8 image-processing unit for, 542 memory hierarchy in, 79PetaBox GB2000, Internet Archive Cluster, D-37 Phase-change memory (PCM) memory hierarchy design, 93 Xpoint memory chips, 93 Phase-ordering problem, A-26 Physical address AMD Opteron data cache, B-12 –13 AMD64 paged virtual memory, B-55 safe calls, B-54 translation, B-36 –40 virtual address, B-45 virtual memory, B-42, B-51 Physical cache, B-36 –37 Physical channels, F-47 –48 Physical layer, F-84 Physical memory centralized shared-memory multiprocessor, 372 directory-based cache coherence, 380 memory hierarchy, B-40 –42 virtual machine, 123 Physical register instruction, 237 register renaming, 235 SIMD instructions, 320 uses of, 234 –235 Physical transfer units (phits), F-64 Physical volumes, D-35 PicoJoules, 541 PID. SeeProcess-identifier tag (PID) Pin-out bandwidth, topology, F-39 Pipelined circuit switching, F-51 Pipelined CPUs, early versions, M-27 –28 Pipeline delays, C-44 Pipeline interlock, C-17 Pipeline latches, C-30 –31 Pipeline organization, data dependences, 172 Pipeline registers data hazards stall minimization, C-14 definition, C-30 –32 pipelining performance, C-8 –10 Pipeline scheduling instruction-level parallelism, 177–182 microarchitectural techniques case study, 266 –273Index ■I-33Pipeline stall cycles ARM Cortex-A53, 250 –251 branch scheme performance, C-21 Pipelining branch cost reduction, C-22 branch hazards, C-18 –22 branch issues, C-35 –37 branch penalty reduction, C-19 –20 branch-prediction buffers, C-23 –25,C-24 –26 branch scheme performance, C-21 –22,C-22 cache access, 99 –100 classic stages RISC processor, C-6–8,C-7 compiler scheduling, M-32 computer design principles, 48 concept, C-2 –3 data hazards, C-12 –17 definition, C-11 instruction set complications, C-45 pipelined execution instructions, C-13 stall minimization forwarding, C-14 –15, C-15 –16 stall requirements, C-16 –17 types, C-12 definition, C-2 detection hazard, C-34 example, C-7 exception arithmetic-logical units, C-5 categories, C-40 floating-point, C-41 –42 precise, C-41 –44 RISC V, C-42 –43,C-42 stopping/restarting, C-41 –42 types requirements, C-38 –41,C-40 unexpected sequences, C-70 floating-point addition speedup, J-25 graphics pipeline history, M-51 –52 hazards, C-10 –25 instruction set complications, C-43 –45 interconnection networks, F-12 MIPS R4000, C-55 –64 performance issues, C-8 –10performance stalls, C-11 –12 predicted-not-taken scheme, C-19 –20,C-19 RISC V, C-30 –33 classic pipeline stages, C-6 –8 control, C-33 –35 exception, C-42 –43,C-42 FP pipeline, C-45 –55,C-57 instruction set, C-3 –4, C-65 instruction set complications, C-43 –45 integer pipeline handle multicycle operations, C-45 –55 multicycle FP operations, C-45 –55 pipeline control, C-33 –35 simple implementation, C-4 –6, C-6, C-26 –29,C-30 simple implementation, C-26 –37 speedup from, C-11 –12 static branch prediction, C-22, C-23 switch microarchitecture, F-64–66 Pipe segment, C-3 Pipe stage definition, C-3 exception stopping/restarting, C-41 performance issues, C-8 –10 program counter, C-31 –32 register additions, C-31 RISC processor, C-8 Pixel Visual Core architecture philosophy, 583 –584 evaluating, 601 –602 example, 588 floor plan, 592 Halo, 584 –585 implementation, 590 –591 instruction set architecture, 587–588 line buffers in, 590 processing element, 588 –589 processor, 585 –587 programmer view of, 589 software, 582 two-dimensional array, 586 two-dimensional line buffers, 589–590 PLA, early computer arithmetic, J-65 Points-to analysis, H-9Point-to-point links bus replacement, D-34 Ethernet, F-30 storage systems, D-34 switched-media networks, F-24 –25 Poison bits, compiler-based speculation, H-28, H-30 –31 Poisson distribution basic equation, D-28 random variables, D-26 –34 Poisson, Sim /C19eon, D-28 Polycyclic scheduling, M-32 Portable computers, interconnection networks, F-89 Portable mobile devices (PMDs), 580–581 Port number, network interfaces, F-7 –8 Position independence, A-17 Power, 442 –443 first-level caches, 95 –98 within microprocessor, 25 –28 systems perspective, 23 –24 Power 3, K-25 additional instructions, K-24 branch registers, K-23 –25 Power consumption. See also Energy efficiency case study, 69 –71 embedded benchmarks, E-13 interconnection networks, F-61, F-89 simultaneous multithreading, 246 speculation, 238 –239 TI TMS320C55 DSP, E-8 Power gating, 28 PowerPC, K-6, K-11, K-25 cluster history, M-64 conditional instructions, H-27 IBM Blue Gene/L, I-41 –42 RISC history, M-21 PowerPC AltiVec, multimedia support, E-11 Power-performance Dell PowerEdge servers, 55 –58,56 servers, 57 Power utilization effectiveness (PUE) pitfall, 515 WSCs, 483, 484–485 Precise exceptions definition, C-41 –44 maintaining, C-53 –55I-34 ■IndexPrecisions, floating-point arithmetic, J-33–34 Predicated instructions example calculations, H-25 exposing parallelism, H-23 –27 IA-64, H-38 –40 Predicate registers, 296 –298 IA-64, H-34 Predication, TI TMS320C6x DSP, E-10 Predicted-not-taken scheme, C-19 –20, C-19 Prediction. See also Misprediction rate branch accuracy, C-25 –26 cost reduction, C-22 dynamic, C-23 –25 instruction-level parallelism, 182–191 static, C-22, C-23 branch-prediction buffers, C-23 –25,C-24 –26 return address, 232 –234 size,187 2-bit scheme, C-24 , 182, 184, 185 Prediction Partial Matching (PPM), 188 Prefetching, 376 instruction, 234 Itanium 2, H-42 memory hierarchy design, 138 software hardware, 148 –164 Prefix, Intel 80x86 integer operations, K-35 Presentation layer, F-84 Present bit, B-52 Price, cost vs.,3 5 Price-performance, 8 Primitives, synchronization, M-64 –65 Principle locality coining term, M-11 computer design principles, 48 –49 definition, B-2 scientific workloads symmetric shared-memory multiprocessors, I-25 –26 Private data, 377 Private memory, NVIDIA GPU memory structures, 326 Procedure calls IA-64 register model, H-33 –34 VAX, K-57Procedure invocation options, A-19 –20 Process-complexity factor, 34 Process concept, B-49, 119 Process-identifier tag (PID), B-37 –38, B-37 Processing element (PE) array, 570, 572, 580, 584 –585, 587, 592 Processor consistency, 420 Processor cycle cache performance, B-3 definition, C-3 Processor-dependent optimizations, A-26 Processor-intensive benchmarks, 41 Processor performance average memory access time and, B-17 –20 equation, 52 –55 multiprocessors, 371 –372 Process switch definition, B-37 , B-49 virtual memory, B-49, 119 Producer-server model, D-15 –16,D-16 Profile-based predictor, misprediction rate,C-23 Program counter (PC), A-17 branch hazards, C-18 branch-target buffers, 228 –229,229 dynamic branch prediction, C-23 –24 exception stopping/restarting, C-41 –42 pipeline branch issues, C-35 –36 pipe stage, C-31 –32 precise exceptions, C-54 RISC V instruction set, C-4 simple RISC implementation, C-27 –29 tagged hybrid predictors, 188 –189 Programmer ’s view, memory consistency, 418 –419 Programming models, warehouse-scale computers, 471 –476 Program order cache coherence, 378 –379 control dependence, 174 –175 data hazards, 173 –174 definition, 173 Protection schemes cross-cutting issues, 126 –127development, M-9 –12 network interfaces, F-8 Pentium vs.Opteron, B-57 processes, B-50 safe calls, B-54 segmented virtual memory, B-51 –54 virtual memory, B-41 Protocol deadlock, routing, F-45 –46 Protocol stack, F-86 –87 PSO. SeePartial store order (PSO) PTE. SeePage table entry (PTE) Pulse amplitude modulation (PAM-4), 59 Q QCDOD, M-64 QPI. SeeQuickPath Interconnect (QPI) QsNetII, F-67, F-80 Quadrics SAN, F-80 Quality service (QoS) dependability benchmarks, D-21 WAN, F-102 –103 Quantitative performance measures, development, M-6 –7 Quantization, DNNs, 556 Queue definition, D-25 discipline, D-26 Intel Core i7, 256 waiting time calculations, D-26 Queuing locks, large-scale multiprocessor synchronization, I-18–21 Queuing theory, D-23 –34 QuickPath Interconnect (QPI), 426–429 R Race-to-halt, 28 Radio frequency amplifier, radio receiver, E-23 Radio receiver, components, E-23 Radio waves, wireless networks, E-21 Radix-8 multiplication, J-49 Radix-2 multiplication/division, J-4 –7, J-4,J-55 Radix-4 multiplication/division, J-49, J-49, J-56, J-56–58, J-61Index ■I-35RAID. SeeRedundant array inexpensive disks (RAID) Random access memory (RAM), F-56 Random Access Method Accounting Control (RAMAC), M-85, M-88 –89 Random replacement, B-9 –10,B-10 Random variables, distribution, D-26 –34 Ranking, 573 Ransomware, 491 Ray casting (RC), 350 Read write (RAW), C-12 –14 check for, C-52 first vector computers, M-47 –48 instruction set complications, C-44 program order, 173 RISC pipeline control, C-34 stalls, C-49, C-50 , C-51 TI TMS320C55 DSP, E-8 Tomasulo ’s algorithm, 195, 217 Read miss, 382 –383, 410 cache coherence, 384–385, 386, 388 directory-based cache coherence protocol, 410 –411 memory stall clock cycles, B-4 miss penalty reduction, B-35 –36 Opteron data cache, B-14 Read operand, C-68 dynamic scheduling pipelines, C-66 ID pipe stage, 194 Real addressing mode, K-31 Real memory, 123, 126 Real mode, K-36 Real numbers, K-39 Real-time constraints, E-2 Real-time performance, 7 –8 requirement, definition, E-3 –4 Real-time processing, embedded systems, E-3 –4 Rearrangeably nonblocking, F-33 Receiving overhead, F-28, F-28, F-41, F-67 Receiving overhead, communication latency, I-3 Reconfiguration deadlock, F-45 –46 Recovery time, vector processor, G-8 Rectified linear unit (ReLU), 546Recurrences basic approach, H-11 loop-carried dependences, H-5 Recurrent neural network (RNN), 553–555 Red-black Gauss-Seidel, I-9 –10 Reduced Instruction Set Computer (RISC), 413 –414, 423 cache performance, B-6 compiler history, M-33 correlating predictors, 183 development, 2 early pipelined CPUs, M-28 FENCE in, 420 –422 historical background, M-20 –23, M-22 multimedia SIMD extensions history, M-50 pipeline scheduling loop unrolling, 177 –178 reduced code size in, A-23 –24 RISC-I, M-20 –21 RISC-II, M-20 –21 Sanyo VPC-SX500 digital camera, E-19 vector processor history, G-26 Reduced Instruction Set Computer (RISC) architectures, A-33 –42 addressing modes instruction formats, K-6 –9 ARM architecture, K-22 16-bit instructions, K-3, K-4, K-5, K-10 compare conditional branch, K-11 –16 conditional branches, K-17 control instructions, K-18 data transfer instructions, K-18 digital signal-processing extensions, K-28 extensions beyond RV64G, K-18 –19 lower-end applications, K-3 MIPS64 R6, K-19 multimedia graphics operations, K-25 –27 Power3, K-23 –25 RISC-V integer ISA, K-13 RV64GC core 16-bit instructions, K-16 –17RV64G core instructions, K-11 RV64G instruction, K-5, K-10 , K-12 SIMD extensions, K-25 –27 SPARC v.9, K-20 –22 survey of, K-3 –29 Reduced Instruction Set Computer (RISC) V, 12 –17,13, 413–414, 423 addressing modes, A-36 control flow instructions, A-39 –40 data types for, A-35 –36 dies,33 FENCE in, 420 –422 floating point instructions for, 16 floating-point operations, A-40 –41 instruction format, A-36 –37 instruction set architecture formats, 16 instruction set organization, A-34 load store instructions, A-38 operations, A-37 –39 pipelining, C-30 –33 classic pipeline stages, C-6 –8 control, C-33 –35 exception, C-42 –43,C-42 FP pipeline, C-45 –55,C-57 instruction set, C-3 –4, C-65 instruction set complications, C-43 –45 integer pipeline handle multicycle operations, C-45 –55 multicycle FP operations, C-45 –55 pipeline control, C-33 –35 simple implementation, C-4–6,C-6, C-26 –29, C-30 registers for, A-34 –35 scalar architecture, RV64V, 284 SIMD Processor, 306 –307 SPECint2006 programs, A-42 subset instructions in, 15 Tomasulo ’s algorithm floating-point operation, 198 instruction set, 195 Reductions, 344 –345 Redundancy chip fabrication cost case study, 67–68I-36 ■Indexcomputer system power consumption case study, 69–71 index checks, B-9 simple RISC implementation, C-29 Redundant array inexpensive disks (RAID) dependability benchmarks, D-21 –23 disk array deconstruction case study, D-51 –54 disk deconstruction case study, D-48 –50 hardware dependability, D-15 historical background, M-86 –88 I/O subsystem design, D-59 –61 logical units, D-35 NetApp FAS6000 filer, D-41 –43 overview, D-6 –8 performance prediction, D-57 –59 RAID 0, D-6 RAID 1, D-6, M-87 RAID 2, D-6, M-87 RAID 3, D-6, M-87 RAID 4, D-7, M-87 RAID 5, D-8, M-87RAID 6, D-8 –10 RAID 10, D-8 row-diagonal parity, D-9 –10,D-9, D-41 –42 Redundant multiplication, integers, J-47 Reference bit, B-45, B-52 Regional explicit congestion notification (RECN), F-70 Register(s) DSP examples, E-6 IA-64, H-33 –34 instructions hazards, C-13 network interface functions, F-7 pipe stage, C-31 tag,202 Register addressing, K-52 Register allocation, A-26 –27 Register deferred addressing, K-52 –53 Register fetch (RF) cycle, C-5 MIPS R4000 pipeline, C-56 simple RISC implementation, C-27Register file, C-27, 200 –201 data hazards, C-16 –17 floating-point operations, C-50 OCNs, F-3 precise exceptions, C-54 RISC instruction set, C-5, C-7 –8 simple RISC implementation, C-29 Register indirect addressing mode, K-34 Register management software- pipelined loops, H-14 Register-memory architecture, A-3 ISAs, 12 Register prefetch, 111 Register pressure, 182 Register renaming antidependence, 196 deallocating registers, 235 definition, 173, 195 –196 expected output, 269 initial state table, 270 microarchitectural techniques case study, 266 –273 name dependences, 196 vs.reorder buffers, 234 –236 reservation stations, 196 –197, 199–200 sample code, 269–270 Register stack engine, IA-64, H-34 Register Transfer Level (RTL) code, 569 Regularity, bidirectional MINs, F-33–34 Reinforcement learning (RL), 549 Relaxed consistency models, 419 –422, 421 Release consistency (RC), 420 –422, 421, 457 Reliability commercial interconnection networks, F-37 I/O subsystem design, D-59 –61 storage systems, D-44 Relocation, virtual memory, B-41 –42 Remainder, floating point, J-31 –32 Remington-Rand, M-5 Remote direct memory access (RDMA), F-80 Remote node, 406 –407 Renaming map, 235Reorder buffer (ROB) compiler-based speculation, H-31 –32 hardware-based speculation, 209–212, 214 –215 issue with, 236 register renaming vs., 234 –236 Replication definition, 377, 379 virtual memory, B-49 Reply, messages, F-6 Reproducibility, 45 Request messages, F-6 switch microarchitecture, F-58–59 Requested protection level, B-54 Request-level parallelism (RLP) definition, 5, 10 –11, 369 WSCs, 467 Request phase, F-49 –50 Request-reply deadlock, F-45 –46 Reservation stations common data bus, 202 fields, 199 –200 register renaming, 196 –197, 199–200 Reserved register, 414 Resource sparing, F-70 –71 Response time. See also Latency definition, 20, 39 DNN applications, 596 –600 I/O benchmarks, D-18 producer-server model, D-16 vs.throughput, D-16 –18,D-17 Restartable pipeline definition, C-40 –41 exception, C-41 –42 Restorations, dependability, 37 Restoring division, J-5, J-6 Resume event, exception, C-40 Return address, predictors, 232 –234 Returns, cache coherence, 378 –379 Reverse path, cell phones, E-24 RF. SeeRegister fetch (RF) Rings, F-43 protection processes, B-50 Ripple-carry adder, J-2 –3,J-3 carry-lookahead adder with, J-42 chip comparison, J-61 Ripply-carry addition, J-2 –3Index ■I-37RISC. SeeReduced Instruction Set Computer (RISC) ROB. SeeReorder buffer (ROB) Role code, 569, 570 Roofline model, 349 CPUs vs.GPUs, 355 DNN applications, 596 –600,597 Round digit, J-18 Rounding modes, J-14, J-17 –20,J-18, J-20 FP precisions, J-34 fused multiply-add, J-33 Round-robin (RR), F-49 Routers, F-64, F-83 Routing algorithm, F-21 –22, F-45 –49 Row access strobe (RAS), memory hierarchy design, 85 –86 Row-diagonal parity, D-9 –10,D-9, D-41 –42 Row major order, blocking, 107 RV64c, 16-bit instruction formats, K-7 RV32E, K-4 RV64GC, K-3 ALU instructions in, K-17 16-bit instructions, K-10 core 16-bit instructions, K-16 –17 register encodings, K-7 RV64G core instructions, K-11 RV64G, extensions beyond, K-18 –19 RV64G instruction, K-12 RV64V extension data sizes, 287 vector architecture, 283 –287,284 vector instructions, 286 RV64V instruction set, 293 Sanyo digital cameras, SOC, E-20 Sanyo VPC-SX500 digital camera, embedded system case study, E-19 SASI, M-88 SATA disks. SeeSerial Advanced Technology Attachment (SATA) disks Saturating arithmetic, DSP media extensions, E-11 Scalability computer design principles, 48 server systems, 9 Scalable GPUs, M-51Scalar lane (SCL), 586 –588 Scalar processors, 310, 326, 332, 334. See also Superscalar processors early pipelined CPUs, M-28 vs.vector, G-19 Scalar registers Cray X1, G-21 –22 set of, 285 Scaled speedup. SeeWeak scaling Scaling CMOS, 442 –443 computation-to-communication ratios, I-11 instruction-level parallelism, 442 multicore processor, 432, 442–444 scientific applications parallel processing, I-34 SPECintRate benchmarks, 429–431,431 strong, 439 weak, 439 Scan Line Interleave (SLI), M-51 Scatter store, 301 –302 Schorr, Herb, M-29 –30 Scientific applications Barnes, I-8 –9 characteristics, I-6 –12 cluster history, M-62 –63 distributed-memory multiprocessors, I-26–32,I-28–32 FFT kernel, I-7 LU kernel, I-8 ocean, I-9 –10 parallel processors, I-33 –34 parallel program computation/ communication, I-10–12,I-11 parallel programming, I-2 symmetric shared-memory multiprocessor, I-21 –26, I-23–26 Scoreboarding definition, 194 –195 dynamic scheduling with, C-66 –70, C-68 SCSI. SeeSmall Computer System Interface (SCSI) SDRWAVE, J-62Second-level caches cache optimization, B-30 –35,B-34 execution time, B-32, B-34 interconnection network, F-74 Itanium 2, H-41 memory hierarchy, B-48 –49,B-48 miss rate calculations, B-30 –35, B-34 Secure Virtual Machine (SVM), 146 Seek distance, D-46, D-46 –47 Seek time, storage disks, D-45 –46,D-46 Segment descriptor, B-52, B-53 Segmented virtual memory bounds checking, B-52 Intel Pentium processors, B-51 –54 memory mapping, B-52 safe calls, B-54 sharing protection, B-52 –53 Segments definition, B-43 pages vs.,B-43 Self-correction, Newton ’s algorithm, J-28–30 Self-draining pipelines, M-30 Self-routing, MINs, F-48 –49 Semiconductor DRAM, 19 flash, 19 ITRS, 58 –59,59 manufacturing, 4 Sending overhead OCNs vs.SANs, F-27 –29 time flight, F-14 Sending overhead, communication latency, I-3 Sense-reversing barrier code example, I-15, I-21 large-scale multiprocessor, synchronization, I-14 –16 Sequency number, packet header, F-8 Sequential consistency (SC), 417, 421, 457 implementation, 418, 423 programmer ’s view, 418 –419 Sequential interleaving, 100 Sequent Symmetry, M-59 –60 Serial Advanced Technology Attachment (SATA) disks NetApp FAS6000 filer, D-42 power consumption, D-5I-38 ■IndexRAID 6, D-8 –9 vs.SAS drives, D-5, D-5 Serial Attach SCSI (SAS) drive historical background, M-88 power consumption, D-5 vs.SATA drives, D-5 Serialization barrier synchronization, I-16 cache coherence, 378 –381 definition, 380 –382, 413 DSM multiprocessor cache coherence, I-37 Serpentine recording, M-85 Serve-longest-queue (SLQ) scheme arbitration, F-49 Server(s), A-2, 8 –9. See also Warehouse- scale computer (WSC) benchmarks, 43 –45 CPU utilization, 475 definition, D-25 Google WSCs, 512 –513,513 single-server model, D-25 system characteristics, E-4 Server computer, RISC architectures survey for, K-3 –29 Serverless Computing, 496 ServerNet interconnection network, F-70–71 Server utilization, D-25, D-28 –29 Service accomplishment, 36 Service interruption, 36 Service level agreements (SLAs), 36 –37 Service level objectives (SLOs), 36, 485–486 Session layer, F-84 Set associativity, 81 AMD Opteron data cache, B-13 cache block, B-8, B-8 cache misses, B-10 Set, definition, B-8 Settle time, D-46 SFS benchmark, NFS, D-20 –21 Shadow page table, 123 Sharding, 480 –482 Shared data, 377 Shared-media networks, F-23 –25 Shared memory, 373, 379, 406 address space, 373 distributed ( seeDistributed shared memory (DSM))Shared-memory communication, large- scale multiprocessors, I-4–5 Shared-memory multiprocessors (SMPs), 371, 373 access time, 371 definition, M-64history background, M-61 snooping coherence protocols, 380 Shared state cache block, 386 definition, 383 –384 Sharing addition, segmented virtual memory, B-52 –53 Shear algorithms, disk array, D-51 –54 Sheet Generator (SHG), 585 Shell code, 569, 570 Shifting zeros, integer multiplication/division, J-45–47 SiFive, 33 Signals, definition, E-2 Signal-to-noise ratio (SNR), wireless networks, E-21 Signed-digit representation example, J-54 integer multiplication, J-53 Signed number arithmetic, J-7 –10 Sign-extended offset, RISC, C-5 Significand, J-15 Sign magnitude, J-7 –8 Silicon Graphics Altix, M-64 Silicon Graphics Challenge, M-60 Silicon Graphics 4D/240, M-59 –60 Silicon Graphics Origin, M-62, M-64 Silicon Graphics systems (SGI), vector processor history, G-27 Simultaneous multithreading (SMT), 424–426,425, 435 definition, 244 historical background, M-35 –36 implementations, 245 Java PARSEC benchmark without, 435 –437,435, 437 multicore processor and, 436 –437 superscalar processors, 245 –247 Single chip multicore processor, 382, 391, 446 –451 Single-event upsets (SEUs), 569Single-extended precision floating- point arithmetic, J-33 –34 Single instruction multiple data (SIMD), 11, 170, 282 historical overview, M-55 –57 instruction DSP media extensions, E-10 IBM Blue Gene/L, I-42 Sony PlayStation 2, E-16 Intel Core i7 920 multicore computer, 309 loop-level parallelism, 170 multimedia extensions 256-bit-wide operations, 304 data-level parallelism, 304 – 310 GPU MIMD vs., 347 –353 vs.GPUs, 335, 335 Intel Core i7 920 multicore computer, 309 NEC SX-9 vector processor, 309 programming, 307 RISC-V, 306 –307 roofline visual performance model, 307 –310 NEC SX-9 vector processor, 309 processors multithreaded, 311, 315, 317 Pascal GPU architecture, 330 RISC V, 306 –307 supercomputer development, M-45 –46 system area network history, F-104 thread instructions, 315 –317,316, 319 thread schedule, 315 –317 TI 320C6x DSP, E-9 Single instruction, multiple thread (SIMT), 311 Single instruction stream, single data stream (SISD), 11, M-56 SIMD, M-46 Single-precision floating point arithmetic, J-33 –34 representation, J-15 Single-precision floating-point arithmetic, 329 Single-Streaming Processor (SSP) Cray X1, G-21 –24 Cray X1E, G-24 Skippy algorithm, D-49, D-50Index ■I-39Small Computer System Interface (SCSI) Berkeley ’s Tertiary Disk project, D-4 dependability benchmarks, D-21 disk storage, D-4 historical background, M-88 I/O subsystem design, D-59 –61 RAID reconstruction, D-56 storage area network history, F-106 –107 Small form factor (SFF) disk, M-86 Smalltalk, K-21 –22 Smart interface cards, vs.smart switches, F-90 Smart switches, vs.smart interface cards, F-90 SMPs. SeeShared-memory multiprocessors (SMPs) SMT. SeeSimultaneous multithreading (SMT) Snooping bandwidth, 389 –390 Snooping cache coherence, 380, 381 example protocol, 383 –387,384 implementation, 392 –393 invalidate protocol, 381 large-scale multiprocessors, I-34–35, M-61 latencies, 447, 448 limitations, 389 –392 maintenance, 380 –381 sample types, M-60 SoC. SeeSystem-on-chip (SoC) Soft cores, 130 Soft errors, 93 Soft real-time, 7 –8 definition, E-3 –4 Software service (SaaS) growth of, 9 WSCs, 467 Software guard extensions (SGX), 125 Software pipelining example calculations, H-13 –14 loops, execution pattern, H-15 technique, H-12 –15,H-13 Software prefetching, 148 –164 Software speculation definition, 176 hardware-based vs., 240 –241 Software technology large-scale multiprocessor, I-6synchronization, I-17 –18 network interfaces, F-7 –8 Solaris, RAID benchmarks, D-21, D-22 , D-23 Sonic Smart Interconnect, OCNs, F-3 Sony PlayStation 2 block diagram, E-16 embedded multiprocessors, E-14 –15 Emotion Engine case study, E-15 –18 Emotion Engine organization, E-18 Sort procedure, VAX code example, K-62 –64 full procedure, K-65 register allocation, K-62 register preservation, K-64 –65 Source routing, F-49 SPARC “annulling ”branch, K-18 –19 SPARCLE processor, M-35 –36 SPARC v.9 additional instructions, K-22 fast traps, K-20 –21 integer arithmetic, K-21 LISP, K-21 –22 misaligned trap, K-21 register windows, K-20 Smalltalk, K-21 –22 SPARC VIS, K-25 –26,K-27 SPARC64 X+, 389, 426, 429 feature, 427 performance, 429 –431,432 Sparse matrices, vector architecture, G-12 –14, 301 –302 Spatial locality, B-26 coining term, M-11 computer design principles, 49 definition, B-2 SPEC benchmark active benchmarks, 44 correlating predictors, 182 desktop performance, 41 –43,42 early performance measures, M-7 organization, 433 –434 server performance, 43 –45 static branch prediction, C-22, C-23 storage systems, D-20 –21 vector processor history, G-27 –28 SPEC89 benchmark, 41 branch-prediction buffer, C-24 –25, C-25misprediction rate, 187 mispredictions rate, C-26 tournament predictors, 187 SPEC92 benchmarks CPI,C-64 stalls, C-61 –62 SPEC95 benchmarks procedure returns, 232 return address buffer, 232, 233 SPEC2000 benchmarks compulsory miss rate, B-23 perl benchmark, 144 speculation, 238 –239 SPEC2006Cint execution times, 47 SPECCPU2006 benchmark Intel Core i7 920/6700, 192 nonblocking caches, 101 –102 virtual machine, 121 SPEC CPU95 benchmark, return address buffer, 232, 233 SPECCPUint2006 benchmark, clock cycles per instruction, 256,257 SPECfp benchmark Intel Core i7, 253 interconnection network, F-91 –92 Itanium 2, H-43 stalls, C-55, C-56 –57 SPECfpRate benchmark cost-performance, 440, 441 speedup, 440, 440 SPEChpc96 benchmark, G-27 –28 Special instruction caches, 128 Special-purpose machines historical background, M-4 –5 SIMD computer history, M-56 –57 Special-purpose register computer, A-3 Special values, floating point, J-14 –15, J-16 SPECInt2006 benchmark ARM Cortex-A53, 132, 132–133, 250 L1 data cache miss rate, 139 SPECINT92 benchmark, nonblocking caches, 101 –102 SPECINT benchmarks interconnection network, F-91 –92 Itanium 2, H-43 SPECint95 benchmarks, F-92 SPECintRate benchmarks cost-performance, 440, 441I-40 ■Indexperformance scaling, 429 –431,431 speedup, 440, 440 SPEC Mail benchmark, D-20 –21 SPEC-optimized processors, vs. density-optimized, F-89 SPECpower benchmark, WSCs, 475–476 SPECRate benchmark, 439 memory-intensive benchmarks, 116,116 server performance, 43 SPECRatios, 46 –47 SPEC SFS benchmarks, D-20 Speculation address aliasing prediction, 239–240 advanced techniques, 228 –240 advantages, 237 –238 challenge issues per clock, 236–237 concept origins, M-31 control dependence, 175 –176 cross-cutting issues, 127 –128 disadvantages, 238 energy efficiency, 238 –239 exception handling, 199 execution, 241 hardware-based, 208 –217 data flow execution, 209 definition, 208 disadvantage, 241 instruction execution step, 211–212 key ideas, 208 reorder buffer, 209 –212, 214–215 vs.software speculation, 240–241 write result, 217 IA-64, H-38 –40 ILP studies, M-33 –34 memory reference, hardware support, H-32 microarchitectural techniques case study, 266 –273 multiple branches, 238 register renaming vs.ROB, 234–236 software, 176, 240 –241 SPEC Web benchmarks, D-20 –21Speedup Amdahl ’s Law, 374 –375 computer design principles, 49 –52 floating-point addition, J-25 –26 integer addition carry-lookahead, J-37 –41 carry-lookahead circuit, J-38 carry-lookahead tree, J-40 carry-lookahead tree adder, J-41 carry-select adder, J-43 –44, J-43–44 carry-skip adder, J-41 –43,J-42 overview, J-37 integer division radix-2 division, J-55 radix-4 division, J-56 radix-4 SRT division, J-57 single adder, J-54 –57 SRT division, J-45 –47,J-46, J-55–57 integer multiplication array multiplier, J-50 Booth recoding, J-49 even/odd array, J-52 many adders, J-50 –54,J-50 multipass array multiplier, J-51 signed-digit addition table, J-54 single adder, J-47 –49, J-48–49 Wallace tree, J-53 integer multiplication/division, shifting zeros, J-45 integer SRT division, J-45 –47,J-46 linear, 438 –439,440 multithreading, 248 pipeline stalls, C-11 –12 SPECfpRate benchmarks, 440, 440 SPECintRate benchmarks, 440, 440 switch buffer organizations, F-59–60 TPC-C benchmarks, 440, 440 Sperry-Rand, M-4 –5 Spin locks, 414 –416 large-scale multiprocessor synchronization barrier synchronization, I-16 exponential back-off, I-17 SPRAM, Sony PlayStation 2 Emotion Engine organization, E-18Sprowl, Bob, F-103 Squared coefficient variance, D-27 SRAM. SeeStatic random-access memory (SRAM) SRT division chip comparison, J-61 complications, J-45 –47 early computer arithmetic, J-65 example, J-46 historical background, J-63 integers, adder, J-55 –57 radix-4, J-56 –57,J-57 Stack, A-3, A-28 architecture, historical background, M-17 –18 Stacked DRAM, 91 Stack frame, K-57 Stack pointer, K-57 Stale copy, cache coherency, 128 Stall control dependences, 176 cycles average memory access time, B-18 branch scheme performance, C-21 definition, B-3 –4, B-6, B-22 miss rate calculation, B-6 out-of-order execution, B-20 data hazards minimization, C-13 , C-14 –15 data hazards requiring, C-16 –17 longer latency pipelines, C-49, C-50 pipelining performance with, C-11 –12 RAW, C-49, C-50 , C-51 SPEC92 benchmarks, C-61 –62 SPECfp benchmarks, C-55, C-56 –57 Standardization, commercial interconnection networks, F-67 –68 Standard Performance Evaluation Corporation (SPEC), 41 Start-up time DAXPY VMIPS, G-20 –21 definition, 292 page size selection, B-47 vector architectures, G-4, G-4,G-8 vector convoys, G-4Index ■I-41Start-up time (Continued) vector performance, G-2 –4, G-16 vector processor, G-7 –9, G-25 VMIPS, G-5 Statically based exploitation, ILP, H-2 Static power, 80 Static random-access memory (SRAM) arithmetic operations energy cost,29 memory hierarchy design, 85 price pressures, 34 vector memory systems, G-9 –10 vector processor, G-9 –10, G-25 Static scheduling definition, C-65 instruction-level parallelism, 218–222 unoptimized code, C-70 Stencil computation, 550 Sticky bit, J-18 Stochastic gradient descent, 548 Storage area networks, F-77 –81, F-106 –108 dependability benchmarks, D-21 –23 Storage systems Amazon, Dynamo key-value, 485–486 asynchronous I/O operating systems, D-35 –36 Berkeley ’s Tertiary Disk project, D-12 –13 block servers vs.filers, D-34 –35 bus replacement, D-34 component failure, D-43 computer system availability, D-43 dependability benchmarks, D-21 –23 dirty bits, D-61 –64 disk array deconstruction, D-51 –54 disk arrays, D-6 –10 disk deconstruction, D-48 –50 disk power, D-5 disk seeks, D-45 disk storage, D-2 –10 file system benchmarking, D-20 –21 Internet Archive Cluster (seeInternet Archive Cluster) I/O performance, D-15 –23 I/O system design/evaluation, D-59 –61mail server benchmarking, D-20 –21 NetApp FAS6000 filer, D-41 –43 operator dependability, D-13 –15 OS-scheduled disk access, D-44 point-to-point links, D-34 queuing theory, D-23 –34 RAID performance prediction, D-57 –59 RAID reconstruction case study, D-55 –57 real faults failures, D-10 –15 reliability, D-15 –23 response time restrictions benchmarks, D-18 seek distance comparison, D-47 seek time vs.distance, D-46 server utilization calculation, D-28 –29 sorting case study, D-64 –67 Tandem Computers, D-13 throughput vs.response time, D-16 –18 TP benchmarks, D-18 –20 transactions components, D-17 web server benchmarking, D-20 –21 WSCs, 478 Store-and-forward packet switching, F-51 Store conditional advantage, 414, 416 definition, 413 –414 Store instructions, C-5 –6, 199. See also Load-store instruction set architecture Store unit bandwidth for, 298 –299 definition, 285 Streaming SIMD Extensions (SSE), 305 Stride, 300 vector memory systems, G-10 –11 Strided accesses, 346 Strided addressing, A-31 –32. See also Unit stride addressing Striping, D-51 disk arrays, D-6 RAID, D-8 Strip-mined vector loop convoys, G-5 DAXPY VMIPS, G-20 –21Strip mining, 180, 296, 297 DAXPY VMIPS, G-20 –21 Strong scaling, 439 Structural hazards check for, C-52 definition, C-11 Subblocking, cache optimization, 114 Subset property, 423 Sun Microsystems, B-38 Sun Microsystems Enterprise, M-60 Sun Microsystems Niagara (T1/T2), multithreading history, M-35 Sun Microsystems SPARC conditional instructions, H-27 integer arithmetic, J-11 –12 integer overflow, J-11 RISC history, M-21 synchronization history, M-64 –65 Sun Microsystems SPARCCenter, M-60 Sun Microsystems SPARCstation-2, F-92 Sun Microsystems SPARCstation-20, F-92 Sun Microsystems SPARC V8, floating-point precisions, J-33 Sun Microsystems SPARC VIS, multimedia support, E-11 Sun Microsystems UltraSPARC, M-63, M-74 Sun Microsystems UltraSPARC T1 processor, F-78 Sun Modular Datacenter, M-76 SUN servers, 94 Sun Ultra 5, 47 Superblock scheduling basic process, H-21 –23,H-22 compiler history, M-33 Supercomputers, 10 clusters, F-80 commercial interconnection networks, F-37 direct network topology, F-37 SAN characteristics, F-30 –31 SIMD, development, M-45 –46 Superpipelining, C-55 Superscalar processors, 223 announcement, M-35 coarse-grained multithreading, 245I-42 ■Indexcoining term, M-31, M-34 dynamically scheduled, M-36, 224 functional unit execution slots, 244–245,244 ILP, M-33 –34 recent advances, M-35 simultaneous multithreading, 245–247 Supervised learning, 547 –548 Supervisor process, virtual memory, 119 Sussenguth, Ed, M-29 –30 Sutherland, Ivan, M-35 Swap procedure, VAX, K-57 code example, K-59 –60 full procedure, K-61 register allocation for, K-59 register preservation, K-60 –61 Switched-media networks, F-2, F-24–25 Switched networks centralized, F-31 –35 distributed, F-35 –40 Switches context, B-49 early LANs WANs, F-29interconnecting node calculations, F-32 –33 vs.NIC, F-90 process switch, B-49 statements, A-17 storage systems, D-34 switched-media networks, F-24 –25 Switch fabric, switched-media networks, F-24 –25 Switching, F-21 –22, F-44 –56 Switch microarchitecture, basic microarchitecture Switch ports, F-30 Syllable, IA-64, H-35 Symbolic loop unrolling, software pipelining, H-12 –15, H-13 Symmetric multiprocessors (SMP), F-106 characteristics, I-45 first vector computers, M-49 Symmetric shared-memory multiprocessors, 371 limitations, 389 –392 performance, 393 –404 commercial workload, 394 –399multiprogramming OS workload, 399 –404 scientific workloads, I-21 –26, I-23 –26 Synapse N+1, M-59 –60 Synchronization, 352, 412 Cray X1, G-23 fetch-and-increment, 413 –414 hardware primitives, 412 –414 historical background, M-64 –65 large-scale multiprocessors barrier synchronization, I-13–16,I-14,I-16,I-19, I-20 hardware primitives, I-18 –21 performance challenges, I-12–16 sense-reversing barrier, I-21 software implementations, I-17–18 tree-based barrier, I-19 locks using coherence, 414 –417, 416 message-passing communication, I-5 Synchronous dynamic random-access memory (SDRAM) capacity access times, 88 IBM Blue Gene/L, I-42 –43 memory hierarchy design, 87 –90 power consumption reduction, 89–90 Synchronous events, exception, C-39 Synchronous I/O, definition, D-35 Synonyms, address translation, B-38 Synthetic benchmarks, 40 System area networks, F-76 –77,F-80, F-104 –106 System call, virtual memory, 119 System interface controller (SIF), F-74 System-on-chip (SoC) cell phone, E-24 cost trends, 31 cross-company interoperability, F-23 DSAs, 592 –594 embedded systems, E-3 Sanyo digital cameras, E-20 Sanyo VPC-SX500 digital camera, E-19 System response time, D-16 Systems software, 503System/storage area networks (SANs) characteristics, F-3 communication protocols, F-8 congestion management, F-68 –70 cross-company interoperability, F-67–68 effective bandwidth, F-19 fat trees, F-34 –35 fault tolerance, F-71 InfiniBand, F-77 –81 interconnection network domain relationship, F-4, F-5 latency effective bandwidth, F-29–30 packet latency, F-13, F-14 –16 time flight, F-14 System virtual machines, 120 –121 Systolic array, 560 Tag, 383 AMD Opteron data cache, B-13 –14 memory hierarchy, 81 registers, 202 virtual memory fast address translation, B-46 write strategy, B-10 Tag check MIPS R4000 pipeline, C-58 –59 write strategy, B-10 Tag field, B-8 –9 Tagged hybrid predictors, 188 –190, 188,190 Tail duplication, superblock scheduling, H-21 Tailgating, G-20 –21 Tail latency, 473 Tail tolerant systems, 486 Tandem Computers, D-13 cluster history, M-62, M-74, M-87 Target address branch hazards, C-18 –19 branch penalty reduction, C-19 –20 branch-target buffers, 231 pipeline branch issues, C-35 –36 RISC instruction set, C-5 Target channel adapters (TCAs), F-90 Target instructions branch-target buffers, 231 GPU conditional branching, 323 Task-level parallelism (TLP), 10 TB-80 VME rack, D-38 , D-41Index ■I-43Technology trends bandwidth latency, 20 implementation technologies, 19–20 scaling transistor performance wires, 21 –23 Temporal locality, B-26 coining term, M-11 computer design principles, 49 definition, B-2 Tensor processing unit (TPU) architecture, 557 –558 block diagram, 558 case study, 606 –617 die,562 factors limiting, 598 guidelines, 566 –567 implementation, 560 –563 improving, 564 –566 instruction set architecture, 559 microarchitecture, 559 –560 origin, 557 printed circuit board, 563 software, 563 TensorFlow program, 564 TERA processor, M-35Terminate event, exception, C-40 Tertiary Disk project, D-12 –13 Tesla, M-52 Test-and-set operation, synchronization, 413 Texas Instruments 8847 arithmetic functions, J-57 –62 chip comparison, J-58 chip layout, J-59–60 Texas Instruments ASC, first vector computers, M-47 TFLOPS, parallel processing debates, M-58 Thacker, Chuck, F-103 Thermal design power (TDP), 24 Thin-film transistor (TFT), Sanyo VPC-SX500 digital camera, E-19 Thinking Machines, M-46, M-56, M-87 Thinking Multiprocessors CM-5, M-60 –61 Think time, D-16 Third-level caches, 166, 262 interconnection network, F-91 –92 Thrash, B-25 –26Thread Block, 311, 315 Thread Block Scheduler, 315, 316 Thread-level parallelism (TLP), 5, 10–11, 369 centralized shared-memory multiprocessor, 371, 377 basic schemes enforcing coherence, 379 –380 cache coherence protocol, 377– 379,378, 383 –387, 384 extensions coherence protocol, 388 implementation techniques, 382–383 SMP snooping limitations, 389–392 snooping coherence protocols, 380–381,381, 392 –393 structure, 372 definition, 242 directory-based cache coherence, 380 case study, 451 –452 protocol example, 408 –412 distributed shared memory, 371, 373 access time, 372 –373 architecture, 373 directory-based cache coherence, 404 –412,405 disadvantages, 372 –373 embedded systems, E-15 memory consistency, 379, 417 –422 case study, 456 –458 compiler optimization, 422 programmer ’s view, 418 –419 relaxed consistency models, 419–422,421 speculation hide latency, 422–423 multicore processor, 369, 371 –372, 382, 387, 408 approaches, 389 architecture, 430 coherence, 387 development, 404 DSM, 373,405,452 Intel i7 920 performance energy efficiency, 434–437on multiprogrammed workload, 426–432 performance, 426 –437,432 scalability Xeon E7 different workloads,433–434 scaling, 432, 442 –444 single chip, 382, 391, 446 –451 SMT, 436 –437 multiprocessor architecture, 370–373 vs.multithreading, M-36 parallel processing challenges, 373–377 single-chip multicore processor, 446–451 synchronization, 412 hardware primitives, 412 –414 locks using coherence, 414–417,416 Thread SIMD instructions GPU programming, 315 –317,316 scheduling, 319 Three-dimensional space, direct networks, F-39 Throttling packets, F-10Throughput, 20, 39. See also Bandwidth computing kernel, 350, 351 definition, C-3, F-13 disk storage, D-4 DNN applications, 596 –600 producer-server model, D-15 –16, D-16 vs.response time, D-16 –18 routing comparison, F-54 uniprocessor, 242 –247 Thumb-2, K-3 16-bit instructions, K-7,K-10 register encodings, K-7 Tilera TILE-Gx processors, OCNs, F-3 Time, cost, 30 –31 Time-constrained scaling, I-33 –34 Time division multiple access (TDMA), cell phones, E-25 Time flight communication latency, I-3 interconnection networks, F-14 Time-sharing, B-49 –50 Timing independent, M-18I-44 ■IndexTI TMS320C55 DSP architecture, E-7 characteristics, E-7 –8 data operands, E-6 TI TMS320C6x DSP architecture, E-9 characteristics, E-8 –10 instruction packet, E-10 TLB. SeeTranslation lookaside buffer (TLB) TLP. SeeThread-level parallelism (TLP) Tomasulo ’s algorithm advantages, 201 definition, 194 –195 dynamic scheduling, 195 –201 RAW, 217 RISC-V floating-point unit, 198 steps in, 216 TOP500, M-59 Top Rack (ToR) switch, 477 –478 Topology, F-21 –22, F-30 –44 Torus networks, F-53 –56, F-76 –77 Total cost ownership (TCO), 577 case study, 519 –521 DSAs, 600 –601,601 resource allocation, 521 –522 Total store ordering (TSO), 420, 421 Tournament predictors, 184 –188,186 advantage, 185 –187 branch address, 186 early schemes, M-29 local/global predictors, 184 –188, 186 Toy programs, 40 TPC-C benchmarks definition, 44, 439 speedup, 440, 440 TPC-C, file system benchmarking, D-18 –20 TPU. SeeTensor processing unit (TPU) Trace compaction, H-19 Trace scheduling, H-19 –21,H-20 Trace selection, definition, H-19 Traffic intensity, queuing theory, D-26 Trailer messages, F-6 packet format, F-7 Transaction components, D-16, I-38–39Transaction-processing (TP) benchmarks, server performance, 43–44 storage system benchmarks, D-18 –20 Transaction Processing Council (TPC), 43–45 benchmarks overview, D-18 –20 Transfers, A-16. See also Data transfers Transforms, DSP, E-5 Transient failure, F-70 Transient faults, D-11, 93 Transistor performance, scaling, 21 –23 Translation lookaside buffer (TLB) address translation, B-37, B-46, B-47 ARM Cortex-A53, 251 –252 coining term, M-9 interconnection network protection, F-91 misses, 346 Opteron, B-47 , B-56 –57 speculation, 237 –238 Transmission Control Protocol (TCP), congestion management, F-69 Transmission Control Protocol/Internet Protocol (TCP/IP), F-86 ATM, F-102 –103 headers, F-88 internetworking, F-85 –89 reliance on, F-99 WAN, F-102 Transmission speed, interconnection network performance, F-13 Transmission time, F-14 communication latency, I-3 Transport latency time flight, F-14 topology, F-25 –26 Transport layer, F-84 Transputer, F-105 Trap-handling routines, C-54 Tree-based barrier, large-scale multiprocessor synchronization, I-19 Tree height reduction, H-11 Trellis codes, definition, E-6 –7 TRIPS Edge processor, F-67 Trojan horses, B-51 –53True dependence, finding, H-7 –8 True sharing misses, 393 –394, 397, 398 TSMC, Stratton, F-3 TSO. SeeTotal store ordering (TSO) TSS operating system, M-9 Turbo mode 2008, 28 Turing, Alan, M-4, M-20 Turn Model routing algorithm, F-48 Two-dimensional line buffer, 589 –590 Two-level predictors, 183, 191 Two’s complement, J-7 –8 Two-way set associativity, B-8 average memory access time, B-19 conflict misses, B-23 Opteron data cache, B-13 –14,B-13 2:1 cache rule thumb, B-29 TX-2, M-35, M-50 U Ultrix, DECstation 5000 reboots, F-73 UMA. SeeUniform memory access (UMA) Unbiased exponent, J-15 Uncached state, 406 Underflow floating-point arithmetic, J-36 –37, J-62 gradual, J-15, J-36 Unicasting, shared-media networks, F-24 Unified buffer, 558 Unified cache AMD Opteron example, B-15, B-15 miss rate, B-16 Unified virtual memory, 330 Uniform memory access (UMA), 371 Uninterruptible power supply (UPS), 504 Uniprocessor, 377 –378 cache coherence mechanism, 384, 386 throughput, 242 –247 Unit stride addressing, A-31 –32 UNIVAC I, M-5, M-17 UNIX systems, B-38 block servers vs.filers, D-34 floating point remainder, J-32 miss statistics, B-59 seek distance comparison, D-47Index ■I-45UNIX systems (Continued) vector processor history, G-26 workload, 399 Unoptimized code, C-70 Unpacked decimal, A-14, J-16 Unshielded twisted pair (UTP), F-104 Up*/down* routing, F-49 USB, Sony PlayStation 2 Emotion Engine case study, E-15 Use bit, B-45 –46, B-52 User-level communication, F-8 User maskable events, exception, C-39 User nonmaskable events, exception, C-39 User requested events, exception, C-39 User Space Driver, 563 Utility computing, M-75 –76 Utilization I/O system calculations, D-26 queuing theory, D-25 V Valid bit, B-8, 383, 393 –394 address translation, B-46 AMD Opteron data cache, B-14 page table entry, B-52 Value prediction, 228, 234 VAPI, InfiniBand, F-81 Variable length, 14 Variables, random, distribution, D-26 –34 VAX architecture fallacies pitfalls, K-65 –67 instructions encoding, K-54 –55 operands addressing modes, K-51 –54 operations, K-56 –57 sort, K-62 –65 swap, K-59 –61 Vector architecture, 10, A-31, 282 computer development, M-45 –46 execution time, 290 –293 fallacy, 356 vs.graphics processing units, 331–334 memory banks, 298 –299 memory systems, G-9 –11 multidimensional arrays, 299 –301 multiple lanes, 293 –294 pitfall, 355 –356 predicate registers, 296 –298processor example, 288 –290 programming, 302 –304 RV64V extension, 283 –287,284 sparse matrices, 301 –302 start-up latency dead time, G-8 vector-length registers, 294 –296 vector-register characteristics, G-3 Vector array, 585 Vector element, 289 Vector functional units, 285 Vector instruction definition, 289 instruction-level parallelism, 170 Vectorized code, 289 Vectorizing compilers effectiveness, G-14 FORTRAN test kernels, G-15 sparse matrices, G-12 –13 Vector kernel implementation, case study, 357 –359 Vector-length register (VLR), 294 –296 performance, G-4 –5 Vector load bandwidth for, 298 –299 definition, 285 Vector-mask control, 297Vector-mask registers Cray X1, G-21 –22 Vector processor Cray X1, G-21 –24,G-22 –23 Cray X1E, G-24 DAXPY VMIPS, G-17, G-19 –21 definition, 370 DSP media extensions, E-10 execution time, G-7 historical background, G-26 –28 measures, G-15 –16 NEC SX-9 vector processor, 309 overview, G-25 –26 performance, G-2 –9 chaining, G-11 –12,G-12 DAXPY VMIPS, G-17 sparse matrices, G-12 –14 start-up multiple lanes, G-7–9 unchaining, G-12 vs.scalar processor, G-19 Sony PlayStation 2 Emotion Engine, E-17 –18 start-up overhead, G-4vector kernel implementation, 357–359 VMIPS DAXPY, G-17, G-19 –21 VMIPS Linpack, G-17 –19 Vector registers, 284 Very-large-scale integration (VLSI) early computer arithmetic, J-63 interconnection network topology, F-30 RISC history, M-21 Wallace tree, J-52 –53 long instruction word (VLIW) compiler history, M-32 EPIC approach, M-33 IA-64, H-33 instruction set, 587, 587 multiple issue processors, 218 –222, 220,271 multiple-issue processors, M-30 multithreading history, M-36TI 320C6x DSP, E-8 –10 VGA controller, M-51 VI interface, M-63, M-74 Virtual address AMD Opteron data cache, B-12 –13 memory hierarchy, B-39 miss rate vs.cache size, B-37 Opteron mapping, B-55 Opteron memory management, B-54 –57,B-55 page size, B-58 physical address, B-45 translation, B-36 –40 virtual memory, B-41 , B-42, B-44 Virtual cache, B-36 –38 Virtual channels (VCs), F-47 –48 HOL blocking, F-60 switching, F-52 switch microarchitecture pipelining, F-66 system area network, F-105 –106 throughput, F-97 Virtual cut-through switching, F-52 Virtual functions, A-17 Virtual instruction set architecture (VISA), 587 –588 Virtualization Intel 80x86 instruction, 145 memory hierarchy design, 126 –127I-46 ■IndexVirtual Machine Control State (VMCS), 146 Virtual machine monitor (VMM), 121 instruction set extension, 124 –125 laissez faire attitude, 145 pitfall, 145 requirements, 122 Xen virtual machine, 126 Virtual machines (VMs), 491 early IBM work, M-10 hardware management, 121 impact virtual memory, 123 –124 instruction set architecture for, 122–123 protection via, 120 –122 software management, 121 Virtual memory, B-2 –3, B-40 –49 address space, B-12, B-41 , B-44, B-55 address translation, B-46, B-47 caches and, B-42 –43,B-42 , B-48 –49,B-48 classes, B-43 instruction set extension, 124 –125 Intel Pentium vs.AMD Opteron, B-57 paged example, B-54 –57 page size selection, B-46 –47 parameter ranges, B-42 Pentium vs.Opteron protection, B-57 protection, B-49 –50, 119 –120 questions, B-44 –46 segmented example, B-51 –54 virtual machine, 123 –124 Virtual output queues (VOQs), F-60–61 VLIW. SeeVery Long Instruction Word (VLIW) VLR. SeeVector-length register (VLR) VLSI. SeeVery-large-scale integration (VLSI) VME rack, D-37 –38,D-38 VMIPS DAXPY, G-18 –21 enhanced, DAXPY performance, G-19 –21 peak performance DAXPY, G-17 performance, G-4 Linpack, G-17 –19sparse matrices, G-13 start-up penalties, G-5 vector execution time, G-6 –7 vector performance measures, G-16 Voltage regulator controller (VRC), F-74 Volume, cost, 30 –31 Von Neumann computer, M-2 –3 Von Neumann, John, M-2 –5 Voodoo2, M-51 W Wafer definition, 31 RISC-V dies, 33 yield, 34 Waiting line, D-25 Wallace tree example, J-52 –53,J-53 historical background, J-63 Wall-clock time, 39 scientific applications parallel processors, I-33 WAR. SeeWrite read (WAR) Warehouse-scale computer (WSC), 4–5, 9–10, 369 –370, 466 active vs.inactive low power modes, 516 average memory latency, 480 capital costs, 516 case study, 487 cloud computing advantages, 490 AWS ( seeAmazon Web Services (AWS)) economies scale, 491 fallacy, 514 cluster history, M-74 –75 computer architecture of, 477 –482 cost, 486 –490 cost-performance, 515, 517 cost trends, 36 efficiency cost, 482 –490 energy, 503 measuring, 483 –486 fault tolerance, 516 Google cooling, 506 –508 networking, 510 –511 power distribution, 504 –506racks, 509 –510 servers, 512 –513 hierarchy switches, 477 Layer 3 network, 481 low-power servers, 519 –521 memory hierarchy, 479 –482 microsecond delays, 517 opportunities/problems, 468 performance, 514 power utilization effectiveness, 483,484 preventing, 501 –503 programming models workloads, 471 –476 resource allocation, 521 –522 server cost power, 519 –521 storage, 478 total cost ownership, 519 –521 Warp, M-32 Water-side economization, 508 Wavelength division multiplexing (WDM), F-103 WAW. SeeWrite write (WAW) Way prediction, hit time, 98 –99 WB cycle. SeeWrite-back (WB) cycle WCET. SeeWorst-case execution time (WCET) Weak ordering, 420, 421 Weak scaling, 439 Web servers benchmarking, D-21 WAN, F-102 Weighted arithmetic mean time, D-27 Weight FIFO, 558 Weight memory, 558 Weitek 3364 arithmetic functions, J-57 –61 chip comparison, J-58 chip layout, J-59–60 West-first routing, F-48 Wet-bulb temperature, 508 Whirlwind project, M-4 Wide area networks (WANs) ATM, F-4 characteristics, F-4 cross-company interoperability, F-68 effective bandwidth, F-19 fault tolerance, F-71 –73 historical overview, F-102 –103 InfiniBand, F-77 –78Index ■I-47Wide area networks (WANs) (Continued) interconnection network domain relationship, F-4, F-5 latency effective bandwidth, F-27 –29 offload engines, F-8 packet latency, F-13, F-14 –16 switching, F-51 time flight, F-14 Wilkes, Maurice, M-3 Winchester disk design, M-86 Window, F-69 TCP/IP headers, F-88 Wireless networks basic challenges, E-21 cell phones, E-21 –22 Wires, scaling of, 21 –23 Within instructions exception, C-39 instruction set complications, C-45 stopping/restarting exception, C-41 Word(s) AMD Opteron data cache, B-14 –15 double, A-7, A-8,A-14 ,A-44 , 300 DSP, E-6half, A-8, A-8,A-14 ,A-44 Word count, B-53 Word displacement addressing, K-52 Word offset, C-28 Working set effect, I-24 Workload, 39 –40 commercial, 394 –399 measurements, 400 multiprogramming OS, 399–404 phases, 399 –400 RAID performance prediction, D-57 –59 scalability Xeon E7 with, 433–434 symmetric shared-memory multiprocessors, I-21–26,I-23–26 warehouse-scale computers, 471–476 Wormhole switching, F-52, F-97, F-105 Worst-case execution time (WCET), E-4Write read (WAR) dynamic scheduling, 193 hazard, C-12, C-69 multiple-issue processors, M-30 program order, 174 register renaming, 196 TI TMS320C55 DSP, E-8Tomasulo ’s algorithm, 195, 207 Write write (WAW), C-12 check for, C-52 dynamic scheduling, 193 longer latency pipelines, C-49, C-51 multiple-issue processors, M-30 program order, 173 register renaming, 196 Tomasulo ’s algorithm, 195 Write allocate, B-11 –12 Write-back cache, B-11 –12 cache coherence protocol, 385, 385 directory-based cache coherence protocol, 411 memory hierarchy, 81 snooping coherence, 380 –384,381 uniprocessor, 386 Write-back (WB) cycle data hazards stall minimization, C-13 –14 MIPS R4000 pipeline, C-58 –59 multicycle FP operations, C-52 RISC classic pipeline, C-8 RISC exception, C-43 RISC instruction set, C-5, C-6 RISC pipeline, C-33 RISC pipeline control, C-35, C-36 simple RISC implementation, C-29 Write broadcast protocol, 381 Write buffer, B-11, B-14, 382 memory hierarchy, 81 merging, 105 –106,106 Write hit cache coherence, 384–385, 386, 387 definition, B-11 Write invalidate protocol, 380 example, 385, 385 implementation, 382 –383 snooping coherence, 381 Write miss, 411, 418 AMD Opteron data cache, B-12cache coherence, 384–385, 385, 387 directory-based cache coherence protocol, 411 memory stall clock cycles, B-4 miss penalty reduction, B-35 –36 operation, 408 Opteron data cache, B-12, B-14 options, B-11 Write result, 199 dynamic scheduling scoreboard, C-69 hardware-based speculation, 217 instruction step, 211 Write serialization cache coherence, 378 –379 definition, 380 –382, 413 Write stall, B-11 Write strategy memory hierarchy, B-45 –46 virtual memory, B-45 –46 Write-through cache, B-11 –12 average memory access time, B-16 –17 coherence protocol, 378, 382 –384 memory hierarchy, 81 Write update protocol, 381 X XALANCBMK benchmarks, 138XBox, M-51 –52 Xen virtual machine, 126 Xeon E7, 389, 426 –429 feature, 427 on-chip organizations, 428 performance, 431, 432 QuickPath Interconnect, 429 scalability, 433 –434,434 Xerox Palo Alto Research Center, F-103 XIMD architecture, M-36 Xon/Xoff, interconnection networks, F-10–11, F-18 Z Zero-copy protocols, F-8 Zero-load latency, F-75 Z-80 microcontroller, cell phones, E-24 Zuse, Konrad, M-4 –5I-48 ■IndexTranslation GPU terms book official NVIDIA OpenCL terms. TypeMore Descriptive Name used BookOfficial CUDA/ NVIDIA Term Book Definition OpenCL Terms Official CUDA/NVIDIA DefinitionProgram AbstractionsVectorizable LoopGrid vectorizable loop, executed GPU, made 1 “Thread Blocks ”(or bodies vectorized loop) execute parallel. OpenCL name “index range. ”A Grid array Thread Blocks execute concurrently, sequentially, mixture. Body Vectorized LoopThread Block vectorized loop executed “Streaming Multiprocessor ”(multithreaded SIMD processor), made 1 “Warps ”(or threads SIMD instructions). “Warps ” (SIMD Threads) communicate via “Shared Memory ”(Local Memory). OpenCL calls thread block “work group. ”A Thread Block array CUDA threads execute concurrently together cooperate communicate via Shared Memory barrier synchronization. Thread Block Thread Block ID within Grid. Sequence SIMD Lane OperationsCUDA Thread vertical cut “Warp ”(or thread SIMD instructions) corresponding one element executed one “Thread Processor ”(or SIMD lane). Result stored depending mask. OpenCL calls CUDA thread “work item. ”A CUDA Thread lightweight thread executes sequential program cooperate CUDA threads executing Thread Block. CUDA thread thread ID within Thread Block.Machine ObjectA Thread SIMD InstructionsWarp traditional thread, contains SIMD instructions executed “Streaming Multiprocessor ”(multithreaded SIMD processor). Results stored depending per element mask.A Warp set parallel CUDA Threads (e.g., 32) execute instruction together ina multithreaded SIMT/SIMD processor. SIMD InstructionPTX Instruction single SIMD instruction executed across “Thread Processors ”(SIMD lanes).A PTX instruction specifies instruction executed CUDA Thread.Processing HardwareMultithreaded SIMD ProcessorStreaming MultiprocessorMultithreaded SIMD processor executes “Warps ”(thread SIMD instructions), independent SIMD processors. OpenCL calls “Compute Unit. ”However, CUDA programmer writes program one lane rather “vector ”of multiple SIMD lanes.A Streaming Multiprocessor (SM) multithreaded SIMT/SIMD processor executes Warps CUDA Threads. SIMT program specifies execution one CUDA thread, rather vector multiple SIMD lanes. Thread Block SchedulerGiga Thread EngineAssigns multiple “Thread Blocks ”(or body vectorized loop) “Streaming Multiprocessors ” (multithreaded SIMD processors).Distributes schedules Thread Blocks Grid Streaming Multiprocessors resources become available. SIMD Thread SchedulerWarp SchedulerHardware unit schedules issues “Warps ” (threads SIMD instructions) ready execute; includes scoreboard track“Warp ”(SIMD thread) execution.A Warp Scheduler Streaming Multiprocessor schedules Warps execution next instruction ready execute. SIMD Lane Thread ProcessorHardware SIMD Lane executes operations “Warp ”(thread SIMD instructions) single element. Results stored depending mask. OpenCL calls “Processing Element. ”A Thread Processor datapath register file portion Streaming Multiprocessor thatexecutes operations one lanes Warp.Memory HardwareGPU Memory Global Memory DRAM memory accessible “Streaming Multiprocessors ”(or multithreaded SIMD processors) GPU. OpenCL calls “Global Memory. ”Global Memory accessible CUDA Threads Thread Block Grid. Implemented region DRAM, may cached. Private MemoryLocal Memory Portion DRAM memory private “Thread Processor ”(SIMD lane). OpenCL calls it“Private Memory. ”Private “thread-local ”memory CUDA Thread. Implemented cached region ofDRAM. Local Memory Shared MemoryFast local SRAM one “Streaming Multiprocessor ”(multithreaded SIMD processor), unavailable Streaming Multiprocessors. OpenCL calls “Local Memory. ”Fast SRAM memory shared CUDA Threads composing Thread Block, privateto Thread Block. Used communication among CUDA Threads Thread Block barrier synchronization points. SIMD Lane RegistersRegisters Registers single “Thread Processor ”(SIMD lane) allocated across full “Thread Block ”(or body vectorized loop).Private registers CUDA Thread. Implemented multithreaded register file certain lanes several warps thread processor.RV64G Instruction Subset Mnemonic Function Data transfer Move data to/from GPRs FPRs lb,lbu,lh,lhu,lw,lwu Load byte, half word, word lower portion GPR with/without sign extension ld,sd sb,sh,sw fld,flw,fsd,fswLoad store double word GPR Store byte, half word, word lowest portion GPR memoryLoad store double word word to/from FPRs ALU Operations Register-register register immediat e ALU operations add, addi,addw,addiw Add, add immediate, add word, add word immediate. Word version affects lower 32 bits. and,andi,or,ori,xor,xori AND, immediate, immediate, exclusive OR, exclusive immediate auipc Add upper immediate PC; puts sum shifted immediate PC register lui Loads immediate value upper portion word. mul,mulw,mulh,mulhsu, mulhuMultiply, multiply word, multiply halfwor d, multiply upper half, signed unsigned. Word affects lower 32 bits. div,diw,divu Divide, divide wor d, divide unsigned. rem,remw,remu,remuw Remainder, remainder wo rd, remainder unsigned. sll,slli,srl,srli,sra,srai Shift left /right logical, right arithmetic, immediate shift amount GPR. sllw,sllwi,srlw,srlwi, sraw, sraiwWord shifts: affect lower 32-bits GPR. slt,slti,sltiu,sltu Set Less then: first operand less second, set destination 1 else 0; immediate form signed/unsigned. sub,subi,subw,subwi Subtract, subtract immediate. Word version affects lower 32 bits. Control Transfer Branches, jumps, procedure calls beq,bge,bgeu,blt,bltu,bne Compare two registers condition true branch PC + offset jal,jalr Jump, Jump register contents. address next instruction saved designated register. Unconditional jump without link setting destination register x0. Floating Point Operations Floating point instructions operating FPRs. fadd.*,fsub.*,fmul.*, fdiv.* ,fsrt.*FP add, subtract, multiply, divide, square root; single (.s) double (.d) precision versions. fmadd.*,fmsub.*,fmnadd.*, fmnsub.*Multiply-add, multiply-subtract, negte multiply -add, negate multiply-subtract; single (.s) double (.d) precision versions. fsgnj.*,sgnjn.*,fsgnjx.* Copy sign, inverse sign, XOR sign first operand; single (.s) double (.d) precision versions. fmin.*,fmax.* Minimum maximum two values; single (.s) double (.d) precision versions. feq.*,flt.*,fle.* Floating point compares; single (.s) double (.d) precision versions. fclass.* Classify type FP value; single (.s) double (.d) precision versions. fmv.*.x,fmv.x.* Move from/to GPRs; single (.s) double (.d) precision versions. fcvt.d.s,fcvt.s.d Convert SP DP DP SP fcvt.*.w,fcvt.*.wu, fcvt.*.i,fct.*.lu fcvt.i.*,fcvt.lu.*Convert word double word, signed unsigned DP DP. fcvt.w.*,fcvt.wu.*, Convert word double word, signed unsigned.