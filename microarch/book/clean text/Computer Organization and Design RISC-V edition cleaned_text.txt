Computer Organization Design Hardware/Software Interface: RISC-V Edition David A. Patterson University California, Berkeley John L. Hennessy Stanford University RISC-V UPDATES CONTRIBUTIONS 2Andrew S. Waterman SiFive, Inc. Yunsup Lee SiFive, Inc. ADDITIONAL CONTRIBUTIONS Perry Alexander University Kansas Peter J. Ashenden Ashenden Designs Pty Ltd Jason D. Bakos University South Carolina Javier Diaz Bruguera Universidade de Santiago de Compostela Jichuan Chang Google Matthew Farrens University California, Davis 3David Kaeli Northeastern University Nicole Kaiyan University Adelaide David Kirk NVIDIA Zachary Kurmas Grand Valley State University James R. Larus School Computer Communications Science EPFL Jacob Leverich Stanford University Kevin Lim Hewlett-Packard Eric Love University California, Berkeley John Nickolls NVIDIA 4John Y. Oliver Cal Poly, San Luis Obispo Milos Prvulovic Georgia Tech Partha Ranganathan Google Mark Smotherman Clemson University 5Table Contents Title page Praise Computer Organization Design: Hardware/Software Interface Copyright Dedication Acknowledgments Preface Book Book RISC-V Edition? Changes Fifth Edition Instructor Support Concluding Remarks Acknowledgments 61. Computer Abstractions Technology Abstract 1.1 Introduction 1.2 Eight Great Ideas Computer Architecture 1.3 Program 1.4 Covers 1.5 Technologies Building Processors Memory 1.6 Performance 1.7 Power Wall 1.8 Sea Change: Switch Uniprocessors Multiprocessors 1.9 Real Stuff: Benchmarking Intel Core i7 1.10 Fallacies Pitfalls 1.11 Concluding Remarks Historical Perspective Reading 1.12 Historical Perspective Reading 1.13 Exercises 2. Instructions: Language Computer Abstract 2.1 Introduction 2.2 Operations Computer Hardware 2.3 Operands Computer Hardware 2.4 Signed Unsigned Numbers 2.5 Representing Instructions Computer 72.6 Logical Operations 2.7 Instructions Making Decisions 2.8 Supporting Procedures Computer Hardware 2.9 Communicating People 2.10 RISC-V Addressing Wide Immediates Addresses 2.11 Parallelism Instructions: Synchronization 2.12 Translating Starting Program 2.13 C Sort Example Put Together 2.14 Arrays versus Pointers Advanced Material: Compiling C Interpreting Java 2.15 Advanced Material: Compiling C Interpreting Java 2.16 Real Stuff: MIPS Instructions 2.17 Real Stuff: x86 Instructions 2.18 Real Stuff: Rest RISC-V Instruction Se 2.19 Fallacies Pitfalls 2.20 Concluding Remarks Historical Perspective Reading 2.22 Historical Perspective Reading 2.22 Exercises 3. Arithmetic Computers Abstract 3.1 Introduction 3.2 Addition Subtraction 3.3 Multiplication 83.4 Division 3.5 Floating Point 3.6 Parallelism Computer Arithmetic: Subword Para llelism 3.7 Real Stuff: Streaming SIMD Extensions Advanced V ector Extensions x86 3.8 Going Faster: Subword Parallelism Matrix Multi ply 3.9 Fallacies Pitfalls 3.10 Concluding Remarks Historical Perspective Reading Historical Perspective Reading 3.12 Exercises 4. Processor Abstract 4.1 Introduction 4.2 Logic Design Conventions 4.3 Building Datapath 4.4 Simple Implementation Scheme 4.5 Overview Pipelining 4.6 Pipelined Datapath Control 4.7 Data Hazards: Forwarding versus Stalling 4.8 Control Hazards 4.9 Exceptions 4.10 Parallelism via Instructions 4.11 Real Stuff: ARM Cortex-A53 Intel Core 7 Pipelines 94.12 Going Faster: Instruction-Level Parallelism Mat rix Multiply Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline ore Pipelining Illustrations 4.13 Advanced Topic: Introduction Digital Design U sing Hardware Design Language Describe Model Pipel ine Pipelining Illustrations 4.14 Fallacies Pitfalls 4.15 Concluding Remarks Historical Perspective Reading 4.16 Historical Perspective Reading 4.17 Exercises 5. Large Fast: Exploiting Memory Hierarchy Abstract 5.1 Introduction 5.2 Memory Technologies 5.3 Basics Caches 5.4 Measuring Improving Cache Performance 5.5 Dependable Memory Hierarchy 5.6 Virtual Machines 5.7 Virtual Memory 5.8 Common Framework Memory Hierarchy 5.9 Using Finite-State Machine Control Simple Cache 5.10 Parallelism Memory Hierarchy: Cache Coherence Parallelism Memory Hierarchy: Redundant Arrays Ine xpensive Disks 105.11 Parallelism Memory Hierarchy: Redundant Arrays f Inexpensive Disks Advanced Material: Implementing Cache Controllers 5.12 Advanced Material: Implementing Cache Controllers 5.13 Real Stuff: ARM Cortex-A53 Intel Core 7 Memory Hierarchies 5.14 Real Stuff: Rest RISC-V System Spe cial Instructions 5.15 Going Faster: Cache Blocking Matrix Multiply 5.16 Fallacies Pitfalls 5.17 Concluding Remarks Historical Perspective Reading 5.18 Historical Perspective Reading 5.19 Exercises 6. Parallel Processors Client Cloud Abstract 6.1 Introduction 6.2 Difficulty Creating Parallel Processing Progr ams 6.3 SISD, MIMD, SIMD, SPMD, Vector 6.4 Hardware Multithreading 6.5 Multicore Shared Memory Multiprocessors 6.6 Introduction Graphics Processing Units 6.7 Clusters, Warehouse Scale Computers, Message - Passing Multiprocessors 6.8 Introduction Multiprocessor Network Topologies Communicating Outside World: Cluster Networking 116.9 Communicating Outside World: Cluster Networ king 6.10 Multiprocessor Benchmarks Performance Models 6.11 Real Stuff: Benchmarking Rooflines Int el Core i7 960 NVIDIA Tesla GPU 6.12 Going Faster: Multiple Processors Matrix Multipl 6.13 Fallacies Pitfalls 6.14 Concluding Remarks Historical Perspective Reading 6.15 Historical Perspective Reading 6.16 Exercises Appendix Appendix A. Basics Logic Design A.1 Introduction A.2 Gates, Truth Tables, Logic Equations A.3 Combinational Logic A.4 Using Hardware Description Language A.5 Constructing Basic Arithmetic Logic Unit A.6 Faster Addition: Carry Lookahead A.7 Clocks A.8 Memory Elements: Flip-Flops, Latches, Registers A.9 Memory Elements: SRAMs DRAMs A.10 Finite-State Machines A.11 Timing Methodologies A.12 Field Programmable Devices 12A.13 Concluding Remarks A.14 Exercises Appendix B. Graphics Computing GPUs B.1 Introduction B.2 GPU System Architectures B.3 Programming GPUs B.4 Multithreaded Multiprocessor Architecture B.5 Parallel Memory System B.6 Floating-point Arithmetic B.7 Real Stuff: NVIDIA GeForce 8800 B.8 Real Stuff: Mapping Applications GPUs B.9 Fallacies Pitfalls B.10 Concluding Remarks B.11 Historical Perspective Reading Reading Appendix C. Mapping Control Hardware C.1 Introduction C.2 Implementing Combinational Control Units C.3 Implementing Finite-State Machine Control C.4 Implementing Next-State Function Seque ncer C.5 Translating Microprogram Hardware C.6 Concluding Remarks C.7 Exercises 13Appendix D. Survey RISC Architectures Desktop, Server, Embedded Computers D.1 Introduction D.2 Addressing Modes Instruction Formats D.3 Instructions: MIPS Core Subset D.4 Instructions: Multimedia Extensions Desktop/Serve r RISCs D.5 Instructions: Digital Signal-Processing Extensions th e Embedded RISCs D.6 Instructions: Common Extensions MIPS Core D.7 Instructions Unique MIPS-64 D.8 Instructions Unique Alpha D.9 Instructions Unique SPARC v9 D.10 Instructions Unique PowerPC D.11 Instructions Unique PA-RISC 2.0 D.12 Instructions Unique ARM D.13 Instructions Unique Thumb D.14 Instructions Unique SuperH D.15 Instructions Unique M32R D.16 Instructions Unique MIPS-16 D.17 Concluding Remarks Reading Answers Check Chapter 1 Chapter 2 Chapter 3 14Chapter 4 Chapter 5 Chapter 6 Glossary Reading Index RISC-V Reference Data Card (“Green Card”) 15In Praise Computer Organization Design: Hardware/Software Interface “Textbook selection often frustrating act compromise— pedagogy, content coverage, quality exposition, level rigor, cost. Computer Organization Design rare book hits right notes across board, without compromise. premier computer organization textbook, shining example computer science textbooks could be.” Michael Goldweber, Xavier University “I using Computer Organization Design years, first edition. new edition yet another outstanding improvement already classic text. evolution desktop computing mobile computing Big Data brings new coverage embedded processors ARM, new material software hardware interact increase performance, cloud computing. without sacrificing fundamentals.” 16Ed Harcourt, St. Lawrence University “To Millennials: Computer Organization Design computer architecture book keep (virtual) bookshelf. book old new, develops venerable principles— Moore’s Law, abstraction, common case fast, redundancy, memory hierarchies, parallelism, pipelining—but illustrates contemporary designs.” Mark D. Hill, University Wisconsin-Madison “The new edition Computer Organization Design keeps pace advances emerging embedded many-core (GPU) systems, tablets smartphones will/are quickly becoming new desktops. text acknowledges changes, continues provide rich foundation fundamentals computer organization design needed designers hardware software power new class devices systems.” Dave Kaeli, Northeastern University “Computer Organization Design provides introduction computer architecture. prepares reader changes necessary meet ever-increasing performance needs mobile systems big data processing time difficulties semiconductor scaling making systems power constrained. new era computing, hardware software must co- designed system-level architecture critical component- level optimizations.” Christos Kozyrakis, Stanford University “Patterson Hennessy brilliantly address issues ever- changing computer hardware architectures, emphasizing interactions among hardware software components various abstraction levels. interspersing I/O parallelism concepts w ith variety mechanisms hardware software throughout 17book, new edition achieves excellent holistic presentation computer architecture post-PC era. book essentia l guide hardware software professionals facing energy efficienc parallelization challenges Tablet PC Cloud computing.” Jae C. Oh, Syracuse University 18Copyright Morgan Kaufmann imprint Elsevier 50 Hampshire Street, 5th Floor, Cambridge, 02139, United States Copyright © 2018 Elsevier Inc. rights reserved. part publication may reproduced transmitted form means, electronic mechanical, including photocopying, recording, information storage retrieval system, without permission writing publisher. Detai ls seek permission, information Publisher’s permissions policies arrangements organizations Copyright Clearance Center Copyright Licensing Agency, found website: www.elsevier.com/permissions . book individual contributions contained protected copyright Publisher (other may noted herein). Notices Knowledge best practice field constantly changing . new research experience broaden understanding, changes research methods, professional practices, medical treatment may become necessary. Practitioners researchers must always rely experience knowledge evaluating using 19information, methods, compounds, experiments described herein. using information methods mindful safety safety others, including parties professional responsibility. fullest extent law, neither Publisher authors, contributors, editors, assume liability inju ry and/or damage persons property matter products liability, negligence otherwise, use operation f methods, products, instructions, ideas contained material herein. RISC-V RISC-V logo registered trademarks managed RISC-V Foundation, used permission RISC-V Foundation. rights reserved. publication independent RISC-V Foundation, affiliated publisher RISC-V Foundation authorize, sponsor, endorse otherwise approve publication. material relating ARM® technology reproduced permission ARM Limited, used education purposes. ARM-based models shown referred text must used, reproduced distributed comme rcial purposes, event shall purchasing textbook construed granting third party, expressly implication, estoppel otherwise, license use ARM technology know how. Materials provided ARM copyright © ARM Limited (or affi liates). British Library Cataloguing-in-Publication Data catalogue record book available British Library Library Congress Cataloging-in-Publication Data catalog record book available Library Congress ISBN: 978-0-12-812275-4 20 Information Morgan Kaufmann publications visit website https://www.elsevier.com/books-and-journals Publisher: Katey Birtcher Acquisition Editor: Steve Merken Development Editor: Nate McFadden Production Project Manager: Lisa Jones Designer: Victoria Pearson Esser Typeset MPS Limited, Chennai, India 21Dedication Linda , been, is, always love life 22Acknowledgments Figures 1.7 , 1.8 Courtesy iFixit ( www.ifixit.com ). Figure 1.9 Courtesy Chipworks ( www.chipworks.com ). Figure 1.13 Courtesy Intel. Figures 1.10.1, 1.10.2, 4.15.2 Courtesy Charles Babbage Institute, University Minnesota Libraries, Minneapolis. Figures 1.10.3, 4.15.1, 4.15.3, 5.12.3, 6.14.2 Courtesy IBM. Figure 1.10.4 Courtesy Cray Inc. Figure 1.10.5 Courtesy Apple Computer, Inc. Figure 1.10.6 Courtesy Computer History Museum. Figures 5.17.1, 5.17.2 Courtesy Museum Science, Boston. Figure 5.17.4 Courtesy MIPS Technologies, Inc. Figure 6.15.1 Courtesy NASA Ames Research Center. 23Preface David A. Patterson beautiful thing experience mysterious. source true art science. Albert Einstein, Believe, 1930 Book believe learning computer science engineering reflect current state field, well introduce principles shaping computing. also feel readers every specialty computing need appreciate organizational paradigms determine capabilities, performance, energy, and, ultimately, success computer systems. Modern computer technology requires professionals every computing specialty understand hardware software. interaction hardware software variety levels also offers framework understanding fundamentals computing. Whether primary interest hardware software, computer science electrical engineering, centr al ideas computer organization design same. Thus, emphasis book show relationship hardware software focus concepts basis current computers. recent switch uniprocessor multicore microprocessors confirmed soundness perspecti ve, given since first edition. programmers could ignore advic e 24and rely computer architects, compiler writers, silicon engineers make programs run faster energy- efficient without change, era over. programs run faster, must become parallel. goal many researchers make possible programmers unaware underlying parallel nature hardware programming, take many years realize vision. view least next decade, programmers going understand hardware/software interface want programs run efficiently parallel computers. audience book includes little experie nce assembly language logic design need understand basic computer organization well readers backgrounds assembly language and/or logic design want learn design computer understand system works performs does. Book readers may familiar Computer Architecture: Quantitative Approach , popularly known Hennessy Patterson. (This book turn often called Patterson Hennessy.) motivation writing earlier book describe princ iples computer architecture using solid engineering fundamentals quantitative cost/performance tradeoffs. used approach combined examples measurements, based commercial systems, create realistic design experiences. goal demonstrate computer architecture could learned using quantitative methodologies instead descriptive approach. intended serious computing professional wanted detailed understanding computers. majority readers book plan become computer architects. performance energy efficiency future software systems dramatically affected, however, well software designers understand basic hardware techniques work system. Thus, compiler writers, operating system designers, database programmers, software engineers need firm grounding principles presented book. Similarly, hardware designers must understand clearly 25effects work software applications. Thus, knew book much subset material Computer Architecture , material extensively revised match different audience. happy result subsequent editions Computer Architecture revised remove introductory material; hence, much less overlap today first editions books. RISC-V Edition? choice instruction set architecture clearly critical pedagogy computer architecture textbook. didn’t want instruction set required describing unnecessary baroque features someone’s first instruction set, matter popul ar is. Ideally, initial instruction set exemplar, jus like first love. Surprisingly, remember fondly. Since many choices time, first edition Computer Architecture: Quantitative Approach invented RISC-style instruction set. Given growing popularity simple elegance MIPS instruction set, switched first edition book later editions othe r book. MIPS served us readers well. It’s 20 years since made switch, billions chips use MIPS continue shipped, typically found embedded devices instruction set nearly invisible. Thus, it’s hard find real computer readers download run MIPS programs. good news open instruction set adheres closely RISC principles recently debuted, rapidly gaining following. RISC-V, developed originally UC Berkeley, cleans quirks MIPS instruction set, offers simple, elegant, modern take instruction set look like 2017. Moreover, proprietary, open-source RISC-V simulators, compilers, debuggers, easily available even open-source RISC-V implementations available written hardware description languages. addition, soon low-cost hardware platforms run RISC-V programs. 26Readers benefit studying RISC-V desi gns, able modify go implementation process order understand impact thei r hypothetical changes performance, die size, energy. exciting opportunity computing industry well education, thus time writing 40 companies joined RISC-V foundation. sponsor list includes virtually major players except ARM Intel, including AMD, Google, Hewlett Packard Enterprise, IBM, Microsoft, NVIDIA, Oracle, Qualcomm. reasons wrote RISC-V edition book , switching Computer Architecture: Quantitative Approach RISC-V well. Given RISC-V offers 32-bit address instructions 64- bit address instructions essentially instructio n set, could switched instruction sets kept address size 32 bits. publisher polled faculty used book found 75% either preferred larger addresses neutral, increased address space 64 bits, may make sense today 32 bits. changes RISC-V edition MIPS edition associated change instruction sets, primarily affects Chapter 2 , Chapter 3 , virtual memory section Chapter 5 , short VMIPS example Chapter 6 . Chapter 4 , switched RISC-V instructions, changed several figures, added “Elaboration” sections, changes simpler feared. Chapter 1 rest appendices virtually unchanged. extensive online documentation combined magnitude RISC-V make difficult come replacement MIPS version Appendix (“Assemblers, Linkers, SPIM Simulator” MIPS Fifth Edition). Instead, Chapters 2 , 3, 5 include quick overviews hundreds RISC-V instructions outside core RISC-V instructions cover detail rest book. Note (yet) saying permanently switching RISC-V. example, addition new RISC-V edition, ARMv8 MIPS versions available sale now. One possibility demand versions 27future editions book, one. We’ll cross brid ge come it. now, look forward reaction feedback effort. Changes Fifth Edition six major goals fifth edition Computer Organization Design demonstrate importance understanding hardware running example; highlight main themes across topics using margin icons introduced early; update examples reflect changeover PC era post-PC era; spread material I/O throughout book rather isolating single chapter; update technical content reflect changes industry since publication fourth edition 2009; put appendices optional sections online instead including CD lower costs make edition viable electronic book. discussing goals detail, let’s look table th e next page. shows hardware software paths material. Chapters 1 , 4, 5, 6 found paths, matter experience focus. Chapter 1 discusses importance energy motivates switch single core multicore microprocessors introduces eight gr eat ideas computer architecture. Chapter 2 likely review material hardware-oriented, essential reading software-oriented, especially readers interested l earning compilers object-oriented programming languages. Chapter 3 readers interested constructing datapath learning floating-point arithmetic. skip par ts Chapter 3 , either don’t need them, offer review. However, introduce running example matrix multiply chapter, showing subword parallels offers fourfold improvement, don’t skip Sections 3.6 3.8. Chapter 4 explains pipelined processors. Sections 4.1 , 4.5, 4.10 give overviews, Section 4.12 gives next performance boost matrix multiply software focus. hardware focus, however, find chapter presents core material; may also, depending background, want read Appendix logic design first. last chapter, multicores, multiprocessors, clusters, mostly new conte nt 28and read everyone. significantly reorganized edition make flow ideas natural include much depth GPUs, warehouse-scale computers, hardware–software interface network interface cards key clusters. 29The first six goals fifth edition demonstr ate importance understanding modern hardware get good performance energy efficiency concrete example. 30mentioned above, start subword parallelism Chapter 3 improve matrix multiply factor 4. double performance Chapter 4 unrolling loop demonstrate value instruction-level parallelism. Chapter 5 doubles performance optimizing caches using blocking. Finally, Chapter 6 demonstrates speedup 14 16 processors using thread- level parallelism. four optimizations total add 24 lines C code initial matrix multiply example. second goal help readers separate forest trees identifying eight great ideas computer architectur e early pointing places occur throughout rest book. use (hopefully) easy-to-remember margin icons highlight corresponding word text remind reade rs eight themes. nearly 100 citations book. chapter less seven examples great ideas, idea cited less five times. Performance via parallelism, pipelining , prediction three popular great ideas, followed closely Moore’s Law. Chapter 4 , Processor, one examples, surprise since probably received attention computer architects. one gr eat idea found every chapter performance via parallelism, pleasant observation given recent emphasis parallelism field editions book. third goal recognize generation change computing PC era post-PC era edition examples material. Thus, Chapter 1 dives guts tablet computer rather PC, Chapter 6 describes computing infrastructure cloud. also feature ARM, instruction set choice personal mobile de vices post-PC era, well x86 instruction set dominated PC era (so far) dominates cloud computing. fourth goal spread I/O material throughout book rather chapter, much spread parallelism throughout chapters fourth edition. Hen ce, I/O material edition found Sections 1.4 , 4.9, 5.2, 5.5, 5.11, 6.9. thought readers (and instructors) likely cover I/O it’s segregated chapter. fast-moving field, and, always case new editions, important goal update technical content. 31running example ARM Cortex A53 Intel Core i7, reflecting post-PC era. highlights include tutorial n GPUs explains unique terminology, depth warehouse-scale computers make cloud, deep dive 10 Gigabyte Ethernet cards. keep main book short compatible electronic books, placed optional material online appendices instead companion CD prior editions. Finally, updated exercises book. elements changed, preserved useful book elements prior editions. make book work better reference, still place definitions new terms margins first occurrence. book element called “Understanding Program Performance” sections helps readers understand performance programs improve it, “Hardware/Software Interface” book element helped readers understand tradeoffs interface. “The Big Picture” section remains reader sees forest despite trees. “Check Yourself” sections help readers confirm comprehension material first time answers provided end chapter. edition still includes green RISC-V reference card, inspired “Green Card” IBM System/360. card updated handy reference writing RISC-V assembly language programs. Instructor Support collected great deal material help instructors teach courses using book. Solutions exercises, figures book, lecture slides, materials available instructor register publisher. addition, companion Web site provides links free RISC-V software. Check publish er’s Web site information: textbooks.elsevier.com/9780128122754 Concluding Remarks 32If read following acknowledgments section, see went great lengths correct mistakes. Since book goes many printings, opportunity make even corrections. uncover remaining, resilient bugs, please contact publisher electronic mail codRISCVbugs@mkp.com low-tech mail using address found copyright page. edition third break long-standing collaborati Hennessy Patterson, started 1989. demands running one world’s great universities meant President Hennessy could longer make substantial commitment create new edition. remaining author felt like tightrope walker without safety net. Hence, people acknowledgments Berkeley colleagues played even larger role shaping contents book. Neverthel ess, time around one author blame new material read. Acknowledgments every edition book, fortunate receive help many readers, reviewers, contributors. people helped make book better. grateful assistance Khaled Benkrid colleagues ARM Ltd., carefully reviewed ARM-related material provided helpful feedback. Chapter 6 extensively revised separate review ideas contents, made changes based feedback every reviewer. I’d like thank Christos Kozyrakis Stanford University suggesting using network inte rface clusters demonstrate hardware–software interface I/ suggestions organizing rest chapter; Mario Flagsilk Stanford University providing details, diagrams, performance measurements NetFPGA NIC; following suggestions improve chapter: David Kaeli Northeastern University, Partha Ranganathan HP Labs, David Wood University Wisconsin, Berkeley colleagues Siamak Faridani , Shoaib Kamil , Yunsup Lee , Zhangxi Tan, Andrew Waterman . 33Special thanks goes Rimas Avizenis UC Berkeley, developed various versions matrix multiply supplied performance numbers well. worked father graduate student UCLA, nice symmetry work Rimas UCB. also wish thank longtime collaborator Randy Katz UC Berkeley, helped develop concept great ideas computer architecture part extensive revision undergraduate class together. I’d like thank David Kirk , John Nickolls , colleagues NVIDIA (Michael Garland, John Montrym, Doug Voorhies, Lars Nyland, Erik Lindholm, Paulius Micikevicius, Massimiliano Fatica, Stuart Oberman, Vasily Volkov) writing first in-depth appendix GPUs. I’d like express appreciation Jim Larus , recently named Dean School Computer Communications Science EPFL, willingness contributing expertise assembly language programming, well welcoming readers book regard using simulator developed maintains. also grateful Zachary Kurmas Grand Valley State University, updated created new exercises, based originals created Perry Alexander (The University Kansas); Jason Bakos (University South Carolina); Javier Bruguera (Universidade de Santiago de Compostela); Matthew Farrens (University California, Davis); David Kaeli (Northeastern University); Nicole Kaiyan (University Adelaide); John Oliver (Cal Poly, San Luis Obispo); Milos Prvulovic (Georgia Tech); Jichuan Chang (Google); Jacob Leverich (Stanford); Kevin Lim (Hewlett-Packard); Partha Ranganathan (Google). Additional thanks goes Peter Ashenden updating lecture slides. grateful many instructors answered publisher’s surveys, reviewed proposals, attended focus groups. include following individuals: Focus Groups: Bruce Barton (Suffolk County Community College), Jeff Braun (Montana Tech), Ed Gehringer (North Carolina State), Michael Goldweber (Xavier University), Ed Harcourt (St. Lawrence University), Mark Hill (University Wisconsin, Madison), Patrick Homer (University Arizona), Norm Jouppi (HP Labs), Dave 34Kaeli (Northeastern University), Christos Kozyrakis (Stanford University), Jae C. Oh (Syracuse University), Lu Peng (LSU), Milos Prvulovic (Georgia Tech), Partha Ranganathan (HP Labs), David Wood (University Wisconsin), Craig Zilles (University Illi nois Urbana-Champaign). Surveys Reviews: Mahmoud Abou- Nasr (Wayne State University), Perry Alexander (The University Kansas), Behnam Arad (Sacramento State University), Hakan Aydin (George Mason University), Hussein Badr (State University New York Stony Brook), Mac Baker (Virginia Military Institute), Ron Barnes (George Mason University), Douglas Blough (Georgia Institute Technology), Kevin Bolding (Seattle Pacif ic University), Miodrag Bolic (University Ottawa), John Bonomo (Westminster College), Jeff Braun (Montana Tech), Tom Briggs (Shippensburg University), Mike Bright (Grove City College ), Scott Burgess (Humboldt State University), Fazli (Bilkent University), Warren R. Carithers (Rochester Institute Technology), Bruce Carlton (Mesa Community College), Nicholas Carter (University Illinois Urbana-Champaign), Anthony Cocchi (The City University New York), Cooley (Utah State University), Gene Cooperman (Northeastern University), Rober D. Cupper (Allegheny College), Amy Csizmar Dalal (Carleton College), Daniel Dalle (Université de Sherbrooke), Edward W. Davis (North Carolina State University), Nathaniel J. Davis (Air Force Institute Technology), Molisa Derk (Oklahoma City University), Andrea Di Blas (Stanford University), Derek Eager (University Saskatchewan), Ata Elahi (Souther Connecticut State University), Ernest Ferguson (Northwest Missouri State University), Rhonda Kay Gaede (The University Alabama), Etienne M. Gagnon (L’Université du Québec à Montréal), Costa Gerousis (Christopher Newport University), Paul Gillard (Memorial University Newfoundland), Michael Goldweber (Xavier University), Georgia Grant (College San Mateo), Paul V. Gratz (Texas A&M University), Merrill Hall (The Master’s College), Tyson Hall (Southern Adventist University), Ed Harcourt (St. Lawrence University), Justin E. Harlow (University South Florida), Paul F. Hemler (Hampden-Sydney College), Jayantha Herath (St. Cloud State University), Martin Herbordt (Boston University), Steve J. Hodges (Cabrillo College), Kenneth Hopkinso n (Cornell University), Bill Hsu (San Francisco State University), 35Dalton Hunkins (St. Bonaventure University), Baback Izadi (State University New York—New Paltz), Reza Jafari, Robert W. Johnson (Colorado Technical University), Bharat Joshi (Universi ty North Carolina, Charlotte), Nagarajan Kandasamy (Drexel University), Rajiv Kapadia, Ryan Kastner (University California, Santa Barbara), E.J. Kim (Texas A&M University), Jihong Kim (Seoul National University), Jim Kirk (Union University), Geoff rey S. Knauth (Lycoming College), Manish M. Kochhal (Wayne State), Suzan Koknar-Tezel (Saint Joseph’s University), Angkul Kongmunvattana (Columbus State University), April Kontostathis (Ursinus College), Christos Kozyrakis (Stanford University), anny Krizanc (Wesleyan University), Ashok Kumar, S. Kumar (The University Texas), Zachary Kurmas (Grand Valley State University), Adrian Lauf (University Louisville), Robert N. Le (University Houston), Alvin Lebeck (Duke University), Baoxi n Li (Arizona State University), Li Liao (University Delaware), Gary Livingston (University Massachusetts), Michael Lyle, Douglas W. Lynn (Oregon Institute Technology), Yashwant K Malaiya (Colorado State University), Stephen Mann (University Waterloo), Bill Mark (University Texas Austin), Ananda Mondal (Claflin University), Alvin Moser (Seattle University) , Walid Najjar (University California, Riverside), Vijaykrishnan Narayanan (Penn State University), Danial J. Neebel (Loras College), Victor Nelson (Auburn University), John Nestor (Lafayette College), Jae C. Oh (Syracuse University), Joe Oldham (Centre College), Timour Paltashev, James Parkerson (University Arkansas), Shaunak Pawagi (SUNY Stony Brook), Steve Pearce, Ted Pedersen (University Minnesota), Lu Peng (Louisiana State University), Gregory D. Peterson (The University Tennessee ), William Pierce (Hood College), Milos Prvulovic (Georgia Tech), Partha Ranganathan (HP Labs), Dejan Raskovic (University Alaska, Fairbanks) Brad Richards (University Puget Sound), Roman Rozanov, Louis Rubinfield (Villanova University), Md Abdus Salam (Southern University), Augustine Samba (Kent State University), Robert Schaefer (Daniel Webster College), Carolyn J . C. Schauble (Colorado State University), Keith Schubert (CSU San Bernardino), William L. Schultz, Kelly Shaw (University Richmond), Shahram Shirani (McMaster University), Scott Sigman (Drury University), Shai Simonson (Stonehill College), Bruce mith, 36David Smith, Jeff W. Smith (University Georgia, Athens), Mark Smotherman (Clemson University), Philip Snyder (Johns Hopkin University), Alex Sprintson (Texas A&M), Timothy D. Stanley (Brigham Young University), Dean Stevens (Morningside College) , Nozar Tabrizi (Kettering University), Yuval Tamir (UCLA), Alexander Taubin (Boston University), Thacker (Winthrop University), Mithuna Thottethodi (Purdue University), Manghui Tu (Southern Utah University), Dean Tullsen (UC San Diego), Steve VanderLeest (Calvin College), Christopher Vickery (Queens College CUNY), Rama Viswanathan (Beloit College), Ken Vollmar (Missouri State University), Guoping Wang (Indiana- Purdue University), Patricia Wenner (Bucknell University), Kent Wilken (University California, Davis), David Wolfe (Gustavus Adolphus College), David Wood (University Wisconsin, Madison), Ki Hwan Yum (University Texas, San Antonio), Mohamed Zahran (City College New York), Amr Zaky (Santa Clara University), Gerald D. Zarnett (Ryerson University), Nian Zhang (South Dakota School Mines & Technology), Jiling Zhong (Troy University), Huiyang Zhou (North Carolina State University ), Weiyu Zhu (Illinois Wesleyan University). special thanks also goes Mark Smotherman making multiple passes find technical writing glitches significantly improved quality edition. wish thank extended Morgan Kaufmann family agreeing publish book able leadership Katey Birtcher , Steve Merken , Nate McFadden : certainly couldn’t completed book without them. also want extend thanks Lisa Jones , managed book production process, Victoria Pearson Esser , cover design. cover cleverly connects post-PC era content editi cover first edition. Finally, owe huge debt Yunsup Lee Andrew Waterman taking conversion RISC-V spare time founding startup company. Kudos Eric Love well, made RISC-V versions exercises edition finishing Ph.D. We’re excited see happen RISC-V academia beyond. contributions nearly 150 people mentioned helped make new edition hope best 37book yet. Enjoy! 381 Computer Abstractions Technology Abstract chapter explains although difficult predict ex actly level cost/performance computers futur e, it’s probable much better today. participate advances, computer designers programmers must understand wider variety issues, including factors power, reliability, cost ownership, scalability. chapte r focuses cost, performance, power, emphasizes best designs strike appropriate balance given market among factors. chapter also discusses post-PC era, personal mobile devices (PMDs) tablets largely replacing deskt op computers, Cloud computing warehouse scale computers (WSCs) taking traditional server. Keywords power wall; uniprocessor; multiprocessor; Intel Core i7; desktop co mputer; server; supercomputer; datacenter; embedded computer; multicore microprocessor; syst ems software; operating system; compiler; instruction; assembler; assembly l anguage; machine language; high-level programming language; input de vice; output device; liquid crystal display; active matrix display; motherboard; integrat ed circuit; chip; memory; dynamic random access memory; DRAM; dual inline memory module; DIMM; central processor unit; CPU; datapath; control; cache memory; st atic random access memory; SRAM; abstraction; instruction set architecture; architecture; application binary interface; ABI; implementation; volatile mory; nonvolatile 39memory; main memory; primary memory; secondary memory; magnetic disk; hard disk; flash memory; local area network; LAN; wide area network; WAN ; vacuum tube; transistor; large scale integrated circuit; VLSI; response time; e xecution time; throughput; bandwidth; CPU execution time; CPU time; user CPU time; system CPU time; clock cycle; clock period; clock cycles per instruction; CPI; instructi count; instruction mix; silicon; semiconductor; silicon crystal ingot; wafer; defect; die; yield; workload; benchmark; Amdahl’s Law; million instructions per second; MIPS; cloud computing; mobile computing; cloud; mobile; personal mobile evice; PMD; warehouse scale computer; WSC; Software Service; SaaS Civilization advances extending number important operations perform without thinking them. Alfred North Whitehead, Introduction Mathemati cs, 1911 OUTLINE 1.1 Introduction 3 1.2 Eight Great Ideas Computer Architecture 11 1.3 Program 13 1.4 Covers 16 1.5 Technologies Building Processors Memory 24 1.6 Performance 28 1.7 Power Wall 40 1.8 Sea Change: Switch Uniprocessors Multiprocessors 43 1.9 Real Stuff: Benchmarking Intel Core i7 46 1.10 Fallacies Pitfalls 49 1.11 Concluding Remarks 52 1.12 Historical Perspective Reading 54 1.13 Exercises 54 1.1 Introduction Welcome book! We’re delighted opportunity 40convey excitement world computer systems. dry dreary field, progress glacial new ideas atrophy neglect. No! Computers product incredibly vibrant information technology industry, aspect responsible almost 10% gross national product United States, whose economy become dependent part rapid improvements information technology promised Moore’s Law. unusual industry embraces innovation breath-taking rate. last 30 years, number new computers whose introduction appeared revolutionize computing industry; revolutions wer e cut short someone else built even better computer. race innovate led unprecedented progress since th e inception electronic computing late 1940s. transportation industry kept pace computer industry, example, today could travel New York London second penny. Take moment contemplate improvement would change society—living Tahiti working San Francisco, going Moscow evening Bolshoi Ballet—and appreciate implications change. Computers led third revolution civilization, th e information revolution taking place alongside agricultur al industrial revolutions. resulting multiplication humankind’s intellectual strength reach naturally affected everyday lives profoundly changed ways search new knowledge carried out. new vein scientific investigation, computational scientists joi ning theoretical experimental scientists exploration ne w frontiers astronomy, biology, chemistry, physics, among others. computer revolution continues. time cost computing improves another factor 10, opportunities computers multiply. Applications economically infeas ible suddenly become practical. recent past, following applications “computer science fiction.” Computers automobiles : microprocessors improved dramatically price performance early 1980s, computer control cars ludicrous. Today, computers reduce 41pollution, improve fuel efficiency via engine controls, increase safety blind spot warnings, lane departure warnings, moving object detection, air bag inflation protect occupants crash. Cell phones : would dreamed advances computer systems would lead half planet mobile phones, allowing person-to-person communication almost anyone anywhere world? Human genome project : cost computer equipment map analyze human DNA sequences hundreds millions dollars. It’s unlikely anyone would considered project computer costs 10 100 times higher, would 15 25 years earlier. Moreover, costs continue drop; soon able acquire genome, allowing medical care tailored you. World Wide Web : existence time first edition book, web transformed society. many, web replaced libraries newspapers. Search engines : content web grew size value, finding relevant information became increasingly important. Today, many people rely search engines large part lives would hardship go without them. Clearly, advances technology affect almost every aspect society. Hardware advances allowed programmers create wonderfully useful software, explai ns computers omnipresent. Today’s science fiction sugges ts tomorrow’s killer applications: already way glasses augment reality, cashless society, cars drive themselves. Traditional Classes Computing Applications Characteristics Although common set hardware technologies (see Sections 1.4 1.5) used computers ranging smart home appliances cell phones largest supercomputers, different applications distinct design requirements employ co hardware technologies different ways. Broadly speaking, computers used three dissimilar classes applications. 42Personal computers (PCs) possibly best-known form computing, readers book likely used extensive ly. Personal computers emphasize delivery good performance single users low cost usually execute third-party software . class computing drove evolution many computing technologies, merely 35 years old! personal computer (PC) computer designed use individual, usually incorporating graphics display, keyboard, mouse. Servers modern form much larger computers, usually accessed via network. Servers oriented carrying sizable workloads, may consist either single complex applications—usually scientific engineerin g application—or handling many small jobs, would occur building large web server. applications usually based software another source (such database simulation system), often modified customized particular function. Servers built basic technology desktop computers, provide greater computing, storage, input/output capacity. general, servers also place higher emphasis dependability, since crash usually costly would single-user PC. server computer used running larger programs multiple users , often simultaneously, typically accessed via network. Servers span widest range cost capability. low end, server may little desktop computer without screen keyboard cost thousand dollars. low-end servers typically used file storage, small business applications, simple web serving. extreme supercomputers , present consist tens thousands processors many terabytes memory, cost tens hundreds millions dollars. Supercomputers usually use high-end scientific engineering calculations, 43weather forecasting, oil exploration, protein structure determination, large-scale problems. Although supercomputers represent peak computing capability, represent relatively small fraction servers thus proportionally tiny fraction overall computer market erms total revenue. supercomputer class computers highest performance cost; configured servers typically cost tens hundreds millions dollars. terabyte (TB) Originally 1,099,511,627,776 (240) bytes, although communications secondary storage systems developers started using term mean 1,000,000,000,000 (1012) bytes. reduce confusion, use term tebibyte (TiB) 240 bytes, defining terabyte (TB) mean 1012 bytes. Figure 1.1 shows full range decimal binary values names. FIGURE 1.1 2X vs. 10Y bytes ambiguity resolved adding binary notation common size terms. last column note much larger binary term corresponding decimal term, compounded head chart. prefixes work bits well bytes, gigabit (Gb) 109 bits gibibits (Gib) 230 bits. 44Embedded computers largest class computers span widest range applications performance. Embedded computers include microprocessors found car, computers television set, networks processors th control modern airplane cargo ship. Embedded computing systems designed run one application one set related applications normally integrated hardware delivered single system; thus, despite large number embedded computers, users never really see using computer! embedded computer computer inside another device used running one predetermined application collection software. Embedded applications often unique application requirements combine minimum performance stringent limitations cost power. example, consider music player: processor need fast necessary handle limited function, beyond that, minimizing cost power important objective. Despite low cost, embedded computers often lower tolerance failure, since result vary upsetting (when new television crashes) devastating (such might occur computer plane cargo ship crashes). consumer-oriented embedded application s, digital home appliance, dependability achieved primarily simplicity—the emphasis one function perfectly possible. large embedded systems, techniques redundancy server world often employed. Although book focuses general-purpose computers, concepts apply directly, slight modifications, embedded computers. Elaboration Elaborations short sections used throughout text pro vide detail particular subject may interest. Disinterested readers may skip elaboration, since subsequent material never depend contents 45elaboration. Many embedded processors designed using processor cores , version processor written hardware description language, Verilog VHDL (see Chapter 4 ). core allows designer integrate application-specific hardware processor core fabrication single chip. Welcome Post-PC Era continuing march technology brings generational changes computer hardware shake entire information technology industry. Since last edition book, undergone change, significant past switch starting 30 years ago personal computers. Replacing PC personal mobile device (PMD) . PMDs battery operated wireless connectivity Internet typically cost hundre ds dollars, and, like PCs, users download software (“apps”) run them. Unlike PCs, longer keyboard mouse, likely rely touch-sensitive screen even speech input. Today’s PMD smart phone tablet computer, tomorrow may include electronic glasses. Figure 1.2 shows rapid growth time tablets smart phones versus PCs traditional cell phones. Personal mobile devices (PMDs) small wireless devices connect Internet; rely n batteries power, software installed downloading apps. Conventional examples smart phones tablets. 46FIGURE 1.2 number manufactured per year tablets smart phones, reflect post- PC era, versus personal computers traditional cell phones. Smart phones represent recent growth cell phone industry, passed PCs 2011. Tablets fastest growing category, nearly doubling 2011 2012. Recent PCs traditional cell phone categories relatively flat declining. Taking conventional server Cloud Computing , relies upon giant datacenters known Warehouse Scale Computers (WSCs). Companies like Amazon Google build WSCs containing 100,000 servers let companies rent portions provide software services PMDs without build WSCs own. Indeed, Software Service (SaaS) deployed via Cloud revolutionizing software industry PMDs WSCs revolutionizing hardware industry. Today’s software developers often portion application runs PMD portion runs Cloud. Cloud Computing refers large collections servers provide services ov er Internet; providers rent dynamically varying numbers 47servers utility. Software Service (SaaS) delivers software data service Internet, usually via thin program browser runs local client devices, instead binary code must installed, runs wholly device. Examples include web search social networking. Learn Book Successful programmers always concerned performance programs, getting results use r quickly critical creating popular software. 1960s 1970s, primary constraint computer performance size computer’s memory. Thus, programmers often followed simple credo: minimize memory space make programs fast. last decade, advances computer design memory technology greatly reduced importance small memory size applications embedded computing systems. Programmers interested performance need understand issues replaced simple memory model 1960s: parallel nature processors hierarchical nature memories. demonstrate importance understanding Chapters 3 6 showing improve performance C program factor 200. Moreover, explain Section 1.7 , today’s programmers need worry energy efficiency thei r programs running either PMD Cloud, also requires understanding code. Programmers seek build competitive versions software theref ore need increase knowledge computer organization. honored opportunity explain what’s inside revolutionary machine, unraveling software program hardware covers computer. time complete book, believe able answer following questions: programs written high-level language, C Java, translated language hardware, 48the hardware execute resulting program? Comprehending concepts forms basis understanding aspects hardware software affect program performance. interface software hardware, software instruct hardware perform needed functions? concepts vital understanding write many kinds software. determines performance program, programmer improve performance? see, depends original program, software translation program computer’s language, effectiveness hardware executing program. techniques used hardware designers improve performance? book introduce basic concepts modern computer design. interested reader find much material topic advanced book, Computer Architecture: Quantitative Approach . techniques used hardware designers improve energy efficiency? programmer help hinder energy efficiency? reasons consequences recent switch sequential processing parallel processing? book gives motivation, describes current hardware mechanisms support parallelism, surveys new generation “multicore” microprocessors (see Chapter 6 ). multicore microprocessor microprocessor containing multiple processors (“cores”) single integrated circuit. Since first commercial computer 1951, great ideas computer architects come lay foundation modern computing? Without understanding answers questions, improving performance program modern computer evaluating features might make one computer better another particular application complex process trial error, rather scientific procedure driv en insight analysis. 49This first chapter lays foundation rest book. introduces basic ideas definitions, places major components software hardware perspective, shows evaluate performance energy, introduces integrated circuit (the technology fuels computer revolution), explains shift multicores. chapter later ones, likely see many new words, words may heard sure mean. Don’t panic! Yes, lot special terminology used describing modern computers, terminology actu ally helps, since enables us describe precisely function capability. addition, computer designers (including authors) love using acronyms , easy understand know letters stand for! help remember locate terms, included highlighted definition every term margins first time appears text. shor time working terminology, fluent, r friends impressed correctly use acronyms BIOS, CPU, DIMM, DRAM, PCIe, SATA, many others. acronym word constructed taking initial letters string wo rds. example: RAM acronym Random Access Memory, CPU acronym Central Processing Unit. reinforce software hardware systems used run program affect performance, use special section, Understanding Program Performance , throughout book summarize important insights program performance. first one appears below. Understanding Program Performance performance program depends combination effectiveness algorithms used program, software systems used create translate program machine instructions, effectiveness computer executi ng instructions, may include input/output (I/O) operations. table summarizes hardware software affect 50performance. Hardware software componentHow component affects performanceWhere topic covered? Algorithm Determines number source-level statements number I/O operations executedOther books! Programming language, compiler, architectureDetermines number computer instructions source-level statementChapters 2 3 Processor memory systemDetermines fast instructions executed Chapters 4 , 5, 6 I/O system (hardware operating system)Determines fast I/O operations may executedChapters 4 , 5, 6 demonstrate impact ideas book, mentioned above, improve performance C program multiplies matrix times vector sequence chapters. step leverages understanding underlying hardware really works modern microprocessor improve performance factor 200! category data-level parallelism , Chapter 3 use subword parallelism via C intrinsics increase performance factor 3.8. category instruction-level parallelism , Chapter 4 use loop unrolling exploit multiple instruction issue out-of-o rder execution hardware increase performance another factor 2.3. category memory hierarchy optimization , Chapter 5 use cache blocking increase performance large matrices another factor 2.0 2.5. category thread-level parallelism , Chapter 6 use parallel loops OpenMP exploit multicore hardware increase performance another factor 4 14. Check Check sections designed help readers assess whether comprehend major concepts introduced chapter understand implications concepts. Check questions simple answers; others discussion among group. Answers specific questions found end chapter. Check questions appear end section, making easy skip sure understand 51the material. 1. number embedded processors sold every year greatly outnumbers number PC even post-PC processors. confirm deny insight based experience? Try count number embedded processors home. compare number conventional computers home? 2. mentioned earlier, software hardware affect performance program. think examples following right place look performance bottleneck? algorithm chosen programming language compiler operating system processor I/O system devices 1.2 Eight Great Ideas Computer Architecture introduce eight great ideas computer architects invented last 60 years computer design. ideas powerful lasted long first computer used them, newer architects demonstrating admiration imitating predecessors. great ideas themes weave subsequent chapters examples arise. point influence, section introduce icon highlighted terms represent great ideas use identify nearly 100 sections book feature use great ideas. Design Moore’s Law one constant computer designers rapid change, driven largely Moore’s Law . states integrated circuit resources double every 18–24 months. Moore’s Law resulted 1965 prediction growth IC capacity made Gordon Moore, one founders Intel. computer designs take 52years, resources available per chip easily double quadruple start finish project. Like skeet shooter, computer architects must anticipate technolo gy design finishes rather design starts. use “up right” Moore’s Law graph represent designing rapid change. Use Abstraction Simplify Design computer architects programmers invent techniques make productive, otherwise design time would lengthen dramatically resources grew Moore’s Law. major productivity technique hardware software use abstractions characterize design different levels representation; lower-level details hi dden offer simpler model higher levels. We’ll use abstract painting icon represent second great idea. 53Make Common Case Fast Making common case fast tend enhance performance better optimizing rare case. Ironically, common case often simpler rare case hence usually easier enhance. common sense advice implies know common case is, possible careful experimentation measurement (see Section 1.6 ). use sports car icon making common case fast, common trip one two passengers, it’s surely easier make fast sports car fast minivan! 54Performance via Parallelism Since dawn computing, computer architects offered designs get performance computing operations parallel. We’ll see many examples parallelism book. use multiple jet engines plane icon parallel performance . Performance via Pipelining particular pattern parallelism prevalent computer 55architecture merits name: pipelining . example, fire engines, “bucket brigade” would respond fire, many cowboy movies show response dastardly act villain. townsfolk form human chain carry water source fire, could much quickly move buckets chain instead individuals running back forth. pipeline icon sequence pipes, section represent ing one stage pipeline. Performance via Prediction Following saying better ask forgiveness th ask permission, next great idea prediction . cases, faster average guess start working rather wait know sure, assuming mechanism recover misprediction expensive prediction relatively accurate. use fortune-teller’s c rystal ball prediction icon. 56Hierarchy Memories Programmers want memory fast, large, cheap, memory speed often shapes performance, capacity limits size problems solved, cost memory today often majority computer cost. Architects found address conflicting demands hierarchy memories , fastest, smallest, expensive memory per bit top hierarchy slowest, largest, cheapest per bit bottom. shall see Chapter 5 , caches give programmer illusion main memory almost fast top hierarchy nearly big cheap bottom hierarchy. use layered triangle icon represent memory hierarchy. shape indicates speed, cost, size: closer top, faster expensive per bit memor y; wider base layer, bigger memory. 57Dependability via Redundancy Computers need fast; need dependable. Since physical device fail, make systems dependable including redundant components take failure occurs help detect failures. use tractor-trailer icon, since dual tires side rear axles allow truck continue driving even one tire fails. (Presumably , truck driver heads immediately repair facility flat tire fixed, thereby restoring redundancy!) 1.3 Program typical application, word processor large database 58system, may consist millions lines code rely sophisticated software libraries implement complex funct ions support application. see, hardware computer execute extremely simple low-level instru ctions. go complex application primitive instructions involves several layers software interpret translate hi gh- level operations simple computer instructions, example f great idea abstraction . Paris simply stared spoke French; never succeed making idiots understand language. Mark Twain, Innocents Abroad, 1869 Figure 1.3 shows layers software organized primarily hierarchical fashion, applications outermost ring variety systems software sitting hardware application software. 59 systems software Software provides services commonly useful, including operating systems, compilers, loaders, assemblers. FIGURE 1.3 simplified view hardware software hierarchical layers, shown concentric circles hardware center application software outermost. complex applications, often multiple layers application software well. example, database system may run top systems software hosting application, turn runs top database. many types systems software, two types systems software central every computer system today: operating system compiler. operating system interfaces 60between user’s program hardware provides variety services supervisory functions. Among important functions are: Handling basic input output operations Allocating storage memory Providing protected sharing computer among multipl e applications using simultaneously operating system Supervising program manages resources computer benefit programs run computer. Examples operating systems use today Linux, iOS, Windows. Compilers perform another vital function: translation program written high-level language, C, C++, Java, Visual Basic instructions hardware execute. Given sophistication modern programming languages simplicity instructions executed hardware, translation high-level language program hardware instructions complex. give brief overview process go depth Chapter 2 . compiler program translates high-level language statements assembly language statements. High-Level Language Language Hardware speak directly electronic hardware, need send electri cal signals. easiest signals computers understand off, computer alphabet two letters. 26 letters English alphabet limit much written, two letters computer alphabet limit computers do. two symbols two letters numbers 0 1, commonly think computer language numbers base 2, binary numbers . refer 61“letter” binary digit bit. Computers slaves commands, called instructions . Instructions, collections bits computer understands obeys, thought numbers. example, bits 1001010100101110 tell one computer add two numbers. Chapter 2 explains use numbers instructions data; don’t want steal chapter’s thunder, using numbers instructions data foundation computing. binary digit Also called bit. One two numbers base 2 (0 1) components information. instruction command computer hardware understands obeys. first programmers communicated computers binary numbers, tedious quickly invented new notations closer way humans think. first, notations translated binary hand, process still tiresome. Using computer help program compute r, pioneers invented software translate symbolic notat ion binary. first programs named assembler . program translates symbolic version instruction binary version. example, programmer would write add A, B assembler would translate notation 1001010100101110 assembler program translates symbolic version instructions binary version. instruction tells computer add two numbers B. name coined symbolic language, still used today, assembly language . contrast, binary language machine understands machine language . 62 assembly language symbolic representation machine instructions. machine language binary representation machine instructions. Although tremendous improvement, assembly language still far notations scientist might like use simulate fl uid flow accountant might use balance books. Assembly language requires programmer write one line every instruction computer follow, forcing programmer think like computer. recognition program could written translate powerful language computer instructions one th e great breakthroughs early days computing. Programmers today owe productivity—and sanity—to creation high-level programming languages compilers translate programs languages instructions. Figure 1.4 shows relationships among programs languages, examples power abstraction . 63 high-level programming language portable language C, C++, Java, Visual Basic composed words algebraic notation translated compiler assembly language. 64FIGURE 1.4 C program compiled assembly 65language assembled binary machine language. Although translation high-level language binary machine language shown two steps, compilers cut middleman produce binary machine language directly. languages program examined detail Chapter 2 . compiler enables programmer write high-level language expression: + B compiler would compile assembly language statement: add A, B shown above, assembler would translate statement binary instructions tell computer add two numbers B. High-level programming languages offer several important benefits. First, allow programmer think natural language, using English words algebraic notation, resulting programs look much like text like tables cryptic symbols (see Figure 1.4 ). Moreover, allow languages designed according intended use. Hence, Fortran designed scientific computation, Cobol business data processing, Lisp symbol manipulation, on. also domain-specific languages even narrower groups users, suc h interested simulation fluids, example. second advantage programming languages improved programmer productivity. One areas widespread agreement software development takes less time develop programs written languages require fewer lines express idea. Conciseness clear advantage high-level languages assembly language. final advantage programming languages allow programs independent computer developed, since compilers assemblers translate high-leve l language programs binary instructions computer. three advantages strong today little programming done assembly language. 661.4 Covers looked program uncover underlying software, let’s open covers computer learn underlying hardware. underlying hardware computer performs basic functions: inputting data, outputting data, processing data, storing data. functions performed primary topic book, subsequent chapters deal different parts four task s. come important point book, point significant hope remember forever, emphasize identifying Big Picture item. dozen Big Pictures book, first five component computer perform tasks inputting, outputting, processing, storing data. Two key components computers input devices , microphone, output devices , speaker. names suggest, input feeds computer, output result computation sent user. devices, wireless networks, provide input output computer. input device mechanism computer fed information, keyboard. output device mechanism conveys result computation user, display, another computer. Chapters 5 6 describe input/output (I/O) devices detail, let’s take introductory tour computer hardware, starting external I/O devices. BIG Picture five classic components computer input, output, memory, datapath, control, last two sometimes combined called processor. Figure 1.5 shows standard 67organization computer. organization independent hardware technology: place every piece every computer, past present, one five categories. help keep perspective, five components computer shown front page following chapters, portion interest chapter highlighte d. 68FIGURE 1.5 organization computer, showing five classic components. processor gets instructions data memory. Input writes data memory, output reads data memory. Control sends signals determine operations datapath, memor y, input, output. Looking Glass fascinating I/O device probably graphics display. personal mobile devices use liquid crystal displays (LCDs) get thin, low-power display. LCD source light; instead, controls transmission light. typical LCD incl udes rod-shaped molecules liquid form twisting helix bends light entering display, either light source beh ind display less often reflected light. rods straight en current applied longer bend light. Since 69liquid crystal material two screens polarized 90 degrees, light cannot pass unless bent. Today, LCDs use active matrix tiny transistor switch pixel control current precisely make sharper images. red- green-blue mask associated dot display determines intensity three-color components final image; color active matrix LCD, three transistor switches point. liquid crystal display (LCD) display technology using thin layer liquid polymers used transmit block light according whether charge applied. active matrix display liquid crystal display using transistor control transmission light individual pixel. image composed matrix picture elements, pixels , represented matrix bits, called bit map . Depending size screen resolution, display matrix typical tablet ranges size 1024×768 2048×1536. color display might use 8 bits three colors (red, blue, green), 24 bits per pixel, permitting millions different colors displayed. pixel smallest individual picture element. Screens composed f hundreds thousands millions pixels, organized matrix. computer displays landed airplane deck moving carrier, observed nuclear particle hit potential well, flow n rocket nearly speed light watched computer reveal innermost workings. Ivan Sutherland, “father” computer graphics, Scie ntific American, 1984 70The computer hardware support graphics consists mainly raster refresh buffer , frame buffer , store bit map. image represented onscreen stored frame buffer, bit pattern per pixel read graphics display refresh rate. Figure 1.6 shows frame buffer simplified design 4 bits per pixel. FIGURE 1.6 coordinate frame buffer left determines shade corresponding coordinate raster scan CRT display right. Pixel (X0, Y0) contains bit pattern 0011, lighter shade screen bit pattern 1101 pixel (X1, Y1). goal bit map represent faithfully screen. challenges graphics systems arise human eye good detecting even subtle changes screen. Touchscreen PCs also use LCDs, tablets smartphones post- PC era replaced keyboard mouse touch-sensitive displays, wonderful user interface advantage users pointing directly interested rather indirectly mouse. variety ways implement touch screen, 71many tablets today use capacitive sensing. Since people electrical conductors, insulator like glass covered transparent conductor, touching distorts electrostatic fi eld screen, results change capacitance. technology allow multiple touches simultaneously, recognizes gestures lead attractive user interfaces. Opening Box Figure 1.7 shows contents Apple iPad 2 tablet computer. Unsurprisingly, five classic components compute r, I/O dominates reading device. list I/O devices includes capacitive multitouch LCD, front-facing camera, rear-facing camera, microphone, headphone jack, speakers, accelerometer, gyroscope, Wi-Fi network, Bluetooth network. datapath, control, memory tiny portion components. FIGURE 1.7 Components Apple iPad 2 72A1395. metal back iPad (with reversed Apple logo middle) center. top th e capacitive multitouch screen LCD. far right 3.8 V, 25 watt-hour, polymer battery, consists three Li-ion cell cases offers 10 hours battery life. far left metal frame hat attaches LCD back iPad. small components surrounding metal back center think computer; often L- shaped fit compactly inside case next battery. Figure 1.8 shows close-up L-shaped board lower left metal case, logic printed circuit board contains processor memory. tiny rectangle logic board contains chip provides wireless communication: Wi-Fi, Bluetooth, FM tuner. fit small slot lower left corner logi c board. Near upper left corner case anothe r L-shaped component, front-facing camera assembly includes camera, headphone jack, microphone. Near right upper corner case board containing volume control silent/screen rotation lock button along gyroscope accelerometer. last two chips combine allow iPad recognize six-axis motion. tiny rectangle next rear-facing camera. Near bottom right case L-shaped speaker assembly. cable bottom connector logic board camera/volume control board. board cable speaker assembly controller capacitive touchscreen. (Courtesy iFixit, www.ifixit.com ) small rectangles Figure 1.8 contain devices drive advancing technology, called integrated circuits nicknamed chips . A5 package seen middle Figure 1.8 contains two ARM processors operate clock rate 1 GHz. processor active part computer, following instructions program letter. adds numbers, tests numbers, signals I/O devices activate, on. Occasionally, people call processor CPU , bureaucratic- sounding central processor unit . 73 integrated circuit Also called chip . device combining dozens millions transistors. central processor unit (CPU) Also called processor. active part computer, contains datapath control adds numbers, tests numbers, signals I/O devices activate, on. FIGURE 1.8 logic board Apple iPad 2 Figure 1.7. photo highlights five integrated circuits. large integrated circuit middle Apple A5 chip, contains dual ARM processor cores run 1 GHz well 512 MB main memory inside package. Figure 1.9 shows photograph processor chip inside A5 package. similar- sized chip left 32 GB flash memory chip non-volatile storage. empty space two chips second flash chip installed double storage capacity iPad. chips right A5 include power controller nd I/O controller chips. (Courtesy iFixit, www.ifixit.com ) Descending even lower hardware, Figure 1.9 reveals details microprocessor. processor logically comprises two main components: datapath control, respective brawn brain processor. datapath performs arithmetic operations, control tells datapath, memory, I/O devices according wishes instructions program. Chapter 4 explains datapath control higher- performance design. 74 datapath component processor performs arithmetic operations. control component processor commands datapath, memory, I/O devices according instructions program. 75FIGURE 1.9 processor integrated circuit inside A5 package. size chip 12.1 10.1 mm, manufactured originally 45-nm process (see Section 1.5 ). two identical ARM processors cores middle left chip PowerVR graphics processing unit (GPU) four datapaths upper left quadrant. left bottom side ARM cores interfaces main memory (DRAM). (Courtesy Chipworks, www.chipworks.com ) A5 package Figure 1.8 also includes two memory chips, 76each 2 gibibits capacity, thereby supplying 512 MiB. memory programs kept running; also contains data needed running programs. memory built DRAM chips. DRAM stands dynamic random access memory . Multiple DRAMs used together contain instructions data program. contrast sequential access memories, magnetic tapes, RAM portion term DRAM means memory accesses take basically amount time matter portion memory read. memory storage area programs kept running contains data needed running programs. dynamic random access memory (DRAM) Memory built integrated circuit; provides random access location. Access times 50 nanoseconds cost per gigabyte 2012 $5 $10. Descending depths component hardware reveals insights computer. Inside processor ano ther type memory—cache memory. Cache memory consists small, fast memory acts buffer DRAM memory. (The nontechnical definition cache safe place hiding things.) Cache built using different memory technology, static random access memory (SRAM) . SRAM faster less dense, hence expensive, DRAM (see Chapter 5 ). SRAM DRAM two layers memory hierarchy . 77 cache memory small, fast memory acts buffer slower, larger memory. static random access memory (SRAM) Also memory built integrated circuit, faster less dense DRAM. mentioned above, one great ideas improve design abstraction. One important abstractions interface hardware lowest-level software. importance, given special name: instruction set architecture , simply architecture , computer. instruction set architecture includes anything programmers ne ed know make binary machine language program work correctly, including instructions, I/O devices, on. Typically, operating system encapsulate details I/O, allocating memory, low-level system functions application programmers need worry details. combination basic instruction set operating system interface provided application programmers called application binary interface (ABI) . 78 instruction set architecture Also called architecture . abstract interface hardware lowest-level software encompasses information necessary write machine language program run correctly, including instructions, registers, memor access, I/O, on. application binary interface (ABI) user portion instruction set plus operating syst em interfaces used application programmers. defines standard binary portability across computers. instruction set architecture allows computer designers alk functions independently hardware performs them. example, talk functions digital clock (keeping time, displaying time, setting alarm) separately clock hardware (quartz crystal, LED displays, plastic buttons). Computer designers distinguish architecture implementation architecture along lines: 79implementation hardware obeys architecture abstractio n. ideas bring us another Big Picture. BIG Picture hardware software consist hierarchical layers using abstraction, lower layer hiding details level above. One key interface levels abstraction instruction set architecture —the interface hardware low-level software. abstract interface enables many implementations varying cost performance run identical software. Safe Place Data Thus far, seen input data, compute using data, display data. lose power computer, however, everything would lost memory inside computer volatile —that is, loses power, forgets. contrast, DVD disk doesn’t forget movie turn power DVD player, therefore nonvolatile memory technology. implementation Hardware obeys architecture abstraction. volatile memory Storage, DRAM, retains data receiving power. nonvolatile memory form memory retains data even absence power source used store programs runs. DVD disk nonvolatile. distinguish volatile memory used hold data programs running nonvolatile memory used store data programs runs, term main 80memory primary memory used former, secondary memory latter. Secondary memory forms next lower layer memory hierarchy . DRAMs dominated main memory since 1975, magnetic disks dominated secondary memory starting even earlier. size form factor, personal mobile devices use flash memory , nonvolatile semiconductor memory, instead disks. Figure 1.8 shows chip containing flash memory iPad 2. slower DRAM, much cheaper DRAM addition nonvolatile. Although costing per bit disks, smaller, comes much smaller capacities, rugged, power efficient disks. Hence, flash memory standard secondary memory PMDs. Alas, unlike disks DRAM, flash memory bits wear 100,000 1,000,000 writes. Thus, file systems must keep track number write strategy avoid wearing storage, moving popular data. Chapter 5 describes disks flash memory detail. main memory Also called primary memory . Memory used hold programs running; typically consists DRAM today’s 81computers. secondary memory Nonvolatile memory used store programs data runs; typically consists flash memory PMDs magnetic disks servers. magnetic disk Also called hard disk . form nonvolatile secondary memory composed rotating platters coated magnetic recording material. rotating mechanical devices, access times 5 20 milliseconds cost per gigabyte 2012 $0.05 $0.10. flash memory nonvolatile semi-conductor memory. cheaper slower DRAM expensive per bit faster magnetic disks. Access times 5 50 microseconds cost per gigabyte 2012 $0.75 $1.00. Communicating Computers We’ve explained input, compute, display, save data, still one missing item found today’s computer s: computer networks. processor shown Figure 1.5 connected memory I/O devices, networks interconnect whole computers, allowing computer users extend power f computing including communication. Networks become popular backbone current computer systems; new personal mobile device server without network interface would ridiculed. Networked computers several major advantages: Communication : Information exchanged computers high speeds. Resource sharing : Rather computer I/O devices, computers network share I/O devices. Nonlocal access : connecting computers long distances, 82users need near computer using. Networks vary length performance, cost communication increasing according speed communication distance information travels. Perhaps popular type network Ethernet . kilometer long transfer 40 gigabits per second. length speed make Ethernet useful connect computers floor building; hence, example generically called local area network . Local area networks interconnected switches also provide routing serv ices security. Wide area networks cross continents backbone Internet, supports web. typically based optical fibers leased telecommunication companies. local area network (LAN) network designed carry data within geographically confined area, typically within single building. wide area network (WAN) network extended hundreds kilometers span continent. Networks changed face computing last 30 years, becoming much ubiquitous making dramatic increases performance. 1970s, individuals access electronic mail, Internet web exist, physically mailing magnetic tapes primary way transfer large amounts data two locations. Local area networks almost nonexistent, existing wide area networks limited capacity restricted access. networking technology improved, became considerably cheaper significantly higher capacity. example, first standardized local area network technology, developed 30 years ago, version Ethernet maximum capacity (also called bandwidth) 10 million bits per second, typically shared tens of, hundred, computers. Today, local area network technology offers capacity 1 40 83gigabits per second, usually shared computers. Optical communications technology allowed similar growth capacity wide area networks, hundreds kilobits gigabits hundreds computers connected worldwide network millions computers connected. dramatic rise deployment networking combined increases capacity made network technology central information revolution last 30 years. last decade another innovation networking reshaping way computers communicate. Wireless technology widespread, enabled post-PC era. ability make radio low-cost semiconductor technology (CMOS) use memory microprocessors enabled significant improvement price, leading explosion deployment. Currently available wireless technologies, called IEEE standard name 802.11, allow transmission rates 1 nearly 100 million bits per second. Wireless technology quite bit different wire-based networks, since users immediat e area share airwaves. Check Semiconductor DRAM memory, flash memory, disk storage differ significantly. technology, list volatility, approximate relative access time, approximate relative cost compared DRAM. 1.5 Technologies Building Processors Memory Processors memory improved incredible rate, computer designers long embraced latest electronic technology try win race design better computer. Figure 1.10 shows technologies used time, estimate relative performance per unit co st technology. Since technology shapes computers able quickly evolve, believe computer professionals familiar basics 84integrated circuits. FIGURE 1.10 Relative performance per unit cost technologies used computers time. Source: Computer Museum, Boston, 2013 extrapolated authors. See Section 1.12 . transistor simply on/off switch controlled electricity. integrated circuit (IC) combined dozens hundreds transistors single chip. Gordon Moore predicted continuous doubling resources, forecasting growt h rate number transistors per chip. describe tremendous increase number transistors hundreds millions, adjective large scale added term, creating abbreviation VLSI , large-scale integrated circuit . transistor on/off switch controlled electric signal. large-scale integrated (VLSI) circuit device containing hundreds thousands millions transistors. rate increasing integration remarkably stable. Figure 1.11 shows growth DRAM capacity since 1977. 35 years, industry consistently quadrupled capacity every 3 years, resulting increase excess 16,000 times! 85FIGURE 1.11 Growth capacity per DRAM chip time. y-axis measured kibibits (210 bits). DRAM industry quadrupled capacity almost every three years, 60% increase per year, 20 years. recent years, rate slowed somewhat closer doubling every two three years. understand manufacture integrated circuits, start beginning. manufacture chip begins silicon , substance found sand. silicon conduct electricity well, called semiconductor . special chemical process, possible add materials silicon ow tiny areas transform one three devices: Excellent conductors electricity (using either microsc opic copper aluminum wire) silicon natural element semiconductor. semiconductor substance conduct electricity well. Excellent insulators electricity (like plastic sheathing glass) Areas conduct insulate specific conditions (as switch) Transistors fall last category. VLSI circuit, then, billions combinations conductors, insulators, switches manufactured single small package. manufacturing process integrated circuits critical 86cost chips hence important computer designers. Figure 1.12 shows process. process starts silicon crystal ingot , looks like giant sausage. Today, ingots 8–12 inches diameter 12–24 inches long. ingot finely sliced wafers 0.1 inches thick. wafers go series processing steps, patterns f chemicals placed wafer, creating transistors, conductors, insulators discussed earlier. Today’s integrated circuits contain one layer transistors may tw eight levels metal conductor, separated layers insulato rs. silicon crystal ingot rod composed silicon crystal 8 12 inches diameter 12 24 inches long. wafer slice silicon ingot 0.1 inches thick, used create chips. FIGURE 1.12 chip manufacturing process. sliced silicon ingot, blank wafers put 20 40 steps create patterned wafers (see Figure 1.13 ). patterned wafers tested wafer tester, map goo parts made. Next, wafers diced dies (see 87Figure 1.9 ). figure, one wafer produced 20 dies, 17 passed testing. (X means die bad.) yield good dies case 17/20, 85%. good dies bonded packages tested one time shipping packaged parts customers. One bad packaged part found final test. single microscopic flaw wafer one dozens patterning steps result area wafer failing. defects , called, make virtually impossible manufacture perfect wafer. simplest way cope imperfection place many independent component single wafer. patterned wafer chopped up, diced , components, called dies informally known chips . Figure 1.13 shows photograph wafer containing microprocessors diced; earlier, Figure 1.9 shows individual microprocessor die. defect microscopic flaw wafer patterning steps result failure die containing defect. die individual rectangular sections cut wafer, informally known chips . 88FIGURE 1.13 12-inch (300 mm) wafer Intel Core i7 (Courtesy Intel). number dies 300 mm (12 inch) wafer 100% yield 280, 20.7 10.5 mm. several dozen partially rounded chips boundaries wafer useless; included it’s easier create masks used pattern silicon. die uses 32-nanometer technology, means smallest features approximately 32 nm size, although typically somewhat smaller actual feature size, refers size transistors “drawn” versus final manufactured size. Dicing enables discard dies unlucky enough contain flaws, rather whole wafer. concept quantified yield process, defined 89the percentage good dies total number dies wafer. yield percentage good dies total number dies wafer. cost integrated circuit rises quickly die size increases, due lower yield fewer dies fi wafer. reduce cost, using next generation process shrinks large die uses smaller sizes transistors wires. improves yield die count per wafer. 32- nanometer (nm) process typical 2012, means essentially smallest feature size die 32 nm. you’ve found good dies, connected input/output pins package, using process called bonding . packaged parts tested final time, since mistakes occur packaging, shipped customers. Elaboration cost integrated circuit expressed three simple equations: first equation straightforward derive. second approximation, since subtract area near border round wafer cannot accommodate rectangular dies (see Figure 1.13 ). final equation based empirical observations yields integrated circuit factories, exponent related number critical processing steps. Hence, depending defect rate size die wafer, costs generally linear die area. 90 Check key factor determining cost integrated circuit volume. following reasons chip made high volume cost less? 1. high volumes, manufacturing process tuned particular design, increasing yield. 2. less work design high-volume part low-volume part. 3. masks used make chip expensive, cost per chip lower higher volumes. 4. Engineering development costs high largely independent volume; thus, development cost per die lower high- volume parts. 5. High-volume parts usually smaller die sizes low- volume parts therefore, higher yield per wafer. 1.6 Performance Assessing performance computers quite challengin g. scale intricacy modern software systems, together wide range performance improvement techniques employed hardware designers, made performance assessment much difficult. trying choose among different computers, performance important attribute. Accurately measuring comparing different computers critical purchasers therefore, designers. people selling computers know well. Ofte n, salespeople would like see computer best possible light, whether light accurately reflects th e needs purchaser’s application. Hence, understanding best measure performance limitations measurements important selecting computer. rest section describes different ways performance determined; then, describe metrics measuring performance viewpoint computer user designer. also look metrics related present classical processor performance equation, use throughout text. 91Defining Performance say one computer better performance another, mean? Although question might seem simple, analogy passenger airplanes shows subtle question performance be. Figure 1.14 lists typical passenger airplanes, together cruising speed, range, capacity. wanted know planes table best performance, would first need define performance. example, considering different measures performance, see plane highest cruising speed Concorde (retired service 2003), plane longest range DC-8, plane largest capacity 747. FIGURE 1.14 capacity, range, speed number commercial airplanes. last column shows rate airplane transports passengers, capacity times cruising speed (ignoring range takeoff landing times). Let’s suppose define performance terms speed. stil l leaves two possible definitions. could define fastest pl ane one highest cruising speed, taking single passenger one point another least time. interested transporting 450 passengers one point another, however, 747 would clearly fastest, last column figure shows. Similarly, define computer performance n several distinct ways. running program two different desktop computers, you’d say faster one desktop computer gets job done first. running datacenter several servers running jobs submitted many users, you’d say faster computer one completed 92jobs day. individual computer user, interested reducing response time —the time start completion task—also referred execution time . Datacenter managers often care increasing throughput bandwidth —the total amount work done given time. Hence, cases, need different performance metrics well different sets applications benchmark personal mobile devi ces, focused response time, versus servers, ar e focused throughput. response time Also called execution time . total time required computer complete task, including disk accesses, memory accesses, I/O activities, operating system overhead, CPU executio n time, on. throughput Also called bandwidth . Another measure performance, number tasks completed per unit time. Throughput Response Time Example following changes computer system increase throughput, decrease response time, both? 1. Replacing processor computer faster version 2. Adding additional processors system uses multiple processors separate tasks—for example, searching web Answer Decreasing response time almost always improves throughput. Hence, case 1, response time throughput improved. case 2, one task gets work done faster, throughput increases. If, however, demand processing second case almost large throughput, system might force requests queue up. case, increasing throughput could also improve response time, since would reduce waiting time n 93the queue. Thus, many real computer systems, changing either execution time throughput often affects other. discussing performance computers, primaril concerned response time first chapters. maximize performance, want minimize response time execution time task. Thus, relate performance execution time computer X: means two computers X Y, performance X greater performance Y, is, execution time longer X, X faster Y. discussing computer design, often want relate performance two different computers quantitatively. us e phrase “X n times faster Y”—or equivalently “X n times fast Y”—to mean X n times fast Y, execution time n times long X: 94 Relative Performance Example computer runs program 10 seconds computer B runs program 15 seconds, much faster B? Answer know n times fast B Thus performance ratio therefore 1.5 times fast B. example, could also say computer B 1.5 times slower computer A, since means simplicity, normally use terminology fast try compare computers quantitatively. performance execution time reciprocals, increasing performance requires decreasing execution time. avoid potential confusion terms increasing decreasing , usually say “improve performance” “improve execution time” 95when mean “increase performance” “decrease execution time.” Measuring Performance Time measure computer performance: computer performs amount work least time fastest. Program execution time measured seconds per program. However, time defined different ways, depending count. straightforward definition time called wall clock time , response time , elapsed time . terms mean total time complete task, including disk accesses, memory accesses, input/output (I/O) activities, operating system overhead— everything. Computers often shared, however, processor may work several programs simultaneously. cases, system may try optimize throughput rather attempt minimize elapsed time one program. Hence, often want distinguish elapsed time time processor working behalf. CPU execution time simply CPU time , recognizes distinction, time CPU spends computing task include time spent waiting I/O running programs. (Remember, though, response time experienced user elapsed time program, CPU time.) CPU time divided CPU time spent program, called user CPU time , CPU time spent operating system performing tasks behalf program, called system CPU time . Differentiating system user CPU time difficult accurately, often hard assign responsibility operating sy stem activities one user program rather another functionality differences operating systems. CPU execution time Also called CPU time . actual time CPU spends computing specific task. user CPU time 96The CPU time spent program itself. system CPU time CPU time spent operating system performing tasks behalf program. consistency, maintain distinction performance based elapsed time based CPU execution time. use term system performance refer elapsed time unloaded system CPU performance refer user CPU time. focus CPU performance chapter, although discussions summarize performance applied either elapsed time CPU time measurements. Understanding Program Performance Different applications sensitive different aspects th e performance computer system. Many applications, especially running servers, depend much I/O performance, which, turn, relies hardware software. Total elapsed time measured wall clock measurement interest. application environments, user may care throughput, response time, complex combination two (e.g., maximum throughput worst-case response time). improve performance program, one must clear definition performance metric matters proceed find performance bottlenecks measuring program execution looking likely bottlenecks. following chapter s, describe search bottlenecks improve performance various parts system. Although computer users care time, examine details computer it’s convenient think performance metrics. particular, computer designers may want think computer using measure relates fast hardware perform basic functions. Almost computers constructed using clock determines events take place hardware. discrete time intervals called clock cycles (or ticks , clock ticks , clock 97periods , clocks , cycles ). Designers refer length clock period time complete clock cycle (e.g., 250 picoseconds, 250 ps) clock rate (e.g., 4 gigahertz, 4 GHz), inverse clock period. next subsection, formalize relationship clock cycles hardware designer seconds computer user. clock cycle Also called tick, clock tick , clock period , clock , cycle . time one clock period, usually processor clock, runs constant rate. clock period length clock cycle. Check 1. Suppose know application uses personal mobile devices Cloud limited network performance . following changes, state whether throughput improves, response time throughput improve, neither improves. a. extra network channel added PMD Cloud, increasing total network throughput reducing delay obtain network access (since two channels). b. networking software improved, thereby reducing network communication delay, increasing throughput. c. memory added computer. 2. Computer C’s performance four times fast performance computer B, runs given application 28 seconds. long computer C take run application? CPU Performance Factors Users designers often examine performance using different metrics. could relate different metrics, could 98determine effect design change performance experienced user. Since confining CPU performance point, bottom-line performance measure CPU execution time. simple formula relates basic metrics (clock cycles clock cycle time) CPU time: Alternatively, clock rate clock cycle time inverses, formula makes clear hardware designer improve performance reducing number clock cycles required program length clock cycle. see later chapters, designer often faces trade-off number clock cycles needed program length cycle. Many techniques decrease number clock cycles may also increase clock cycle time. Improving Performance Example favorite program runs 10 seconds computer A, 2 GHz clock. trying help computer designer build computer, B, run program 6 seconds. designer determined substantial increase clock rate possible, increase affect rest CPU des ign, causing computer B require 1.2 times many clock cycles computer program. clock rate tell designer target? Answer Let’s first find number clock cycles required p rogram A: 99CPU time B found using equation: run program 6 seconds, B must twice clock rate A. Instruction Performance performance equations include reference number instructions needed program. However, sinc e compiler clearly generated instructions execute, computer execute instructions run program, execution time must depend number instructions program. One way think execution time equals number instructions executed multiplied average time per instruction. Therefore, number clock cycles require program written term clock cycles per instruction , average number clock cycles instruction takes execute, often abbreviated CPI. Since different instructions may take different amounts time depending do, CPI average 100all instructions executed program. CPI provides one way comparing two different implementations identical instruction set architecture, since number instruction executed program will, course, same. clock cycles per instruction (CPI) Average number clock cycles per instruction program program fragment. Using Performance Equation Example Suppose two implementations instruction set architecture. Computer clock cycle time 250 ps CPI 2.0 program, computer B clock cycle time 500 ps CPI 1.2 program. computer faster program much? Answer know computer executes number instructions program; let’s call number I. First, find number processor clock cycles computer: compute CPU time computer: Likewise, B: Clearly, computer faster. amount faster given 101ratio execution times: conclude computer 1.2 times fast computer B program. Classic CPU Performance Equation write basic performance equation terms instruction count (the number instructions executed program), CPI, clock cycle time: or, since clock rate inverse clock cycle time: instruction count number instructions executed program. formulas particularly useful separate three key factors affect performance. use formulas compare two different implementations evaluate design alternative know impact three parameters. Comparing Code Segments Example compiler designer trying decide two code sequences computer. hardware designers supplied following facts: 102For particular high-level language statement, compiler writer considering two code sequences require foll owing instruction counts: code sequence executes instructions? wil l faster? CPI sequence? Answer Sequence 1 executes 2 +1 +2 =5 instructions. Sequence 2 executes 4 +1 +1 =6 instructions. Therefore, sequence 1 executes fewer instructions. use equation CPU clock cycles based instruction count CPI find total number clock cycles sequence: 103This yields code sequence 2 faster, even though executes one extra instruction. Since code sequence 2 takes fewer overall clock cyc les instructions, must lower CPI. CPI values computed BIG Picture Figure 1.15 shows basic measurements different levels computer measured case. see factors combined yield execution time measured seconds per program: 104FIGURE 1.15 basic components performance measured. Always bear mind complete reliable measure computer performance time. example, changing instruction set lower instruction count may lead organization slower clock cycle time higher CPI offsets improvement instruction count. Similarly, becaus e CPI depends type instructions executed, code executes fewest number instructions may faste st. determine value factors performance equation? measure CPU execution time running program, clock cycle time usually published part documentation computer. instruction count CPI difficult obtain. course, know clock rate CPU execution time, need one instruction count CPI determine other. measure instruction count using software tools profile execution using simulator architecture. Alternatively, use hardware counters, included processors, record variety measurements, including number instructions executed , average CPI, often, sources performance loss. Since instruction count depends architecture, ex act implementation, measure instruction count without knowing details implementation. CPI, however, depends wide variety design details computer, including memory system processor structure (as see Chapter 4 Chapter 5 ), well mix instruction types executed application. Thus, CPI varies application, well among implementations instruction set. example shows danger using one factor (instruction count) assess performance. comparing two computers, must look three components, combine form execution time. factors identical, like clock rate example, performance determined comparing nonidentical factors. Since CPI varies 105instruction mix , instruction count CPI must compared, even clock rates equal. Several exercises end chapter ask evaluate series computer compiler enhancements affect clock rate, CPI, instruction count. Section 1.10 , we’ll examine common performance measurement incorporate terms thus misleading. instruction mix measure dynamic frequency instructions across one many programs. Understanding Program Performance performance program depends algorithm, language, compiler, architecture, actual hardware. following table summarizes components affect factors CPU performance equation. Hardware software componentAffects what?How? Algorithm Instruction count, CPIThe algorithm determines number source program instructions executed hence number processor instructions executed. algorithm may also affect CPI, favoring slower faster instructions. example, algori thm uses divides, tend higher CPI. Programming languageInstruction count, CPIThe programming language certainly affects instruction coun t, since statements language translated processor instructions, determine instruction count. language may also affect CPI features; example, language heavy support data abstraction (e.g., Java) require indirect calls, use higher CPI instructions. Compiler Instruction count, CPIThe efficiency compiler affects instruction c ount average cycles per instruction, since compiler determines translation source language instructions computer instructions. compiler’s role complex affect CPI varied ways. Instruction set architectureInstruction count, clock rate, CPIThe instruction set architecture affects three aspects CP U performance, since affects instructions needed func tion, cost cycles instruction, overall clock rate f processor. Elaboration Although might expect minimum CPI 1.0, we’ll 106see Chapter 4 , processors fetch execute multiple instructions per clock cycle. reflect approach, designers invert CPI talk IPC, instructions per clock cycle . processor executes average two instructions per clock cyc le, IPC 2 hence CPI 0.5. Elaboration Although clock cycle time traditionally fixed, save energy temporarily boost performance, today’s processors vary clock rates, would need use average clock rate program. example, Intel Core i7 temporarily increase clock rate 10% chip gets warm. Intel calls Turbo mode . Check given application written Java runs 15 seconds desktop processor. new Java compiler released requires 0.6 many instructions old compiler. Unfortunately, increases CPI 1.1. fast expect application run using new compiler? Pick right answer three choice below: a. b. c. 1.7 Power Wall Figure 1.16 shows increase clock rate power eight generations Intel microprocessors 30 years. clock rat e power increased rapidly decades flattened recently. reason grew together correlated , reason recent slowing run practical power limit cooling commodity microprocessors. 107FIGURE 1.16 Clock rate power Intel x86 microprocessors eight generations 30 years. Pentium 4 made dramatic jump clock rate power less performance. Prescott thermal problems led abandonment Pentium 4 line. Core 2 line reverts simpler pipeline lower clock rates multiple processors per chip. Core i5 pipelines follow footsteps. Although power provides limit cool, post-PC era really valuable resource energy. Battery life trump performance personal mobile device, architects warehouse scale computers try reduce costs f powering cooling 100,000 servers costs high scale. measuring time seconds safer evaluation program performance rate like MIPS (see Section 1.10 ), energy metric joules better measure power rate like watts, joules/second. dominant technology integrated circuits called CMOS (complementary metal oxide semiconductor ). CMOS, primary source energy consumption so-called dynamic energy—that is, energy consumed transistors switch states 0 1 vice versa. dynamic energy depends capacitive loading transistor voltage applied: equation energy pulse logic transitio n 0 → 1 → 0 1 → 0 → 1. energy single transition 108The power required per transistor product energy transition frequency transitions: Frequency switched function clock rate. capacitive load per transistor function number transistor connected output (called fanout ) technology, determines capacitance wires transistors. regard Figure 1.16 , could clock rates grow factor 1000 power increased factor 30? Energy thus power reduced lowering voltage, occurred new generation technology, power function voltage squared. Typically, voltage reduced 15% per generation. 20 years, voltages gone 5 V 1 V, increase power 30 times. Relative Power Example Suppose developed new, simpler processor 85% capacitive load complex older processor. , assume adjust voltage reduce voltage 15% compared processor B, results 15% shrink frequency. impact dynamic power? Answer Thus power ratio 109Hence, new processor uses half power old processor. modern problem lowering voltage appears make transistors leaky, like water faucets cannot completely shut off. Even today 40% power consumption server chips due leakage. transistors starte leaking more, whole process could become unwieldy. try address power problem, designers already attached large devices increase cooling, turn parts f chip used given clock cycle. Although many expensive ways cool chips thereby raise power to, say, 300 watts, techniques generally costly personal computers even servers, mention personal mobile devices. Since computer designers slammed power wall, needed new way forward. chose different path way designed microprocessors first 30 years. Elaboration Although dynamic energy primary source energy consumption CMOS, static energy consumption occurs leakage current flows even transistor off. servers, leakage typically responsible 40% energy consumption. Thus, increasing number transistors increase power dissipation, even transistors always off. variety design techniques technology innovations deployed control leakage, it’s hard lower voltage further. Elaboration Power challenge integrated circuits two reasons. First , power must brought distributed around chip; modern microprocessors use hundreds pins power ground! Similarly, multiple levels chip interconnect us ed solely power ground distribution portions chi p. Second, power dissipated heat must removed. Server chips burn 100 watts, cooling chip surrounding system major expense warehouse scale 110computers (see Chapter 6 ). 1.8 Sea Change: Switch Uniprocessors Multiprocessors power limit forced dramatic change design microprocessors. Figure 1.17 shows improvement response time programs desktop microprocessors time. Since 2002, rate slowed factor 1.5 per year factor 1.2 per year. now, software like music written solo performer; current generation chips we’re getting little experience duets quartets small ensembles; scoring work large orchestra chorus different kind challenge. Brian Hayes, Computing Parallel Universe, 2007. 111FIGURE 1.17 Growth processor performance since mid-1980s. chart plots performance relative VAX 11/780 measured SPECint benchmarks (see Section 1.10 ). Prior mid-1980s, processor performance growth largely technology-driven averaged 25% per year. increase growth 52% since attributable advanced architectural organizational ideas. higher annual performance improvement 52% since mid-1980s meant performance factor seven larger 2002 would stayed 25%. Since 2002, limits power, available instruction-level parallelism, long memory latency slowed uniprocessor performance recently, 22% per year. Rather continuing decrease response time one program running single processor, 2006 desktop server companies shipping microprocessors multiple processors per chip, benefit often throug hput response time. reduce confusion words processor microprocessor, companies refer processors “cores,” microprocessors generically called multicor e microprocessors. Hence, “quadcore” microprocessor chip contains four processors four cores. past, programmers could rely innovations hardware, 112architecture, compilers double performance programs every 18 months without change line code. Today, programmers get significant improvement respon se time, need rewrite programs take advantage multiple processors. Moreover, get historic benefit running faster new microprocessors, programmers continu e improve performance code number cores increases. reinforce software hardware systems work together, use special section, Hardware/Software Interface , throughout book, first one appearing below. elements summarize important insights critical interface . Hardware/Software Interface Parallelism always crucial performance computing, often hidden. Chapter 4 explain pipelining , elegant technique runs programs faster overlapping execution instructions. optimization one example instruction-level parallelism , parallel nature hardware abstracted away programmer compiler think hardware executing instructions sequentially. 113Forcing programmers aware parallel hardware rewrite programs parallel “third rail” computer architecture, companies past depended change behavior failed (see Section 6.15 ). historical perspective, it’s startling whole industr bet future programmers finally successfully swit ch explicitly parallel programming. hard programmers write explicitly parallel programs? first reason parallel programming definition performance programming, increases difficulty programming. program need correct, solve important problem, provide useful interface people programs invoke it; program must also fast. Otherwise, don’t need performance, write sequential program. second reason fast parallel hardware means programmer must divide application processor roughly amount time, overhead scheduling coordination doesn’t fritter away potential performance benefits parallelism. 114As analogy, suppose task write newspaper story. Eight reporters working story could potentially wr ite story eight times faster. achieve increased speed, one woul need break task reporter something time. Thus, must schedule sub-tasks. anything went wrong one reporter took longer seven othe rs did, benefits eight writers would diminish ed. Thus, must balance load evenly get desired speedup. Another danger would reporters spend lot time talking write sections. would also fall short one part story, conclusion, couldn’t written parts completed. Thus, care must taken reduce communication synchronization overhead . analogy parallel programming, challenges include scheduling, load balancing, time synchronization, overhead communication parties. might guess, challenge stiffer reporters newspaper story processors parallel programming. reflect sea change industry, next five chapters edition book section implications parallel revolution chapter: Chapter 2 , Section 2.11 : Parallelism Instructions: Synchronization . Usually independent parallel tasks need coordinate times, say completed work. chapter explains instructions used multicore processo rs synchronize tasks. Chapter 3 , Section 3.6 : Parallelism Computer Arithmetic: Subword Parallelism . Perhaps simplest form parallelism build involves computing elements parallel, multiplying two vectors. Subword parallelism takes advantage resources supplied Moore’s Law provide wider arithmetic units operate many operands simultaneously. 115 Chapter 4 , Section 4.10 : Parallelism via Instructions . Given difficulty explicitly parallel programming, tremendous eff ort invested 1990s hardware compiler uncover implicit parallelism, initially via pipelining . chapter describes aggressive techniques, including fetching executing multiple instructions concurrently guessing outcomes decisions, executing instructions speculatively using prediction . 116 Chapter 5 , Section 5.10 : Parallelism Memory Hierarchies : Cache Coherence . One way lower cost communication processors use address space, processor read write data. Given processors today use caches keep temporary copy data faster memory near processor, it’s easy imagine parallel programming would even difficult caches associated processor inconsistent values shared data. chapter describes mechanisms keep data caches consistent. 117 Chapter 5 , Section 5.11 : Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks . section describes using many disks conjunction offer much higher throughput, original inspiration Redundant Arrays 118Inexpensive Disks (RAID). real popularity RAID proved much greater dependability offered including modest number redundant disks. section explains differences performance, cost, dependability various RAID levels. addition sections, full chapter parallel processing. Chapter 6 goes detail challenges parallel programming; presents two contrasting approaches communication shared addressing explicit message passing; describes restricted model parallelism easier prog ram; discusses difficulty benchmarking parallel processors; introduces new simple performance model multicore microprocessors; and, finally, describes evaluates four examples multicore microprocessors using model. mentioned above, Chapters 3 6 use matrix vector multiply running example show type parallelism significantly increase performance. Appendix B describes increasingly popular hardware component included desktop computers, graphics processing unit (GPU). Invented accelerate graphics, GPUs becoming programming platforms right. might expect, given times, GPUs rely parallelism . Appendix B describes NVIDIA GPU highlights parts parallel programming environment. 119I thought [computers] would universally applicable idea, like book is. didn’t think would develop fast did, didn’t envision we’d able get many parts chip finally got. transistor came along unexpectedly. happened much faster expected. J. Presper Eckert, coinventor ENIAC, speaking 1991 1.9 Real Stuff: Benchmarking Intel Core i7 chapter section entitled “Real Stuff” ties concepts book computer may use every day. sections cover technology underlying modern compu ters. first “Real Stuff” section, look integrated circu manufactured performance power measured, Intel Core i7 example. SPEC CPU Benchmark computer user runs programs day day would perfect candidate evaluate new computer. set programs run would form workload . evaluate two 120computer systems, user would simply compare execution tim e workload two computers. users, however, situation. Instead, must rely methods measure performance candidate computer, hoping methods reflect well computer perform user’s workload. alternative usually followed evaluating computer using set benchmarks —programs specifically chosen measure performance. benchmarks form workload user hopes predict performance actual workload. noted above, make common case fast , first need know accurately case common, benchmarks play critical role computer architecture. workload set programs run computer either actual collection applications run user constructed real programs approximate mix. typical workload specifies programs relative frequencies. benchmark program selected use comparing computer performance. SPEC ( System Performance Evaluation Cooperative ) effort funded supported number computer vendors create standard sets benchmarks modern computer systems. 1989, 121SPEC originally created benchmark set focusing processor performance (now called SPEC89), evolved five generations. latest SPEC CPU2006, consists set 12 integer benchmarks (CINT2006) 17 floating-point benchmarks (CFP2006). integer benchmarks vary part C compiler chess program quantum computer simulation. floating-point benchmarks include structured grid codes f finite element modeling, particle method codes molecular dynamics, sparse linear algebra codes fluid dynamics. Figure 1.18 describes SPEC integer benchmarks execution time Intel Core i7 shows factors explain execution time: instruction count, CPI, clock cycle ti me. Note CPI varies factor 5. FIGURE 1.18 SPECINTC2006 benchmarks running 2.66 GHz Intel Core i7 920. equation page 36 explains, execution time product three factors table: instructi count billions, clocks per instruction (CPI), clock cycle time nanoseconds. SPECratio simply reference time, supplied SPEC, divided measured execution time. single number quoted SPECINTC2006 geometric mean SPECratios. simplify marketing computers, SPEC decided report single number summarizing 12 integer benchmarks. Dividing execution time reference processor execution ti 122the evaluated computer normalizes execution time measurements; normalization yields measure, called SPECratio , advantage bigger numeric results indicate faster performance. is, SPECratio inverse f execution time. CINT2006 CFP2006 summary measurement obtained taking geometric mean SPECratios. Elaboration comparing two computers using SPECratios, apply geometric mean gives relative answer matter computer used normalize results. averaged normalized execution time values arithmetic mean, results would vary depending computer choose reference. formula geometric mean Execution time ratioi execution time, normalized reference computer, ith program total n workload, SPEC Power Benchmark Given increasing importance energy power, SPEC added benchmark measure power. reports power consumption servers different workload levels, divided 10% increments , period time. Figure 1.19 shows results server using Intel Nehalem processors similar above. 123FIGURE 1.19 SPECpower_ssj2008 running dual socket 2.66 GHz Intel Xeon X5650 16 GB DRAM one 100 GB SSD disk. SPECpower started another SPEC benchmark Java business applications (SPECJBB2005), exercises processors, caches, main memory well Java virtual machine, compiler, garbage collector, pieces operating system. Performance measured throughput, units business operations per second. again, simplify marketing computers, SPEC boils numbers one number, called “overall ssj_ops per watt.” formula single summarizing metric ssj_opsi performance 10% increment poweri power consumed performance level. 1.10 Fallacies Pitfalls Science must begin myths, criticism myths. Sir Karl Popper, Philosophy Science, 1957 124The purpose section fallacies pitfalls, found every chapter, explain commonly held misconceptions might encounter. call fallacies . discussing fallacy, try give counterexample. also discuss pitfalls , easily made mistakes. Often pitfalls generalizations principles true limited context. purpose sections help avoid making mistakes computers may design use. Cost/performance fallacies pitfalls ensnared many computer architect, including us. Accordingly, section suff ers shortage relevant examples. start pitfall traps many designers reveals important relationship computer design. Pitfall: Expecting improvement one aspect computer increase overall performance amount proportional size improvement. great idea making common case fast demoralizing corollary plagued designers hardware software. reminds us opportunity improvement affected much time event consumes. simple design problem illustrates well. Suppose program runs 100 seconds computer, multiply operations responsible 80 seconds time. much improve speed multiplication want program run 125five times faster? execution time program making improvement given following simple equation known Amdahl’s Law : Amdahl’s Law rule stating performance enhancement possible given improvement limited amount improved feature used. quantitative version law diminishi ng returns. problem: Since want performance five times faster, new execution time 20 seconds, giving is, amount enhance-multiply achieve fivefold increase performance, multiply accounts 80% workload. performance enhancement possible given improvement limited amount improved feature used. everyday life concept also yiel ds call law diminishing returns. use Amdahl’s Law estimate performance improvements know time consumed 126function potential speedup. Amdahl’s Law, together CPU performance equation, handy tool evaluating possible enhancements. Amdahl’s Law explored detail exercises. Amdahl’s Law also used argue practical limits number parallel processors. examine argument Fallacies Pitfalls section Chapter 6 . Fallacy: Computers low utilization use little power. Power efficiency matters low utilizations server workloads vary. Utilization servers Google’s warehouse scale computer, example, 10% 50% time 100% less 1% time. Even given 5 years learn run SPECpower benchmark well, specially configured computer best results 2012 still uses 33% peak power 10% load. Systems field configured SPECpower benchmark surely worse. Since servers’ workloads vary use large fraction peak power, Luiz Barroso Urs Hölzle [2007] argue redesign hardware achieve “energy-proportional computing.” future servers used, say, 10% peak power 10% workload, could reduce electricity bill datacenters become good corporate citizens era increasing concern CO2 emissions. Fallacy: Designing performance designing energy efficiency unrelated goals. Since energy power time, often case hardware software optimizations take less time save energy overall even optimization takes bit energy used. One reason rest computer consuming energy program running, even optimized portion us es little energy, reduced time save energy whole system. Pitfall: Using subset performance equation performan ce metric. 127We already warned danger predicting performance based simply one clock rate, instruction count, CPI. Another common mistake use two three factors compare performance. Although using two three factors may valid limited context, concept also easily misused. Indeed, nearly proposed alternatives use time performance metric led eventually misleadin g claims, distorted results, incorrect interpretations. One alternative time MIPS (million instructions per second) . given program, MIPS simply Since MIPS instruction execution rate, MIPS specifies performance inversely execution time; faster computers higher MIPS rating. good news MIPS easy understand, quicker computers mean bigger MIPS, matches intuition. million instructions per second (MIPS) measurement program execution speed based number millions instructions. MIPS computed instructi count divided product execution time 106. three problems using MIPS measure comparing computers. First, MIPS specifies instruction execution rate take account capabilities instructions. cannot compare computers different instruction sets using MIPS, since instruction counts wil l certainly differ. Second, MIPS varies programs computer; thus, computer cannot single MIPS rating. example, substituting execution time, see relationship MIPS, clock rate, CPI: 128The CPI varied factor 5 SPEC CPU2006 Intel Core i7 computer Figure 1.18 , MIPS well. Finally, importantly, new program executes instructions bu instruction faster, MIPS vary independently performance! Consider following performance measurements program: Check Measurement Computer Computer B Instruction count 10 billion 8 billion Clock rate 4 GHz 4 GHz CPI 1.0 1.1 a. computer higher MIPS rating? b. computer faster? 1.11 Concluding Remarks Although difficult predict exactly level cost/performance computers future, it’s safe bet much better today. participate advances, computer designers programmers must understand wider variety issues. … ENIAC equipped 18,000 vacuum tubes weighs 30 tons, computers future may 1,000 vacuum tubes perhaps weigh 1½ tons. Popular Mechanics, March 1949 hardware software designers construct computer systems hierarchical layers, lower layer hiding detail level above. great idea abstraction fundamental understanding today’s computer systems, mean designers limit knowing single abstractio n. 129Perhaps important example abstraction interface hardware low-level software, called instruction set architecture . Maintaining instruction set architecture constant enables many implementations architecture— presumably varying cost performance—to run identical software. downside, architecture may preclude introducing innovations require interface change. reliable method determining reporting performance using execution time real programs metric. execution time related important measurements make following equation: use equation constituent factors many times. Remember, though, individually factors determine performance: product, equals execution time, 130reliable measure performance. BIG Picture Execution time valid unimpeachable measure performance. Many metrics proposed found wanting. Sometimes metrics flawed start reflecting execution time; times metric sound limited context extended used beyond context without additional clarification needed make valid. key hardware technology modern processors silicon. Equal importance understanding integrated circuit technology understanding expected rates technological change, predicted Moore’s Law . silicon fuels rapid advance hardware, new ideas organization computers improved price/performance. Two key ideas exploiting parallelism program, normally today via multiple processors, exploiting locality accesses memory hierarchy , typically via caches. 131Energy efficiency replaced die area critical resource microprocessor design. Conserving power rying increase performance forced hardware industry switch multicore microprocessors, thereby requiring softwar e industry switch programming parallel hardware. Parallelism required performance. Computer designs always measured cost 132performance, well important factors energy, dependability, cost ownership, scalability. Although chapter focused cost, performance, energy, best designs strike appropriate balance given market among factors. Road Map Book bottom abstractions five classic components computer: datapath, control, memory, input, output (refer Figure 1.5 ). five components also serve framework rest chapters book: Datapath : Chapter 3 , Chapter 4 , Chapter 6 , Appendix B Control : Chapter 4 , Chapter 6 , Appendix B Memory : Chapter 5 Input : Chapters 5 6 Output : Chapters 5 6 mentioned above, Chapter 4 describes processors exploit implicit parallelism, Chapter 6 describes explicitly parallel multicore microprocessors heart parallel revolution, Appendix B describes highly parallel graphics processor chip. Chapter 5 describes memory hierarchy exploits locality. Chapter 2 describes instruction sets—the interface compilers computer—and emphasizes role compilers programming languages using features instruction set. Chapter 3 describes computers handle arithmetic data. Appendix introduces logic design. 133 Historical Perspective Reading active field science like immense anthill; individual almost vanishes mass minds tumbling other, carrying information place place, passing around speed light. Lewis Thomas, “Natural Science,” Lives Cell, 1974 chapter text, section devoted historical perspective found online site accompanies book. may trace development idea series computers describe important projects, provide references case interested probing further. historical perspective chapter provides backgrou nd key ideas presented opening chapter. purpose give human story behind technological advances place achievements historical context. studying past, may better able understand forces shape computing future. Historical Perspectiv e section online ends suggestions reading, whi ch also collected separately online section “ Reading .” rest Section 1.12 found online. 1.12 Historical Perspective Reading active field science like immense anthill; individual 134almost vanishes mass minds tumbling other, carrying information place place, passing around speed light. Lewis Thomas, “Natural Science,” Lives Cell, 1974 chapter text, section devoted historical perspective found online. may trace development idea series machines describe important projects, provide references case interested probing further. historical perspective chapter provides background key ideas presented therein. purpose gi human story behind technological advances place achievements historical context. learning past, may better able understand forces shape computing future. historical perspective section en ds suggestions additional reading, also collected separately online section “Further Reading.” First Electronic Computers J. Presper Eckert John Mauchly Moore School University Pennsylvania built widely accepted th e world’s first operational electronic, general-purpose comput er. machine, called ENIAC ( Electronic Numerical Integrator Calculator ), funded United States Army started working World War II publicly disclosed 1946. ENIAC general-purpose machine used computing artillery-firing tables. Figure e1.12.1 shows U-shaped computer, 80 feet long 8.5 feet high several feet wide. 20 10-digit registers 2 feet long. total, ENIAC used 18,000 vacuum tubes. 135FIGURE E1.12.1 ENIAC, world’s first general- purpose electronic computer. size, ENIAC two orders magnitude bigger machines built today, yet eight orders magnitude slower, performing 1900 additions per second. ENIAC provided conditional jumps programmable, clearly distinguishing earlier calculators. Programming done manually plugging cables setting switches, data entered punched cards. Programming typical calculations required half hour whole day. ENIAC general- purpose machine, limited primarily small amount storage tedious programming. 1944, John von Neumann attracted ENIAC project. group wanted improve way programs entered discussed storing programs numbers; von Neumann helped crystallize ideas wrote memo proposing stored-program computer called EDVAC ( Electronic Discrete Variable Automatic Computer ). Herman Goldstine distributed memo put von Neumann’s name it, much dismay Eckert Mauchly, whose names omitted. memo served basis commonly used term von Neumann computer . Several early pioneers computer field believe term gives much credit von Neumann, wrote ideas, little engineers, Eckert Mauchly, worked machines. reason, term appear elsewhere book 136in online sections. 1946, Maurice Wilkes Cambridge University visited Moore School attend latter part series lectures developments electronic computers. returned Cambridge, Wilkes decided embark project build stored-program computer named EDSAC ( Electronic Delay Storage Automatic Calculator ). EDSAC started working 1949 world’s first full-scale, operational, stored-program computer [Wilkes, 1985]. (A small prototype called Mark-I, built University Manchester 1948, might called first operational stored-program machine.) Section 2.5 explains stored-program concept. 1947, Eckert Mauchly applied patent electronic computers. dean Moore School demanded patent turned university, may helped Eckert Mauchly conclude leave. departure crippled EDVAC project, delaying completion 1952. Goldstine left join von Neumann Institute Advanced Study (IAS) Princeton 1946. Together Arthur Burks, issued report based memo written earlier [Burks et al., 1946]. paper incredible period; reading today, would never guess landmark paper written 50 years ago, discusses architectural concepts seen modern computers. paper led IAS machine built Julian Bigelow. total 1024 40-bit words roughly 10 times faster ENIAC. group thought uses machine, published set reports, encouraged visitor s. reports visitors inspired development numbe r new computers. Recently, controversy work John Atanasoff, built small-scale electronic computer early 1940s. machine, designed Iowa State University, special-purpose computer never completely operational . Mauchly briefly visited Atanasoff built ENIAC. presence Atanasoff machine, together delays filing ENIAC patents (the work classified patents could filed war) distribution von Neumann’ EDVAC paper, used break Eckert-Mauchly patent. Though controversy still rages Atanasoff’s role, Eckert 137Mauchly usually given credit building first working, general-purpose, electronic computer [Stern, 1980]. Another pioneering computer deserves credit special- purpose machine built Konrad Zuse Germany late 1930s early 1940s. Although Zuse design programmable computer ready, German government decided fund scientific investigations taking 2 years bureaucrats expected war would deadline. Across English Channel, World War II special- purpose electronic computers built decrypt interce pted German messages. team Bletchley Park, including Alan Turing, built Colossus 1943. machines kept secret 1970; war, group little impact commercial British computers. work ENIAC went forward, Howard Aiken building electro-mechanical computer called Mark-I Harvard (a name Manchester later adopted machine). followed Mark-I relay machine, Mark-II, pair vacuum tube machines, Mark-III Mark-IV. contrast earlier machines like EDSAC, used single memory instructions data, Mark-III Mark-IV separate memories instructions data. machines regarded reactionary advocates stored-program computers; term Harvard architecture coined describe machines distinct memories. Paying respect history, term used today different sense describe machines single main memory separate caches instructions data. Whirlwind project begun MIT 1947 aimed applications real-time radar signal processing. Although led several inventions, important innovation magnetic core memory. Whirlwind 2048 16-bit words magnetic core. Magnetic cores served main memory technology nearly 30 years. Commercial Developments December 1947, Eckert Mauchly formed Eckert-Mauchly Computer Corporation. first machine, BINAC, built 138for Northrop shown August 1949. financial difficulties, firm acquired Remington-Rand, built UNIVAC (Universal Automatic Computer), designed sold general-purpose computer ( Figure e1.12.2 ). Originally delivered June 1951, UNIVAC sold $1 million first successful commercial computer—48 systems built! early machine, along many fascinating pieces computer lore, may seen Computer History Museum Mountain View, California. FIGURE E1.12.2 UNIVAC I, first commercial computer United States. correctly predicted outcome 1952 presidential election, initial forecast withheld broadcast experts doubted use early results. IBM punched card office automation business didn’t start building computers 1950. first IBM computer, IBM 701, shipped 1952, eventually 19 units sold. early 1950s, many people pessimistic future computers, believing market opportunities “highly specialized” machines quite limited. 139In 1964, investing $5 billion, IBM made bold move announcement System/360. IBM spokesman said following time: humble announcement. important product announcement corporation ever made history. It’s computer previous sense. It’s product, line products … spans performance low part computer line high. Moving idea architecture abstraction commercial reality, IBM announced six implementations System/360 architecture varied price performance factor 25. Figure e1.12.3 shows four models. IBM bet company success computer family , IBM won. System/360 successors dominated large computer market. 140FIGURE E1.12.3 IBM System/360 computers: models 40, 50, 65, 75 introduced 1964. four models varied cost performance factor almost 10; grows 25 include models 20 30 (not shown). clock rate, range memory sizes, approximate price processor memory average size: (a) model 40, 1.6 MHz, 32 KB–256 KB, $225,000; (b) model 50, 2.0 MHz, 128 KB–256 KB, $550,000; (c) model 65, 5.0 MHz, 256 KB–1 MB, $1,200,000; (d) model 75, 5.1 MHz, 256 KB–1 MB, $1,900,000. Adding I/O devices typically increased price factors 1.8 3.5, higher factors cheaper models. year later, Digital Equipment Corporation (DEC) unveiled PDP-8, first commercial minicomputer . small machine breakthrough low-cost design, allowing DEC offer computer $20,000. Minicomputers forerunners microprocessors, Intel inventing first microproc essor 1971—the Intel 4004. 1963 came announcement first supercomputer . 141announcement came neither large companies even high-tech centers. Seymour Cray led design Control Data Corporation CDC 6600 Minnesota. machine included many ideas beginning found latest microprocessors. Cray later left CDC form Cray Research, Inc., Wisconsin. 1976, announced Cray-1 ( Figure e1.12.4 ). machine simultaneously fastest world, expensive, computer best cost/performance scientific programs. 142FIGURE E1.12.4 Cray-1, first commercial vector supercomputer, announced 1976. machine unusual distinction fastest computer scientific applications computer best price/performance applications. Viewed top, computer looks like letter C. Seymour Cray passed away 1996 injuries sustained automobile accident. time death, 70-year-old computer pioneer working vision next generation supercomputers. (See www.cray.com details.) Seymour Cray creating world’s expensive 143computer, designers around world looking using microprocessor create computer cheap could home. single fountainhead personal computer , 1977, Apple IIe ( Figure e1.12.5 ) Steve Jobs Steve Wozniak set standards low cost, high volume, high reliability defined personal computer industry. FIGURE E1.12.5 Apple IIc Plus. Designed Steve Wozniak, Apple IIc set standards cost reliability industry. However, even 4-year head start, Apple’s personal computers finished second popularity. IBM Personal Computer, announced 1981, became best-selling computer kind; success gave Intel popular microprocessor Microsoft popular operating system. Today, popular CD Microsoft operating system, even though cos ts many times music CD! course, 30 years IBM-compatible personal computer existed, evolved greatly. fact, first personal computers 16-bit processors 64 kilobytes memory , low-density, slow floppy disk nonvolatile storage! Floppy disks originally developed IBM loading diagnostic programs 144mainframes, major I/O device personal computers almost 20 years advent CDs networking made obsolete method exchanging data. course, Intel microprocessors also evolved since fir st PC, used 16-bit processor 8-bit external interface! Chapter 2, write evolution Intel architecture . first personal computers quite simple, little graphics capability, pointing devices, primitive operating systems compared today. computer inspired many architectural software concepts characterize modern desktop machines Xerox Alto, shown Figure e1.12.6 . Alto created experimental prototype future computer; several hundred Altos built, inclu ding significant number donated universities. Among technologies incorporated Alto were: bit-mapped graphics display integrated computer (earlier graphics displays acted terminals, usually connected larger computers) mouse, invented earlier, included every Alto used extensively user interface local area network (LAN), became precursor Ethernet user interface based Windows featuring WYSIWYG 145(what see get) editor interactive drawing programs 146FIGURE E1.12.6 Xerox Alto primary inspiration modern desktop computer. included mouse, bit-mapped scheme, Windows-based user interface, local network connection. 147In addition, file servers print servers developed interfaced via local area network, connections local area network wide area ARPAnet produced first versions Internet-style networking. Xerox Alto w incredibly influential clearly affected design wide variety computers software systems, including Apple Macintosh, IBM-compatible PC, MacOS Windows, Sun early workstations. Measuring Performance earliest days computing, designers specified performance goals—ENIAC 1000 times faster Harvard Mark-I, IBM Stretch (7030) 100 times faster fastest computer existence. wasn’t clear, though, performance measured. original measure performance time required perform individual operation, addition. Since instructions took execution time, timing one others. execution times instructions computer became diverse, however, time required e operation longer useful comparisons. consider differences, instruction mix calculated measuring relative frequency instructions computer across many programs. Multiplying time instruction weight mix gave user average instruction execution time. (If measured clock cycles, average instruction execution time average CPI.) Since instruction sets similar, precise comparison add times. average instruction execution time, then, small step MIPS. MIPS virtue easy understand; hence, grew popularity. Quest Average Program processors becoming sophisticated relied memory hierarchies (the topic Chapter 5 ) pipelining (the topic Chapter 4 ), single execution time instruction longer existed; neither execution time MIPS, therefore, co uld 148calculated instruction mix manual. Although might seem obvious today right thing would develop set real applications could used standard benchmarks, difficult task relatively recent times. Variations operating systems language standards made hard create large programs could moved computer computer simply recompiling. Instead, next step benchmarking using synthetic programs. Whetstone synthetic program created measuring scientific programs written Algol-60 (see Curnow Wichmann’s [1976] description). program converted Fortran widely used characterize scientific program performance. Whetstone performance typically quoted Whetstones per second—the number executions single iteration Whetstone benchmark! Dhrystone another synthetic benchmark still used embedded computing circles (see Weicker’s [1984] description methodology). time Whetstone developed, concept kernel benchmarks gained popularity. Kernels small, time- intensive pieces real programs extracted used benchmarks. approach developed primarily benchmarking high-end computers, especially supercomputers. Livermore Loops Linpack best-known examples. Livermore Loops consist series 21 small loop fragments. Linpack consists portion linear algebra subroutine package. Kernels best used isolate performance individual features computer explain reasons differences n performance real programs. scientific applications often use small pieces code execute long time, characterizing performance kernels popular application class. Although kernels help illuminate performance, frequently overstate performance real applications. SPECulating Performance important advance performance evaluation formation System Performance Evaluation Cooperative 149(SPEC) group 1988. SPEC comprises representatives many computer companies—the founders Apollo/ Hewlett- Packard, DEC, MIPS, Sun—who agreed set real programs inputs run. worth noting SPEC couldn’t come portable operating systems popularity high-level languages. compilers, too, accepted proper part performance computer systems must measured evaluation. History teaches us SPEC effort may useful current computers, meet needs next gener ation without changing. 1991, throughput measure added, based running multiple versions benchmark. usefu l evaluating timeshared usage uniprocessor multiprocessor . system benchmarks include OS-intensive I/O- intensive activities also added. Another change decision drop benchmarks add others. One result difficulty finding benchmarks initial version SPEC benchmarks (called SPEC89) contained six floating-point benchmarks four integer benchmarks. Calculating single summary measurement using geometric mean execution times normalized VAX-11/780 meant measure favored computers strong floating-point performance. 1992, new benchmark set (called SPEC92) introduced. incorporated additional benchmarks, dropped matrix300, provided separate means (SPEC INT SPECFP) integer floating-point programs. addition, SPECbase measure, disallows program-specific optimization flags, added provide users performance measurement would closely match might experience programs. SPECFP numbers show largest increase versus base SPECFP measurement, typically ranging 15% 30% higher. 1995, benchmark set updated, adding new integer floating-point benchmarks, well removing benchmarks suffered flaws running times become small given factor 20 performance improvement since first SPEC release. SPEC95 also changed base computer normalization Sun SPARC Station 10/40, since operating versions original base computer becoming difficult find! 150The recent version SPEC SPEC2006. perhaps surprising floating-point programs SPEC2006 new, integer programs two SPEC2000, one SPEC95, none SPEC92, one SPEC89. sole survivor SPEC89 gcc compiler. SPEC also added benchmark suites beyond original suites targeted CPU performance. 2008, SPEC provided benchmark sets graphics, high-performance scientific computing, obje ct- oriented computing, file systems, Web servers clients, Java, engineering CAD applications, power. Growth Embedded Computing Embedded processors around long time; fact, first minicomputers first microprocessors wer e originally developed controlling functions laboratory r industrial application. many years, dominant use embedded processors industrial control applications, although use continued grow, processors tended cheap performance relatively low. example, best-selling processor world remains 8-bit micro contr oller used cars, home appliances, simple applications. late 1980s early 1990s saw emergence new opportunities embedded processors, ranging advanced video games set-top boxes cell phones personal digital assistants. rapidly increasing number information appliances growth networking driven dramatic surges number embedded processors, well performance requirements. evaluate performance, embedded community inspired SPEC create Embedded Microprocessor Benchmark Consortium (EEMBC ). Started 1997, consists collection kernels organized suites add ress different portions embedded industry. announced th e second generation benchmarks 2007. Half-Century Progress Since 1951, thousands new computers using wide range technologies widely varying capabilities. 151Figure e1.12.7 summarizes key characteristics machines mentioned section shows dramatic changes occurred 50 years . adjusting inflation, price/performance improved almost 100 billion 55 years, 58% per year. Another way say we’ve seen factor 10,000 improvement cost factor 10,000,000 improvement performance. FIGURE E1.12.7 Characteristics key commercial computers since 1950, actual dollars 2007 dollars adjusted inflation. 152The last row assumes fully utilize potential performance four cores Barcelona. contrast Figure e1.12.3 , price IBM S/360 model 50 includes I/O devices. (Source: Computer History Museum Producer Price Index Industrial Commodities.) Readers interested computer history consult Annals History Computing , journal devoted history computing. Several books describing early days computing also appeared, many written pioneers including Goldstine [1972], Metropolis et al. [1980], Wilkes [1985]. Reading Barroso, L. U. Hölzle [2007]. “The case energy-proportional computing”, IEEE Computer December. plea change nature computer components use much less power lightly utilized. Bell, C. G. [1996]. Computer Pioneers Pioneer Computers , ACM Computer Museum, videotapes. Two videotapes history computing, produced Gordon Gwen Bell, including following machines inventors: Harvar Mark-I, ENIAC, EDSAC, IAS machine, many others. Burks, A. W., H. H. Goldstine, J. von Neumann [1946]. “Preliminary discussion logical design electronic computing instrument,” Report U.S. Army Ordnance Department, p. 1; also appears Papers John von Neumann , W. Aspray A. Burks (Eds.), MIT Press, Cambridge, MA, Tomash Publishers, Los Angeles, 1987, 97–146. classic paper explaining computer hardware software first stored-program computer built. quote extensively Chapter 3. simultaneously explained computers world source controversy first draft give credit Eck ert Mauchly. Campbell-Kelly, M. W. Aspray [1996]. Computer: History Information Machine , Basic Books, New York. Two historians chronicle dramatic story. New York Times call well written authoritative. Ceruzzi, P. F. [1998]. History Modern Computing , MIT Press, Cambridge, MA. 153Contains good description later history computing: integrated circuit impact, personal computers, UNIX, Internet. Curnow, H. J. B. A. Wichmann [1976]. “A synthetic benchmark”, Computer J. 19(1):80. Describes first major synthetic benchmark, Whetstone, created. Flemming, P. J. J. J. Wallace [1986]. “How lie statistics: correct way summarize benchmark results”, Comm. ACM 29:3 (March), 218–21. Describes underlying principles using different means summarize performance results . Goldstine, H. H. [1972]. Computer: Pascal von Neumann , Princeton University Press, Princeton, NJ. personal view computing one pioneers worked von Neumann. Hayes, B. [2007]. “Computing parallel universe”, American Scientist Vol. 95(November–December):476–480. overview parallel computing challenge written layman. Hennessy, J. L. D. A. Patterson [2007]. Chapter 1 Computer Architecture: Quantitative Approach , fourth edition, Morgan Kaufmann Publishers, San Francisco. Section 1.5 goes detail power , Section 1.6 contains much detail cost integrated circuits explains reason difference price cost, Section 1.8 gives details evaluating performance . Lampson, B. W. [1986]. “Personal distributed computing; Alto Ethernet software.” ACM Conference History Personal Workstations (January). Thacker, C. R. [1986]. “Personal distributed computing: Alto Ethernet hardware,” ACM Conference History Personal Workstations (January). two papers describe software hardware landmark Alto. Metropolis, N., J. Howlett, G.-C. Rota (Eds.) [1980]. History Computing Twentieth Century , Academic Press, New York. collection essays describe people, software, computers, nd laboratories involved first experimental commercial com puters. authors personally involved projects. excellen 154bibliography early reports concludes interesting book. Public Broadcasting System [1992]. Machine Changed World , videotapes. five 1-hour programs include rare footage interviews pioneers computer industry. Slater, R. [1987]. Portraits Silicon , MIT Press, Cambridge, MA. Short biographies 31 computer pioneers . Stern, N. [1980]. “Who invented first electronic digital computer?” Annals History Computing 2:4 (October), 375–76 historian’s perspective Atanasoff versus Eckert Mauchly. Weicker, R. P. [1984]. “Dhrystone: synthetic systems programming benchmark”, Communications ACM 27(10):1013– 1030. Description synthetic benchmarking program systems code. Wilkes, M.V. [1985]. Memoirs Computer Pioneer , MIT Press, Cambridge, MA. personal view computing one pioneers. 1.13 Exercises relative time ratings exercises shown square bracket exercise number. average, exercise rated [10] take twice long one rated [5]. Sections text read attempting exercise given angled brackets; example, <§1.4> means read Section 1.4 , Covers, help solve exercise. 1.1 [2] <§1.1> Aside smart cell phones used billion people, list describe four types computers. 1.2 [5] <§1.2> eight great ideas computer architecture similar ideas fields. Match eight ideas computer architecture, “Design Moore’s Law,” “Use Abstraction Simplify Design,” “Make Common Case Fast,” “Performance via Parallelism,” “Performance via Pipelining,” “Performance via Prediction,” “Hierarchy Memories,” “Dependability via Redundancy” following ideas fields: a. Assembly lines automobile manufacturing b. Suspension bridge cables c. Aircraft marine navigation systems incorporate 155wind information d. Express elevators buildings e. Library reserve desk f. Increasing gate area CMOS transistor decrease switching time g. Adding electromagnetic aircraft catapults (which electrically powered opposed current steam-powered models), allowed increased power generation offered new reactor technology h. Building self-driving cars whose control systems partially rely existing sensor systems already installed base vehicle, lane departure systems smart cruise control systems 1.3 [2] <§1.3> Describe steps transform program written high-level language C representation directly executed computer processor. 1.4 [2] <§1.4> Assume color display using 8 bits primary colors (red, green, blue) per pixel frame size 1280 ×1024. a. minimum size bytes frame buffer store frame? b. long would take, minimum, frame sent 100 Mbit/s network? 1.5 [4] <§1.6> Consider three different processors P1, P2, P3 executing instruction set. P1 3 GHz clock rate CPI 1.5. P2 2.5 GHz clock rate CPI 1.0. P3 4.0 GHz clock rate CPI 2.2. a. processor highest performance expressed instructions per second? b. processors execute program 10 seconds, find number cycles number instructions. c. trying reduce execution time 30%, leads increase 20% CPI. clock rate get time reduction? 1.6 [20] <§1.6> Consider two different implementations instruction set architecture. instructions divided nto four classes according CPI (classes A, B, C, D). P1 clock rate 2.5 GHz CPIs 1, 2, 3, 3, P2 clock rate 3 GHz CPIs 2, 2, 2, 2. 156Given program dynamic instruction count 1.0E6 instructions divided classes follows: 10% class A, 20% class B, 50% class C, 20% class D, faster: P1 P2? a. global CPI implementation? b. Find clock cycles required cases. 1.7 [15] <§1.6> Compilers profound impact performance application. Assume program, compiler results dynamic instruction count 1.0E9 execution time 1.1 s, compiler B results dynamic instruction count 1.2E9 execution time 1.5 s. a. Find average CPI program given processor clock cycle time 1 ns. b. Assume compiled programs run two different processors. execution times two processors same, much faster clock processor running compiler A’s code versus clock processor running compiler B’s code? c. new compiler developed uses 6.0E8 instructions average CPI 1.1. speedup using new compiler versus using compiler B original processor? 1.8 Pentium 4 Prescott processor, released 2004, clock rate 3.6 GHz voltage 1.25 V. Assume that, average, consumed 10 W static power 90 W dynamic power. Core i5 Ivy Bridge, released 2012, clock rate 3.4 GHz voltage 0.9 V. Assume that, average, consumed 30 W static power 40 W dynamic power. 1.8.1 [5] <§1.7> processor find average capacitive loads. 1.8.2 [5] <§1.7> Find percentage total dissipated power comprised static power ratio static power dynamic power technology. 1.8.3 [15] <§1.7> total dissipated power reduced 10%, much voltage reduced maintain leakage current? Note: power defined product voltage current. 1.9 Assume arithmetic, load/store, branch instructions, processor CPIs 1, 12, 5, respectively. Also assume single processor program requires execution 2.56E9 157arithmetic instructions, 1.28E9 load/store instructions, 256 million branch instructions. Assume processor 2 GHz clock frequency. Assume that, program parallelized run multiple cores, number arithmetic load/store instructions per processor divided 0.7 × p (where p number processors) number branch instructions per process remains same. 1.9.1 [5] <§1.7> Find total execution time program 1, 2, 4, 8 processors, show relative speedup 2, 4, 8 processors result relative single processor result. 1.9.2 [10] <§§1.6, 1.8> CPI arithmetic instructions doubled, would impact execution time program 1, 2, 4, 8 processors? 1.9.3 [10] <§§1.6, 1.8> CPI load/store instructions reduced order single processor matc h performance four processors using original CPI values? 1.10 Assume 15 cm diameter wafer cost 12, contains 84 dies, 0.020 defects/cm2. Assume 20 cm diameter wafer cost 15, contains 100 dies, 0.031 defects/cm2. 1.10.1 [10] <§1.5> Find yield wafers. 1.10.2 [5] <§1.5> Find cost per die wafers. 1.10.3 [5] <§1.5> number dies per wafer increased 10% defects per area unit increases 15%, find die area yield. 1.10.4 [5] <§1.5> Assume fabrication process improves yield 0.92 0.95. Find defects per area unit version technology given die area 200 mm2. 1.11 results SPEC CPU2006 bzip2 benchmark running AMD Barcelona instruction count 2.389E12, execution time 750 s, reference time 9650 s. 1.11.1 [5] <§§1.6, 1.9> Find CPI clock cycle time 0.333 ns. 1.11.2 [5] <§1.9> Find SPECratio. 1.11.3 [5] <§§1.6, 1.9> Find increase CPU time number instructions benchmark increased 10% without affecting CPI. 1581.11.4 [5] <§§1.6, 1.9> Find increase CPU time number instructions benchmark increased 10% CPI increased 5%. 1.11.5 [5] <§§1.6, 1.9> Find change SPECratio change. 1.11.6 [10] <§1.6> Suppose developing new version AMD Barcelona processor 4 GHz clock rate. added additional instructions instruction set way number instructions reduced 15%. execution time reduced 700 new SPECratio 13.7. Find new CPI. 1.11.7 [10] <§1.6> CPI value larger obtained 1.11.1 clock rate increased 3 GHz 4 GHz. Determine whether increase CPI similar clock rate. dissimilar, why? 1.11.8 [5] <§1.6> much CPU time reduced? 1.11.9 [10] <§1.6> second benchmark, libquantum, assume execution time 960 ns, CPI 1.61, clock rate 3 GHz. execution time reduced additional 10% without affecting CPI clock rate 4 GHz, determine number instructions. 1.11.10 [10] <§1.6> Determine clock rate required give 10% reduction CPU time maintaining number instructions CPI unchanged. 1.11.11 [10] <§1.6> Determine clock rate CPI reduced 15% CPU time 20% number instructions unchanged. 1.12 Section 1.10 cites pitfall utilization subset performance equation performance metric. illustrate this, consider following two processors. P1 clock rate 4 GHz, average CPI 0.9, requires execution 5.0E9 instructions. P2 clock rate 3 GHz, average CPI 0.75, requires execution 1.0E9 instructions. 1.12.1 [5] <§§1.6, 1.10> One usual fallacy consider computer largest clock rate highest performance. Check true P1 P2. 1.12.2 [10] <§§1.6, 1.10> Another fallacy consider processor executing largest number instructions need larger CPU time. Considering processor P1 159executing sequence 1.0E9 instructions CPI processors P1 P2 change, determine number instructions P2 execute time P1 needs execute 1.0E9 instructions. 1.12.3 [10] <§§1.6, 1.10> common fallacy use MIPS ( millions instructions per second ) compare performance two different processors, consider processor largest MIPS largest performance. Check true P1 P2. 1.12.4 [10] <§1.10> Another common performance figure MFLOPS (millions floating-point operations per second), defined figure problems MIPS. Assume 40% instructions executed P1 P2 floating-point instructions. Find MFLOPS figures processors. 1.13 Another pitfall cited Section 1.10 expecting improve overall performance computer improving one aspect computer. Consider computer running program requires 250 s, 70 spent executing FP instructions, 85 executed L/S instructions, 40 spent executing branch instructions. 1.13.1 [5] <§1.10> much total time reduced time FP operations reduced 20%? 1.13.2 [5] <§1.10> much time INT operations reduced total time reduced 20%? 1.13.3 [5] <§1.10> total time reduced 20% reducing time branch instructions? 1.14 Assume program requires execution 50 ×106 FP instructions, 110 ×106 INT instructions, 80 ×106 L/S instructions, 16 ×106 branch instructions. CPI type instruction 1, 1, 4, 2, respectively. Assume processor 2 GHz clock rate. 1.14.1 [10] <§1.10> much must improve CPI FP instructions want program run two times faster? 1.14.2 [10] <§1.10> much must improve CPI L/S 160instructions want program run two times faster? 1.14.3 [5] <§1.10> much execution time program improved CPI INT FP instructions reduced 40% CPI L/S Branch reduced 30%? 1.15 [5] <§1.8> program adapted run multiple processors multiprocessor system, execution time eac h processor comprised computing time overhead time required locked critical sections and/or send data one processor another. Assume program requires =100 execution time one processor. run p processors, processor requires t/p s, well additional 4 overhead, irrespective number processors. Compute per-processor execution ime 2, 4, 8, 16, 32, 64, 128 processors. case, list corresponding speedup relative single processor ratio actual speedup versus ideal speedup (speedup overhead). Answers Check §1.1, page 10: Discussion questions: many answers acceptable. §1.4, page 24: DRAM memory: volatile, short access time 50 70 nanoseconds, cost per GB $5 $10. Disk memory: nonvolatile, access times 100,000 400,000 times slower DRAM, cost per GB 100 times cheaper DRAM. Flash memory: nonvolatile, access times 100 1000 times slower DRAM, cost per GB 7 10 times cheaper DRAM. §1.5, page 28: 1, 3, 4 valid reasons. Answer 5 generally true high volume make extra investment reduce die size by, say, 10% good economic decision, doesn’t true. §1.6, page 33: 1. a: both, b: latency, c: neither. 7 seconds. §1.6, page 40: b. §1.10, page 51: a. Computer higher MIPS rating. b. Computer B faster. 1611622 Instructions Language Computer Abstract chapter describes instructions, language compute r. explains two principles stored-program computer: th e use instructions indistinguishable numbers use f alterable memory programs. “instruction set architecture” (ISA) abstract interface hardware lowest- level software encompasses information necessary wr ite machine language program run correctly. machine level assembly language, language humans read. assembler translates language binary numbers machines understand, even “extends” instruction set creating symbolic instructions aren’t hardware. category RISC-V instructions associated constructs appear programming languages. popularity instructions dominates many. varying popularity instructions plays important role chapters datapath, control, pipelining. Keywords Operand; signed number; unsigned number; instructions; logical operations; procedures; synchronization; C Sort; arrays; pointers; compiling C; C; interpreting Java; Java; ARM instructions; RISC-V; MIPS; x86 instructions; instruction se t; stored-program concept; word; doubleword; data transfer instruction ; address; alignment restriction; binary digit; binary bit; least signifi cant bit; significant bit; one’s complement; biased notation; instruction format; machine language; hexadecimal; opcode; AND; OR; NOT; XOR; EOR; ORR; condition al branch; basic block; branch address table; branch table; procedure; branch-and-li nk instruction; 163return address; caller; callee; program counter; PC; stack; stack point er; push; pop; global pointer; procedure frame; activation record; frame pointe r; text segment; PC- relative addressing; addressing mode; data race; assembly language ; pseudoinstruction; symbol table; linker; link editor; executable file; loader; dynamically linked libraries; DLL; Java bytecode; Java Virtual Machin e; JVM; Time compiler; JIT; object oriented language; general-purpose register; GPR speak Spanish God, Italian women, French men, German horse. Charles V, Holy Roman Emperor (1500–1558) OUTLINE 2.1 Introduction 62 2.2 Operations Computer Hardware 63 2.3 Operands Computer Hardware 67 2.4 Signed Unsigned Numbers 74 2.5 Representing Instructions Computer 81 2.6 Logical Operations 89 2.7 Instructions Making Decisions 92 2.8 Supporting Procedures Computer Hardware 98 2.9 Communicating People 108 2.10 RISC-V Addressing Wide Immediates Addresses 113 2.11 Parallelism Instructions: Synchronization 121 2.12 Translating Starting Program 124 2.13 C Sort Example Put Together 133 2.14 Arrays versus Pointers 141 2.15 Advanced Material: Compiling C Interpreting Java 144 2.16 Real Stuff: MIPS Instructions 145 2.17 Real Stuff: x86 Instructions 146 2.18 Real Stuff: Rest RISC-V Instruction Se 155 2.19 Fallacies Pitfalls 157 1642.20 Concluding Remarks 159 2.21 Historical Perspective Reading 162 2.22 Exercises 162 165The Five Classic Components Computer 2.1 Introduction command computer’s hardware, must speak language. words computer’s language called instructions , vocabulary called instruction set . chapter, see instruction set real computer, form written b people form read computer. introduce instructions top-down fashion. Starting notation looks like restricted programming language, refine step-by- step see actual language real computer. Chapter 3 continues downward descent, unveiling hardware arithmetic representation floating-point numbers. instruction set 166The vocabulary commands understood given architecture. might think languages computers would diverse people, reality, computer languages quite similar, like regional dialects independent languages. Hence, learn one, easy pick others. chosen instruction set RISC-V, originally developed UC Berkeley starting 2010. demonstrate easy pick instruction sets, also take quick look two popular instruction sets . 1. MIPS elegant example instruction sets designed since 1980s. several respects, RISC-V follows similar design. 2. Intel x86 originated 1970s, still today powers PC Cloud post-PC era. similarity instruction sets occurs computer constructed hardware technologies based similar underlying principles basic operations computers must provide. Moreover, computer designers common goal: find language makes easy build hardware compiler maximizing performance minimizing cost energy. goal time-honored; following quote written could buy computer, true today 1947: easy see formal-logical methods exist certain [instruction sets] abstract adequate control cause execution sequence operations.… really decisive considerations present point view, selecting [instruction set], practical nature: simplicity equipment demanded [instruction set], clarity application actually important problems together speed handling problems. Burks, Goldstine, von Neumann, 1947 “simplicity equipment” valuable consideration today’s computers 1950s. goal chapter teach instruction set follows advice, showing represented hardware 167relationship high-level programming languages primitive one. examples C programming language; Section 2.15 shows would change object-oriented language like Java. learning represent instructions, also discove r secret computing: stored-program concept . Moreover, exercise “foreign language” skills writing programs language computer running simulator comes book. also see impact programming languages compiler optimization performance. conclude look historical evolution instruction sets overview computer dialects. stored-program concept idea instructions data many types stored memory numbers thus easy change, leading stored-program computer. reveal first instruction set piece time, giving rationale along computer structures. top-down, ste p- by-step tutorial weaves components explanations , making computer’s language palatable. Figure 2.1 gives sneak preview instruction set covered chapter. 168FIGURE 2.1 RISC-V assembly language revealed chapter. information also found Column 1 RISC - V Reference Data Card front book. 2.2 Operations Computer 169Hardware must certainly instructions performing fundament al arithmetic operations. Burks, Goldstine, von Neumann, 1947 Every computer must able perform arithmetic. RISC-V assembly language notation add a, b, c instructs computer add two variables b c put sum a. notation rigid RISC-V arithmetic instruction performs one operation must always exactly three variables. example, suppose want place sum four variables b, c, d, e variable a. (In section, deliberately vague “variable” is; next section, we’ll explain detail.) following sequence instructions adds four variables: add a, b, c // sum b c placed add a, a, // sum b, c, add a, a, e // sum b, c, d, e Thus, takes three instructions sum four variables. words right double slashes ( //) line comments human reader, computer ignores them. Note unlike programming languages, line language contain one instruction. Another difference C comments always terminate end line. natural number operands operation like addition three: two numbers added together place put sum. Requiring every instruction exactly three operands , less, conforms philosophy keeping hardware simple: hardware variable number operands complicated hardware fixed number. situation illustrates first three underlying principles hardwar e design: Design Principle 1: Simplicity favors regularity. 170We show, two examples follow, relationship programs written higher-level programming languages programs primitive notation. Compiling Two C Assignment Statements RISC-V Example segment C program contains five variables a, b, c, d, e. Since Java evolved C, example next work either high-level programming language: = b + c; = −e; compiler translates C RISC-V assembly language instructions. Show RISC-V code produced compiler. Answer RISC-V instruction operates two source operands places result one destination operand. Hence, two simple statements compile directly two RISC-V assembly language instructions: add a, b, c sub d, a, e Compiling Complex C Assignment RISC-V Example somewhat complicated statement contains five variables f, g, h, i, j: f = (g + h) −(i + j); might C compiler produce? Answer compiler must break statement several assembly instructions, since one operation performed per RISC-V instruction. first RISC-V instruction calculates sum g h. must place result somewhere, compiler creates temporary variable, called t0: 171add t0, g, h // temporary variable t0 contains g + h Although next operation subtract, need calculate sum j subtract. Thus, second instruction places sum j another temporary variable created compiler, called t1: add t1, i, j // temporary variable t1 contains + j Finally, subtract instruction subtracts second sum first places difference variable f, completing compiled code: sub f, t0, t1 // f gets t0 −t1, (g + h) −(i + j) Check given function, programming language likely takes lines code? Put three representations ord er. 1. Java 2. C 3. RISC-V assembly language Elaboration increase portability, Java originally envisioned relying software interpreter. instruction set interprete r called Java bytecodes (see Section 2.15 ), quite different RISC-V instruction set. get performance close equivalent C program, Java systems today typically compile Java bytecodes native instruction sets like RISC-V. compilation normally done much later C programs, Java compilers often called Time (JIT) compilers. Section 2.12 shows JITs used later C compilers start-up process, Section 2.13 shows performance consequences compiling versus interpreting Java programs. 2.3 Operands Computer Hardware Unlike programs high-level languages, operands arithmetic instructions restricted; must limi ted 172number special locations built directly hardware called registers . Registers primitives used hardware design also visible programmer computer completed, think registers bricks computer constructio n. size register RISC-V architecture 64 bits; groups f 64 bits occur frequently given name doubleword RISC-V architecture. (Another popular size group 32 bits, called word RISC-V architecture.) doubleword Another natural unit access computer, usually group 64 bits; corresponds size register RISC-V architect ure. word natural unit access computer, usually group 32 bits. One major difference variables programming language registers limited number registers, typic ally 32 current computers, like RISC-V. (See Section 2.21 history number registers.) Thus, continuing p- down, stepwise evolution symbolic representation th e RISC-V language, section added restriction three operands RISC-V arithmetic instructions must chosen one 32 64-bit registers. reason limit 32 registers may found second three underlying design principles hardware technology: Design Principle 2: Smaller faster. large number registers may increase clock cycle time simply takes electronic signals longer must travel farther. Guidelines “smaller faster” absolutes; 31 registers may faster 32. Even so, truth behind observations causes computer designers take seriously . case, designer must balance craving programs 173more registers designer’s desire keep clock c ycle fast. Another reason using 32 number bits would take instruction format, Section 2.5 demonstrates. Chapter 4 shows central role registers play hardware construction; shall see chapter, effective use registers critical program performance. Although could simply write instructions using numbers f registers, 0 31, RISC-V convention x followed number register, except register names wil l cover later. Compiling C Assignment Using Registers Example compiler’s job associate program variables registers. Take, instance, assignment statement earlier example: f = (g + h) −(i + j); variables f, g, h, i, j assigned registers x19, x20, x21, x22, x23, respectively. compiled RISC-V code? Answer compiled program similar prior example, except replace variables register names mentioned plus two temporary registers, x5 x6, correspond temporary variables above: add x5, x20, x21// register x5 contains g + h add x6, x22, x23// register x6 contains + j sub x19, x5, x6// f gets x5 –x6, (g + h) −(i + j) Memory Operands Programming languages simple variables contain single data elements, examples, also complex data structures—arrays structures. composite data structures contain many data elements registers computer. computer represent access 174such large structures? Recall five components computer introduced Chapter 1 repeated page 61. processor keep small amount data registers, computer memory contains billions data elements. Hence, data structures (arrays structures) kept memory. explained above, arithmetic operations occur register RISC-V instructions; thus, RISC-V must include instructions hat transfer data memory registers. instructions called data transfer instructions . access word doubleword memory, instruction must supply memory address . Memory large, single-dimensional array, address acting index array, starting 0. example, Figure 2.2 , address third data element 2, value memory [2] 10. data transfer instruction command moves data memory registers. address value used delineate location specific data element within memory array. 175FIGURE 2.2 Memory addresses contents memory locations. elements doublewords, addresses would incorrect, since RISC-V actually uses byte addressing, doubleword representing 8 bytes. Figure 2.3 shows correct memory addressing sequential doubleword addresses. data transfer instruction copies data memory register traditionally called load. format load instruction name operation followed regist er loaded, register constant used access memory. sum constant portion instruction contents second register forms memory address. real RISC-V name instruction ld, standing load doubleword . Compiling Assignment Operand Memory Example 176Let’s assume array 100 doublewords compiler associated variables g h registers x20 x21 before. Let’s also assume starting address, base address , array x22. Compile C assignment statement: g = h + A[8]; Answer Although single operation assignment statemen t, one operands memory, must first transfer A[8] register. address array element sum base array A, found register x22, plus number select element 8. data placed temporary register use next instruction. Based Figure 2.2 , first compiled instruction ld x9, 8(x22) // Temporary reg x9 gets A[8] (We’ll making slight adjustment instruction, we’ll use simplified version now.) following instructio n operate value x9 (which equals A[8] ) since register. instruction must add h (contained x21) A[8] (contained x9) put sum register corresponding g (associated x20): add x20, x21, x9 // g = h + A[8] register added form address ( x22) called base register , constant data transfer instruction (8) called offset . Hardware/Software Interface addition associating variables registers, compiler allocates data structures like arrays structures locations memory. compiler place proper starting address data transfer instructions. Since 8-bit bytes useful many programs, virtually architectures today address individual bytes. Therefore, address doubleword matches address one 8 bytes within doubleword, addresses sequential doublewords differ 8. example, Figure 2.3 shows actual RISC-V addresses doublewords Figure 2.2 ; byte address 177the third doubleword 16. FIGURE 2.3 Actual RISC-V memory addresses contents memory doublewords. changed addresses highlighted contrast Figure 2.2 . Since RISC-V addresses byte, doubleword addresses multiples 8: 8 bytes doubleword. Computers divide use address leftmost “big end” byte doubleword address versus use rightmost “little end” byte. RISC-V belongs latter camp, referred little-endian . Since order matters access identical data doubleword eight individual bytes, need aware “endianness.” Byte addressing also affects array index. get proper byte address code above, offset added base register x22 must 8 ×8, 64 , load address select A[8] A[8/8] . (See related Pitfall page 159 Section 2.19.) 178The instruction complementary load traditionally called store; copies data register memory. format store similar load: name operation, followed register stored, base register, finally fset select array element. again, RISC-V address specified part constant part contents register. actual RISC-V name sd, standing store doubleword . Elaboration many architectures, words must start addresses multiples 4 doublewords must start addresses multiples 8. requirement called alignment restriction . (Chapter 4 suggests alignment leads faster data transfers.) RISC-V Intel x86 alignment restrictions, MIPS does. alignment restriction requirement data aligned memory natural boundaries. Hardware/Software Interface addresses loads stores binary numbers, see DRAM main memory comes binary sizes rather decimal sizes. is, gibibytes (230) tebibytes (240), gigabytes (109) terabytes (1012); see Figure 1.1 . Compiling Using Load Store Example Assume variable h associated register x21 base address array x22. RISC-V assembly code C assignment statement below? A[12] = h + A[8]; Answer Although single operation C statement, two 179the operands memory, need even RISC-V instructions. first two instructions prio r example, except time use proper offset byte addressing load register instruction select A[8] , add instruction places sum x9: ld x9, 64(x22) // Temporary reg x9 gets A[8] add x9, x21, x9 // Temporary reg x9 gets h + A[8] final instruction stores sum A[12] , using 96 (8×12) offset register x22 base register. sdx9, 96(x22)// Stores h + A[8] back A[12] Load doubleword store doubleword instructions copy doublewords memory registers RISC-V architecture. brands computers use instructions along load store transfer data. architecture alternatives Intel x86, described Section 2.17 . Hardware/Software Interface Many programs variables computers registers. Consequently, compiler tries keep frequently used variables registers places rest memory, using loads stores move variables registers memory. process putting less frequently u sed variables (or needed later) memory called spilling registers. hardware principle relating size speed suggests memory must slower registers, since fewer registers. suggestion indeed case; data accesses faste r data registers instead memory. Moreover, data useful register. RISC-V arithmetic instruction read two registers, operate them, write result. RISC-V data transfer instruction reads e operand writes one operand, without operating it. Thus, registers take less time access higher throughput memory, making data registers considerably faster access simpler use. Accessing regis ters also uses much less energy accessing memory. achieve highest performance conserve energy, instruction set architecture must enough registers, compilers must use 180registers efficiently. Elaboration Let’s put energy performance registers versus memory perspective. Assuming 64-bit data, registers roughly 200 times faster (0.25 vs. 50 nanoseconds) 10,000 times energy efficient (0.1 vs. 1000 picoJoules) DRAM 2015. large differences led caches, reduce performance energy penalties going memory (see Chapter 5 ). Constant Immediate Operands Many times program use constant operation—for example, incrementing index point next element array. fact, half RISC-V arithmetic instructions constant operand running SPEC CPU2006 benchmarks. Using instructions seen far, would load constant memory use one. (The constants would placed memory program loaded.) example, add constant 4 register x22, could use code ld x9, AddrConstant4(x3) // x9 = constant 4 add x22, x22, x9 // x22 = x22 + x9 (where x9 == 4) assuming x3 + AddrConstant4 memory address constant 4. alternative avoids load instruction offer versions arithmetic instructions one operand constant. quick add instruction one constant operand called add immediate addi . add 4 register x22, write addi x22, x22, 4 // x22 = x22 + 4 Constant operands occur frequently; indeed, addi popular instruction RISC-V programs. including constants inside arithmetic instructions, operations much fast er use less energy constants loaded memory. constant zero another role, simplify instruction set offering useful variations. example, c negate value register using sub instruction zero 181for first operand. Hence, RISC-V dedicates register x0 hard-wired value zero. Using frequency justify inclusions constants another example great idea Chapter 1 making common case fast . Check Given importance registers, rate increase th e number registers chip time? 1. fast: increase fast Moore’s Law , predicts doubling number transistors chip every 18 months. 2. slow: Since programs usually distributed language computer, inertia instruction set architecture, number registers increases fast new instruction sets become viable. 182 Elaboration Although RISC-V registers book 64 bits wide, RISC-V architects conceived multiple variants ISA. addition variant, known RV64, variant named RV32 32-bit registers, whose reduced cost make RV32 better suited low-cost processors. Elaboration RISC-V offset plus base register addressing excellent match structures well arrays, since register point beginning structure offset select desir ed element. We’ll see example Section 2.13 . Elaboration register data transfer instructions originally invented hold index array offset used starting address array. Thus, base register also called index register . Today’s memories much larger, software model data allocation sophisticated, base address array normally passed register since won’t fit offset, shall see. 183 Elaboration migration 32-bit address computers 64-bit address computers left compiler writers choice size data types C. Clearly, pointers 64 bits, integers? Moreover, C data types int, long int , long long int . problems come converting one data type another unexpected overflow C code fully standard compliant, unfortunately rare code. table shows two popular options: compiler could different choices, generally compilers associated operating system make decision. keep examples simple, book we’ll assume pointers 64 bits declare C integers long long int keep size. also follow C99 standard declare variables used indexes arrays size_t , guarantees right size matter big array. typically declared long int . 2.4 Signed Unsigned Numbers First, let’s quickly review computer represents numbers . Humans taught think base 10, numbers may represented base. example, 123 base 10=1111011 base 2. Numbers kept computer hardware series high low electronic signals, considered base 2 numbers. (Just base 10 numbers called decimal numbers, base 2 numbers called binary numbers.) single digit binary number thus “atom” computing, since information composed binary digits bits. fundamental building block one two values, thought several alternatives: high low, 184off, true false, 1 0. binary digit Also called bit. One two numbers base 2, 0 1, components information. Generalizing point, number base, value ith digit starts 0 increases right left. representation leads obvious way number bits doubleword: simply use power base bit. subscript decimal numbers ten binary numbers two. example, 1011two represents (1 ×23) + (0 ×22) + (1 ×21) + (1 ×20)ten = (1 ×8) + (0 ×4)+ (1 ×2)+ (1 ×1)ten = 8+0+2+1ten = 11ten number bits 0, 1, 2, 3, … right left doubleword. drawing shows numbering bits within RISC-V doubleword placement number 1011two, (which must unfortunately split half fit page book): Since doublewords drawn vertically well horizontally, leftmost rightmost may unclear. Hence, phrase least significant bit used refer rightmost bit (bit 0 above) significant bit leftmost bit (bit 63). 185 least significant bit rightmost bit RISC-V doubleword. significant bit leftmost bit RISC-V doubleword. RISC-V doubleword 64 bits long, represent 264 different 64-bit patterns. natural let combinations represent numbers 0 264 −1 (18,446,774,073,709,551,615ten): 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000000two= 0ten 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000001two= 1ten 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000010two= 2ten . . . . . . 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111101two= 18,446,774,073,709,551,613ten 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111110two= 18,446,744,073,709,551,614ten 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111111two= 18,446,744,073,709,551,615ten is, 64-bit binary numbers represented terms bit value times power 2 (here xi means ith bit x): reasons shortly see, positive numbers calle unsigned numbers. Hardware/Software Interface Base 2 natural human beings; 10 fingers find base 10 natural. didn’t computers use decimal? fact, first commercial computer offer decimal arithmetic. problem computer still used signals, decimal digit simply represented several binary digits. Decimal proved inefficient subsequent computers reve rted 186to binary, converting base 10 relatively infreque nt input/output events. Keep mind binary bit patterns simply representatives numbers. Numbers really infinite number digits, almost 0 except rightmost digits. don’t normally show leading 0s. Hardware designed add, subtract, multiply, divide binary bit patterns. number proper result operations cannot represented rightmost hardwar e bits, overflow said occurred. It’s programming language, operating system, program determine overflow occurs. Computer programs calculate positive negative numbers, need representation distinguishes positive negative. obvious solution add separate sign, conveniently represented single bit ; name representation sign magnitude . Alas, sign magnitude representation several shortcomings. First, it’s obvious put sign bit. right? left? Early computers tried both. Second, adder sign magnitude may need extra step set sign can’t know advance proper sign be. Finally, separate sign bit means sign magnitude positive negative zero, lead problems inattentive programmers. shortcomings, sign magnitude representation soon abandoned. search attractive alternative, question arose would result unsigned numbers tried subtract large number small one. answer would try borrow string leading 0s, result would string leading 1s. Given obvious better alternative, final solution pick representation made hardware simple: leading 0s mean positive, leading 1s mean negative. convention representing signed binary numbers cal led two’s complement representation: 00000000 00000000 00000000 00000000 00000000 000000 00 00000000two= 0ten 00000000 00000000 00000000 00000000 00000000 000000 00 18700000001two= 1ten 00000000 00000000 00000000 00000000 00000000 000000 00 00000010two= 2ten . . . . . . 01111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111101two= 9,223,372,036,854,775,805ten 01111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111110two= 9,223,372,036,854,775,806ten 01111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111111two= 9,223,372,036,854,775,807ten 10000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000000two= −9,223,372,036,854,775,808ten 10000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000001two= −9,223,372,036,854,775,807ten 10000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000010two= −9,223,372,036,854,775,806ten … . . . 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111101two= −3ten 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111110two= −2ten 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111111two= −1ten positive half numbers, 0 9,223,372,036,854,775,807ten (263−1), use representation before. following bit pattern (1000 … 0000two) represents negative number −9,223,372,036,854,775,808ten (−263). followed declining set negative numbers: −9,223,372,036,854,775,807ten (1000 … 0001two) −1ten (1111 … 1111two). Two’s complement one negative number corresponding positive number: −9,223,372,036,854,775,808ten. imbalance also worry inattentive programmer, sign magnitude problems programmer hardware designer. Consequently, every computer today uses two’s complement binary representations signed numbers. Two’s complement representation advantage negative numbers 1 significant bit. Thus, hardware needs test bit see number positive r negative (with number 0 considered positive). bit ften 188called sign bit . recognizing role sign bit, represent positive negative 64-bit numbers terms bi value times power 2: sign bit multiplied −263, rest bits multiplied positive versions respective base valu es. Binary Decimal Conversion Example decimal value 64-bit two’s complement number? 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111100two Answer Substituting number’s bit values formula above: We’ll see shortcut simplify conversion negative positive soon. operation unsigned numbers overflow capacity hardware represent result, operation two’s complement numbers. Overflow occurs leftmost retained bit binary bit pattern infinit e number digits left (the sign bit incorrect): 0 left bit pattern number negative 1 number positive. Hardware/Software Interface Signed versus unsigned applies loads well arithmetic. 189The function signed load copy sign repeatedly fill rest register—called sign extension —but purpose place correct representation number within register. Unsigned loads simply fill 0s left data, since number represented bit pattern unsigned. loading 64-bit doubleword 64-bit register, point moot; signed unsigned loads identical. RISC-V offer two flavors byte loads: load byte unsigned (lbu) treats byte unsigned number thus zero-extends fill leftmost bits register, load byte (lb) works signed integers. Since C programs almost always use bytes represent characters rather consider bytes short signed integers , lbu used practically exclusively byte loads. Hardware/Software Interface Unlike signed numbers discussed above, memory addresses naturally start 0 continue largest address. Put another way, negative addresses make sense. Thus, programs want deal sometimes numbers positive negative sometimes numbers positive. programming languages reflect distinction. C, example, names former integers (declared long long int program) latter unsigned integers (unsigned long long int ). C style guides even recommend declaring former signed long long int keep distinction clear. Let’s examine two useful shortcuts working two’s complement numbers. first shortcut quick way negate two’s complement binary number. Simply invert every 0 1 every 1 0, add one result. shortcut based observation sum number inverted representation must 111 … 111two, represents −1. Since , therefore . (We use notation mean invert every bit x 0 1 vice versa.) Negation Shortcut Example 190Negate 2ten, check result negating −2ten. Answer 2ten=00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000010two Negating number inverting bits adding one, Going direction, 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111110two first inverted incremented: next shortcut tells us convert binary number represented n bits number represented n bits. shortcut take significant bit smal ler quantity—the sign bit—and replicate fill new bits larger quantity. old nonsign bits simply copied right portion new doubleword. shortcut commonl called sign extension . Sign Extension Shortcut Example Convert 16-bit binary versions 2ten −2ten 64-bit binary numbers. 191Answer 16-bit binary version number 2 00000000 00000010two= 2ten converted 64-bit number making 48 copies value significant bit (0) placing left doubleword. right part gets old value: 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00000010two= 2ten Let’s negate 16-bit version 2 using earlier shortcut. Thus, 0000 0000 0000 0010two becomes Creating 64-bit version negative number means copying sign bit 48 times placing left: 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111110two= −2ten trick works positive two’s complement numbers really infinite number 0s left negative two’s complement numbers infinite number 1s. binary bit pattern representing number hides leading bits fit width hardware; sign extension simply restores them. Summary main point section need represent positive negative integers within computer, although pros cons option, unanimous choice since 1921965 two’s complement. Elaboration signed decimal numbers, used “−” represent negative limits size decimal number. Given fixed data size, binary hexadecimal (see Figure 2.4 ) bit strings encode sign; therefore, normally use “+” “−” binary hexadecimal notation. FIGURE 2.4 hexadecimal–binary conversion table. replace one hexadecimal digit corresponding four binary digits, vice versa. length binary number multiple 4, g right left. Check decimal value 64-bit two’s complement number? 11111111 11111111 11111111 11111111 11111111 111111 11 11111111 11111000two 1) −4ten 2) −8ten 3) −16ten 4) 18,446,744,073,709,551,609ten Elaboration Two’s complement gets name rule unsigned sum n-bit number n-bit negative 2n; hence, negation complement number x 2n−x, “two’s complement.” 193A third alternative representation two’s complement sign magnitude called one’s complement . negative one’s complement found inverting bit, 0 1 1 0, . relation helps explain name since complement x 2n − x −1. also attempt better solution sign magnitude, several early scientific computers use notation. representation similar two’s complement except also two 0s: 00 … 00two positive 0 11 … 11two negative 0. negative number, 10 … 000two, represents −2,147,483,647ten, positives negatives balanced. One’s complement adders need extra step subtract number, hence two’s complement dominates today. one’s complement notation represents negative value 10 … 000two positive value 01 … 11two, leaving equal number negatives positives ending two zeros, one positive (00 … 00two) one negative (11 … 11two). term also used mean inversion every bit pattern: 0 1 1 0. final notation, look discuss floating point Chapter 3 , represent negative value 00 … 000two positive value 11 … 11two, 0 typically value 10 … 00two. representation called biased notation , since biases number number plus bias non-negative representation. biased notation notation represents negative value 00 … 000two positive value 11 … 11two, 0 typically value 10 … 00two, thereby biasing number number plus bias non-negative representation. 1942.5 Representing Instructions Computer ready explain difference way humans instruct computers way computers see instructions. Instructions kept computer series high low electronic signals may represented numbers. fact, piece instruction considered individual number, placing numbers side side forms instruction. 32 registers RISC-V referred number, 0 31. Translating RISC-V Assembly Instruction Machine Instruction Example Let’s next step refinement RISC-V language example. We’ll show real RISC-V language version instruction represented symbolically add x9, x20, x21 first combination decimal numbers binary numbers. Answer decimal representation segments instruction called field. first, fourth, sixth fields (containing 0, 0, 51 case) collectively tell RISC-V computer instruction p erforms addition. second field gives number register second source operand addition operation (21 x21), third field gives source operand addition (20 x20). fifth field contains number register 195is receive sum (9 x9). Thus, instruction adds register x20 register x21 places sum register x9. instruction also represented fields binary numbers instead decimal: layout instruction called instruction format . see counting number bits, RISC-V instruction takes exactly 32 bits—a word, one half doubleword. keeping design principle simplic ity favors regularity, RISC-V instructions 32 bits long. instruction format form representation instruction composed fields binary numbers. distinguish assembly language, call numeric version instructions machine language sequence instructions machine code . machine language Binary representation used communication within computer system. would appear would reading writing long, tiresome strings binary numbers. avoid tedium using higher base binary converts easily binary. Since almost computer data sizes multiples 4, hexadecimal (base 16) numbers popular. base 16 power 2, trivially convert replacing group four binary digits single hexadecimal digit, vice versa. Figure 2.4 converts hexadecimal binary. 196 hexadecimal Numbers base 16. frequently deal different number bases, avoid confusion, subscript decimal numbers ten, binary numbers two, hexadecimal numbers hex. (If subscript, default base 10.) way, C Java use notation 0x nnnn hexadecimal numbers. Binary Hexadecimal Back Example Convert following 8-digit hexadecimal 32-bit binary numbers base: eca8 6420hex 0001 0011 0101 0111 1001 1011 1101 1111two Answer Using Figure 2.4 , answer table lookup one way: direction: 197RISC-V Fields RISC-V fields given names make easier discuss: meaning name fields RISC-V instructions: opcode: Basic operation instruction, abbreviation traditional name. rd: register destination operand. gets result operation. funct3: additional opcode field. rs1: first register source operand. rs2: second register source operand. funct7: additional opcode field. opcode field denotes operation format instruction. problem occurs instruction needs longer fields shown above. example, load register instruction mus specify two registers constant. address use one 5-bit fields format above, largest constant within load register instruction would limited 25−1 31. constant used select elements arrays data structures, often needs much larger 31. 5-bit field small useful. Hence, conflict desire keep instructions length desire single instruction format. conflict leads us final hardware design principle: Design Principle 3: Good design demands good compromises. 198The compromise chosen RISC-V designers keep instructions length, thereby requiring distinct ins truction formats different kinds instructions. example, fo rmat called R-type (for register). second type instruction format I-type used arithmetic operands one constant operand, including addi , load instructions. fields I-type format 12-bit immediate interpreted two’s complement value, represent integers −211 211−1. I-type format used load instructions, immediate represents byte offset, load doubleword instruction refer doubleword within region ±211 2048 bytes (±28 256 doublewords) base address base register rd. see 32 registers would difficult format, rd rs1 fields would need another bit, making harder fit everything one word. Let’s look load register instruction page 71: ld x9, 64(x22) // Temporary reg x9 gets A[8] Here, 22 (for x22) placed rs1 field, 64 placed immediate field, 9 (for x9) placed rd field. also need format store doubleword instruction, sd, needs two source registers (for base address store data) immediate address offset. fields S-type format 12-bit immediate S-type format split two fields, supply lower 5 bits upper 7 bits. RISC-V architects chose design keeps rs1 rs2 field place instruction formats. Keeping instructio n formats similar possible reduces hardware complexity. 199Similarly, opcode funct3 fields size locations, always place. case wondering, formats distinguished values opcode field: format assigned distinct set opcode values first field (opcode) hardware know treat rest instruction. Figure 2.5 shows numbers used field RISC-V instructions covered far. Translating RISC-V Assembly Language Machine Language Example take example way programmer writes computer executes. x10 base array x21 corresponds h, assignment statement A[30] = h + A[30] + 1; compiled ld x9, 240(x10) // Temporary reg x9 gets A[30] add x9, x21, x9 // Temporary reg x9 gets h+A[30] addi x9, x9, 1 // Temporary reg x9 gets h+A[30]+1 sd x9, 240(x10) // Stores h+A[30]+1 back A[30] RISC-V machine language code three instructions? Answer convenience, let’s first represent machine language instructions using decimal numbers. Figure 2.5 , determine three machine language instructions: 200The ld instruction identified 3 (see Figure 2.5 ) opcode field 3 funct3 field. base register 10 specified rs1 field, destination register 9 specified rd field. offset select A[30] (240=30×8) found immediate field. add instruction follows specified 51 opcode field, 0 funct3 field, 0 funct7 field. three register operands (9, 21, 9) found rd, rs1, rs2 fields. subsequent addi instruction specified 19 opcode field 0 funct3 field. register operands (9 9) found rd rs1 fields, constant addend 1 found immediate field. sd instruction identified 35 opcode field 3 funct3 field. register operands (9 10) found rs2 rs1 fields, respectively. address offset 240 split across two immediate fields. Since upper part immediate holds bits 5 above, decompose offset 240 dividing 25. upper part immediate holds 201quotient, 7, lower part holds remainder, 16. Since 240ten = 0000 1111 0000two, binary equivalent decimal form is: Elaboration RISC-V assembly language programmers aren’t forced use addi working constants. programmer simply writes add, assembler generates proper opcode proper instruction format depending whether operands registers (R-type) one constant (I-type). use expl icit names RISC-V different opcodes formats think less confusing introducing assembly language versus machine language. Elaboration Although RISC-V add sub instructions, subi counterpart addi . immediate field represents two’s complement integer, addi used subtract constants. 202 Hardware/Software Interface desire keep instructions size conflicts desire many registers possible. increase number registers uses least one bit every registe r field instruction format. Given constraints design principle smaller faster, instruction sets day 16 32 general-purpose registers. FIGURE 2.5 RISC-V instruction encoding. table above, “reg” means register number 0 31 “address” means 12-bit address constant. funct3 funct7 fields act additional opcode fields. Figure 2.6 summarizes portions RISC-V machine language described section. shall see Chapter 4 , similarity binary representations related instructions simplif ies hardware design. similarities another example regularity RISC-V architecture. 203FIGURE 2.6 RISC-V architecture revealed Section 2.5 . three RISC-V instruction formats far R, I, S. R-type format two source register operand one destination register operand. I- type format replaces one source register operand 12-bit immediate field. S-type format two source operands 12-bit immediate field, destination register operand. S-type immediate field split two parts, bits 11—5 leftmost field bits 4—0 second-rightmost field. BIG Picture Today’s computers built two key principles: 1. Instructions represented numbers. 2. Programs stored memory read written, like data. principles lead stored-program concept; invention let computing genie bottle. Figure 2.7 shows power concept; specifically, memory contain source code editor program, corresponding compiled machine code, text compiled program using, even compiler generated machine code. 204FIGURE 2.7 stored-program concept. Stored programs allow computer performs accounting become, blink eye, computer helps author write book. switch happens simply loading memory programs data telling computer begin executing given location memory. Treating instructions way data greatly simplifies memory hardware software computer systems. Specifically, memory technology needed 205for data also used programs, programs like compilers, instance, translate code written notation far convenient humans code computer understand. One consequence instructions numbers programs often shipped files binary numbers. commercial implication computers inherit ready-made software provided compatible existing instruction set. uch “binary compatibility” often leads industry align around small number instruction set architectures. Check RISC-V instruction represent? Choose one four options below. 1. subx9,x10,x11 2. addx11,x9,x10 3. subx11,x10,x9 4. subx11,x9,x10 2.6 Logical Operations Although first computers operated full words, soon became clear useful operate fields bits within word even individual bits. Examining characters within word, stored 8 bits, one example operation (see Section 2.9 ). follows operations added programming languages instruction set architectures simplify, among things, packing unpacking bits words. instructions called logical operations . Figure 2.8 shows logical operations C, Java, RISC-V. 206“Contrariwise,” continued Tweedledee, “if so, might be; so, would be; isn’t, ain’t. That’s logic.” Lewis Carroll, Alice’s Adventures Wonderland, 1865 FIGURE 2.8 C Java logical operators corresponding RISC-V instructions. One way implement use XOR one operand ones (FFFF FFFF FFFF FFFFhex). first class operations called shifts . move bits doubleword left right, filling emptied bi ts 0s. example, register x19 contained 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 00001001two= 9ten instruction shift left 4 executed, new value would be: 00000000 00000000 00000000 00000000 00000000 000000 00 00000000 10010000two= 144ten dual shift left shift right. actual names two RISC-V shift instructions shift left logical immediate (slli ) shift right logical immediate (srli ). following instruction performs operation above, original value register x19 result go register x11: slli x11, x19, 4 // reg x11 = reg x19 << 4 bits shift instructions use I-type format. Since isn’t us eful shift 64-bit register 63 bits, lower 6 bits I-type format’s 12-bit immediate actually used. remaining 6 bits repurposed additional opcode field, funct6. 207The encoding slli 19 opcode field, rd contains 11, funct3 contains 1, rs1 contains 19, immediate contains 4, funct6 contains 0. Shift left logical provides bonus benefit. Shifting left bits gives identical result multiplying 2i, shifting decimal number digits equivalent multiplying 10 i. example, slli shifts 4, gives result multiplying 24 16. first bit pattern represents 9, 9×16=144, value second bit pattern. RISC-V provides third type shift, shift right arithmetic (srai ). variant similar srli , except rather filling vacated bits left zeros, fills copies old sign bit. also provide variants three shifts take shift amount register , rather immediate: sll, srl, sra. Another useful operation isolates fields . (We capitalize word avoid confusion operation English conjunction.) bit-by-bit operation leave 1 result bits operands 1. example, register x11 contains logical bit-by-bit operation two operands calculates 1 1 operands. 00000000 00000000 00000000 00000000 00000000 000000 00 00001101 11000000two register x10 contains 00000000 00000000 00000000 00000000 00000000 000000 00 00111100 00000000two then, executing RISC-V instruction x9, x10, x11 // reg x9 = reg x10 & reg x11 value register x9 would 00000000 00000000 00000000 00000000 00000000 000000 00 20800001100 00000000two see, apply bit pattern set bits force 0s 0 bit pattern. bit pattern conjunction traditionally called mask , since mask “conceals” bits. place value one seas 0s, dual AND, called OR. bit-by-bit operation places 1 result either operand bit 1. elaborate, registers x10 x11 unchanged preceding example, result RISC-V instruction logical bit-by-bit operation two operands calculates 1 1 either operand. x9, x10, x11 // reg x9 = reg x10 | reg x11 value register x9: 00000000 00000000 00000000 00000000 00000000 000000 00 00111101 11000000two final logical operation contrarian. takes one operand places 1 result one operand bit 0, vice versa. Using prior notation, calculates . logical bit-by-bit operation one operand inverts bits; is, replaces every 1 0, every 0 1. keeping three-operand format, designers RISC - V decided include instruction XOR (exclusive OR) instead NOT. Since exclusive creates 0 bits 1 different, equivalent xor 111…111. XOR logical bit-by-bit operation two operands calculates exclusive two operands. is, calculates 1 values different two operands. register x10 unchanged preceding example 209register x12 value 0, result RISC-V instruction xor x9, x10, x12 // reg x9 = reg x10 ^ reg x12 value register x9: 00000000 00000000 00000000 00000000 00000000 000000 00 00110001 11000000two Figure 2.8 shows relationship C Java operators RISC-V instructions. Constants useful logical operations well arithmetic operations, RISC-V also provides instructions immediate (andi ), immediate (ori), exclusive immediate (xori ). Elaboration C allows bit fields fields defined within doublewords, allowing objects packed within doubleword match externally enforced interface I/O device. fields must fit within single doubleword. Fields unsigned intege rs short 1 bit. C compilers insert extract fields using logical instructions RISC-V: andi , ori, slli , srli . Check operations isolate field doubleword? 1. 2. shift left followed shift right utility automatic computer lies possibility usin g given sequence instructions repeatedly, number times iterated dependent upon results computation.… choice made depend upon sign number (zero reckoned plus machine purposes). Consequently, introduce [instruction] (the conditional transfer [instruction]) w hich will, depending sign given number, cause proper one two routines executed. Burks, Goldstine, von Neumann, 1947 2.7 Instructions Making Decisions 210What distinguishes computer simple calculator abilit make decisions. Based input data values created computation, different instructions execute. Decision making commonly represented programming languages using statement, sometimes combined go statements labels. RISC-V assembly language includes two decision-making instructions, similar statement go . first instruction beq rs1, rs2, L1 instruction means go statement labeled L1 value register rs1 equals value register rs2. mnemonic beq stands branch equal . second instruction bne rs1, rs2, L1 means go statement labeled L1 value register rs1 equal value register rs2. mnemonic bne stands branch equal . two instructions traditionally called conditional branches . conditional branch instruction tests value allows subsequent transfer control new address program based outcome test. Compiling if-then-else Conditional Branches Example following code segment, f, g, h, i, j variables. five variables f j correspond five registers x19 x23, compiled RISC-V code C statement? (i == j) f = g + h; else f = g −h; Answer Figure 2.9 shows flowchart RISC-V code do. first expression compares equality two variables registers. would seem would want branch j equal ( beq). general, code efficient test 211for opposite condition branch code branches values equal ( bne). code: FIGURE 2.9 Illustration options statement above. left box corresponds part statement, right box corresponds else part. bne x22, x23, Else // go Else ≠ j next assignment statement performs single operation, operands allocated registers, one instruction: add x19, x20, x21 // f = g + h (skipped ≠ j) need go end statement. example introduces another kind branch, often called unconditional branch . instruction says processor always follows branch. One way express unconditional branch RISC-V use conditional branch whose condition always true: beq x0, x0, Exit // 0 == 0, go Exit assignment statement else portion statement compiled single instruction. need append label Else instruction. also show label Exit instruction, showing end if-then-else 212compiled code: Else:sub x19, x20, x21 // f = g −h (skipped = j) Exit: Notice assembler relieves compiler assembly language programmer tedium calculating addresses branches, calculating data addresses loads stores (see Section 2.12 ). Hardware/Software Interface Compilers frequently create branches labels appear programming language. Avoiding burden writing explicit labels branches one benefit writing n high-level programming languages reason coding faster level. Loops Decisions important choosing two alternativ es —found statements—and iterating computation—found loops. assembly instructions building blocks f cases. Compiling Loop C Example traditional loop C: (save[i] == k) += 1; Assume k correspond registers x22 x24 base array save x25. RISC-V assembly code corresponding C code? Answer first step load save[i] temporary register. load save[i] temporary register, need address. add base array save form address, must multiply index 8 due byte 213addressing issue. Fortunately, use shift left, since shifti ng left 3 bits multiplies 23 8 (see page 90 prior section). need add label Loop branch back instruction end loop: Loop: slli x10, x22, 3 // Temp reg x10 = * 8 get address save[i] , need add x10 base save x25: add x10, x10, x25 // x10 = address save[i] use address load save[i] temporary register: ld x9, 0(x10) // Temp reg x9 = save[i] next instruction performs loop test, exiting save[i] ≠ k: bne x9, x24, Exit // go Exit save[i] ≠ k following instruction adds 1 i: addi x22, x22, 1 // = + 1 end loop branches back test top loop. add Exit label it, we’re done: beq x0, x0, Loop // go Loop Exit: (See exercises optimization sequence.) Hardware/Software Interface sequences instructions end branch fundamental compiling given buzzword: basic block sequence instructions without branches, except possibly end, without branch targets branch labels, except possibly beginning. One first early phases compilation breaking program basic blocks. basic block sequence instructions without branches (except possibly end) without branch targets branch labels (except possibly beginning). test equality inequality probably popular test, many relationships two numbers . 214For example, loop may want test see index variable less 0. full set comparisons less ( <), less equal ( ≤), greater ( >), greater equal ( ≥), equal ( =), equal ( ≠). Comparison bit patterns must also deal dichotomy signed unsigned numbers. Sometimes bit pattern 1 significant bit represents negative number and, course, less positive number, must 0 significant bit. unsigned integers, hand, 1 significant bit represents number larger begins 0. (We’ll soon take advantage dual meaning significant bit reduce cost array bounds checking.) RISC-V provides instructions handle cases. instructions form beq bne, perform different comparisons. branch less (blt) instruction compares values registers rs1 rs2 takes branch value rs1 smaller, treated two’s complement numbers. Branch greater equal ( bge) takes branch opposite case, is, value rs1 least value rs2. Branch less than, unsigned ( bltu ) takes branch value rs1 smaller value rs2 values treated unsigned numbers. Finally, branch greater equal, unsigned ( bgeu ) takes branch opposite case. alternative providing additional branch instructions set register based upon result comparison, branch value temporary register beq bne instructions. approach, used MIPS instruction set, make processor datapath slightly simpler, takes instructions express program. Yet another alternative, used ARM’s instruction sets, keep extra bits record occurred instruction. additional bits, called condition codes flags , indicate, example, result arithmetic operation negative, zero, resulted overflow. Conditional branches use combinations condition codes perform desired test. One downside condition codes many instructions always set them, create dependencies make difficult pipelined execution (see Chapter 4 ). 215Bounds Check Shortcut Treating signed numbers unsigned gives us low- cost way checking 0 ≤ x<y, matches index out-of- bounds check arrays. key negative integers two’s complement notation look like large numbers unsigned notatio n; is, significant bit sign bit former notation large part number latter. Thus, unsigned comparison x<y checks x negative well x less y. Example Use shortcut reduce index-out-of-bounds check: branc h IndexOutOfBounds x20 ≥ x11 x20 negative. Answer checking code uses unsigned greater equal checks: bgeu x20, x11, IndexOutOfBounds // x20 >= x11 x20 < 0, goto IndexOutOfBounds Case/Switch Statement programming languages case switch statement allows programmer select one many alternatives depending single value. simplest way implement switch via sequence conditional tests, turning switch statement chain if-then-else statements. Sometimes alternatives may efficiently encoded table addresses alternative instruction sequences, called branch address table branch table , program needs index table branch appropriate sequence. branch table therefore array double- words containing addresses correspond labels code. program loads appropriate entry branch table register. needs branch using address registe r. support situations, computers like RISC-V include indirect jump instruction, performs unconditional branch address specified register. RISC-V, jump-and-lin k register instruction ( jalr ) serves purpose. We’ll see even 216more popular use versatile instruction next secti on. branch address table Also called branch table . table addresses alternative instruction sequences. Hardware/Software Interface Although many statements decisions loops programming languages like C Java, bedrock statement implements instruction set level condit ional branch. Check I. C many statements decisions loops, RISC-V few. following explain imbalance? Why? 1. decision statements make code easier read understand. 2. Fewer decision statements simplify task underlying layer responsible execution. 3. decision statements mean fewer lines code, generally reduces coding time. 4. decision statements mean fewer lines code, generally results execution fewer operations. II. C provide two sets operators (& &&) two sets operators (| ||), RISC-V doesn’t? 1. Logical operations ORR implement & |, conditional branches implement && ||. 2. previous statement backwards: && || correspond logical operations, & | map conditional branches. 3. redundant mean thing: && || simply inherited programming language B, predecessor C. 2172.8 Supporting Procedures Computer Hardware procedure function one tool programmers use structure programs, make easier understand allow code reused. Procedures allow programmer concentrat e one portion task time; parameters act interface procedure rest program data, since pass values return results. describe equivalent procedures Java Section 2.15 , Java needs everything computer C needs. Procedures one way implement abstraction software. procedure stored subroutine performs specific task based parameters provided. think procedure like spy leaves secret 218plan, acquires resources, performs task, covers track s, returns point origin desired result . Nothing else perturbed mission complet e. Moreover, spy operates “need know” basis, spy can’t make assumptions spymaster. Similarly, execution procedure, program must follow six steps: 1. Put parameters place procedure access them. 2. Transfer control procedure. 3. Acquire storage resources needed procedure. 4. Perform desired task. 5. Put result value place calling program access it. 6. Return control point origin, since procedure called several points program. mentioned above, registers fastest place hold data computer, want use much possible. RISC-V software follows following convention procedure calli ng allocating 32 registers: x10–x17 : eight parameter registers pass parameters return values. x1: one return address register return point origin. addition allocating registers, RISC-V assembly language includes instruction procedures: branc hes address simultaneously saves address following instruction destination register rd. jump-and- link instruction (jal) written jal x1, ProcedureAddress // jump ProcedureAdd ress write return address x1 jump-and-link instruction instruction branches address simultaneously saves address following instruction register (us ually x1 RISC-V). link portion name means address link formed points calling site allow procedure return proper address. “link,” stored register x1, called return address . return address needed 219the procedure could called several parts program. return address link calling site allows procedure return proper address; RISC-V stored register x1. support return procedure, computers like RISC-V use indirect jump, like jump-and-link instruction ( jalr ) introduced help case statements: jalr x0, 0(x1) jump-and-link register instruction branches address stored register x1—which want. Thus, calling program, caller , puts parameter values x10–x17 uses jal x1, X branch procedure X (sometimes named callee ). callee performs calculations, places results parameter registers, returns control caller using jalr x0, 0(x1) . caller program instigates procedure provides necessary parameter values. callee procedure executes series stored instructions based parameters provided caller returns control caller. Implicit stored-program idea need register hold address current instruction executed. F historical reasons, register almost always called program counter , abbreviated PC RISC-V architecture, although sensible name would instruction address register . jal instruction actually saves PC+4 designation register (usually x1) link byte address following instruction set procedure return. 220 program counter (PC) register containing address instruction program executed. Elaboration jump-and-link instruction also used perform unconditional branch within procedure using x0 destination register. Since x0 hard-wired zero, effect discard return address: jal x0, Label // unconditionally branch Label Using Registers Suppose compiler needs registers procedure eight argument registers. Since must cover tracks ou r mission complete, registers needed caller must restored values contained procedure invoked. situation example need spill registers memory, mentioned Hardware/Software Interface section page 69. ideal data structure spilling registers stack —a last-in- first-out queue. stack needs pointer recently allocated address stack show next procedure place registers spilled old register values found. RISC-V, stack pointer register x2, also known name sp. stack pointer adjusted one doubleword register saved restored. Stacks popular buzzwords transferring data stack: placing data onto stack called push , removing data stack called pop. stack data structure spilling registers organized last-in-first -out queue. stack pointer value denoting recently allocated address stack 221shows registers spilled old register values found. RISC-V, register sp, x2. push Add element stack. pop Remove element stack. historical precedent, stacks “grow” higher addresses lower addresses. convention means push values onto stack subtracting stack pointer. Adding stack pointer shrinks stack, thereby popping values stack. Compiling C Procedure Doesn’t Call Another Procedure Example Let’s turn example page 66 Section 2.2 C procedure: long long int leaf_example (long long int g, long l ong int h, long long int i, long long int j) { long long int f; f = (g + h) −(i + j); return f; } compiled RISC-V assembly code? Answer parameter variables g, h, i, j correspond argument registers x10, x11, x12, x13, f corresponds x20. compiled program starts label procedure: leaf_example: next step save registers used procedure. C assignment statement procedure body identical example page 67, uses two temporary registers ( x5 x6). Thus, need save three registers: x5, x6, x20. 222“push” old values onto stack creating space three doublewords (24 bytes) stack store them: addi sp, sp, -24 // adjust stack make room 3 items sdx5, 16(sp) // save register x5 use afterwards sdx6, 8(sp) // save register x6 use afterwards sdx20, 0(sp) // save register x20 use afterwards Figure 2.10 shows stack before, during, procedure call. FIGURE 2.10 values stack pointer stack (a) before, (b) during, (c) procedure call. stack pointer always points “top” stack, last doubleword stack drawing. next three statements correspond body procedure, follows example page 67: add x5, x10, x11// register x5 contains g + h add x6, x12, x13// register x6 contains + j sub x20, x5, x6// f = x5 − x6, (g + h) −(i + j) return value f, copy parameter register: addi x10, x20, 0 // returns f (x10 = x20 + 0) returning, restore three old values registe rs saved “popping” stack: ld x20, 0(sp) // restore register x20 caller ld x6, 8(sp) // restore register x6 caller ld x5, 16(sp) // restore register x5 caller addi sp, sp, 24 // adjust stack delete 3 items procedure ends branch register using return 223address: jalr x0, 0(x1) // branch back calling routine previous example, used temporary registers assumed old values must saved restored. avoid saving restoring register whose value never used, might happen temporary register, RISC-V software separates 19 registers two groups: x5−x7 x28−x31 : temporary registers preserved callee (called procedure) procedure call x8−x9 x18−x27 : saved registers must preserved procedure call (if used, callee saves restores them) simple convention reduces register spilling. exam ple above, since caller expect registers x5 x6 preserved across procedure call, drop two stores two loads code. still must save restore x20, since callee must assume caller needs value. Nested Procedures Procedures call others called leaf procedures. Life would simple procedures leaf procedures, aren’t. spy might employ spies part mission, turn might use even spies, procedures invoke procedures. Moreover, recursive procedures even invoke “clones” themselves. need careful using registers procedures, attention must paid invoking nonleaf procedures. example, suppose main program calls procedure argument 3, placing value 3 register x10 using jal x1, . suppose procedure calls procedure B via jal x1, B argument 7, also placed x10. Since hasn’t finished task yet, conflict use register x10. Similarly, conflict return address register x1, since return address B. Unless take steps prevent problem, conflict eliminate procedure A’s ability return caller. One solution push registers must preserved stack, saved registers. 224caller pushes argument registers ( x10–x17 ) temporary registers ( x5-x7 x28-x31 ) needed call. callee pushes return address register x1 saved registers (x8-x9 x18-x27 ) used callee. stack pointer sp adjusted account number registers placed stack . Upon return, registers restored memory, stack pointer readjusted. Compiling Recursive C Procedure, Showing Nested Procedure Linking Example Let’s tackle recursive procedure calculates factorial: long long int fact (long long int n) { (n < 1) return (1); else return (n * fact(n −1)); } RISC-V assembly code? Answer parameter variable n corresponds argument register x10. compiled program starts label procedure saves two registers stack, return address x10: fact: addi sp, sp, -16// adjust stack 2 items sd x1, 8(sp)// save return address sd x10, 0(sp)// save argument n first time fact called, sd saves address program called fact . next two instructions test whether n less 1, going L1 n ≥ 1. addi x5, x10, -1 // x5 = n - 1 bge x5, x0, L1 // (n - 1) >= 0, go L1 n less 1, fact returns 1 putting 1 value register: adds 1 0 places sum x10. pops two saved values stack branches return address: addi x10, x0, 1 // return 1 addi sp, sp, 16 // pop 2 items stack jalr // return caller 225Before popping two items stack, could loaded x1 x10. Since x1 x10 don’t change n less 1, skip instructions. n less 1, argument n decremented fact called decremented value: L1: addi x10, x10, -1 // n >= 1: argument gets (n −1) jalx1, fact // call fact (n −1) next instruction fact returns; result x10. old return address old argument restored, along stack pointer: addi x6, x10, 0 // return jal: move result fact(n - 1) x6: ld x10, 0(sp) // restore argument n ld x1, 8(sp) // restore return address addi sp, sp, 16 // adjust stack pointer pop 2 items Next, argument register x10 gets product old argument result fact(n - 1), x6. assume multiply instruction available, even though covered Chapter 3: mul x10, x10, x6 // return n * fact (n −1) Finally, fact branches return address: jalr x0, 0(x1) // return caller Hardware/Software Interface C variable generally location storage, interpretation depends type storage class . Example types include integers characters (see Section 2.9 ). C two storage classes: automatic static . Automatic variables local procedure discarded procedure exits. Static variables exist across exits entries procedures. C variables declared outside procedures considered static, variables declared using keyword static . rest automatic. simplify access static data, RISC-V compilers reserve register x3 use global pointer , gp. global pointer register reserved point static area. 226Figure 2.11 summarizes preserved across procedure call. Note several schemes preserve stack, guaranteeing caller get data back load stack stored onto stack. stack sp preserved simply making sure callee write sp; sp preserved callee adding exactly amount subtracted it; registers preserved savin g stack (if used) restoring there. FIGURE 2.11 preserved across procedure call. software relies global pointer register, discussed following subsections, also preserved. Allocating Space New Data Stack final complexity stack also used store variables local procedure fit registers, local arrays structures. segment stack containing procedure’s saved registers local variables called procedure frame activation record . Figure 2.12 shows state stack before, during, procedure call. procedure frame Also called activation record . segment stack containing procedure’s saved registers local variables. 227FIGURE 2.12 Illustration stack allocation (a) before, (b) during, (c) procedure cal l. frame pointer ( fp x8) points first doubleword frame, often saved argument register, stack pointer ( sp) points top stack. stack adjusted make room saved registers memory-resident local variables. Since stack pointer may change program execution, it’s easier programmers reference variables via stable frame pointer, although could done stack pointer little address arithmetic. local variables stack within procedure, compiler save time setting restoring frame pointer. frame pointer used, initialize using address sp call, sp restored using fp. information also found Column 4 RISC-V Reference Data Card front book. RISC-V compilers use frame pointer fp, register x8 point first doubleword frame procedure. stack pointer might change procedure, references local variable memory might different offsets depending n procedure, making procedure harder understand. Alternatively, frame pointer offers stable base register within procedure local memory-references. Note hat activation record appears stack whether explicit frame pointer used. We’ve avoiding using fp avoiding changes sp within procedure: examples, stack 228adjusted entry exit procedure. frame pointer value denoting location saved registers local variables given procedure. Allocating Space New Data Heap addition automatic variables local procedures, C programmers need space memory static variables dynamic data structures. Figure 2.13 shows RISC-V convention allocation memory running Linux operating system. stack starts high end user addresses space (see Chapter 5 ) grows down. first part low end memory reserved, followed home RISC-V machine code, traditionally called text segment . code static data segment , place constants static variables. Although arrays tend fixed length thus good match static data segment, data structures like linked lists tend grow shrink lifetimes. segmen data structures traditionally called heap, placed next memory. Note allocation allows stack heap grow toward other, thereby allowing efficient use memory two segments wax wane. 229 text segment segment UNIX object file contains machine language code routines source file. FIGURE 2.13 RISC-V memory allocation program data. addresses software convention, part RISC-V architecture. user address space set 238 potential 264 total address space given 64-bit architecture (see Chapter 5 ). stack pointer initialized 0000 003f ffff fff0hex grows toward data segment. end, program code (“text”) starts 0000 0000 0040 0000hex. static data starts immediately end text segment; example, assume address 0000 0000 1000 0000hex. Dynamic data, allocated malloc C new Java, next. grows toward stack area called heap . information also found Column 4 RISC - V Reference Data Card front book. C allocates frees space heap explicit functions. malloc() allocates space heap returns pointer it, free() releases space heap pointer points. C 230programs control memory allocation, source many common difficult bugs. Forgetting free space leads “memory leak,” ultimately uses much memory operating system may crash. Freeing space early leads “dangling pointers,” cause pointers point things program never intended. Java uses automatic memory allocation garbage collection avoid bugs. Figure 2.14 summarizes register conventions RISC-V assembly language. convention another example making common case fast : procedures satisfied eight argument registers, twelve saved registers, seven temporary registers without ever going memory. Elaboration eight parameters? RISC-V convention place extra parameters stack frame pointer. procedure expects first eight parameters registers x10 x17 rest memory, addressable via frame pointer. mentioned caption Figure 2.12 , frame pointer convenient references variables stack withi n procedure offset. frame pointer necessary, however. RISC-V C compiler uses frame pointer procedures change stack pointer body procedure. 231 Elaboration recursive procedures implemented iteratively wit hout using recursion. Iteration significantly improve performan ce removing overhead associated recursive procedure calls. example, consider procedure used accumulate sum: long long int sum (long long int n, long long int cc) { (n > 0) return sum(n − 1, acc + n); else return acc; } FIGURE 2.14 RISC-V register conventions. information also found Column 2 RISC - V Reference Data Card front book. Consider procedure call sum(3,0) . result recursive calls sum(2,3) , sum(1,5) , sum(0,6) , result 6 returned four times. recursive call sum referred tail call , example use tail recursion implemented efficiently (assume x10 = n, x11 = acc , result goes x12): sum: ble x10, x0, sum_exit // go sum_exit n <= 0 add x11, x11, x10 // add n acc addi x10, x10, -1 // subtract 1 n jal x0, sum // jump sum 232sum_exit: addi x12, x11, 0 // return value acc jalr x0, 0(x1) // return caller Check following statements C Java generally true? 1. C programmers manage data explicitly, it’s automatic Java. 2. C leads pointer bugs memory leak bugs Java. !(@ |=>(wow open tab bar great) Fourth line keyboard poem “Hatless Atlas,” 1991 ( give names ASCII characters: “!” “wow,” “(” open, “|” ba r, on). 2.9 Communicating People Computers invented crunch numbers, soon became commercially viable used process text. computers today offer 8-bit bytes represent characters, American Standard Code Information Interchange (ASCII) representation nearly everyone follows. Figure 2.15 summarizes ASCII. ASCII versus Binary Numbers Example could represent numbers strings ASCII digits instead integers. much storage increase number 1 billion represented ASCII versus 32-bit integer? Answer One billion 1,000,000,000, would take 10 ASCII digits, 8 bits long. Thus storage expansion would (10×8)/32 2.5. Beyond expansion storage, hardware add, subtract, multiply, divide decimal numbers difficult would 233consume energy. difficulties explain computing professionals raised believe binary natural occasional decimal computer bizarre. FIGURE 2.15 ASCII representation characters. Note upper- lowercase letters differ exactly 32; observation lead shortcuts checking changing upper- lowercase. Values shown include formatting characters. example, 8 represents backspace, 9 represents tab character, 13 carriage return. Another useful value 0 null, value programming language C uses mark end string. series instructions extract byte doubleword, load register store register sufficient transferring bytes well words. popularity text programs, however, RISC-V provides instructions move bytes . Load byte unsigned ( lbu) loads byte memory, placing rightmost 8 bits register. Store byte (sb) takes byte rightmost 8 bits register writes memory. Thus, copy byte sequence lbu x12, 0(x10) // Read byte source sbx12, 0(x11) // Write byte destination Characters normally combined strings, variable number characters. three choices representing string: (1) first position string res erved give length string, (2) accompanying variable 234length string (as structure), (3) last position string indicated character used mark end string. C uses third choice, terminating string byte whose value 0 (named null ASCII). Thus, string “Cal” represented C following 4 bytes, shown decimal numbers: 67, 97, 108, 0. (As shall see, Java uses first option.) Compiling String Copy Procedure, Showing Use C Strings Example procedure strcpy copies string string x using null byte termination convention C: void strcpy (char x[], char y[]) { size_t i; = 0; ((x[i] = y[i]) != ‘\0’) /* copy & test byte * / += 1; } RISC-V assembly code? Answer basic RISC-V assembly code segment. Assume base addresses arrays x found x10 x11, x19. strcpy adjusts stack pointer saves saved register x19 stack: strcpy: addi sp, sp, -8 // adjust stack 1 item sd x19, 0(sp) // save x19 initialize 0, next instruction sets x19 0 adding 0 0 placing sum x19: add x19, x0, x0// = 0+0 beginning loop. address y[i] first formed adding y[]: L1: add x5, x19, x11 // address y[i] x5 Note don’t multiply 8 since array bytes doublewords, prior examples. load character y[i] , use load byte unsigned, 235puts character x6: lbu x6, 0(x5) // x6 = y[i] similar address calculation puts address x[i] x7, character x6 stored address. add x7, x19, x10 // address x[i] x7 sb x6, 0(x7) // x[i] = y[i] Next, exit loop character 0. is, exit last character string: beq x6, x0, L2 not, increment loop back: addi x19, x19, 1 // = + 1 jal x0, L1 // go L1 don’t loop back, last character string; restore x19 stack pointer, return. L2: ld x19, 0(sp) // restore old x19 addi sp, sp, 8 // pop 1 doubleword stack jalr x0, 0(x1) // return String copies usually use pointers instead arrays C avoid operations code above. See Section 2.14 explanation arrays versus pointers. Since procedure strcpy leaf procedure, compiler could allocate temporary register avoid saving restoring x19. Hence, instead thinking registers temporaries, think registers th e callee use whenever convenient. compiler finds leaf procedure, exhausts temporary registers using registers must save. Characters Strings Java Unicode universal encoding alphabets human languages. Figure 2.16 gives list Unicode alphabets; almost many alphabets Unicode useful symbols ASCII. inclusive, Java uses Unicode characters. default, uses 16 bits represent character. 236FIGURE 2.16 Example alphabets Unicode. Unicode version 4.0 160 “blocks,” name collection symbols. block multiple 16. example, Greek starts 0370hex, Cyrillic 0400hex. first three columns show 48 blocks correspond human languages roughly Unicode numerical order. last column 16 blocks multilingual order. 1 6- bit encoding, called UTF-16, default. variable - length encoding, called UTF-8, keeps ASCII subset eight bits uses 16 32 bits characters. UTF-32 uses 32 bits per character. learn more, see www.unicode.org . RISC-V instruction set explicit instructions load store 16-bit quantities, called halfwords . Load half unsigned loads halfword memory, placing rightmost 16 bits register, filling leftmost 48 bits zeros. Like load byte, load half (lh) treats halfword signed number thus sign- extends fill 48 leftmost bits register. Store half (sh) takes halfword rightmost 16 bits register writes memory. copy halfword sequence lhu x19, 0(x10) // Read halfword (16 bits) sou rce shx19, 0(x11) // Write halfword (16 bits) dest Strings standard Java class special built-in support predefined methods concatenation, comparison, conversion. Unlike C, Java includes word gives length string, similar Java arrays. 237 Elaboration RISC-V software required keep stack aligned “quadword” (16 byte) addresses get better performance. convention means char variable allocated stack may occupy much 16 bytes, even though needs less. However, C string variable array bytes pack 16 bytes per quadword, Java string variable array shorts packs 8 halfwords per quadword. Elaboration Reflecting international nature web, web pages today use Unicode instead ASCII. Hence, Unicode may even popular ASCII today. Elaboration RISC-V also includes instructions move 32-bit values memory. Load word unsigned (lwu) loads 32-bit word memory rightmost 32 bits register, filling leftm ost 32 bits zeros. Load word (lw) instead fills leftmost 32 bits copies bit 31. Store word (sw) takes word rightmost 32 bits register stores memory. Check I. following statements characters strings C Java true? 1. string C takes half memory string Java. 2. Strings informal name single-dimension arrays characters C Java. 3. Strings C Java use null (0) mark end string. 4. Operations strings, like length, faster C Java. II. type variable contain 1,000,000,000ten takes memory space? 1. long long int C 2. string C 3. string Java 2382.10 RISC-V Addressing Wide Immediates Addresses Although keeping RISC-V instructions 32 bits long simplifie hardware, times would convenient 32- bit larger constants addresses. section starts general solution large constants, shows optimizations instruction addresses used branches. Wide Immediate Operands Although constants frequently short fit 12-bit fields, sometimes bigger. RISC-V instruction set includes instruction Load upper immediate (lui) load 20-bit constant bits 12 31 register. leftmost 32 bits filled copies bit 31, rightmost 12 bits filled zeros. instruction allows, fo r example, 32-bit constant created two instructions. lui uses new instruction format, U-type, formats cannot accommodate large constant. Loading 32-Bit Constant Example RISC-V assembly code load 64-bit constant register x19? 00000000 00000000 00000000 00000000 00000000 001111 01 00000101 00000000 Answer First, would load bits 12 31 bit pattern, 976 decimal, using lui: lui x19, 976 // 976decimal = 0000 0000 0011 1101 0000 value register x19 afterward is: 0000000 00000000 00000000 00000000 00000000 0011110 1 00000000 00000000 next step add lowest 12 bits, whose decimal value 1280: addi x19, x19, 1280// 1280decimal = 00000101 00000000 239The final value register x19 desired value: 00000000 00000000 00000000 00000000 00000000 001111 01 00000101 00000000 Elaboration previous example, bit 11 constant 0. bit 11 set, would additional complication: 12- bit immediate sign-extended, addend would negative. means addition adding rightmost 11 bits constant, would also subtracted 212. compensate error, suffices add 1 constant loaded lui, since lui constant scaled 212. Hardware/Software Interface Either compiler assembler must break large constants pieces reassemble register. might expect, immediate field’s size restriction may problem fo r memory addresses loads stores well constants immediate instructions. Hence, symbolic representation RISC-V machine language longer limited hardware, whatever creator assembler chooses include (see Section 2.12 ). stick close hardware explain architecture computer, noting use enhanced language assembler found processor. Addressing Branches RISC-V branch instructions use RISC-V instruction fo rmat called SB-type. format represent branch addresses −4096 4094, multiples 2. reasons revealed shortly, possible branch even addresses. SB-type format consists 7-bit opcode, 3-bit function code, two 5-bit register operands (rs1 rs2), 12-bit address immediate. address uses unusual encoding, simplifies datapath design complicates assembly. instruction bne x10, x11, 2000 // x10 != x11, go location 2000ten = 0111 1101 0000 could assembled format (it’s actually bit 240complicated, see): opcode conditional branches 1100111two bne’s funct3 code 001two. unconditional jump-and-link instruction ( jal) instruction uses UJ-type format. instruction consists 7-bit opcode, 5-bit destination register operand (rd), 20-bit address immediate. link address, address instruction following jal, written rd. Like SB-type format, UJ-type format’s address operand uses unusual immediate encoding, cannot encode odd addresses. So, jal x0, 2000 // go location 2000ten = 0111 1101 0000 assembled format: addresses program fit 20-bit field, would mean program could bigger 220, far small realistic option today. alternative would specify register would always added branch offset, branch instruction would calculate following: sum allows program large 264 still able use conditional branches, solving branch address size problem. question is, register? answer comes seeing conditional branches used. Conditional branches found loops statements, tend branch nearby instruction. example, half conditional branches SPEC benchmarks go locations less 16 instructions away. Since program counter (PC) 241contains address current instruction, branch within ±210 words current instruction, jump within ±218 words current instruction, use PC registe r added address. Almost loops statements smaller 210 words, PC ideal choice. form branch addressing called PC-relative addressing . PC-relative addressing addressing regime address sum program counter (PC) constant instruction. Like recent computers, RISC-V uses PC-relative addressing conditional branches unconditional jumps, destination instructions likely close branch. hand, procedure calls may require jumping 218 words away, since guarantee callee close caller. Hence, RISC-V allows long jumps 32-bit address two-instruction sequence: lui writes bits 12 31 address temporary register, jalr adds lower 12 bits address temporary register jumps sum. Since RISC-V instructions 4 bytes long, RISC-V branch instructions could designed stretch reach b PC-relative address refer number words branch target instruction, rather number bytes. However, RISC-V architects wanted support possibility instructions 2 bytes lon g, branch instructions represent number halfwords branch branch target. Thus, 20-bit address field jal instruction encode distance ±219 halfwords, ±1 MiB current PC. Similarly, 12-bit field conditional branch instructions also halfword address, meaning represents 13-bit byte address. Showing Branch Offset Machine Language Example 242The loop page 94 compiled RISC-V assembler code: Loop:slli x10, x22, 3 // Temp reg x10 = * 8 add x10, x10, x25 // x10 = address save[i] ld x9, 0(x10) // Temp reg x9 = save[i] bne x9, x24, Exit // go Exit save[i] != k addi x22, x22, 1 // = + 1 beq x0, x0, Loop // go Loop Exit: Answer assume place loop starting location 80000 memory, RISC-V machine code loop? assembled instructions addresses are: Remember RISC-V instructions byte addresses, addresses sequential words differ 4. bne instruction fourth line adds 3 words 12 bytes address instruction, specifying branch destination relative br anch instruction (12+80012) using full destination address (80024). branch instruction last line similar calculation backwards branch (−20+80020), corresponding label Loop . Hardware/Software Interface conditional branches nearby location, 243occasionally branch far away, farther represented 12-bit address conditional branch instruction. assembler comes rescue large addresses r constants: inserts unconditional branch branch target, inverts condition conditional branch decides whether skip unconditional branch. Branching Far Away Example Given branch register x10 equal zero, beq x10, x0, L1 replace pair instructions offers much greater branching distance. Answer instructions replace short-address conditional bran ch: bne x10, x0, L2 jal x0, L1 L2: RISC-V Addressing Mode Summary Multiple forms addressing generically called addressing modes . Figure 2.17 shows operands identified addressing mode. addressing modes RISC-V instructio ns following: 1. Immediate addressing , operand constant within instruction itself. 2. Register addressing , operand register. 3. Base displacement addressing , operand memory location whose address sum register constant instruction. 4. PC-relative addressing , branch address sum PC constant instruction. addressing mode One several addressing regimes delimited varied use 244operands and/or addresses. 245FIGURE 2.17 Illustration four RISC-V addressing modes. operands shaded color. operand mode 3 memory, whereas operand mode 2 register. Note versions load store access bytes, halfwords, words, doublewords. mode 1, operand part instruction itself. Mode 4 addresses instructions memory, mode 4 adding long address PC. Note single operation use one addressing mode. Add, example, uses immediate ( addi) register ( add) addressing. Decoding Machine Language Sometimes forced reverse-engineer machine language create original assembly language. One example looking “core dump.” Figure 2.18 shows RISC-V encoding opcodes RISC-V machine language. figure helps translating hand assembly language machine language. Decoding Machine Code 246Example assembly language statement corresponding machine instruction? 00578833hex Answer first step converting hexadecimal binary: 0000 0000 0101 0111 1000 1000 0011 0011 know interpret bits, need determine instruction format, first need determine opcode . opcode rightmost 7 bits, 0110011. Searching Figure 2.20 value, see opcode corresponds R-type arithmetic instructions. Thus, parse binary format fields listed Figure 2.21 : decode rest instruction looking field values. funct7 funct3 fields zero, indicating instruction add. decimal values register operands 5 rs2 field, 15 rs1, 16 rd. numbers represent registers x5, x15, x16. reveal assembly instruction: add x16, x15, x5 247FIGURE 2.18 RISC-V instruction encoding. 248All instructions opcode field, formats except U-type UJ-type use funct3 field. R-type instructions use funct7 field, immediate shifts (slli, srli, srai) use funct6 field. Figure 2.19 shows RISC-V instruction formats. Figure 2.1 pages 64–65 shows RISC-V assembly language revealed chapter. next chapter covers RISC-V instructions multiply, divide, arithmetic real numbers. Check I. range byte addresses conditional branches RISC-V (K =1024)? 1. Addresses 0 4K −1 2. Addresses 0 8K −1 3. Addresses 2K branch 2K 4. Addresses 4K branch 4K II. range byte addresses jump-and-link instruction RISC-V (M =1024K)? 1. Addresses 0 512K −1 2. Addresses 0 1M −1 3. Addresses 512K branch 512K 4. Addresses 1M branch 1M FIGURE 2.19 RISC-V instruction formats. 2.11 Parallelism Instructions: Synchronization Parallel execution easier tasks independent, often 249they need cooperate. Cooperation usually means tasks writing new values others must read. know task finished writing safe another read, tasks need synchronize. don’t synchronize, danger data race , results program change depending events happen occur. data race Two memory accesses form data race different threads location, least one write, occur one another. example, recall analogy eight reporters writing story pages 44–45 Chapter 1 . Suppose one reporter needs read prior sections writing conclusion. Hence, must know reporters finished sections, danger sections changed afterwards. is, better synchronize writing reading section conclusion consistent printed prior sections. computing, synchronization mechanisms typically built user-level software routines rely hardware-supplie 250synchronization instructions. section, focus implementation lock unlock synchronization operations. Lock unlock used straightforwardly create regions single processor operate, called mutual exclusion , well implement complex synchronization mechanisms. critical ability require implement synchronization multiprocessor set hardware primitives ability atomically read modify memory location. is, nothing else interpose read write memory location. Without capability, cost building basic synchronization primitives high increase unreasonably processor count increases. number alternative formulations basic hardware primitives, provide ability atomically read modify location, together way tell read write performed atomically. general, architects expect users employ basic hardware primitives, instead expect system programmers use primitives b uild synchronization library, process often complex tricky. Let’s start one hardware primitive show used build basic synchronization primitive. One typical operation building synchronization operations atomic exchange atomic swap , inter-changes value register value memory. see use build basic synchronization primitive, assume want build simple lock value 0 used indicate lock free 1 used indicate lock unavailable. processor tries set lock exchange 1, register, memory address corresponding lock. value returned exchange instruction 1 processor already claimed access , 0 otherwise. latter case, value also changed 1, preventing competing exchange another processor also retrieving 0. example, consider two processors try exchange simultane-ously: race prevented, since exactly one processors perform exchange first, returning 0, second processor return 1 exchange. 251key using exchange primitive implement synchronization operation atomic: exchange indivisible, two simultaneous exchanges ordered hardware. impossible two processors trying set synchronizati variable manner think simultaneously set variable. Implementing single atomic memory operation introduces challenges design processor, since require memory read write single, uninterruptible instruction. alternative pair instructions second instruction returns value showing whether pair instructions executed pair atomic. pair instructions effectively atomic appears operations executed processor occurred th e pair. Thus, instruction pair effectively atomic, processor change value pair instructions. RISC-V pair instructions includes special load called load-reserved doubleword (lr.d ) special store called store- conditional doubleword (sc.d ). instructions used sequence: contents memory location specified th e load-reserved changed store-conditional sam e address occurs, store-conditional fails writ e value memory. store-conditional defined sto value (presumably different) register memory change value another register 0 succeeds nonzero value fails. Thus, sc.d specifies three registers: one hold address, one indicate whether atomic operation failed succeeded, one hold value stored memory succeeded. Since load-reserved returns itial value, store-conditional returns 0 succeeds, following sequence implements atomic exchange memor location specified contents x20: again:lr.d x10, (x20) // load-reserved sc.d x11, x23, (x20) // store-conditional bnex11, x0, // branch store fails addi x23, x10, 0 // put loaded value x23 time processor intervenes modifies value memory lr.d sc.d instructions, sc.d writes 252nonzero value x11, causing code sequence try again. end sequence, contents x23 memory location specified x20 atomically exchanged. Elaboration Although presented multiprocessor synchronization , atomic exchange also useful operating system dealing multiple processes single processor. make sure nothing interferes single processor, store-conditio nal also fails processor context switch two instructions (see Chapter 5 ). Elaboration advantage load-reserved/store-conditional mechanism used build synchronization primitives, atomic compare swap atomic fetch-and-increment , used parallel programming models. involve instructions lr.d sc.d , many. Since store-conditional fail either another attempted store load reservation address exception, care must taken choosing instructions inserted two instructions. particular, integer arithme tic, forward branches, backward branches load- reserved/store-conditional block safely permitted; otherwise, possible create deadlock situations th e processor never complete sc.d repeated page faults. addition, number instructions load- reserved store-conditional small minimize th e probability either unrelated event competing process causes store-conditional fail frequently. Elaboration code implemented atomic exchange, following code would efficiently acquire lock locati register x20, value 0 means lock free 1 mean lock acquired: addi x12, x0, 1 // copy locked value 253again: lr.d x10, (x20) // load-reserved read lock bnex10, x0, // check 0 yet sc.d x11, x12, (x20) // attempt store new value bnex11, x0, // branch store fails release lock using regular store write 0 location: sd x0, 0(x20) // free lock writing 0 Check use primitives like load-reserved store- conditional? 1. cooperating threads parallel program need synchronize get proper behavior reading writing shared data. 2. cooperating processes uniprocessor need synchronize reading writing shared data. 2.12 Translating Starting Program section describes four steps transforming C progr file storage (disk flash memory) program running computer. Figure 2.20 shows translation hierarchy. systems combine steps reduce translation time, programs go four logical phases. section follo ws translation hierarchy. 254FIGURE 2.20 translation hierarchy C. high-level language program first compiled assembly language program assembled object module machine language. linker combines multiple modules library routines resolve references. loader places machine code proper memory locations execution processor. speed translation process, steps skipped combined. compilers produce object modules directly, systems use linking loaders perform last two steps. identify type file , UNIX follows suffix convention files: C source files named x.c, assembly files x.s, object files named x.o, statically linked library routines x.a, dynamically linked library routes x.so, executable files default called a.out . MS-DOS uses suffixes .C, .ASM, .OBJ, .LIB, .DLL, .EXE effect. Compiler 255The compiler transforms C program assembly language program , symbolic form machine understands. High- level language programs take many fewer lines code assembly language, programmer productivity much higher. 1975, many operating systems assemblers written assembly language memories small compilers inefficient. million-fold increase memory capacity p er single DRAM chip reduced program size concerns, optimizing compilers today produce assembly language programs nearly well assembly language expert, sometimes even better large programs. assembly language symbolic language translated binary machine language. Assembler Since assembly language interface higher-level software, th e assembler also treat common variations machine language instructions instructions right. hardware need implement instructions; however, appearance assembly language simplifies translation programming. instructions called pseudoinstructions . pseudoinstruction common variation assembly language instructions often treated instruction right. mentioned above, RISC-V hardware makes sure register x0 always value 0. is, whenever register x0 used, supplies 0, programmer attempts change value x0, new value simply discarded. Register x0 used create assembly language instruction copies conte nts one register another. Thus, RISC-V assembler accepts following instruction even though found RISC-V machine language: li x9, 123 // load immediate value 123 register x9 256The assembler converts assembly language instruction machine language equivalent following instruction: addi x9, x0, 123 // register x9 gets register x0 + 123 RISC-V assembler also converts mv (move) addi instruction. Thus mv x10, x11 // register x10 gets register x11 becomes addi x10, x11, 0 // register x10 gets register x11 + 0 assembler also accepts j Label unconditionally branch label, stand-in jal x0, Label . also converts branches faraway locations branch jump. mentioned above, RISC-V assembler allows large constants loaded register despite limited size immediate instructio ns. Thus, load immediate (li) pseudoinstruction introduced create constants larger addi ’s immediate field contain; load address (la) macro works similarly symbolic addresses. Finally, simplify instruction set determining whic h variation instruction programmer wants. example, RISC-V assembler require programmer specify immediate version instruction using constant arithmetic logical instructions; generates proper opcode. Thus x9, x10, 15 // register x9 gets x10 15 becomes andi x9, x10, 15 // register x9 gets x10 15 include “ i” instructions remind reader andi produces different opcode different instruction format instruction immediate operands. summary, pseudoinstructions give RISC-V richer set assembly language instructions implemented hardware. going write assembly programs, use pseudoinstructions simplify task. understand RIS C- V architecture sure get best performance, however, study real RISC-V instructions found Figures 2.1 2.18. Assemblers also accept numbers variety bases. addition binary decimal, usually accept base succinct binary yet converts easily bit pattern. RISC- V assemblers use hexadecimal octal. features convenient, primary task 257assembler assembly machine code. assembler turns assembly language program object file , combination machine language instructions, data, information needed place instructions properly memory. produce binary version instruction assembl language program, assembler must determine addresses corresponding labels. Assemblers keep track labels used branches data transfer instructions symbol table . might expect, table contains pairs symbols addresses. symbol table table matches names labels addresses memory words instructions occupy. object file UNIX systems typically contains six distinct pieces: object file header describes size position pieces object file. text segment contains machine language code. static data segment contains data allocated life program. (UNIX allows programs use static data , allocated throughout program, dynamic data , grow shrink needed program. See Figure 2.13 .) relocation information identifies instructions data words depend absolute addresses program loaded memory. symbol table contains remaining labels defined, external references. debugging information contains concise description modules compiled debugger associate machine instructions C source files make data structures readable. next subsection shows attach routines already assembled, library routines. Linker presented far suggests single change one line one procedure requires compiling assembling wh ole 258program. Complete retranslation terrible waste computing resources. repetition particularly wasteful standard library routines, programmers would compiling assembling routines definition almost never change. alternative compile assemble procedure independently, change one line would require compiling assembling one procedure. alternative requires new systems program, called link editor linker , takes independently assembled machine language programs “stitches” together. reason linker useful much faster patch code recompile reassemble. linker Also called link editor . systems program combines independently assembled machine language programs resolves undefined labels executable file. three steps linker: 1. Place code data modules symbolically memory. 2. Determine addresses data instruction labels. 3. Patch internal external references. linker uses relocation information symbol table object module resolve undefined labels. referenc es occur branch instructions data addresses, job program much like editor: finds old addresses replaces new addresses. Editing origin name “link editor,” linker short. external references resolved, linker next determine memory locations module occupy. Recall Figure 2.13 page 106 shows RISC-V convention allocation program data memory. Since files assembled isolation, assembler could know module’s instructions data would placed relative modules. linker places module memory, absolute references, is, memory addresses relative register, must relocated reflect true location. linker produces executable file run 259computer. Typically, file format object file, except contains unresolved references. possibl e partially linked files, library routines, still unresolved addresses hence result object files. executable file functional program format object file contains unresolved references. contain symbol tables debugging information. “stripped executable” contain information. Relocation information may included loader. Linking Object Files Example Link two object files below. Show updated addresses fir st instructions completed executable file. show instructions assembly language make example understandable; reality, instructions would numbers. Note object files highlighted addresses symbols must updated link process: instructions refer addresses procedures B instructions refer addresses data doublewords X Y. 260261Answer Procedure needs find address variable labeled X put load instruction find address procedure B place jal instruction. Procedure B needs address variable labeled store instruction address procedure jal instruction. Figure 2.14 page 107, know text segment starts address 0000 0000 0040 0000hex data segment 0000 0000 1000 0000hex. text procedure placed first address data second. object file header procedure says text 100hex bytes data 20hex bytes, starting address procedure B text 40 0100hex, data starts 1000 0020hex. Executable file header Text size 300hex Data size 50hex Text segment Address Instruction 0000 0000 0040 0000hexld x10, 0(x3) 0000 0000 0040 0004hexjal x1, 252ten . . . . . . 0000 0000 0040 0100hexsd x11, 32(x3) 0000 0000 0040 0104hexjal x1, -260ten . . . . . . Data segment Address 0000 0000 1000 0000hex(X) . . . . . . 0000 0000 1000 0020hex(Y) . . . . . . linker updates address fields instructions. uses instruction type field know format address edited. three types here: 1. jump link instructions use PC-relative addressing. Thus, jal address 40 0004hex go 40 0100hex (the address procedure B), must put ( 40 0100hex – 40 0004hex) 252ten address field. Similarly, since 40 0000hex address procedure A, jal 40 0104hex gets negative number -260ten (40 0000hex – 40 0104hex) address field. 2. load addresses harder relative base 262register. example uses x3 base register, assuming initialized 0000 0000 1000 0000hex. get address 0000 0000 1000 0000hex (the address doubleword X), place 0ten address field ld address 40 0000hex. Similarly, place 20hex address field sd address 40 0100hex get address 0000 0000 1000 0020hex (the address doubleword Y). 3. Store addresses handled like load addresses, except S-type instruction format represents immediates differ ently loads’ I-type format. place 32ten address field sd address 40 0100hex get address 0000 0000 1000 0020hex (the address doubleword Y). Loader executable file disk, operating system reads memory starts it. loader follows steps UNIX systems: loader systems program places object program main memory ready execute. 1. Reads executable file header determine size text data segments. 2. Creates address space large enough text data. 3. Copies instructions data executable file memory. 4. Copies parameters (if any) main program onto stack. 5. Initializes processor registers sets stack pointer first free location. 6. Branches start-up routine copies parameters argument registers calls main routine program. main routine returns, start-up routine terminates th e program exit system call. Virtually every problem computer science solved another 263level indirection. David Wheeler Dynamically Linked Libraries first part section describes traditional approach linking libraries program run. Although static approach fastest way call library routines, disadvantages: library routines become part executable code. new version library released fixes bugs supports ne w hardware devices, statically linked program keeps using old version. loads routines library called anywhere executable, even calls executed. library large relative program; example, standard C library RISC-V system running Linux operating system 1.5 MiB. disadvantages lead dynamically linked libraries (DLLs) , library routines linked loaded program run. program library routines keep extra information location nonlocal procedures names. original version DLLs, loader ran dynamic linker, using extra information file find appropr iate libraries update external references. dynamically linked libraries (DLLs) Library routines linked program execution. downside initial version DLLs still linke routines library might called, versus called running program. observation led lazy procedure linkage version DLLs, routine linked called. Like many innovations field, trick relies level indirection. Figure 2.21 shows technique. starts nonlocal routines calling set dummy routines end program, one entry per nonlocal routine. dummy 264entries contain indirect branch. 265FIGURE 2.21 Dynamically linked library via lazy procedure linkage. (a) Steps first time call made DLL routine. (b) steps find routine, remap it, link skipped subsequent calls. see Chapter 5 , operating system may avoid copying desired routine remapping using virtual memory management. first time library routine called, program calls 266dummy entry follows indirect branch. points code th puts number register identify desired library routi ne branches dynamic linker/loader. linker/loader finds wanted routine, remaps it, changes address indirect branch location point routine. branches it. routine completes, returns original calling ite. Thereafter, call library routine branches indirectly routine without extra hops. summary, DLLs require additional space information needed dynamic linking, require whole librarie copied linked. pay good deal overhead first time routine called, single indirect branch thereafter. e return library pays extra overhead. Microsoft’ Windows relies extensively dynamically linked libraries, also default executing programs UNIX systems today. Starting Java Program discussion captures traditional model executin g program, emphasis fast execution time program targeted specific instruction set architecture, ev en particular implementation architecture. Indeed, poss ible execute Java programs like C. Java invented different set goals, however. One run safely computer, even might slow execution time. Figure 2.22 shows typical translation execution steps Java. Rather compile assembly language target computer, Java compiled first instructions easy interpret: Java bytecode instruction set (see Section 2.15 ). instruction set designed close Java language compilation step trivial. Virtually optimizations ar e performed. Like C compiler, Java compiler checks type data produces proper operation type. Java programs distributed binary version bytecod es. Java bytecode Instruction instruction set designed interpret Java 267programs. FIGURE 2.22 translation hierarchy Java. Java program first compiled binary version Java bytecodes, addresses defined compiler. Java program ready run interpreter, called Java Virtual Machine (JVM). JVM links desired methods Java library program running. achieve greater performance, JVM invoke JIT compiler, selectively compiles methods native machine language machine running. software interpreter, called Java Virtual Machine (JVM) , execute Java bytecodes. interpreter program simulates instruction set architecture. example, RISC-V simulator used book interpreter. need separat e assembly step since either translation simple compiler fills addresses JVM finds runtime. Java Virtual Machine ( JVM) program interprets Java bytecodes. upside interpretation portability. availability software Java virtual machines meant people could write run Java programs shortly Java announced. Today, Java virtual machines found billions devices, everythi ng 268from cell phones Internet browsers. downside interpretation lower performance. incredible advances performance 1980s 1990s made interpretation viable many important applications, fact 10 slowdown compared traditionally compiled C programs made Java unattractive applications. preserve portability improve execution speed, next phase Java’s development compilers translated program running. Time compilers (JIT) typically profile running program find “hot” methods compile native instruction set virtual machine running. compiled portion saved next time program run, run faster time run. balance interpretation compilation evolves time, frequently run Java programs suffer little overhead interpretation. Time compiler ( JIT) name commonly given compiler operates runtime, translating interpreted code segments native code computer. computers get faster compilers more, researchers invent betters ways compile Java fly, performance gap Java C C++ closing. Section 2.15 goes much greater depth implementation Java, Java bytecodes, JVM, JIT compilers. Check advantages interpreter translator important designers Java? 1. Ease writing interpreter 2. Better error messages 3. Smaller object code 4. Machine independence 2692.13 C Sort Example Put Together One danger showing assembly language code snippets idea full assembly language program looks like. section, derive RISC-V code two procedures written C: one swap array elements one sort them. Procedure swap Let’s start code procedure swap Figure 2.23 . procedure simply swaps two locations memory. translating C assembly language hand, follow general steps: 1. Allocate registers program variables. 2. Produce code body procedure. 3. Preserve registers across procedure invocation. FIGURE 2.23 C procedure swaps two locations memory. subsection uses procedure sorting example. section describes swap procedure three pieces, concluding putting pieces together. Register Allocation swap mentioned page 98, RISC-V convention parameter 270passing use registers x10 x17. Since swap two parameters, v k, found registers x10 x11. variable temp , associate register x5 since swap leaf procedure (see page 102). register allocation corresponds variable declarations first part wap procedure Figure 2.23 . Code Body Procedure swap remaining lines C code swap temp= v[k]; v[k]= v[k+1]; v[k+1] = temp; Recall memory address RISC-V refers byte address, doublewords really 8 bytes apart. Hence, need multiply index k 8 adding address. Forgetting sequential doubleword addresses differ 8 instead b 1 common mistake assembly language programming . Hence, first step get address v[k] multiplying k 8 via shift left 3: slli x6, x11, 3 // reg x6 = k * 8 add x6, x10, x6 // reg x6 = v + (k * 8) load v[k] using x6, v[k+1] adding 8 x6: ld x5, 0(x6) // reg x5 (temp) = v[k] ld x7, 8(x6) // reg x7 = v[k + 1] // refers next element v Next store x9 x11 swapped addresses: sd x7, 0(x6) // v[k] = reg x7 sd x5, 8(x6) // v[k+1] = reg x5 (temp) allocated registers written code perform operations procedure. missing code preserving saved registers used within swap . Since using saved registers leaf procedure, nothing preserve. Full swap Procedure ready whole routine. remains add procedure label return branch. swap: sllix6, x11, 3 // reg x6 = k * 8 271 addx6, x10, x6 // reg x6 = v + (k * 8) ldx5, 0(x6) // reg x5 (temp) = v[k] ldx7, 8(x6) // reg x7 = v[k + 1] sdx7, 0(x6) // v[k] = reg x7 sdx5, 8(x6) // v[k+1] = reg x5 (temp) jalrx0, 0(x1) // return calling routine Procedure sort ensure appreciate rigor programming assembly language, we’ll try second, longer example. case, we’ll build routine calls swap procedure. program sorts array integers, using bubble exchange sort, one simplest fastest sorts. Figure 2.24 shows C version program. again, present procedure several steps, concluding full procedure. 272FIGURE 2.24 C procedure performs sort array v. Register Allocation sort two parameters procedure sort , v n, parameter registers x10 x11, assign register x19 register x20 j. Code Body Procedure sort procedure body consists two nested loops call swap includes parameters. Let’s unwrap code outside middle. first translation step first loop: (i = 0; < n; += 1) { Recall C statement three parts: initialization, loop test, iteration increment. takes one instruction ini tialize 0, first part statement: li x19, 0 (Remember li pseudoinstruction provided assembler convenience assembly language programmer; see page 125.) also takes one instruction increment i, last part statement: addi x19, x19, 1 // += 1 loop exited < n true or, said another way, exited ≥ n . test takes one instruction: for1tst: bge x19, x11, exit1// go exit1 x19 ≥ x1 (i≥n) bottom loop branches back loop test: j for1tst // branch test outer loop exit1: 273The skeleton code first loop li x19, 0 // = 0 for1tst: bge x19, x11, exit1 // go exit1 x19 ≥ x1 (i ≥n) … (body first loop) … addi x19, x19, 1 // += 1 j for1tst // branch test outer loop exit1: Voila! (The exercises explore writing faster code similar loops.) second loop looks like C: (j = – 1; j >= 0 && v[j] > v[j + 1]; j -= 1) { initialization portion loop one instruction: addi x20, x19, -1 // j = – 1 decrement j end loop also one instruction: addi x20, x20, -1 j -= 1 loop test two parts. exit loop either condition fails, first test must exit loop fails ( j < 0): for2tst: blt x20, x0, exit2 // go exit2 x20 < 0 (j < 0) branch skip second condition test. n’t skip, j ≥ 0 . second test exits v[j] > v[j + 1] true, exits v[j] ≤ v[j + 1] . First create address multiplying j 8 (since need byte address) add base address v: slli x5, x20, 3 // reg x5 = j * 8 add x5, x10, x5 // reg x5 = v + (j * 8) load v[j] : ld x6, 0(x5) // reg x6 = v[j] Since know second element following doubleword, add 8 address register x5 get v[j + 1] : ld x7, 8(x5) // reg x7 = v[j + 1] test v[j] ≤ v[j + 1] exit loop: ble x6, x7, exit2 // go exit2 x6 ≤ x7 bottom loop branches back inner loop test: jfor2tst // branch test inner loop Combining pieces, skeleton second loop looks 274like this: addi x20, x19, -1 // j = - 1 for2tst: blt x20, x0, exit2 // go exit2 x20 < 0 (j < 0) sllix5, x20, 3 // reg x5 = j * 8 addx5, x10, x5 // reg x5 = v + (j * 8) ldx6, 0(x5) // reg x6 = v[j] ldx7, 8(x5) // reg x7 = v[j + 1] ble x6, x7, exit2 // go exit2 x6 ≤ x7 . . . (body second loop) . . . addi x20, x20, -1 // j -= 1 j for2tst // branch test inner loop exit2: Procedure Call sort next step body second loop: swap(v,j); Calling swap easy enough: jal x1, swap Passing Parameters sort problem comes want pass parameters sort procedure needs values registers x10 x11, yet swap procedure needs parameters placed registers. One solution copy parameters sort registers earlier procedure, making registers x10 x11 available call swap . (This copy faster saving restoring stack.) first copy x10 x11 x21 x22 procedure: mv x21, x10 // copy parameter x10 x21 mv x22, x11 // copy parameter x11 x22 pass parameters swap two instructions: mv x10, x21 // first swap parameter v mv x11, x20 // second swap parameter j Preserving Registers sort 275The remaining code saving restoring registers. Clearly, must save return address register x1, since sort procedure called. sort procedure also uses callee-saved registers x19, x20, x21, x22, must saved. prologue sort procedure addi sp, sp, -40 // make room stack 5 regs sd x1, 32(sp) // save x1 stack sd x22, 24(sp) // save x22 stack sd x21, 16(sp) // save x21 stack sd x20, 8(sp) // save x20 stack sd x19, 0(sp) // save x19 stack tail procedure simply reverses instruction s, adds jalr return. Full Procedure sort put pieces together Figure 2.25 , careful replace references registers x10 x11 loops references registers x21 x22. again, make code easier follow, identify block code purpose procedure. example, nine lines sort procedure C became 34 lines RISC-V assembly language. 276FIGURE 2.25 RISC-V assembly version procedure sort Figure 2.27 . Elaboration One optimization works example procedure inlining . Instead passing arguments parameters invoking code jal instruction, compiler would copy code body swap procedure call swap appears code. Inlining would avoid four instructions examp le. downside inlining optimization compiled code would bigger inlined procedure called several locations. code expansion might turn lower performance increased cache miss rate; see Chapter 5 . 277 Understanding Program Performance Figure 2.26 shows impact compiler optimization sort program performance, compile time, clock cycles, instruction count, CPI. Note unoptimized code best CPI, O1 optimization lowest instruction count, O3 fastest, reminding us time accurate measure program performance. FIGURE 2.26 Comparing performance, instruction count, CPI using compiler optimization Bubble Sort. programs sorted 100,000 32-bit words array initialized random values. programs run Pentium 4 clock rate 3.06 GHz 533 MHz system bus 2 GB PC2100 DDR SDRAM. used Linux version 2.4.20. Figure 2.27 compares impact programming languages, compilation versus interpretation, algorithms performance sorts. fourth column shows unoptimized C program 8.3 times faster interpreted Java code Bubble Sort. Using JIT compiler makes Java 2.1 times faster unoptimized C within factor 1.13 highest optimized C code. ( Section 2.15 gives details interpretation versus compilation Java Java jalr code Bubble Sort.) ratios aren’t close Quicksort Column 5, presumably harder amortize cost runtime compilation shorter execution time. last column demonstrates impact better algorithm, offering three orde rs magnitude performance increase sorting 100,000 items. Even comparing interpreted Java Column 5 C compiler highest optimization Column 4, Quicksort beats 278Bubble Sort factor 50 (0.05×2468, 123 times faster unoptimized C code versus 2.41 times faster). FIGURE 2.27 Performance two sort algorithms C Java using interpretation optimizing compilers relative unoptimized C version. last column shows advantage performance Quicksort Bubble Sort language execution option. programs run system Figure 2.29 . JVM Sun version 1.3.1, JIT Sun Hotspot version 1.3.1. 2.14 Arrays versus Pointers challenge new C programmer understanding pointers. Comparing assembly code uses arrays array indices assembly code uses pointers offers insights pointers. section shows C RISC-V assembly versions two procedures clear sequence doublewords memory: one using array indices one pointers. Figure 2.28 shows two C procedures. 279FIGURE 2.28 Two C procedures setting array zeros. clear1 uses indices, clear2 uses pointers. second procedure needs explanation unfamiliar C. address variable indicated &, object pointed pointer indicated *. declarations declare array p pointers integers. first part loop clear2 assigns address first element array pointer p. second part loop tests see pointer pointing beyond last element array . Incrementing pointer one, bottom part loop, means moving pointer next sequential object declared size. Since p pointer integers, compiler generate RISC-V instructions increment p eight, number bytes RISC-V integer. assignment loop places 0 object pointed p. purpose section show pointers map RISC-V instructions, endorse dated programming style. We’ll see impact modern compiler optimization two procedures end section. Array Version Clear Let’s start array version, clear1 , focusing body loop ignoring procedure linkage code. assume two parameters array size found registers x10 x11, allocated register x5. initialization i, first part loop, straightforward: li x5, 0 // = 0 (register x5 = 0) set array[i] 0 must first get address. Start multiplying 8 get byte address: loop1: slli x6, x5, 3 // x6 = * 8 Since starting address array register, must add index get address array[i] using add instruction: add x7, x10, x6 // x7 = address array[i] 280Finally, store 0 address: sd x0, 0(x7) // array[i] = 0 instruction end body loop, next ste p increment i: addi x5, x5, 1 // = + 1 loop test checks less size: blt x5, x11, loop1 // (i < size) go loop1 seen pieces procedure. RISC-V code clearing array using indices: li x5, 0 // = 0 loop1: slli x6, x5, 3 // x6 = * 8 add x7, x10, x6 // x7 = address array[i] sd x0, 0(x7) // array[i] = 0 addi x5, x5, 1 // = + 1 blt x5, x11, loop1 // (i < size) go loop1 (This code works long size greater 0; ANSI C requires test size loop, we’ll skip legality here.) Pointer Version Clear second procedure uses pointers allocates two parameters array size registers x10 x11 allocates p register x5. code second procedure starts assigning pointer p address first element array: mv x5, x10 // p = address array[0] next code body loop, simply stores 0 p: loop2: sd x0, 0(x5) // Memory[p] = 0 instruction implements body loop, next code iteration increment, changes p point next doubleword: addi x5, x5, 8 // p = p + 8 Incrementing pointer 1 means moving pointer next sequential object C. Since p pointer integers declared long long int , uses 8 bytes, compiler increments p 8. 281The loop test next. first step calculating address last element array . Start multiplying size 8 get byte address: slli x6, x11, 3 // x6 = size * 8 add product starting address array get address first doubleword array: add x7, x10, x6 // x7 = address array[size] loop test simply see p less last element array : bltu x5, x7, loop2 // (p<&array[size]) go loop2 pieces completed, show pointer version code zero array: mvx5, x10 // p = address array[0] loop2: sdx0, 0(x5) // Memory[p] = 0 addi x5, x5, 8 // p = p + 8 slli x6, x11, 3 // x6 = size * 8 add x7, x10, x6 // x7 = address array[size] bltu x5, x7, loop2 // (p<&array[size]) go loop2 first example, code assumes size greater 0. Note program calculates address end array every iteration loop, even though change. faster version code moves calculation outside lo op: mv x5, x10 // p = address array[0] slli x6, x11, 3 // x6 = size * 8 add x7, x10, x6 // x7 = address array[size] loop2: sdx0, 0(x5) // Memory[p] = 0 addi x5, x5, 8 // p = p + 8 bltu x5, x7, loop2 // (p < &array[size]) go loop2 Comparing Two Versions Clear Comparing two code sequences side side illustrates difference array indices pointers (the changes introduced pointer version highlighted): 282The version left must “multiply” add inside loop incremented address must recalculated new index. memory pointer version right increments pointer p directly. pointer version moves scaling shift array bound addition outside loop, thereby reducing instructions executed per iteratio n five three. manual optimization corresponds compiler optimization strength reduction (shift instead multiply) induction variable elimination (eliminating array address calculations within loops). Section 2.15 describes two many optimizations. Elaboration mentioned earlier, C compiler would add test sure size greater 0. One way would branch instruction loop blt x0 , x11, afterLoop . Understanding Program Performance People taught use pointers C get greater efficiency available arrays: “Use pointers, even can’t understand code.” Modern optimizing compilers produce code array version good. programmers today prefer compiler heavy lifting. 283 Advanced Material: Compiling C Interpreting Java section gives brief overview C compiler works Java executed. compiler significantly affect performance computer, understanding compiler technology today critical understanding performance. Keep n mind subject compiler construction usually taught one- two-semester course, introduction necess arily touch basics. second part section readers interested seei ng object-oriented language like Java executes RISC-V architecture. shows Java byte-codes used interpretation RISC-V code Java version C segments prior sections, including Bubble Sort. covers Java Virtual Machine JIT compilers. object-oriented language programming language oriented around objects rather actions, data versus logic. rest Section 2.15 found online. 2.15 Advanced Material: Compiling C Interpreting Java section gives brief overview C compiler works Java executed. compiler significantly affect performance computer, understanding compiler 284technology today critical understanding performance. Keep mind subject compiler construction usually taugh one- two-semester course, introduction necess arily touch basics. second part section, starting page 150.e15, readers interested seeing objected-oriented language like Java executes RISC-V architecture. shows Java bytecodes used interpretation RISC-V code J ava version C segments prior sections, including Bubble Sort. covers Java virtual machine just-in-tim e (JIT) compilers. Compiling C first part section introduces internal anatomy compiler. start, Figure e2.15.1 shows structure recent compilers, describe optimizations order passes structure. 285FIGURE E2.15.1 structure modern optimizing compiler consists number passes phases. Logically, pass thought running completion next occurs. practice, passes may handle one procedure time, essentially interleaving another pass. illustrate concepts part section, use C version loop page 95: (save[i] == k) += 1; Front End function front end read source program; check syntax semantics; translate source program intermediate form interprets language-specifi c operation program. see, intermediate forms usually simple, are, fact, similar Java bytecodes (see Figure e2.15.8 ). front end typically broken four separate functions: 1. Scanning reads individual characters creates string tokens. Examples tokens reserved words, names, operators, punctuation symbols. example, token sequence while, (, save, [, i, ], ==, k, ), i, +=, 1 . word like recognized reserved word C, save , i, j 286are recognized names, 1 recognized number. 2. Parsing takes token stream, ensures syntax correct, produces abstract syntax tree , representation syntactic structure program. Figure e2.15.2 shows abstract syntax tree might look like program fragment. 3. Semantic analysis takes abstract syntax tree checks program semantic correctness. Semantic checks normally ensu variables types properly declared types operators objects match, step called type checking . process, symbol table representing named objects—classes , variables, functions—is usually created used type-check program. 4. Generation intermediate representation (IR) takes symbol table abstract syntax tree generates intermediate representation output front end. Intermediate representations usually use simple operations small set primitive types, integers, characters, reals. Java bytecodes represent one type intermediate form. modern compilers, common intermediate form looks much like RISC-V instruction set infinite number virtual registers; later, describe map virtual registers finite set real registers. Figure e2.15.3 shows example might represented intermediate form. 287FIGURE E2.15.2 abstract syntax tree example. roots tree consist informational token numbers names. Long chains straight- line descendents often omitted constructing tree. 288FIGURE E2.15.3 loop example shown using typical intermediate representation. practice, names save, i, k would replaced sort address, reference either local stack pointer global pointer, offset, similar way save[i] accessed. Note format RISC-V instructions different rest chapter, represent intermediate representations using rXX notation virtual registers. intermediate form specifies functionality prog ram manner independent original source. front e nd created intermediate form, remaining passes largely language independent. High-Level Optimizations High-level optimizations transformations done something close source level. common high-level transformation probably procedure inlining , replaces call function body function, substituting caller’s arguments proc edure’s parameters. high-level optimizations involve loop transformations reduce loop overhead, improve memory access, exploit hardware effectively. example, loops execute many iterations, traditionally 289controlled statement, optimization loop-unrolling often useful. Loop-unrolling involves taking loop, replicatin g body multiple times, executing transformed loop fewer times. Loop-unrolling reduces loop overhead provides opportunities many optimizations. types high- level transformations include sophisticated loop transformat ions interchanging nested loops blocking loops obtain better memory behavior; see Chapter 5 examples. loop-unrolling technique get performance loops access arrays, multiple copies loop body made instructions different iterations scheduled togethe r. Local Global Optimizations Within pass dedicated local global optimization, three classes optimization performed: 1. Local optimization works within single basic block. local optimization pass often run precursor successor global optimization “clean up” code global optimization. 2. Global optimization works across multiple basic blocks; see example shortly. 3. Global register allocation allocates variables registers regions code. Register allocation crucial getting good performance modern processors. Several optimizations performed locally globally, including common subexpression elimination, constant propagation, copy propagation, dead store elimination, strength reduction. Let’s look simple examples optimizations. Common subexpression elimination finds multiple instances expression replaces second one reference first. Consider, example, code segment add 4 array element: x[i] = x[i] + 4 address calculation x[i] occurs twice identical since neither starting address x value changes. 290Thus, calculation reused. Let’s look intermediate code fragment, since allows several optimizations performed. unoptimized intermediate code left. right optimized code, using common subexpression elimination replace second address calculation fi rst. Note register allocation yet occurred, compiler using virtual register numbers like r100 here. // x[i] + 4 // x[i] + 4 la r100,x la r100,x ld r101,i ld r101,i mul r102,r101,8 slli r102,r101,3 add r103,r100,r102 add r103,r100,r102 ld r104, 0(r103) ld r104, 0(r103) // // value x[i] r104 addi r105, r104,4 addi r105, r104,4 la r106,x sd r105, 0(r103) ld r107,i mul r108,r107,8 add r109,r106,r107 sd r105,0(r109) optimization possible across two basic blocks, would instance global common subexpression elimination. Let’s consider optimizations: Strength reduction replaces complex operations simpler ones applied code segment, replacing mul shift left. Constant propagation sibling constant folding find constants code propagate them, collapsing constant values whenever possible. Copy propagation propagates values simple copies, eliminating need reload values possibly enabling optimizations, common subexpression elimination. Dead store elimination finds stores values used eliminates store; “cousin” dead code elimination , finds unused code—code cannot affect result program—and eliminates it. heavy use macros, templates, similar techniques designed reuse code high-level languages, dead code occurs surprisingly often. Compilers must conservative . first task compiler 291produce correct code; second task usually produce fast code, although factors, code size, may sometimes important well. Code fast incorrect—for possible combination inputs—is simply wrong. Thus, say compiler “conservative,” mean performs optimization knows 100% certainty that, matter inputs, code perform user wrote it. Since compilers translate optimize one function procedure time, compilers, especially lower optimization levels, assume worst function calls parameters. Understanding Program Performance Programmers concerned performance critical loops, especially real-time embedded applications, find staring assembly language produced compiler wondering compiler failed perform global optimization allocate variable register throughout loop. answer often lies dictate compiler conservative. opportunity improving c ode may seem obvious programmer, programmer often knowledge compiler have, absence aliasing two pointers absence side effects function call. compiler may indeed able perform transformation little help, could eliminate worst-case behavior must assume. insight also illustrates important observation: programmers use pointers try improve performance accessing variables, especially pointers values stack also names variables elements arrays, likely disable many compiler optimizations. result lower-level pointe r code may run better, perhaps even worse, higher- level code optimized compiler. Global Code Optimizations Many global code optimizations aims used local case, including common subexpression elimination, constant propagation, copy propagation, dead store dead 292code elimination. two important global optimizations: code motion induction variable elimination. loop optimizations; is, aimed code loops. Code motion finds code loop invariant: particular piece code computes value every iteration loop and, hence, may computed outside loop. Induction variable elimination combination transformations reduce overhead indexing arrays, essentially replacing array indexing pointer accesses. Rather examine induction variable elimination depth, point reader Section 2.14 , compares use array indexing pointers; loops, modern optimizing compiler perform transformation obvious array code faster pointer code. Implementing Local Optimizations Local optimizations implemented basic blocks scanning basic block instruction execution order, looking optimization opportunities. assignment statement example page 150.e6, duplication entire address calculation recognized series sequential passes code. process might proceed, including description checks needed: 1. Determine two LDA operations return result observing operand x value address changed two LDA operations. 2. Replace uses R106 basic block R101 . 3. Observe cannot change two LDURs reference it. replace uses R107 R101 . 4. Observe MUL instructions input operands, R108 may replaced R102 . 5. Observe two ADD instructions identical input operands ( R100 R102 ), replace R109 R103 . 6. Use dead store code elimination delete second set LDA,LDUR, MUL , ADD instructions since results unused. Throughout process, need know two instances operand value. easy determine refer virtual registers, since intermediate represen tation 293uses registers once, problem trickier w hen operands variables memory, even though considering references within basic block. reasonably easy compiler make common subexpression elimination determination conservative fashi case; see next subsection, difficult branches intervene. Implementing Global Optimizations understand challenge implementing global optimizations , let’s consider examples: Consider case opportunity common subexpression elimination, say, IR statement like ADD Rx , R20, R50. determine whether two statements compute value, must determine whether values R20 R50 identical two statements. practice, means values R20 R50 changed first statement second. single basic block, easy decide; difficult complex program structur e involving multiple basic blocks branches. Consider second LDUR R107 within earlier example: know whether value used again? consider single basic block, know uses R107 within block, easy see. optimization proceeds, however, common subexpression elimination copy propagation may create uses value. Determining value unused code dead difficult case multiple basic blocks. Finally, consider load k loop, candidate code motion. simple example, might argue easy see k changed loop is, hence, loop invariant. Imagine, however, complex loop multiple nestings statements within body. Determining load k loop invariant harder case. information need perform global optimizations similar: need know operand IR statement could changed defined (use-definition information). dual information also needed: is, finding 294uses changed operand (definition-use information). Data flow analysis obtains types information. Global optimizations data flow analysis operate control flow graph , nodes represent basic blocks arcs represent control flow basic blocks. Figure e2.15.4 shows control flow graph simple loop example, one important transformation introduced. describe transformation caption, see discover it, done, own! 295FIGURE E2.15.4 control flow graph loop example. node represents basic block, terminates branch sequential fall-through anoth er basic block also target branch. IR statements numbered ease referring them. important transformation performed move test conditional branch 296end. eliminates unconditional branch formerly inside loop places loop. transformation important many compilers generation IR. mul also replaced (“strength-reduced to”) slli. Suppose computed use-definition information control flow graph Figure e2.15.4 . information allow us perform code motion? Consider IR statements number 1 6: cases, use-definition information tells us definitions (changes) operands statements within loop. Thus, IR statements moved outside loop. Notice LDA save LDUR k executed once, prior loop entrance, computational effect same, program runs faster since two statements outside loop. contrast, consider IR statement 2, loads value i. definitions affect statement outside loop, initially defined, inside loop statement 10 stored. Hence, statement loop invariant. Figure e2.15.5 shows code performing code motion induction variable elimination, simplifies address calculation. variable still register allocated, eliminating need load store every time, see done next subsection. 297FIGURE E2.15.5 control flow graph showing representation loop example code motion induction variable elimination. number instructions inner loop 298reduced 10 6. turn register allocation, need mention caveat also illustrates complexity difficulty optimizer s. Remember compiler must cautious. conservative, compiler must consider following question: way variable k could possibly ever change loop? Unfortunately, one way. Suppose variable k variable actually refer memory location, could happen accessed pointers reference parameters . sure many readers saying, “Well, would certainly stupid piece code!” Alas, response ope n compiler, must translate code written. Recal l aliasing information must also conservative; thus, compilers often find negating optimization opportunities possible alias exists one place code incomplete information aliasing. Register Allocation Register allocation perhaps important optimization fo r modern load-store architectures. Eliminating load store gets rid instruction. Furthermore, register allocation enhances value optimizations, common subexpression elimination. Fortunately, trend toward larger register counts n modern architectures made register allocation simpler effective. Register allocation done local basis global basis, is, across multiple basic blocks within single function. Local register allocation usually done late compilation, final code generated. focus challenging opportunistic global register allocatio n. Modern global register allocation uses region-based approach, region (sometimes called live range ) represents section code particular variable could allocated particular register. region selected? process iterative: 1. Choose definition (change) variable given basic block; add block region. 2. Find uses definition, data flow analysis 299problem; add basic blocks contain uses, well basic block value passes reach use, region. 3. Find definitions also affect use found previous step add basic blocks containing definiti ons, well blocks definitions pass reach use, region. 4. Repeat steps 2 3 using definitions discovered step 3 convergence. set basic blocks found technique special property: designated variable allocated register basic blocks, need loading storing variable. Modern global register allocators start constructing regions every virtual register function. regio ns constructed, key question allocate register region: challenge certain regions overlap may use register. Regions overlap (i.e., share common basic blocks) share register. One way record interference among regions interference graph , node represents region, arcs nodes represent regions basic blocks common. interference graph constructed, problem allocating registers equivalent famous problem called graph coloring: find color node graph two adjacent nodes color. number colors equals number registers, coloring interference graph equivalent allocating register region! insight initial motivation allocation method known region-based allocation, originally called graph-colorin g approach. Figure e2.15.6 shows flow graph representation loop example register allocation. 300FIGURE E2.15.6 control flow graph showing representation loop example code motion induction variable elimination register allocation, using RISC-V register names. number IR statements inner loop dropped four six register allocation 10 global optimizations. value 301resides x12 end loop may need stored eventually maintain program semantics. unused loop, could store avoided, also increment inside loop could eliminated! happens graph cannot colored using number registers available? allocator must spill registers complete coloring. coloring based priority function takes account number memory reference saved cost tying register, allocator attempts avoid spilling important candidates. Spilling equivalent splitting region (or live range); f region split, fewer regions interfere wo separate nodes representing original region. process splitting regions successive coloring used allow allocation process complete, point candidates allocated register. course, whenever region split, loads stores must introduced get value memory store there. location chosen split region must balance cost loads stores must introduced advantage freeing register reducing number interferences. Modern register allocators incredibly effective using large register counts available modern processors. many programs, effectiveness register allocation limited n ot availability registers possibilities aliasing cause compiler conservative choice candidates . Code Generation final steps compiler code generation assembly. compilers use stand-alone assembler accepts assembly language source code; save time, instead perform functions: filling symbolic values generating binary code last stage code generation. modern processors, code generation reasonably straightforward, since simple architectures make choice instruction relatively obvious. Code generation compl ex complicated architectures, x86, since multiple 302IR instructions may collapse single machine instruction. modern compilers, compilation process uses pattern matchi ng either tree-based pattern matcher pattern matcher driven parser. code generation, final stages machine-dependent optimization also performed. include constant folding optimizations, well localized instruction scheduling (see Chapter 4 ). Optimization Summary Figure e2.15.7 gives examples typical optimizations, last column indicates optimization performed gcc compiler. sometimes difficult separate simp ler optimizations—local processor-dependent optimizations— transformations done code generator, optimizations done multiple times, especially local optimizations, may performed global optimization well code generation. Hardware/Software Interface Today, essentially programming desktop server applications done high-level languages, programming embedded applications. development means since instructions executed output compiler, instruction set architecture mainly compiler target. Moore’s Law comes temptation adding sophisticated operations instruction set. challenge may exactly match compiler needs produce may general aren’t fast. example, consider special loop instructions found computers. Suppose instead decrementing one, compiler wanted incremen four, instead branching equal zero, compiler wanted branch index less equal limit. loop instruction may mismatch. faced objections, instruction set designer might next generalize operation, adding another operand specify increment perhaps option branch condition use. danger common case, say, incrementing one, 303slower sequence simple operations. Elaboration sophisticated compilers, many research compilers, use analysis technique called interprocedural analysis obtain information functions called. Interprocedural analysis attempts discover properties remain true across function call. example, might discover function call never change global variables, might useful optimizing loop calls function. information called may-information flow-insensitive information obtained reasonably efficiently, although analyzing call function F requires analyzing functions F calls, makes process somewhat time consuming large programs. costly property discover function must always change variable; information called must-information flow-sensitive information . Recall dictate conservative: may-information never used must-information—just function may change variable mean must change it. conservative, however, use negation may-information, compiler rely 304the fact function never change variable optimizations around call site function. FIGURE E2.15.7 Major types optimizations explanation class. third column shows occur different levels optimization gcc. GNU organization calls three optimization levels medium (O1), full (O2), full integration small procedures (O 3). One important uses interprocedural analysis obtain so-called alias information. alias occurs two names may designate variable. example, quite helpful know two pointers passed function may never designate variable. Alias information usually flow-insensitive must used conservatively. Interpreting Java second part section readers interested see ing object-oriented language like Java executes RISC-V architecture. shows Java bytecodes used interpretation RISC-V code Java version C segments prior sections, including Bubble Sort. object-oriented language programming language oriented around objects rather 305than actions, data versus logic. Let’s quickly review Java lingo make sure page. big idea object-oriented programming programmers think terms abstract objects, operations associated type object. New types often thought refinements existing types, new types use operations existing types without change. hope programmer thinks higher level, code reused readily programmer implements common operations many different types. different perspective led different set terms. type object class , definition new data type together operations defined work data type. particular object instance class, creating object class called instantiation . operations class called methods , similar C procedures. Rather call procedure C, invoke method Java. members class fields , correspond variables C. Variables inside objects called instance fields . Rather access structure pointer, Java uses object reference access object. syntax method invocation x.y, x object reference method name. parent–child relationship older newer classes captured verb “extends”: child class extends (or subclasses) parent class. child class typically redefine methods found parent match new data type. methods work fine, child class inherits methods. reduce number errors associated pointers explicit memory deallocation, Java automatically frees unused storage, using separate garbage collector frees memory full. Hence, new creates new instance dynamic object heap, free Java. Java also requires array bounds checked runtime catch another class errors occur C programs. Interpretation mentioned before, Java programs distributed Java 306bytecodes, Java Virtual Machine (JVM) executes Java byte codes. JVM understands binary format called class file format. class file stream bytes single class, containing table valid methods bytecodes, pool constants acts part symbol table, information parent class class. JVM first started, looks class method main . start Java class, JVM dynamically loads, links, initializes class. JVM loads class first finding binary representation proper class (class file) creating class binary representation. Linking combines class runtime state JVM executed. Finally , executes class initialization method included ev ery class. Figure e2.15.8 shows Java bytecodes corresponding RISC-V instructions, illustrating five major differences betw een two: 1. simplify compilation, Java uses stack instead registers operands. Operands pushed stack, operated on, popped stack. 2. designers JVM concerned code size, bytecodes vary length one five bytes, versus four-byte, fixed-size RISC-V instructions. save space, JVM even redundant instructions varying lengths whose difference size immediate. decision illustrates c ode size variation third design principle: make common case small . 3. JVM safety features embedded architecture. example, array data transfer instructions check sure first operand reference second index operand within bounds. 4. allow garbage collectors find live pointers, JVM uses different instructions operate addresses versus intege rs JVM know operands contain addresses. RISC-V generally lumps integers addresses together. 5. Finally, unlike RISC-V, Java bytecodes include Java-specific instructions perform complex operations, like allocating array heap invoking method. 307 Compiling Loop Java Using Bytecodes Example Compile loop page 95, time using Java bytecodes: (save[i] == k) += 1; Assume i, k, save first three local variables. Show addresses bytecodes. RISC-V version C loop Figure e2.15.3 took six instructions 24 bytes. big bytecode version? Answer first step put array reference save stack: 0 aload_3 // Push local variable 3 (save[]) onto st ack 1-byte instruction informs JVM address local variable 3 put stack. 0 left instruction byte address first instruction; byt ecodes method start 0. next step put index stack: 1 iload_1 // Push local variable 1 (i) onto stack Like prior instruction, 1-byte instruction short version general instruction takes 2 bytes load local variable onto stack. next instruction get valu e array element: 2 iaload // Put array element (save[i]) onto stack 1-byte instruction checks prior two operands, pops stack, puts value desired array element onto new top stack. Next, place k stack: 3 iload_2 // Push local variable 2 (k) onto stack ready test: 4 if_icompne, Exit // Compare exit equal 3-byte instruction compares top two elements stack, pops stack, branches equal. finally prepared body loop: 7 iinc, 1, 1 // Increment local variable 1 1 (i+ =1) unusual 3-byte instruction increments local variable 1 without using operand stack, optimization saves space. Finally, return top loop 3-byte 308branch: 10 go 0 // Go top Loop (byte address 0) Thus, bytecode version takes seven instructions 13 bytes, half size RISC-V C code. (As before, optimize code branch less.) FIGURE E2.15.8 Java bytecode architecture versus RISC-V. Although many bytecodes simple, last half-dozen rows complex specific Java. Bytecodes one five bytes length, hence name. Java mnemonics uses prefix 32-bit integer, reference (address), 16-bit integers (short), b 8-bit bytes. use I8 8-bit constant I16 16-bit constant. RISC-V uses registers operands, JVM uses stack. compiler knows maximum size operand stack method simply allocates space current frame. notation 309Meaning column: TOS: top stack; NOS: next position TOS; NNOS: next position NOS; pop: remove TOS; pop2: remove TOS NOS; push: add position stack. *NOS *NNOS mean access memory location pointed address stack positions. Const[] refers runtime constant pool class created JVM, Frame[] refers variables local method frame. missing Java bytecodes Figure e2.1 arithmetic logical operators, tricky stack management, compares 0 branch, support branch tables, type conversions, variations complex, Java-specific instructions plus operations floating-point data, 64-bit intege rs (longs), 16-bit characters. Compiling Java Since Java derived C Java built-in types C, assignment statement examples Sections 2.2 2.6 Java C. true statement example Section 2.7 . Java version loop different, however. designers C leave programmers sure code exceed array bounds. designers Java wanted catch array bound bugs, thus require compiler check violations. check bounds, compiler needs know are. Java includes extra doubleword every array holds upper bound. lower bound defined 0. Compiling Loop Java Example Modify RISC-V code loop page 95 include array bounds checks required Java. Assume length array located first element array. Answer 310Let’s assume Java arrays reserved first two doublewords arrays data start. We’ll see use first doubleword soon, second doubleword array length. enter loop, let’s load length array temporary register: ld x5, 8(x25) // Temp reg x5 = length array save multiply 8, must test see it’s less 0 greater last element array. first step check less 0: Loop: blt x22, x0, IndexOutOfBounds // i<0, goto Error Since array starts 0, index last array element one less length array. Thus, test upper array bound sure less length array. Thus, second step branch error it’s greater equal length . bge x22, x5, IndexOutOfBounds //if i>=length, goto Error next two lines RISC-V loop unchanged C version: slli x10, x22, 3 // Temp reg x10 = * 8 add x10, x10, x25 // x10 = address save[i] need account first 16 bytes array reserved Java. changing address field load 0 16: ld x9, 16(x10) // Temp reg x9 = save[i] rest RISC-V code C loop fine is: bne x9, x24, Exit // go Exit save[i] ≠ k addi x22, x22, 1 // = + 1 beq x0, x0, Loop // go Loop Exit: (See exercises optimization sequence.) Invoking Methods Java compiler picks appropriate method depending type object. cases, unambiguous, method invoked overhead C procedure. general, however, compiler knows given variable contains pointer object belongs subtype general class. Since doesn’t know compile time subclass object is, 311and thus method invoked, compiler generate code first tests sure pointer isn’t null uses code load pointer table legal methods type. first doubleword object method table address, Java arrays reserve two doublewords. Let’s say it’s using fifth method declar ed class. (The method order subclasses.) compiler takes fifth address table invokes method address. cost object orientation general method invocatio n takes five steps: 1. conditional branch sure pointer object valid; 2. load get address table available methods; 3. Another load get address proper method; 4. Placing return address return register; finally 5. branch register invoke method. Sort Example Java Figure e2.15.9 shows Java version exchange sort. simple difference need pass length array separate parameter, since Java arrays include length: v.length denotes length v. FIGURE E2.15.9 initial Java procedure 312performs sort array v. Changes Figures e2.24 e2.26 highlighted. significant difference Java methods prepended keywords found C procedures. sort method declared public static swap declared protected static . Public means sort invoked method, protected means swap called methods within package methods within derived classes. static method another name class method—methods perform class-wide operations apply individual object. Static methods essentially C procedures. public Java keyword allows method invoked method. protected Java key word restricts invocation method methods package. package Basically directory contains group related classes. static method method applies whole class rather individual object. unrelated static C. straightforward translation C static methods means ambiguity method invocation, efficient C. also limited sorting integers, means different sort written data type. demonstrate object orientation Java, Figure e2.15.10 shows new version changes highlighted. First, declare v type Comparable replace v[j] > v[j + 1] invocation compareTo . changing v new class, 313can use code sort many data types. 314FIGURE E2.15.10 revised Java procedure sorts array v take types. Changes Figure e2.15.9 highlighted. method compareTo compares two elements returns value greater 0 parameter larger object, 0 equal, negative number smaller object. two changes generalize code sort integers, characters, strings, on, subclasses Comparable types version compareTo type. pedagogic purposes, redefine class Comparable method compareTo compare integers. actual definition Comparable Java library considerably different. Starting RISC-V code generated C, show changes made create RISC-V code Java. swap , significant differences must check sure object reference null array reference within bounds. first test checks address n first parameter zero: swap: beq x10, x0, Error x10, NullPointer // X0==0,goto Error Next, load length v register check index 315k OK. ld x5, 8(x10) // Temp reg x5 = length array v blt x11, x0, IndexOutOfBounds // k < 0, goto Error bge x11, x5, IndexOutOfBounds // k >= length, goto Error check followed check k+1 within bounds. addi x6,x11,1 // Temp reg x6 = k+1 blt x6, x0, IndexOutOfBounds // k+1 < 0, goto Error bge x6, x5, IndexOutOfBounds // k+1 >= length, goto Error Figure e2.15.11 highlights extra RISC-V instructions swap Java compiler might produce. must adjust offset load store account two doublewords reserved method table length. FIGURE E2.15.11 RISC-V assembly code procedure swap Figure e2.24. Figure e2.15.12 shows method body new instructions sort . (We take saving, restoring, return Figure e2.28.) 316FIGURE E2.15.12 RISC-V assembly version method body Java version sort. new code highlighted figure. must stil l add code save restore registers return RISC-V code found Figure e2.27. keep code similar figure, load v.length x22 instead temporary register. reduce number lines code, make simplifying assumption compareTo leaf procedure need push registers saved stack. first test make sure pointer v null: beq x10, x0, Error // x10==0,goto Error Next, load length array (we use register x22 keep similar code C version sort): ld x22, 8(x10) // x22 = length array v must ensure index within bounds. Since first test inner loop test j negative, skip initial bound test. leaves test big: bge x20, x22, IndexOutOfBounds // j > = length, goto Error code testing j + 1 quite similar code checking k + 1 swap , skip here. key difference invocation compareTo . first load address table legal methods, assume two 317doublewords beginning array: ld x28, 0(x10) // x28 = address method table Given address method table object, get desired method. Let’s assume compareTo third method Comparable class. pick address third method, load address temporary register: ld x28, 16(x28) // x28 = address third method ready call compareTo . next step save necessary registers stack. Fortunately, don’t need temporary registers argument registers method invocation, nothing save. Thus, simply pass parameters compareTo : mv x10, x6 // 1st parameter compareTo v[j] mv x11, x7 // 2nd parameter compareTo v[j+1] Then, use jump-and-link register invoke compareTo: jalr x1, 0(x28) // invoke compareTo, save return address x1 method returns, x10 determining two elements larger. x10 > 0 , v[j] >v[j+1] , need swap . Thus, skip swap , need test x10 ≤ 0: ble x10, x0, exit2 // go exit2 v[j] ≤ v[j+1] RISC-V code compareTo left exercise. Hardware/Software Interface main changes Java versions sort swap testing null object references index out-of-bounds errors, extra method invocation give general compare. method invocation expensive C procedure call, since requires, conditional branch, pair chained loads, indirect branch. see Chapter 4 , dependent loads indirect branches relatively slow modern processors. increasing popularity Java suggests many programmers today willing leverage high performance modern processors pay error checking code reuse. Elaboration Although test reference j j + 1 sure 318indices within bounds, assembly language programmer might look code reason follows: 1. inner loop executed j ≤ 0 since j + 1 > j, need test j + 1 see less 0. 2. Since takes values, 0, 1, 2, …, (data.length −1) since j takes values −1, −2, …, 2, 1, 0, need test j ≤ data.length since largest value j data.length −2. 3. Following reasoning, need test whether j + 1 ≤data.length since largest value j+1 data.length −1. coding tricks Chapter 2 superscalar execution Chapter 4 lower effective cost bounds checking, high optimizing compilers reason way. Note compiler inlined swap method sort, many checks would unnecessary. Elaboration Look carefully code swap Figure e2.15.11 . See anything wrong code, least explanation code works? implicitly assumes Comparable element v 8 bytes long. Surely, need much 8 bytes complex subclass Comparable , could contain number fields. Surprisingly, code work, important property Java’s semantics forces use same, small representation variables, fields, array elements belong Comparable subclasses. Java types divided primitive types —the predefined types numbers, characters, Booleans—and reference types —the built-in classes like String, user-defined classes, arrays. Value reference types pointers (also called references ) anonymous objects allocated heap. programmer, means assigning one variable another create new object, instead makes variables refer object. objects anonymous, programs therefore way refer directly, program must use indirection variable read write objects’ fields (variables). Thus, data structure allocated array v consists entirely pointers, safe assume size, swapping code 319works Comparable ’s subtypes. write sorting swapping functions arrays primitive types requires write new versions functions, one type. replication two reasons. First, primitive ty pe values include references dispatching tables w e used Comparables determine runtime compare values. Second, primitive values come different sizes: 1, 2, 4, 8 bytes. pervasive use pointers Java elegant consistenc y, penalty level indirection requirement objects allocated heap. Furthermore, language lifetimes heap-allocated anonymous objects independent lifetimes named variables, fields, array elements reference them, programmers must deal problem deciding safe deallocate heap-allocate storage. Java’s designers chose use garbage collection. cours e, use garbage collection rather explicit user memory management also improves program safety. C++ provides interesting contrast. Although programmers write essentially pointer-manipulating solution C++, another option. C++, programmers elect forgo level indirection directly manipulate array objects, rather array pointers objects. so, C++ programmers would typically use template capability, allows class function parameterized type data acts. Templates, however, compiled using equivalent macro expansion. is, declared instance sort capable sorting types X Y, C++ would create two copies code class: one sort<X> one sort<Y>, specialized accordingly. solution increases code size exchange making comparison faster (since function calls would indirect, might even subject inline expansion). course, speed advantage would canceled swapping objects required moving large amounts data instead single pointers. always, best design depend details problem. 2.16 Real Stuff: MIPS Instructions 320The instruction set similar RISC-V, MIPS, also originated n academia, owned Imagination Technologies. MIPS RISC-V share design philosophy, despite MIPS 25 years senior RISC-V. good news know RISC-V, easy pick MIPS. show similarity, Figure 2.29 compares instruction formats RISC-V MIPS. 321FIGURE 2.29 Instruction formats RISC-V MIPS. similarities result part instruction set 32 registers. MIPS ISA 32-bit address 64-bit address versions, sensibly called MIPS-32 MIPS-64. instruction sets virtually identical except larger address size nee ding 64-bit registers instead 32-bit registers. common features RISC-V MIPS: instructions 32 bits wide architectures. 32 general-purpose registers, one register hardwired 0. way access memory via load store instructions architectures. Unlike architectures, instructions load store many registers MIPS RISC-V. instructions branch register equal zero branch register equal zero. sets addressing modes work word sizes. One main differences RISC-V MIPS conditional branches equal equal. Whereas RISC- V simply provides branch instructions compare two register s, MIPS relies comparison instruction sets register 0 1 depending whether comparison true. Programmers 322follow comparison instruction branch equal equal zero depending desired outcome comparison . Keeping minimalist philosophy, MIPS performs les comparisons, leaving programmer switch order operands switch condition tested bran ch get desired outcomes. MIPS signed unsigned versions set less instructions: slt sltu . look beyond core instructions commonly used, main difference full MIPS much larger instruction set RISC-V, shall see Section 2.18. 2.17 Real Stuff: x86 Instructions Beauty altogether eye beholder. Margaret Wolfe Hungerford, Molly Bawn, 1877 Designers instruction sets sometimes provide powerfu l operations found RISC-V MIPS. goal generally reduce number instructions executed program. danger reduction occur cost simplicity, increasing time program takes execute instructions slower. slowness may result slower clock cycle time requiring clock cycles simpler sequence. path toward operation complexity thus fraught peril. Section 2.19 demonstrates pitfalls complexity. Evolution Intel x86 RISC-V MIPS vision single groups working time; pieces architectures fit nicely together . case x86; product several independent groups evolved architecture almost 40 years, adding new features original instruction set someone might add clothing packed bag. important x86 milestones. 323general-purpose register (GPR) register used addresses data virtually instruction. 1978 : Intel 8086 architecture announced assembly language-compatible extension then-successful Intel 8080, 8-bit microprocessor. 8086 16-bit architecture, internal registers 16 bits wide. Unlike RISC-V, registers dedicated uses, hence 8086 considered general- purpose register (GPR) architecture. 1980 : Intel 8087 floating-point coprocessor announced. architecture extends 8086 60 floating-point instructions. Instead using registers, relies stack (see Section 2.21 Section 3.7 ). 1982 : 80286 extended 8086 architecture increasing address space 24 bits, creating elaborate memory- mapping protection model (see Chapter 5 ), adding instructions round instruction set manipulate protection model. 1985 : 80386 extended 80286 architecture 32 bits. addition 32-bit architecture 32-bit registers 32-bit address space, 80386 added new addressing modes additional operations. expanded instructions make 80386 nearly general-purpose register machine. 80386 also added paging support addition segmented addressing (see Chapter 5). Like 80286, 80386 mode execute 8086 programs without change. 1989–95 : subsequent 80486 1989, Pentium 1992, Pentium Pro 1995 aimed higher performance, four instructions added user-visible instructio n set: three help multiprocessing (see Chapter 6 ) conditional move instruction. 1997 : Pentium Pentium Pro shipping, Intel announced would expand Pentium Pentium Pro architectures MMX ( Multi Media Extensions ). new set 57 instructions uses floating-point stack accelerate multimedia communication applications. MMX instructions typically operate multiple short data elements time, 324tradition single instruction, multiple data (SIMD) architectures (see Chapter 6 ). Pentium II introduce new instructions. 1999 : Intel added another 70 instructions, labeled SSE ( Streaming SIMD Extensions ) part Pentium III. primary changes add eight separate registers, double width 128 bits, add single precision floating-point data type. Hence, four 32-bit floating-point operations performed parallel. improve memory performance, SSE includes cache prefetch instructions plus streaming store instructions bypass th e caches write directly memory. 2001 : Intel added yet another 144 instructions, time labeled SSE2. new data type double precision arithmetic, allows pairs 64-bit floating-point operations parallel. Almost 144 instructions versions existing MMX SSE instructions operate 64 bits data parallel. change enable multimedia operations; gives compiler different target floating-point operations th e unique stack architecture. Compilers choose use eight SSE registers floating-point registers like found ther computers. change boosted floating-point performance Pentium 4, first microprocessor include SSE2 instructions. 2003 : company Intel enhanced x86 architecture time. AMD announced set architectural extensions increase address space 32 64 bits. Similar transition 16- 32-bit address space 1985 80386, AMD64 widens registers 64 bits. also increases number registers 16 increases number 128-bit SSE registers 16. primary ISA change comes adding new mode called long mode redefines execution x86 instructions 64-bit addresses data. address larger number registers, adds new prefix instructions. Depending count, long mode also adds four 10 new instructions drops 27 old ones. PC-relative data addressing another extension. AMD64 still mode identical x86 (legacy mode ) plus mode restricts user programs x86 allows operating systems use AMD64 ( compatibility mode ). modes allow graceful transition 64-bit 325addressing HP/Intel IA-64 architecture. 2004 : Intel capitulates embraces AMD64, relabeling Extended Memory 64 Technology (EM64T). major difference Intel added 128-bit atomic compare swap instruction, probably included AMD64. time, Intel announced another generation media extensions. SSE3 adds 13 instructions support complex arithmetic, graphics operations arrays structures, video encoding, floating-point conversion, thread synchronization (see Section 2.11 ). AMD added SSE3 subsequent chips missing atomic swap instruction AMD64 maintain binary compatibility Intel. 2006 : Intel announces 54 new instructions part SSE4 instruction set extensions. extensions perform tweaks like sum absolute differences, dot products arrays structur es, sign zero extension narrow data wider sizes, population count, on. also added support virtual machines (see Chapter 5 ). 2007 : AMD announces 170 instructions part SSE5, including 46 instructions base instruction set adds three operand instructions like RISC-V. 2011 : Intel ships Advanced Vector Extension expands SSE register width 128 256 bits, thereby redefining 250 instructions adding 128 new instructions. history illustrates impact “golden handcuffs” compatibility x86, existing software base step important jeopardize significant architectural changes. Whatever artistic failures x86, keep mind instruction set largely drove PC generation computers still dominates Cloud portion post-PC era. Manufacturi ng 350M x86 chips per year may seem small compared 14 billion ARM chips, many companies would love control market. Nevertheless, checkered ancestry led architecture difficult explain impossible love. Brace see! try read section care would need write x86 programs; goal instead give familiarity strengths weaknesses world’s popular desktop architecture. 326Rather show entire 16-bit, 32-bit, 64-bit instruction set, section concentrate 32-bit subset originated 80386. start explanation registers addressing modes, move integer operations , conclude examination instruction encoding. x86 Registers Data Addressing Modes registers 80386 show evolution instruction set (Figure 2.30 ). 80386 extended 16-bit registers (except segment registers) 32 bits, prefixing E name indicate 32-bit version. We’ll refer generically GPRs ( general- purpose registers ). 80386 contains eight GPRs. means RISC-V MIPS programs use four times many. 327FIGURE 2.30 80386 register set. Starting 80386, top eight registers extended 32 bits could also used general- purpose registers. Figure 2.31 shows arithmetic, logical, data transfer instructions two-operand instructions. two import ant differences here. x86 arithmetic logical instructions mus one operand act source destination; RISC-V MIPS allow separate registers source destination. restriction puts pressure limited registers, sin ce one source register must modified. second important differe nce one operands memory. Thus, virtually instruction may one operand memory, unlike RISC-V MIPS. 328FIGURE 2.31 Instruction types arithmetic, logical, data transfer instructions. x86 allows combinations shown. restriction absence memory-memory mode. Immediates may 8, 16, 32 bits length; register one 14 major registers Figure 2.33 (not EIP EFLAGS). Data memory-addressing modes, described detail below, offer two sizes addresses within instruction. so-called displacements 8 bits 32 bits. Although memory operand use addressing mode, restrictions registers used mode. Figure 2.32 shows x86 addressing modes GPRs cannot used mode, well get effect using RISC-V instructions. FIGURE 2.32 x86 32-bit addressing modes register restrictions equivalent RISC-V code. Base plus Scaled Index addressing mode, found RISC-V MIPS, included avoid multiplies 8 (scale factor 3) turn index register byte address (see Figures 2.26 2.28). scale factor 1 used 16-bit data, scale factor 2 32-bit data. scale factor 0 means address scaled. displacement 329longer 12 bits second fourth modes, RISC-V equivalent mode would need instructions, usually lui load bits 12 31 displacement, followed add sum bits base register. (Intel gives two different name called Based addressing mode—Based Indexed—but essentially identical combine here.) x86 Integer Operations 8086 provides support 8-bit ( byte) 16-bit ( word ) data types. 80386 adds 32-bit addresses data ( doublewords ) x86. (AMD64 adds 64-bit addresses data, called quad words ; we’ll stick 80386 section.) data type distinctions apply register operations well memory accesses. Almost every operation works 8-bit data one longer data size. size determined mode either 16 bits 32 bits. Clearly, programs want operate data three sizes, 80386 architects provided convenient way specify version without expanding code size significantly. decided either 16-bit 32-bit data dominate programs, made sense able set default large size. default data size set bit code segment register. override default data size, 8-bit prefix attached instruction tell machine use large size instruction. prefix solution borrowed 8086, allows multiple prefixes modify instruction behavior. three riginal prefixes override default segment register, lock bus support synchronization (see Section 2.11 ), repeat following instruction register ECX counts 0. last pr efix intended paired byte move instruction move variable number bytes. 80386 also added prefix override default address size. x86 integer operations divided four major classes: 1. Data movement instructions, including move, push, pop. 2. Arithmetic logic instructions, including test, integer, 330decimal arithmetic operations. 3. Control flow, including conditional branches, unconditional branches, calls, returns. 4. String instructions, including string move string compare. first two categories unremarkable, except arithmetic logic instruction operations allow destinati either register memory location. Figure 2.33 shows typical x86 instructions functions. FIGURE 2.33 typical x86 instructions functions. list frequent operations appears Figure 2.37 . CALL saves EIP next instruction stack. (EIP Intel PC.) Conditional branches x86 based condition codes flags . Condition codes set side effect operation; used compare value result 0. Branches test condition codes. PC-relative branch addresses must specified number bytes, since unlike RISC-V MIPS, 80386 instructions alignment restriction. String instructions part 8080 ancestry x86 commonly executed programs. often slower equivalent software routines (see Fallacy page 157). Figure 2.34 lists integer x86 instructions. Many instructions available byte word formats. 331FIGURE 2.34 typical operations x86. Many operations use register-memory format, either source destination may memory may register immediate operand. x86 Instruction Encoding Saving worst last, encoding instructions 80386 complex, many different instruction formats. Instruction 80386 may vary 1 byte, one operand, 15 bytes. Figure 2.35 shows instruction format several example instructions Figure 2.33 . opcode byte usually contains bit saying whether operand 8 bits 32 bits. instructions, opcode may include addressing mode register; true many instructions fo rm “register=register op immediate.” instructions use “postbyte” extra opcode byte, labeled “mod, reg, r/m,” 332contains addressing mode information. postbyte used f many instructions address memory. base plus scaled index mode uses second postbyte, labeled “sc, index, base.” 333FIGURE 2.35 Typical x86 instruction formats. Figure 2.39 shows encoding postbyte. Many instructions contain 1-bit field w, says whether operation byte doubleword. field MOV used instructions may move memory shows direction move. ADD instruction requires 32 bits immediate field, 32-bit mode, immediates either 8 bits 32 bits. immediate field TEST 32 bits long 8-bit immediate test 32-bit mode. Overall, instructions may vary 1 15 bytes length. long length comes extra 1-byte prefixes, 4-byte immediate 4-byte displacement address, using opcode 2 bytes, using scaled index mode specifier, adds another byte. 334Figure 2.36 shows encoding two postbyte address specifiers 16-bit 32-bit modes. Unfortunately, understand fully registers addressing modes available, need see encoding addressing modes sometimes even encoding instructions. FIGURE 2.36 encoding first address specifier x86: mod , reg, r/m. first four columns show encoding 3-bit reg field, depends w bit opcode whether machine 16-bit mode (8086) 32-bit mode (80386). remaining columns explain mod r/m fields. meaning 3-bit r/m field depends value 2-bit mod field address size. Basically, registers used address calculation listed sixth seventh columns, mod =0, mod =1 adding 8-bit displacement mod =2 adding 16-bit 32-bit displacement, depending address mode. exceptions 1) r/m =6 mod =1 mod =2 16-bit mode selects BP plus displacement; 2) r/m =5 mod =1 mod =2 32-bit mode selects EBP plus displacement; 3) r/m =4 32-bit mode mod equal 3, (sib) means use scaled index mode shown Figure 2.35 . mod =3, r/m field indicates register, using encoding reg field combined w bit. x86 Conclusion Intel 16-bit microprocessor two years competitors ’ elegant architectures, Motorola 68000, head start led selection 8086 CPU IBM 335PC. Intel engineers generally acknowledge x86 difficult build computers like RISC-V MIPS, large market meant PC era AMD Intel could afford resources help overcome added complexity. x86 lacks style, rectifies market size, making beautiful right perspective. saving grace frequently used x86 architectural components difficult implement, AMD Intel demonstrated rapidly improving performance integer programs since 1978. get performance, compilers must avoid portions architecture hard implement fast. post-PC era, however, despite considerable architectural manufacturing expertise, x86 yet competitive personal mobile device. 2.18 Real Stuff: Rest RISC- V Instruction Set goal making instruction set architecture suitable fo r wide variety computers, RISC-V architects partitioned instruction set base architecture several extensions . named letter alphabet, base architecture named integer . base architecture instructions relative popular instruction sets today; indeed, ch apter already covered nearly them. section rounds base architecture, describes five standard extensions. Figure 2.37 lists remaining instructions base RISC-V architecture. first instruction, auipc , used PC-relative memory addressing. Like lui instruction, holds 20-bit constant corresponds bits 12 31 integer. auipc ’s effect add number PC write sum register. Combined instruction like addi , possible address byte memory within 4 GiB PC. feature useful position-independent code , execute correctly matter memory loaded. frequently used dynamically linked libraries. 336FIGURE 2.37 remaining 14 instructions base RISC-V instruction set architecture. next four instructions compare two integers, write th e Boolean result comparison register. slt sltu compare two registers signed unsigned numbers, respectively, write 1 register first value less second value, 0 otherwise. slti sltiu perform comparisons, immediate second operand. remaining instructions look familiar, names instructions discussed chapter , letter w, short word , appended. instructions perform operation similarly named ones we’ve discussed, except operate lower 32 bits operands, ignoring bits 32 63. Additionally, produce sign-extended 32-bit results: is, bits 32 63 bit 31. RISC-V architects included w instructions operations 32-bit numbers remain common computers 64-bit addresses. main reason popular data type int remains 32 bits Java implementations C language. That’s base architecture! Figure 2.38 lists five standard extensions. first, M, adds instructions multiply divide integers. Chapter 3 introduce several instructions extension. 337FIGURE 2.38 RISC-V instruction set architecture divided base ISA, named I, five standard extensions, M, A, F, D, C. second extension, A, supports atomic memory operations multiprocessor synchronization. load-reserved ( lr.d ) store- conditional ( sc.d ) instructions introduced Section 2.11 members extension. Also included versions operate 32-bit words ( lr.w sc.w ). remaining 18 instructions optimizations common synchronization patter ns, like atomic exchange atomic addition, add additional functionality load-reserved store-conditi onal. third fourth extensions, F D, provide operations floating-point numbers, described Chapter 3 . last extension, C, provides new functionality all. Rather, takes popular RISC-V instructions, like addi , provides equivalent instructions 16 bits length, rather 32. thereby allows programs expressed fewer bytes, reduce cost and, see Chapter 5 , improve performance. fit 16 bits, new instructions restrictions operands: example, instructions access 32 registers, immediate fields narrower. Taken together, RISC-V base extensions 184 instructions, plus 13 system instructions introduc ed end Chapter 5 . 2.19 Fallacies Pitfalls 338Fallacy: powerful instructions mean higher performance. Part power Intel x86 prefixes modify execution following instruction. One prefix repeat th e subsequent instruction counter steps 0. Thus, move data memory, would seem natural instruction sequence use move repeat prefix perform 32-bit memory-to-memory moves. alternative method, uses standard instructions found computers, load data registers store registers back memory. second version program, code replicated reduce loop overhead, copies 1.5 times fast. third version, uses larger floating-point registers instead integer registers x86, copies 2.0 times fast complex move instruction. Fallacy: Write assembly language obtain highest performance. one time compilers programming languages produced naïve instruction sequences; increasing sophistication compilers means gap compiled code code produced hand closing fast. fact, compete current compilers, assembly language programmer needs understand concepts Chapters 4 5 thoroughly (processor pipelining memory hierarchy). battle compilers assembly language coders another situation humans losing ground. example, C offers programmer chance give hint compiler variables keep registers versus spilled memory. compilers poor register allocation, hints vital performance. fact, old C textbooks spent fair amount time giving examples effectively use registe r hints. Today’s C compilers generally ignore hints, compiler better job allocation programmer does. Even writing hand resulted faster code, dangers writing assembly language protracted time spent coding debugging, loss portability, difficulty maintaining code. One widely accepted axioms 339software engineering coding takes longer write mor e lines, clearly takes many lines write program assembly language C Java. Moreover, coded, next danger become popular program. programs always live longer expected, meaning someone update code several years make work new releases operating systems recent computers. Writing higher-level language instead assembly language allows future compilers tailor code forthcoming machines; also makes software easier maintain allows program run brands computers. Fallacy: importance commercial binary compatibility means successful instruction sets don’t change. backwards binary compatibility sacrosanct, Figure 2.39 shows x86 architecture grown dramatically. average one instruction per month 35-year lifetime! Pitfall: Forgetting sequential word doubleword addresses machines byte addressing differ one. 340FIGURE 2.39 Growth x86 instruction set time. clear technical value extensions, rapid change also increases difficulty companies try build compatible processors. Many assembly language programmer toiled errors made assuming address next word doubleword found incrementing address registe r one instead word doubleword size bytes. Forewarned forearmed! Pitfall: Using pointer automatic variable outside defin ing procedure. common mistake dealing pointers pass result procedure includes pointer array local procedure. Following stack discipline Figure 2.12 , memory contains local array reused soon procedure returns. Pointers automatic variables lead chaos. 2.20 Concluding Remarks Less more. Robert Browning, Andrea del Sarto, 1855 two principles stored-program computer use instructions indistinguishable numbers us e alterable memory programs. principles allow single machine aid cancer researchers, financial advisers, novelists specialties. selection set instructions machine understand demands delicate balance among number instructions needed execute program, number f clock cycles needed instruction, speed clock. illustrated chapter, three design principles guide authors instruction sets making tricky tradeoff: 1. Simplicity favors regularity. Regularity motivates many features 341the RISC-V instruction set: keeping instructions single si ze, always requiring register operands arithmetic instructions, keeping register fields place instruction formats. 2. Smaller faster. desire speed reason RISC-V 32 registers rather many more. 3. Good design demands good compromises. One RISC-V example compromise providing larger addresses constants instructions keeping instructions length. also saw great idea Chapter 1 making common cast fast applied instruction sets well computer architecture. Examples making common RISC-V case fast include PC-relative addressing conditional branches immediate addressing larger constant operands. machine level assembly language, language humans read. assembler translates binary numbers machines understand, even “extends” instruction set creating symbolic instructions aren’t n hardware. instance, constants addresses big broken properly sized pieces, common variations instructions given name, on. Figure 2.40 lists RISC-V instructions covered far, real pseudoinstructions. Hiding details higher level ano ther example great idea abstraction . 342343FIGURE 2.40 RISC-V instruction set covered far, real RISC-V instructions left pseudoinstructions right. Figure 2.1 shows details RISC-V architecture revealed chapter. information given also found Columns 1 2 RISC-V Reference Data Card front book. category RISC-V instructions associated constructs appear programming languages: Arithmetic instructions correspond operations found assignment statements. Transfer instructions likely occur dealing wit h 344data structures like arrays structures. Conditional branches used statements loops. Unconditional branches used procedure calls returns case/switch statements. instructions born equal; popularity dominates many. example, Figure 2.41 shows popularity class instructions SPEC CPU2006. varying popularity instructions plays important role chapter datapath, control, pipelining. FIGURE 2.41 RISC-V instruction classes, examples, correspondence high-level program language constructs, percentage RISC-V instructions executed category average integer floating point SPEC CPU2006 benchmarks. Figure 3.24 Chapter 3 shows average percentage individual RISC-V instructions executed. explain computer arithmetic Chapter 3 , reveal RISC-V instruction set architecture. Historical Perspective Reading 345This section surveys history instruction set architectures (ISAs) time, give short history programming languages compilers. ISAs include accumulator architectures, general- purpose register architectures, stack architectures, brief history x86 ARM’s 32-bit architecture, ARMv7. also review controversial subjects high-level-language comp uter architectures reduced instruction set computer architect ures. history programming languages includes Fortran, Lisp, Algol, C, Cobol, Pascal, Simula, Smalltalk, C++, Java, history compilers includes key milestones pion eers achieved them. rest Section 2.21 found online. 2.22 Historical Perspective Reading section surveys history instruction set architect ures time, give short history programming languages compilers. ISAs include accumulator architectures, general-purp ose register architectures, stack architectures, brief history f ARMv7 x86. also review controversial subjects high-level-language computer architectures reduced instruction set computer architectures. history progr amming languages includes Fortran, Lisp, Algol, C, Cobol, Pascal, Simula, Smalltalk, C++, Java, history compilers includes key milestones pioneers achieved them. Accumulator Architectures Hardware precious earliest stored-program computers. Consequently, computer pioneers could afford number f registers found today’s architectures. fact, architect ures single register arithmetic instructions. Since oper ations would accumulate one register, called accumulator , style instruction set given name. example, EDSAC 1949 single accumulator. accumulator 346Archaic term register. On-line use synonym “register” fairly reliable indication user around quite while. Eric Raymond, New Hacker’s Dictionary, 1991 three-operand format RISC-V suggests single register least two registers shy needs. accumulator source operand destination operation fills part shortfall, still leaves us one ope rand short. final operand found memory. Accumulator architectures memory-based operand-addressing mode suggested earlier. follows add instruction accumulator instruction set would look like this: ADD 200 instruction means add accumulator word memory address 200 place sum back accumulator. registers specified accumulator known source destination operation. next step evolution instruction sets addit ion registers dedicated specific operations. Hence, registers might included act indices array references data transfer instructions, act separate accumulators multiply divide instructions, serve top-of-stack pointer. Perhaps best-known example style instruction set found Intel 80x86. style instruction set labeled extended accumulator , dedicated register , special-purpose register . Like single-register accumulator architectures, one operand may memory arithmetic instructions. Like RISC-V architectu re, however, also instructions operands registers. General-Purpose Register Architectures generalization dedicated-register architecture allows registers used purpose, hence name general- purpose register . RISC-V example general-purpose register architecture. style instruction set may divid ed allow one operand memory (as found accumulator architectures), called register-memory architecture, 347and demand operands always registers, called either load-store register-register architecture. Figure e2.22.1 shows history number registers popular computers. load-store architecture Also called register-register architecture. instruction set architecture operations registers data memory may accessed via loads stores. 348FIGURE E2.22.1 number general-purpose registers popular architectures years. first load-store architecture CDC 6600 1963, considered many first supercomputer. RISC-V, ARMv7, ARMv8, MIPS recent examples load-store architecture. 80386 Intel’s attempt transform 8086 general-purpose register-memory instruction set. Perhaps th e best- known register-memory instruction set IBM 360 architectur e, first announced 1964. instruction set still core IBM’s mainframe computers—responsible large part business largest computer company world. Register- memory architectures popular 1960s first half 1970s. Digital Equipment Corporation’s VAX architecture took memory operands one step 1977. allowed instruction use combination registers memory operands. style architecture operands memory called memory-memory . (In truth VAX instruction set, like almost 349other instruction sets since IBM 360, hybrid, since also general-purpose registers.) Intel x86 many versions 64-bit add specify whether operand memory register. addition, memory operand accessed seven addressing modes. combination address modes register-memory operands means dozens variants x86 add instruction. Clearly, variability makes x86 implementations challenging. Compact Code Stack Architectures memory scarce, also important keep programs small, architectures like Intel x86, IBM 360, VAX variable-length instructions, match varying operand specifications minimize code size. Intel x86 instructions 1 15 bytes long; IBM 360 instructions 2, 4, 6 bytes long; VAX instruction lengths anywhere 1 54 bytes. One place code size still important embedded applications. recognition need, ARM, MIPS, RISC-V made versions instructions sets offer 16-bi instruction formats 32-bit instruction formats: Thumb Thumb-2 ARM, MIPS-16, RISC-V Compressed. Despite limited two sizes, Thumb, Thumb-2, MIPS-16, RISC-V Compressed programs 25% 30% smaller, makes code sizes smaller 80x86. Smaller code sizes added benefit improving instruction cache hit rates (see Chapter 5 ). 1960s, companies followed radical approach instruction sets. belief hard compilers utilize registers effectively, companies abandoned regist ers altogether! Instruction sets based stack model execution, like found older Hewlett-Packard handheld calculators. Operands pushed stack memory popped stack memory. Operations take operands stack place result back onto stack. addition simplifying compilers eliminating register allocation, stac k architectures lent compact instruction encodi ng, thereby removing memory size excuse program 350high-level languages. Memory space perceived precious Java, memory space limited keep costs low embedded applications programs may downloaded Internet phone lines Java applets, smaller programs take less time transmit. Hence, compact instruction encoding desirable Java bytecodes. High-Level-Language Computer Architectures 1960s, systems software rarely written high-level languages. example, virtually every commercial operating system UNIX programmed assembly language, recently even OS/2 originally programmed low level. people blamed code density instructio n sets, rather programming languages compiler technology. Hence, architecture design philosophy called high-level- language computer architecture advocated, goal making hardware like programming languages. efficient programming languages compilers, plus expanding memory, doomed movement historical footnote. Burroughs B5000 commercial fountainhead philosophy, today significant commercial descendant 1960s radical. Reduced Instruction Set Computer Architectures language-oriented design philosophy replaced 1980s RISC (reduced instruction set computer) . Improvements programming languages, compiler technology, memory cost meant less programming done assembly level, instruction sets could measured well compilers us ed them, contrast skillfully assembly language programmers used them. Virtually new instruction sets since 1982 followed RISC philosophy fixed instruction lengths, load-store ins truction 351sets, limited addressing modes, limited operations. ARMv7, ARMv8 Hitachi SH, IBM PowerPC, MIPS, Sun SPARC, and, course, RISC-V, examples RISC architectures. Brief History ARMv7 ARM started processor Acorn computer, hence original name Acorn RISC Machine. Berkeley RISC papers influenced architecture. One important early applications emulation 6502, 16-bit microprocessor. emulation provide software Acorn computer. 6502 variable-length instruction set multiple bytes, 6502 emulation helps explain emphasis shifting masking ARMv7 instruction set. popularity low-power embedded computer began selection processor ill-fated Apple Newton pers onal digital assistant. Although Newton popular Apple hoped, Apple’s blessing gave visibility earlier ARM instruction sets, subsequently caught several markets, including cell phones. Unlike Newton experience, extraordinary success cell phones explains 12 billion ARM processors shipped 2014. One major events ARM’s history 64-bit address extension called version 8. ARM took opportunity redesig n instruction set make look much like MIPS like earlier ARM versions. Brief History x86 ancestors x86 first microprocessors, produce starting 1972. Intel 4004 8008 extremely simple 4- bit 8-bit accumulator-style architectures. Morse et al. [1980] describe evolution 8086 8080 late 1970s attempt provide 16-bit architecture better throughput. time, almost programming microprocessors done assembly language—both memory compilers short supply. Intel wanted keep base 8080 users, 8086 designed “compatible” 8080. 8086 never 352object-code compatible 8080, architectures close enough translation assembly language programs could done automatically. early 1980, IBM selected version 8086 8-bit external bus, called 8088, use IBM PC. chose 8-bit version reduce cost architecture. choice , together tremendous success IBM PC, made 8086 architecture ubiquitous. success IBM PC due part IBM opened architecture PC enabled PC-clone industry flourish. discussed Section 2.18 , 80286, 80386, 80486, Pentium, Pentium Pro, Pentium II, Pentium III, Pentium 4, AMD64 extended architecture provided series performance enhancements. Although 68000 chosen Macintosh, Mac never pervasive PC, partly Apple allow Mac clones based 68000, 68000 acquire software following 8086 enjoys. Motorola 68000 may significant technically 8086, impact IBM’s selection open architecture strategy dominated technical advantages 68000 market. argue inelegance x86 instruction set unavoidable, price must paid rampant success architecture. reject notion. Obviously, successful architecture jettison features added previous implementations, time, features may seen undesirable. awkwardness x86 begins core 8086 instruction set exacerbated architecturally inconsistent expansions found 8087, 80286, 80386, MMX, SSE, SSE2, SSE3, SSE4, AMD64 (EM64T), AVX. counterexample IBM 360/370 architecture, much older x86. dominated mainframe market x86 dominated PC market. Due undoubtedly better base compatible enhancements, instruction set makes much sense x86 50 years first implementation. Extending x86 64-bit addressing means architecture may last several decades. Instruction set anthropologist future peel layer layer architectu res uncover artifacts first microprocessor. Gi ven 353such find, judge today’s computer architecture? Brief History Programming Languages 1954, John Backus led team IBM create natural notation scientific programming. goal Fortran, “FORmula TRANslator,” reduce time develop programs. Fortran included many ideas found programming languages today, including assignment statements, expressions, typed variables, loops, arrays. development language compiler went hand hand. language became standard evolved time improve programmer productivity program portability. evolutionary steps Fortran I, II, IV, 77, 90. Fortran developed IBM’s second commercial computer, 704, also cradle another important programming language: Lisp. John McCarthy invented “LISt Processing” language 1958. mantra programming considered manipulating lists, language contains operations follow links compose new lists old ones . list notation used code well data, modifying composing Lisp programs common. big contribution dynamic data structures and, hence, pointers. Given inventor pioneer artificial intelligence, L isp became popular AI community. Lisp type declarations, Lisp traditionally reclaims storage automatically via built-in garbage collection. Lisp originally interpreted, although compilers later developed it. Fortran inspired international community invent programming language natural express algorithms Fortran, less emphasis coding. language became Algol, “ALGOrithmic Language.” Like Fortran, included type declarations, added recursive procedure calls, nested if-then-else statements, loops, begin-end statements structure code, call-by-name. Algol-60 became classic language academics teach programming 1960s. Although engineers, AI researchers, computer scientists programming languages, could said 354business data processing. Cobol, “COmmon Business-Oriented Language,” developed standard purpose contemporary Algol-60. Cobol created easy read, follows English vocabulary punctuation. added records programming languages, separated description data description code. Niklaus Wirth member Algol-68 committee, supposed update Algol-60. bothered complexity result, wrote minority report sho w programming language could combine algorithmic power Algol-60 record structure Cobol simple understand, easy implement, yet still powerful. minority report became Pascal. first implemented interpreter set Pascal bytecodes. ease implementation led widely deployed, much Algol-68, soon replaced Algol-60 popular language academics teach programming. period, Dennis Ritchie invented C programming language use building UNIX. inventors say “very high level” programming language big one, aimed particular application. Given birthplace, good systems programming, UNIX operating system C compiler written C. UNIX’s popularity helped spur C’s popularity. concept object orientation first captured Simula-67, simulation language successor Algol-60. Invented Ole-Johan Dahl Kristen Nygaard University Oslo 1967, introduced objects, classes, inheritance. Object orientation proved powerful idea. led Alan Kay others Xerox Palo Alto Research Center invent Smalltalk 1970s. Smalltalk-80 married typeless variables garbage collection Lisp object orientation Simula- 67. relied interpretation defined Smalltalk virtual machine Smalltalk bytecode instruction set. Kay colleagues argued processors getting faster, must eventually willing sacrifice performance improve program development. Another example CLU, demonstrated object-oriented language could defined allowed compile-time type checking. Simula-67 also inspired 355Bjarne Stroustrup Bell Labs develop object-oriented vers ion C called C++ 1980s. C++ became widely used industry. Dissatisfied C++, group Sun led James Gosling invented Oak early 1990s. invented object- oriented C dialect embedded devices part major Sun project. make portable, interpreted virtual machine bytecode instruction set. Since new language, elegant object-oriented design C++ much easier learn compile Smalltalk-80. Since Sun’s embedded project failed, might never heard someone made connection Oak programmable browsers World Wide Web. rechristened Java, 1995, Netscape announced would shipping browser. soon became extraordinarily popular. Java rare distinction becoming standard language new business data processing applications favored language academics teach programming. Java languages like encourage reuse code, hence programmers make heavy use libraries, whereas past likely write everything scratch. Brief History Compilers Backus group concerned Fortran would unsuccessful skeptics found examples Fortran ver sion ran half speed equivalent assembly language program. success one first compilers created beachhead many others followed. Early compilers ad hoc programs performed steps described Section 2.15 online. ad hoc approaches replaced solid theoretical foundation step s. time theory established, tool built based theory automated creation step. theoretical roots underlying scanning parsing derive automata theory, relationship languages automata known early. scanning task corresponds recognition language accepted finite-state automata, parsing corresponds recognition language push-down automata (basically automata stack). Languages 356described grammars, set rules tell legal program generated. scanning pass compiler well understood early, parsing harder. earliest parsers use precedence technique s, derived structure arithmetic statements, generalized. great breakthrough modern parsing made Donald Knuth invention LR-parsing, codified two key steps parsing technique, pushing token stack reducing set tokens stack using grammar rule. strong theory formulation scanning parsing led development automated tools compiler constructions, lex yacc , tools developed part UNIX. Optimizations occurred many compilers, harder determine first examples cases. However, Victor Vyssotsky first papers data flow analysis 1963, William McKeeman generally credited first peephole optimizer 1965. group IBM, including John Cocke Fran Allan, developed many early optimization concepts, well defining extending concepts flow analysis. Important contributions also made Al Aho Jeff Ullman. One biggest challenges optimization register allocation. difficult architects used stack architectures avoid problem. breakthrough came researchers working compilers 801, early RISC architecture, recognized coloring graph minimum number colors equivalent allocating fixed number registers unlimited number virtual registers used n intermediate forms. Compilers also played important role open-source movement. Richard Stallman’s self-appointed mission make public domain version UNIX. built GNU C Compiler (gcc) open-source compiler 1987. soon ported many architectures, used many systems today. Reading Bayko, J. [1996]. “Great microprocessors past present,” 357search http://www.cpushack.com/CPU/cpu.html . personal view history representative unusual microprocessors, Intel 4004 Patriot Scientific ShBoom ! Kane, G. J. Heinrich [1992]. MIPS RISC Architecture , Prentice Hall, Englewood Cliffs, NJ. book describes MIPS architecture greater detail Appendix . Levy, H. R. Eckhouse [1989]. Computer Programming Architecture , VAX, Digital Press, Boston. book concentrates VAX, also includes descriptions f Intel 8086, IBM 360, CDC 6600 . Morse, S., B. Ravenal, S. Mazor, W. Pohlman [1980]. “Intel microprocessors—8080 8086”, Computer 13 10 (October). architecture history Intel 4004 8086, according people participated designs . Wakerly, J. [1989]. Microcomputer Architecture Programming, Wiley, New York. Motorola 6800 main focus book, covers Intel 8086, Motorola 6809, TI 9900, Zilog Z8000 . 2.22 Exercises 2.1 [5] <§2.2> following C statement, write corresponding RISC-V assembly code. Assume C variables f, g, h, already placed registers x5, x6, x7 respectively. Use minimal number RISC-V assembly instructions. f = g + (h − 5); 2.2 [5] <§2.2> Write single C statement corresponds two RISC-V assembly instructions below. add f, g, h add f, i, f 2.3 [5] <§§2.2, 2.3> following C statement, write corresponding RISC-V assembly code. Assume variables f, g, h, i, j assigned registers x5, x6, x7, x28 , x29, respectively. Assume base address arrays B registers x10 x11, respectively. B[8] = A[i−j]; 2.4 [10] <§§2.2, 2.3> RISC-V assembly instructions below, 358what corresponding C statement? Assume variables f, g, h, i, j assigned registers x5, x6, x7, x28, x29, respectively. Assume base address arrays B registers x10 x11, respectively. slli x30, x5, 3 // x30 = f*8 add x30, x10, x30 // x30 = &A[f] slli x31, x6, 3 // x31 = g*8 add x31, x11, x31 // x31 = &B[g] ld x5, 0(x30) // f = A[f] addi x12, x30, 8 ld x30, 0(x12) add x30, x30, x5 sd x30, 0(x31) 2.5 [5] <§2.3> Show value 0xabcdef12 would arranged memory little-endian big-endian machine. Assume data stored starting address 0 word size 4 bytes. 2.6 [5] <§2.4> Translate 0xabcdef12 decimal. 2.7 [5] <§§2.2, 2.3> Translate following C code RISC-V. Assume variables f, g, h, i, j assigned registers x5, x6, x7, x28 , x29, respectively. Assume base address arrays B registers x10 x11, respectively. Assume elements arrays B 8-byte words: B[8] = A[i] + A[j]; 2.8 [10] <§§2.2, 2.3> Translate following RISC-V code C. Assume variables f, g, h, i, j assigned registers x5, x6, x7, x28 , x29, respectively. Assume base address arrays B registers x10 x11, respectively. addi x30, x10, 8 addi x31, x10, 0 sd x31, 0(x30) ld x30, 0(x30) add x5,x30, x31 2.9 [20] <§§2.2, 2.5> RISC-V instruction Exercise 2.8 , show value opcode (op), source register (rs1), destination register (rd) fields. I-type instructions, value immediate field, R-type instructions, show value second source register (rs2). non U- 359UJ-type instructions, show funct3 field, R-type S- type instructions, also show funct7 field. 2.10 Assume registers x5 x6 hold values 0x8000000000000000 0xD000000000000000 , respectively. 2.10.1 [5] <§2.4> value x30 following assembly code? add x30, x5, x6 2.10.2 [5] <§2.4> result x30 desired result, overflow? 2.10.3 [5] <§2.4> contents registers x5 x6 specified above, value x30 following assembly code? sub x30, x5, x6 2.10.4 [5] <§2.4> result x30 desired result, overflow? 2.10.5 [5] <§2.4> contents registers x5 x6 specified above, value x30 following assembly code? add x30, x5, x6 add x30, x30, x5 2.10.6 [5] <§2.4> result x30 desired result, overflow? 2.11 Assume x5 holds value 128ten. 2.11.1 [5] <§2.4> instruction add x30, x5, x6 , range(s) values x6 would result overflow? 2.11.2 [5] <§2.4> instruction sub x30, x5, x6 , range(s) values x6 would result overflow? 2.11.3 [5] <§2.4> instruction sub x30, x6, x5 , range(s) values x6 would result overflow? 2.12 [5] <§§2.2, 2.5> Provide instruction type assembly language instruction following binary value: 0000 0000 0001 0000 1000 0000 1011 0011two Hint: Figure 2.20 may helpful. 2.13 [5] <§§2.2, 2.5> Provide instruction type hexadecimal representation following instruction: sd x5, 32(x30) 2.14 [5] <§2.5> Provide instruction type, assembly language instruction, binary representation instruction describe following RISC-V fields: opcode=0x33, funct3=0x0, funct7=0x20, rs2=5, rs1=7, rd=6 3602.15 [5] <§2.5> Provide instruction type, assembly language instruction, binary representation instruction describ ed following RISC-V fields: opcode=0x3, funct3=0x3, rs1=27, rd=3, imm=0x4 2.16 Assume would like expand RISC-V register file 128 registers expand instruction set contain four times many instructions. 2.16.1 [5] <§2.5> would affect size bit fields R-type instructions? 2.16.2 [5] <§2.5> would affect size bit fields I-type instructions? 2.16.3 [5] <§§2.5, 2.8, 2.10> could two proposed changes decrease size RISC-V assembly program? hand, could proposed change increase size RISC-V assembly program? 2.17 Assume following register contents: x5 = 0x00000000AAAAAAAA, x6 = 0x1234567812345678 2.17.1 [5] <§2.6> register values shown above, value x7 following sequence instructions? slli x7, x5, 4 orx7, x7, x6 2.17.2 [5] <§2.6> register values shown above, value x7 following sequence instructions? slli x7, x6, 4 2.17.3 [5] <§2.6> register values shown above, value x7 following sequence instructions? srli x7, x5, 3 andi x7, x7, 0xFEF 2.18 [10] <§2.6> Find shortest sequence RISC-V instructions extracts bits 16 11 register x5 uses value field replace bits 31 26 register x6 without changing bits registers x5 x6. (Be sure test code using x5 = 0 x6 = 0xffffffffffffffff . may reveal common oversight.) 2.19 [5] <§2.6> Provide minimal set RISC-V instructions may used implement following pseudoinstruction: x5, x6 // bit-wise invert 2.20 [5] <§2.6> following C statement, write minimal 361sequence RISC-V assembly instructions performs identical operation. Assume x6 = , x17 base address C. = C[0] << 4; 2.21 [5] <§2.7> Assume x5 holds value 0x00000000001010000 . value x6 following instructions? bge x5, x0, ELSE jal x0, DONE ELSE: ori x6, x0, 2 DONE: 2.22 Suppose program counter (PC) set 0x20000000 . 2.22.1 [5] <§2.10> range addresses reached using RISC-V jump-and-link (jal) instruction? (In words, set possible values PC jump instruction executes?) 2.22.2 [5] <§2.10> range addresses reached using RISC-V branch equal (beq) instruction? (In words, set possible values PC branch instruction executes?) 2.23 Consider proposed new instruction named rpt. instruction combines loop’s condition check counter decrement single instruction. example rpt x29, loop would following: (x29 > 0) { x29 = x29 −1; goto loop } 2.23.1 [5] <§2.7, 2.10> instruction added RISC-V instruction set, appropriate instruction format? 2.23.2 [5] <§2.7> shortest sequence RISC-V instructions performs operation? 2.24 Consider following RISC-V loop: LOOP: beqx6, x0, DONE addi x6, x6, -1 addi x5, x5, 2 jalx0, LOOP DONE: 2.24.1 [5] <§2.7> Assume register x6 initialized 362value 10. final value register x5 assuming x5 initially zero? 2.24.2 [5] <§2.7> loop above, write equivalent C code. Assume registers x5 x6 integers acc i, respectively. 2.24.3 [5] <§2.7> loop written RISC-V assembly above, assume register x6 initialized value N. many RISC-V instructions executed? 2.24.4 [5] <§2.7> loop written RISC-V assembly above, replace instruction “ beq x6, x0, DONE ” instruction “blt x6, x0, DONE ” write equivalent C code. 2.25 [10] <§2.7> Translate following C code RISC-V assembly code. Use minimum number instructions. Assume values a, b, i, j registers x5, x6, x7 , x29, respectively. Also, assume register x10 holds base address array D. for(i=0; i<a; i++) for(j=0; j<b; j++) D[4*j] = + j; 2.26 [5] <§2.7> many RISC-V instructions take implement C code Exercise 2.25 ? variables b initialized 10 1 elements initially 0, total number RISC-V instructions executed complete loop? 2.27 [5] <§2.7> Translate following loop C. Assume C-level integer held register x5,x6 holds C-level integer called result , x10 holds base address integer MemArray . addi x6, x0, 0 addi x29, x0, 100 LOOP: ldx7, 0(x10) addx5, x5, x7 addi x10, x10, 8 addi x6, x6, 1 bltx6, x29, LOOP 2.28 [10] <§2.7> Rewrite loop Exercise 2.27 reduce number RISC-V instructions executed. Hint: Notice variable used loop control. 2.29 [30] <§2.8> Implement following C code RISC-V 363assembly. Hint: Remember stack pointer must remain aligned multiple 16. int fib(int n){ (n==0) return 0; else (n == 1) return 1; else return fib(n−1) + fib(n−2); } 2.30 [20] <§2.8> function call Exercise 2.29 , show contents stack function call made. Assume stack pointer originally address 0x7ffffffc , follow register conventions specified Figure 2.11 . 2.31 [20] <§2.8> Translate function f RISC-V assembly language. Assume function declaration g int g(int a, int b) . code function f follows: int f(int a, int b, int c, int d){ return g(g(a,b), c+d); } 2.32 [5] <§2.8> use tail-call optimization function? no, explain not. yes, difference number executed instructions f without optimization? 2.33 [5] <§2.8> Right function f Exercise 2.31 returns, know contents registers x10-x14 , x8, x1, sp? Keep mind know entire function f looks like, function g know declaration. 2.34 [30] <§2.9> Write program RISC-V assembly convert ASCII string containing positive negative integer decimal string integer. program expect register x10 hold address null-terminated string containing optional “+” “−” followed combination digits 0 9. program compute integer value equivalent string digits, place number register x10. non-digit character appears anywhere string, program stop value −1 register x10. example, register x10 points sequence three bytes 50ten, 52ten, 0ten (the null-terminated string “24”), 364the program stops, register x10 contain value 24ten. RISC-V mul instruction takes two registers input. “muli ” instruction. Thus, store constant 10 register. 2.35 Consider following code: lb x6, 0(x7) sd x6, 8(x7) Assume register x7 contains address 0×10000000 data address 0×1122334455667788 . 2.35.1 [5] <§2.3, 2.9> value stored 0×10000008 big- endian machine? 2.35.2 [5] <§2.3, 2.9> value stored 0×10000008 little- endian machine? 2.36 [5] <§2.10> Write RISC-V assembly code creates 64- bit constant 0x1122334455667788two stores value register x10. 2.37 [10] <§2.11> Write RISC-V assembly code implement following C code atomic “set max” operation using lr.d/sc.d instructions. Here, argument shvar contains address shared variable replaced x x greater value points to: void setmax(int* shvar, int x) { // Begin critical section (x > *shvar) *shvar = x; // End critical section} } 2.38 [5] <§2.11> Using code Exercise 2.37 example, explain happens two processors begin execute critical section time, assuming processor executes exactly one instruction per cycle. 2.39 Assume given processor CPI arithmetic instructions 1, CPI load/store instructions 10, CPI branch instructions 3. Assume program following instruction breakdowns: 500 million arithmetic instructions, 300 million load/store instructions, 100 million branch instructions. 2.39.1 [5] <§§1.6, 2.13> Suppose new, powerful arithmetic instructions added instruction set. average, use powerful arithmetic instructions, reduce number arithmetic instructions needed execute program 25%, 365increasing clock cycle time 10%. good design choice? Why? 2.39.2 [5] <§§1.6, 2.13> Suppose find way double performance arithmetic instructions. overall speedup machine? find way improve performance arithmetic instructions 10 times? 2.40 Assume given program 70% executed instructions arithmetic, 10% load/store, 20% branch. 2.40.1 [5] <§§1.6, 2.13> Given instruction mix assumption arithmetic instruction requires two cycles, load/store instruction takes six cycles, branch instruction takes three cycles, find average CPI. 2.40.2 [5] <§§1.6, 2.13> 25% improvement performance, many cycles, average, may arithmetic instruction take load/store branch instructions improved all? 2.40.3 [5] <§§1.6, 2.13> 50% improvement performance, many cycles, average, may arithmetic instruction take load/store branch instructions improved all? 2.41 [10] <§2.19> Suppose RISC-V ISA included scaled offset addressing mode similar x86 one described Section 2.17 (Figure 2.35 ). Describe would use scaled offset loads reduce number assembly instructions needed carry function given Exercise 2.4 . 2.42 [10] <§2.19> Suppose RISC-V ISA included scaled offset addressing mode similar x86 one described Section 2.17 (Figure 2.35 ). Describe would use scaled offset loads reduce number assembly instructions needed implement C code given Exercise 2.7 . Answers Check §2.2, page 66: RISC-V, C, Java. §2.3, page 73: 2) slow. §2.4, page 80: 2) −8ten §2.5, page 89: 3) sub x11, x10, x9 §2.6, page 92: Both. mask pattern 1s leaves 0s 366everywhere desired field. Shifting left correct amount removes bits left field. Shifting righ appropriate amount puts field right-most bits f doubleword, 0s rest doubleword. Note leaves field originally, shift pair moves field rightmost part doubleword. §2.7, page 97: I. true. II. 1). §2.8, page 108: true. §2.9, page 113: I. 1) 2) II. 3). §2.10, page 121: I. 4) ±4 K. II. 4) ± 1 M. §2.11, page 124: true. §2.12, page 133: 4) Machine independence. 3673 Arithmetic Computers Abstract chapter describes computers handle arithmetic data. explains computer words (which composed bits) represented binary numbers. integers easily converte decimal binary form, fractions real numbers also converted, chapter shows. chapter also explains happens operation creates number bigger represented, answers question computer hardwar e multiplies divides numbers. Keywords Computer arithmetic; addition; subtraction; multiplication; ivision; associativity; x86; Arithmetic Logic Unit; ALU; exception; interrupt; dividend; di visor; quotient; remainder; scientific notation; normalized; floating point; fraction; exponent; overflow; underflow; double precision; single precision; guard; round; units last place; ULP; sticky bit; fused multiply add; matrix multiply; SIMD; subword parallelism Numerical precision soul science. Sir D’arcy Wentworth Thompson, Growth Form, 191 7 OUTLINE 3.1 Introduction 174 3683.2 Addition Subtraction 174 3.3 Multiplication 177 3.4 Division 183 3.5 Floating Point 191 3.6 Parallelism Computer Arithmetic: Subword Parallelism 216 3.7 Real Stuff: Streaming SIMD Extensions Advanced V ector Extensions x86 217 3.8 Going Faster: Subword Parallelism Matrix Multi ply 218 3.9 Fallacies Pitfalls 222 3.10 Concluding Remarks 225 3.11 Historical Perspective Reading 227 3.12 Exercises 227 369The Five Classic Components Computer 3.1 Introduction Computer words composed bits; thus, words represented binary numbers. Chapter 2 shows integers represented either decimal binary form, numbers commonly occur? example: fractions real numbers? happens operation creates number bigger represented? underlying questions mystery: hardware really multiply divide numbers? goal chapter unravel mysteries—including representation real numbers, arithmetic algorithms, hardware follows algorithms—and implications instruction sets. insights may explain quirks 370already encountered computers. Moreover, show use knowledge make arithmetic-intensive programs go much faster. 3.2 Addition Subtraction Addition would expect computers. Digits added bit bit right left, carries passed next digit left, would hand. Subtraction uses addition: appropriate operand simply negated added. Subtraction: Addition’s Tricky Pal No. 10, Top Ten Courses Athletes Football Fact ory, David Letterman et al., Book Top Ten Lists, 1990 Binary Addition Subtraction Example Let’s try adding 6ten 7ten binary subtracting 6ten 7ten binary. 4 bits right action; Figure 3.1 shows sums carries. Parentheses identify carries, arrows illustrating passed. FIGURE 3.1 Binary addition, showing carries 371right left. rightmost bit adds 1 0, resulting sum bit 1 carry bit 0. Hence, operation second digit right 0 +1 +1. generates 0 sum bit carry 1. third digit sum 1 +1 +1, resul ting carry 1 sum bit 1. fourth bit 1 +0 +0, yielding 1 sum carry. Answer Subtracting 6ten 7ten done directly: via addition using two’s complement representation −6: Recall overflow occurs result operation cannot represented available hardware, case 64-bit word. overflow occur addition? adding operands different signs, overflow cannot occur. reason sum must larger one operands. example, −10 +4 =−6. Since operands fit 64 bits sum larger operand, sum must fit 64 bits well. Therefore, overflow occur adding positive negative operands. similar restrictions occurrence overflow subtract, it’s opposite principle: igns operands , overflow cannot occur. see this, remember c−a=c+(−a) subtract negating second operand add. Therefore, subtract operands sign end adding operands different signs. prior paragraph, know overflow cannot 372occur case either. Knowing overflow cannot occur addition subtraction well good, detect occur? Clearly, adding subtracting two 64-bit numbers yield result needs 65 bits fully expressed. lack 65th bit means overflow occurs, sign bit set value result instead proper sign result. Since need one extra bit, sign bit c wrong. Hence, overflow occurs adding two positive numbers sum negative, vice versa. spurious sum means carry occurred sign bit. Overflow occurs subtraction subtract negative number positive number get negative result, subtract positive number negative number get positive result. ridiculous result means borrow occurre sign bit. Figure 3.2 shows combination operations, operands, results indicate overflow. FIGURE 3.2 Overflow conditions addition subtraction. seen detect overflow two’s complement numbers computer. overflow unsigned integers? Unsigned integers commonly used memory addresses overflows ignored. Fortunately, compiler easily check unsigned overflow using branch instruction. Addition overflowed sum less either addends, whereas subtraction overflowe difference greater minuend. Appendix describes hardware performs addition subtraction, called Arithmetic Logic Unit ALU . 373 Arithmetic Logic Unit (ALU) Hardware performs addition, subtraction, usually logical operations OR. Hardware/Software Interface computer designer must decide handle arithmetic overflows. Although languages like C Java ignore integer overflow, languages like Ada Fortran require program notified. programmer programming environment must decide overflow occurs. Summary major point section that, independent representation, finite word size computers means arithmetic operations create results large fit fixed word size. It’s easy detect overflow unsigned numbers, although almost always ignored programs don’t want detect overflow address arithmetic, th e common use natural numbers. Two’s complement presents greater challenge, yet software systems require recognizin g overflow, today computers way detect it. Check programming languages allow two’s complement integer arithmetic variables declared byte half, whereas RISC-V integer arithmetic operations full words. recall Chapter 2 , RISC-V data transfer operations bytes halfwords. RISC-V instructions generated byte halfword arithmetic operations? 1. Load lb, lh ; arithmetic add, sub, mul, div, using mask result 8 16 bits operation; store using sb, sh . 2. Load lb, lh ; arithmetic add, sub, mul, div ; store using sb, sh . 374 Elaboration One feature generally found general-purpose microprocessors saturating operations. Saturation means calculation overflows, result set largest posi tive number negative number, rather modulo calculation two’s complement arithmetic. Saturation likel want media operations. example, volume knob radio set would frustrating if, turned it, volume would get continuously louder immediately soft. knob saturation would stop highest volume matter far turned it. Multimedia extensions standard instruction sets often offer saturating arithmetic. Elaboration speed addition depends quickly carry high-order bits computed. variety schemes anticipate carry worst-case scenario function log2 number bits adder. anticipatory signals faster go fewer gates sequence, takes many gates anticipate proper carry. popular carry lookahead , Section A.6 Appendix describes. 3.3 Multiplication completed explanation addition subtraction, ready build vexing operation multiplication. Multiplication vexation, Division bad; rule three doth puzzle me, practice drives mad. Anonymous, Elizabethan manuscript, 1570 First, let’s review multiplication decimal numbers longhand remind steps multiplication names operands. reasons become clear shortly, 375we limit decimal example using digits 0 1. Multiplying 1000ten 1001ten: first operand called multiplicand second multiplier . final result called product . may recall, algorithm learned grammar school take digits multiplier one time right left, multiplying multiplicand single digit multiplier, shifting th e intermediate product one digit left earlier interm ediate products. first observation number digits product considerably larger number either multiplicand r multiplier. fact, ignore sign bits, length multiplication n-bit multiplicand m-bit multiplier product n+m bits long. is, n+m bits required represent possible products. Hence, like add, multiply must c ope overflow frequently want 64-bit product result multiplying two 64-bit numbers. example, restricted decimal digits 0 1. two choices, step multiplication simple: 1. place copy multiplicand (1 ×multiplicand) proper place multiplier digit 1, 3762. Place 0 (0 ×multiplicand) proper place digit 0. Although decimal example happens use 0 1, multiplication binary numbers must always use 0 1, thus always offers two choices. reviewed basics multiplication, traditional next step provide highly optimized multipl hardware. break tradition belief gain better understanding seeing evolution multiply hardware algorithm multiple generations. now, let’s assume multiplying positive numbers. Sequential Version Multiplication Algorithm Hardware design mimics algorithm learned grammar school; Figure 3.3 shows hardware. drawn hardware data flow top bottom resemble closely paper-and-pencil method. FIGURE 3.3 First version multiplication hardware. Multiplicand register, ALU, Product register 128 bits wide, Multiplier regist er containing 64 bits. ( Appendix describes ALUs.) 64-bit multiplicand starts right half 377Multiplicand register shifted left 1 bit step. multiplier shifted opposite directio n step. algorithm starts product initialized 0. Control decides shift Multiplicand Multiplier registers writ e new values Product register. Let’s assume multiplier 64-bit Multiplier regis ter 128-bit Product register initialized 0. paper-and-pencil example above, it’s clear need move multiplicand left one digit step, may added intermediate products. 64 steps, 64-bit multiplicand would move 64 bits left. Hence, need 128-bit Multiplicand register, initialized 64-bit multiplicand n right half zero left half. register shifted le ft 1 bit step align multiplicand sum accumulated 128-bit Product register. Figure 3.4 shows three basic steps needed bit. least significant bit multiplier (Multiplier0) determi nes whether multiplicand added Product register. le ft shift step 2 effect moving intermediate operands left, multiplying paper pencil. shift right step 3 gives us next bit multiplier examine n following iteration. three steps repeated 64 times obtain product. step took clock cycle, algorithm would require almost 200 clock cycles multiply two 64-bit numbers. relative importance arithmetic operations like multiply varies program, addition subtraction may anywhere 5 100 times popular multiply. Accordingly, many applications, multiply take several clock cycles without significantly affecting performance. However, Amdahl’s Law (see Section 1.10 ) reminds us even moderate frequency slow operation limit performance. 378FIGURE 3.4 first multiplication algorithm, using hardware shown Figure 3.3 . least significant bit multiplier 1, ad multiplicand product. not, go next ste p. Shift multiplicand left multiplier right next two steps. three steps repeated 64 times. 379This algorithm hardware easily refined take one clock cycle per step. speed comes performing operations parallel: multiplier multiplicand shifted multiplicand added product multiplier bit 1. Th e hardware ensure tests right bit multiplier gets preshifted version multiplican d. hardware usually optimized halve width adder registers noticing unused portions registers adders. Figure 3.5 shows revised hardware. FIGURE 3.5 Refined version multiplication hardware. Compare first version Figure 3.3 . Multiplicand register ALU reduced 64 bits. product shifted right. separate Multiplier register also disappeared. multiplier placed instead right half Product registe r, grown one bit 129 bits hold carry-out adder. changes highlighted color. Hardware/Software Interface Replacing arithmetic shifts also occur multiplying constants. compilers replace multiplies short constants series shifts adds. one bit left represents number twice large base 2, shifting bits left 380has effect multiplying power 2. mentioned Chapter 2 , almost every compiler perform strength reduction optimization substituting left shift multip ly power 2. Multiply Algorithm Example Using 4-bit numbers save space, multiply 2ten × 3ten, 0010two × 0011two. Answer Figure 3.6 shows value register steps labeled according Figure 3.4 , final value 0000 0110two 6ten. Color used indicate register values change step, bit circled one examined determine operation next step. FIGURE 3.6 Multiply example using algorithm Figure 3.4 . bit examined determine next step circled color. Signed Multiplication far, dealt positive numbers. easiest way 381understand deal signed numbers first convert th e multiplier multiplicand positive numbers remember original signs. algorithms next ru n 31 iterations, leaving signs calculation. learned grammar school, need negate product original signs disagree. turns last algorithm work signed numbers , remember dealing numbers infinite digits, representing 64 bits. Hence, shifting steps would need extend sign product signed numbers. algorithm completes, lower doubleword would 64-bit product. Faster Multiplication Moore’s Law provided much resources hardware designers build much faster multiplication hardware. Whether multiplicand added known beginning multiplication looking 64 multiplier bits. Faster multiplications possible essent ially providing one 64-bit adder bit multiplier: one inpu multiplicand ANDed multiplier bit, output prior adder. 382A straightforward approach would connect outputs adders right inputs adders left, making stack adders 64 high. alternative way organize 64 additions parallel tree, Figure 3.7 shows. Instead waiting 64 add times, wait log2 (64) six 64-bit add times. FIGURE 3.7 Fast multiplication hardware. Rather use single 64-bit adder 63 times, hardware “unrolls loop” use 63 adders organizes minimize delay. fact, multiply go even faster six add times use carry save adders (see Section A.6 Appendix ), easy pipeline design able support many multiplies simultaneously (see Chapter 4 ). 383Multiply RISC-V produce properly signed unsigned 128-bit product, RISC-V four instructions: multiply (mul), multiply high (mulh ), multiply high unsigned (mulhu ), multiply high signed-unsigned (mulhsu ). get integer 64-bit product, programmer uses mul. get upper 64 bits 128-bit product, programmer uses ( mulh ) operands signed, ( mulhu ) operands unsigned, (mulhsu ) one operand signed unsigned. Summary Multiplication hardware simply shifts adds, derived paper-and-pencil method learned grammar school. Compilers even use shift instructions multiplications powers 2. much hardware adds parallel , much faster. 384 Hardware/Software Interface Software use multiply-high instructions check overflow 64-bit multiplication. overflow 64-b unsigned multiplication mulhu ’s result zero. overflow 64-bit signed multiplication bits mulh ’s result copies sign bit mul’s result. 3.4 Division reciprocal operation multiply divide, operation even less frequent even quirkier. even offers opport unity perform mathematically invalid operation: dividing 0. Divide et impera. Latin “Divide rule,” ancient political maxim cit ed Machiavelli, 1532 Let’s start example long division using decimal numbers recall names operands division algorithm grammar school. reasons similar previous section, limit decimal digits 0 1. example dividing 1,001,010ten 1000ten: 385Divide’s two operands, called dividend divisor , result, called quotient , accompanied second result, called remainder . another way express relationship components: dividend number divided. divisor number dividend divided by. quotient primary result division; number multiplied divisor added remainder produces dividend. remainder secondary result division; number added product quotient divisor produces divid end. remainder smaller divisor. Infrequently, programs use divide instruction get remainder, 386ignoring quotient. basic division algorithm grammar school tries see big number subtracted, creating digit quotient attempt. carefully selected decimal example uses numbers 0 1, it’s easy figure many times divisor goes portion dividend: it’s either 0 tim es 1 time. Binary numbers contain 0 1, binary division restricted two choices, thereby simplifying binary div ision. Let’s assume dividend divisor positive hence quotient remainder nonnegative. division operands results 64-bit values, ignore sign now. Division Algorithm Hardware Figure 3.8 shows hardware mimic grammar school algorithm. start 64-bit Quotient register set 0. iteration algorithm needs move divisor right one digit, start divisor placed left half 128- bit Divisor register shift right 1 bit step align w ith dividend. Remainder register initialized dividend. FIGURE 3.8 First version division hardware. Divisor register, ALU, Remainder register 387all 128 bits wide, Quotient register bei ng 62 bits. 64-bit divisor starts left half Divisor register shifted right 1 bit iteratio n. remainder initialized dividend. Contro l decides shift Divisor Quotient registers write new value Remainder register. Figure 3.9 shows three steps first division algorithm. Unlike human, computer isn’t smart enough know advance whether divisor smaller dividend. must first subtract divisor step 1; remember performed comparison. result positive, divisor smaller equal dividend, generate 1 quotient (step 2a). result negative, next step restore original value adding divisor back remainder generate 0 quotient (step 2b). divisor shifted right, iterate again. remainder quotient found namesake registers iterations complete. 388FIGURE 3.9 division algorithm, using hardware Figure 3.8 . remainder positive, divisor go th e dividend, step 2a generates 1 quotient. negative remainder step 1 means divisor go dividend, step 2b generates 0 quotient adds divisor remainder, thereby reversing subtraction step 1. final 389shift, step 3, aligns divisor properly, relative dividend next iteration. steps repeated 65 times. Divide Algorithm Example Using 4-bit version algorithm save pages, let’s try dividing 7ten 2ten, 0000 0111two 0010two. Answer Figure 3.10 shows value register steps, quotient 3ten remainder 1ten. Notice test step 2 whether remainder positive negative simply checks whether sign bit Remainder register 0 1. surprising requirement algorithm takes n + 1 steps get proper quotient remainder. FIGURE 3.10 Division example using algorithm Figure 3.9 . bit examined determine next step circled color. 390This algorithm hardware refined faster cheaper. speed-up comes shifting operands quotient simultaneously subtraction. refinemen halves width adder registers noticing unused portions registers adders. Figure 3.11 shows revised hardware. FIGURE 3.11 improved version division hardware. Divisor register, ALU, Quotient register al l 64 bits wide. Compared Figure 3.8 , ALU Divisor registers halved remainder shifted left. version also combines Quotient register right half Remainder registe r. Figure 3.5 , Remainder register grown 129 bits make sure carry adder lost. Signed Division far, ignored signed numbers division. simplest solution remember signs divisor dividend negate quotient signs disagree. Elaboration one complication signed division must also set 391sign remainder. Remember following equation must always hold: understand set sign remainder, let’s look example dividing combinations ±7ten ±2ten. first case easy: Checking results: change sign dividend, quotient must change well: Rewriting basic formula calculate remainder: So, Checking results again: reason answer isn’t quotient −4 remainder +1, would also fit formula, absolute value quotient would change depending sign 392dividend divisor! Clearly, programming would even greater challenge. anomalous behavior avoided following rule dividend remainder must identical signs, matter signs divisor quotient. calculate combinations following rule: Thus, correctly signed division algorithm negates quoti ent signs operands opposite makes sign nonzero remainder match dividend. Faster Division Moore’s Law applies division hardware well multiplication, would like able speed division b throwing hardware it. used many adders speed multiply, cannot trick divide. reason need know sign difference perform next step algorithm, whereas multiply could calculate 64 partial products immediately. 393There techniques produce one bit quotient per step. SRT division technique tries predict several quotient bits per step, using table lookup based upper bits dividend remainder. relies subsequent steps correct wrong predictions. typical value today 4 bits. key guessing value subtract. binary division, single choice. algorithms use 6 bits remainder 4 bits divisor index table determines guess fo r step. 394The accuracy fast method depends proper values lookup table. Fallacy page 224 Section 3.8 shows happen table incorrect. Divide RISC-V may already observed sequential hardware used multiply divide Figures 3.5 3.11. requirement 128-bit register shift left rig ht 64-bit ALU adds subtracts. handle signed integers unsigned integers, RISC-V two instructions division two instructions remainder: divide (div), divide unsigned (divu ), remainder (rem), remainder unsigned (remu ). Summary common hardware support multiply divide allows RISC-V provide single pair 64-bit registers used bot h multiply divide. accelerate division predicting 395multiple quotient bits correcting mispredictions lat er. Figure 3.12 summarizes enhancements RISC-V architecture last two sections. Hardware/Software Interface RISC-V divide instructions ignore overflow, software must determine whether quotient large. addition overflow, division also result improper calculation: division 0. computers distinguish two anomalous events. RISC-V software must check divisor discover division 0 well overflow. Elaboration even faster algorithm immediately add divisor back remainder negative. simply adds dividend shifted remainder following step, since ( r+d)×2 − d=r − 2 + × 2 −d=r × 2 + d. nonrestoring division algorithm, takes one clock cycle per step, explored exercises; algorithm called restoring division. third algorithm doesn’t save result subtract it’s negative called nonperforming division algorithm. averages one-third fewer arithmetic operations. 396FIGURE 3.12 RISC-V core architecture. RISC-V machine language listed RISC-V Reference Data Card front book. 3973.5 Floating Point Going beyond signed unsigned integers, programming languages support numbers fractions, called reals mathematics. examples reals: Speed gets nowhere you’re headed wrong way. American proverb 3.14159265… ten (pi) 2.71828… ten (e) 0.000000001ten 1.0ten ×10−9 (seconds nanosecond) 3,155,760,000ten 3.15576ten × 109 (seconds typical century) Notice last case, number didn’t represent small fraction, bigger could represent 32-bit signed integer. alternative notation last two numbers called scientific notation , single digit left decimal point. number scientific notation leading 0s called normalized number, usual way write it. example, 1.0ten × 10−9 normalized scientific notation, 0.1ten × 10−8 10.0ten × 10−10 not. scientific notation notation renders numbers single digit left decimal point. normalized number floating-point notation leading 0s. show decimal numbers scientific notation, also show binary numbers scientific notation: keep binary number normalized form, need base increase decrease exactly number bits 398number must shifted one nonzero digit left decimal point. base 2 fulfills need. Since base 10, also need new name decimal point; binary point fine. Computer arithmetic supports numbers called floating point represents numbers binary point fixed, integers. programming language C uses name float numbers. scientific notation, numbers represented single nonzero digit left th e binary point. binary, form (Although computer represents exponent base 2 well rest number, simplify notation show exponent decimal.) floating point Computer arithmetic represents numbers binary point fixed. standard scientific notation reals normalized form offers three advantages. simplifies exchange data include floating-point numbers; simplifies floating-point arith metic algorithms know numbers always form; increases accuracy numbers stored word, since real digits right binary point replace unnecessary leading 0s. Floating-Point Representation designer floating-point representation must find compromise size fraction size exponent , fixed word size means must take bit one add bit other. tradeoff precisio n range: increasing size fraction enhances precision fraction, increasing size exponent increases range numbers represented. design 399guideline Chapter 2 reminds us, good design demands good compromise. fraction value, generally 0 1, placed fraction field. fraction also called mantissa . exponent numerical representation system floating-point arithme tic, value placed exponent field. Floating-point numbers usually multiple size word. representation RISC-V floating-point number shown below, sign floating-point number (1 meaning negative), exponent value 8-bit exponent field (including sign exponent), fraction 23-bit number. recall Chapter 2 , representation sign magnitude , since sign separate bit rest number. general, floating-point numbers form F involves value fraction field E involves value exponent field; exact relationship fields spelled soon. (We shortly see RISC-V somethin g slightly sophisticated.) chosen sizes exponent fraction give RISC-V computer arithmetic extraordinary range. Fractions almost small 2.0ten × 10−38 numbers almost large 2.0ten × 1038 represented computer. Alas, extraordinary differs infinite, still possible numbers large. Thus, 400overflow interrupts occur floating-point arithmetic wel l integer arithmetic. Notice overflow means exponent large represented exponent field. overflow (floating-point) situation positive exponent becomes large fit n exponent field. Floating point offers new kind exceptional event well. programmers want know calculated number large represented, want know nonzero fraction calculating become small cannot represented; either event could result program giving incorrect answers. distinguish overflow, cal l event underflow . situation occurs negative exponent large fit exponent field. underflow (floating-point) situation negative exponent becomes large fit exponent field. One way reduce chances underflow overflow offer another format larger exponent. C, number called double , operations doubles called double precision floating-point arithmetic; single precision floating point name earlier format. double precision floating-point value represented 64-bit doubleword. single precision floating-point value represented 32-bit word. representation double precision floating-point numbe r takes one RISC-V doubleword, shown below, still sign number, exponent value 11-bit exponent field, fraction 52-bit number fraction field. 401RISC-V double precision allows numbers almost small 2.0ten × 10−308 almost large 2.0ten × 10308. Although double precision increase exponent range, primary advantage greater precision much larger fraction. Exceptions Interrupts happen overflow underflow let user know problem occurred? computers signal event raising exception , sometimes called interrupt . exception interrupt essentially unscheduled procedu call. address instruction overflowed saved regis ter, computer jumps predefined address invoke appropriate routine exception. interrupted address saved situations program continue corrective code executed. ( Section 4.9 covers exceptions detail; Chapter 5 describes situations exceptions interrupts occur.) RISC-V computers raise exception overflow underflow; instead, software read floating-point control status register (fcsr) check whether overflow underflow occurred. exception Also called interrupt . unscheduled event disrupts program execution; used detect overflow. interrupt exception comes outside processor. (Some architectures use term interrupt exceptions.) IEEE 754 Floating-Point Standard 402These formats go beyond RISC-V. part IEEE 754 floating-point standard , found virtually every computer invented since 1980. standard greatly improved ease porting floating-point programs quality computer arithmetic. pack even bits number, IEEE 754 makes leading 1 bit normalized binary numbers implicit. Hence, number actually 24 bits long single precision (implied 1 23-bit fraction), 53 bits long double precision (1 + 52). precise, use term significand represent 24- 53-bit number 1 plus fraction, fraction mean 23- 52-bit number. Since 0 leading 1, given reserved exponent value 0 hardware won’t attach leading 1 it. Thus 00 … 00two represents 0; representation rest numbers uses form hidden 1 added: bits fraction represent number 0 1 E specifies value exponent field, given det ail shortly. number bits fraction left right s1, s2, s3, …, value Figure 3.13 shows encodings IEEE 754 floating-point numbers. features IEEE 754 special symbols represent unusual events. example, instead interrupting divide 0, software set result bit pattern representing +∞ −∞; largest exponent reserved special symbol s. programmer prints results, program output infinity symbol. (For mathematically trained, purpose infinity form topological closure reals.) 403FIGURE 3.13 IEEE 754 encoding floating-point numbers. separate sign bit determines sign. Denormalized numbers described Elaboration page 216. information also found Column 4 RISC - V Reference Data Card front book. IEEE 754 even symbol result invalid operations, 0/0 subtracting infinity infinity. symbol NaN , Number . purpose NaNs allow programmers postpone tests decisions later time program convenient. designers IEEE 754 also wanted floating-point representation could easily processed integer comparisons, especially sorting. desire sign significant bit, allowing quick test less than, greater than, equal 0. (It’s little complicated simple integer sort, since notation essentially sign magnitude rather two’s complement.) Placing exponent significand also simplifies th e sorting floating-point numbers using integer comparison instructions, since numbers bigger exponents look large r numbers smaller exponents, long exponents sign. Negative exponents pose challenge simplified sorting. use two’s complement notation negative exponents 1 significant bit exponent field, negative exponent look like big number. example, 1.0two × 2−1 would represented single precision 404(Remember leading 1 implicit significand.) value 1.0two × 2+1 would look like smaller binary number desirable notation must therefore represent negative exponent 00 … 00two positive 11 … 11two. convention called biased notation , bias number subtracted normal, unsigned representation determine real value. IEEE 754 uses bias 127 single precision, exponent −1 represented bit pattern value −1 +127ten, 126ten = 0111 1110two, +1 represented 1 +127, 128ten = 1000 0000two. exponent bias double precision 1023. Biased exponent means value represented floating-point number really range single precision numbers small large Let’s demonstrate. Example Show IEEE 754 binary representation number −0.75ten single double precision. 405Answer number −0.75ten also also represented binary fraction scientific notation, value normalized scientific notation, general representation single precision number Subtracting bias 127 exponent −1.1two × 2−1 yields single precision binary representation −0.75ten double precision representation 406Now let’s try going direction. Converting Binary Decimal Floating Point Example decimal number single precision float represent? Answer sign bit 1, exponent field contains 129, fraction field contains 1 ×2−2 = 1/4, 0.25. Using basic equation, next subsections, give algorithms floating-point addition multiplication. core, us e corresponding integer operations significands, ex tra bookkeeping necessary handle exponents normalize result. first give intuitive derivation algorithm decimal give detailed, binary version figures. 407 Elaboration Following IEEE guidelines, IEEE 754 committee reformed 20 years standard see changes, any, made. revised standard IEEE 754-2008 includes nearly IEEE 754-1985 adds 16-bit format (“half precision”) 128-bit format (“quadruple precision”). revised standard also adds decimal floating point arithmetic. Elaboration attempt increase range without removing bits significand, computers IEEE 754 standard used base 2. example, IBM 360 370 mainframe computers use base 16. Since changing IBM exponent one means shifting significand 4 bits, “normalized” base 16 numbers 3 leading bits 0s! Hence, hexadecimal digits mean 3 bits must dropped significand, leads surprising problems accuracy floating- point arithmetic. IBM mainframes support IEEE 754 well old hex format. Floating-Point Addition Let’s add numbers scientific notation hand illustrate problems floating-point addition: 9.999ten × 101 + 1.610ten × 10−1. Assume store four decimal digits significan two decimal digits exponent. Step 1. able add numbers properly, must align decimal point number smaller exponent. Hence, need form smaller number, 1.610ten × 10−1, matches larger exponent. obtain observing multiple representations unnormalized floating- point number scientific notation: number right version desire, since exponent matches exponent larger number, 9.999ten × 101. Thus, first step shifts 408significand smaller number right corrected exponent matches larger number. represent four decimal digits so, shifting, number really Step 2. Next comes addition significands: sum 10.015ten × 101. Step 3. sum normalized scientific notation, need adjust it: Thus, addition may shift sum put normalized form, adjusting exponent appropriately. example shows shifting right, one number positive negative, would possible sum many leading 0s, requiring left shifts. Whenever exponent increased decreased, must check overflow underflow—that is, must make sure exponent still fits field. Step 4. Since assumed significand could four digits long (excluding sign), must round number. grammar school algorithm, rules truncate number digit right desired point 0 4 add 1 digit number right 5 9. number rounded four digits significand 409since fourth digit right decimal point 5 9. Notice bad luck rounding, adding 1 string 9s, sum may longer normalized would need perform step 3 again. Figure 3.14 shows algorithm binary floating-point addition follows decimal example. Steps 1 2 similar example discussed: adjust significand number smaller exponent add two significands. Step 3 normalizes results, forcing check overflow underflow. test overflow underflow tep 3 depends precision operands. Recall pattern 0 bits exponent reserved used floating- point representation zero. Moreover, pattern 1 bits exponent reserved indicating values situations outside scope normal floating-point numbers (see Elaboration page 216). example below, remember single precision, maximum exponent 127, minimum exponent −126. Binary Floating-Point Addition Example Try adding numbers 0.5ten −0.4375ten binary using algorithm Figure 3.14 . 410411FIGURE 3.14 Floating-point addition. normal path execute steps 3 4 once, rounding causes sum unnormalized, must repeat step 3. Answer Let’s first look binary version two numbers normalized scientific notation, assuming keep 4 bits precision: follow algorithm: Step 1. significand number lesser exponent (−1.11two × 2−2) shifted right exponent matches larger number: Step 2. Add significands: Step 3. Normalize sum, checking overflow underflow: Since 127 ≥−4 ≥−126, overflow underflow. (The biased exponent would −4 +127, 123, 1 254, smallest largest unreserved biased exponents.) 412Step 4. Round sum: sum already fits exactly 4 bits, change bits due rounding. sum sum would expect adding 0.5ten −0.4375ten. Many computers dedicate hardware run floating-point operations fast possible. Figure 3.15 sketches basic organization hardware floating-point addition. 413FIGURE 3.15 Block diagram arithmetic unit dedicated floating-point addition. steps Figure 3.14 correspond block, top bottom. First, exponent one operan subtracted using small ALU determine larger much. difference controls three multiplexors; left right, select larger exponent, significand f smaller number, significand larger number. smaller significand shifted right, significands added together using big ALU. normalization step shifts sum left right increments decrements exponent. Rounding creates final result, may require normalizing produce actual final result. 414Floating-Point Multiplication explained floating-point addition, let’s try floating-point multiplication. start multiplying decimal numbers scientific notation hand: 1.110ten × 1010 × 9.200ten × 10−5. Assume store four digits significand two digits exponent. Step 1. Unlike addition, calculate exponent product simply adding exponents operands together: Let’s biased exponents well make sure obtain result: 10 +127 =137, −5 +127 =122, result large 8-bit exponent field, something amiss! problem bias adding biases well exponents: Accordingly, get correct biased sum add biased numbers, must subtract bias sum : 5 indeed exponent calculated initially. Step 2. Next comes multiplication significands: 415There three digits right decimal point operand, decimal point placed six digits right product significand: keep three digits right decimal point, product 10.212 ×105. Step 3. product unnormalized, need normalize it: Thus, multiplication, product shifted right one digit put normalized form, adding 1 exponent. point, check overflow underflow. Underflow may occur operands small— is, large negative exponents. Step 4. assumed significand four digits long (excluding sign), must round number. number 416is rounded four digits significand Step 5. sign product depends signs origin al operands. same, sign positive; otherwise, it’s negative. Hence, product sign sum addition algorithm determined addition significands, multiplication, signs operands determine sign product. again, Figure 3.16 shows, multiplication binary floating-point numbers quite similar steps completed. start calculating new exponent product adding biased exponents, sure subtract one bias get proper result. Next multiplication f significands, followed optional normalization step. size exponent checked overflow underflow, product rounded. rounding leads normalization, check exponent size. Finally, set sign bit 1 signs operands different (negative product) 0 (positive product). 417418FIGURE 3.16 Floating-point multiplication. normal path execute steps 3 4 once, rounding causes sum unnormalized, must repeat step 3. Binary Floating-Point Multiplication Example Let’s try multiplying numbers 0.5ten −0.4375ten, using steps Figure 3.16 . Answer binary, task multiplying 1.000two × 2−1 −1.110two × 2−2. Step 1. Adding exponents without bias: or, using biased representation: Step 2. Multiplying significands: 419The product 1.110000two × 2−3, need keep 4 bits, 1.110two × 2−3. Step 3. check product make sure normalized, check exponent overflow underflow. product already normalized and, since 127 ≥−3 ≥−126, overflow underflow. (Using biased representation, 254 ≥124 ≥1, exponent fits.) Step 4. Rounding product makes change: Step 5. Since signs original operands differ, make sig n product negative. Hence, product Converting decimal check results: 420The product 0.5ten −0.4375ten indeed −0.21875ten. Floating-Point Instructions RISC-V RISC-V supports IEEE 754 single-precision double- precision formats instructions: Floating-point addition, single (fadd.s ) addition, double (fadd.d ) Floating-point subtraction, single (fsub.s ) subtraction, double (fsub.d ) Floating-point multiplication, single (fmul.s ) multiplication, double (fmul.d ) Floating-point division, single (fdiv.s ) division, double (fdiv.d ) Floating-point square root, single ( fsqrt.s ) square root, double ( fsqrt.d ) Floating-point equals, single ( feq.s ) equals, double ( feq.d ) Floating-point less-than, single ( flt.s ) less-than, double (flt.d ) Floating-point less-than-or-equals, single ( fle.s ) less-than-or- equals, double ( fle.d ) comparison instructions, feq, flt, fle, set integer register 0 comparison false 1 true. Software thus branch result floating-point comparison using th e integer branch instructions beq bne. RISC-V designers decided add separate floating-point registers. called f0, f1, …, f31. Hence, included separate loads stores floating-point registers: fld fsd double-precision flw fsw single-precision. base registers floating-point data transfers used addresses remain integer registers. RISC-V code load two single precision numbers memory, add them, store sum might look like this: flw f0, 0(x10) // Load 32-bit F.P. number f0 flw f1, 4(x10) // Load 32-bit F.P. number f1 fadd.s f2, f0, f1 // f2 = f0 + f1, single precisio n fsw f2, 8(x10) // Store 32-bit F.P. number f2 421A single precision register lower half double- precision register. Note that, unlike integer register x0, floating- point register f0 hard-wired constant 0. Figure 3.17 summarizes floating-point portion RISC-V architecture revealed chapter, new pieces sup port floating point shown color. floating-point instructions use format integer counterparts: loads use I-type format, stores use S-type format, arithmetic instructions use R-type format. FIGURE 3.17 RISC-V floating-point architecture revealed thus far. information also found column 2 RISC- V Reference Data Card front book. Hardware/Software Interface One issue architects face supporting floating-point arithmetic whether select registers used 422integer instructions add special set floating point. programs normally perform integer operations floating-point operations different data, separating regist ers slightly increase number instructions neede execute program. major impact create distinct set data transfer instructions move data floating-point registers memory. benefits separate floating-point registers twic e many registers without using bits instruction format, twice register bandwidth separate integer floating-point register sets, able custo mize registers floating point; example, computers conver sized operands registers single internal format. Compiling Floating-Point C Program RISC-V Assembly Code Example Let’s convert temperature Fahrenheit Celsius: float f2c (float fahr) { return ((5.0f/9.0f) *(fahr – 32.0f)); } Assume floating-point argument fahr passed f10 result also go f10. RISC-V assembly code? Answer assume compiler places three floating-point constants memory within easy reach register x3. first two instructions load constants 5.0 9.0 floating-point registers: f2c: flw f0, const5(x3) // f0 = 5.0f flw f1, const9(x3) // f1 = 9.0f divided get fraction 5.0/9.0: fdiv.s f0, f0, f1 // f0 = 5.0f / 9.0f (Many compilers would divide 5.0 9.0 compile time save single constant 5.0/9.0 memory, thereby avoiding 423divide runtime.) Next, load constant 32.0 subtract fahr (f10): flw f1, const32(x3) // f1 = 32.0f fsub.s f10, f10, f1 // f10 = fahr – 32.0f Finally, multiply two intermediate results, placing product f10 return result, return fmul.s f10, f0, f10 // f10 = (5.0f / 9.0f)*(fahr – 32.0f) jalr x0, 0(x1) // return let’s perform floating-point operations matrices, code commonly found scientific programs. Compiling Floating-Point C Procedure Two-Dimensional Matrices RISC-V Example floating-point calculations performed double precis ion. Let’s per-form matrix multiply C =C +A * B. commonly called DGEMM , Double precision, General Matrix Multiply. We’ll see versions DGEMM Section 3.8 subsequently Chapters 4 , 5, 6. Let’s assume C, A, B square matrices 32 elements dimension. void mm (double c[][], double a[][], double b[][]) { size_t i, j, k; (i = 0; < 32; = + 1) (j = 0; j < 32; j = j + 1) (k = 0; k < 32; k = k + 1) c[i][j] = c[i][j] + a[i][k] *b[k][j]; } array starting addresses parameters, x10, x11, x12. Assume integer variables x5, x6, x7, respectively. RISC-V assembly code body procedure? Answer Note c[i][j] used innermost loop above. Since loop index k, index affect c[i][j] , avoid loading storing c[i][j] iteration. Instead, compiler loads c[i][j] register outside loop, accumulates sum 424of products a[i][k] b[k][j] register, stores sum c[i][j] upon termination innermost loop. keep code simpler using assembly language pseudoinstruction li, loads constant register. body procedure starts saving loop termination value 32 temporary register initializing three loop variables: mm:... li x28, 32 // x28 = 32 (row size/loop end) li x5, 0 // = 0; initialize 1st loop L1: li x6, 0 // j = 0; initialize 2nd loop L2: li x7, 0 // k = 0; initialize 3rd loop calculate address c[i][j] , need know 32 ×32, two-dimensional array stored memory. might expect, layout 32 single-dimensional arrays, 32 elements. first step skip “single-dimensional arrays,” rows, get one want. Thus, multiply index first dimension size r ow, 32. Since 32 power 2, use shift instead: slli x30, x5, 5 // x30 = * 25(size row c) add second index select jth element desired row: add x30, x30, x6 // x30 = * size(row) + j turn sum byte index, multiply size matrix element bytes. Since element 8 bytes double precision, instead shift left three: slli x30, x30, 3 // x30 = byte offset [i][j] Next add sum base address c, giving address c[i][j] , load double precision number c[i][j] f0: add x30, x10, x30 // x30 = byte address c[i][j] fld f0, 0(x30) // f0 = 8 bytes c[i][j] following five instructions virtually identical las five: calculate address load double precision number b[k][j] . L3: slli x29, x7, 5 // x29 = k * 25(size row b) add x29, x29, x6 // x29 = k * size(row) + j slli x29, x29, 3 // x29 = byte offset [k][j] 425 add x29, x12, x29 // x29 = byte address b[k][j] fld f1, 0(x29) // f1 = 8 bytes b[k][j] Similarly, next five instructions like last five: calcu late address load double precision number a[i][k] . slli x29, x5, 5 // x29 = * 25(size row a) add x29, x29, x7 // x29 = * size(row) + k slli x29, x29, 3 // x29 = byte offset [i][k] add x29, x11, x29 // x29 = byte address a[i][k] fld f2, 0(x29) // f2 = a[i][k] loaded data, finally ready floating-point operations! multiply elements b located registers f2 f1, accumulate sum f0. fmul.d f1, f2, f1 // f1 = a[i][k] * b[k][j] fadd.d f0, f0, f1 // f0 = c[i][j] + a[i][k] * b[k][j] final block increments index k loops back index 32. 32, thus end innermost loop, need store sum accumulated f0 c[i][j] . addi x7, x7, 1 // k = k + 1 bltu x7, x28, L3 // (k < 32) go L3 fsd f0, 0(x30) // c[i][j] = f0 Similarly, final six instructions increment index variable middle outermost loops, looping back index 32 exiting index 32. addi x6, x6, 1 // j = j + 1 bltu x6, x28, L2 // (j < 32) go L2 addi x5, x5, 1 // = + 1 bltu x5, x28, L1 // (i < 32) go L1 . . . Looking ahead, Figure 3.20 shows x86 assembly language code slightly different version DGEMM Figure 3.19. Elaboration C many programming languages use array layout discussed example, called row-major order . Fortran instead uses column-major order , whereby array stored column column. Elaboration 426Another reason separate integers floating-point register microprocessors 1980s didn’t enough transistors put floating-point unit chip integer unit. Hence, floating-point unit, including floating-point registers, optionally available second chip. optional accelerator chips called coprocessor chips . Since early 1990s, microprocessors integrated floating point (and everything else) chip, thus term coprocessor chip joins accumulator core memory quaint terms date speaker. Elaboration mentioned Section 3.4 , accelerating division challenging multiplication. addition SRT, another technique leverage fast multiplier Newton’s iteration , division recast finding zero function produce reciprocal 1/ c, multiplied operand. Iteration techniques cannot rounded properly without calculating many extra bits. TI chip solved problem calculating extra-precise reciprocal. Elaboration Java embraces IEEE 754 name definition Java floating- point data types operations. Thus, code first example could well generated class method converted Fahrenheit Celsius. second example uses multiple dimensional arrays, explicitly supported Java. Java allows arrays arrays, array may length, unlike multiple dimensional arrays C. Like examples Chapter 2 , Java version second example would require good deal checking code array bounds, including new length calculation end row accesses. would also need check object reference null. Accurate Arithmetic Unlike integers, represent exactly every number betwe en smallest largest number, floating-point numbers 427normally approximations number can’t really represent. reason infinite variety real numbers exists betwe en, say, 1 2, 253 represented exactly double precision floating point. best getting floating-point representation close actual number. Thus, EEE 754 offers several modes rounding let programmer pick desired approximation. Rounding sounds simple enough, round accurately requires hardware include extra bits calculation. th e preceding examples, vague number bits intermediate representation occupy, clearly, every intermediate result truncated exact number digits, would opportunity round. IEEE 754, therefore, always keeps two extra bits right intervening additions, called guard round , respectively. Let’s decimal example illustrate value. guard first two extra bits kept right intermediate calculations floating-point numbers; used improve roundin g accuracy. round Method make intermediate floating-point result fit floating-point format; goal typically find nearest number represented format. also name second two extra bits kept right intermediate floating-point calculations, improves rounding accuracy. Rounding Guard Digits Example Add 2.56ten × 100 2.34ten × 102, assuming three significant decimal digits. Round nearest decimal number three significant decimal digits, first guard round digits, without them. Answer 428First must shift smaller number right align exponents, 2.56ten × 100 becomes 0.0256ten × 102. Since guard round digits, able represent two least significant digits align exponents. guard digit hold 5 round digit holds 6. sum Thus sum 2.3656ten × 102. Since two digits round, want values 0 49 round 51 99 round up, 50 tiebreaker. Rounding sum three significant digits yields 2.37ten × 102. without guard round digits drops two digits calculation. new sum answer 2.36ten × 102, 1 last digit sum above. Since worst case rounding would actual number halfway two floating-point representations, accuracy floating point normally measured terms number bits error least significant bits signi ficand; measure called number units last place , ulp. number 2 least significant bits, would called 2 ulps. Provided overflow, underflow, invalid operation exceptions, IEEE 754 guarantees computer uses number within one-half ulp. units last place (ulp) number bits error least significant bits 429significand actual number number represented. Elaboration Although example really needed one extra digit, multiply require two. binary product may one leading 0 bit; hence, normalizing step must shift product one bit left. shifts guard digit least significant bit f product, leaving round bit help accurately round product. IEEE 754 four rounding modes: always round (toward +∞), always round (toward −∞), truncate, round nearest even. final mode determines number exactly halfway between. U.S. Internal Revenue Service (IRS) always rounds 0.50 dollars up, possibly benefit IRS. equitable way would round case half time round half. IEEE 754 says least significant bit retained halfway case would odd, add one; it’s even, truncate. method always creates 0 least significant bit tie-breaking case, giving rounding mode name. mode commonly used, one Java supports. goal extra rounding bits allow computer get results intermediate results calculated infinite precision rounded. support goal rou nd nearest even, standard third bit addition guard round; set whenever nonzero bits right round bit. sticky bit allows computer see difference 0.50 … 00ten 0.50 … 01ten rounding. sticky bit bit used rounding addition guard round set whenever nonzero bits right round bit. sticky bit may set, example, addition, smaller number shifted right. Suppose added 5.01ten × 43010−1 2.34ten × 102 example above. Even guard round, would adding 0.0050 2.34, sum 2.3450. sticky bit would set, since nonzero bits rig ht. Without sticky bit remember whether 1s shifted , would assume number equal 2.345000 … 00 round nearest even 2.34. sticky bit remember number larger 2.345000 … 00, round instead 2.35. Elaboration RISC-V, MIPS-64, PowerPC, SPARC64, AMD SSE5, Intel AVX architectures provide single instruction multip ly add three registers: a=a+(b×c). Obviously, instruction allows potentially higher floating-point performance common operation. Equally important instead performing two roundings—after multiply add—which would happen separate instructions, multiply add instruction perform single rounding add. single rounding step increases precision multiply add. operations single rounding called fused multiply add . added revised IEEE 754-2008 standard (see Section 3.11). fused multiply add floating-point instruction performs multiply add, rounds add. Summary Big Picture follows reinforces stored-program concept Chapter 2 ; meaning information cannot determined looking bits, bits represent variety objects. section shows computer arithmetic finite thus disagree natural arithmetic. example, IEEE 754 standard floating-point representation 431is almost always approximation real number. Computer systems must take care minimize gap computer arithmetic arithmetic real world, programmers times need aware implications approximation. BIG Picture Bit patterns inherent meaning. may represent signed integers, unsigned integers, floating-point numbers, instructio ns, character strings, on. represented depends instruction operates bits word. major difference computer numbers numbers real world computer numbers limited size hence limited precision; it’s possible calculate number big small represented computer word. Programmers must remember limits write programs accordingly. Hardware/ Software Interface last chapter, presented storage classes programming language C (see Hardware/Software Interface section Section 2.7 ). table shows C Java data types, data transfer instructions, instructions operate types appear Chapter 2 chapter. Note Java omits unsigned integers. Check revised IEEE 754-2008 standard added 16-bit floating-point format five exponent bits. think likely range numbers could represent? 1. 1.0000 00 ×20 1.1111 1111 11 ×231, 0 2. ±1.0000 0000 0 ×2−14 ±1.1111 1111 1 ×215, ±0, ±∞, NaN 4323. ±1.0000 0000 00 ×2−14 ±1.1111 1111 11 ×215, ±0, ±∞, NaN 4. ±1.0000 0000 00 ×2−15 ±1.1111 1111 11 ×214, ±0, ±∞, NaN Elaboration accommodate comparisons may include NaNs, standard includes ordered unordered options compares. RISC-V provide instructions unordered comparison s, careful sequence ordered comparisons effect. (Java support unordered compares.) attempt squeeze every bit precision floating- point operation, standard allows numbers represented unnormalized form. Rather gap 0 smallest normalized number, IEEE allows denormalized numbers (also known denorms subnormals ). exponent zero nonzero fraction. allow number degrade significance becomes 0, called gradual underflow . example, smallest positive single precision normalized number smallest single precision denormalized number double precision, denorm gap goes 1.0 ×2−1022 1.0 ×2−1074. possibility occasional unnormalized operand given headaches floating-point designers trying build fas floating-point units. Hence, many computers cause exception operand denormalized, letting software complete operation. Although software implementations perfectly vali d, lower performance lessened popularity denorms portable floating-point software. Moreover, programmers expect denorms, programs may surprise them. 4333.6 Parallelism Computer Arithmetic: Subword Parallelism Since every microprocessor phone, tablet, laptop definition graphical display, transistor budgets increased inevitable support would added graphics operations. Many graphics systems originally used 8 bits represent three primary colors plus 8 bits location pixel. addition speakers microphones teleconferencing video games suggested support sound well. Audio samples need 8 bits precision, 16 bits sufficient. Every microprocessor special support bytes halfwords take less space stored memory (see Section 2.9), due infrequency arithmetic operations data sizes typical integer programs, little support beyond data transfers. Architects recognized many graphics audio applications would perform operation vectors data. partitioning carry chains within 128- bit adder, processor could use parallelism perform simultaneous operations short vectors sixteen 8-bit operands, eight 16-bit operands, four 32-bit operands, two 64-bit operands. 434The cost partitioned adders small yet speedups could large. Given parallelism occurs within wide word, extensions classified subword parallelism . also classified general name data level parallelism . known well vector SIMD, single instruction, multiple data (see Section 6.6 ). rising popularity multimedia applications led arithmetic instructions support narrower operations easily compute parallel. writing, RISC-V additional instructions exploit subword parallelism, next section presents real-world example architecture. 3.7 Real Stuff: Streaming SIMD Extensions Advanced Vector Extensions x86 original MMX ( MultiMedia eXtension ) x86 included instructions operate short vectors integers. Later, SSE (Streaming SIMD Extension ) provided instructions operate short vectors single-precision floating-point numbers. Chapter 2 notes 2001 Intel added 144 instructions architecture part SSE2, including double precision floating-point regist ers operations. included eight 64-bit registers used floating-point operands. AMD expanded number 16 registers, called XMM, part AMD64, Intel relabeled EM64T use. Figure 3.18 summarizes SSE SSE2 instructions. 435FIGURE 3.18 SSE/SSE2 floating-point instructions x86. xmm means one operand 128-bit SSE2 register, {mem|xmm} means operand either memory SSE2 register. table uses regular expressions show variations instructions. Thus, MOV[AU]{SS|PS|SD|PD} represents eight instructions MOVASS,MOVAPS,MOVASD,MOVAPD,MOVUSS,MOVUPS,MOVUSD, MOVUPD . use square brackets [] show single-letter alternatives: means 128-bit operand aligned memory; U means 128-bit operand unaligned memory; H means move high half 128-bit operand; L means move low half 128-bit operand. use curly brackets {} vertical bar | show multiple letter variations basic operations: SS stands Scalar Single precision floating point, one 32-bit operand 128-bit register; PS stands Packed Single precision floating point, four 32-bit operands 128-bit register; SD stands Scalar Double precision floating point, one 64-bit operand 128-bit register; PD stands Packed Double precision floating point, two 64-bit operands 128-bit register. addition holding single precision double precision number register, Intel allows multiple floating-point oper ands packed single 128-bit SSE2 register: four single precisi two double precision. Thus, 16 floating-point registers fo r SSE2 actually 128 bits wide. operands arranged memory 128-bit aligned data, 128-bit data transfers load store multiple operands per instruction. packed floating - point format supported arithmetic operations 436compute simultaneously four singles (PS) two doubles (P D). 2011, Intel doubled width registers again, called YMM, Advanced Vector Extensions (AVX). Thus, single operation specify eight 32-bit floating-point operations four 64-bit floating-point operations. legacy SSE SSE2 instructions operate lower 128 bits YMM registers. Thus, go 128-bit 256-bit operations, prepend letter “v” (for vector) front SSE2 assembly language operations use YMM register names instead XMM register name. example, SSE2 instruction perform two 64-bit floating-point additions addpd %xmm0, %xmm4 becomes vaddpd %ymm0, %ymm4 produces four 64-bit floating-point multiplies. Intel announced plans widen AVX registers first 512 bits later 1024 bits later editions x86 architecture. Elaboration AVX also added three address instructions x86. example, vaddpd specify vaddpd %ymm0, %ymm1, %ymm4 // %ymm4 = %ymm0 + %ymm1 instead standard, two address version addpd %xmm0, %xmm4 // %xmm4 = %xmm4 + %xmm0 (Unlike RISC-V, destination right x86.) Three addresses reduce number registers instructions needed computation. 3.8 Going Faster: Subword Parallelism Matrix Multiply demonstrate performance impact subword parallelism, we’ll run code Intel Core i7 first without AVX it. Figure 3.19 shows unoptimized version matrix- matrix multiply written C. saw Section 3.5 , program commonly called DGEMM , stands Double precision GEneral Matrix Multiply. Starting edition, w e added new section entitled “Going Faster” demonstrate 437the performance benefit adapting software underlying hardware, case Sandy Bridge version Intel Core i7 microprocessor. new section Chapters 3, 4, 5, 6 incrementally improve DGEMM performance using ideas chapter introduces. FIGURE 3.19 Unoptimized C version double precision matrix multiply, widely known DGEMM Double-precision GEneral Matrix Multiply. passing matrix dimension parameter n, version DGEMM uses single- dimensional versions matrices C, A, B address arithmetic get better performance instead using intuitive two-dimensional arrays saw Section 3.5 . comments remind us intuitive notation. Figure 3.20 shows x86 assembly language output inner loop Figure 3.19 . five floating point-instructions start v like AVX instructions, note use XMM registers instead YMM, include sd name, stands scalar double precision. We’ll define subword parallel instructions shortly. 438FIGURE 3.20 x86 assembly language body nested loops generated compiling unoptimized C code Figure 3.19 . Although dealing 64 bits data, compiler uses AVX version instructions instead SSE2 presumably use three address per instruction instead two (see Elaboration Section 3.7 ). compiler writers may eventually able produce high- quality code routinely uses AVX instructions x86, fo r must “cheat” using C intrinsics less tell compiler exactly produce good code. Figure 3.21 shows enhanced version Figure 3.19 Gnu C compiler produces AVX code. Figure 3.22 shows annotated x86 code output compiling using gcc –O3 level optimization. 439FIGURE 3.21 Optimized C version DGEMM using C intrinsics generate AVX subword-parallel instructions x86. Figure 3.22 shows assembly language produced compiler inner loop. FIGURE 3.22 x86 assembly language body nested loops generated compiling optimized C code Figure 3.21 . Note similarities Figure 3.20 , primary difference five floating-point operati ons using YMM registers using pd versions instructions packed double precision 440instead sd version scalar double precision. declaration line 6 Figure 3.21 uses __m256d data type, tells compiler variable hold four doubl e- precision floating-point values. intrinsic _mm256_load_pd() also line 6 uses AVX instructions load four double-precision floating-point numbers parallel ( _pd) matrix C c0. address calculation C+i+j*n line 6 represents element C[i+j*n] . Symmetrically, final step line 11 uses intrinsic _mm256_store_pd() store four double-precision floating-point numbers c0 matrix C. we’re going four elements iteration, outer loop line 4 increments 4 instead 1 line 3 Figure 3.19 . Inside loops, line 9 first load four elements using _mm256_load_pd() . multiply elements one element B, line 10 first use intrinsic _mm256_broadcast_sd() , makes four identical copies scalar double precision number—in case element B—in one YMM registers. use _mm256_mul_pd() line 9 multiply four double- precision results parallel. Finally, _mm256_add_pd() line 8 adds four products four sums c0. Figure 3.22 shows resulting x86 code body inner loops produced compiler. see five AVX instructions—they start v four five use pd packed double precision—that correspond C intrinsics mentioned above. code similar Figure 3.20 above: use 12 instructions, integer instructions nearl identical (but different registers), floating-point ins truction differences generally going scalar double (sd) using XMM registers packed double (pd) YMM registers. one exception line 4 Figure 3.22 . Every element must multiplied one element B. One solution place four identical copies 64-bit B element side-by-side 256-b YMM register, instruction vbroadcastsd does. matrices dimensions 32 32, unoptimized DGEMM Figure 3.19 runs 1.7 GigaFLOPS (FLoating point Operations Per Second) one core 2.6 GHz Intel Core i7 (Sandy Bridge). optimized code Figure 3.21 performs 6.4 GigaFLOPS. AVX version 3.85 times fast, close factor 4414.0 increase might hope performing four times many operations time using subword parallelism . Elaboration mentioned Elaboration Section 1.6 , Intel offers Turbo mode temporarily runs higher clock rate chip gets hot. Intel Core i7 (Sandy Bridge) increase 2.6 GHz 3.3 GHz Turbo mode. results Turbo mode turned off. turn on, improve results increase clock rate 3.3/2.6 =1.27 2.1 GFLOPS unoptimized DGEMM 8.1 GFLOPS AVX. Turbo mode works particularly well using single core eight- core chip, case, lets single core use much fair share power since cores idle. 3.9 Fallacies Pitfalls Thus mathematics may defined subject never know talking about, whether saying true. 442Bertrand Russell, Recent Words Principles Mathe matics, 1901 Arithmetic fallacies pitfalls generally stem differ ence limited precision computer arithmetic unlimited precision natural arithmetic. Fallacy: left shift instruction replace integer mu ltiply power 2, right shift integer division power 2. Recall binary number x, xi means ith bit, represents number Shifting bits c right n bits would seem dividing 2 n. true unsigned integers. problem signed integers. example, suppose want divide −5ten 4ten; quotient −1ten. two’s complement representation −5ten According fallacy, shifting right two divide 4ten (22): 0 sign bit, result clearly wrong. value created shift right actually 4,611,686,018,427,387,902ten instead −1ten. solution would arithmetic right shift extends sign bit instead shifting 0s. 2-bit arithmetic shift rig ht −5ten produces 443The result −2ten instead −1ten; close, cigar. Pitfall: Floating-point addition associative. Associativity holds sequence two’s complement intege r additions, even computation overflows. Alas, floating-point numbers approximations real numbers computer arithmetic limited precision, ho ld floating-point numbers. Given great range numbers represented floating point, problems occur adding two large numbers opposite signs plus small number. example, let’s see c+(a+b)=(c+a)+b. Assume c=−1.5ten × 1038, = 1.5ten × 1038, b = 1.0, single precision numbers. Since floating-point numbers limited precision result approximations real results, 1.5ten × 1038 much larger 1.0ten 1.5ten × 1038 + 1.0 still 1.5ten × 1038. sum c, a, b 0.0 1.0, depending order floating-point additions, c+(a+b) ≠(c+a)+b. Therefore, floating-point addition associative. Fallacy: Parallel execution strategies work integer data ypes also work floating-point data types. Programs typically written first run sequentially rewritten run concurrently, natural question is, “Do two versions get answer?” answer no, presume bug parallel version need track down. approach assumes computer arithmetic affect results going sequential parallel. is, 444were add million numbers together, would get results whether used one processor 1000 processors. assumption holds two’s complement integers, since intege r addition associative. Alas, since floating-point addition associative, assumption hold. vexing version fallacy occurs parallel computer operating system scheduler may use different number processors depending progr ams running parallel computer. varying number processors run would cause floating-point sums calculated different orders, getting slightly different ans wers time despite running identical code identical input ay flummox unaware parallel programmers. Given quandary, programmers write parallel code floating-point numbers need verify whether results credible, even don’t give exact answer sequential code. field deals issues called numerical analysis, subject textbooks right. concerns one reason popularity numerical libraries LAPACK ScaLAPAK, validated sequential parallel forms. Fallacy: theoretical mathematicians care floating-poi nt accuracy. Newspaper headlines November 1994 prove statement fallacy (see Figure 3.23 ). following inside story behind headlines. 445FIGURE 3.23 sampling newspaper magazine articles November 1994, including New York Times, San Jose Mercury News, San Francisco Chronicle, Infoworld . Pentium floating-point divide bug even made “Top 10 List” David Letterman Late Show television. Intel eventually took $300 million write-o ff replace buggy chips. Pentium uses standard floating-point divide algorithm generates multiple quotient bits per step, using signi ficant bits divisor dividend guess next 2 bits quotie nt. guess taken lookup table containing −2, −1, 0, +1, +2. guess multiplied divisor subtracted remainder generate new remainder. Like nonrestoring division, previous guess gets large remainder, partial remainder adjusted subsequent pass. Evidently, five elements table 80486 Intel engineers thought could never accessed, optimized PLA return 0 instead 2 situations Pentium. Intel wrong: first 11 bits always correct, errors would show occasionally bits 12 52, 4th 15th decimal digits. 446A math professor Lynchburg College Virginia, Thomas Nicely, discovered bug September 1994. calling Intel technical support getting official reaction, posted discovery Internet. post led story trade magazine, turn caused Intel issue press release. called bug glitch would affect theoretical mathematicians, average spreadsheet user seeing error every 27,000 years. IBM Research soon counterclaimed average spreadsheet user would see error every 24 days. Intel soon threw towel making following announcement December 21: Intel wish sincerely apologize handling recently publicized Pentium processor flaw. Intel Inside symbol means computer microprocessor second none quality performance. Thousands Intel employees work hard ensure true. microprocessor ever perfect . Intel continues believe technically extremely minor problem taken life own. Although Intel firmly stands behind quality current version Pentium processor, recognize many users concerns. want resolve concerns. Intel exchange current version Pentium processor updated version, floating-point divide flaw corrected, owner requests it, free charge anytime life computer. Analysts estimate recall cost Intel $500 million, Intel engineers get Christmas bonus year. story brings points everyone ponder. much cheaper would fix bug July 1994? cost repair damage Intel’s reputation? corporate responsibility disclosing bugs product widely used relied upon microprocessor? 3.10 Concluding Remarks decades, computer arithmetic become largely standardized, greatly enhancing portability programs. Two’s 447complement binary integer arithmetic found every compute r sold today, includes floating point support, offers th e IEEE 754 binary floating-point arithmetic. Computer arithmetic distinguished paper-and-pencil arithmetic constraints limited precision. limit may result invalid operations calculating numbers larger smaller predefined limits. anomalies, called “overflow” “underflow,” may result exceptions interrupts, emergency events similar unplanned subroutine calls. Chapters 4 5 discuss exceptions detail. Floating-point arithmetic added challenge approximation real numbers, care needs taken ensure computer number selected representatio n closest actual number. challenges imprecision limited representation floating point part inspirat ion field numerical analysis. switch parallelism shine searchlight numerical analysis again, solutions long considered safe sequential computers must reconsidered trying find fastest algorithm parall el computers still achieves correct result. Data-level parallelism, specifically subword parallelism, offers simple path higher performance programs intensive 448in arithmetic operations either integer floating-point ata. showed could speed matrix multiply nearly fourfold using instructions could execute four floating-point op erations time. explanation computer arithmetic chapter comes description much RISC-V instruction set. Figure 3.24 ranks popularity twenty common RISC-V instructions SPEC CPU2006 integer floating- point benchmarks. see, relatively small number instructions dominate rankings. observation significant implications design processor, wi see Chapter 4 . 449FIGURE 3.24 frequency RISC-V instructions SPEC CPU2006 benchmarks. 20 popular instructions, collectively account 76% instructions executed, included table. Pseudoinstructions converted RISC-V execution, hence appear here, explaining part popularity addi. matter instruction set size—RISC-V, MIPS, x86 —never forget bit patterns inherent meaning. bit pattern may represent signed integer, unsigned integer, floating-point number, string, instruction, on. stored- program computers, operation bit pattern determines meaning. 450 Historical Perspective Reading Gresham’s Law (“Bad money drives Good”) computers would say, “The Fast drives Slow even Fast wrong.” W. Kahan, 1992 section surveys history floating point going b ack von Neumann, including surprisingly controversial IEEE standards effort, plus rationale 80-bit stack architectur e floating point x86. See rest Section 3.11 online. Historical Perspective Reading Gresham’s Law (“Bad money drives Good”) computers would say, “The Fast drives Slow even Fast wrong.” W. Kahan, 1992 section surveys history floating point going b ack 451von Neumann, including surprisingly controversial IEEE standards effort, rationale 80-bit stack architecture floating point IA-32, update next round standard. first may hard imagine subject less interest correctness computer arithmetic accuracy, harder still understand subject old mathematical contentious. Computer arithmetic old computing , subject’s earliest notions, like economical r euse registers serial multiplication division, still co mmand respect today. Maurice Wilkes [1985] recalled conversation notion visit United States 1946, earliest stored-program computer built: … project von Neumann set Institute Advanced Studies Princeton.… Goldstine explained principal features design, including device whereby digits multiplier put tail accumulator shifted least significant part product shifted in. expressed admiration way registers shifting circuits arranged … Goldstine remarked things nature came easily von Neumann. controversy here; hardly arise context exact integer arithmetic, long general agreement integer correct result be. However, soon approximate arithmetic enters picture, controversy , one person’s “negligible” must another’s “everything.” First Dispute Floating-point arithmetic kindled disagreement eve r built. John von Neumann aware Konrad Zuse’s proposal computer Germany 1939 never built, probably floating point made appear complicated finish Germans expected World War II end. Hence, von Neumann refused include computer built Princeton. influential report coauthored 1946 H. H. Goldstine A. W. Burks, gave arguments 452floating point. favor: … retain sum product many significant digits possible … free human operator burden estimating inserting problem “scale factors”—multiplication constan ts serve keep numbers within limits machine. Floating point excluded several reasons: is, course, denying fact human time consumed arranging introduction suitable scale factors. ly argue time consumed small percentage total time spend preparing interesting problem machine. first advantage floating point is, feel, somewhat illusory. order floating point, one must waste memory capacity could otherwise used carrying digits per word. would therefore seem us clear whether modest advantages floating binary point offset loss memory capacity increased complexity arithmetic control circuits. argument seems bits devoted exponent fields would bits wasted. Experience proven otherwise. One software approach accommodate reals without floating- point hardware called floating vectors ; idea compute runtime one scale factor whole array numbers, choosing scale factor array’s biggest number would barely fill field. 1951, James H. Wilkinson used scheme extensively matrix computations. problem proved hat program might encounter large value, hence scale factor must accommodate rare sizeable numbers. common numbers would thus many leading 0s, since numbers use single scale factor. Accuracy sacrificed, least significant bits lost right accommodate leading 0s. wastage became obvious practitioners early computers displayed memory bits dots cathode ray tubes (like TV screens) los precision visible. floating point deserved use d, 453no practical alternative existed. Thus, true floating-point hardware became popular useful. 1957, floating-point hardware almost ubiquitous. decimal floating-point unit available IBM 650, soon IBM 704, 709, 7090, 7094 … series would offer binary floating-point hardware double well single precision. result, everybody floating point, every implementation different. Diversity versus Portability Since roundoff introduces error almost floating-p oint operations, complain another bit error seems picayune. 20 years, nobody complained much operations behaved little differently different computers. software required clever tricks circumvent idiosyncrasies finally deliver results correct last several bits, tric ks deemed part programmer’s art. long time, matrix computations mystified people notion error analysis; perhaps continues true. may people still surprised numerically stable matrix computations depend upon quality arithmetic places, far fewer generally supposed. Books Wilkinson widely used software packages like Linpack Eispack sustained false impression, widespread early 1970s, modicum skill sufficed produce portable numerical software. “Portable” means software distributed source code standard language compiled executed practically commercially significant computer, perform task well program performs task computer. Insofar numerical software often thought consist entirely computer-independent mathemat ical formulas, portability commonly taken granted; mistake presumption become clear shortly. Packages like Linpack Eispack cost much develop— hundred dollars per line Fortran delivered—that could developed without U.S. government subsidy; portability precondition subsidy. nobody 454thought distinguish various components contributed cost. One component algorithmic—devise algorithm deserves work least one computer despite roundof f over-/underflow limitations. Another component software engineering effort required achieve confirm portability diverse computers commercially significant time; component grew onerous ever diverse floating-point arithmetics blossomed 1970s. yet scarcely anybody realized much diversity inflated cost software packages. Backward Step Early evidence somewhat different arithmetics could engend er grossly different software development costs presented n 1964. happened meeting SHARE, IBM mainframe users’ group, IBM announced System/360, successor 7094 series. One speakers described tricks forced devise achieve level quality S/360 library quite high previously achieved 7094. Von Neumann could foretold part trouble, still alive. 1948, Goldstine published lengthy error analysis difficult pessimistic hardly anybody paid attention it. predict correctly, however, computati ons larger arrays data would probably fall prey roundoff often. IBM S/360s bigger memories 7094s, data arrays could grow larger, did. make matters worse, S/360s narrower single precision words (32 bits versus 36) used cruder arithmetic (hexadecimal base 16 versus binary base 2) consequently poorer worst-case precision (21 significant bits versus 27) old 7094s. Consequently, software almost always provided (barely) satisfactory accuracy 7094s often produced inaccurate results run S/360s. quickest way recover adequate accuracy replace old codes’ single precision declarations double precision recompilation S/360. practice exercis ed S/360 double precision far expected. early S/360’s worst troubles caused lack guard 455digit double precision. lack showed multiplicatio n failure identities like 1.0* x=x multiplying x 1.0 dropped x’s last hexadecimal digit (4 bits). Similarly, x close different exponents, subtraction dropped last digit smaller operand computing x−y. final aberration double precision undermined precious theorem th single precision (and now) honored: 1/2 ≤ x/y ≤2, rounding error occur x−y computed; must computed exactly. Innumerable computations benefited minor theorem, often unwittingly, several decades f irst formal announcement proof. taking stuff granted. identities theorems exact relationships persisted, despite roundoff, reasonable implementations f approximate arithmetic appreciated lost . Previously, thought things matter precision (how many significant digits carried) range (the spread over-/underflow thresholds). Since S/360’s double precision precision wider range 7094’s, software expected continue work least well before. didn’t. Programmers matured program managers appalled cost converting 7094 software run S/360s. small subcommittee SHARE proposed improvements S/360 floating point. committee surprised grateful get fair part asked IBM, including all- important guard digits. 1968, retrofitted S/360s field considerable expense; worse customers’ loss faith IBM’s infallibility (a lesson learned Intel 30 year later). IBM employees remember incident still shudd er. People Built Bombs Seymour Cray associated decades CDC Cray computers were, built them, world’s biggest fastest. always understood customers wanted most: speed . gave even if, doing, also gave arithmetics “interesting” anyone else’s. Among 456customers great government laboratories like Livermore Los Alamos, nuclear weapons designed. challenges “interesting” arithmetics pretty tame people overcome Mother Nature’s challenges. Perhaps us could learn live arithmetic idiosyncrasy one computer’s idiosyncrasies endured. Instead , accumulating different computers’ different anomalies, software dies Death Thousand Cuts. example Cray’s computers: (x == 0.0) = 17.0 else = z/x Could statement stopped divide-by-zero error? CDC 6600 could. reason conflict 6600’s adder, x compared 0.0, multiplier divider. adder’s comparison examined x’s leading 13 bits, sufficed distinguish zero normal nonzero floating- point numbers x. multiplier divider examined 12 leading bits. Consequently, tiny numbers existed nonzer adder zero multiplier divider! avoid disasters tiny numbers, programmers learned replace statements like one (1.0 * x == 0.0) = 17.0 else = z/x statement unsafe use would-be portable software malfunctions obscurely computers designed b Cray, ones marketed Cray Research, Inc. x huge 2.0 * x would overflow, 1.0 * x might overflow too! Overflow happens Cray computers check product’s exponent product’s exponent normalized, save delay single gate. Rounding error anomalies far worse over- /underflow anomaly discussed also affect Cray computers. worst error came lack guard digit add/subtract, affliction IBM S/360s. bad luck software occasioned way Cray economized multiplier; one-third bits normal multiplier arrays generate left multipliers, would contribute less unit last place final Cray-rounded product. Consequently, Cray multiplier errs almost bit might expected. error compounded division takes three multiplic ations improve approximate reciprocal divisor 457multiply numerator it. Square root compounds multiplication errors. fast way drove slow, even though fast occasionally slightly wrong. Making World Safe Floating Point, Vice Versa William Kahan undergraduate University Toronto 1953 learned program Ferranti-Manchester Mark-I computer. entered field early, Kahan became acquainted wide range devices large proportion personalities active computing; numbers small time. performed computations slide rules, desktop mechanical calculators, tabletop analog differential analyzers, on; used earliest electronic computers calculators mentioned book. Kahan’s desire deliver reliable software led interest error analysis intensified two years postdoctoral study England, became acquainted Wilkinson. 1960, resumed teaching Toronto, IBM 7090 acquired, granted free rein tinker operating system, Fortran compiler, runtime library. (He denies ever came near 7090 hardware soldering iron admits asking so.) One story time illuminates misconceptions numerical anomalies computer systems incur awesome hidden costs. graduate student aeronautical engineering used 7090 simulate wings designing short takeoffs landings. knew wing would difficult control characteristics included abrupt onset stall, thought could avoid that. simulations telling otherwise. sure roundoff interfering, repeated many calculations double precision gotten results much li ke single; wings stalled abruptly precision s. Disheartened, student gave up. Meanwhile Kahan replaced IBM’s logarithm program (ALOG) one own, hoped would provide better accuracy. testing it, Kahan reran programs using new 458version ALOG. student’s results changed significantly; Kahan approached find happened. student puzzled. Much student preferred results produced new ALOG—they predicted gradual stall—he knew must wrong disagreed double precision results. discrepancy single double precision results disappeared days later new release IBM’s double precision arithmetic software 7090 arrived. (The 7090 double precision hardware.) went write thesis build wings; performed predicted. end story. 1963, 7090 replaced faster 7094 double precision floating-point hardware otherwise practicall instruction set 7090. double precision using new hardware wing stall abruptly again. lot time spent find why. 7094 hardware turned out, like superseded 7090 software subsequent early S/360s, lack guard bit double precision. Like many programmers computers Cray’s, student discovered trick compensate lack guard digit; wrote expression (0.5 — x) + 0.5 place 1.0 — x . Nowadays would blush explain trick might necessary, solved student’s problem. Meanwhile lure California working Kahan family; came Berkeley University California. opportunity presented 1974 accuracy questions induced Hewlett-Packard’s calculator designers call consultant. consultant Kahan, work dramatically improved accuracy HP calculators, another story. Fruitful collaboration congenial coworkers, however, forti fied next crucial opportunity. came 1976, John F. Palmer Intel empowered specify “best possible” floating-point arithmetic ntel’s product line, Moore’s Law made possible create whole floating-point unit single chip. floating-point standard originally started iAPX-432, late, Intel started 8086 short-term emergency stand-in iAPX-432 ready. iAPX-432 never became popular, emergency stand-in became standard-bearer Intel. 4598087 floating-point coprocessor 8086 contemplated. (A coprocessor simply additional chip accelerates portion work processor; case, accelerated floating-point computation.) Palmer obtained Ph.D. Stanford years knew call counsel perfection—Kahan. put together design obviously would impossible years earlier looked quite possible time. new Israeli team Intel employees led Rafi Navé felt challenged prove prowess Americans leaped opportunity put something impossible chip—the 8087. now, floating-point arithmetics merely diverse among mainframes become chaotic among microprocessors, one might host dozen varieties arithmetic ROM firmware software. Robert G. Stewart, engineer prominent IEEE activities, got fed anarchy proposed IEEE draft decent floating-point standard. Simultaneously, word leaked Silicon Valley Intel going put one chip awesome floating point well beyond anything competitors mind. competition find way slow Intel down, formed committee Stewart requested. 460Meetings committee began late 1977 plethora competing drafts innumerable sources dragged 1985, IEEE Standard 754 Binary Floating Point made official. winning draft close one submitted Kahan, student Jerome T. Coonen, Harold S. Stone, professor visiting Berkeley time. draft based Intel design, Intel’s permission, course, simplified Coonen. harmonious combination features, almost none new, outset attracted support within committee outside experts like Wilkinson draft, win nearly unanimous support within committee win official IEEE endorsement, took time. First IEEE 754 Chips 1980, Intel became tired waiting released 8087 use IBM PC. floating-point architecture companion 8087 retrofitted 8086 opcode space, making inconvenient offer two operands per instruction found rest 8086. Hence decision one operand per instruction using stack: “The designer’s task make Virtue Necessity.” (Kahan’s [1990] history stack architecture selection 8087 entertaining reading.) Rather classical stack architecture, provision avoiding common subexpressions push ed popped memory top stack found registers, Intel tried combine flat register file stack . reasoning restriction top stack one operand bad since required execution FXCH instruction (which swapped registers) get result two-operand instruction, FXCH much faster floating-point operations 8087. Since floating-point expressions complex, Kahan reasoned eight registers meant stack would rarely overflow. Hence, urged 8087 use hybrid scheme provision stack overflow stack underflow would interrupt 8086 interrupt software could give illusi compiler writer unlimited stack floating-point data. Intel 8087 implemented Israel, 7500 miles 10 461time zones made communication California difficult. According Palmer Morse ( 8087 Primer , J. Wiley, New York, 1984, p. 93): Unfortunately, nobody tried write software stack manager unti l 8087 built, late; complicated perform hardware turned even worse software. One thing found lacking ability conveniently determine invalid operation indeed due stack overflow.… Also lacking ability restart instruction caused stack overflow … result stack exceptions slow handle software. Kahan [1990] says: Consequently, almost higher-level languages’ compilers emit inefficient code 80x87 family, degrading chip’s performance typically 50% spurious stores loads necessary simply preclude stack over/under-flow.… still regret 8087’s stack implementation quite neat original intention.… original design realized, compilers today would use 80x87 descendents efficiently, Intel’s competitors could easily marke faster compatible 80x87 imitations. 1982, Motorola announced 68881, found place Sun 3s Macintosh IIs; Apple supporter proposal beginning. Another Berkeley graduate student, George S. Taylor, soon designed high-speed implementation proposed standard early superminicomputer (ELXSI 6400). standard becoming de facto final draft’s ink dry. early rush adoptions gave computing industry false impression IEEE 754, like many standards, could implemented easily following standard recipe. true. enthusiasm ingenuity early implementors made look easy. fact, implement IEEE 754 correctly demands extraordinarily 462diligent attention detail; make run fast demands extraordinarily competent ingenuity design. industry’s engineering managers realized this, might quick affirm that, matter policy, “We conform applicable standards.” IEEE 754 Today Unfortunately, compiler-writing community represented adequately wrangling, features didn’t balance language compiler issues points. community slow make IEEE 754’s unusual features available applications programmer. Humane exception handling one unusual feature; directed rounding another. Without compiler support, features atrophied. successful parts IEEE 754 widely implemented standard common floating-point format, requires minimum accuracy one-half ulp least significan bit, operations must commutative. IEEE 754/854 implemented considerable degree fidelity least part product line every North American computer manufacturer. significant exceptions DEC VAX, IBM S/370 descendants, Cray Research vector supercomputers, three replaced compliant computers. IEEE rules ask standard revisited periodically updating. committee started 2000, drafts revised standards circulated voting, approved 2008. revised standard, IEEE Std 754-2008 [2008], includes several new types: 16-bit floating point, called half precision ; 128-bit floating point, called quad precision ; three decimal types, matching length 32-bit, 64-bit, 128-bit binary formats. 1989, Association Computing Machinery, acknowledging benefits conferred upon computing industry IEEE 754, honored Kahan Turing Award. accepting it, thanked many associates diligent support, adversaries blunders. . . . errors bad. Reading 463If interested learning floating point, two publications David Goldberg [1991, 2002] good starting points; abound pointers reading. Several stories told section come Kahan [1972, 1983]. latest word state art computer arithmetic often found n Proceedings recent IEEE-sponsored Symposium Computer Arithmetic, held every two years; 23rd held 2016. Burks, A.W., H.H. Goldstine, J. von Neumann [1946]. “Preliminary discussion logical design electronic computing instrument,” Report U.S. Army Ordnance Dept. , p. 1; also Papers John von Neumann , W. Aspray A. Burks (Eds.), MIT Press, Cambridge, MA, Tomash Publishers, Los Angeles, 1987, 97–146. classic paper includes arguments floating-point hardwar e. Goldberg, D. [2002]. “Computer arithmetic”. Appendix Computer Architecture: Quantitative Approach , third edition, J. L. Hennessy D. A. Patterson, Morgan Kaufmann Publishers, San Francisco. advanced introduction integer floating-point arithmeti c, emphasis hardware. covers Sections 3.4 –3.6 book 10 pages, leaving another 45 pages advanced topics. Goldberg, D. [1991]. “What every computer scientist know floating-point arithmetic”, ACM Computing Surveys 23(1), 5–48. Another good introduction floating-point arithmetic author, time emphasis software. Kahan, W. [1972]. “A survey error-analysis,” Info. Processing 71 (Proc. IFIP Congress 71 Ljubljana), Vol. 2, North-Holland Publishing, Amsterdam, 1214–1239. survey source stories importance accurate arithmetic. Kahan, W. [1983]. “Mathematics written sand,” Proc. Amer. Stat. Assoc. Joint Summer Meetings 1983, Statistical Computing Section , 12–26. title refers silicon another source stories illu strating importance accurate arithmetic. Kahan, W. [1990]. “On advantage 8087’s stack,” unpublished course notes, Computer Science Division, Univer sity 464of California, Berkeley. 8087 floating-point architecture could been. Kahan, W. [1997]. Available http://www.cims.nyu.edu/~dbindel/class/cs279/87stack.pdf . collection memos related floating point, including “Beastly numbers” (another less famous Pentium bug), “Notes IEEE floating point arithmetic” (including comments feature atrophying), “The baleful effects computing benchmarks” (on unhealthy preoccupation speed versus correctness, accuracy, ease use, flexibility, …) . Koren, I. [2002]. Computer Arithmetic Algorithms , second edition, A. K. Peters, Natick, MA. textbook aimed seniors first-year graduate students explains fundamental principles basic arithmetic, well com plex operations logarithmic trigonometric functions. Wilkes, M. V. [1985]. Memoirs Computer Pioneer , MIT Press, Cambridge, MA. computer pioneer’s recollections include derivation standard hardware multiply divide developed von Neumann. 3.12 Exercises Never give in, never give in, never, never, never—in nothing, great small, large petty—never give in. Winston Churchill, address Harrow School, 1941 3.1 [5] <§3.2>What 5ED4 −07A4 values represent unsigned 16-bit hexadecimal numbers? result written hexadecimal. Show work. 3.2 [5] <§3.2>What 5ED4 −07A4 values represent signed 16-bit hexadecimal numbers stored sign-magnitude format? result written hexadecimal. Show work. 3.3 [10] <§3.2>Convert 5ED4 binary number. makes base 16 (hexadecimal) attractive numbering system representing values computers? 3.4 [5] <§3.2>What 4365 −3412 values represent unsigned 12-bit octal numbers? result written 465octal. Show work. 3.5 [5] <§3.2>What 4365 −3412 values represent signed 12-bit octal numbers stored sign-magnitude format? result written octal. Show work. 3.6 [5] <§3.2>Assume 185 122 unsigned 8-bit decimal integers. Calculate 185–122. overflow, underflow, neither? 3.7 [5] <§3.2>Assume 185 122 signed 8-bit decimal integers stored sign-magnitude format. Calculate 185 +122. overflow, underflow, neither? 3.8 [5] <§3.2>Assume 185 122 signed 8-bit decimal integers stored sign-magnitude format. Calculate 185 −122. overflow, underflow, neither? 3.9 [10] <§3.2>Assume 151 214 signed 8-bit decimal integers stored two’s complement format. Calculate 151 +214 using saturating arithmetic. result written decimal. Show work. 3.10 [10] <§3.2>Assume 151 214 signed 8-bit decimal integers stored two’s complement format. Calculate 151 −214 using saturating arithmetic. result written decimal. Show work. 3.11 [10] <§3.2>Assume 151 214 unsigned 8-bit integers. Calculate 151+214 using saturating arithmetic. result written decimal. Show work. 3.12 [20] <§3.3>Using table similar shown Figure 3.6 , calculate product octal unsigned 6-bit integers 62 12 using hardware described Figure 3.3 . show contents register step. 3.13 [20] <§3.3>Using table similar shown Figure 3.6 , calculate product hexadecimal unsigned 8-bit integers 62 12 using hardware described Figure 3.5 . show contents register step. 3.14 [10] <§3.3>Calculate time necessary perform multiply using approach given Figures 3.3 3.4 integer 8 bits wide step operation takes four time units. Assume step 1a addition always performed—either multiplicand added, zero be. Also assume registers already initialized (you counting long takes multiplication loop itself) . 466this done hardware, shifts multiplicand multiplier done simultaneously. done software, done one other. Solve case. 3.15 [10] <§3.3>Calculate time necessary perform multiply using approach described text (31 adders stacked vertically) integer 8 bits wide adder takes four time units. 3.16 [20] <§3.3>Calculate time necessary perform multiply using approach given Figure 3.7 integer 8 bits wide adder takes four time units. 3.17 [20] <§3.3>As discussed text, one possible performance enhancement shift add instead actual multiplication. Since 9 ×6, example, written (2 ×2 ×2 +1)×6, calculate 9 ×6 shifting 6 left three times adding 6 result. Show best way calculate 0 ×33 ×0 ×55 using shifts adds/subtracts. Assume inputs 8-bit unsigned integers. 3.18 [20] <§3.4>Using table similar shown Figure 3.10 , calculate 74 divided 21 using hardware described Figure 3.8 . show contents register step. Assume inputs unsigned 6-bit integers. 3.19 [30] <§3.4>Using table similar shown Figure 3.10 , calculate 74 divided 21 using hardware described Figure 3.11 . show contents register step. Assume B unsigned 6-bit integers. algorithm requires slightly different approach shown Figure 3.9 . want think hard this, experiment two, else go web figure make work correctly. (Hint: one possible solution involves using fact Figure 3.11 implies remainder register shifted either direction.) 3.20 [5] <§3.5>What decimal number bit pattern 0×0C000000 represent two’s complement integer? unsigned integer? 3.21 [10] <§3.5>If bit pattern 0×0C000000 placed Instruction Register, MIPS instruction executed? 3.22 [10] <§3.5>What decimal number bit pattern 0×0C000000 represent floating point number? Use 467IEEE 754 standard. 3.23 [10] <§3.5>Write binary representation decimal number 63.25 assuming IEEE 754 single precision format. 3.24 [10] <§3.5>Write binary representation decimal number 63.25 assuming IEEE 754 double precision format. 3.25 [10] <§3.5>Write binary representation decimal number 63.25 assuming stored using single precision IBM format (base 16, instead base 2, 7 bits exponent). 3.26 [20] <§3.5>Write binary bit pattern represent −1.5625 ×10−1 assuming format similar employed DEC PDP-8 (the leftmost 12 bits exponent stored two’s complement number, rightmost 24 bits fraction stored two’s complement number). hidden 1 used. Comment range accuracy 36-bit pattern compares single double precision IEEE 754 standards. 3.27 [20] <§3.5>IEEE 754-2008 contains half precision 16 bits wide. leftmost bit still sign bit, exponent 5 bits wide bias 15, mantissa 10 bits long. hidden 1 assumed. Write bit pattern represent −1.5625 ×10−1 assuming version format, uses excess-16 format store exponent. Comment range accuracy 16-bit floating point format compares single precision IEEE 754 standard. 3.28 [20] <§3.5>The Hewlett-Packard 2114, 2115, 2116 used format leftmost 16 bits fraction stored two’s complement format, followed another 16-bit field leftmost 8 bits extension fraction (making fraction 24 bits long), rightmost 8 bits representing exponent. However, interesting twist, exponent stored sign-magnitude format sign bit far right! Write bit pattern represent −1.5625 ×10−1 assuming format. hidden 1 used. Comment range accuracy 32-bit pattern compares single precision IEEE 754 standard. 3.29 [20] <§3.5>Calculate sum 2.6125 ×101 4.150390625 ×10−1 hand, assuming B stored 16-bit half precision described Exercise 3.27 . Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show 468steps. 3.30 [30] <§3.5>Calculate product –8.0546875 ×100 −1.79931640625 ×10–1 hand, assuming B stored 16-bit half precision format described Exercise 3.27 . Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps; however, done example text, multiplication human-readable format instead using techniques described Exercises 3.12 3.14. Indicate overflow underflow. Write answer 16-bit floating point format described Exercise 3.27 also decimal number. accurate result? compare number get multiplication calculator? 3.31 [30] <§3.5>Calculate hand 8.625 ×101 divided −4.875 ×100. Show steps necessary achieve answer. Assume guard, round bit, sticky bit, use necessary. Write final answer 16-bit floating point format described Exercise 3.27 decimal compare decimal result get use calculator. 3.32 [20] <§3.10>Calculate (3.984375 ×10−1 + 3.4375 ×10−1)+1.771 ×103 hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, write answer 16-bit floating point format decimal. 3.33 [20] <§3.10>Calculate 3.984375 ×10−1+(3.4375 ×10−1 + 1.771 ×103) hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, write answer 16-bit floating point format decimal. 3.34 [10] <§3.10>Based answers Exercises 3.32 3.33, (3.984375 ×10−1 + 3.4375 ×10−1)+1.771 ×103 =3.984375 ×10−1+ (3.4375 ×10−1 + 1.771 ×103)? 3.35 [30] <§3.10>Calculate (3.41796875 ×10−3 × 6.34765625 ×10−3)×1.05625 ×102 hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, 469write answer 16-bit floating point format decimal. 3.36 [30] <§3.10>Calculate 3.41796875 ×10−3×(6.34765625 ×10−3 × 1.05625 ×102) hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, write answer 16-bit floating point format decimal. 3.37 [10] <§3.10>Based answers Exercises 3.35 3.36, (3.41796875 ×10−3 × 6.34765625 ×10−3)×1.05625 ×102 =3.41796875 ×10−3×(6.34765625 ×10−3 × 1.05625 ×102)? 3.38 [30] <§3.10>Calculate 1.666015625 ×100×(1.9760 ×104+−1.9744 ×104) hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, write answer 16-bit floating point format decimal. 3.39 [30] <§3.10>Calculate (1.666015625 ×100 × 1.9760 ×104)+ (1.666015625 ×100×−1.9744 ×104) hand, assuming values stored 16-bit half precision format described Exercise 3.27 (and also described text). Assume 1 guard, 1 round bit, 1 sticky bit, round nearest even. Show steps, write answer 16-bit floating point format decimal. 3.40 [10] <§3.10>Based answers Exercises 3.38 3.39, (1.666015625 ×100 × 1.9760 ×104)+(1.666015625 ×100×−1.9744 ×104) =1.666015625 ×100×(1.9760 ×104+−1.9744 ×104)? 3.41 [10] <§3.5>Using IEEE 754 floating point format, write bit pattern would represent −1/4. represent −1/4 exactly? 3.42 [10] <§3.5>What get add −1/4 four times? −1/4 ×4? same? be? 3.43 [10] <§3.5>Write bit pattern fraction value 1/3 assuming floating point format uses binary numbers fraction. Assume 24 bits, need normalize. representation exact? 3.44 [10] <§3.5>Write bit pattern fraction value 4701/3 assuming floating point format uses Binary Coded Decimal (base 10) numbers fraction instead base 2. Assume 24 bits, need normalize. representation exact? 3.45 [10] <§3.5>Write bit pattern assuming using base 15 numbers fraction value 1/3 instead base 2. (Base 16 numbers use symbols 0–9 A–F. Base 15 numbers would use 0–9 A–E.) Assume 24 bits, need normalize. representation exact? 3.46 [20] <§3.5>Write bit pattern assuming using base 30 numbers fraction value 1/3 instead base 2. (Base 16 numbers use symbols 0–9 A–F. Base 30 numbers would use 0–9 A–T.) Assume 20 bits, need normalize. representation exact? 3.47 [45] <§§3.6, 3.7>The following C code implements four-tap FIR filter input array sig_in . Assume arrays 16-bit fixed-point values. (i = 3;i< 128;i+ +) sig_out[i] = sig_in[i − 3] * f[0] + sig_in[i − 2] * f[1] + sig_in[i − 1] * f[2] + sig_in[i] * f[3]; Assume write optimized implementation code assembly language processor SIMD instructions 128-bit registers. Without knowing details instruction set, briefly describe would implemen code, maximizing use sub-word operations minimizing amount data transferred registers memory. State assumptions instructions use . Answers Check §3.2, page 177: 2. §3.5, page 215: 3. 4714 Processor Abstract chapter describes processors exploit implicit paralle lism. contains explanation principles techniques used implementing processor, starting highly abstract simplified overview. overview followed section b uilds datapath constructs simple version processor sufficient implement instruction set like RISC-V. bulk chap ter covers realistic pipelined RISC-V implementation, concludes section develops concepts necessary implement complex instruction sets, like x86. Keywords ARMv8; logic design; datapath; pipelining; control hazard; nstruction-level parallelism; digital design; combinational element; state e lement; asserted; deasserted; clocking methodology; edge-triggered clocking; control signal; data signal; datapath element; program counter; PC; register file; sign-extend; branch target address; branch taken; branch taken; untaken branch; truth table; don’t- care term; opcode; single-cycle implementation; pipelining; struct ural hazard; data hazard; forwarding; bypassing; load-use data hazard; pipeline stall; bubble; control hazard; branch hazard; branch prediction; latency; nop; flush; ynamic branch prediction; branch prediction buffer; branch history table; bran ch delay slot; branch target buffer; correlating predictor; tournament branch predictor; exception; interrupt; vectored interrupt; imprecise interrupt; imprecise exception; precise nterrupt; precise exception; parallelism; instruction-level parallelism; multiple ssue; static multiple issue; dynamic multiple issue; issue slots; speculation; issue packet ; Long Instruction Word; VLIW; use latency; loop unrolling; register naming; antidependence; name dependence; superscalar; dynamic pipeline scheduling; commit unit; reservation station; reorder buffer; out-of-order execut ion; in-order commit; microarchitecture; architectural registers; instruction latency; trix multiply; ARM Cortex-A53; Intel Core i7 major matter, details small. 472French Proverb OUTLINE 4.1 Introduction 236 4.2 Logic Design Conventions 240 4.3 Building Datapath 243 4.4 Simple Implementation Scheme 251 4.5 Overview Pipelining 262 4.6 Pipelined Datapath Control 276 4.7 Data Hazards: Forwarding versus Stalling 294 4.8 Control Hazards 307 4.9 Exceptions 315 4.10 Parallelism via Instructions 321 4.11 Real Stuff: ARM Cortex-A53 Intel Core 7 Pipelines 334 4.12 Going Faster: Instruction-Level Parallelism Mat rix Multiply 342 4.13 Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline Pipelining Illustrations 345 4.14 Fallacies Pitfalls 345 4.15 Concluding Remarks 346 4.16 Historical Perspective Reading 347 4.17 Exercises 347 473The Five Classic Components Computer 4.1 Introduction Chapter 1 explains performance computer determined three key factors: instruction count, clock cycl e time, clock cycles per instruction (CPI). Chapter 2 explains compiler instruction set architecture determine instruction count required given program. However, implementation processor determines clock cycl e time number clock cycles per instruction. chap ter, construct datapath control unit two different implementations RISC-V instruction set. chapter contains explanation principles techniques used implementing processor, starting hig hly abstract simplified overview section. followed b section builds datapath constructs simple version 474a processor sufficient implement instruction set like RIS C-V. bulk chapter covers realistic pipelined RISC-V implementation, followed section develops concepts necessary implement complex instruction sets, like x 86. reader interested understanding high-level interpretation instructions impact program performance, initial section Section 4.5 present basic concepts pipelining. Current trends covered Section 4.10 , Section 4.11 describes recent Intel Core i7 ARM Cortex- A53 architectures. Section 4.12 shows use instruction-level parallelism double performance matrix multiply Section 3.9 . sections provide enough background understand pipeline concepts high level. reader interested understanding processor performance depth, Sections 4.3 , 4.4, 4.6 useful. interested learning build processor als cover Sections 4.2 , 4.7–4.9. readers interest modern hardware design, Section 4.13 describes hardware design languages CAD tools used implement hardware, 475then use hardware design language describe pipelined implementation. also gives several illustrati ons pipelining hardware executes. Basic RISC-V Implementation examining implementation includes subset core RISC-V instruction set: memory-reference instructions load doubleword (ld) store doubleword (sd) arithmetic-logical instructions add, sub, and, conditional branch instruction branch equal (beq) subset include integer instructions (for example, shift, multiply, divide missing), include floating-point instructions. However, illustrate key principles used creating datapath designing control. implementation remaining instructions similar. examining implementation, opportunity see instruction set architecture determines many asp ects implementation, choice various implementation strategies affects clock rate CPI computer. Many key design principles introduced Chapter 1 illustrated looking implementation, Simplicity favors regularity . addition, concepts used implement RISC-V subset chapter basic ideas used construct broad spectrum computers, high-performance servers general-purpose microproce ssors embedded processors. Overview Implementation Chapter 2 , looked core RISC-V instructions, including integer arithmetic-logical instructions, memory-refer ence instructions, branch instructions. Much needs b e done implement instructions same, independent exact class instruction. every instruction, first wo steps identical: 1. Send program counter (PC) memory contains 476code fetch instruction memory. 2. Read one two registers, using fields instruction sele ct registers read. ld instruction, need read one register, instructions require reading two registers. two steps, actions required complete instruction depend instruction class. Fortunately, e ach three instruction classes (memory-reference, arithmetic -logical, branches), actions largely same, independent exact instruction. simplicity regularity RISC-V instruction set simplify implementation making execution many instruction classes similar. example, instruction classes use arithmetic-logical u nit (ALU) reading registers. memory-reference instructions use ALU address calculation, arithmetic - logical instructions operation execution, condition al branches equality test. using ALU, actions required complete various instruction classes differ. mem ory- reference instruction need access memory either read data load write data store. arithmetic-logical load instruction must write data ALU memory back register. Lastly, conditional branch instruction, may need change next instruction address based comparison; otherwise, PC incremented four get address subsequent instruction. Figure 4.1 shows high-level view RISC-V implementation, focusing various functional units ir interconnection. Although figure shows flow data processor, omits two important aspects instruction execution. 477FIGURE 4.1 abstract view implementation RISC-V subset showing major functional units major connections them. instructions start using program counter supply instruction address instruction memory. instruction fetched, register operands used instruction specified fields instruction. register operands fetched, operated compute memory address (for load store), compute arithmetic result (for integer arithmetic-logical instruction), equality check (for branch). instruction arithmetic-logical instruction, resul ALU must written register. operation load store, ALU result used address either store value registers load value memory registers. result ALU memory written back register file. Branches require use ALU output determine next instruction address, comes either adder (where PC branch offset summed) adder increments current PC four. thick lines interconnecting functional units represent buses, consist multiple signals. arrows used guide reader knowing information flows. Since signal lines may cross, explicitly show crossing lines connected presence dot lines cross. 478First, several places, Figure 4.1 shows data going particular unit coming two different sources. example, value written PC come one two adders, data written register file come either ALU data memory, second input ALU come register immediate field instruction. practice, hese data lines cannot simply wired together; must add logic element chooses among multiple sources steers one sources destination. selection commo nly done device called multiplexor , although device might better called data selector . Appendix describes multiplexor, selects among several inputs based setting control lines. control lines set based pri marily information taken instruction executed. second omission Figure 4.1 several units must controlled depending type instruction. exampl e, data memory must read load write store. register file must written load arithmetic-logic al instruction. And, course, ALU must perform one several operations. ( Appendix describes detailed design ALU.) Like multiplexors, control lines set based various fields instruction direct operations. Figure 4.2 shows datapath Figure 4.1 three required multiplexors added, well control lines majo r functional units. control unit , instruction input, used determine set control lines functional units two multiplexors. top multiplex or, determines whether PC +4 branch destination address written PC, set based Zero output ALU, used perform comparison beq instruction. regularity simplicity RISC-V instruction set mean simple decoding process used determine set control lines. 479FIGURE 4.2 basic implementation RISC- V subset, including necessary multiplexors control lines. top multiplexor (“Mux”) controls value replaces PC (PC + 4 branch destination address); multiplexor controlled gate “ANDs” together Zero output ALU control signal indicates instruction branch. middle multiplexor, whose output returns register file, used steer output th e ALU (in case arithmetic-logical instruction) output data memory (in case load) writing register file. Finally, bott om-most multiplexor used determine whether second ALU input registers (for arithmetic-logi cal instruction branch) offset field th e instruction (for load store). added control li nes straightforward determine operation performed ALU, whether data memory read write, whether registers perform write operation. control lines shown color make easier see. 480In remainder chapter, refine view fill details, requires add functional units, increase number connections units, and, course, enhance control unit control actions taken different instruction classes. Sections 4.3 4.4 describe simple implementation uses single long clock cycle every instruction follows general form Figures 4.1 4.2. first design, every instruction begins execution one c lock edge completes execution next clock edge. easier understand, approach practical, since clock cycle must severely stretched accommodate longest instruction. designing control sim ple computer, look pipelined implementation complexities, including exceptions. Check many five classic components computer—shown page 235—do Figures 4.1 4.2 include? 4.2 Logic Design Conventions discuss design computer, must decide hardware logic implementing computer operate computer clocked. section reviews key ideas digital logic use extensively chapter. little background digital logic, find helpfu l read Appendix continuing. datapath elements RISC-V implementation consist two different types logic elements: elements operate data values elements contain state. elements operate data values combinational , means outputs depend current inputs. Given input, combinational element always produces output. ALU shown Figure 4.1 discussed Appendix example combinational element. Given set inputs, always produces output internal storage. combinational element 481An operational element, gate ALU elements design combinational, instead contain state . element contains state internal storage. call elements state elements because, pulled power plug computer, could restart accurately loading state elements values contained pulled plug. Furthermore, saved restored state elements, would computer never lost power. Thus, state elements completely characterize computer. Figure 4.1 , instruction data memories, well registers, examples state elements. state element memory element, register memory. state element least two inputs one output. required inputs data value written element clock, determines data value written. output state element provides value written earlier clock cycle. example, one logically simplest state elements D-type flip-flop (see Appendix ), exactly two inputs (a value clock) one output. addition flip-flops, RISC-V implementation use two types state elements: memories registers, f appear Figure 4.1 . clock used determine state element written; state element read time. Logic components contain state also called sequential , outputs depend inputs content internal state. example, output functional unit representing registers depends registe r numbers supplied written registers previously. Appendix discusses operation combinational sequential elements construction detail. 482Clocking Methodology clocking methodology approach used determine data valid stable relative clock. clocking methodology defines signals read written. important specify timing reads writes, signal written time read, value read could correspond old value, newly written value, even mix two! Computer designs cannot tolerate unpredictability. clocking methodology designed make hardware predictable. edge-triggered clocking clocking scheme state changes occur clock edge. simplicity, assume edge-triggered clocking methodology. edge-triggered clocking methodology means hat values stored sequential logic element updated clock edge, quick transition low high vice versa (see Figure 4.3 ). state elements store data value, collection combinational logic must inputs come set state elements outputs written set state elements. inputs values written previous clock cycle, outputs values used following clock cycle. FIGURE 4.3 Combinational logic, state elements, clock closely related. 483In synchronous digital system, clock determines elements state write values interna l storage. inputs state element must reach stable value (that is, reached value change clock edge) active clock edge causes state updated. state elements chapter, including memory, assumed positive edge-triggered; is, change rising clock edge. Figure 4.3 shows two state elements surrounding block combinational logic, operates single clock cycle: signals must propagate state element 1, combinational logic, state element 2 time one clock cycle. time necessary signals reach state element 2 defines length clock cycle. control signal signal used multiplexor selection directing operation functional unit; contrasts data signal , contains information operated functional unit. simplicity, show write control signal state element written every active clock edge. contrast, f state element updated every clock, explicit write control signal required. clock signal write control signal inputs, state element changed write control signal asserted clock edge occurs. use word asserted indicate signal logically high assert specify signal driven logically high, deassert deasserted represent logically low. use terms assert deassert implement hardware, times 1 represents logically high times represent logically low. asserted signal logically high true. deasserted 484The signal logically low false. edge-triggered methodology allows us read contents register, send value combinational logic, write register clock cycle. Figure 4.4 gives generic example. doesn’t matter whether assume writes take place rising clock edge (from low high) falling clock edge (from high low), since inputs combination al logic block cannot change except chosen clock edge. book, use rising clock edge. edge-triggered timing methodology, feedback within single clock cycle, logic Figure 4.4 works correctly. Appendix , briefly discuss additional timing constraints (such setup hold time s) well timing methodologies. FIGURE 4.4 edge-triggered methodology allows state element read written clock cycle without creating race could lead indeterminate data values. course, clock cycle still must long enough input values stable active clock edge occurs. Feedback cannot occur within one clock cycle edge-triggered update state element. feedback possible, design could work properly. designs chapter next rely edge-triggered timing methodology structures like one shown figure. 64-bit RISC-V architecture, nearly state logic elements inputs outputs 64 bits wide, since width data handled processor . make clear whenever unit input output 64 bits width. figures indicate buses , 485are signals wider 1 bit, thicker lines. times, want combine several buses form wider bus; example, may want obtain 64-bit bus combining two 32-bit buses. cases, labels bus lines make clear concatenating buses form wider bus. Arrows also added help clarify direction flow data elements. Finally, color indicates control signal contrary signal carries data; distinction become clearer proceed chapter. Check True false: register file read written clock cycle, RISC-V datapath using edge-triggered writes must one copy register file. Elaboration also 32-bit version RISC-V architecture, and, naturally enough, paths implementation would 32 bits wide. 4.3 Building Datapath reasonable way start datapath design examine major components required execute class RISC-V instructio ns. Let’s start top looking datapath elements instruction needs, work way levels abstraction . show datapath elements, also show control signals. use abstraction explanation, starting bottom up. 486 datapath element unit used operate hold data within processor. RISC-V implementation, datapath elements include instruction data memories, register file, ALU, adders. program counter (PC) register containing address instruction program executed. Figure 4.5a shows first element need: memory unit store instructions program supply instructions giv en address. Figure 4.5b also shows program counter (PC) , saw Chapter 2 register holds address current instruction. Lastly, need adder increment th e PC address next instruction. adder, combinational, built ALU described detail Appendix simply wiring control lines control always specifies add operation. draw ALU 487the label Add, Figure 4.5c , indicate permanently made adder cannot perform ALU functions. FIGURE 4.5 Two state elements needed store access instructions, adder needed compute next instruction address. state elements instruction memory program counter. instruction memory need provide read access datapath write instructions. Since instruction memory reads, treat combinational logic: output time reflects contents location specified address input, read control signal needed. (We need write instruction memory load program; hard add, ignore simplicity.) program counter 64-bit register written end every clock cycle thus need write control signal. adder ALU wired always add two 64-bit inputs place sum output. execute instruction, must start fetching instruction memory. prepare executing next instruction, must also increment program counter points next instruction, 4 bytes later. Figure 4.6 shows combine three elements Figure 4.5 form datapath fetches instructions increments PC obtain address next sequential instruction. 488FIGURE 4.6 portion datapath used fetching instructions incrementing program counter. fetched instruction used parts datapath. let’s consider R-format instructions (see Figure 2.19 page 120). read two registers, perform ALU operation contents registers, write result regist er. call instructions either R-type instructions arithmetic- logical instructions (since perform arithmetic logical operations). instruction class includes add, sub, and, or, introduced Chapter 2 . Recall typical instance instruction add x1, x2, x3 , reads x2 x3 writes sum x1. register file state element consists set registers read written supplying register number accessed. processor’s 32 general-purpose registers stored 489structure called register file . register file collection registers register read written specify ing number register file. register file contai ns register state computer. addition, need ALU operate values read registers. R-format instructions three register operands, need read two data words register file write one data word register file instruction. data word read registers, need input registe r file specifies register number read output register file carry value read th e registers. write data word, need two inputs: one specify register number written one supply data written register. register file always outputs contents whatever register numbers Read register inputs. Writes, however, controlled write control sign al, must asserted write occur clock edge. Figure 4.7a shows result; need total three inputs (two register numbers one data) two outputs (both data). register number inputs 5 bits wide specify one 32 registers (32 = 25), whereas data input two data output buses 64 bits wide. FIGURE 4.7 two elements needed implement R-format ALU operations register file ALU. register file contains registers two read ports one write port. design multiported register files discussed Section A.8 490Appendix . register file always outputs contents registers corresponding Read register inputs outputs; control inputs needed. contrast, register write must explicitly indicated asserting write control signal. Remember writes edge-triggered, write inputs (i.e., value written, register number, write control signal) must valid clock edge. Since writes register file edge-triggered, design legally read write register within clock cycle: read get value written earlier clock cycle, whil e value written available read subsequent clock cycle. inputs carrying register number register file 5 bits wi de, whereas lines carrying data values 64 bits wide. operation performed ALU controlled ALU operation signal, b e 4 bits wide, using ALU designed Appendix . use Zero detection output ALU shortly implement conditional branches. Figure 4.7b shows ALU, takes two 64-bit inputs produces 64-bit result, well 1-bit signal result 0. 4-bit control signal ALU described detail Appendix ; review ALU control shortly need know set it. Next, consider RISC-V load register store register instructions, general form ld x1, offset(x2) sd x1, offset(x2) . instructions compute memory address adding base register, x2, 12-bit signed offset field contained instruction. instruction store, v alue stored must also read register file resid es x1. instruction load, value read memory must written register file specified register, whic h x1. Thus, need register file ALU Figure 4.7. sign-extend increase size data item replicating high-order sign bit original data item high-order bits lar ger, 491destination data item. branch target address address specified branch, becomes new program counter (PC) branch taken. RISC-V architecture, branch target given sum offset fi eld instruction address branch. addition, need unit sign-extend 12-bit offset field instruction 64-bit signed value, data memory unit read write to. data memory must written store instructions; hence, data memory read write control signals, address input, input data written memory. Figure 4.8 shows two elements. FIGURE 4.8 two units needed implement loads stores, addition register file ALU Figure 4.7 , data memory unit immediate generation unit. memory unit state element inputs address write data, single output th e read result. separate read write controls, although one may asserted given clock. memory unit needs read signal, since, unlike register file, reading valu e invalid address cause problems, see Chapter 5 . immediate generation unit 492(ImmGen) 32-bit instruction input selects 12-bit field load, store, branch equal hat sign-extended 64-bit result appearing output (see Chapter 2 ). assume data memory edge-triggered writes. Standard memory chips actually write enable signal used writes. Although write enable edge-trigger ed, edge-triggered design could easily adapted work real memory chips. See Section A.8 Appendix discussion real memory chips work. beq instruction three operands, two registers compared equality, 12-bit offset used compute branch target address relative branch instruction address. form beq x1, x2, offset . implement instruction, must compute branch target address adding sign- extended offset field instruction PC. two details definition branch instructions (see Chapter 2 ) must pay attention: instruction set architecture specifies base branch address calculation address branch instruction. architecture also states offset field shifted lef 1 bit half word offset; shift increases effective range offset field factor 2. deal latter complication, need shift offset field 1. well computing branch target address, must also determine whether next instruction instruction hat follows sequentially instruction branch target addre ss. condition true (i.e., two operands equal), branch target address becomes new PC, say branch taken . operand zero, incremented PC replace current PC (just normal instruction); case, say branch taken . branch taken branch branch condition satisfied program counter (PC) becomes branch target. unconditional 493branches taken branches. branch taken (untaken branch) branch branch condition false program counter (PC) becomes address instruction sequentially follows branch. Thus, branch datapath must two operations: compute branch target address test register contents. (Branches also affect instruction fetch portion datapath, deal shortly.) Figure 4.9 shows structure datapath segment handles branches. compute branch target address, branch datapath includes immediate generation unit, Figure 4.8 adder. perform compare, need use register file shown Figure 4.7a supply two register operands (although need write register file). addition, equality comparison done using ALU designed Appendix . Since ALU provides output signal indicates whether result 0, send register operands ALU control se subtract two values. Zero signal ALU unit asserted, know register values equal. Although Zero output always signals result 0, using onl implement equality test conditional branches. Later, show exactly connect control signals ALU f use datapath. 494FIGURE 4.9 datapath branch uses ALU evaluate branch condition separate adder compute branch target sum PC sign-extended 12 bits instruction (the branch displacement), shifted left 1 bit. unit labeled Shift left 1 simply routing signals input output adds 0two low-order end sign-extended offset field; actual shift hardware needed, since amount “shift” constant. Since know offset sign-extended 12 bits, shift throw away “sign bits.” Control logic used decide whether incremented PC branch target replace PC, based Zero output ALU. branch instruction operates adding PC 12 bits instruction shifted left 1 bit. Simply concatenating 0 branch offset accomplishes shift, described Chapter 2 . Creating Single Datapath examined datapath components needed 495the individual instruction classes, combine single datapath add control complete implementation. simplest datapath attempt execute instructions one clock cycle. design means datapath resource used per instruction, element needed must duplicated. therefore need memory instructions separate one data. Although functional units need duplicated , many elements shared different instruction flow s. share datapath element two different instruction classes, may need allow multiple connections input element, using multiplexor control signal select among multiple inputs. Building Datapath Example operations arithmetic-logical (or R-type) instructions memory instructions datapath quite similar. key differences following: arithmetic-logical instructions use ALU, input coming two registers. memory instructions also use ALU address calculation, although second input sign-extended 12-bit offset field instru ction. value stored destination register comes ALU (for R-type instruction) memory (for load). Show build datapath operational portion memory-reference arithmetic-logical instructions use single register file single ALU handle types instructions, adding necessary multiplexors. Answer create datapath single register file single ALU, must support two different sources second ALU input, well two different sources data stored register file. Thus, one multiplexor placed ALU input another data input register file. Figure 4.10 shows operational portion combined datapath. 496FIGURE 4.10 datapath memory instructions R-type instructions. example shows single datapath assembled pieces Figures 4.7 4.8 adding multiplexors. Two multiplexors needed, described example. combine pieces make simple datapath core RISC-V architecture adding datapath instruction fetch ( Figure 4.6 ), datapath R-type memory instructions ( Figure 4.10 ), datapath branches ( Figure 4.9). Figure 4.11 shows datapath obtain composing separate pieces. branch instruction uses main ALU compare two register operands equality, must keep adder Figure 4.9 computing branch target address. additional multiplexor required select either seque ntially following instruction address (PC +4) branch target address written PC. Check I. following correct load instruction? Refe r Figure 4.10 . a. MemtoReg set cause data memory sent register file. b. MemtoReg set cause correct register destination sent register file. c. care setting MemtoReg loads. 497II. single-cycle datapath conceptually described se ction must separate instruction data memories, a. formats data instructions different RISC-V, hence different memories needed; b. separate memories less expensive; c. processor operates one cycle cannot use (single- ported) memory two different accesses within cycle. 498FIGURE 4.11 simple datapath core RISC-V architecture combines elements required different instruction classes. components come Figures 4.6 , 4.9, 4.10. datapath execute basic instructions (load- store register, ALU operations, branches) single clock cycle. one additional multiplexor needed integrate branches. completed simple datapath, add control unit. control unit must able take inputs generate write signal state element, selector contro l multiplexor, ALU control. ALU control different number ways, useful design firs design rest control unit. Elaboration immediate generation logic must choose sign- extending 12-bit field instruction bits 31:20 load instructions, bits 31:25 11:7 store instructions, bits 31, 7, 30:25, 11:8 conditional branch. Since input 32 bits instruction, use opcode bits instruc tion select proper field. RISC-V opcode bit 6 happens 0 499data transfer instructions 1 conditional branches, RISC- V opcode bit 5 happens 0 load instructions 1 store instructions. Thus, bits 5 6 control 3:1 multiplexor inside immediate generation logic selects appropriate 12-bit field load, store, conditional branch instructions. 4.4 Simple Implementation Scheme section, look might thought simple implementation RISC-V subset. build simple implementation using datapath last section adding simple control function. simple implementation covers load doubleword (ld), store doubleword (sd), branch equal (beq), arithmetic-logical instructions add, sub, and, or. ALU Control RISC-V ALU Appendix defines four following combinations four control inputs: ALU control lines Function 0000 0001 0010 add 0110 subtract Depending instruction class, ALU need perfor one four functions. load store instructions, u se ALU compute memory address addition. R- type instructions, ALU needs perform one four action (AND, OR, add, subtract ), depending value 7-bit funct7 field (bits 31:25) 3-bit funct3 field (bits 14:12) instruction (see Chapter 2 ). conditional branch equal instruction, ALU subtracts two operands tests see result 0. generate 4-bit ALU control input using small control unit inputs funct7 funct3 fields instruction 2-bit control field, call ALUOp. ALUOp indicates whether operation performed add (00) loads stores, subtract test zero (01) beq, determined operation encoded funct7 funct3 field (10). output ALU control unit 4-bit signal 500directly controls ALU generating one 4-bit combinations shown previously. Figure 4.12 , show set ALU control inputs based 2-bit ALUOp control, funct7, funct3 fields. Later chapter, see ALUOp bits generated main control unit. FIGURE 4.12 ALU control bits set depends ALUOp control bits different opcodes R-type instruction. instruction, listed first column, determines setting ALUOp bits. encodings shown binary. Notice ALUOp code 00 01, desired ALU action depend funct7 funct3 fields; case, say “don’t care” value opcode, bi ts shown Xs. ALUOp value 10, funct7 funct3 fields used set ALU control input. See Appendix . style using multiple levels decoding—that is, main control unit generates ALUOp bits, used input ALU control generates actual signals contr ol ALU unit—is common implementation technique. Using multiple levels control reduce size main control unit. Using several smaller control units may also potentially reduce latency control unit. optimizations 501important, since latency control unit often critical factor determining clock cycle time. several different ways implement mapping 2-bit ALUOp field funct fields four ALU operation control bits. small number possible funct field values interest funct fields used wh en ALUOp bits equal 10, use small piece logic recognizes subset possible values generates appropriate ALU control signals. step designing logic, useful create truth table interesting combinations funct fields ALUOp signals, we’ve done Figure 4.13 ; truth table shows 4-bit ALU control set depending input fields. Sin ce full truth table large, don’t care value ALU control many input combinations, show truth table entries ALU control must specific value. Throughout chapter, use practic e showing truth table entries outputs must asserted showing deasserted don’t care. (This practice disadvantage, discuss Section C.2 Appendix C .) truth table logic, representation logical operation listing values inputs case showing resulting outputs be. FIGURE 4.13 truth table 4 ALU control bits (called Operation). inputs ALUOp funct fields. entries ALU control asserted 502shown. don’t-care entries added. example, ALUOp use encoding 11, truth table contain entries 1X X1, rather 10 01. show 10 bits funct fields, note bits different values fo r four R-format instructions bits 30, 14, 13, 12. Thus, need four funct field bits input ALU control instead 10. many instances care values inputs, wish keep tables compact, also include don’t-care terms . don’t-care term truth table (represented X input column) indicates output depend value input corresponding column. example, ALUOp bits 00, first row Figure 4.13 , always set ALU control 0010, independent funct fields. case, then, funct inputs don’t cares line truth table . Later, see examples another type don’t-care term. unfamiliar concept don’t-care terms, see Appendix information. truth table constructed, optimized turned gates. process completely mechanical. Thus, rather show final steps here, describe proces result Section C.2 Appendix C . don’t-care term element logical function output depend values inputs. Don’t-care terms may specified different ways. Designing Main Control Unit described design ALU uses opcode 2-bit signal control inputs, return looking rest control. start process, let’s ide ntify fields instruction control lines needed datapath constructed Figure 4.11 . understand connect fields instruction datapath, useful 503review formats four instruction classes: arithmetic , load, store, conditional branch instructions. Figure 4.14 shows formats. FIGURE 4.14 four instruction classes (arithmetic, load, store, conditional branch) use four different instruction formats. (a) Instruction format R-type arithmetic instructions (opcode = 51ten), three register operands: rs1, rs2, rd. Fields rs1 rd sources, rd destination. ALU function funct3 funct7 fields decoded ALU control design previous section. R-type instructions implement add, sub, and, or. (b) Instruction format I-type load instructions (opcode = 3ten). register rs1 base register added 12-bit immediate field form memory address. Field rd destination register loaded value. (c) Instruction format S-type store instructions (opcode = 35ten). register rs1 base register added 12-bit immediate fi eld form memory address. (The immediate field split 7-bit piece 5-bit piece.) Field rs2 source register whose value stored memory. (d) Instruction format SB-type conditional branch instructions (opcode =99ten). registers rs1 rs2 compared. 12-bit immediate address field sign-extended, shifted left 1 bit, added PC compute branch target address. several major observations instruction format rely on: 504 opcode field denotes operation format instruction. opcode field, saw Chapter 2 , always bits 6:0. Depending opcode, funct3 field (bits 14:12) funct7 field (bits 31:25) serve extended opcode field. first register operand always bit positions 19:15 (rs1) R-type instructions branch instructions. field also specifies base register load store instructions. second register operand always bit positions 24:20 (rs2) R-type instructions branch instructions. field also specifies register operand gets copied memory store instructions. Another operand also 12-bit offset branch load- store instructions. destination register always bit positions 11:7 (rd) R- type instructions load instructions. first design principle Chapter 2 —simplicity favors regularity —pays specifying control. Using information, add instruction labels simple datapath. Figure 4.15 shows additions plus ALU control block, write signals state elements, read sign al data memory, control signals multiplexors. Sinc e multiplexors two inputs, require single control line. 505FIGURE 4.15 datapath Figure 4.11 necessary multiplexors control lines identified. control lines shown color. ALU control block also added, depends funct3 field part funct7 field. PC doe require write control, since written end every clock cycle; branch control logic determines whether written incremented PC branch target address. Figure 4.15 shows six single-bit control lines plus 2-bit ALUOp control signal. already defined ALUOp control signal works, useful define six control signals informally determine set se control signals instruction execution. Figure 4.16 describes function six control lines. 506FIGURE 4.16 effect six control signals. 1-bit control two-way multiplexor asserted, multiplexor selects input corresponding 1. Otherwise, control deasserted, multiplexor selects 0 input. Remember state elements clock implicit input clock used controlling writes. Gating clock externally state element create timing problems. (See Appendix discussion problem.) looked function control signals, look set them. control unit set one control signals based solely opcode func fields instruction. PCSrc control line except ion. control line asserted instruction branch equal (a decision control unit make) Zero output ALU, used equality test, asserted . generate PCSrc signal, need together signal control unit, call Branch , Zero signal ALU. eight control signals (six Figure 4.16 two ALUOp) set based input signals control unit, opcode bits 6:0. Figure 4.17 shows datapath control unit control signals. 507FIGURE 4.17 simple datapath control unit. input control unit 7-bit opcode fiel instruction. outputs control unit consist two 1-bit signals used control multiplexors (ALUSrc MemtoReg), three signals controlling reads writes register file nd data memory (RegWrite, MemRead, MemWrite), 1-bit signal used determining whether possibly branch (Branch), 2-bit control signal ALU (ALUOp). gate used combine branch control signal Zero output ALU; gate output controls selection next PC. Notice PCSrc derived signal, rather one coming directly control unit. Thus, drop signal name subsequent figures. try write set equations truth table control unit, useful try define control fun ction informally. setting control lines depends onl opcode, define whether control signal 0, 1, don’t care (X) opcode values. Figure 4.18 defines 508how control signals set opcode; information follows directly Figures 4.12 , 4.16, 4.17. FIGURE 4.18 setting control lines completely determined opcode fields instruction. first row table corresponds R-format instructions ( add, sub, and, or). instructions, source register fields rs1 rs2, destination register field rd; defines h ow signals ALUSrc set. Furthermore, R-type instruction writes register (RegWrite =1), neithe r reads writes data memory. Branch control signal 0, PC unconditionally replaced PC +4; otherwise, PC replaced branch target Zero output ALU also hi gh. ALUOp field R-type instructions set 10 indicate ALU control generated funct fields. second third rows table give control signal settings ld sd. ALUSrc ALUOp fields set perform address calculation. MemRead MemWrite set perform memory access. Finally, RegWrite set load cause result stored r register. ALUOp field branch set subtract (ALU control =01), used test equality. Notice MemtoReg field irrelevant RegWrite signal 0: since register written, value data register data w rite port used. Thus, entry MemtoReg last two rows table replaced X don’t care . type don’t care must added designer, since depends knowledge datapath works. Operation Datapath 509With information contained Figures 4.16 4.18, design control unit logic, that, let’s look instruction uses datapath. next figures, show flow three different instruction classes datapath. asserted control signals active datapath elements highlighted these. Note multiplexor whose control 0 definite action, even control line highlighted. Multiple-bit control signals highlighted constituent signal asserted. Figure 4.19 shows operation datapath R-type instruction, add x1, x2, x3 . Although everything occurs one clock cycle, think four steps execute instruction; steps ordered flow information: 1. instruction fetched, PC incremented. 2. Two registers, x2 x3, read register file; also, main control unit computes setting control lines dur ing step. 3. ALU operates data read register file, using portions opcode generate ALU function. 4. result ALU written destination registe r (x1) register file. 510FIGURE 4.19 datapath operation R- type instruction, add x1, x2, x3 . control lines, datapath units, connections active highlighted. Similarly, illustrate execution load register, ld x1, offset(x2) style similar Figure 4.19 . Figure 4.20 shows active functional units asserted control lines load. think load instruction operating five steps (similar R- type executed four): 1. instruction fetched instruction memory, PC incremented. 2. register ( x2) value read register file. 3. ALU computes sum value read register file sign-extended 12 bits instruction ( offset ). 4. sum ALU used address data memory. 5. data memory unit written register file 511(x1). 512FIGURE 4.20 datapath operation load instruction. control lines, datapath units, connections active highlighted. store instruction would operate similarly. main difference would memory control would indicate write rather read, second register value read would used data store, operation writ ing data memory value register file would occur. Finally, show operation branch-if-equal instruction, beq x1, x2, offset , fashion. operates much like R-format instruction, ALU output used determine whether PC written PC +4 branch target address. Figure 4.21 shows four steps execution: 1. instruction fetched instruction memory, PC incremented. 2. Two registers, x1 x2, read register file. 3. ALU subtracts one data value data value, 513read register file. value PC added sign- extended, 12 bits instruction ( offset ) left shifted one; result branch target address. 4. Zero status information ALU used decide adder result store PC. 514FIGURE 4.21 datapath operation branch-if-equal instruction. control lines, datapath units, connections active highlighted. using register fi le ALU perform compare, Zero output used select next program counter two candidates. Finalizing Control seen instructions operate steps, let’s continue control implementation. control functio n precisely defined using contents Figure 4.18 . outputs control lines, inputs opcode bits. Thus, create truth table outputs based binary encoding opcodes. Figure 4.22 defines logic control unit one large truth table combines outputs uses opcode bits inputs. completely specifies control function, implement directly gates automated fashion. show 515this final step Section C.2 Appendix C . FIGURE 4.22 control function simple single-cycle implementation completely specified truth table. top half table gives combinations input signals correspond four instruction classes, one per column, determine control output settings. bottom portion table gives outputs four opcodes. Thus, output RegWrite asserted two different combinations inputs. consider four opcodes shown table, simplify truth table using don’t cares input portion. example, detect R-format instruction expression Op4 ∙ Op5, since sufficient distinguish R-format instructions ld, sd, beq. take advantage simplification, since rest RISC-V opcodes used full implementation. Single-Cycle Implementation Used Today Although single-cycle design work correctly, 516inefficient used modern designs. see so, notice clock cycle must length every instruction single-cycle design. course, longe st possible path processor determines clock cycle. Thi path likely load instruction, uses five functional unit series: instruction memory, register file, ALU, ata memory, register file. Although CPI 1 (see Chapter 1 ), overall performance single-cycle implementation lik ely poor, since clock cycle long. penalty using single-cycle design fixed cloc k cycle significant, might considered acceptable thi small instruction set. Historically, early computers sim ple instruction sets use implementation technique. Howev er, tried implement floating-point unit instruction set complex instructions, single-cycle design wo uldn’t work well all. must assume clock cycle equal worst-case delay instructions, it’s useless try implementation techniques reduce delay common case improve worst-case cycle time. single-cycl e implementation thus violates great idea Chapter 1 making common case fast . next section, we’ll look another implementation technique, called pipelining, uses datapath similar single- cycle datapath much efficient much higher throughput. Pipelining improves efficiency executing mul tiple 517instructions simultaneously. Check Look control signals Figure 4.22 . combine together? control signal output figure replaced inverse another? (Hint: take account don’t cares.) so, use one signal without adding inverter? 4.5 Overview Pipelining pipelining implementation technique multiple instructions ar e overlapped execution, much like assembly line. Pipelining implementation technique multiple instructions overlapped execution. Today, pipelining nearly universal. Never waste time. American proverb 518This section relies heavily one analogy give overview pipelining terms issues. interested th e big picture, concentrate section skip Sections 4.10 4.11 see introduction advanced pipelining techniques used recent processors Int el Core i7 ARM Cortex-A53. curious exploring anatomy pipelined computer, section good introduction Sections 4.6 4.9. Anyone done lot laundry intuitively used pipelining. non-pipelined approach laundry would follows: 1. Place one dirty load clothes washer. 2. washer finished, place wet load dryer. 3. dryer finished, place dry load table fold. 4. folding finished, ask roommate put clothes away. roommate done, start next dirty load. pipelined approach takes much less time, Figure 4.23 shows. soon washer finished first load placed dryer, load washer second dirty load. first load dry, place table start folding, move wet load dryer, put next dirty load 519washer. Next, roommate put first load away, start folding second load, dryer third load, put fourth load washer. point steps—called stages pipelining—are operating concurrently. long separate resources stage, pipeline tasks. 520FIGURE 4.23 laundry analogy pipelining. Ann, Brian, Cathy, dirty clothes washed, dried, folded, put away. washer, dryer, “folder,” “storer” take 30 minutes task. Sequential laundry takes 8 hours four loads washing, pipelined laundry takes 3.5 hours. show pipeline stage different loads time showing copies four resources two-dimensional time line, really one resource. pipelining paradox time placing single dirty sock washer dried, folded, put away shorter pipelining; reason pipelining faster many l oads everything working parallel, loads finished per hour. Pipelining improves throughput laundry system. Hence, pipelining would decrease time complete one load laundry, many loads laundry do, improvement throughput decreases total time complete work. stages take amount time 521enough work do, speed-up due pipelining equal number stages pipeline, case four: washing, drying, folding, putting away. Therefore, pipelined laundry potentially four times faster nonpipelined: 20 loads would take five times long one load, 20 loads sequential laundry takes 20 times long one load. It’s 2.3 times faster Figure 4.23 , show four loads. Notice beginning end workload pipelined version Figure 4.23 , pipeline completely full; start-up wind-down affects performance number tasks large compared number stages pipeline. number loads much larger four, stages full time increase throughput close four. principles apply processors pipeline instruction execution. RISC-V instructions classically take f ive steps: 1. Fetch instruction memory. 2. Read registers decode instruction. 3. Execute operation calculate address. 4. Access operand data memory (if necessary). 5. Write result register (if necessary). Hence, RISC-V pipeline explore chapter five stages. following example shows pipelining speeds instruction execution speeds laundry. Single-Cycle versus Pipelined Performance Example make discussion concrete, let’s create pipeline. example, rest chapter, limit attention seven instructions: load doubleword ( ld), store doubleword ( sd), add ( add), subtract ( sub), ( and), ( or), branch equal (beq). Contrast average time instructions single-cycle implementation, instructions take one clock cycle, pipelined implementation. Assume operation times th e major functional units example 200 ps memory access instructions data, 200 ps ALU operation, 522100 ps register file read write. single-cycle model, every instruction takes exactly one clock cycle, clock cyc le must stretched accommodate slowest instruction. Answer Figure 4.24 shows time required seven instructions. single-cycle design must allow slowe st instruction—in Figure 4.24 ld—so time required every instruction 800 ps. Similarly Figure 4.23 , Figure 4.25 compares nonpipelined pipelined execution three load register instructions. Thus, time first fourth instructions nonpipelined design 3 ×800 ps 2400 ps. FIGURE 4.24 Total time instruction calculated time component. calculation assumes multiplexors, control unit, PC accesses, sign extension unit delay. 523FIGURE 4.25 Single-cycle, nonpipelined execution (top) versus pipelined execution (bottom). use hardware components, whose time listed Figure 4.24 . case, see fourfold speed-up average time instructions, 800 ps 200 ps. Compare figure Figure 4.23. laundry, assumed stages equal. dryer slowest, dryer stage would set stage time. pipeline stage times computer also limited slowest resource, either ALU operation memory access. assume write register file occurs first half clock cycle read register file occurs second half. use assumption throughout chapter. pipeline stages take single clock cycle, clock cycl e must long enough accommodate slowest operation. single-cycle design must take worst-case clock cycle f 800 ps, even though instructions fast 500 ps, pipelined execution clock cycle must worst-case clock cycle 200 ps, even though stages take 100 ps. Pipelining still offers fourfold performance improvement: th e time first fourth instructions 3 ×200 ps 600 ps. 524We turn pipelining speed-up discussion formula. stages perfectly balanced, time instructions pipelined processor—assuming ideal conditi ons —is equal ideal conditions large number instructions, speed-up pipelining approximately equal number pipe stages; five-stage pipeline nearly five times faster. formula suggests five-stage pipeline offer nearly fivefold improvement 800 ps nonpipelined time, 160 ps clock cycle. example shows, however, stages may imperfectly balanced. Moreover, pipelining involves overhead, source clearer shortly. Thus, time per instruction pipelined processor exc eed minimum possible, speed-up less number pipeline stages. However, even claim fourfold improvement example reflected total execution time thr ee instructions: it’s 1400 ps versus 2400 ps. course, number instructions large. would happen increased number instructions? could extend previous figures 1,000,003 instructions. would add 1,000,000 instructions pipelined example; instruction adds 200 ps total execution time. total execution time would 1,000,000 ×200 ps +1400 ps, 200,001,400 ps. nonpipelined example, would add 1,000,000 instructions, taking 800 ps, total execution time would 1,000,000 ×800 ps +2400 ps, 800,002,400 ps. conditions, ratio total execution times real programs nonpipelined pipelined processor close ratio times instructions: 525Pipelining improves performance increasing instruction throughput, contrast decreasing execution time individual instruction , instruction throughput important metric real programs execute billions instructions. Designing Instruction Sets Pipelining Even simple explanation pipelining, get insigh design RISC-V instruction set, design ed pipelined execution. First, RISC-V instructions length. restricti makes much easier fetch instructions first pipeline tage decode second stage. instruction set like x86, instructions vary 1 byte 15 bytes, pipelining considerably challenging. Modern implementations th e x86 architecture actually translate x86 instructions simple operations look like RISC-V instructions pipeline simple operations rather native x86 instructions! (See Section 4.10 .) Second, RISC-V instruction formats, source destination register fields located place instruction. Third, memory operands appear loads stores RISC- V. restriction means use execute stage calculate memory address access memory following stage. could operate operands memory, x86, stages 3 4 would expand address stage, memory stage, execute stage. shortly see downside longer pipelines. Pipeline Hazards situations pipelining next instruction cann ot execute following clock cycle. events called hazards , three different types. Structural Hazard structural hazard 526When planned instruction cannot execute proper clock cycle hardware support combination instructions set execute. first hazard called structural hazard . means hardware cannot support combination instructions want execute clock cycle. structural hazard laundry room would occur used washer-dryer combination instead separate washer dryer, roommate busy something else wouldn’t put clothes away. carefully scheduled pipeline plans would foiled. said above, RISC-V instruction set designed pipelined, making fairly easy designers avoid structural hazards designing pipeline. Suppose, however, single memory instead two memories. pipeline Figure 4.25 fourth instruction, would see clock cycle, first instruction accessing data memory fourth instruction fetching instruction memory. Without two memories, pipeline could structural hazard. Data Hazards data hazard Also called pipeline data hazard . planned instruction cannot execute proper clock cycle data needed execute instruction yet available. Data hazards occur pipeline must stalled one step must wait another complete. Suppose found sock folding station match existed. One possible strategy run room search clothes bureau see find match. Obviously, search, loads completed drying ready fold finished washing ready dry. computer pipeline, data hazards arise dependence one instruction earlier one still pipeline ( relationship really exist laundry). 527example, suppose add instruction followed immediately subtract instruction uses sum ( x19): add x19, x0, x1 sub x2, x19, x3 Without intervention, data hazard could severely stall pipeline. add instruction doesn’t write result fifth stage, meaning would waste three clock cycles pipeline. Although could try rely compilers remove hazards, results would satisfactory. dependences happen often delay far long expect compiler rescue us dilemma. primary solution based observation don’t need wait instruction complete trying solve data hazard. code sequence above, soon ALU creates sum add, supply input subtract. Adding extra hardware retrieve missing item early internal resources called forwarding bypassing . forwarding Also called bypassing . method resolving data hazard retrieving missing data element internal buffers rathe r waiting arrive programmer-visible registers memory. Forwarding Two Instructions Example two instructions above, show pipeline stages would connected forwarding. Use drawing Figure 4.26 represent datapath five stages pipeline. Align copy datapath instruction, similar laundry pipeline Figure 4.23 . 528FIGURE 4.26 Graphical representation instruction pipeline, similar spirit laun dry pipeline Figure 4.23 . use symbols representing physical resources abbreviations pipeline stages used throughout chapter. symbols five stages: instruction fetch stage, box representing instruction memory; ID instruction decode/register file read stage, drawing showing register file read; EX execution stage, drawing representing ALU; MEM memory access stage, box representing data memory; WB write-back stage, drawing showing register file bein g written. shading indicates element used instruction. Hence, MEM white background add access data memory. Shading right half register file mory means element read stage, shading left half means written stage. nce right half ID shaded second stage register file read, left half WB shaded fifth stage register file written. Answer Figure 4.27 shows connection forward value x1 execution stage add instruction input execution stage sub instruction. 529FIGURE 4.27 Graphical representation forwarding. connection shows forwarding path output EX stage add input EX stage sub, replacing value register x1 read second stage sub. graphical representation events, forwarding paths valid destination stage later time source stage. example, cannot valid forwarding path output memory access stage first instruction input execution stage following, since would going backward time. Forwarding works well described detail Section 4.7. cannot prevent pipeline stalls, however. example, suppose first instruction load x1 instead add. imagine looking Figure 4.27 , desired data would available fourth stage first instruction dependence, late input third stage sub. Hence, even forwarding, would stall one stage load-use data hazard , Figure 4.28 shows. figure shows important pipeline concept, officially called pipeline stall , often given nickname bubble . shall see stalls elsewhere pipeline. Section 4.7 shows handle hard cases like these, using either hardware detection stalls software reorders code try avoid load-use pipeline stalls, example illustrates. load-use data hazard specific form data hazard data loaded 530load instruction yet become available needed another instruction. pipeline stall Also called bubble . stall initiated order resolve hazard. Reordering Code Avoid Pipeline Stalls Example Consider following code segment C: = b + e; c = b + f; generated RISC-V code segment, assuming variables memory addressable offsets x31: ld x1, 0(x31) // Load b ld x2, 8(x31) // Load e add x3, x1, x2 // b + e sd x3, 24(x31) // Store ld x4, 16(x31) // Load f add x5, x1, x4 // b + f sd x5, 32(x31) // Store c Find hazards preceding code segment reorder instructions avoid pipeline stalls. Answer add instructions hazard respective dependence previous ld instruction. Notice forwarding eliminates several potential hazards, including dependence first add first ld hazards store instructions. Moving third ld instruction become third instruction eliminates hazards: ld x1, 0(x31) ld x2, 8(x31) ld x4, 16(x31) add x3, x1, x2 sd x3, 24(x31) add x5, x1, x4 sd x5, 32(x31) 531On pipelined processor forwarding, reordered sequence complete two fewer cycles original version. FIGURE 4.28 need stall even forwarding R-format instruction following load tries use data. Without stall, path memory access stage output execution stage input would going backward time, impossible. figure actually simplification, since cannot know subtract instruction fetched decoded whether stall necessary. Section 4.7 shows details really happens case hazard. Forwarding yields another insight RISC-V architecture, addition three mentioned page 267. RISC-V instruction writes one result last st age pipeline. Forwarding harder multiple resul ts forward per instruction need write result early instruction execution. Elaboration name “forwarding” comes idea result passed forward earlier instruction later instruction. “Bypassing” comes passing result around register fil e desired unit. 532Control Hazards third type hazard called control hazard , arising need make decision based results one instruction others executing. control hazard Also called branch hazard . proper instruction cannot execute proper pipeline clock cycle instruc tion fetched one needed; is, flow instruction addresses pipeline expected. Suppose laundry crew given happy task cleaning uniforms football team. Given filthy laundry is, need determine whether detergent water temperature setting select strong enough get uniforms clean bu strong uniforms wear sooner. laundry pipeline, wait second stage examine dry uniform see need change washer setup not. do? first two solutions control hazards laundr room computer equivalent. Stall: operate sequentially first batch dry repeat right formula. conservative option certainly works, slow. equivalent decision task computer conditional branch instruction. Notice must begin fetching instruction following branch following clock cycle . Nevertheless, pipeline cannot possibly know next instruction be, since received branch instruction memory! laundry, one possible solution stall immediately fetch branch, waiting pipeline determines outcome branch know instruction address fetch from. Let’s assume put enough extra hardware test register, calculate branch address, update PC second stage pipeline (see Section 4.8 details). Even added hardware, pipeline involving condition al branches would look like Figure 4.29 . instruction executed 533if branch fails stalled one extra 200 ps clock cycle starting. Performance “Stall Branch” Example Estimate impact clock cycles per instruction (CPI) stalling branches. Assume instructions CPI 1. Answer Figure 3.28 Chapter 3 shows conditional branches 17% instructions executed SPECint2006. Since instructions run CPI 1, conditional branches took one extra clock cycle stall, would see CPI 1.17 hence slowdown 1.17 versus ideal case. FIGURE 4.29 Pipeline showing stalling every conditional branch solution control hazards. example assumes conditional branch taken, instruction destination branch instruction. one-stage pipeline stall, bubble, branch. reality, process creating stall slightly complicated, see Section 4.8 . effect performance, however, would occur bubble inserted. cannot resolve branch second stage, often case longer pipelines, we’d see even larger slowdown stall conditional branches. cost option hig h 534for computers use motivates second solution control hazard using one great ideas Chapter 1 : Predict: you’re sure right formula wash uniforms, predict work wash second load waiting first load dry. option slow pipeline correct. wrong, however, need redo load washed guessing decision. Computers indeed use prediction handle conditional branches. One simple approach predict always conditional branches untaken. you’re right, pipeline proceed full speed. conditional branches taken pipeline stall. Figure 4.30 shows example. 535FIGURE 4.30 Predicting branches taken solution control hazard. top drawing shows pipeline branch taken. bottom drawing shows pipeline branch taken. noted Figure 4.29 , insertion bubble fashion simplifies wha actually happens, least first clock cycle immediately following branch. Section 4.8 reveal details. sophisticated version branch prediction would conditional branches predicted taken untaken. analogy, dark home uniforms might take one formula light road uniforms might take another. case programming, bottom loops conditional branches branch back top loop. Since likely taken branch backward, could always predict taken conditional branches branch earlier address. 536branch prediction method resolving branch hazard assumes given outcome conditional branch proceeds assumption rather waiting ascertain actual outcome. rigid approaches branch prediction rely stereotypical behavior don’t account individuality specific branch instruction. Dynamic hardware predictors, stark contrast, make guesses depending behavior conditional bran ch may change predictions conditional branch life program. Following analogy, dynamic prediction person would look dirty uniform guess formula, adjusting next prediction depending success recent guesses. One popular approach dynamic prediction conditional branches keeping history conditional branch taken untaken, using recent past behavior predict future. see later, amount type history kept 537have become extensive, result dynamic branch predictors correctly predict conditional branches mo 90% accuracy (see Section 4.8 ). guess wrong, pipeline control must ensure instructions followin g wrongly guessed conditional branch effect must restart pipeline proper branch address. laundry analogy, must stop taking new loads restart load incorrectly predicted. case solutions control hazards, longer pipelines exacerbate problem, case raising cost f misprediction. Solutions control hazards described mor e detail Section 4.8 . Elaboration third approach control hazard, called delayed decision . analogy, whenever going make decision laundry, place load non-football clothes washer waiting football uniforms dry. long enough dirty clothes affected test, thi solution works fine. Called delayed branch computers, solution actually used MIPS architecture. delayed branch always executes next sequential instruction, branch taking place one instruction delay. hidden MIPS assembly language programmer assembler automatically arrange instructions get branch behavior desired programmer. MIPS software place instruction immediately delayed branch instruction hat affected branch, taken branch changes address instruction follows safe instruction. example, add instruction branch Figure 4.29 affect branch moved branch hide branch delay fully. Since delayed branches useful branches short, rare see processor delayed branch one cycle. longer branch delays, hardware-based branch prediction usually used. Pipeline Overview Summary 538Pipelining technique exploits parallelism instructions sequential instruction stream. substan tial advantage that, unlike programming multiprocessor (see Chapter 6), fundamentally invisible programmer. next sections chapter, cover concept pipelining using RISC-V instruction subset sing le-cycle implementation Section 4.4 show simplified version pipeline. look problems pipelining introduces performance attainable typical situations. 539If wish focus software performance implications pipelining, sufficient background skip Section 4.10 . Section 4.10 introduces advanced pipelining concepts, superscalar dynamic scheduling, Section 4.11 examines pipelines recent microprocessors. Alternatively, interested understanding pipelining implemented challenges dealing hazards, proceed examine design pipelined datapath basic control, explained Section 4.6 . use understanding explore implementation forwarding stalls Section 4.7 . next read Section 4.8 learn solutions branch hazards, finally see exceptions handled Section 4.9 . Check code sequence below, state whether must stall, avoid stalls using forwarding, execute without stallin g forwarding. Sequence 1 Sequence 2 Sequence 3 ld x10, 0(x10) add x11, x10, x10 addi x11, x10, 1 add x11, x10, x10 addi x12, x10, 5 addi x12, x10, 2 addi x14, x11, 5 addi x13, x10, 3 addi x14, x10, 4 540 addi x15, x10, 5 Understanding Program Performance Outside memory system, effective operation pipe line usually important factor determining CPI th e processor hence performance. see Section 4.10 , understanding performance modern multiple-issue pipelined processor complex requires understanding mo issues arise simple pipelined processor. Nonetheless, structural, data, control hazards remain important simple pipelines sophisticated ones. modern pipelines, structural hazards usually revolve around floating-point unit, may fully pipelined, control hazards usually problem integer programs, tend higher conditional branch frequencies well less predictable branches. Data hazards performance bottlenecks integer floating-point programs. Often easier deal data hazards floating-point programs lower conditional branch frequency regular memory access patterns allow compiler try schedule instructi ons avoid hazards. difficult perform optimizations integer programs less regular memory accesses, involving use pointers. see Section 4.10 , ambitious compiler hardware techniques reducing data dependences scheduling. BIG Picture Pipelining increases number simultaneously executing instructions rate instructions started completed. Pipelining reduce time takes comp lete individual instruction, also called latency . example, five-stage pipeline still takes five clock cycles inst ruction complete. terms used Chapter 1 , pipelining improves instruction throughput rather individual instruction execution time latency . 541Instruction sets either make life harder simpler pipeline designers, must already cope structural, contro l, data hazards. Branch prediction forwarding help make computer fast still getting right answers. 542 latency (pipeline) number stages pipeline number stages two instructions execution. 4.6 Pipelined Datapath Control less meets eye. Tallulah Bankhead, remark Alexander Woollcott, 1922 Figure 4.31 shows single-cycle datapath Section 4.4 pipeline stages identified. division instruction nto five stages means five-stage pipeline, turn means five instructions execution single cl ock cycle. Thus, must separate datapath five pieces, piece named corresponding stage instruction executi on: 1. IF: Instruction fetch 2. ID: Instruction decode register file read 5433. EX: Execution address calculation 4. MEM: Data memory access 5. WB: Write back 544FIGURE 4.31 single-cycle datapath Section 4.4 (similar Figure 4.17 ). step instruction mapped onto datapath left right. exceptions th e update PC write-back step, shown color, sends either ALU result data memory left written regist er file. (Normally use color lines control, data lines.) Figure 4.31 , five components correspond roughly way data-path drawn; instructions data move generally left right five stages complete execution. Returning laundry analogy, clothes get cleaner, drier, organized move line, never move backward. are, however, two exceptions left-to-right flow instructions: write-back stage, places result back registe r file middle datapath selection next value PC, choosing incremented PC branch address MEM stage 545Data flowing right left affect current instruction; reverse data movements influence later instructions pipeline. Note first right-to-lef flow data lead data hazards second leads control hazards. One way show happens pipelined execution pretend instruction datapath, place datapaths timeline show relationship. Figure 4.32 shows execution instructions Figure 4.25 displaying private datapaths common timeline. use stylized version datapath Figure 4.31 show relationships Figure 4.32 . FIGURE 4.32 Instructions executed using single-cycle datapath Figure 4.31 , assuming pipelined execution. Similar Figures 4.26 4.28, figure pretends instruction datapath, shades portion according use. Unlike figures, stage labeled physical resource used stage, corresponding portions datapath Figure 4.31 . IM represents instruction memory PC instruction fetch stage, Reg stands register file sign extender instruction decode/register file read 546stage (ID), on. maintain proper time order, stylized datapath breaks register file two logical parts: registers read register fetch (ID) registers written write back (WB). dual use represented drawing unshaded left half register file using dashed lines ID stage, written, unshaded right half dashed lines WB stage, read. before, assume register file written first half clock cycle register file read second half. Figure 4.32 seems suggest three instructions need three datapaths. Instead, add registers hold data portions single datapath shared instruction execution. example, Figure 4.32 shows, instruction memory used one five stages instruction, allowin g shared following instructions four tages. retain value individual instruction fou r stages, value read instruction memory must saved register. Similar arguments apply every pipeline stage, must place registers wherever dividing lines betwee n stages Figure 4.31 . Returning laundry analogy, might basket pair stages hold clothes next step. Figure 4.33 shows pipelined datapath pipeline registers high-lighted. instructions advance clo ck cycle one pipeline register next. registers named two stages separated register. example, pipeline register ID stages called IF/ID. 547FIGURE 4.33 pipelined version datapath Figure 4.31 . pipeline registers, color, separate pipeline stage. labeled stages separate; example, first labeled IF/ID separates instruction fetch instruction decode stages. registers must wide enough store data corresponding line go them. example, IF/ID register must 96 bits wide, must hold 32-bit instruction fetched memory incremented 64-bit PC address. expand registers course chapter, three pipeline registers contain 256, 193, nd 128 bits, respectively. Notice pipeline register end write - back stage. instructions must update state process —the register file, memory, PC—so separate pipeline register redundant state updated. example, load instruction place result one 32 registers, later instruction needs data simply read appropriate register. course, every instruction updates PC, whether incrementing setting branch destination address. PC thought pipeline register: one feeds stage pipeline. Unlike shaded pipeline registers Figure 4.33, however, PC part visible architectural state; contents must saved exception occurs, contents pipeline registers discarded. laundr 548analogy, could think PC corresponding basket holds load dirty clothes wash step. show pipelining works, throughout chapter show sequences figures demonstrate operation time. extra pages would seem require much time understand. Fear not; sequences take much less time might appear, compare see changes occur clock cycle. Section 4.7 describes happens data hazards pipelined instructions; ignore now. Figures 4.34 4.37, first sequence, show active portions datapath highlighted load instruction goes five stages pipelined execution. show load firs active five stages. Figures 4.26 4.28, highlight right half registers memory read highlight left half written . show instruction ld name pipe stage active figure. five stages following: 1. Instruction fetch: top portion Figure 4.34 shows instruction read memory using address PC placed IF/ID pipeline register. PC addre ss incremented 4 written back PC ready next clock cycle. PC also saved IF/ID pipeli ne register case needed later instruction, beq. computer cannot know type instruction fetche d, must prepare instruction, passing potentially needed information pipeline. 2. Instruction decode register file read: bottom portion Figure 4.34 shows instruction portion IF/ID pipeline register supplying immediate field, sign-extende 64 bits, register numbers read two registers. thre e values stored ID/EX pipeline register, along PC address. transfer everything might needed instruction later clock cycle. 3. Execute address calculation: Figure 4.35 shows load instruction reads contents register sign-extende immediate ID/EX pipeline register adds using ALU. sum placed EX/MEM pipeline register. 4. Memory access: top portion Figure 4.36 shows load 549instruction reading data memory using address EX/MEM pipeline register loading data MEM/WB pipeline register. 5. Write-back: bottom portion Figure 4.36 shows final step: reading data MEM/WB pipeline register writing register file middle figure. 550FIGURE 4.34 ID: First second pipe stages instruction, active portions datapath Figure 4.33 highlighted. highlighting convention used Figure 4.26 . Section 4.2 , confusion reading writing registers, contents change clock edge. Although load needs top register stage 2, doesn’t hurt potentially extra work, sign-extends constant reads registers ID/EX pipeline register. don’t need three operands, simplifies control keep three. 551552FIGURE 4.35 EX: third pipe stage load instruction, highlighting portions datapath Figure 4.33 used pipe stage. register added sign-extended immediate, sum placed EX/MEM pipeline register. 553FIGURE 4.36 MEM WB: fourth fifth pipe stages load instruction, highlighting portions datapath Figure 4.33 used pipe stage. Data memory read using address EX/MEM pipeline registers, data placed MEM/WB pipeline register. Next, data read MEM/WB pipeline register written register file middle datapath. Note: bug design repaired Figure 4.39 . walk-through load instruction shows information needed later pipe stage must passed 554stage via pipeline register. Walking store instruction shows similarity instruction execution, well passing information later stages. five pipe stages store instruction: 1. Instruction fetch: instruction read memory using address PC placed IF/ID pipeline registe r. stage occurs instruction identified, top portion Figure 4.34 works store well load. 2. Instruction decode register file read: instruction IF/ID pipeline register supplies register numbers reading wo registers extends sign immediate operand. three 64-bit values stored ID/EX pipeline register. Th e bottom portion Figure 4.34 load instructions also shows operations second stage stores. first two stages ar e executed instructions, since early know typ e instruction. (While store instruction uses rs2 field read second register pipe stage, detail shown pipeline diagram, use figure both.) 3. Execute address calculation: Figure 4.37 shows third step; effective address placed EX/MEM pipeline register. 4. Memory access: top portion Figure 4.38 shows data written memory. Note register containing data stored read earlier stage stored ID/EX. way make data available MEM stage place data EX/MEM pipeline register EX stage, stored effective address EX/MEM. 5. Write-back: bottom portion Figure 4.38 shows final step store. instruction, nothing happens write -back stage. Since every instruction behind store already progress, way accelerate instructions. Hence, instruction passes stage even nothing , later instructions already progressing maximum rate. 555FIGURE 4.37 EX: third pipe stage store instruction. Unlike third stage load instruction Figure 4.35, second register value loaded EX/MEM pipeline register used next stage. Although wouldn’t hurt always write second register EX/MEM pipeline register, write second register store instruction make pipeline easier understand. 556FIGURE 4.38 MEM WB: fourth fifth pipe stages store instruction. fourth stage, data written data memory store. Note data come EX/MEM pipeline register nothing changed MEM/WB pipeline register. data written memory, nothing left store instruction do, nothing happens stage 5. store instruction illustrates pass something early pipe stage later pipe stage, information must placed pipeline register; otherwise, information lost 557the next instruction enters pipeline stage. store instruction, needed pass one registers read ID stage MEM stage, stored memory. data first placed ID/EX pipeline register passed EX/MEM pipeline register. Load store illustrate second key point: logical component datapath—such instruction memory, register read ports, ALU, data memory, register write port—can used within single pipeline stage. Otherwise, would structural hazard (see page 267). Hence, components, control, associated single pipeline stage. uncover bug design load instruction. see it? register changed final stage load? specifically, instruction supplies write register number? instruction IF/ID pipeline regis ter supplies write register number, yet instruction occ urs considerably load instruction! Hence, need preserve destination register number load instruction. store passed register value ID/EX EX/MEM pipeline registers use MEM stage, load must pass register number ID/EX EX/MEM MEM/WB pipeline register use WB stage. Another way think passing register number share pipelined datapath, need preserve instruction read stage, pipeline register contains portion instruction needed stage late r stages. Figure 4.39 shows correct version datapath, passing write register number first ID/EX register, EX/MEM register, finally MEM/WB register. register number used WB stage specify register written. Figure 4.40 single drawing corrected datapath, highlighting hardware used five stages load regis ter instruction Figures 4.34 4.36. See Section 4.8 explanation make branch instruction work expected. 558FIGURE 4.39 corrected pipelined datapath handle load instruction properly. write register number comes MEM/WB pipeline register along data. register number passed ID pipe stage reaches MEM/WB pipeline register, adding five bits last three pipeline registers. new path shown color. FIGURE 4.40 portion datapath Figure 4.39 used five stages load instruction. Graphically Representing Pipelines Pipelining difficult master, since many instructions 559simultaneously executing single datapath every clock cycle . aid understanding, two basic styles pipeline figur es: multiple-clock-cycle pipeline diagrams , Figure 4.32 page 278, single-clock-cycle pipeline diagrams , Figures 4.34 4.38. multiple-clock-cycle diagrams simpler contain details. example, consider following five- instruction sequence: ld x10, 40(x1) sub x11, x2, x3 add x12, x3, x4 ld x13, 48(x1) add x14, x5, x6 Figure 4.41 shows multiple-clock-cycle pipeline diagram instructions. Time advances left right across page diagrams, instructions advance top bottom page, similar laundry pipeline Figure 4.23 . representation pipeline stages placed portion alon g instruction axis, occupying proper clock cycles. stylized datapaths represent five stages pipeline graphically, rectangle naming pipe stage works well. Figure 4.42 shows traditional version multiple- clock-cycle pipeline diagram. Note Figure 4.41 shows physical resources used stage, Figure 4.42 uses name stage. 560FIGURE 4.41 Multiple-clock-cycle pipeline diagram five instructions. style pipeline representation shows complete execution instructions single figure. Instructions listed instruction execution order top bottom, clock cycles move left right. Unlike Figure 4.26 , show pipeline registers stage. Figure 4.42 shows traditional way draw diagram. FIGURE 4.42 Traditional multiple-clock-cycle 561pipeline diagram five instructions Figure 4.41 . Single-clock-cycle pipeline diagrams show state enti datapath single clock cycle, usually five instructions pipeline identified labels thei r respective pipeline stages. use type figure show th e details happening within pipeline cloc k cycle; typically, drawings appear groups show pipeline operation sequence clock cycles. use multiple-clock - cycle diagrams give overviews pipelining situations. ( Section 4.13 gives illustrations single-clock diagrams would like see details Figure 4.41 .) single-clock- cycle diagram represents vertical slice one clock cycle thro ugh set multiple-clock-cycle diagrams, showing usage datapath instructions pipeline designated clock cycle. example, Figure 4.43 shows single- clock-cycle diagram corresponding clock cycle 5 Figures 4.41 4.42. Obviously, single-clock-cycle diagrams detail take significantly space show number clock cycles. exercises ask create diagrams code sequences. Check group students debating efficiency five-s tage pipeline one student pointed instructions active every stage pipeline. deciding ignore effects hazards, made following four statements. ones correct? 1. Allowing branches ALU instructions take fewer stages five required load instruction increase pipeline performance circumstances. 2. Trying allow instructions take fewer cycles help, since throughput determined clock cycle; number pipe stages per instruction affects latency, throughput. 3. cannot make ALU instructions take fewer cycles write-back result, branches take fewer cycles, opportunity improvement. 5624. Instead trying make instructions take fewer cycles, explore making pipeline longer, instructions take cycles, cycles shorter. could improve performance. FIGURE 4.43 single-clock-cycle diagram corresponding clock cycle 5 pipeline Figures 4.41 4.42. see, single-clock-cycle figure vertical slice multiple-clock-cycle diagram. Pipelined Control 6600 Computer, perhaps even previous computer, control system difference. James Thornton, Design Computer: Control Data 6600, 1970 added control single-cycle datapath Section 4.4 , add control pipelined datapath. start simple design views problem rose-colored glas ses. first step label control lines existing datap ath. Figure 4.44 shows lines. borrow much control simple datapath Figure 4.17 . particular, use ALU control logic, branch logic, control lines. 563These functions defined Figures 4.12 , 4.16, 4.18. reproduce key information Figures 4.45 4.47 single page make following discussion easier absorb. 564FIGURE 4.44 pipelined datapath Figure 4.39 control signals identified. datapath borrows control logic PC source, register destination number, ALU control Section 4.4 . Note need funct fields instruction EX stage input ALU control, bits must also included ID/EX pipeline register. case single-cycle implementation, assume PC written clock cycle, separate write signal PC. argument, separate write signals pipeline registers (IF/ID, ID/EX, EX/MEM , MEM/WB), since pipeline registers also written clock cycle. specify control pipeline, need set contr ol values pipeline stage. control line associated component active single pipeline stage, divide control lines five groups according pipeline stage. 1. Instruction fetch: control signals read instruction memory write PC always asserted, nothing special control pipeline stage. 2. Instruction decode/register file read: two source registers always location RISC-V instruction formats, nothing special control pipeline stage. 5653. Execution/address calculation: signals set ALUOp ALUSrc (see Figures 4.45 4.46). signals select ALU operation either Read data 2 sign-extended immediate inputs ALU. 4. Memory access: control lines set stage Branch, MemRead, MemWrite. branch equal, load, store instructions set signals, respectively. Recall PCSrc Figure 4.46 selects next sequential address unless control asserts Branch ALU result 0. 5. Write-back: two control lines MemtoReg, decides sending ALU result memory value register file, RegWrite, writes chosen value. FIGURE 4.45 copy Figure 4.12 . figure shows ALU control bits set depending ALUOp control bits different opcodes R-type instruction. FIGURE 4.46 copy Figure 4.16 . function six control signals defined. ALU control lines (ALUOp) defined second column Figure 4.45 . 1-bit control two-way multiplexor asserted, multiplexor 566selects input corresponding 1. Otherwise, control deasserted, multiplexor selects 0 input. Note PCSrc controlled gate Figure 4.44 . Branch signal ALU Zero signal set, PCSrc 1; otherwise, 0. Control sets Branch signal beq instruction; otherwise, PCSrc set 0. Since pipelining datapath leaves meaning control lines unchanged, use control values. Figure 4.47 values Section 4.4 , seven control lines grouped pipeline stage. FIGURE 4.47 values control lines Figure 4.18 , shuffled three groups corresponding last three pipeline stages. Implementing control means setting seven control lines values stage instruction. Since rest control lines starts EX stage, create control information instruction decode later stages. simplest way pass control signals extend pipeline registers include control informatio n. Figure 4.48 shows control signals used appropriate pipeline stage instruction moves pipeline, destination register number loads move pipeline Figure 4.39 . Figure 4.49 shows full datapath extended pipeline registers contr ol lines connected proper stage. ( Section 4.13 gives examples RISC-V code executing pipelined hardware using single-clock diagrams, would like see details.) 567FIGURE 4.48 seven control lines final three stages. Note two seven control lines used EX phase, remaining five control lines passed EX/MEM pipeline register extended hold control lines; three used MEM stage, last two passed MEM/WB use WB stage. 568FIGURE 4.49 pipelined datapath Figure 4.44 , control signals connected control portions pipeline registers. control values last three stages created instruction decode stage placed ID/EX pipeline register. control lines pipe stage used, remaining control lines passed next pipeline stage. 4.7 Data Hazards: Forwarding versus Stalling examples previous section show power pipelin ed execution hardware performs task. It’s time take rose-colored glasses look happens real programs. RISC-V instructions Figures 4.41 4.43 independent; none used results calculated others. Yet, Section 4.5 , saw data hazards obstacles pipelined execution. mean, why’s got built? It’s bypass. You’ve got 569to build bypasses. Douglas Adams, Hitchhiker’s Guide Galaxy, 1979 Let’s look sequence many dependences, shown color: sub x2, x1, x3 // Register z2 written sub x12, x2, x5 // 1st operand(x2) depends sub x13, x6, x2 // 2nd operand(x2) depends sub add x14, x2, x2 // 1st(x2) & 2nd(x2) depend sub sd x15, 100( x2) // Base (x2) depends sub last four instructions dependent result register x2 first instruction. register x2 value 10 subtract instruction −20 afterwards, programmer intends −20 used following instructions ref er register x2. would sequence perform pipeline? Figure 4.50 illustrates execution instructions using multi ple-clock- cycle pipeline representation. demonstrate execution f instruction sequence current pipeline, top Figure 4.50 shows value register x2, changes middle clock cycle 5, sub instruction writes result. 570FIGURE 4.50 Pipelined dependences five- instruction sequence using simplified datapaths show dependences. dependent actions shown color, “CC 1” top figure means clock cycle 1. first instruction writes x2, following instructions read x2. register written clock cycle 5, proper value unavailable clock cycle 5. (A read register clock cycle returns value written end first half cycle, write occurs.) colored lines top datapath lower ones show dependences. must go backward time pipeline data hazards . last potential hazard resolved design register file hardware: happens register read written clock cycle? assume write first half clock cycle read second half, read delivers written. case many implementations register files, data hazard case. Figure 4.50 shows values read register x2 would 571be result sub instruction unless read occurred clock cycle 5 later. Thus, instructions would get correct value −20 add sd; instructions would get incorrect value 10! Using style drawing, problems become apparent dependence line goes backward time. mentioned Section 4.5 , desired result available end EX stage sub instruction clock cycle 3. data actually needed instructions? answer beginning EX stage instructions, clock cycles 4 5, respectively. Thus, execute segment without stalls simply forward data soon available units need ready read register file. forwarding work? simplicity rest section, consider challenge forwarding operation EX stage, may either ALU operation effective address calculation. means instruction tries use register EX stage earlier instruction intends write WB stage, actually need values inputs ALU. notation names fields pipeline registers allows precise notation dependences. example, “ID/EX.RegisterRs1” refers number one register whose value found pipeline register ID/EX; is, one fro first read port register file. first part name , left period, name pipeline register; econd part name field register. Using notation, th e two pairs hazard conditions 1a. EX/MEM.RegisterRd =ID/EX.RegisterRs1 1b. EX/MEM.RegisterRd =ID/EX.RegisterRs2 2a. MEM/WB.RegisterRd =ID/EX.RegisterRs1 2b. MEM/WB.RegisterRd =ID/EX.RegisterRs2 first hazard sequence page 295 register x2, result sub x2, x1, x3 first read operand x12, x2, x5 . hazard detected instruction EX stage prior instruction MEM stage, hazard 1a: EX/MEM.RegisterRd =ID/EX.RegisterRs1 = x2 572 Dependence Detection Example Classify dependences sequence page 295: subx2, x1, x3 // Register x2 set sub andx12, x2, x5 // 1st operand(z2) set sub orx13, x6, x2 // 2nd operand(x2) set sub addx14, x2, x2 // 1st(x2) & 2nd(x2) set sub sdx15, 100(x2) // Index(x2) set sub Answer mentioned above, sub–and type 1a hazard. remaining hazards follows: sub–or type 2b hazard: MEM/WB.RegisterRd =ID/EX.RegisterRs2 = x2 two dependences sub–add hazards register file supplies proper data ID stage add. data hazard sub sd sd reads x2 clock cycle sub writes x2. instructions write registers, policy inaccurate; sometimes would forward shouldn’t. One solution simply check see RegWrite signal active: examining WB control field pipeline register EX MEM stages determines whether RegWrite asserted. Recall RISC-V requires every use x0 operand must yield operand value 0. instruction pipeline x0 destination (for example, addi x0, x1, 2 ), want avoid forwarding possibly nonzero result value. forwarding results destined x0 frees assembly programmer compiler requirement avoid using x0 destination. conditions thus work properly long add EX/MEM.RegisterRd ≠0 first hazard condition MEM/WB.RegisterRd ≠0 second. detect hazards, half problem resolved —but must still forward proper data. Figure 4.51 shows dependences pipeline registers inputs ALU code sequence Figure 5734.50. change dependence begins pipeline register, rather waiting WB stage write register file. Thus, required data exist time later instructions, wi th pipeline registers holding data forwarded. 574FIGURE 4.51 dependences pipeline registers move forward time, possible supply inputs ALU needed instruction instruction forwarding results found pipeline registers. values pipeline registers show desired value available written register file. assume register file forward values read written clock cycle, add stall, values come register file instead pipeline register. Register file “forwarding”—that is, read gets value write clock cycle—is clock cycle 5 shows register x2 value 10 beginning −20 end clock cycle. take inputs ALU pipeline register rather ID/EX, forward correct data. adding multiplexors input ALU, proper controls, run pipeline full speed presence data hazards. now, assume instructions need 575forward four R-format instructions: add, sub, and, or. Figure 4.52 shows close-up ALU pipeline register adding forwarding. Figure 4.53 shows values control lines ALU multiplexors select either th e register file values one forwarded values. 576FIGURE 4.52 top ALU pipeline registers adding forwarding. bottom, multiplexors expanded add forwarding paths, show forwarding unit. new hardware shown color. figure stylized drawing, however, leaving details full datapath sign extension hardware. 577FIGURE 4.53 control values forwarding multiplexors Figure 4.52 . signed immediate another input ALU described Elaboration end section. forwarding control EX stage, ALU forwarding multiplexors found stage. Thus, must pass operand register numbers ID stage via ID/EX pipeline register determine whether forward values. Befo forwarding, ID/EX register need include space hol rs1 rs2 fields. Hence, added ID/EX. Let’s write conditions detecting hazards, control signals resolve them: 1. EX hazard: (EX/MEM.RegWrite (EX/MEM.RegisterRd ≠ 0) (EX/MEM.RegisterRd = ID/EX.RegisterRs1)) ForwardA = 10 (EX/MEM.RegWrite (EX/MEM.RegisterRd ≠ 0) (EX/MEM.RegisterRd = ID/EX.RegisterRs2)) ForwardB = 10 case forwards result previous instruction either input ALU. previous instruction going write register file, write register number matches read reg ister number ALU inputs B, provided register 0, steer multiplexor pick value instead pipelin e register EX/MEM. 2. MEM hazard: (MEM/WB.RegWrite (MEM/WB.RegisterRd ≠ 0) (MEM/WB.RegisterRd = ID/EX.RegisterRs1)) ForwardA = 01 (MEM/WB.RegWrite (MEM/WB.RegisterRd ≠ 0) 578and (MEM/WB.RegisterRd = ID/EX.RegisterRs2)) ForwardB = 01 mentioned above, hazard WB stage, assume register file supplies correct result instruction ID stage reads register written th e instruction WB stage. register file performs another form forwarding, occurs within register file. One complication potential data hazards result instruction WB stage, result instruction MEM stage, source operand instruction ALU stage. example, summing vector numbers single register, sequence instructions read write register: add x1, x1, x2 add x1, x1, x3 add x1, x1, x4 . . . case, result forwarded MEM stage result MEM stage recent result. Thu s, control MEM hazard would (with additions highlighted): (MEM/WB.RegWrite (MEM/WB.RegisterRd ≠ 0) not(EX/MEM.RegWrite (EX/MEM.RegisterRd ≠ 0) (EX/MEM.RegisterRd = ID/EX.RegisterRs1)) (MEM/WB.RegisterRd = ID/EX.RegisterRs1)) ForwardA = 01 (MEM/WB.RegWrite (MEM/WB.RegisterRd ≠ 0) not(EX/MEM.RegWrite (EX/MEM.RegisterRd ≠ 0) (EX/MEM.RegisterRd = ID/EX.RegisterRs2)) (MEM/WB.RegisterRd = ID/EX.RegisterRs2)) ForwardB = 01 Figure 4.54 shows hardware necessary support forwarding operations use results EX stage. Note EX/MEM.RegisterRd field register destination either ALU instruction load. 579FIGURE 4.54 datapath modified resolve hazards via forwarding. Compared datapath Figure 4.49 , additions multiplexors inputs ALU . figure stylized drawing, however, leaving details full datapath, branch hardware sign extension hardware. would like see illustrated examples using single - cycle pipeline drawings, Section 4.13 figures show two pieces RISC-V code hazards cause forwarding. Elaboration Forwarding also help hazards store instructions dependent instructions. Since use one data val ue MEM stage, forwarding easy. However, consider loads immediately followed stores, useful performing memory-to-memory copies RISC-V architecture. Since co pies frequent, need add forwarding hardware make run faster. redraw Figure 4.51 , replacing sub instructions ld sd, would see possible avoid stall, since data exist MEM/WB register load instruction time use MEM stage store instruction. would need add forwarding memory 580access stage option. leave modification exercise reader. addition, signed-immediate input ALU, needed loads stores, missing datapath Figure 4.54 . Since central control decides register immediate, sinc e forwarding unit chooses pipeline register registe r input ALU, easiest solution add 2:1 multiplexor chooses ForwardB multiplexor output signed immediate. Figure 4.55 shows addition. 581FIGURE 4.55 close-up datapath Figure 4.52 shows 2:1 multiplexor, added select signed immediate ALU input. Data Hazards Stalls said Section 4.5 , one case forwarding cannot save day instruction tries read register following load instruction writes register. Figure 4.56 illustrates problem. data still read memory clock cycl e 4 ALU performing operation following instruction. Something must stall pipeline combinat ion load followed instruction reads result. first don’t succeed, redefine success. Anonymous 582FIGURE 4.56 pipelined sequence instructions. Since dependence load following instruction ( and) goes backward time, hazard cannot solved forwarding. Hence, combination must result stall hazard detection unit. Hence, addition forwarding unit, need hazard detection unit . operates ID stage insert stall load instruction dependent it. Checking load instructions, control hazard detection unit single condition: (ID/EX.MemRead ((ID/EX.RegisterRd = IF/ID.RegisterRs1) (ID/EX.RegisterRd = IF/ID.RegisterRs2))) stall pipeline Recall using RegisterRd refer register specified instruction bits 11:7 load R-type instructions. first line tests see instruction load: instruction reads data memory load. next two lines check see destination register field load EX stage matches either source register instruction th e ID 583stage. condition holds, instruction stalls one clock c ycle. one-cycle stall, forwarding logic handle dependence execution proceeds. (If forwarding, instructions Figure 4.56 would need another stall cycle.) instruction ID stage stalled, instructio n stage must also stalled; otherwise, would lose fetched instruction. Preventing two instructions fro making progress accomplished simply preventing PC register IF/ID pipeline register changing. Provided reg isters preserved, instruction stage continue b e read using PC, registers ID stage continue read using instruction fields IF/ID pipeline register. Returning favorite analogy, it’s restart washer clothes let dryer continu e tumbling empty. course, like dryer, back half pipeline starting EX stage must something; executing instructions effect: nops . nops instruction operation change state. insert nops, act like bubbles, pipeline? Figure 4.47 , see deasserting seven control signals (setting 0) EX, MEM, WB stages create “do nothing” nop instruction. identifying hazard ID stage, insert bubble pipeline changing EX, MEM, WB control fields ID/EX pipeline register 0. benign control values percolated forward clock cycle proper effect: registers memories written control values 0. Figure 4.57 shows really happens hardware: pipeline execution slot associated instruction turned nop instructions beginning instruction delayed one cycle. Like air bubble water pipe, stall bubble delays everything behind proceeds instruction pipe one stage clock cycle exits e nd. example, hazard forces instructions repeat clock cycle 4 clock cycle 3: reads 584registers decodes, refetched instruction memory. repeated work stall looks like, effect stretch time instructions delay fetch add instruction. 585FIGURE 4.57 way stalls really inserted pipeline. bubble inserted beginning clock cycle 4, changing instruction nop. Note instruction really fetched decoded clock cycles 2 3, EX stage delayed clock cycle 5 (versus unstalled position clock cycle 4). Likewise, instruction fetched clock cycle 3, ID stage delayed clock cycle 5 (versus unstalled clock cycle 4 position). insertion bubble, dependences go forward time hazards occur. Figure 4.58 highlights pipeline connections hazard detection unit forwarding unit. before, forwarding unit controls ALU multiplexors replace v alue general-purpose register value proper pipeline register. hazard detection unit controls writi ng PC IF/ID registers plus multiplexor chooses real control values 0s. hazard detection unit stalls deasserts control fields load-use hazard tes true. would like see details, Section 4.13 gives example illustrated using single-clock pipeline 586diagrams RISC-V code hazards cause stalling. BIG Picture Although compiler generally relies upon hardware resolve hazards thereby ensure correct execution, compil er must understand pipeline achieve best performance. Otherwise, unexpected stalls reduce performance th e compiled code. Elaboration Regarding remark earlier setting control lines 0 avoid writing registers memory: signals RegWrite MemWrite need 0, control signals don’t cares. thousand hacking branches evil one striking root. Henry David Thoreau, Walden, 1854 587FIGURE 4.58 Pipelined control overview, showing two multiplexors forwarding, hazard detection unit, forwarding unit. Although ID EX stages simplified— sign-extended immediate branch logic missing—this drawing gives essence forwarding hardware requirements. 4.8 Control Hazards Thus far, limited concern hazards involving arithmetic operations data transfers. However, saw Section 4.5 , also pipeline hazards involving conditional branches. Figure 4.59 shows sequence instructions indicates branch would occur pipeline. instruction must fetched every clock cycle sustain pipeline, yet design decision whether branch doesn’t occur MEM pipeline stage. mentioned Section 4.5 , delay determining proper instruction fetch called control hazard branch hazard , contrast data hazards examined. 588FIGURE 4.59 impact pipeline branch instruction. numbers left instruction (40, 44, …) addresses instructions. Since branch instruction decides whether branch MEM stage—clock cycle 4 beq instruction above—the three sequential instructions follow branch wil l fetched begin execution. Without intervention, three following instructions begin execution beq branches ld location 72. ( Figure 4.29 assumed extra hardware reduce control hazard one clock cycle; figure uses nonoptimized datapath.) section control hazards shorter previous sections data hazards. reasons control hazards relatively simple understand, occur less frequently th data hazards, nothing effective control hazards forwarding data hazards. Hence, use simpler schemes. look two schemes resolving control hazards one optimization improve schemes. Assume Branch Taken 589As saw Section 4.5 , stalling branch complete slow. One improvement branch stalling predict conditional branch taken thus continue execution sequential instruction stream. conditional branc h taken, instructions fetched decoded must discarded. Execution continues branch target. condition al branches untaken half time, costs little discard instructions, optimization halves cost control hazards. discard instructions, merely change original control values 0s, much stall load-use data hazard. difference must also change three instructions IF, ID, EX stages branch reaches MEM stage; load-use stalls, change control 0 ID stage let percolate pipeline. Discarding instructions, hen, means must able flush instructions IF, ID, EX stages pipeline. 590 flush discard instructions pipeline, usually due unexpecte event. Reducing Delay Branches One way improve conditional branch performance reduce cost taken branch. Thus far, assumed next PC branch selected MEM stage, move conditional branch execution earlier pipeline, fewer instructions need flushed. Moving branch decision requires two actions occur earlier: computing branch target address evaluating branch decision. easy part change move branch address calculation. already PC value immediate field IF/ID pipeline register, move branch adder EX stage ID stage; course, address calculation branch targets performed instructions, used needed. harder part branch decision itself. branch equal, would compare two register reads ID stage see equal. Equality tested XORing individual bit positions two registers ORing XORed result. Moving th e branch test ID stage implies additional forwarding hazard detection hardware, since branch dependent result still pipeline must still work properly opti mization. example, implement branch equal (and inverse), need forward results equality test logic operates ID. two complicating factors: 1. ID, must decode instruction, decide whether bypass equality test unit needed, complete equali ty test instruction branch, set PC branch target address. Forwarding operand branches formerly handled ALU forwarding logic, introduction equality test unit ID require new forwarding logic. Note bypassed source operands branch come either EX/MEM MEM/WB pipeline registers. 2. value branch comparison needed ID may produced later time, possible data hazard 591can occur stall needed. example, ALU instruction immediately preceding branch produces operan test conditional branch, stall required, sinc e EX stage ALU instruction occur ID cycle branch. extension, load immediately followed conditional branch depends load result, two stall cycle needed, result load appears end MEM cycle needed beginning ID branch. Despite difficulties, moving conditional branch execution ID stage improvement, reduces penalty branch one instruction branch taken, namely, one currently fetched. exercises explore details implementing forwarding path detecting hazard. flush instructions stage, add control line, called IF.Flush, zeros instruction field IF/ID pipeline register. Clearing register transforms fetched instruc tion nop, instruction action changes state. Pipelined Branch Example Show happens branch taken instruction sequence, assuming pipeline optimized branches taken, moved branch execution ID stage: 36 sub x10, x4, x8 40 beq x1, x3, 16 // PC-relative branch 40+16*2=72 44 x12, x2, x5 48 x13, x2, x6 52 add x14, x4, x2 56 sub x15, x6, x7 . . . 72 ld x4, 50(x7) Answer Figure 4.60 shows happens conditional branch taken. Unlike Figure 4.59 , one pipeline bubble taken branch. 592593FIGURE 4.60 ID stage clock cycle 3 determines branch must taken, selects 72 next PC address zeros instruction fetched next clock cycle. Clock cycle 4 shows instruction location 72 fetched single bubble nop instruction pipeline taken branch. 594Dynamic Branch Prediction Assuming conditional branch taken one simple form branch prediction . case, predict conditional branches untaken, flushing pipeline wrong. simple five-stage pipeline, approach, possibly coupled wi th compiler-based prediction, probably adequate. deeper pipelines, branch penalty increases measured clock cycles. Similarly, multiple issue (see Section 4.10 ), branch penalty increases terms instructions lost. combinati means aggressive pipeline, simple static prediction scheme probably waste much performance. mentioned Section 4.5 , hardware possible try predict branch behavior program execution. One approach look address instruction see conditional branch taken last time instruction executed, and, so, begin fetching new instructions place last time. technique called dynamic 595branch prediction . dynamic branch prediction Prediction branches runtime using runtime information. branch prediction buffer Also called branch history table. small memory indexed lower portion address branch instruction contains one bits indicating whether branch recently taken not. One implementation approach branch prediction buffer branch history table. branch prediction buffer small memory indexed lower portion address branch instruction. memory contains bit says whether branch recently taken not. prediction uses simplest sort buffer; don’t know , fact, prediction right one—it may put another conditional branch low-order address bits. However, doesn’t affect correctness. Prediction ju st hint hope correct, fetching begins predicted direction. hint turns wrong, incorrectly predi cted instructions deleted, prediction bit inverted sto red back, proper sequence fetched executed. simple 1-bit prediction scheme performance shortcoming: even conditional branch almost always taken, predict incorrectly twice, rather once, taken. following example shows dilemma. Loops Prediction Example Consider loop branch branches nine times row, taken once. prediction accuracy branch, assuming prediction bit branch remains prediction buffer? Answer 596The steady-state prediction behavior mispredict fi rst last loop iterations. Mispredicting last iteration inevi table since prediction bit indicate taken, branch taken nine times row point. misprediction first iteration happens bit flipped prior exec ution last iteration loop, since branch taken exiting iteration. Thus, prediction accuracy branch taken 90% time 80% (two incorrect predictions eight correct ones). Ideally, accuracy predictor would match taken branch frequency highly regular branches. remedy weakness, 2-bit prediction schemes often used. 2-bit scheme, prediction must wrong twice changed. Figure 4.61 shows finite-state machine 2-bit prediction scheme. FIGURE 4.61 states 2-bit prediction scheme. using 2 bits rather 1, branch strongly favors taken taken—as many branches do—will mispredicted once. 2 bits used encode four states system. 2-bit scheme general instance counter-based predictor, incremented prediction accurate 597and decremented otherwise, uses mid-point range division taken taken. branch prediction buffer implemented small, special buffer accessed instruction address th e pipe stage. instruction predicted taken, fetching begin target soon PC known; mentioned page 308, early ID stage. Otherwise, sequential fetching executing continue. prediction turns wrong, th e prediction bits changed shown Figure 4.61 . Elaboration branch predictor tells us whether conditional branch taken, still requires calculation branch target. fiv e- stage pipeline, calculation takes one cycle, meaning taken branches one-cycle penalty. One approach use cache hold destination program counter destination instruction using branch target buffer . branch target buffer structure caches destination PC destination instruction branch. usually organized cache tags, making costly simple prediction buffer. correlating predictor branch predictor combines local behavior particular branch global information behavior recent number executed branches. 2-bit dynamic prediction scheme uses information particular branch. Researchers noticed using information local branch global behavior recently executed branches together yields greater prediction accuracy number prediction bits. predictors called correlating predictors . typical correlating predictor might two 2-bit predictors branch, choice predictors made based whether last executed branch taken taken. Thus, global branch behavior thought 598of adding additional index bits prediction lookup. Another approach branch prediction use tournament predictors. tournament branch predictor uses multiple predictors, tracking, branch, predictor yields best results. typical tournament predictor might contain two predictions branch index: one based local information one based global branch behavior. selector would choose predictor use given prediction. selector c operate similarly 1- 2-bit predictor, favoring whichever two predictors accurate. recent microprocessors use ensemble predictors. tournament branch predictor branch predictor multiple predictions branch selection mechanism chooses predictor enable given branch. Elaboration One way reduce number conditional branches add conditional move instructions. Instead changing PC conditional branch, instruction conditionally changes destination register move. example, ARMv8 instruction set architecture conditional select instructi called CSEL . specifies destination register, two source registers, condition. destination register gets value first operand condition true second operand otherwise. Thus, CSEL X8 , X11, X4, NE copies contents register 11 register 8 condition codes say result operation equal zero copy register 4 register 11 zero. Hence, programs using ARMv8 instruction set could fewer conditional branches programs written RISC-V. Pipeline Summary started laundry room, showing principles pipelining everyday setting. Using analogy guide, explained instruction pipelining step-by-step, starting singl e-cycle datapath adding pipeline registers, forwarding paths, data 599hazard detection, branch prediction, flushing instructions mispredicted branches load-use data hazards. Figure 4.62 shows final evolved datapath control. ready yet another control hazard: sticky issue exceptions. Check Consider three branch prediction schemes: predict taken, predict taken, dynamic prediction. Assume zero penalty predict correctly two cycles wrong. Assume average predict accuracy dynamic predictor 90%. predictor best choice th e following branches? 600 1. conditional branch taken 5% frequency 2. conditional branch taken 95% frequency 3. conditional branch taken 70% frequency make computer automatic program-interruption faciliti es behave [sequentially] easy matter, number instructions various stages processing interrupt sign al occurs may large. Fred Brooks, Jr., Planning Computer System: Project Str etch, 1962 601FIGURE 4.62 final datapath control chapter. Note stylized figure rather detail ed datapath, it’s missing ALUsrc Mux Figure 4.55 multiplexor controls Figure 4.49 . 4.9 Exceptions Control challenging aspect processor design: hardest part get right toughest part make fast. One demanding tasks control implementing exceptions interrupts —events branches change normal flow instruction execution. initially created hand le unexpected events within processor, like undefined instruction. basic mechanism extended I/O devices communicate processor, see Chapter 5 . exception Also called interrupt . unscheduled event disrupts program execution; used detect undefined instructions. 602 interrupt exception comes outside processor. (Some architectures use term interrupt exceptions.) Many architectures authors distinguish interrupts exceptions, often using either name refer bo th types events. example, Intel x86 uses interrupt. use term exception refer unexpected change control flow without distinguishing whether cause internal extern al; use term interrupt event externally caused. examples showing whether situation internally generated processor externally generated name RISC-V uses: Type event where? RISC-V terminology System reset External Exception I/O device request External Interrupt Invoke operating system user program Internal Excepti Using undefined instruction Internal Exception Hardware malfunctions Either Either Many requirements support exceptions come specific situation causes exception occur. Accordingl y, return topic Chapter 5 , better understand motivation additional capabilities exception mechanism. section, deal control implementation detecting types exceptions arise fr om portions instruction set implementation already discussed. Detecting exceptional conditions taking appropriate action often critical timing path processor, determines clock cycle time thus performance. Without proper attention exceptions design control un it, attempts add exceptions intricate implementation significantly reduce performance, well complicate task getting design correct. Exceptions Handled RISC-V Architecture types exceptions current implementation 603generate execution undefined instruction hardware malfunction. We’ll assume hardware malfunction occurs instruction add x11, x12, x11 example exception next pages. basic action processor must perform exception occurs save address unfortunate instruction supervisor exception cause register (SEPC) transfer control operating system specified addre ss. operating system take appropriate action, may involve providing service user program, taking predefined action response malfunction, stopping execution program reporting error. performing whatever action required exception , operating system terminate program may continue execution, using SEPC determine restart execution program. Chapter 5 , look closely issue restarting execution. operating system handle exception, must know reason exception, addition instruction caused it. two main methods used communicate reason exception. method used RISC-V architecture include register (called Supervisor Exception Cause Register SCAUSE ), holds field indicates reason exception. vectored interrupt interrupt address control transfer red determined cause exception. second method use vectored interrupts . vectored interrupt, address control transferred determ ined cause exception, possibly added base register points memory range vectored interrupts. example, might define following exception vector addresses accommodate exception types: Exception typeException vector address added Vector Table Base Register Undefined instruction 00 0100 0000two System Error (hardware malfunction)01 1000 0000two 604The operating system knows reason exception address initiated. exception vectore d, RISC-V, single entry point exceptions used, operating system decodes status register find caus e. architectures vectored exceptions, addresses mig ht separated by, say, 32 bytes eight instructions, operating system must record reason exception may perform limited processing sequence. perform processing required exceptions adding extra registers control signals basic implementation slightly extending control. Let’s assume implementing exception system single interrupt entry point address 0000 0000 1C09 0000hex. (Implementing vectored exceptions difficult.) w ill need add two additional registers current RISC-V implementation: SEPC: 64-bit register used hold address affected instruction. (Such register needed even exceptions ar e vectored.) SCAUSE: register used record cause exception. RISC-V architecture, register 64 bits, although bits currently unused. Assume field encodes two possible exception sources mentioned above, 2 representing undefined instruction 12 representing hardware malfunction. Exceptions Pipelined Implementation pipelined implementation treats exceptions another form control hazard. example, suppose hardware malfunction add instruction. taken branch previous section, must flush instructions th follow add instruction pipeline begin fetching instructions new address. use mechanism used taken branches, time exception causes deasserting control lines. dealt branch misprediction, saw flush instruction stage turning nop. flush instructions ID stage, use multiplexor already 605ID stage zeros control signals stalls. new control signal, called ID.Flush, ORed stall signal hazard detection unit flush ID. flush instruction EX phase, use new signal called EX.Flush cause new multiplexors zero control lines. start fetching instru ctions location 0000 0000 1C09 0000hex, using RISC-V exception address, simply add additional input PC multiplexor sends 0000 0000 1C09 0000hex PC. Figure 4.63 shows changes. 606FIGURE 4.63 datapath controls handle exceptions. key additions include new input value 0000 0000 1C09 0000hex multiplexor supplies new PC value; SCAUSE register record cause exception; SEPC register save address instruction caused exception. 0000 0000 1C09 0000hex input multiplexor initial address begi n fetching instructions event exception. example points problem exceptions: stop execution middle instruction, programme r able see original value register x1 clobbered destination register add instruction. assume exception detected EX stage, use EX.Flush signal prevent instruction EX stage fr om writing result WB stage. Many exceptions require eventually complete instruction caused exception executed normally. easiest way flush instruction restart beginning exceptio n handled. final step save address offending instructio n supervisor exception program counter (SEPC). Figure 4.63 shows stylized version datapath, including branch hardware 607and necessary accommodations handle exceptions. Exception Pipelined Computer Example Given instruction sequence, 40 hex sub x11, x2, x4 44 hex x12, x2, x5 48 hex x13, x2, x6 4C hex add x1, x2, x1 50 hex sub x15, x6, x7 54 hex ld x16, 100(x7) . . . assume instructions invoked exception begin like this: 1C090000 hex sd x26, 1000(x10) 1C090004 hex sd x27, 1008(x10) . . . Show happens pipeline hardware malfunction exception occurs add instruction. Answer Figure 4.64 shows events, starting add instruction EX stage. Assume hardware malfunction detected phase, 0000 0000 1C09 0000hex forced PC. Clock cycle 7 shows add following instructions flushed, first instruction exception-handling code fet ched. Note address add instruction saved: 4Chex. 608FIGURE 4.64 result exception due hardware malfunction add instruction. exception detected EX stage clock 6, saving address add instruction SEPC register (4Chex). causes Flush signals set near end clock cycle, deasserting control values (setting 0) add. Clock cycle 7 shows instructions converted bubbles pipeline plus fetching first instruction exception routine— sd x26, 1000(x0) —from instruction location 0000 0000 1C09 0000hex. Note 609the instructions, prior add, still complete. mentioned several examples exceptions page 315, see others Chapter 5 . five instructions active clock cycle, challenge associate exception appropriate instruction. Moreover, multiple exceptions occ ur simultaneously single clock cycle. solution prior itize exceptions easy determine serviced first. RISC-V implementations, hardware sorts exceptions earliest instruction interrupted. I/O device requests hardware malfunctions associated specific instruction, implementation flexibility interrupt pipeline. Hence, mechanism used exceptions works fine. SEPC register captures address interrupted instructions, SCAUSE register records highest prio rity exception clock cycle one exception occurs. Hardware/Software Interface hardware operating system must work conjunction exceptions behave would expect. hardware contract normally stop offending instruction midst ream, let prior instructions complete, flush following instr uctions, set register show cause exception, save address offending instruction, branch prearranged address. operating system contract look cause exception act appropriately. undefined instruction hardware failure, operating system normally kills program returns indicator reason. I/O device request operating system service call, operating system saves state program, performs desired task, and, point future, restores program continue execution. th e case I/O device requests, may often choose run another task resuming task requested I/O, since tas k may often able proceed I/O complete. Exceptions ability save restore state task critical. One important frequent uses 610exceptions handling page faults; Chapter 5 describes exceptions handling detail. Elaboration difficulty always associating proper exception correct instruction pipelined computers led comp uter designers relax requirement noncritical cases. processors said imprecise interrupts imprecise exceptions . example above, PC would normally 58hex start clock cycle exception detected, eve n though offending instruction address 4Chex. processor imprecise exceptions might put 58hex SEPC leave operating system determine instruction caused problem. RISC-V vast majority computers today support precise interrupts precise exceptions . One reason designers deeper pipeline processor might tempted record different value SEPC, would create headaches OS. prevent them, deeper pipeline would likely required record PC would recorded five-stage pipeline. simpler everyone rec ord PC faulting instruction instead. (Another reason suppo rt virtual memory, shall see Chapter 5 .) imprecise interrupt Also called imprecise exception . Interrupts exceptions pipelined computers associated exact instruction cause interrupt exception. precise interrupt Also called precise exception . interrupt exception always associated correct instruction pipelined computers. Elaboration show RISC-V uses exception entry address 0000 0000 1C09 0000hex, chosen somewhat arbitrarily. Many RISC-V 611computers store exception entry address special regist er named Supervisor Trap Vector (STVEC), OS load value choosing. Check exception recognized first sequence? 1. xxx x11, x12, x11 // undefined instruction 2. sub x11, x12, x11 // hardware error 4.10 Parallelism via Instructions forewarned: section brief overview fascinating complex topics. want learn details, consult advanced book, Computer Architecture: Quantitative Approach , fifth edition, material covered 13 pages expanded almost 200 pages (including appendices)! 612Pipelining exploits potential parallelism among instructions. parallelism called, naturally enough, instruction-level parallelism (ILP) . two primary methods increasing potential amount instruction-level parallelism. first increasing depth pipeline overlap instructio ns. Using laundry analogy assuming washer cycle longer others were, could divide washer thre e machines perform wash, rinse, spin steps traditional washer. would move four-stage six- stage pipeline. get full speed-up, need rebalance remaining steps length, processors laundry. amount parallelism exploited higher, since operations overlapped. Performance potentially greater since clock cycle shorter. instruction-level parallelism parallelism among instructions. Another approach replicate internal components computer launch multiple instructions every pipeline stage. general name technique multiple issue . multiple-issue laundry would replace household washer dryer with, say, three washers three dryers. would also recruit assistants fold put away 613three times much laundry amount time. downside extra work keep machines busy transferring loads next pipeline stage. multiple issue scheme whereby multiple instructions launched one clock cycle. Launching multiple instructions per stage allows instruct ion execution rate exceed clock rate or, stated alternatively, CPI less 1. mentioned Chapter 1 , sometimes useful flip metric use IPC, instructions per clock cycle . Hence, 3- GHz four-way multiple-issue microprocessor execute peak rate 12 billion instructions per second best-case CPI 0.33, IPC 3. Assuming five-stage pipeline, processor would 20 instructions execution given time. Today’s high-end microprocessors attempt iss ue three six instructions every clock cycle. Even mode rate designs aim peak IPC 2. typically, however, many constraints types instructions may executed simultaneously, happens dependences arise. two main ways implement multiple-issue processor, major difference division work compiler hardware. division work dictates whether decisions made statically (that is, compile time) dynamically (that is, execution), approaches sometimes called static multiple issue dynamic multiple issue . see, approaches other, commonly used names, may less precise restrictive. static multiple issue approach implementing multiple-issue processor many decisions made compiler execution. dynamic multiple issue approach implementing multiple-issue processor 614many decisions made execution processor. Two primary distinct responsibilities must dealt multiple-issue pipeline: 1. Packaging instructions issue slots : processor determine many instructions instructions issued given clock cycle? static issue processors, process least partially handled compiler; dynamic issue designs, normally dealt runtime process or, although compiler often already tried help improv e issue rate placing instructions beneficial order. 2. Dealing data control hazards: static issue processors, compiler handles consequences data control hazards statically. contrast, dynamic issue processors attempt alleviate least classes hazards using hardware techniques operating execution time. issue slots positions instructions could issue given c lock cycle; analogy, correspond positions starting blocks sprint. Although describe distinct approaches, reality, one approach often borrows techniques other, neither approach claim perfectly pure. Concept Speculation One important methods finding exploiting ILP speculation. Based great idea prediction , speculation approach allows compiler processor “guess” properties instruction, enable execution begin instructions may depend speculated instruction. example, might speculate th e outcome branch, instructions branch could executed earlier. Another example might speculate store precedes load refer address, would allow load executed store. difficu lty speculation may wrong. So, speculation 615mechanism must include method check guess right method unroll back effects instructions executed speculatively. implement ation back-out capability adds complexity. speculation approach whereby compiler processor guesses outcome instruction remove dependence executing instructions. Speculation may done compiler hardware. example, compiler use speculation reorder instruction s, moving instruction across branch load across store. processor hardware perform transformation runtime using techniques discuss later section. recovery mechanisms used incorrect speculation rather different. case speculation software, compil er 616usually inserts additional instructions check accuracy speculation provide fix-up routine use speculation wrong. hardware speculation, processor usually buffers speculative results knows longer speculative. speculation correct, instruct ions completed allowing contents buffers writte n registers memory. speculation incorrect, hardware flushes buffers re-executes correct instru ction sequence. Misspeculation typically requires pipeline flushed, least stalled, thus reduces performance. Speculation introduces one possible problem: speculati ng certain instructions may introduce exceptions merly present. example, suppose load instruction moved speculative manner, address uses within bounds speculation incorrect. result would exception occurred would occur. proble complicated fact load instruction speculative, exception must occur! compiler-based speculation, problems avoided adding special speculation support allows exceptions ignored un til clear really occur. hardware-based speculation, exceptions simply buffered clear th instruction causing longer speculative ready complete; point, exception raised, normal exception handling proceeds. Since speculation improve performance done properly decrease performance done carelessly, significant effort goes deciding appropriate speculate. Later thi section, examine static dynamic techniques speculation. Static Multiple Issue Static multiple-issue processors use compiler assis packaging instructions handling hazards. static issue processor, think set instructions issued giv en clock cycle, called issue packet , one large instruction multiple operations. view analogy. Since static multiple-issue processor usually restrict 617what mix instructions initiated given clock cycle, useful think issue packet single instruction allowi ng several operations certain predefined fields. view led original name approach: Long Instruction Word (VLIW) . issue packet set instructions issues together one clock cycle ; packet may determined statically compiler dynamically processor. Long Instruction Word (VLIW) style instruction set architecture launches many operations defined independent single-wide instruction, typically many separate opcode fields. static issue processors also rely compiler take responsibility handling data control hazards. compiler’s responsibilities may include static branch predic tion code scheduling reduce prevent hazards. Let’s look simple static issue version RISC-V processor, describe use techniques aggressive proces sors. Example: Static Multiple Issue RISC-V SA give flavor static multiple issue, consider simple two- issue RISC-V processor, one instructions integer ALU operation branch load store. design like used embedded processors . Issuing two instructions per cycle require fetching decoding 64 bits instructions. many static multiple-issue processors, essentially VLIW processors, layout simultaneously issuing instructions restricted simpl ify decoding instruction issue. Hence, require instructions paired aligned 64-bit boundary, ALU branch portion appearing first. Furthermore, one instruction pair cannot used, require replace nop. Thus, instructions always issue pairs, possibly 618with nop one slot. Figure 4.65 shows instructions look go pipeline pairs. FIGURE 4.65 Static two-issue pipeline operation. ALU data transfer instructions issued time. assumed five- stage structure used single-issue pipeline. Although strictly necessary, advantages. particular, keeping register writes end pipeline simplifies handl ing exceptions maintenance precise exception model, become difficult multiple-issue processors. Static multiple-issue processors vary deal potential data control hazards. designs, compiler takes full responsibility removing hazards, scheduling code, inserting no-ops code executes without need hazard detection hardware-generated stalls. others, hardware detects data hazards generates stalls two issue packets, requiring compiler avoid dependences within instruction packet. Even so, hazard generally forces entire issue packet containing depend ent instruction stall. Whether software must handle hazards try reduce fraction hazards separate issue packets, appearance large single instruction multiple operations reinforced. assume second approach example. issue ALU data transfer operation parallel, first need additional hardware—beyond usual hazard detection stall logic—is extra ports register file (see Figure 4.66 ). 619one clock cycle, may need read two registers ALU operation two store, also one write port ALU operation one write port load. Since ALU tied ALU operation, also need separate adder calculate effective address data transfers. Without extra resources, two-issue pipeline would hindered structural hazards. 620FIGURE 4.66 static two-issue datapath. additions needed double issue highlighted: another 32 bits instruction memory, two read ports one write port register fi le, another ALU. Assume bottom ALU handles address calculations data transfers top ALU handles everything else. Clearly, two-issue processor improve performance factor two! so, however, requires twice many instructions overlapped execution, additional ove rlap increases relative performance loss data control hazards. example, simple five-stage pipeline, loads use latency one clock cycle, prevents one instruction using result without stalling. two-issue, five- stage pipeline result load instruction cannot used next clock cycle . means next two instructions cannot use load result without stalling. Furthermore, ALU instructions th use latency simple five-stage pipeline one-instruction use latency, since results cannot used n paired load store. effectively exploit parallelism availab le multiple-issue processor, ambitious compiler hardw scheduling techniques needed, static multiple issue req uires compiler take role. 621 use latency Number clock cycles load instruction instruction use result load without stalling th e pipeline. Simple Multiple-Issue Code Scheduling Example would loop scheduled static two-issue pipeline RISC-V? Loop: ldx31, 0(x20) // x31=array element addx31, x31, x21 // add scalar x21 sdx31, 0(x20) // store result addix20, x20, -8 // decrement pointer bltx22, x20, Loop // compare loop limit, // branch x20 > x22 Reorder instructions avoid many pipeline stalls possible. Assume branches predicted, control hazards handled hardware. Answer first three instructions data dependences, next two. Figure 4.67 shows best schedule instructions. Notice one pair instructions issue slots us ed. takes five clocks per loop iteration; four clocks execute fi instructions, get disappointing CPI 0.8 versus best case 0.5, IPC 1.25 versus 2.0. Notice computing CPI IPC, count nops executed useful instructions. would improve CPI, performance! FIGURE 4.67 scheduled code would look two-issue RISC-V pipeline. empty slots no-ops. Note since moved addi sd, adjust sd’s offset 6228. important compiler technique get performance loops loop unrolling , multiple copies loop body made. unrolling, ILP available overlapping instructions different iterations. loop unrolling technique get performance loops access arrays, multiple copies loop body made instructions different iterations scheduled together . Loop Unrolling Multiple-Issue Pipelines Example See well loop unrolling scheduling work example above. simplicity, assume loop index multiple four. Answer schedule loop without delays, turns need make four copies loop body. unrolling eliminating unnecessary loop overhead instructions, loo p contain four copies ld, add, sd, plus one addi , one blt. Figure 4.68 shows unrolled scheduled code. FIGURE 4.68 unrolled scheduled code Figure 4.67 would look static two-issue RISC-V pipeline. 623The empty slots no-ops. Since first instruction loop decrements x20 32, addresses loaded original value x20, address minus 8, minus 16, minus 24. unrolling process, compiler introduced additi onal registers ( x28, x29, x30). goal process, called register renaming , eliminate dependences true data dependences, could either lead potential hazards prevent compiler flexibly scheduling code. Consider unrolled code would look using x31. would repeated instances ld x31 , 0(x20) , add x31 , x31, x21 followed sd x31 , 8(x20) , sequences, despite using x31, actually completely independent—no data values flow one set instructions next set. case called antidependence name dependence , ordering forced purely reuse name, rather real data dependence also called true dependence. Renaming registers unrolling process allows compiler move independent instructions subsequen tly better schedule code. renaming process eliminates name dependences, preserving true dependences. Notice 12 14 instructions loop execute pairs. takes eight clocks four loop iterations, yields IPC 14/8 =1.75. Loop unrolling scheduling doubled performance—8 versus 20 clock cycles 4 iterations— partly reducing loop control instructions partly f rom dual issue execution. cost performance improvement using four temporary registers rather one, well doubling code size. register renaming renaming registers compiler hardware remove antidependences. antidependence Also called name dependence ordering forced reuse name, typically register, 624rather true dependence carries value two instructions. Dynamic Multiple-Issue Processors Dynamic multiple-issue processors also known superscalar processors, simply superscalars. simplest superscalar processors, instructions issue order, processor dec ides whether zero, one, instructions issue given clock cycle. Obviously, achieving good performance processor still requires compiler try schedule instructions move dependences apart thereby improve instruction issue rate . Even compiler scheduling, important difference simple superscalar VLIW processor: code, whether scheduled not, guaranteed hardware execute correctly. Furthermore, compiled code always run correctly independent issue rate pipeline structur e processor. VLIW designs, case, recompilation required moving across different processor models; static issue processors, code would run correctly across different implementations, often poorly make compilation effectively required. superscalar advanced pipelining technique enables processor execute one instruction per clock cycle selectin g execution. Many superscalars extend basic framework dynamic issue decisions include dynamic pipeline scheduling . Dynamic pipeline scheduling chooses instructions execute given clock cycle trying avoid hazards stalls. Let’s st art simple example avoiding data hazard. Consider following code sequence: dynamic pipeline scheduling Hardware support reordering order instruction execu tion avoid stalls. 625ld x31, 0(x21) add x1, x31, x2 sub x23, x23, x3 andi x5, x23, 20 Even though sub instruction ready execute, must wait ld add complete first, might take many clock cycles memory slow. ( Chapter 5 explains cache misses, reason memory accesses sometimes slow.) Dynamic pipeline scheduling allows hazards avoided either fully partially. Dynamic Pipeline Scheduling Dynamic pipeline scheduling chooses instructions e xecute next, possibly reordering avoid stalls. processors, pipeline divided three major units: instruction fe tch issue unit, multiple functional units (a dozen high- end designs 2015), commit unit . Figure 4.69 shows model. first unit fetches instructions, decodes them, se nds instruction corresponding functional unit executi on. functional unit buffers, called reservation stations , hold operands operation. (In next section, 626will discuss alternative reservation stations used many recent processors.) soon buffer contains operands functional unit ready execute, result calculated. result completed, sent reservation stations waiting particular result well commit unit, buffers result safe put result register file or, store, memory. buffer commit unit, often called reorder buffer , also used supply operands, much way forwarding logic statically scheduled pipeline. result committed register file, fetched directly there, rmal pipeline. commit unit unit dynamic out-of-order execution pipeline decides safe release result operation programmer-visible registers memory. reservation station buffer within functional unit holds operands operation. reorder buffer buffer holds results dynamically scheduled process safe store results memory register. 627FIGURE 4.69 three primary units dynamically scheduled pipeline. final step updating state also called retirement graduation. combination buffering operands reservation stations results reorder buffer provides form register renaming, like used compiler earlier loop- unrolling example page 327. see conceptually works, consider following steps: 1. instruction issues, copied reservation station fo r appropriate functional unit. operands available register file reorder buffer also immediately copied reservation station. instruction buffered res ervation station operands functional unit available. issuing instruction, register copy operand longer required, write register occurred, value could overwritten. 2. operand register file reorder buffer, mus waiting produced functional unit. name functional unit produce result tracked. unit eventually produces result, copied directly int waiting reservation station functional unit bypassing registers. 628These steps effectively use reorder buffer reservat ion stations implement register renaming. Conceptually, think dynamically scheduled pipeline analyzing data flow structure program. processor executes instructions order preserves data flow order program. style execution called out- of-order execution , since instructions executed different order fetched. out-of-order execution situation pipelined execution instruction blocked executing cause following instructions wai t. in-order commit commit results pipelined execution writt en programmer visible state order instructions fetched. make programs behave running simple in- order pipeline, instruction fetch decode unit require issue instructions order, allows dependences trac ked, commit unit required write results registers memory program fetch order. conservative mode called in-order commit . Hence, exception occurs, computer point last instruction executed, registers updated written instructions inst ruction causing exception. Although front end (fetch issue) back end (commit) pipeline run order, functional units free initiate execution whenever data need ar e available. Today, dynamically scheduled pipelines use in-order commit. Dynamic scheduling often extended including hardware- based speculation, especially branch outcomes. predicting direction branch, dynamically scheduled processor continue fetch execute instructions along predicted p ath. instructions committed order, know whethe r branch correctly predicted instructions fro predicted path committed. speculative, dynamically 629scheduled pipeline also support speculation load addresse s, allowing load-store reordering, using commit unit avoi incorrect speculation. next section, look us e dynamic scheduling speculation Intel Core i7 design . Understanding Program Performance Given compilers also schedule code around data dependences, might ask superscalar processor would use dynamic scheduling. three major reasons. First, stalls predictable. particular, cache misses (see Chapter 5 ) memory hierarchy cause unpredictable stalls. Dynamic scheduling allows processor hide stalls continuing execute instructions waiting stall end. 630Second, processor speculates branch outcomes using dynamic branch prediction , cannot know exact order instructions compile time, since depends predicte actual behavior branches. Incorporating dynamic speculation exploit instruction-level parallelism (ILP) without incorporating dynamic scheduling would significantly restrict benefits f speculation. Third, pipeline latency issue width change one implementation another, best way compile code sequence also changes. example, schedule sequence dependent instructions affected issue width late ncy. pipeline structure affects number times loop ust unrolled avoid stalls well process compiler-base register renaming. Dynamic scheduling allows hardware hide details. Thus, users software distributors need worry multiple versions program different implementations instruction set. Similar ly, old legacy code get much benefit new implementation without need recompilation. 631 BIG Picture pipelining multiple-issue execution increase peak instruction throughput attempt exploit instruction-l evel parallelism (ILP). Data control dependences programs, however, offer upper limit sustained performance processor must sometimes wait dependence resolved. Software-centric approaches exploiting ILP rely ability compiler find reduce effects dependences, hardware-centric approaches rely extensions pipeline issue mechanisms. Speculation, performed compiler hardware, increase amount ILP exploited via prediction , although care must taken since speculating incorrectly likely reduc e performance. 632 Hardware/Software Interface Modern, high-performance microprocessors capable issuin g 633several instructions per clock; unfortunately, sustaining sue rate difficult. example, despite existence processors four six issues per clock, applicatio ns sustain two instructions per clock. two primary reasons this. First, within pipeline, major performance bottlenecks arise dependences cannot alleviated, thus reducing parallelism among instructions sustained issue rate. Although little done true data dependences, often compiler hardware know precisely whether dependence exists not, must conservatively assume dependence exists. example, code makes use pointers, particularly ways may lead aliasing, lead implied potential dependences. contrast, greater regularit array accesses often allows compiler deduce dependences exist. Similarly, branches cannot accurately predicted whether runtime compile time limit abi lity exploit ILP. Often, additional ILP available, ability compiler hardware find ILP may widely separated (sometimes execution thousands instructions) limited. Second, losses memory hierarchy (the topic Chapter 5 ) also limit ability keep pipeline full. memory sys tem stalls hidden, limited amounts ILP also limit extent stalls hidden. 634Energy Efficiency Advanced Pipelining downside increasing exploitation instruction-l evel parallelism via dynamic multiple issue speculation potenti al energy inefficiency. innovation able turn transistors performance, often inefficiently. collided power wall, seeing designs multiple processors per chip processors deeply pipelined aggressively speculative predecessors. belief simpler processors fast sophisticated brethren, deliver better performance per Joule, deliver performance per chip designs constrained energy number transistors. Figure 4.70 shows number pipeline stages, issue width, speculation level, clock rate, cores per chip, power several past recent Intel microprocessors. Note drop pipelin e stages power companies switch multicore designs. Elaboration commit unit controls updates register file memory. dynamically scheduled processors update register fil e 635immediately execution, using extra registers implem ent renaming function preserving older copy register instruction updating register longer spec ulative. processors buffer result, which, mentioned above, typically structure called reorder buffer, actual update register file occurs later part commit. Stor es memory must buffered commit time either store buffer (see Chapter 5 ) reorder buffer. commit unit allows store write memory buffer buffer valid address valid data, store longer dependent predicted branches. Elaboration Memory accesses benefit nonblocking caches , continue servicing cache accesses cache miss (see Chapter 5 ). Out- of-order execution processors need cache allow instruc tions execute miss. Check State whether following techniques components associated primarily software- hardware-based approach exploiting ILP. cases, answer may both. 1. Branch prediction 2. Multiple issue 3. VLIW 4. Superscalar 5. Dynamic scheduling 6. Out-of-order execution 7. Speculation 8. Reorder buffer 9. Register renaming 636FIGURE 4.70 Record Intel Microprocessors terms pipeline complexity, number cores, power. Pentium 4 pipeline stages include commit stages. included them, Pentium 4 pipelines would even deeper. 4.11 Real Stuff: ARM Cortex-A53 Intel Core i7 Pipelines Figure 4.71 describes two microprocessors examine section, whose targets two endpoints post-PC era. FIGURE 4.71 Specification ARM Cortex-A53 Intel Core i7 920. ARM Cortex-A53 ARM Corxtex-A53 runs 1.5 GHz eight-stage pipeline executes ARMv8 instruction set. uses dynamic multiple 637issue, two instructions per clock cycle. static in-ord er pipeline, instructions issue, execute, commit order . pipeline consists three sections instruction fetc h, instruction decode, execute. Figure 4.72 shows overall pipeline. 638FIGURE 4.72 Cortex-A53 pipeline. first three stages fetch instructions 13-entry instruction queue. Address Generation Unit (AGU) uses Hybrid Predictor, Indirect Predictor , Return Stack predict branches try keep instruction queue full. Instruction decode three stages instruction execution three stages. two additional stages floating point SIMD operations. first three stages fetch two instructions time try keep 13-entry instruction queue full. uses 6k-bit hybrid conditional branch predictor, 256-entry indirect branch predicto r, 8-entry return address stack predict future function returns. prediction indirect branches takes additional pipeline stage. design choice incur extra latency instruction queue cannot decouple decode execute stages fetch stage, primarily case branch misprediction instruction cache miss. branch prediction wrong, empties pipeline, resulting eight-clock cycle misprediction penalty. decode stages pipeline determine dependences pair instructions, would force sequential execution, pipeline execution stag es send instructions. instruction execution section primarily occupies three 639pipeline stages provides one pipeline load instruction s, one pipeline store instructions, two pipelines integer ar ithmetic operations, separate pipelines integer multiply divid e operations. Either instruction pair issued load store pipelines. execution stages full forwarding pipelines. Floating-point SIMD operations add two pipeline stages instruction execution section feature one pip eline multiply/divide/square root operations one pipeline fo r arithmetic operations. Figure 4.73 shows CPI Cortex-A53 using SPEC2006 benchmarks. ideal CPI 0.5, best case achieved 1.0, median case 1.3, worst case 8.6. median case, 60% stalls due pipelining hazards 40% stalls due memory hierarchy. Pipeline stalls caused branch mispredictions, structural hazards, data dependencies pairs instructions. Given static pipeline Cortex-A53, compiler try avoid structural hazards data dependences. Elaboration Cortex-A53 configurable core supports ARMv8 instruction set architecture. delivered IP ( Intellectual Property ) core. IP cores dominant form technology delivery embedded, personal mobile device, related markets; billions ARM MIPS processors created IP cores. Note IP cores different cores Intel i7 multicore computers. IP core (which may multicore ) designed incorporated logic (hence th e “core” chip), including application-specific processors (su ch encoder decoder video), I/O interfaces, memory interfaces, fabricated yield processor optimized particular application. Although processor core almost identical logically, resultant chips many differences. One parameter size L2 cache, vary factor 16. 640641FIGURE 4.73 CPI ARM Cortex-A53 SPEC2006 integer benchmarks. Intel Core i7 920 x86 microprocessors employ sophisticated pipelining approache s, using dynamic multiple issue dynamic pipeline scheduling out-of-order execution speculation ir pipelines. processors, however, still faced challenge implementing complex x86 instruction set, described Chapter 2 . Intel fetches x86 instructions translates internal RISC-V-like instructions, Intel calls micro- operations . micro-operations executed sophisticated, dynamically scheduled, speculative pipeline capab le sustaining execution rate six micro-operations per clock cycle. section focuses micro-operation pipel ine. consider design processors, design functional units, cache register file, instruction issu e, overall pipeline control become intermingled, making difficult separate datapath pipeline. thi s, many engineers researchers adopted term microarchitecture refer detailed internal architecture 642processor. microarchitecture organization processor, including major functional units, interconnection, control. architectural registers instruction set visible registers processor; exam ple, RISC-V, 32 integer 32 floating-point registers. Intel Core i7 uses scheme resolving antidependences incorrect speculation uses reorder buffer together wit h register renaming. Register renaming explicitly renames architectural registers processor (16 case 64-bit version x86 architecture) larger set physical register s. Core i7 uses register renaming remove antidependences. Register renaming requires processor maintain map architectural registers physical registers, indicating physical register current copy architectural register. keeping track renamings occurred, register renaming offers another approach recovery event incorrect speculation: simply undo mappings occurred since first incorrectly speculated instruc tion. undo cause state processor return last correctly executed instruction, keeping correct mapping architectural physical registers. Figure 4.74 shows overall organization pipeline Core i7. eight steps x86 instruction goes execution. 1. Instruction fetch—The processor uses multilevel branch targe buffer achieve balance speed prediction accuracy. also return address stack speed function return. Mispredictions cause penalty 15 cycles. Using predicted address, instruction fetch unit fetches 16 bytes f rom instruction cache. 2. 16 bytes placed predecode instruction buffer—The predecode stage transforms 16 bytes individual x86 instructions. predecode nontrivial since length x86 643instruction 1 15 bytes predecoder must look number bytes knows instruction length . Individual x86 instructions placed 18-entry instructio n queue. 3. Micro-op decode—Individual x86 instructions translated micro-operations (micro-ops). Three decoders handle x86 instructions translate directly one micro-op. x86 instructions complex semantics, microcode engine used produce micro-op sequenc e; produce four micro-ops every cycle continues unti l necessary micro-op sequence generated. micro- ops placed according order x86 instructions 28-entry micro-op buffer. 4. micro-op buffer performs loop stream detection —If small sequence instructions (less 28 instructions 256 byte length) comprises loop, loop stream detector fi nd loop directly issue micro-ops buffer, eliminating need instruction fetch instruction decode stages activated. 5. Perform basic instruction issue—Looking register location register tables, renaming registers, allocating reorder buffer entry, fetching results registers reorder buffer sending micro-ops reservati stations. 6. i7 uses 36-entry centralized reservation station shared six functional units. six micro-ops may dispatched functional units every clock cycle. 7. individual function units execute micro-ops resul ts sent back waiting reservation station well register retirement unit, update register tate, known instruction longer speculative. Th e entry corresponding instruction reorder buffer marked complete. 8. one instructions head reorder buffer marked complete, pending writes register retirement unit executed, instructions removed fr om reorder buffer. Elaboration 644Hardware second fourth steps combine fuse operations together reduce number operations must performed. Macro-op fusion second step takes x86 instruction combinations, compare followed branch, fuses single operation. Microfusion fourth step combines micro-operation pairs load/ALU operation ALU operation/store issues single reservation station (where still issue independently), thus increas ing usage buffer. study Intel Core architecture, also incorporated microfusion macrofusion, Bird et al. [2007] discovered microfusion little impact performance, macrofusion appears modest positive impact integer performance little impact floating-poin performance. 645FIGURE 4.74 Core i7 pipeline memory components. total pipeline depth 14 stages, branch mispredictions costing 17 clock cycles. design buffer 48 loads 32 stores. six independent units begin execution ready micro-operation clock cycle. Performance Intel Core i7 920 Figure 4.75 shows CPI Intel Core i7 SPEC2006 benchmarks. ideal CPI 0.25, best case achieved 0.44, median case 0.79, worst case 2.67. 646647FIGURE 4.75 CPI Intel Core i7 920 running SPEC2006 integer benchmarks. Although difficult differentiate pipeline st alls memory stalls dynamic out-of-order execution pipeline, c show effectiveness branch prediction speculation. Figure 4.76 shows percentage branches mispredicted percentage work (measured numbers micro-ops dispatched pipeline) retire (that is, results annulled) relative micro-op dispatches. min , median, max branch mispredictions 0%, 2%, 10%. wasted work, 1%, 18%, 39%. 648FIGURE 4.76 Percentage branch mispredictions wasted work due unfruitful speculation Intel Core i7 920 running SPEC2006 integer benchmarks. wasted work cases closely matches branch misprediction rates, gobmk astar. several instances, mcf, wasted work seems relatively larger misprediction rate. divergence likely due mory behavior. high data cache miss rates, mcf dispatch many instructions incorrect speculation long sufficient reservation stations available stalled memo ry references. branch among many speculated instructions finally mispredicted, micro-ops corresponding thes e instructions flushed. Understanding Program Performance Intel Core i7 combines 14-stage pipeline aggressive multiple issue achieve high performance. keeping latencies back-to-back operations low, impact data dependences reduced. serious potential performance bottlenecks programs running processo r? following list includes possible performance probl ems, last three apply form high- 649performance pipelined processor. use x86 instructions map simple micro- operations Branches difficult predict, causing misprediction stal ls restarts speculation fails Long dependences—typically caused long-running instructions memory hierarchy —that lead stalls Performance delays arising accessing memory (see Chapter 5 ) cause processor stall 4.12 Going Faster: Instruction-Level Parallelism Matrix Multiply Returning DGEMM example Chapter 3 , see impact instruction-level parallelism unrolling loop multiple-issue, out-of-order execution processor mo instructions work with. Figure 4.77 shows unrolled version Figure 3.22 , contains C intrinsics produce AVX instructions. 650FIGURE 4.77 Optimized C version DGEMM using C intrinsics generate AVX subword-parallel instructions x86 ( Figure 3.22 ) loop unrolling create opportunities instruction-level parallelism. Figure 4.78 shows assembly language produced compiler inner loop, unrolls three for-loop bodies expose instruction-level parallelism. Like unrolling example Figure 4.68 above, going unroll loop four times. Rather manually unrolling loo p C making four copies intrinsics Figure 3.22 , rely gcc compiler unrolling −O3 optimization. (We use constant UNROLL C code control amount unrolling case want try values.) surround intrinsic simple loop four iterations (lines 9, 15, 20) replace scalar C0 Figure 3.22 four-element array c[] (lines 8, 10, 16, 21). Figure 4.78 shows assembly language output unrolled code. expected, Figure 4.78 four versions AVX instructions Figure 3.23 , one exception. need one copy vbroadcastsd instruction, since use 651four copies B element register %ymm0 repeatedly throughout loop. Thus, five AVX instructions Figure 3.23 become 17 Figure 4.78 , seven integer instructions appear both, although constants addressing changes account unrolling. Hence, despite unrolling four times, number instructions body loop doubles: 12 24. 652FIGURE 4.78 x86 assembly language body nested loops generated compiling unrolled C code Figure 4.77 . Figure 4.79 shows performance increase DGEMM 32 ×32 matrices going unoptimized AVX AVX unrolling. Unrolling doubles performance, going 6.4 GFLOPS 14.6 GFLOPS. Optimizations subword parallelism instruction-level parallelism result overall speedup 8.59 versus unoptimized DGEMM Figure 3.21 . 653 Elaboration mentioned Elaboration Section 3.8 , results Turbo mode turned off. turn on, like Chapter 3 , improve results temporary increase clock rat e 3.3/2.6 =1.27 2.1 GFLOPS unoptimized DGEMM, 8.1 GFLOPS AVX, 18.6 GFLOPS unrolling AVX. mentioned Section 3.8 , Turbo mode works particularly well case using single core eight-core ch ip. Elaboration pipeline stalls despite reuse register %ymm5 lines 9 17 Figure 4.78 Intel Core i7 pipeline renames registers. Check following statements true false? 1. Intel Core i7 uses multiple-issue pipeline directly ex ecute x86 instructions. 2. Cortex-A53 Core i7 use dynamic multiple issue. 3. Core i7 microarchitecture many registers x86 requires. 4. Intel Core i7 uses less half pipeline stages 654earlier Intel Pentium 4 Prescott (see Figure 4.70 ). FIGURE 4.79 Performance three versions DGEMM 32 × 32 matrices. Subword parallelism instruction-level parallelism led speedup almost factor 9 unoptimized code Figure 3.21 . Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline 655More Pipelining Illustrations Modern digital design done using hardware description languages modern computer-aided synthesis tools create detailed hardware designs descriptions using bot h libraries logic synthesis. Entire books written languages use digital design. section, appears online, gives brief introduction shows hardware design language, Verilog case, used describe processor control behaviorally form suitable hardware synthesis. provides series behavioral models Verilog five-stage pipeline. init ial model ignores hazards, additions model highlight changes forwarding, data hazards, branch hazards. provide dozen illustrations using single- cycle graphical pipeline representation readers want se e detail pipelines work sequences RISC-V instructions. 4.13 Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline Pipelining Illustrations online section covers hardware description languages gives dozen examples pipeline diagrams, starting page 366.e18. mentioned Appendix , Verilog describe processors simulation intention Verilog specificatio n synthesized. achieve acceptable synthesis results size speed, behavioral specification intended synthesis must carefully delineate highly combinational portions des ign, datapath, control. datapath synthesized using available libraries. Verilog specification intended synthesis usually longer complex. 656We start behavioral model five-stage pipeline. illustrate dichotomy behavioral synthesizable designs, give two Verilog descriptions multiple-cyc le- per-instruction RISC-V processor: one intended solely simulations one suitable synthesis. Using Verilog Behavioral Specification Simulation Five-Stage Pipeline Figure e4.13.1 shows Verilog behavioral description pipeline handles ALU instructions well loads stores. accommodate branches (even incorrectly!), postpone including later chapter. 657658FIGURE E4.13.1 Verilog behavioral model RISC-V five-stage pipeline, ignoring branch data hazards. design earlier Chapter 4, use separate instruction data memories, would implemented using separate caches describe Chapter 5 . Verilog lacks ability define registers named fields structures C, use several independent regist ers pipeline register. name registers prefix using convention; hence, IFIDIR IR portion IFID pipeline register. version behavioral description intended synthesis. Instructions take number clock cycles ur hardware design, control done simpler fashion repeatedly decoding fields instruction pipe stag e. difference, instruction register (IR) ne eded throughout pipeline, entire IR passed pipe stag e pipe stage. read Verilog descriptions chapte r, remember actions always block occur parallel every clock cycle. Since blocking assignments, order events within always block arbitrary. Implementing Forwarding Verilog extend Verilog model further, Figure e4.13.2 shows addition forwarding logic case source destination ALU instructions. Neither load stalls branches handled; add shortly. changes earlier 659Verilog description highlighted. Check Someone proposed moving write result ALU instruction WB MEM stage, pointing would reduce maximum length forwards ALU instruction one cycle. following accurate reasons consider change? 1. would actually change forwarding logic, advantage. 2. impossible implement change circumstanc e since write ALU result must stay pipe stage write load result. 3. Moving write ALU instructions would create possibility writes occurring two different instruc tions clock cycle. Either extra write port would required register file structural hazard would created. 4. result ALU instruction available time write MEM. 660661FIGURE E4.13.2 behavioral definition five- stage RISC-V pipeline bypassing ALU operations address calculations. code added Figure e4.13.1 handle bypassing highlighted. bypasses require changing ALU inputs come from, changes required combinational logic 662responsible selecting ALU inputs. Behavioral Verilog Stall Detection ignore branches, stalls data hazards RISC-V pipeline confined one simple case: loads whose results currently n WB clock stage. Thus, extending Verilog handle load destination either ALU instruction effective address calculation reasonably straightforward, Figure e4.13.3 shows additions needed. Check Someone asked possibility data hazards occurring memory, contrary register. following statements hazards true? 1. Since memory accesses occur MEM stage, memory operations done order instruction execution, making hazards impossible pipeline. 2. hazards possible pipeline; discussed yet. 3. pipeline ever hazard involving memory, since programmer’s job keep order memory references accurate. 4. Memory hazards may possible pipelines, cannot occur particular pipeline. 5. Although pipeline control would obligated maintain ordering among memory references avoid hazards, impossible design pipeline references could order. 663664665FIGURE E4.13.3 behavioral definition five- stage RISC-V pipeline stalls loads destination ALU instruction effective address calculation. changes Figure e4.13.2 highlighted. Implementing Branch Hazard Logic Verilog extend Verilog behavioral model implement control branches. add code model branch equal using “predict taken” strategy. Verilog code shown Figure e4.13.4 . implements branch hazard detecting taken branch ID using signal squash instruction (by setting IR 0x00000013 , effective NOP RISC- V); addition, PC assigned branch target. Note prevent unexpected latch, important PC clearly assigned every path always block; hence, assign PC single statement. Lastly, note although Figure e4.13.4 incorporates basic logic branches control hazards, supporting branches requires additional bypassing data hazard detection, included. 666667668FIGURE E4.13.4 behavioral definition five- stage RISC-V pipeline stalls loads destination ALU instruction effective address calculation. changes Figure e4.13.2 highlighted. Using Verilog Behavioral Specification Synthesis demonstrate contrasting types Verilog, show two descriptions different, nonpipelined implementation sty le RISC-V uses multiple clock cycles per instruction. (Sinc e instructors make synthesizable description RISC-V pipel ine project class, chose include here. would also long.) Figure e4.13.5 gives behavioral specification multicycle implementation RISC-V processor. use behavioral operations, would difficult synthesize separat e datapath control unit reasonable efficiency. version demonstrates another approach control using Mealy finite-state machine (see discussion Section A.10 Appendix ). use Mealy machine, allows output depend inputs current state, allows us decrease total number states. 669670FIGURE E4.13.5 behavioral specification multicycle RISC-V design. cycle behavior multicycle design, purely simulation specification. cannot used synthesis. Since version RISC-V design intended synthesis considerably complex, relied number Verilog modules specified Appendix , including following: 4-to-1 multiplexor shown Figure A.4.2, 2-to-1 multiplexor trivially derived based 4-to-1 multiplexor. RISC-V ALU shown Figure A.5.15. RISC-V ALU control defined Figure A.5.16. RISC-V register file defined Figure A.8.11. Now, let’s look Verilog version RISC-V processor intended synthesis. Figure e4.13.6 shows structural version RISC-V datapath. Figure e4.13.7 uses datapath module specify RISC-V CPU. version also demonstrates another 671approach implementing control unit, well optimizations rely relationships various contro l signals. Observe state machine specification provide sequencing actions. 672673FIGURE E4.13.6 Verilog version multicycle RISC-V datapath appropriate synthesis. datapath relies several units Appendix . Initial statements synthesize, version used synthesis would incorporate reset signal effect. Also note resetting R0 0 every clock best way ensure R0 stays 0; instead, modifying register file module produce 0 whenever R0 read ignore writes R0 would efficient solution. 674FIGURE E4.13.7 RISC-V CPU using datapath Figure e4.13.6 . setting control lines done series assign statements depend state well opcode field instruction register. one fold setting control 675into state specification, would look like Mealy-style f inite- state control unit. setting control lines pecified using assign statements outside always block, logic synthesis systems generate small implementation finite- state machine determines setting state register uses external logic derive control inputs dat apath. writing version control, also taken advantage number insights relationship various control signals well situations don’t care control signal value; examples given following elaboration. Illustrations Instruction Execution Hardware reduce cost book, starting third edition , moved sections figures used minority instructors online. subsection recaptures figures readers would like supplemental material understand pipelining better. single-clock-cycle pipeline di agrams, take many figures illustrate execution sequence f instructions. three examples respectively code hazards, example forwarding pipelined implementation, example bypassing pipelined implementation. Hazard Illustrations page 285, gave example code sequence ld x10, 40(x1) sub x11, x2, x3 add x12, x3, x4 ld x13, 48(x1) add x14, x5, x6 Figures e4.42 e4.43 showed multiple-clock-cycle pipeline diagrams two-instruction sequence executing across ix clock cycles. Figures e4.13.8 e4.13.10 show corresponding single-clock-cycle pipeline diagrams se two instructions. Note order instructions differs b etween two types diagrams: newest instruction bottom right multiple-clock-cycle pipeline diagram, 676on left single-clock-cycle pipeline diagram. 677FIGURE E4.13.8 Single-cycle pipeline diagrams clock cycles 1 (top diagram) 2 (bottom diagram). style pipeline representation snapshot every instruction executing one clock cycle. example two instructions, two stages identified clock cycle; normally, five stages occupied. highlighted portions datapath active clock cycle. load fetched clock cycle 1 decoded clock cycle 2, 678with subtract fetched second clock cycle. make figures easier understand, pipeline stages empty, normally instruction every pipeline stage. 679FIGURE E4.13.9 Single-cycle pipeline diagrams clock cycles 3 (top diagram) 4 (bottom diagram). third clock cycle top diagram, ld enters EX stage. time, sub enters ID. fourth clock cycle (bottom datapath), ld moves MEM stage, reading memory using address found EX/MEM beginning clock cycle 4. time, ALU subtracts places difference 680into EX/MEM end clock cycle. 681FIGURE E4.13.10 Single-cycle pipeline diagrams clock cycles 5 (top diagram) 6 (bottom diagram). clock cycle 5, ld completes writing data MEM/WB register 10, sub sends difference EX/MEM MEM/WB. next clock cycle, sub writes value MEM/WB register 11. Examples understand pipeline control works, let’s consider five instructions going pipeline: ld x10, 40(x1) 682sub x11, x2, x3 x12, x4, x5 x13, x6, x7 add x14, x8, x9 Figures e4.13.11 e4.13.15 show instructions proceeding nine clock cycles takes compl ete execution, highlighting active stage identifying th e instruction associated stage clock cycle. examine carefully, may notice: Figure e4.13.13 see sequence destination register numbers left right bottom pipeli ne registers. numbers advance right clock cycle, MEM/WB pipeline register supplying number register written WB stage. stage inactive, values control lines deasserted shown 0 X (for don’t care). Sequencing control embedded pipeline structure self. First, instructions take number clock cycles, special control instruction duration. Second, control information computed instruction decode passed along pipeline registers. 683FIGURE E4.13.11 Clock cycles 1 2. phrase “ <i> ” means ith instruction ld. ld instruction top datapath stage. end clock cycle, ld instruction IF/ID pipeline registers. 684second clock cycle, seen bottom datapath, ld moves ID stage, sub enters stage. Note values instruction fields selected source registers shown ID stage. Hence, register x1 constant 40, operands ld, written ID/EX pipeline register. number 10, representing destination register number ld, also placed ID/EX. top ID/EX pipeline register shows control values ld used remaining stages. control values read ld row table Figure e4.18. 685FIGURE E4.13.12 Clock cycles 3 4. top diagram, ld enters EX stage third clock cycle, adding x1 40 form address EX/MEM pipeline register. (The ld instruction written ld x10 , … upon reaching EX, identity instruction operands needed EX subsequent stages. version pipeline, 686the actions EX, MEM, WB depend instruction destination register target address.) time, sub enters ID, reading registers x2 x3, instruction starts IF. fourth clock cycle (bottom datapath), ld moves MEM stage, reading memory using value EX/MEM address. clock cycle, ALU subtracts x3 x2 places difference EX/MEM, reads registers x4 x5 ID, instruction enters IF. two diagrams show control signals created ID stage peeled used subsequent pipe stages. 687FIGURE E4.13.13 Clock cycles 5 6. add, final instruction example, entering top datapath, instructions engaged. writing data MEM/WB register 10, ld completes; data register number MEM/WB. clock cycle, sub sends difference EX/MEM MEM/WB, rest 688instructions move forward. next clock cycle, sub selects value MEM/WB write register number 11, found MEM/WB. remaining instructions play follow-the-leader: ALU calculates x6 x7 instruction EX stage, registers x8 x9 read ID stage add instruction. instructions add shown inactive emphasize occurs five instructions example. phrase “after <i>” means ith instruction add. 689FIGURE E4.13.14 Clock cycles 7 8. top datapath, add instruction brings rear, adding values corresponding registers x8 x9 EX stage. result instruction passed EX/MEM MEM/WB MEM stage, WB stage writes result instruction MEM/WB register x12. Note 690the control signals deasserted (set 0) ID stage, since instruction executed. following clock cycle (lower drawing), WB stage writes result register x13, thereby completing or, MEM stage passes sum add EX/MEM MEM/WB. instructions add shown inactive pedagogical reasons. 691FIGURE E4.13.15 Clock cycle 9. WB stage writes ALU result MEM/WB register x14, completing add five-instruction sequence. instructions add shown inactive pedagogical reasons. Forwarding Illustrations use single-clock-cycle pipeline diagrams show forwarding operates, well control activates forwarding paths. Consider following code sequence dependences highlighted: sub x2, x1, x3 x4, x2, x5 x4, x4, x2 add x9, x4, x2 Figures e4.13.16 e4.13.17 show events clock cycles 3–6 execution instructions. 692FIGURE E4.13.16 Clock cycles 3 4 instruction sequence page 366.e26. bold lines active clock cycle, italicized register numbers color indicate hazard. forwarding unit highlighted shading forwarding data ALU. instruction sub shown inactive emphasize occurs four instructions example. Operand names used EX control forwarding; thus included instruction label EX. Operand names needed MEM WB, … used. Compare Figures e4.13.12 e4.13.15 , show datapath 693without forwarding ID last stage need operand information. 694FIGURE E4.13.17 Clock cycles 5 6 instruction sequence page 366.e26. forwarding unit highlighted forward ing data ALU. two instructions add shown inactive emphasize occurs four instructions example. bold lines active clock cycle, italicized register numbers color indicate hazard. Thus, clock cycle 5, forwarding unit selects EX/MEM pipeline register upper input ALU MEM/WB pipeline register lower input ALU. following add instruction reads register x4, target instruction, 695and register x2, sub instruction already written. Notice prior two instructions write register x4, forwarding unit must pick immediately preceding one (MEM stage). clock cycle 6, forwarding unit thus selects EX/MEM pipeline register, containing result instruction, upper ALU input uses non-forwarding register value th e lower input ALU. Illustrating Pipelines Stalls Forwarding use single-clock-cycle pipeline diagrams show control stalls works. Figures e4.13.18 e4.13.20 show single-cycle diagram clocks 2 7 following code sequence (dependences highlighted): 696FIGURE E4.13.18 Clock cycles 2 3 instruction sequence page 366.e26 load replacing sub. bold lines active clock cycle, italicized register numbers color indicate hazard, … place operands means identity information needed stage. values significant control lines, registers, register numbers labeled figures. instruction wants read value created ld instruction clock cycle 3, hazard detection unit stalls instructions. Hence, hazard 697detection unit highlighted. 698FIGURE E4.13.19 Clock cycles 4 5 instruction sequence page 366.e26 load replacing sub. bubble inserted pipeline clock cycle 4, instruction allowed proceed clock cycle 5. forwarding unit highlighted clock cycle 5 forwarding data ld ALU. Note clock cycle 4, forwarding unit forwards address ld contents register x2; rendered harmless insertion bubble. bold lines active clock cycle, italicized register numbers color indicate hazard. 699700FIGURE E4.13.20 Clock cycles 6 7 instruction sequence page 366.e26 load replacing sub. Note unlike Figure e4.13.17 , stall allows ld complete, forwarding MEM/WB clock cycle 6. Register x4 add EX stage still depends result EX/MEM, forwarding unit passes result ALU. bold lines show ALU input lines active clock cycle, italicized register numbers indicate hazard. instructions add shown inactive pedagogical reasons. ld x2, 40(x1) x4, x2, x5 701or x4, x4, x2 add x9, x4, x2 4.14 Fallacies Pitfalls Fallacy: Pipelining easy. books testify subtlety correct pipeline execut ion. advanced book pipeline bug first edition, despite reviewed 100 people class-tested 18 universities. bug uncovered someone tried build computer book. fact Verilog describe pipeline like Intel Core i7 hundreds thousands lines indication complexity. Beware! Fallacy: Pipelining ideas implemented independent technology. number transistors on-chip speed transistors made five-stage pipeline best solution, th e delayed branch (see Elaboration page 274) simple solution control hazards. longer pipelines, superscalar execution, dynamic branch prediction, redundant. early 1990s, dynamic pipeline scheduling took many resources required high performance, transistor budgets continued double due Moore’s Law logic became much faster memory, multiple functional units dynamic pipelining made sense. Today, concerns power leading less aggressive efficient designs. 702Pitfall: Failure consider instruction set design adversely impact pipelining. Many difficulties pipelining arise instruc tion set complications. examples: Widely variable instruction lengths running times lead imbalance among pipeline stages severely complicate hazard detection design pipelined instruction set level. Thi problem overcome, initially DEC VAX 8500 late 1980s, using micro-operations micropipelined scheme Intel Core i7 employs today. course, overhead translation maintaining correspondence micro- operations actual instructions remains. Sophisticated-addressing modes lead different sorts problems. Addressing modes update registers complicate hazard detection. addressing modes require multiple memory accesses substantially complicate pipeline control make difficult keep pipeline flowing smoothly. Perhaps best example DEC Alpha DEC NVAX. comparable technology, newer instruction set architectu Alpha allowed implementation whose performance twice fast NVAX. another example, Bhandarkar Clark [1991] compared MIPS M/2000 703DEC VAX 8700 counting clock cycles SPEC benchmarks; concluded although MIPS M/2000 executes instructions, VAX average executes 2.7 times many clock cycles, MIPS faster. 4.15 Concluding Remarks Nine-tenths wisdom consists wise time. American proverb seen chapter, datapath control processor designed starting instruction set architecture understanding basic characteristics technology. Section 4.3 , saw datapath RISC-V processor could constructed based architecture th e decision build single-cycle implementation. course, underlying technology also affects many design decisions dictating components used datapath, well whether single-cycle implementation even makes sense. Pipelining improves throughput inherent execution time, instruction latency , instructions; instructions, latency similar length single-cycle approach. Multiple instruction issue adds additional datapath hardware allow multiple instructions begin every clock cycle, increase effective latency. Pipelining presented reduc ing clock cycle time simple single-cycle datapath. Multipl e instruction issue, comparison, clearly focuses reducing clock cycles per instruction (CPI). 704 instruction latency inherent execution time instruction. Pipelining multiple issue attempt exploit instruc tion- level parallelism. presence data control dependences, become hazards, primary limitations much parallelism exploited. Scheduling speculation via prediction , hardware software, primary techniques used reduce performance impact dependenc es. 705We showed unrolling DGEMM loop four times exposed instructions could take advantage out-of-order execution engine Core i7 double performance. switch longer pipelines, multiple instruction issue, dynamic scheduling mid-1990s helped sustain 60% per year processor performance increase started early 1980s. mentioned Chapter 1 , microprocessors preserved sequential programming model, eventually ran power wall. Thus, industry forced switch multiprocessors, exploit parallelism much coarser level (the subject Chapter 6 ). trend also caused designers reassess energy-performance implications inventions since mid-1990s, resulting simplification pipelines recent versions microarchitectures. sustain advances processing performance via parallel processors, Amdahl’s law suggests another part system become bottleneck. bottleneck topic n ext chapter: memory hierarchy . 706 Historical Perspective Reading section, appears online, discusses history first pipelined processors, earliest superscalars, development out-of-order speculative techniques, wel l important developments accompanying compiler technology. 4.16 Historical Perspective Reading supercomputer: machine still drawing board. Stan Kelly-Bootle, Devil’s DP Dictionary, 1981 707This section discusses history original pipelined processors, earliest superscalars, development -of- order speculative techniques, well important developments accompanying compiler technology. generally agreed one first general-purpose pipelined computers Stretch, IBM 7030 ( Figure e4.16.1 ). Stretch followed IBM 704 goal 100 times faster 704. goals “stretch” state art time—hence nickname. plan obtain factor 1.6 overlapping fetch, decode, execute using four- stage pipeline. Apparently, rest come much hardware faster logic. Stretch also training ground architects IBM 360, Gerrit Blaauw Fred Brooks, Jr., architect IBM RS/6000, John Cocke. 708FIGURE E4.16.1 Stretch computer, one first pipelined computers. Control Data Corporation (CDC) delivered considered first supercomputer, CDC 6600, 1964 ( Figure e4.16.2 ). core instructions Cray’s subsequent computers many similarities original CDC 6600. CDC 6600 unique many ways. interaction pipelining instruction set design understood, instruction set kept simple promote pipelining. CDC 6600 also used advanced packaging technology. James Thornton’s book [1970] provides excellent description entire computer, technology architecture, includes foreword Seymour Cray. (Unfortunately, book currently print.) Jim Smit h, working CDC, developed original 2-bit branch prediction scheme explored several techniques enhancin g instruction issue CDC Cyber 180/990. Cray, Thornton, Smith ACM Eckert-Mauchly Award (in 1989, 1994, 1999, respectively). 709FIGURE E4.16.2 CDC 6600, first supercomputer. IBM 360/91 introduced many new concepts, including dynamic detection memory hazards, generalized forwarding, reservation stations ( Figure e4.16.3 ). approach normally named Tomasulo’s algorithm , engineer worked project. team created 360/91 led Michael Flynn, given 1992 ACM Eckert-Mauchly Award, part contributions IBM 360/91; 1997, award went Robert Tomasulo pioneering work out-of-order processing. 710FIGURE E4.16.3 IBM 360/91 pushed state art pipelined execution unveiled 1966. internal organization 360/91 shares many features Pentium III Pentium 4, well several microprocessors. One major difference branch prediction 360/91 hence speculation. Another major difference commit unit, instructions finished execution, updated registers. ut-of- order instruction commit led imprecise interrupts , proved unpopular led commit units dynamically scheduled pipelined processors since time. Although 360/91 success, key ideas resurrected later exist fo rm majority microprocessors last decade. Improving Pipelining Effectiveness Adding Multiple Issue RISC processors refined notion compiler-schedul ed pipelines early 1980s. concepts delayed branches delayed loads—common microprogramming—were extended high-level architecture. fact, Stanford processor hat led commercial MIPS architecture called “Microprocessor without Interlocked Pipelined Stages” becaus e 711was assembler compiler avoid data hazards. addition contribution development RISC concepts, IBM pioneering work multiple issue. 1960s, project called ACS under-way. included multiple- instruction issue concepts notion integrated compile r architecture design, never reached product stage. earliest proposal superscalar processor dynamically makes issue decisions John Cocke; described key ideas several talks mid-1980s and, Tilak Agarwala, coined name superscalar . original design two-issue machine named Cheetah, followed widely discussed four-issue machine named America. IBM Power-1 architecture, used RS/6000 line, based ideas, PowerPC variation Power-1 architecture. Cocke Turing Award, highest award computer science engineering, architecture work. Static multiple issue, exemplified long instruction word (LIW) sometimes long instruction word (VLIW) approaches, appeared real designs superscalar approach. fact, earliest multiple-issue machines special-purpose att ached processors designed scientific applications. Culler Scie ntific Floating Point Systems two prominent manufacturers computers. Another inspiration use multiple operations per instruction came workin g microcode compilers. inspiration led research project Yale led Josh Fisher, coined term VLIW. Cydrome Multiflow two early companies involved building mini- supercomputers using processors multiple-issue capabil ity. processors, built bit-slice multiple-chip gate arr ay implementations, arrived market time initial RISC microprocessors. Despite promising perform ance high-end scientific codes, much better cost/performance microprocessor-based computers doomed first generat ion VLIW computers. Bob Rau Josh Fisher Eckert-Mauchly Award 2002 2003, respectively, contributions development multiple processors software techniques exploit ILP. beginning 1990s saw first superscalar processors using static scheduling speculation, includin g 712versions MIPS PowerPC architectures. early 1990s also saw important research number universities, including Wisconsin, Stanford, Illinois, Michigan, focused techniques exploiting additional ILP multiple issue without speculation. research insights used build dynamically scheduled, speculative processors, including Motorola 88110, MIPS R10000, DEC Alpha 21264, PowerPC 603, Intel Pentium Pro, Pentium III, Pentium 4. 2001, Intel introduced IA-64 architecture first implementation, Itanium. Itanium represented return compiler-intensive approach called EPIC. EPIC represented considerable enhancement early VLIW architectures, removing many drawbacks. modest sales. 2013, IA-64 architecture used low- volume, high-end servers outnumbered x86 processors 100:1. Compiler Technology Exploiting ILP Successful development processors exploit ILP depe nded progress compiler technology. concept loop unrol ling understood early, number companies researchers —including Floating Point Systems, Cray, Stanford MIPS project—developed compilers made use loop unrolling pipeline scheduling improve instruction throughput. spe cial- purpose processor called WARP, designed Carnegie Mellon University, inspired development software pipelining , approach symbolically unrolls loops. exploit higher levels ILP, aggressive compiler technology needed. VLIW project Yale developed concept trace scheduling Multi-flow implemented th eir compilers. Trace scheduling relies aggressive loop unrolli ng path prediction compile favored execution traces efficient ly. Cydrome designers created early versions predication support software pipelining. Hwu Illinois worked extended versions loop unrolling, called superblocks , techniques compiling predication. concepts Multiflow, Cydrome, research group Illinois served architectural compiler basis IA-64 architecture. 713Further Reading Bhandarkar, D. D. W. Clark [1991]. “Performance architecture: Comparing RISC CISC similar hardware organizations,” Proc. Fourth Conf. Architectural Support Programming Languages Operating Systems , IEEE/ACM (April), Palo Alto, CA, 310–19. quantitative comparison RISC CISC written scholars argued CISCs well built them; conclude MIPS 2 4 times faster VAX built similar technology, mean 2.7 . Fisher, J. A. B. R. Rau [1993]. Journal Supercomputing (January), Kluwer. entire issue devoted topic exploiting ILP. contains papers architecture software wonderful source references . Hennessy, J. L. D. A. Patterson [2001]. Computer Architecture: Quantitative Approach , fourth edition, Morgan Kaufmann, San Francisco. Chapter 2 Appendix go considerably detail pipelined processors (almost 200 pages), including superscalar processor VLIW processors. Appendix G describes Itanium . Jouppi, N. P. D. W. Wall [1989]. “Available instruction-level parallelism supersc2alar superpipelined processors,” Proc. Third Conf. Architectural Support Programming Languages Operating Systems , IEEE/ACM (April), Boston, 272–82. comparison deeply pipelined (also called superpipelined) superscalar systems . Kogge, P. M. [1981]. Architecture Pipelined Computers , McGraw-Hill, New York. formal text pipelined control, emphasis underlying principles . Russell, R. M. [1978]. "The CRAY-1 computer system", Comm. ACM 21:1 (January), 63–72. short summary classic computer uses vectors operation remove pipeline stalls . Smith, A. J. Lee [1984]. "Branch prediction strategies branch target buffer design", Computer 17:1 (January), 6–22. early survey branch prediction . 714Smith, J. E. A. R. Plezkun [1988]. "Implementing precise interrupts pipelined processors", IEEE Trans. Computers 37:5 (May), 562–73 Covers difficulties interrupting pipelined computers . Thornton, J. E. [1970]. Design Computer. Control Data 6600 , Glenview, IL: Scott, Foresman. classic book describing classic computer, considered first supercomputer . 4.17 Exercises 4.1 Consider following instruction: Instruction: rd, rs1, rs2 Interpretation: Reg[rd] = Reg[rs1] Reg[rs2] 4.1.1 [5] <§4.3>What values control signals generated control Figure 4.10 instruction? 4.1.2 [5] <§4.3>Which resources (blocks) perform useful function instruction? 4.1.3 [10] <§4.3>Which resources (blocks) produce output instruction? resources produce output used? 4.2 [10] <§4.4>Explain “don’t cares” Figure 4.18 . 4.3 Consider following instruction mix: 4.3.1 [5] <§4.4>What fraction instructions use data memory? 4.3.2 [5] <§4.4>What fraction instructions use instruction memory? 4.3.3 [5] <§4.4>What fraction instructions use sign extend? 4.3.4 [5] <§4.4>What sign extend cycles output needed? 4.4 silicon chips fabricated, defects materials (e.g., silicon) manufacturing errors result defective circu its. 715A common defect one signal wire get “broken” always register logical 0. often called “stuck-at-0” fault. 4.4.1 [5] <§4.4>Which instructions fail operate correctly MemToReg wire stuck 0? 4.4.2 [5] <§4.4>Which instructions fail operate correctly ALUSrc wire stuck 0? 4.5 exercise, examine detail instruction executed single-cycle datapath. Problems exercise refer clock cycle processor fetches follo wing instruction word: 0x00c6ba23 . 4.5.1 [10] <§4.4>What values ALU control unit’s inputs instruction? 4.5.2 [5] <§4.4>What new PC address instruction executed? Highlight path value determined. 4.5.3 [10] <§4.4> mux, show values inputs outputs execution instruction. List values register outputs Reg [xn] . 4.5.4 [10] <§4.4> input values ALU two add units? 4.5.5 [10] <§4.4> values inputs registers unit? 4.6 Section 4.4 discuss I-type instructions like addi andi . 4.6.1 [5] <§4.4> additional logic blocks, any, needed add I-type instructions CPU shown Figure 4.21 ? Add necessary logic blocks Figure 4.21 explain purpose. 4.6.2 [10] <§4.4> List values signals generated control unit addi . Explain reasoning “don’t care” control signals. 4.7 Problems exercise assume logic blocks used implement processor’s datapath following latencies: “Register read” time needed rising clock edge new register value appear output. value applies PC only. “Register setup” amount time register’s data input must stable 716before rising edge clock. value applies PC Register File. 4.7.1 [5] <§4.4> latency R-type instruction (i.e., long must clock period ensure instruction works correctly)? 4.7.2 [10] <§4.4> latency ld? (Check answer carefully. Many students place extra muxes critical path.) 4.7.3 [10] <§4.4> latency sd? (Check answer carefully. Many students place extra muxes critical path.) 4.7.4 [5] <§4.4> latency beq? 4.7.5 [5] <§4.4> latency I-type instruction? 4.7.6 [5] <§4.4> minimum clock period CPU? 4.8 [10] <§4.4> Suppose could build CPU clock cycle time different instruction. would speedup new CPU CPU presented Figure 4.21 given instruction mix below? 4.9 Consider addition multiplier CPU shown Figure 4.21 . addition add 300 ps latency ALU, reduce number instructions 5% (because longer need emulate multiply instruction). 4.9.1 [5] <§4.4> clock cycle time without improvement? 4.9.2 [10] <§4.4> speedup achieved adding improvement? 4.9.3 [10] <§4.4> slowest new ALU still result improved performance? 4.10 processor designers consider possible improvement processor datapath, decision usually depends 717cost/performance trade-off. following three problems, assume beginning datapath Figure 4.21 , latencies Exercise 4.7, following costs: Suppose doubling number general purpose registers 32 64 would reduce number ld sd instruction 12%, increase latency register file 150 ps 160 ps double cost 200 400. (Use instruction mix Exercise 4.8 ignore effects ISA discussed Exercise 2.18.) 4.10.1 [5] <§4.4>What speedup achieved adding improvement? 4.10.2 [10] <§4.4>Compare change performance change cost. 4.10.3 [10] <§4.4>Given cost/performance ratios calculated, describe situation makes sense add registers describe situation doesn’t make sense add registers. 4.11 Examine difficulty adding proposed lwi.d rd, rs1, rs2 (“Load Increment”) instruction RISC-V. Interpretation: Reg[rd]=Mem[Reg[rs1]+Reg[rs2]] 4.11.1 [5] <§4.4> new functional blocks (if any) need instruction? 4.11.2 [5] <§4.4> existing functional blocks (if any) require modification? 4.11.3 [5] <§4.4> new data paths (if any) need instruction? 4.11.4 [5] <§4.4> new signals need (if any) control unit support instruction? 4.12 Examine difficulty adding proposed swap rs1, rs2 instruction RISC-V. Interpretation: Reg[rs2]=Reg[rs1]; Reg[rs1]=Reg[rs2] 4.12.1 [5] <§4.4> new functional blocks (if any) need instruction? 4.12.2 [10] <§4.4> existing functional blocks (if any) require modification? 7184.12.3 [5] <§4.4> new data paths need (if any) support instruction? 4.12.4 [5] <§4.4> new signals need (if any) control unit support instruction? 4.12.5 [5] <§4.4> Modify Figure 4.21 demonstrate implementation new instruction. 4.13 Examine difficulty adding proposed ss rs1, rs2, imm (Store Sum) instruction RISC-V. Interpretation: Mem[Reg[rs1]]=Reg[rs2]+immediate 4.13.1 [10] <§4.4> new functional blocks (if any) need instruction? 4.13.2 [10] <§4.4> existing functional blocks (if any) require modification? 4.13.3 [5] <§4.4> new data paths need (if any) support instruction? 4.13.4 [5] <§4.4> new signals need (if any) control unit support instruction? 4.13.5 [5] <§4.4> Modify Figure 4.21 demonstrate implementation new instruction. 4.14 [5] <§4.4> instructions (if any) Imm Gen block critical path? 4.15 ld instruction longest latency CPU Section 4.4 . modified ld sd offset (i.e., address loaded from/stored must calculated placed rs1 calling ld/sd ), instruction would use ALU Data memory. would allow us reduce clock cycle time. However, would also increase number instructions, many ld sd instructions would need replaced ld/add sd/add combinations. 4.15.1 [5] <§4.4> would new clock cycle time be? 4.15.2 [10] <§4.4> Would program instruction mix presented Exercise 4.7 run faster slower new CPU? much? (For simplicity, assume every ld sd instruction replaced sequence two instructions.) 4.15.3 [5] <§4.4> primary factor influences whether program run faster slower new CPU? 4.15.4 [5] <§4.4> consider original CPU (as shown Figure 4.21 ) better overall design; consider new CPU better overall design? Why? 7194.16 exercise, examine pipelining affects clock cycle time processor. Problems exercise assume hat individual stages datapath following latencies: Also, assume instructions executed processor broken follows: 4.16.1 [5] <§4.5> clock cycle time pipelined non-pipelined processor? 4.16.2 [10] <§4.5> total latency ld instruction pipelined non-pipelined processor? 4.16.3 [10] <§4.5> split one stage pipelined datapath two new stages, half latency original stage, stage would split new clock cycle time processor? 4.16.4 [10] <§4.5> Assuming stalls hazards, utilization data memory? 4.16.5 [10] <§4.5> Assuming stalls hazards, utilization write-register port “Registers” uni t? 4.17 [10] <§4.5> minimum number cycles needed completely execute n instructions CPU k stage pipeline? Justify formula. 4.18 [5] <§4.5> Assume x11 initialized 11 x12 initialized 22. Suppose executed code version pipeline Section 4.5 handle data 720hazards (i.e., programmer responsible addressing data hazards inserting NOP instructions necessary). would final values registers x13 x14 be? addix11, x12, 5 addx13, x11, x12 addix14, x11, 15 4.19 [10] <§4.5> Assume x11 initialized 11 x12 initialized 22. Suppose executed code version pipeline Section 4.5 handle data hazards (i.e., programmer responsible addressing data hazards inserting NOP instructions necessary). would final values register x15 be? Assume register file written beginning cycle read end cycle. Therefore, ID stage return results WB state occurring cycle. See Section 4.7 Figure 4.51 details. addix11, x12, 5 addx13, x11, x12 addix14, x11, 15 addx15, x11, x11 4.20 [5] <§4.5> Add NOP instructions code run correctly pipeline handle data hazards. addix11, x12, 5 addx13, x11, x12 addix14, x11, 15 addx15, x13, x12 4.21 Consider version pipeline Section 4.5 handle data hazards (i.e., programmer responsible addressing data hazards inserting NOP instructions necessary). Suppose (after optimization) typical n- instruction program requires additional 4*n NOP instructions correctly handle data hazards. 4.21.1 [5] <§4.5> Suppose cycle time pipeline without forwarding 250 ps. Suppose also adding forwarding hardware reduce number NOPs .4*n .05*n , increase cycle time 300 ps. speedup new pipeline compared one without forwarding? 4.21.2 [10] <§4.5> Different programs require different amounts NOPs. many NOPs (as percentage code instructions) remain typical program 721program runs slower pipeline forwarding? 4.21.3 [10] <§4.5> Repeat 4.21.2; however, time let x represent number NOP instructions relative n. (In 4.21.2, x equal .4.) answer respect x. 4.21.4 [10] <§4.5> program .075*n NOPs possibly run faster pipeline forwarding? Explain not. 4.21.5 [10] <§4.5> minimum, many NOPs (as percentage code instructions) must program possibly run faster pipeline forwarding? 4.22 [5] <§4.5> Consider fragment RISC-V assembly below: sd x29, 12(x16) ld x29, 8(x16) sub x17, x15, x14 beqz x17, label add x15, x11, x14 sub x15, x30, x14 Suppose modify pipeline one memory (that handles instructions data). case, structural hazard every time program needs fetch instruction cycle another instruction accesses data. 4.22.1 [5] <§4.5> Draw pipeline diagram show code stall. 4.22.2 [5] <§4.5> general, possible reduce number stalls/ NOPs resulting structural hazard reordering code? 4.22.3 [5] <§4.5> Must structural hazard handled hardware? seen data hazards eliminated adding NOPs code. structural hazard? so, explain how. not, explain not. 4.22.4 [5] <§4.5> Approximately many stalls would expect structural hazard generate typical program? (Use instruction mix Exercise 4.8.) 4.23 change load/store instructions use register (without offset) address, instructions longer need us e ALU. (See Exercise 4.15.) result, MEM EX stages overlapped pipeline four stages. 4.23.1 [10] <§4.5> reduction pipeline depth affect cycle time? 7224.23.2 [5] <§4.5> might change improve performance pipeline? 4.23.3 [5] <§4.5> might change degrade performance pipeline? 4.24 [10] <§4.7> two pipeline diagrams better describes operation pipeline’s hazard detection unit? Why? Choice 1: ld x11, 0(x12): ID EX WB add x13, x11, x14: ID EX..ME WB x15, x16, x17: ID..EX WB Choice 2: ld x11, 0(x12): ID EX WB add x13, x11, x14: ID..EX WB x15, x16, x17: IF..ID EX WB 4.25 Consider following loop. LOOP: ldx10, 0(x13) ldx11, 8(x13) addx12, x10, x11 subix13, x13, 16 bnezx12, LOOP Assume perfect branch prediction used (no stalls due control hazards), delay slots, pipeline full forwarding support, branches resolved EX (as opposed ID) stage. 4.25.1 [10] <§4.7> Show pipeline execution diagram first two iterations loop. 4.25.2 [10] <§4.7> Mark pipeline stages perform useful work. often pipeline full cycle five pipeline stages useful work? (Begin cycle subi stage. End cycle bnez stage.) 4.26 exercise intended help understand cost/complexity/performance trade-offs forwarding pipelined processor. Problems exercise refer pipe lined datapaths Figure 4.53 . problems assume that, instructions executed processor, following fractio n instructions particular type RAW data dependence. type RAW data dependence identified stage produces result (EX MEM) next instruction 723consumes result (1st instruction follows one produces result, 2nd instruction follows, both). assume register write done first half cloc k cycle register reads done second half cycle, “EX 3rd” “MEM 3rd” dependences counted cannot result data hazards. also assume branches resolved EX stage (as opposed ID stage), CPI processor 1 data hazards. Assume following latencies individual pipeline stages. EX stage, latencies given separately processor without forwarding processor different kinds forwarding. 4.26.1 [5] <§4.7> RAW dependency listed above, give sequence least three assembly statements exhibits dependency. 4.26.2 [5] <§4.7> RAW dependency above, many NOPs would need inserted allow code 4.26.1 run correctly pipeline forwarding hazard detection? Show NOPs could inserted. 4.26.3 [10] <§4.7> Analyzing instruction independently over-count number NOPs needed run program pipeline forwarding hazard detection. Write sequence three assembly instructions that, consider instruction sequence independently, sum stalls larger number stalls sequence actually needs avoid data hazards. 4.26.4 [5] <§4.7> Assuming hazards, CPI program described table run pipeline forwarding? percent cycles stalls? (For simplicity, assume necessary cases listed 724and treated independently.) 4.26.5 [5] <§4.7> CPI use full forwarding (forward results forwarded)? percent cycles stalls? 4.26.6 [10] <§4.7> Let us assume cannot afford three-input multiplexors needed full forwarding. decide better forward EX/MEM pipeline register (next-cycle forwarding) fr om MEM/WB pipeline register (two-cycle forwarding). CPI option? 4.26.7 [5] <§4.7> given hazard probabilities pipeline stage latencies, speedup achieved type forwarding (EX/MEM, MEM/WB, full) compared pipeline forwarding? 4.26.8 [5] <§4.7> would additional speedup (relative fastest processor 4.26.7) added “time- travel” forwarding eliminates data hazards? Assume yet-to-be-invented time-travel circuitry adds 100 ps latency full-forwarding EX stage. 4.26.9 [5] <§4.7> table hazard types separate entries “EX 1st” “EX 1st EX 2nd”. entry “MEM 1st MEM 2nd”? 4.27 Problems exercise refer following sequence instructions, assume executed five-stage pipelined datapath: add x15, x12, x11 ld x13, 4(x15) ld x12, 0(x2) x13, x15, x13 sd x13, 0(x15) 4.27.1 [5] <§4.7> forwarding hazard detection, insert NOPs ensure correct execution. 4.27.2 [10] <§4.7> Now, change and/or rearrange code minimize number NOPs needed. assume register x17 used hold temporary values modified code. 4.27.3 [10] <§4.7> processor forwarding, forgot implement hazard detection unit, happens original code executes? 4.27.4 [20] <§4.7> forwarding, first seven cycles 725during execution code, specify signals asserted cycle hazard detection forwarding units Figure 4.59 . 4.27.5 [10] <§4.7> forwarding, new input output signals need hazard detection unit Figure 4.59? Using instruction sequence example, explain signal needed. 4.27.6 [20] <§4.7> new hazard detection unit 4.26.5, specify output signals asserts first five cycles execution code. 4.28 importance good branch predictor depends often conditional branches executed. Together branch predictor accuracy, determine much time spent stalling due mispredicted branches. exercise, assume breakdown dynamic instructions various instruction categories follows: Also, assume following branch predictor accuracies: Always-Taken Always-Not-Taken 2-Bit 45% 55% 85% 4.28.1 [10] <§4.8> Stall cycles due mispredicted branches increase CPI. extra CPI due mispredicted branches always-taken predictor? Assume branch outcomes determined ID stage applied EX stage data hazards, delay slots used. 4.28.2 [10] <§4.8> Repeat 4.28.1 “always-not-taken” predictor. 4.28.3 [10] <§4.8> Repeat 4.28.1 2-bit predictor. 4.28.4 [10] <§4.8> 2-bit predictor, speedup would 726be achieved could convert half branch instructions ALU instruction? Assume correctly incorrectly predicted instructions chance replaced. 4.28.5 [10] <§4.8> 2-bit predictor, speedup would achieved could convert half branch instructions way replaced branch instruction two ALU instructions? Assume correctly incorrectly predict ed instructions chance replaced. 4.28.6 [10] <§4.8> branch instructions much predictable others. know 80% executed branch instructions easy-to-predict loop-back branches always predicted correctly, accuracy 2-bit predictor remaining 20% branch instructions? 4.29 exercise examines accuracy various branch predictors following repeating pattern (e.g., loop) branch outcomes: T, NT, T, T, NT. 4.29.1 [5] <§4.8> accuracy always-taken always-not-taken predictors sequence branch outcomes? 4.29.2 [5] <§4.8> accuracy 2-bit predictor first four branches pattern, assuming predictor starts bottom left state Figure 4.61 (predict taken)? 4.29.3 [10] <§4.8> accuracy 2-bit predictor pattern repeated forever? 4.29.4 [30] <§4.8> Design predictor would achieve perfect accuracy pattern repeated forever. predictor sequential circuit one output provides prediction (1 taken, 0 taken) inputs clock control signal indicates instruction conditional branch. 4.29.5 [10] <§4.8> accuracy predictor 4.29.4 given repeating pattern exact opposite one? 4.29.6 [20] <§4.8> Repeat 4.29.4, predictor able eventually (after warm-up period make wrong predictions) start perfectly predicting pattern opposite. predictor input 727that tells real outcome was. Hint: input lets predictor determine two repeating patterns given. 4.30 exercise explores exception handling affects pipelin e design. first three problems exercise refer following two instructions: Instruction 1 Instruction 2 beqz x11, LABEL ld x11, 0(x12) 4.30.1 [5] <§4.9> exceptions instructions trigger? exceptions, specify pipeline st age detected. 4.30.2 [10] <§4.9> separate handler address exception, show pipeline organization must changed able handle exception. assume addresses handlers known processor designed. 4.30.3 [10] <§4.9> second instruction fetched immediately first instruction, describe happens pipelin e first instruction causes first exception list ed Exercise 4.30.1. Show pipeline execution diagram time first instruction fetched time fi rst instruction exception handler completed. 4.30.4 [20] <§4.9> vectored exception handling, table exception handler addresses data memory known (fixed) address. Change pipeline implement exception handling mechanism. Repeat Exercise 4.30.3 using modified pipeline vectored exception handling. 4.30.5 [15] <§4.9> want emulate vectored exception handling (described Exercise 4.30.4) machine one fixed handler address. Write code fixed address. Hint: code identify exception, get right address exception vector table , transfer execution handler. 4.31 exercise compare performance 1-issue 2- issue processors, taking account program transformations made optimize 2-issue execution. Problems exercise refer following loop (written C): for(i=0;i!=j;i+=2) 728 b[i]=a[i]–a[i+1]; compiler little optimization might produce following RISC-V assembly code: li x12, 0 jalENT TOP: slli x5, x12, 3 add x6, x10, x5 ld x7, 0(x6) ld x29, 8(x6) sub x30, x7, x29 add x31, x11, x5 sd x30, 0(x31) addi x12, x12, 2 ENT: bnex12, x13, TOP code uses following registers: Assume two-issue, statically scheduled processor exercise following properties: 1. One instruction must memory operation; must arithmetic/logic instruction branch. 2. processor possible forwarding paths stages (including paths ID stage branch resolution). 3. processor perfect branch prediction. 4. Two instruction may issue together packet one depends other. (See page 324.) 5. stall necessary, instructions issue packet must stall. (See page 324.) complete exercises, notice much effort goes generating code produce near-optimal speedup. 4.31.1 [30] <§4.10> Draw pipeline diagram showing RISC-V code given executes two-issue processor. Assume 729that loop exits two iterations. 4.31.2 [10] <§4.10> speedup going one-issue two-issue processor? (Assume loop runs thousands iterations.) 4.31.3 [10] <§4.10> Rearrange/rewrite RISC-V code given achieve better performance one-issue processor. Hint: Use instruction “ beqz x13,DONE ” skip loop entirely j = 0 . 4.31.4 [20] <§4.10> Rearrange/rewrite RISC-V code given achieve better performance two-issue processor. (Do unroll loop, however.) 4.31.5 [30] <§4.10> Repeat Exercise 4.31.1, time use optimized code Exercise 4.31.4. 4.31.6 [10] <§4.10> speedup going one-issue processor two-issue processor running optimized code Exercises 4.31.3 4.31.4. 4.31.7 [10] <§4.10> Unroll RISC-V code Exercise 4.31.3 iteration unrolled loop handles two iterations original loop. Then, rearrange/rewrite unrolled code achieve better performance one-issue processor. may assume j multiple 4. 4.31.8 [20] <§4.10> Unroll RISC-V code Exercise 4.31.4 iteration unrolled loop handles two iterations original loop. Then, rearrange/rewrite unrolled code achieve better performance two-issue processor. may assume j multiple 4. (Hint: Re-organize loop calculations appear outside loop end loop. may assume values temporary registers needed loop.) 4.31.9 [10] <§4.10> speedup going one-issue processor two-issue processor running unrolled, optimized code Exercises 4.31.7 4.31.8? 4.31.10 [30] <§4.10> Repeat Exercises 4.31.8 4.31.9, time assume two-issue processor run two arithmetic/logic instructions together. (In words, first instruction packet type instruction, second must arithmetic logic instruction. Two memory operations cannot scheduled time.) 4.32 exercise explores energy efficiency relationship 730with performance. Problems exercise assume following energy consumption activity Instruction memory, Registers, Data memory. assume components datapath consume negligible amount energy. (“Register Read” “Register Write” refer register file only.) Assume components datapath following latencies. assume components datapath negligible latencies. 4.32.1 [5] <§§4.3, 4.6, 4.14> much energy spent execute add instruction single-cycle design five-stage pipelined design? 4.32.2 [10] <§§4.6, 4.14> worst-case RISC-V instruction terms energy consumption? energy spent execute it? 4.32.3 [10] <§§4.6, 4.14> energy reduction paramount, would change pipelined design? percentage reduction energy spent ld instruction change? 4.32.4 [10] <§§4.6, 4.14> instructions potentially benefit change discussed Exercise 4.32.3? 4.32.5 [10] <§§4.6, 4.14> changes Exercise 4.32.3 affect performance pipelined CPU? 4.32.6 [10] <§§4.6, 4.14> eliminate MemRead control signal data memory read every cycle, i.e., permanently MemRead=1 . Explain processor still functions correctly change. 25% 731instructions loads, effect change clock frequency energy consumption? 4.33 silicon chips fabricated, defects materials (e.g., silicon) manufacturing errors result defective circuits. common defect one wire affect signal another. called “cross-talk fault”. special class cross-talk faults signal connected wire constant logical value (e.g., power supply wire). faults, affected signal always logical value either 0 1 called “stuck-at-0” “stuck-at-1” faults. following problems refer bit 0 Write Register input register file Figure 4.21 . 4.33.1 [10] <§§4.3, 4.4> Let us assume processor testing done (1) filling PC, registers, data instruction memories values (you choose values), (2) letting single instruction execute, (3) reading PC, memories, registers. values examined determine particular fault present. design test (values PC, memories, registers) would determine stuck-at-0 fault signal? 4.33.2 [10] <§§4.3, 4.4> Repeat Exercise 4.33.1 stuck-at-1 fault. use single test stuck-at-0 stuck- at-1? yes, explain how; no, explain not. 4.33.3 [10] <§§4.3, 4.4> know processor stuck-at-1 fault signal, processor still usable? usable, must able convert program executes normal RISC-V processor program works processor. assume enough free instruction memory data memory let make program longer store additional data. 4.33.4 [10] <§§4.3, 4.4> Repeat Exercise 4.33.1; fault test whether MemRead control signal becomes 0 branch control signal 0, fault otherwise. 4.33.5 [10] <§§4.3, 4.4> Repeat Exercise 4.33.1; fault test whether MemRead control signal becomes 1 RegRd control signal 1, fault otherwise. Hint: problem requires knowledge operating systems. Consider causes segmentation faults. 732 Answers Check §4.1, page 240: 3 5: Control, Datapath, Memory. Input Output missing. §4.2, page 243: false. Edge-triggered state elements make simultaneous reading writing possible unambiguous. §4.3, page 250: I. a. II. c. §4.4, page 262: Yes, Branch ALUOp0 identical. addition, use flexibility don’t care bits combine signals together. ALUSrc MemtoReg made setting two don’t care bits MemtoReg 1 0. ALUOp1 MemtoReg made inverses one another setting don’t care bit MemtoReg 1. don’t need inverter; simply use signal flip order inputs MemtoReg multiplexor! §4.5, page 275: 1. Stall due load-use data hazard ld result. 2. Avoid stalling third instruction read-aft er- write data hazard x11 forwarding add result. 3. need stall, even without forwarding. §4.6, page 288: Statements 2 4 correct; rest incorrect. §4.8, page 314: 1. Predict taken. 2. Predict taken. 3. Dynamic prediction. §4.9, page 321: first instruction, since logically executed others. §4.10, page 334: 1. Both. 2. Both. 3. Software. 4. Hardware. 5. Hardware. 6. Hardware. 7. Both. 8. Hardware. 9. Both. §4.12, page 344: First two false last two true. 7335 Large Fast Exploiting Memory Hierarchy Abstract chapter describes memory hierarchy exploits locality. shows principle locality gives designers way come long latency memory access. explains strategies ex ploit locality used levels memory hierarchy. chapter concludes emphasizing memory systems central design issue parallel processors, growing importance memory hierarchy determining system performance means thi important area continue focus designers researchers years come. Keywords cache performance; virtual memory; memory hierarchy; ARM Cortex-A53; Intel Co i7; Intel; temporal locality; spatial locality; memory hierarchy; block; line; hit rate; miss rate; hit time; miss penalty; cache coherence; snooping protocol; direct-mapped cache; tag; valid bit; cache miss; write-through; write buffer; write-back; split cache; fully associative cache; set-associative cache; least recently used; LR U; multilevel cache; global miss rate; local miss rate; virtual memory; virtual machin e; physical address; protection; page fault; virtual address; address translat ion; segmentation; page table; swap space; reference bit; use bit; translation-l ookaside buffer; TLB; virtually addressed cache; aliasing; physically addressed cache; supervi sor mode; kernel mode; system call; context switch; exception enable; restarta ble instruction; handler; unmapped; three Cs model; compulsory miss; cold-start miss; cap acity miss; conflict miss; collision miss; finite-state machine; next-state f unction; false sharing; nonblocking cache; prefetching; dependability; relia bility; availability; disk storage; flash storage; I/O device; I/O; operating system; file system; array; redundant array; networks; matrix multiply; common case fast 734Ideally one would desire indefinitely large memory capacity particular … word would immediately available. … … forced recognize possibility constructing hierarchy memories, greater capacity preceding less quickly accessible. A. W. Burks, H. H. Goldstine, J. von Neumann, Prelim inary Discussion Logical Design Electronic Computing Instrum ent, 1946 OUTLINE 5.1 Introduction 366 5.2 Memory Technologies 370 5.3 Basics Caches 375 5.4 Measuring Improving Cache Performance 390 5.5 Dependable Memory Hierarchy 410 5.6 Virtual Machines 416 5.7 Virtual Memory 419 5.8 Common Framework Memory Hierarchy 443 5.9 Using Finite-State Machine Control Simple Cache 449 5.10 Parallelism Memory Hierarchy: Cache Coherence 454 5.11 Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks 458 5.12 Advanced Material: Implementing Cache Controllers 459 5.13 Real Stuff: ARM Cortex-A53 Intel Core 7 Memory Hierarchies 459 5.14 Real Stuff: Rest RISC-V System Spe cial Instructions 464 5.15 Going Faster: Cache Blocking Matrix Multiply 465 5.16 Fallacies Pitfalls 468 5.17 Concluding Remarks 472 735 5.18 Historical Perspective Reading 473 5.19 Exercises 473 736The Five Classic Components Computer 5.1 Introduction earliest days computing, programmers wanted unlimited amounts fast memory. topics chapter aid programmers creating illusion. look creating illusion, let’s consider simple analogy illustrates key principles mechanisms use. Suppose student writing term paper important historical developments computer hardware. sitting desk library collection books pulled shelves examining. find several important computers need write described books have, nothing EDSAC. Therefore, go back shelves look additional book. find book early British computers covers 737EDSAC. good selection books desk front you, high probability many topics need found them, may spend time using books desk without returning shelves . several books desk front saves time compared one book constantly go back shelves return take another. principle allows us create illusion large memory access fast small memory. need access books library equal probability, program access code data equal probability. Otherwise, would impossible make memory accesses fast still large memory computers, would impossible fit library books desk still find wanted quickly. principle locality underlies way work library way programs operate. principle locality states programs access relatively small portion address space instant time, accessed small portion library’s collection. two different types locality: Temporal locality (locality time): item referenced, tend referenced soon. recently brought book desk look at, probably need look soon. Spatial locality (locality space): item referenced, items whose addresses close tend referenced soon. example, brought book early English computers learn EDSAC, also noticed another book shelved next early mechanical computers, likewise brought back book and, later on, found something useful book. Libraries put books topic together shelves increase spatial locality. We’ll see memory hierarchies use spatial locality little later chapter. temporal locality locality principle stating data location referenced 738then tend referenced soon. spatial locality locality principle stating data location referenced, data locations nearby addresses tend referenced soon. accesses books desk naturally exhibit locality, locality programs arises simple natural program structures. example, programs contain loops, instructions data likely accessed repeatedly, showing large temporal locality. Since instructions normally accessed sequentially, programs also show high spatial locality. Accesses data also exhibit natural spatial locality. example, sequential accesses elements array record naturally high degrees spatial locality. take advantage principle locality implementing memory computer memory hierarchy . memory hierarchy consists multiple levels memory differe nt speeds sizes. faster memories expensive per bit slower memories thus smaller. memory hierarchy structure uses multiple levels memories; distanc e processor increases, size memories access time increase. Figure 5.1 shows faster memory close processor slower, less expensive memory it. goal pres ent user much memory available cheapest technology, providing access speed offered fastest memory. 739FIGURE 5.1 basic structure memory hierarchy. implementing memory system hierarchy, user illusion memory large largest level hierarchy, accessed built fastest memory. Flash memory replaced disks many personal mobile devices, may lead new level storage hierarchy desktop server computers; see Section 5.2 . data similarly hierarchical: level closer processor generally subset level away, data stored lowest level. analogy, books desk form subset library working in, turn subset libraries campus. Furthermore, move away processor, levels take progressively longer access, j ust might encounter hierarchy campus libraries. memory hierarchy consist multiple levels, data copied two adjacent levels time, focus attention two levels. upper level—the one closer processor—is smaller faster lower level, since upper level uses technology expensive. Figure 5.2 shows minimum unit information either present present two-level hierarchy called block line; library analogy, block information one book. 740 block (or line) minimum unit information either present present cache. FIGURE 5.2 Every pair levels memory hierarchy thought upper lower level. Within level, unit information pre sent called block line. Usually transfer entire block copy something levels. data requested processor appear block upper level, called hit (analogous finding 741information one books desk). data found upper level, request called miss. lower level hierarchy accessed retrieve block containing requested data. (Continuing analogy, go desk shelves find desired book.) hit rate , hit ratio , fraction memory accesses found upper level; often used measure performance memory hierarchy. miss rate (1−hit rate) fraction memory accesses found upper level. hit rate fraction memory accesses found level memory hierarchy. miss rate fraction memory accesses found level memory hierarchy. Since performance major reason memory hierarchy, time service hits misses important. Hit time time access upper level memory hierarchy, whi ch includes time needed determine whether access hit miss (that is, time needed look books desk). miss penalty time replace block upper level corresponding block lower level, plus time deliver block processor (or time get another book shelves place desk). upper level smaller built using faster memory parts, hit time much smaller time access next level hierarchy, major component miss penalty. (The time examine books desk much smaller time get get new book shelves.) hit time time required access level memory hierarchy, including time needed determine whether access h miss. 742 miss penalty time required fetch block level memory hierarchy lower level, including time access block, transmit one level other, insert leve l experienced miss, pass block requesto r. see chapter, concepts used build memory systems affect many aspects computer, including operating system manages memory I/O, compilers generate code, even applications use computer. course, programs spend much time accessing memory, memory system necessarily major factor determining performance. reliance memory hierarchies achieve performance meant programmers, used able think memory flat, random access storage device, need understand memory hierarchy get good performance. show important understanding later examples, Figure 5.18 page 400, Section 5.14 , shows double matrix multiply performance. Since memory systems critical performance, computer designers devote great deal attention systems develop sophisticated mechanisms improving performan ce memory system. chapter, discuss major conceptual ideas, although use many simplifications abstractions keep material manageable length complexity. BIG Picture Programs exhibit temporal locality, tendency reuse recently accessed data items, spatial locality, tendency reference data items close recently accessed ite ms. Memory hierarchies take advantage temporal locality keeping recently accessed data items closer process or. Memory hierarchies take advantage spatial locality moving blocks consisting multiple contiguous words memory upper levels hierarchy. Figure 5.3 shows memory hierarchy uses smaller faster memory technologies close processor. Thus, access es 743that hit highest level hierarchy processed quickly. Accesses miss go lower levels hierarchy, larger slower. hit rate high enough, memory hierarchy effective access time close highest (and fastest) level size equal lowest (and largest) level. FIGURE 5.3 diagram shows structure memory hierarchy: distance processor increases, size. structure, appropriate operating mechanisms, allows processor access time determined primarily level 1 hierarchy yet memory large level n. Maintaining illusion subject chapter. Although local disk normally bottom hierarchy, systems use tape file server local area network next levels hierarchy. systems, memory true hierarchy, meaning data cannot present level unless also present level + 1. 744 Check following statements generally true? 1. Memory hierarchies take advantage temporal locality. 2. read, value returned depends blocks cache. 3. cost memory hierarchy highest level. 4. capacity memory hierarchy lowest level. 5.2 Memory Technologies four primary technologies used today memory hierarchies. Main memory implemented DRAM ( dynamic random access memory ), levels closer processor (caches) use SRAM ( static random access memory ). DRAM less costly per bit SRAM, although substantially slower. price difference arises DRAM uses significantly less area per bit memory, DRAMs thus larger capacity amount silicon; speed difference arises several factors describ ed Section A.9 Appendix . third technology flash memory. nonvolatile memory secondary memory Personal Mobile Devices. fourth technology, used implement largest slowest level hierarchy servers, magnetic disk. access time price per bit vary widely among technologies, table shows, using typical values 2012. Memory technology Typical access time $ per GiB 2012 SRAM semiconductor memory 0.5–2.5 ns $500–$1000 DRAM semiconductor memory 50–70 ns $10–$20 Flash semiconductor memory 5,000–50,000 ns $0.75–$1.00 Magnetic disk 5,000,000–20,000,000 ns $0.05–$0.10 describe memory technology remainder section. SRAM Technology SRAMs simply integrated circuits memory arrays (usually) single access port provide either read write. SRAMs fixed access time datum, though 745read write access times may differ. SRAMs don’t need refresh access time close cycle time. SRAMs typically use six eight transistors per bit prevent information disturbed read. SRAM needs minimal power retain charge standby mode. past, PCs server systems used separate SRAM chips either primary, secondary, even tertiary caches . Today, thanks Moore’s Law , levels caches integrated onto processor chip, market independent SRAM chip nearly evaporated. DRAM Technology SRAM, long power applied, value kept indefinitely. dynamic RAM (DRAM), value kept cell stored charge capacitor. single transistor used access stored charge, either read value overwrite charge stored there. DRAMs use one transistor per bit storage, much denser cheaper per bit SRAM. DRAMs store charge capacitor, cannot kept indefinitely must periodically refreshed. memory structure called dynamic, contrast static stor age 746in SRAM cell. refresh cell, merely read contents write back. charge kept several milliseconds. every bit read DRAM written back individually, would constantly refreshing DRAM, leaving time accessing it. Fortunately, DRAMs use two-level decoding structure, allows us refresh entire row (which shares word line) read cycle followed immediately write cycle . Figure 5.4 shows internal organization DRAM, Figure 5.5 shows density, cost, access time DRAMs changed years. FIGURE 5.4 Internal organization DRAM. Modern DRAMs organized banks, typically four DDR3. bank consists series rows. Sending PRE (precharge) command opens closes bank. row address sent Act (activate), causes row transfer buffer. row buffer, transferred successive column addresses whatever width DRAM (typically 4, 8, 16 bits DDR3) specifying block transfer starting address. command, well block transfers, synchronized clock. 747FIGURE 5.5 DRAM size increased multiples four approximately every 3 years 1996, thereafter considerably slower. improvements access time slower continuous, cost roughly tracks density improvements, although cost often affected issues, availability demand. cost per gibibyte adjusted inflation. row organization helps refresh also helps performance. improve performance, DRAMs buffer rows repeated access. buffer acts like SRAM; changing address, random bits accessed buffer next row access. capability improves access time significantl y, since access time bits row much lower. Making chip wider also improves memory bandwidth chip. row buffer, transferred successive addresses whatever width DRAM (typically 4, 8, 16 bits), specifying block transfer starting address within buffer. improve interface processors further, DRAMs added clocks properly called synchronous DRAMs SDRAMs. advantage SDRAMs use clock eliminates time memory processor synchronize. speed advantage synchronous DRAMs comes ability transfer bits burst without specify additio nal address bits. Instead, clock transfers successive bits burst. fastest version called Double Data Rate (DDR) SDRAM. 748The name means data transfers rising falling edge clock, thereby getting twice much bandwidth might expect based clock rate data width. latest version technology called DDR4. DDR4-3200 DRAM 3200 million transfers per second, means 1600- MHz clock. Sustaining much bandwidth requires clever organization inside DRAM. Instead faster row buffer, DRAM internally organized read write multiple banks , row buffer. Sending address several banks permits read write simultaneously. example, four banks, one access time accesses rotate four banks supply four times bandwidth. rotating access scheme called address interleaving . Although personal mobile devices like iPad (see Chapter 1 ) use individual DRAMs, memory servers commonly sold small boards called dual inline memory modules (DIMMs). DIMMs typically contain 4–16 DRAMs, normally organized 8 bytes wide server systems. DIMM using DDR4-3200 SDRAMs could transfer 8 ×3200 =25,600 megabytes per second. DIMMs named bandwidth: PC25600. Since DIMM many DRAM chips portion used particular transfer, need term refer subset chips DIMM share common address lines. avoid confusion internal DRAM names row banks, use term memory rank subset chips DIMM. Elaboration One way measure performance memory system behind caches Stream benchmark [ McCalpin, 1995 ]. measures performance long vector operations. temporal locality access arrays larger cache computer tested. Flash Memory Flash memory type electrically erasable programmable read-only memory (EEPROM). Unlike disks DRAM, like EEPROM technologies, 749writes wear flash memory bits. cope limits, flash products include controller spread writes remapping blocks written many times less trodden blocks. technique called wear leveling . wear leveling, personal mobile devices unlikely exceed th e write limits flash. wear leveling lowers potential performance flash, needed unless higher-level softw monitors block wear. Flash controllers perform wear levelin g also improve yield mapping memory cells manufactured incorrectly. Disk Memory Figure 5.6 shows, magnetic hard disk consists collection platters, rotate spindle 5400 15,000 revolutions per minute. metal platters covered magnetic recording material sides, similar material found cassette videotape. read write information hard disk, movable arm containing small electromagnetic coil called read-write head located surface. entire drive permanently sealed control environment inside drive, which, tu rn, allows disk heads much closer drive surface. 750FIGURE 5.6 disk showing 10 disk platters read/write heads. diameter today’s disks 2.5 3.5 inches, typically one two platters per drive today. disk surface divided concentric circles, called tracks . typically tens thousands tracks per surface. track turn divided sectors contain information; track may thousands sectors. Sectors typically 512 751to 4096 bytes size. sequence recorded magnetic media sector number, gap, information sector including error correction code (see Section 5.5 ), gap, sector number next sector, on. track One thousands concentric circles make surface magnetic disk. sector One segments make track magnetic disk; sector smallest amount information read writte n disk. disk heads surface connected together move conjunction, every head track every surface. term cylinder used refer tracks heads given point surfaces. access data, operating system must direct disk three-stage process. first step position head proper track. operation called seek , time move head desired track called seek time . seek process positioning read/write head proper trac k disk. Disk manufacturers report minimum seek time, maximum seek time, average seek time manuals. first two easy measure, average open wide interpretation depends seek distance. industry calculates average seek time sum time possible seeks divided number possible seeks. Average seek times usually advertised 3 ms 13 ms, but, depending application scheduling disk requests, actual average seek time may 25% 33% advertised number locality disk references. locality arises successi 752accesses file operating system tries schedule accesses together. head reached correct track, must wait desired sector rotate read/write head. time called rotational latency rotational delay . average latency desired information halfway around disk. Disks rotate 5400 RPM 15,000 RPM. average rotational latency 5400 RPM rotational latency Also called rotational delay . time required desired sector disk rotate read/write head; usually assumed half rotation time. last component disk access, transfer time , time transfer block bits. transfer time function secto r size, rotation speed, recording density track. Transfer rates 2012 100 200 MB/sec. One complication disk controllers built-in cache stores sectors passed over; transfer rates cache typically higher, 750 MB/sec (6 Gbit/sec) 2012. Alas, block numbers located longer intuitive. assumptions sector-track-cylinder model nearby blocks track, blocks cylinder take less time access since seek time, tracks closer others. reason change raising level disk interfaces. speed-up sequential transfe rs, higher-level interfaces organize disks like tapes th like random access devices. logical blocks ordered serpentine fashion across single surface, trying capture sectors recorded bit density try get best performance. Hence, sequential blocks may different tracks. 753In summary, two primary differences magnetic disks semiconductor memory technologies disks slower access time mechanical devices—flash 1000 times fast DRAM 100,000 times fast—yet cheaper per bit high storage capacity modest cost—disks 10 100 times cheaper. Magnetic disks nonvolatile like flash, unlike flash write wear-ou problem. However, flash much rugged hence better match jostling inherent personal mobile devices. 5.3 Basics Caches Cache: safe place hiding storing things. Webster’s New World Dictionary American Language, Third College Edition, 1988 library example, desk acted cache—a safe place store things (books) needed examine. Cache name chosen represent level memory hierarchy processor main memory first commercial computer extra level. memories datapath Chapter 4 simply replaced caches. Today, although remains dominant use word cache , term also used refer storage managed take advantage locality access. Caches first appeared research computers early 1960s production computers later decade; every general- purpose computer built servers low-power embedd ed processors, includes caches. section, begin looking simple cache processor requests one word, blocks also consist single word. (Readers already familiar cache basics may want skip Section 5.4 .) Figure 5.7 shows simple cache, requesting data item initially cache. request, cache contains collection recent references X1, X2, …, Xn−1, processor requests word Xn cache. request results miss, word Xn brought memory cache. 754FIGURE 5.7 cache reference word Xn initially cache. reference causes miss forces cache fetch Xn memory insert cache. looking scenario Figure 5.7 , two questions answer: know data item cache? Moreover, is, find it? answers related. word go exactly one place cache, straightforward find word cache. simplest way assign location cache word memory assign cache location based address word memory. cache structure called direct mapped , since memory location mapped directly exactly one location cache. typical mapping addresses cache locations direct- mapped cache usually simple. example, almost direct- mapped caches use mapping find block: direct-mapped cache cache structure memory location mapped 755exactly one location cache. number entries cache power 2, modulo computed simply using low-order log2 (cache size blocks) bits address. Thus, 8-block cache uses three lowest bits (8=23) block address. example, Figure 5.8 shows memory addresses 1ten (00001two) 29ten (11101two) map locations 1ten (001two) 5ten (101two) direct- mapped cache eight words. FIGURE 5.8 direct-mapped cache eight entries showing addresses memory words 0 31 map cache locations. eight words cache, address X maps direct-mapped cache word X modulo 8. is, low-order log2(8)=3 bits used cache index. Thus, addresses 00001two, 01001two, 10001two, 11001two map entry 001two 756of cache, addresses 00101two, 01101two, 10101two, 11101two map entry 101two cache. cache location contain contents number different memory locations, know whether data cache corresponds requested word? is, know whether requested word cache not? answer question adding set tags cache. tags contain address information required identify whether word cache corresponds requested word. tag needs contain upper portion address, corresponding b used index cache. example, Figure 5.8 need upper two five address bits tag, since lower 3-bit index field address selects b lock. Architects omit index bits redundant, since b definition, index field address cache block must block number. tag field table used memory hierarchy contains address information required identify whether associate block hierarchy corresponds requested word. also need way recognize cache block valid information. instance, processor starts up, cache good data, tag fields meaningless. Even executing many instructions, th e cache entries may still empty, Figure 5.7 . Thus, need know tag ignored entries. common method add valid bit indicate whether entry contains valid address. bit set, cannot match block. valid bit field tables memory hierarchy indicates associated block hierarchy contains valid data. 757For rest section, focus explaining cache deals reads. general, handling reads little simpler handling writes, since reads change contents cache. seeing basics reads work cache misses handled, we’ll examine cache designs real computers detail caches handle writes. BIG Picture Caching perhaps important example big idea prediction . relies principle locality try find desired data higher levels memory hierarchy, provides mechanisms ensure prediction wron g finds uses proper data lower levels memory hierarchy. hit rates cache prediction modern computers often 95% (see Figure 5.46 ). Accessing Cache 758Below sequence nine memory references empty eight- block cache, including action reference. Figure 5.9 shows contents cache change miss. Since eight blocks cache, low-order 3 bits address give block number: 759FIGURE 5.9 cache contents shown reference request misses, index tag fields shown binary sequence addresses page 379. cache initially empty, valid bits (V entry cache) turned (N). processor requests following addresses: 10110two (miss), 11010two (miss), 10110two (hit), 11010two (hit), 10000two (miss), 00011two (miss), 10000two (hit), 10010two (miss), 10000two (hit). figures show cache contents miss sequence handled. address 10010two (18) referenced, entry address 11010two (26) must replaced, reference 11010two cause subsequent miss. tag field contain upper portion address. full address word contained cache block tag field j cache j×8+i, equivalently concatenation tag field j index i. example, cache f above, index 010two 760has tag 10two corresponds address 10010two. Since cache empty, several first references misse s; caption Figure 5.9 describes actions memory reference. eighth reference conflicting demands block. word address 18 (10010two) brought cache block 2 (010two). Hence, must replace word address 26 (11010two), already cache block 2 (010two). behavior allows cache take advantage temporal locality: recently referenced words replace less recently referenced words. situation directly analogous needing book shelves space desk—some book already desk must returned shelves. direct- mapped cache, one place put newly requested item hence one choice replace. know look cache possible address: low-order bits address used find unique cache entry address could map. Figure 5.10 shows referenced address divided tag field , used compare value tag field cache cache index , used select block 761FIGURE 5.10 cache, lower portion address used select cache entry consisting data word tag. cache holds 1024 words 4 KiB. Unless noted otherwise, assume 64-bit addresses chapter. tag cache compared upper portion address determine whether entry cache corresponds requested address. cache 210 (or 1024) words block size one word, 10 bits used index cache, leaving 64 −10 −2 =52 bits compared tag. tag upper 52 bits address equal valid bit on, request hits cache, word supplied processor. Otherwise, miss occurs. index cache block, together tag contents 762block, uniquely specifies memory address word contained cache block. index field used address reference cache, n-bit field 2n values, total number entries direct-mapped cache must power 2. Since words aligned multiples four bytes, least significant two bits every address specify byte wit hin word. Hence, words aligned memory, least significant two bits ignored selecting word block. chapter, we’ll assume data aligned memory, discuss handle unaligned cache accesses Elaboration. total number bits needed cache function cache size address size, cache includes storage data tags. size block one word (4 bytes), normally several. following situation: 64-bit addresses direct-mapped cache cache size 2n blocks, n bits used index block size 2m words (2m+2 bytes), bits used word within block, two bits used byte part address size tag field total number bits direct-mapped cache Since block size 2m words (2m+5 bits), need 1 bit valid field, number bits cache Although actual size bits, naming convention exclude size tag valid field count 763size data. Thus, cache Figure 5.10 called 4 KiB cache. Bits Cache Example many total bits required direct-mapped cache 16 KiB data four-word blocks, assuming 64-bit address? Answer know 16 KiB 4096 (212) words. block size four words (22), 1024 (210) blocks. block 4 ×32 128 bits data plus tag, 64 −10 −2 −2 bits, plus valid bit. Thus, complete cache size 22.4 KiB 16 KiB cache. cache, total number bits cache 1.4 times many needed storage data. Mapping Address Multiword Cache Block Example Consider cache 64 blocks block size 16 bytes. block number byte address 1200 map? Answer saw formula page 376. block given address block 764Notice block address block containing addresses Thus, 16 bytes per block, byte address 1200 block address maps cache block number (75 modulo 64)=11. fact, block maps addresses 1200 1215. Larger blocks exploit spatial locality lower miss rates. Figure 5.11 shows, increasing block size usually decreases miss rate. miss rate may go eventually block size becomes significant fraction cache size, number blocks held cache become small, great deal competition blocks. result, block bumped cache many words accessed. Stated alternatively, spatial locality among words block decreases large block; consequently, benefi ts miss rate become smaller. 765FIGURE 5.11 Miss rate versus block size. Note miss rate actually goes block size large relative cache size. line represents cache different size. (This figure independent associativity, discussed soon.) Unfortunately, SPEC CPU2000 traces would take long block size included, data based SPEC92. serious problem associated increasing block size cost miss rises. miss penalty determined time required fetch block next lower le vel hierarchy load cache. time fetch block two parts: latency first word transfer time rest block. Clearly, unless change memory system, transfer time—and hence miss penalty—will likely increase block size expands. Furthermore, improvement miss rate starts decrease blocks become larger. result increase miss penalty overwhelms decrease miss rate blocks large, cache performance thus decreases. course, design memory transfer larger blocks efficiently, increase block size obtain improvements cache performance. discuss topi c next section. Elaboration Although hard anything longer latency 766component miss penalty large blocks, may able hide transfer time miss penalty effecti vely smaller. easiest method this, called early restart , simply resume execution soon requested word block returned, rather wait entire block. Many processors use technique instruction access, w orks best. Instruction accesses largely sequential, memory system deliver word every clock cycle, processor may able restart operation requested word returned, wi th memory system delivering new instruction words ime. technique usually less effective data caches likely words requested block less predictable way, probability processor need another word different cache block transfer completes high. processor cannot access data cache transfer ongoing, must stall. even sophisticated scheme organize memory requested word transferred memory cache first. remainder block transferred, starti ng address requested word wrapping around beginning block. technique, called requested word first critical word first , slightly faster early restart, limited properties restrain early restart. Handling Cache Misses look cache real system, let’s see control unit deals cache misses . (We describe cache controller detail Section 5.9 .) control unit must detect miss process miss fetching requested data memory (or, shall see, lower-level cache). cache reports hit, computer continues using data nothing happened. cache miss request data cache cannot filled data present cache. Modifying control processor handle hit trivial; 767misses, however, require extra work. cache miss handling done collaboration processor control unit wit h separate controller initiates memory access refills cache. processing cache miss creates pipeline stall (Chapter 4 ) contrast exception interrupt, would require saving state registers. cache miss, stall entire processor, essentially freezing contents temporary programmer-visible registers, wait memory. sophisticated out-of-order processors allow execution instructions waiting cache miss, we’ assume in-order processors stall cache misses sec tion. Let’s look little closely instruction misses handled; approach easily extended handle data misses. instruction access results miss, content f Instruction register invalid. get proper instruc tion cache, must able tell lower level memory hierarchy perform read. Since program counter incremented first clock cycle execution, address instruction generates instruction cache miss equal value program counter minus 4. address, need instruct main memory perform read. wait memory respond (since access take multiple c lock cycles), write words containing desired instru ction cache. define steps taken instruction cache miss: 1. Send original PC value memory. 2. Instruct main memory perform read wait memory complete access. 3. Write cache entry, putting data memory data portion entry, writing upper bits address (from ALU) tag field, turning valid bit on. 4. Restart instruction execution first step, refetch instruction, time finding cache. control cache data access essentially identical: miss, simply stall processor memory responds data. Handling Writes 768Writes work somewhat differently. Suppose store instruction , wrote data data cache (without changing main memory); then, write cache, memory would different value cache. case, cache memory said inconsistent . simplest way keep main memory cache consistent always write data memory cache. scheme called write-through . write-through scheme writes always update cache next lower level memory hierarchy, ensuring data always consistent two. key aspect writes occurs write miss. first fetch words block memory. block fetched placed cache, overwrite word caused miss cache block. also write word main memory using full address. Although design handles writes simply, would provide good performance. write-through scheme, every write causes data written main memory. writes take long time, likely least 100 processor clock cycles, could slow processor considerably. example, suppose 10% instructions stores. CPI without cache misses 1.0, spending 100 extra cycles every write would lead CPI 1.0 +100 ×10%=11, reducing performance factor 10. One solution problem use write buffer . write buffer stores data waiting written memory. writing data cache write buffer, processor continue execution. write main memory completes, entry write buffer freed. write buffer full processor reaches write, processor must stall empty position write buffer. cou rse, rate memory complete writes less rate processor generating writes, amount buffering help, writes generated faster 769the memory system accept them. write buffer queue holds data data waiting written memory. rate writes generated may also less rate memory accept them, yet stalls may still occur. happen writes occur bursts. reduce occurrence stalls, processors usually increase de pth write buffer beyond single entry. alternative write-through scheme scheme called write-back . write-back scheme, write occurs, new value written block cache. modified block written lower level hierarchy replaced. Write-back schemes improve performance, especially processors generate writes fast faster writes handled main memory; write-back scheme is, however, complex implement write-through. write-back scheme handles writes updating values block cache, writing modified block lower level hierarchy block replaced. rest section, describe caches real processors, examine handle reads writes. Section 5.8 , describe handling writes detail. Elaboration Writes introduce several complications caches present reads. discuss two them: policy write misses efficient implementation writes write-back cache s. Consider miss write-through cache. common strategy allocate block cache, called write allocate . block fetched memory appropriate portion 770the block overwritten. alternative strategy update portion block memory put cache, called write allocate . motivation sometimes programs write entire blocks data, operating system zeros page memory. cases, fetch associated initial write miss may unnecessary. computers allow write allocation policy changed per-page basis. Actually implementing stores efficiently cache uses write-back strategy complex write-through cache . write-through cache write data cache read tag; tag mismatches, miss occurs. cache write-through, overwriting block cache catastrophic, since memory correct value. write- back cache, must first write block back memory data cache modified cache miss. simply overwrote block store instruction kn ew whether store hit cache (as could write- cache), would destroy contents block, whic h backed next lower level memory hierarchy. write-back cache, cannot overwrite block, stores either require two cycles (a cycle check hit foll owed cycle actually perform write) require write buffer hold data—effectively allowing store take one cycl e pipelining it. store buffer used, processor th e cache lookup places data store buffer normal cache access cycle. Assuming cache hit, new data written store buffer cache next unused cache access cycle. comparison, write-through cache, writes always done one cycle. read tag write data portion selected block. tag matches address block written, processor continue normally, since correct block updated. tag match, processor generates write miss fetch rest block correspond ing address. Many write-back caches also include write buffers used reduce miss penalty miss replaces modified block. case, modified block moved write-back buffer associated cache requested block read 771memory. write-back buffer later written back memory. Assuming another miss occur immediately, technique halves miss penalty dirty block must replaced. Example Cache: Intrinsity FastMATH Processor Intrinsity FastMATH embedded microprocessor uses MIPS architecture simple cache implementation. Near end chapter, examine complex cache designs ARM Intel microprocessors, start simple, yet real, example pedagogical reasons. Figure 5.12 shows organization Intrinsity FastMATH data cache. Note address size computer 32 bits, 64 rest book. 772FIGURE 5.12 16 KiB caches Intrinsity FastMATH contain 256 blocks 16 words per block. Note address size computer 32 bits. tag field 18 bits wide index field 8 bits wide, 4-bit field (bits 5–2) used dex block select word block using 16-to-1 multiplexor. practice, eliminate multiplexor, caches use separate large RAM data smaller RAM tags, block offset supplying extra address bits large data RAM. case, large RAM 32 bits wide must 16 times many words blocks cache. processor 12-stage pipeline. operating peak speed, processor request instruction word data word every clock. satisfy demands pipeline without stalling, separate instruction data caches used. cache 16 KiB, 4096 words, 16-word blocks. Read requests cache straightforward. separate data instruction caches, need separate control signals read write cache. (Remember need update instruction cache miss occurs.) Thus, steps read request either cache follows: 7731. Send address appropriate cache. address comes either PC (for instruction) ALU (for data). 2. cache signals hit, requested word available data lines. Since 16 words desired block, need select right one. block index field used control multiplexor (shown bottom figure), selects requested word 16 words indexed block. 3. cache signals miss, send address main memory. memory returns data, write cache read fulfill request. writes, Intrinsity FastMATH offers write-through write-back, leaving operating system decide strategy use application. one-entry write buffer. cache miss rates attained cache structure like used Intrinsity FastMATH? Figure 5.13 shows miss rates instruction data caches. combined miss rate effective miss rate per reference program accountin g differing frequency instruction data accesses. FIGURE 5.13 Approximate instruction data miss rates Intrinsity FastMATH processor SPEC CPU2000 benchmarks. combined miss rate effective miss rate seen combination 16 KiB instruction cache 16 KiB data cache. obtained weighting instruction data individual miss rates frequency instruction data references. Although miss rate important characteristic cache designs, ultimate measure effect memory system program execution time; we’ll see miss rate execution time related shortly. Elaboration combined cache total size equal sum two 774split caches usually better hit rate. higher rate occurs combined cache rigidly divide number entries may used instructions th may used data. Nonetheless, almost processors today use split instruction data caches increase cache bandwidth match modern pipelines expect. (There may also fewer conflict misses; see Section 5.8 .) miss rates caches size found Intrinsity FastMATH processor, combined cache whose size equal sum two caches: Total cache size: 32 KiB Split cache effective miss rate: 3.24% Combined cache miss rate: 3.18% miss rate split cache slightly worse. advantage doubling cache bandwidth, supporting instruction data access simultaneously, easily overcomes disadvantage slightly increased miss rate. observation cautions us cannot use miss rate sole measure cache performance, Section 5.4 shows. split cache scheme level memory hierarchy composed two independent caches operate parallel other, one handling instructions one handling data. Summary began previous section examining simplest caches : direct-mapped cache one-word block. cache, hits misses simple, since word go exactly one location separate tag every word. keep cache memory consistent, write-through scheme used, every write cache also causes memory updated. alternative write-through write-back scheme copies block back memory replaced; we’ll discuss scheme upcoming sections. take advantage spatial locality, cache must block size larger one word. use bigger block decreases miss rate improves efficiency cache reducing th e 775amount tag storage relative amount data storage cache. Although larger block size decreases miss rate, also increase miss penalty. miss penalty increased linear ly block size, larger blocks could easily lead lower performance. avoid performance loss, bandwidth main memory increased transfer cache blocks efficiently. Common methods increasing bandwidth external DRAM making memory wider interleaving. DRAM designers steadily improved interface processor memo ry increase bandwidth burst mode transfers reduce co st larger cache block sizes. Check speed memory system affects designer’s decisio n size cache block. following cache designer guidelines generally valid? 1. shorter memory latency, smaller cache block 2. shorter memory latency, larger cache block 3. higher memory bandwidth, smaller cache block 4. higher memory bandwidth, larger cache block 5.4 Measuring Improving Cache Performance section, begin examining ways measure analyze cache performance. explore two different techniques improving cache performance. One focuses reducing miss rate reducing probability two dist inct memory blocks contend cache location. second technique reduces miss penalty adding additional level hierarchy. technique, called multilevel caching , first appeared high-end computers selling $100,000 1990; since become common personal mobile devices selling hundred dollars! CPU time divided clock cycles CPU spends executing program clock cycles CPU 776spends waiting memory system. Normally, assume costs cache accesses hits part normal CPU execution cycles. Thus, memory-stall clock cycles come primarily cache misses , make assumption here. also restrict discussion simplified model memory system. real processors, stalls generated reads writes quite complex, accurate performance prediction usually requires detailed simulations processor memory system. Memory-stall clock cycles defined sum stall cycles coming reads plus coming writes: read-stall cycles defined terms number read accesses per program, miss penalty clock cycles read, read miss rate: Writes complicated. write-through scheme, two sources stalls: write misses, usually require th fetch block continuing write (see Elaboration page 386 details dealing writes), write buffer stalls, occur write buffer full wri te happens. Thus, cycles stalled writes equal sum two: 777Because write buffer stalls depend proximity writ es, frequency, impossible give simple equat ion compute stalls. Fortunately, systems reasonable write buffer depth (e.g., four words) memory capable accepting writes rate significantly exceeds average write frequency programs (e.g., factor 2), write buffer stalls small, safely ignore them. system meet criteria, would well designed; instead, th e designer used either deeper write buffer write - back organization. Write-back schemes also potential additional stalls arising need write cache block back memory block replaced. discuss Section 5.8 . write-through cache organizations, read write miss penalties (the time fetch block memory). assume write buffer stalls negligible, combine reads writes using single miss rate miss penalty: also factor Let’s consider simple example help us understand impact cache performance processor performance. Calculating Cache Performance Example Assume miss rate instruction cache 2% miss rate data cache 4%. processor CPI 2 without memory stalls, miss penalty 100 cycles misses, determine much faster processor would run perfect 778cache never missed. Assume frequency loads stores 36%. Answer number memory miss cycles instructions terms Instruction count (I) frequency loads stores 36%, find number memory miss cycles data references: total number memory-stall cycles 2.00 +1.44 =3.44 I. three cycles memory stall per instruction. Accordingly, total CPI including memory stalls 2 +3.44 =5.44. Since change instruction count clock rate, rat io CPU execution times performance perfect cache better . happens processor made faster, memory system not? amount time spent memory stalls take increasing fraction execution time; Amdahl’s Law, examined Chapter 1 , reminds us fact. simple examples show serious problem be. Suppose speed-up computer previous example reducing ts CPI 2 1 without changing clock rate, might done improved pipeline. system cache misses 779would CPI 1 +3.44 =4.44, system perfect cache would amount execution time spent memory stalls would risen Similarly, increasing clock rate without changing memory system also increases performance lost due cache misses. previous examples equations assume hit time factor determining cache performance. Clearly, hit time increases, total time access word memory system increase, possibly causing increase process cycle time. Although see additional examples raise hit time shortly, one example increasing cache size. larger cache could clearly bigger access time, as, desk library large (say, 3 square meters), would take longer locate book desk. increase hit time likely adds another stage pipeline, since may take multiple cycles cache hit. Although complex calculate performance impact deeper pipeline, point increase hit time larger cache could dominate improvement hit rate, leading decrease processor performance. capture fact time access data hits misses affects performance, designers sometime use average memory access time (AMAT) way examine alternative cache designs. Average memory access time average time access memory considering hits misses frequency different 780accesses; equal following: Calculating Average Memory Access Time Example Find AMAT processor 1 ns clock cycle time, miss penalty 20 clock cycles, miss rate 0.05 misses per instruction, cache access time (including hit detection) 1 clock cycle. Assume read write miss penalties ignore write stalls. Answer average memory access time per instruction 2 ns. next subsection discusses alternative cache organizations th decrease miss rate may sometimes increase hit time; additional examples appear Section 5.16 . Reducing Cache Misses Flexible Placement Blocks far, put block cache, used simple placement scheme: block go exactly one place cache. mentioned earlier, called direct mapped direct mapping block address memory single location upper level hierarchy. However, actually whole range schemes placing blocks. Direct mapped, block placed exactly one location, one extreme. 781At extreme scheme block placed location cache. scheme called fully associative , block memory may associated entry cache. find given block fully associative cache, entries cache must searched block placed one. make search practical, done parallel comparator associated cache entry. comparators significantly increase hardware cost, effectively making ful ly associative placement practical caches small numbers blocks. fully associative cache cache structure block placed location cache. middle range designs direct mapped fully associative called set associative . set-associative cache, fixed number locations block placed. set-associative cache n locations block called n-way set-associative cache. n-way set-associative cache consists number sets, consists n blocks. block memory maps unique set cache given index field, block placed element set. Thus, set-associative placement combines direct-mapped placement fully associative placement: block directly mapped set, blocks set searched match. example, Figure 5.14 shows block 12 may put cache eight blocks total, according three block placement policies. set-associative cache cache fixed number locations (at least two) block placed. 782FIGURE 5.14 location memory block whose address 12 cache eight blocks varies direct-mapped, set-associative, full associative placement. direct-mapped placement, one cache block memory block 12 found, block given (12 modulo 8)=4. two-way set- associative cache, would four sets, memory block 12 must set (12 mod 4)=0; memory block could either element set. fully associative placement, memory block block address 12 appear eight cache blocks. Remember direct-mapped cache, position memory block given set-associative cache, set containing memory block given Since block may placed element set, tags elements set must searched. fully associative cache, block go anywhere, tags blocks cache must searched. also think block placement strategies variation set associativity. Figure 5.15 shows possible associativity 783structures eight-block cache. direct-mapped cache jus one-way set-associative cache: cache entry holds one block set one element. fully associative cache entries simply m-way set-associative cache; one set blocks, entry reside block within set. 784FIGURE 5.15 eight-block cache configured direct-mapped, two-way set associative, four-way set associative, fully associative. total size cache blocks equal number sets times associativity. Thus, fixed cache size, increasing associativity decreases number sets increasing number elements per set. eight blocks, eight-way set- associative cache fully associative cache. advantage increasing degree associativity usually decreases miss rate, next example shows. main disadvantage, discuss detail shortly, potential increase hit time. Misses Associativity Caches Example Assume three small caches, consisting four one- word blocks. One cache fully associative, second two-way set 785associative, third direct-mapped. Find number misses cache organization given following sequence block addresses: 0, 8, 0, 6, 8. Answer direct-mapped case easiest. First, let’s determine whic h cache block block address maps: Block address Cache block 0 (0 modulo 4)=0 6 (6 modulo 4)=2 8 (8 modulo 4)=0 fill cache contents reference, using blank entry mean block invalid, colored text show new entry added cache associated reference, plain text show old entry cache: direct-mapped cache generates five misses five accesses. set-associative cache two sets (with indices 0 1) two elements per set. Let’s first determine set bl ock address maps: Block address Cache set 0 (0 modulo 2)=0 6 (6 modulo 2)=0 8 (8 modulo 2)=0 choice entry set replace miss, need replacement rule. Set-associative caches usually replace least recently used block within set; is, bloc k used furthest past replaced. (We discuss replacement rules detail shortly.) Using 786replacement rule, contents set-associative cache reference look like this: Notice block 6 referenced, replaces block 8, since block 8 less recently referenced block 0. two-way set-associative cache four misses, one less direct- mapped cache. fully associative cache four cache blocks (in single set); memory block stored cache block. fully associative cache best performance, three misses : series references, three misses best do, three unique block addresses accessed. Notice eight blocks cache, would replacements two-way set-associative cache (check yourself), would number misses fully associative cache. Similarly, 16 blocks, three caches would identical number misses. Even trivial example shows cache size associativity independent determining cache performance. much reduction miss rate achieved 787associativity? Figure 5.16 shows improvement 64 KiB data cache 16-word block, associativity ranging direct- mapped eight-way. Going one-way two-way associativity decreases miss rate 15%, little improvement going higher associativity. FIGURE 5.16 data cache miss rates organization like Intrinsity FastMATH processor SPEC CPU2000 benchmarks associativity varying one-way eight-way. results 10 SPEC CPU2000 programs Hennessy Patterson (2003). Locating Block Cache Now, let’s consider task finding block cache set associative. direct-mapped cache, block set- associative cache includes address tag gives block address. tag every cache block within appropriate set checked see matches block address processo r. Figure 5.17 decomposes address. index value used select set containing address interest, tags blocks set must searched. speed essence, tags selected set searched parallel. fully associative cache, sequential search would make hit time set-associative cache slow. FIGURE 5.17 three portions address set-associative direct-mapped cache. index used select set, tag used choose block comparison blocks 788the selected set. block offset address desired data within block. total cache size kept same, increasing associativity raises number blocks per set, number simultaneous compares needed perform search parallel: increase factor 2 associativity doubles number blocks per set halves number sets. Accordingly, factor-of-2 increase associativity decreases size inde x 1 bit expands size tag 1 bit. fully associative cache, effectively one set, blo cks must checked parallel. Thus, index, entire address, excluding block offset, compared tag every block. words, search full cache without indexing. direct-mapped cache, single comparator needed, entry one block, access cache simply indexing. Figure 5.18 shows four-way set- associative cache, four comparators needed, together 4- to-1 multiplexor choose among four potential members th e selected set. cache access consists indexing appropriate set searching tags set. costs associative cache extra comparators delay imposed compare select among elements set. 789FIGURE 5.18 implementation four-way set- associative cache requires four comparators 4-to-1 multiplexor. comparators determine element selected set (if any) matches tag. output comparators used select data one four blocks indexed set, using multiplexor decoded select signal. implementations, Output enable signals data portions cache RAMs used select entry set drives output. Output enable signal comes comparators, causing element matches drive data outputs. organization eliminates need multiplexor. choice among direct-mapped, set-associative, fully associative mapping memory hierarchy depend cost miss versus cost implementing associativity, bot h time extra hardware. 790 Elaboration Content Addressable Memory (CAM) circuit combines comparison storage single device. Instead supplying address reading word like RAM, send data CAM looks see copy returns index matching row. CAMs mean cache designers afford implement much higher set associativity needed build hardware SRAMs comparators. 2013, greater size power CAM generally leads two-way four-way set associativity built standard SRAMs comparators, eight-way built using CAMs. Choosing Block Replace miss occurs direct-mapped cache, requested block go exactly one position, block occupying position must replaced. associative cache, choice place requested block, hence choice block replace. fully associative cache, blocks candidates replacement. set-associative cache, must choose among blocks selected set. commonly used scheme least recently used (LRU) , used previous example. LRU scheme, block replaced one unused longest time . set-associative example page 397 uses LRU, replaced Memory(0) instead Memory(6). least recently used (LRU) replacement scheme block replaced one unused longest time. LRU replacement implemented keeping track element set used relative elements set. two-way set-associative cache, tracking two elements used implemented keeping single bit set setting bit indicate element whenever element referenced. associativity increases, implementin g LRU gets harder; Section 5.8 , see alternative scheme 791for replacement. Size Tags versus Set Associativity Example Increasing associativity requires comparators tag bits per cache block. Assuming cache 4096 blocks, four-word block size, 64-bit address, find total number sets total number tag bits caches direct-mapped, two- way four-way set associative, fully associative. Answer Since 16 (=24) bytes per block, 64-bit address yields 64 −4 =60 bits used index tag. direct-mapped cache number sets blocks, hence 12 bits index, since log2(4096)=12; hence, total number (60 −12)×4096 =48 ×4096 =197 K tag bits. degree associativity decreases number sets factor 2 thus decreases number bits used index cache 1 increases number bits tag 1. Thus, two-way set-associative cache, 2048 sets, total number tag bits (60 −11)×2 ×2048 =98 ×2048 =401 Kbits. four-way set-associative cache, total number sets 1024, total number (60 −10)×4 ×1024 =100 ×1024 =205 K tag bits. fully associative cache, one set 4096 blocks, tag 60 bits, leading 60 ×4096 ×1 =246 K tag bits. Reducing Miss Penalty Using Multilevel Caches modern computers make use caches. close gap fast clock rates modern processors increasingly long time required access DRAMs, microprocessors support additional level caching. second-level cache normally chip accessed whenever miss occurs primary cache. second-level cache contains desired data, miss penalty first-level 792cache essentially access time second-level cac he, much less access time main memory. neither primary secondary cache contains data, main memory access required, larger miss penalty incurred. significant performance improvement use secondary cache? next example shows us. Performance Multilevel Caches Example Suppose processor base CPI 1.0, assuming references hit primary cache, clock rate 4 GHz. Assume main memory access time 100 ns, including miss handling. Suppose miss rate per instruction primar cache 2%. much faster processor add secondary cache 5- ns access time either hit miss large enough reduce miss rate main memory 0.5%? Answer miss penalty main memory effective CPI one level caching given processor one level caching, two levels caching, miss primary (or first-level) cache satisfied either secondary cache main 793memory. miss penalty access second-level cache miss satisfied secondary cache, entire miss penalty. miss needs go main memory, total miss penalty sum secondary cache access time main memory access time. Thus, two-level cache, total CPI sum stall cycles levels cache base CPI: Thus, processor secondary cache faster Alternatively, could computed stall cycles summing stall cycles references hit secondary cache ((2%−0.5%)×20 =0.3). references go main memory, must include cost access secondary cache well main memory access time, (0.5%×(20 +400)=2.1). sum, 1.0 +0.3 +2.1, 3.4. design considerations primary secondary cache significantly different, presence cache changes best choice versus single-level cache. particular , two-level cache structure allows primary cache focus minimizing hit time yield shorter clock cycle fewer pipe line stages, allowing secondary cache focus miss rate reduce penalty long memory access times. effect changes two caches seen comparing cache optimal design single level 794cache. comparison single-level cache, primary cache multilevel cache often smaller. Furthermore, primary cache may use smaller block size, go smaller cache size also reduce miss penalty. comparison, secondary cache much larger single-level cache, since access time secondary cache less critical. larger total size, secondary cache may use larger block size appropriate single-level cache. often uses higher associativity primary cache given focus reducing miss rates. multilevel cache memory hierarchy multiple levels caches, rather cache main memory. Understanding Program Performance Sorting exhaustively analyzed find better algorithms: Bubble Sort, Quicksort, Radix Sort, on. Figure 5.19(a) shows instructions executed item searched Radix Sort versus Quicksort. expected, large arrays, Radix Sort algorithmic advantage Quicksort terms number operations. Figure 5.19(b) shows time per key instead instructions executed. see lines start trajectory Figure 5.19(a) , Radix Sort line diverges data sort increase. going on? Figure 5.19(c) answers looking cache misses per item sorted: Quicksort consistently many fewer misses per item sorted. 795796FIGURE 5.19 Comparing Quicksort Radix Sort (a) instructions executed per item sorted, (b) time per item sorted, (c) cache misses per item sorted. data paper LaMarca Ladner [1996]. Due results, new versions Radix Sort invented take memory hierarchy account, regain algorithmic advantages (see Section 5.15 ). basic idea cache optimizations use data block repeatedly ar e replaced miss. Alas, standard algorithmic analysis often ignores impact memory hierarchy. faster clock rates Moore’s Law allow architects squeeze performance stream instructions, using memory hierarchy well vital high performance. said introduction, understanding behavior memory hierarchy critical understanding performance programs today’s computers. Software Optimization via Blocking 797Given importance memory hierarchy program performance, surprisingly many software optimizations invented dramatically improve performance reusing data within cache hence lower miss rates due improved temporal locality. dealing arrays, get good performance memory system store array memory accesses array sequential memory. Suppose dealing multiple arrays, however, arrays accessed rows columns. Storing arrays row-by-row (called row major order ) column-by-column ( column major order ) solve problem rows columns used every loop iteration. Instead operating entire rows columns array, blocked algorithms operate submatrices blocks . goal maximize accesses data loaded cache data replaced; is, improve temporal locality reduce cache misses. example, inner loops DGEMM (lines 4 9 Figure 3.22 Chapter 3 ) (int j = 0; j < n; ++j) { double cij = C[i+j*n]; /* cij = C[i][j] */ for( int k = 0; k < n; k++ ) cij += A[i+k*n] * B[k+j*n]; /* cij += A[i][k]*B[k][ j] */ C[i+j*n] = cij; /* C[i][j] = cij */ } } reads N-by-N elements B, reads N elements corresponds one row repeatedly, writes corresponds one row N elements C. (The comments make rows columns matrices easier identify.) Figure 5.20 gives snapshot accesses three arrays. dark shade indicates recent access, light shade indicates older access, white means yet accessed. 798FIGURE 5.20 snapshot three arrays C, A, B N = 6 = 1. age accesses array elements indicated shade: white means yet touched, light means older accesses, dark means newer accesses. Compared Figure 5.22 , elements B read repeatedly calculate new elements C. variables i, j, k shown along rows columns used access arrays. number capacity misses clearly depends N size cache. hold three N-by-N matrices, well, provided cache conflicts. purposely picked matrix size 32 32 DGEMM Chapters 3 4 would case. matrix 32 ×32 =1024 elements element 8 bytes, three matrices occupy 24 KiB, comfortably fit 32 KiB data cache Intel Core i7 (Sandy Bridge). cache hold one N-by-N matrix one row N, least ith row array B may stay cache. Less misses may occur B C. worst case, would 2 N3+N2 memory words accessed N3 operations. ensure elements accessed fit cache, original code changed compute submatrix. Hence, essentially invoke version DGEMM Figure 4.78 Chapter 4 repeatedly matrices size BLOCKSIZE BLOCKSIZE . BLOCKSIZE called blocking factor . Figure 5.21 shows blocked version DGEMM. function do_block DGEMM Figure 3.22 three new parameters si, sj, sk specify starting position submatrix A, B, C. two inner loops do_block compute steps size BLOCKSIZE rather full length B C. gcc 799optimizer removes function call overhead “inlining” function; is, inserts code directly avoid conventional parameter passing return address bookkeeping instructions. FIGURE 5.21 Cache blocked version DGEMM Figure 3.22 . Assume C initialized zero. do_block function basically DGEMM Chapter 3 new parameters specify starting positions submatrices BLOCKSIZE . gcc optimizer remove function overhead instructions inlining do_block function. Figure 5.22 illustrates accesses three arrays using blocking. Looking capacity misses, total number memory words accessed 2 N3/BLOCKSIZE + N2. total improvement factor BLOCKSIZE . Hence, blocking exploits combination spatial temporal locality, since benefits spatial locality B benefits temporal locality. 800FIGURE 5.22 age accesses arrays C, A, B BLOCKSIZE = 3. Note that, contrast Figure 5.20 , fewer elements accessed. Although aimed reducing cache misses, blocking also used help register allocation. taking small blocking size, block held registers, minimize number loads stores program, improves performance. Figure 5.23 shows impact cache blocking performance unoptimized DGEMM increase matrix size beyond three matrices fit cache. unoptimized performance halved largest matrix. cache-blocked version less 10% slower even matrices 960 ×960, 900 times larger 32 ×32 matrices Chapters 3 4. Elaboration Multilevel caches create many complications. First, several different types misses corresponding miss rates . example pages 402–403, saw primary cache miss rate global miss rate —the fraction references missed cache levels. also miss rate secondary cache, ratio misses secondary cache divided number accesses it. miss rate called local miss rate secondary cache. primary cache filters accesses, especially good spatial temporal locality, local miss rate secondary cache much higher global miss rate. example pages 402–403, compute local miss rate secondary cache 8010.5%/2%=25%! Luckily, global miss rate dictates often must access main memory. global miss rate fraction references miss levels multilevel cache. local miss rate fraction references one level cache miss; used multilevel hierarchies. Elaboration out-of-order processors (see Chapter 4 ), performance complex, since execute instructions miss pen alty. Instead instruction miss rates data miss rates, use misses per instruction, formula: general way calculate overlapped miss latency, evaluations memory hierarchies out-of-order processor inevitably require simulation processor memory hierarchy. seeing execution processor durin g miss see processor stalls waiting data simply finds work do. guideline processor often hides miss penalty L1 cache miss hits L2 cache, rarely hides miss L2 cache. Elaboration performance challenge algorithms memory hierarchy varies different implementations architecture cache size, associativity, block size, number caches. cope variability, recent numerical libraries parameterize algorithms search parameter space runtime find best combination 802particular computer. approach called autotuning . Check following generally true design multiple levels caches? 1. First-level caches concerned hit time, second- level caches concerned miss rate. 2. First-level caches concerned miss rate, second-level caches concerned hit time. FIGURE 5.23 Performance unoptimized DGEMM (Figure 3.22 ) versus cache blocked DGEMM ( Figure 5.21) matrix dimension varies 32 ×32 (where three matrices fit cache) 960 ×960. Summary section, focused four topics: cache performance, usin g associativity reduce miss rates, use multilevel cache hierarchies reduce miss penalties, software optimizations improve effectiveness caches. memory system significant effect program execution time. number memory-stall cycles depends mis rate miss penalty. challenge, see Section 5.8, reduce one factors without significantly affecti ng 803other critical factors memory hierarchy. reduce miss rate, examined use associative placement schemes. schemes reduce miss rate cache allowing flexible placement blocks within cache. Fully associative schemes allow blocks placed anywhere, also require every block cache searched satisfy request. higher costs make large fully associative caches impractical. Set-associative caches practical alternative, since need search among elements unique set chosen indexing. Set-associative caches higher miss rate faster access. amount associativity yields best performance depends technology details implementation. looked multilevel caches technique reduce miss penalty allowing larger secondary cache handle misses primary cache. Second-level caches become commonplace designers find limited silicon goals high clock rates prevent primary caches becoming large. secondary cache, often 10 times larger primary cache, handles many accesses miss primary cache. cases, miss penalty access time secondary cache (typically <10 processor cycles) versus access time memory (typically >100 processor cycles). associativity, design tradeoffs size secondar cache access time depend number aspects implementation. Finally, given importance memory hierarchy performance, looked change algorithms improve cache behavior, blocking important technique dealing large arrays. 5.5 Dependable Memory Hierarchy Implicit prior discussion memory hierarch doesn’t forget. Fast undependable attractive. learned Chapter 1 , one great idea dependability redundancy. section we’ll first go terms def ine terms measures associated failure, show redundancy make nearly unforgettable memories. 804Defining Failure start assumption specification proper service. Users see system alternating two states delivered service respect service specificati on: 1. Service accomplishment , service delivered specified 2. Service interruption , delivered service different specified service Transitions state 1 state 2 caused failures , transitions state 2 state 1 called restorations . Failures permanent intermittent. latter difficult cas e; harder diagnose problem system oscillates betwee n two states. Permanent failures far easier diagnose. definition leads two related terms: reliability availability. Reliability measure continuous service accomplishment —or, equivalently, time failure—from reference point. Hence, mean time failure (MTTF) reliability measure. related term annual failure rate (AFR), percentage devices would expected fail year given MTTF. MTTF gets large misleading, AFR leads better intuition. MTTF vs. AFR Disks Example disks today quoted 1,000,000-hour MTTF. 1,000,000 hours 1,000,000/(365 ×24)=114 years, would seem like practically never fail. Warehouse-scale computers run Internet services Search might 50,000 servers. Assume 805each server two disks. Use AFR calculate many disks would expect fail per year. Answer One year 365 ×24 =8760 hours. 1,000,000-hour MTTF means AFR 8760/1,000,000 =0.876%. 100,000 disks, would expect 876 disks fail per year, average two disk failures per day! Service interruption measured mean time repair (MTTR). Mean time failures (MTBF) simply sum MTTF +MTTR. Although MTBF widely used, MTTF often appropriate term. Availability measure service accomplishment respect alternation two states accomplishment interruption. Availability statistically quantified Note reliability availability actually quantifiable measures, rather synonyms dependability. Shrinking MTTR help availability much increasing MTTF. example, tools fault detection, diagnosis, repair help reduce time repair faults thereby improve availability. want availability high. One shorthand quote number “nines availability” per year. instance, good Internet service today offers 4 5 nines availability. Give n 365 days per year, 365 ×24 ×60 =526,000 minutes, shorthand decoded follows: One nine: 90% =>36.5 days repair/year Two nines: 99% =>3.65 days repair/year Three nines: 99.9% =>526 minutes repair/year Four nines: 99.99% =>52.6 minutes repair/year Five nines: 99.999% =>5.26 minutes repair/year on. increase MTTF, improve quality components design systems continue operation 806presence components failed. Hence, failure needs defined respect context, failure component may lead failure system. make distinction clear, term fault used mean failure component. three ways improve MTTF: 1. Fault avoidance: Preventing fault occurrence construction. 2. Fault tolerance: Using redundancy allow service comply service specification despite faults occurring. 3. Fault forecasting: Predicting presence creation faults, allowing component replaced fails. Hamming Single Error Correcting, Double Error Detecting Code (SEC/DED) Richard Hamming invented popular redundancy scheme memory, received Turing Award 1968. invent redundant codes, helpful talk “close” correct bit patterns be. call Hamming distance 807the minimum number bits different two correct bit patterns. example, distance 01 1011 001111 two. happens minimum distance members code two, get one-bit error? turn valid pattern code invalid one. Thus, detect whether members code accurate not, detect single bit errors, say single bit error detection code . error detection code code enables detection error data, precise location and, hence, correction error. Hamming used parity code error detection. parity code, number 1 word counted; word odd parity number 1 odd even otherwise. word written memory, parity bit also written (1 odd, 0 even). is, parity N+1 bit word always even. Then, word read out, parity bit read checked. parity memory word stored parity bit match, error occurred. Example Calculate parity byte value 31ten show pattern stored memory. Assume parity bit right. Suppose significant bit inverted memory, read back. detect error? happens two significant bits inverted? Answer 31ten 00011111two, five 1 s. make parity even, need write 1 parity bit, 000111111two. significant bit inverted read back, would see 100111111two seven 1 s. Since expect even parity calculated odd parity, would signal error. two significant bits inverted, would see 11 0111111two eight 1 even parity, would signal error. 808If 2 bits error, 1-bit parity scheme detect errors, since parity match data two errors. (Actually, 1-bit parity scheme detect odd number errors; however, probability three errors muc h lower probability two, so, practice, 1-bit parity code limited detecting single bit error.) course, parity code cannot correct errors, Hamming wanted well detect them. used code minimum distance 3, single bit error would closer correct pattern valid pattern. came easy understand mapping data distance 3 code call Hamming Error Correction Code (ECC) honor. use extra parity bits allow position identification singl e error. steps calculate Hamming ECC 1. Start numbering bits 1 left, contrary traditional numbering rightmost bit 0. 2. Mark bit positions powers 2 parity bits (positions 1, 2, 4, 8, 16, …). 3. bit positions used data bits (positions 3, 5, 6, 7, 9, 10, 11, 12, 13, 14, 15, …). 4. position parity bit determines sequence data bits checks ( Figure 5.24 shows coverage graphically) is: Bit 1 (0001two) checks bits (1,3,5,7,9,11,...), bits rightmost bit address 1 (0001two, 0011two, 0101two, 0111two, 1001two, 1011two,…). Bit 2 (0010two) checks bits (2,3,6,7,10,11,14,15,…), bits second bit right address 1. Bit 4 (0100two) checks bits (4–7, 12–15, 20–23,…), bits third bit right address 1. Bit 8 (1000two) checks bits (8–15, 24–31, 40–47,...), bits fourth bit right address 1. Note data bit covered two parity bits. 5. Set parity bits create even parity group. 809FIGURE 5.24 Parity bits, data bits, field coverage Hamming ECC code eight data bits. seems like magic trick, determine whether bits incorrect looking parity bits. Using 12 bit code Figure 5.24 , value four parity calculations (p8,p4,p2,p1) 0000, error. However, pattern was, say, 1010, 10ten, Hamming ECC tells us bit 10 (d6) error. Since number binary, correct error b inverting value bit 10. Example Assume one byte data value 10011010two. First show Hamming ECC code byte, invert bit 10 show ECC code finds corrects single bit error. Leaving spaces parity bits, 12 bit pattern is__ 1_ 0 0 1_ 1 0 1 0. Answer Position 1 checks bits 1,3,5,7,9, 11, highlight:__ 1_ 0 0 1_ 1 0 1 0. make group even parity, set bit 1 0. Position 2 checks bits 2,3,6,7,10,11, 0_ 1_ 0 0 1_ 1 0 1 0 odd parity, set position 2 1. Position 4 checks bits 4,5,6,7,12, 0 1 1_ 0 0 1 _ 1 0 1, set 1. Position 8 checks bits 8,9,10,11,12, 0 1 1 1 0 0 1_ 1 0 1 0 , set 0. final code word 011100101010. Inverting bit 10 changes 011100101 110. 810Parity bit 1 0 ( 011100101110 four 1 s, even parity; group OK). Parity bit 2 1 (0 11100101110 five 1 s, odd parity; error somewhere). Parity bit 4 1 (011 1001 0111 0 two 1 s, even parity; group OK). Parity bit 8 1 (0111001 01110 three 1 s, odd parity; error somewhere). Parity bits 2 8 incorrect. 2 +8 =10, bit 10 must wrong. Hence, correct error inverting bit 10: 011100101 010. Voila! Hamming stop single bit error correction code. cost one bit, make minimum Hamming distance code 4. means correct single bit errors detect double bit errors . idea add parity bit calculated whole word. Let’s use 4-bit data word example, would need 7 bits single bit error detection. Hamming parity bits H (p1 p2 p3) computed (even parity usual) plus even parity entire word, p4: 1 2 3 4 5 6 7 8 p1 p2 d1 p3 d2 d3 d4 p4 algorithm correct one error detect two calculate parity ECC groups (H) plus one whole group (p4). four cases: 1. H even p4 even, error occurred. 2. H odd p4 odd, correctable single error occurred. (p4 calculate odd parity one error occurred.) 3. H even p4 odd, single error occurred p4 bit, rest word, correct p4 bit. 4. H odd p4 even, double error occurred. (p4 calculate even parity two errors occurred.) Single Error Correcting/Double Error Detecting (SEC/DED) common memory servers today. Conveniently, 8-byte data blocks get SEC/DED one byte, many DIMMs 72 bits wide. 811Elaboration calculate many bits needed SEC, let p total number parity bits number data bits p+d bit word. p error correction bits point error bit ( p +d cases) plus one case indicate error exists, need: example, 8 bits data means = 8 2p≥p + 8 +1, p = 4. Similarly, p = 5 16 bits data, 6 32 bits, 7 64 bits, on. Elaboration large systems, possibility multiple errors wel l complete failure single wide memory chip becomes signific ant. IBM introduced chipkill solve problem, many big systems use technology. (Intel calls version SDDC.) Similar nature RAID approach used disks (see Section 5.11 ), Chipkill distributes data ECC information, complete failure single memory chip handled supporting reconstruction missing data remaining memory chips. Assuming 10,000-processor cluster 4 GiB per processor, IBM calculated following rates unrecoverable memory errors 3 years operation: Parity only—about 90,000, one unrecoverable (or undetected) failure every 17 minutes. SEC/DED only—about 3500, one undetected unrecoverable failure every 7.5 hours. Chipkill—6, one undetected unrecoverable failure every 2 months. Hence, Chipkill requirement warehouse-scale computers. Elaboration single double bit errors typical memory systems , networks bursts bit errors. One solution called Cyclic Redundancy Check . block k bits, transmitter generates 812n-k bit frame check sequence. transmits n bits exactly divisible number. receiver divides frame number. remainder, assumes error. is, receiver rejects message, asks transmitter send . might guess Chapter 3 , easy calculate division binary numbers shift register, made CRC codes popular even hardware precious. Going even further, Reed-Solomon codes use Galois fields correct multibit transmission errors, data considered coefficients polynomial code space values polynomial. Reed-Solomon calculation considerably complicated binary division! 5.6 Virtual Machines Virtual machines (VM) first developed mid-1960s, remained important part mainframe computing years. Although largely ignored single-user PC e ra 1980s 1990s, recently gained popularity due increasing importance isolation security modern systems failures security reliability standard operating systems sharing single computer among many unrelated users, particular Cloud computing dramatic increases raw speed processors decades, makes overhead VMs acceptable broadest definition VMs includes basically emulation methods provide standard software interface, Java VM. section, interested VMs provide complete system-level environment binary instruction set architecture (ISA) level. Although VMs run different ISAs VM native hardware, assume always match hardware. VMs called (Operating) System Virtual Machines . IBM VM/370, VirtualBox, VMware ESX Server, Xen examples. System virtual machines present illusion users entire computer themselves, including copy operati ng system. single computer runs multiple VMs support 813number different operating systems (OSes). conventional platform, single OS “owns” hardware resources, VM, multiple OSes share hardware resources. software supports VMs called virtual machine monitor (VMM) hypervisor ; VMM heart virtual machine technology. underlying hardware platform called host, resources shared among guest VMs. VMM determines map virtual resources physical resources: physical resource may time-shared, partitioned, even emulated software. VMM much smaller traditional OS; isolation portion VMM perhaps 10,000 lines code. Although interest VMs improving protectio n, VMs provide two benefits commercially significan t: 1. Managing software . VMs provide abstraction run complete software stack, even including old operating systems lik e DOS. typical deployment might VMs running legacy OSes, many running current stable OS release, testing next OS release. 2. Managing hardware . One reason multiple servers application running compatible version operating system separate computers, separation improve dependability. VMs allow separate software stacks run independently yet share hardware, thereby consolidating number servers. Another example VMMs support migration running VM different computer, either balance load evacuate failing hardware. Hardware/Software Interface Amazon Web Services (AWS) uses virtual machines Cloud computing offering EC2 five reasons: 1. allows AWS protect users sharing server. 2. simplifies software distribution within warehouse-scale computer. customer installs virtual machine image configured appropriate software, AWS distributes instances customer wants use. 3. Customers (and AWS) reliably “kill” VM control 814resource usage customers complete work. 4. Virtual machines hide identity hardware customer running, means AWS keep using old servers introduce new, efficient servers. customer expects performance instances match ratings “EC2 Compute Units,” AWS defines: “provide equivalent CPU capacity 1.0–1.2 GHz 2007 AMD Opteron 2007 Intel Xeon processor.” Thanks Moore’s Law , newer servers clearly offer EC2 Compute Units older ones, AWS keep renting old servers long economical. 5. Virtual machine monitors control rate VM uses processor, network, disk space, allows AWS offer many price points instances different types runnin g underlying servers. example, 2012 AWS offered 14 instance types, small standard instances $0.08 per hour high I/O quadruple extra-large instances $3.10 per hour. general, cost processor virtualization depends workload. User-level processor-bound programs zero virtualization overhead, OS rarely invoked, everything runs native speeds. I/O-intensive workloads generally also OS-intensive, executing many system calls 815privileged instructions result high virtualization overhead. hand, I/O-intensive workload also I/O-bound , cost processor virtualization completely hidden, since processor often idle waiting I/O. overhead determined number instructions must emulated VMM much time takes emulate. Hence, guest VMs run ISA host, assume here, goal architecture VMM run almost instructions directly native hardware. Requirements Virtual Machine Monitor must VM monitor do? presents software interface guest software, must isolate state guests , must protect guest software (including guest OSes). qualitative requirements are: Guest software behave VM exactly running native hardware, except performance-related behavior limitations fixed resources shared multiple VMs. Guest software able change allocation real system resources directly. “virtualize” processor, VMM must control everything—access privileged state, I/O, exceptions, interrupts—even though guest VM OS presently running temporarily using them. example, case timer interrupt, VMM would suspend currently running guest VM, save state, handle interrupt, determine guest VM run next, load state. Guest VMs rely timer interrupt provided virtual timer emulated timer interrupt VMM. charge, VMM must higher privilege level guest VM, generally runs user mode; also ensures execution privileged instruction handled VMM. basic system requirements support VMMs are: least two processor modes, system user. privileged subset instructions available sys tem mode, resulting trap executed user mode; system 816resources must controllable via instructions. (Lack of) Instruction Set Architecture Support Virtual Machines VMs planned design ISA, it’s relativel easy reduce number instructions must executed VMM improve emulation speed. architecture allows VM execute directly hardware earns title virtualizable , IBM 370 RISC-V architectures proudly bear label. Alas, since VMs considered PC server applications fairly recently, instruction sets cr eated without virtualization mind. culprits include x86 RISC architectures, including ARMv7 MIPS. VMM must ensure guest system interacts virtual resources, conventional guest OS runs user mode program top VMM. Then, guest OS attempts access modify information related hardware resources via privileged instruction—for example, reading writing status bit enables interrupts—it trap VMM . VMM affect appropriate changes corresponding real resources. Hence, instruction tries read write sensitiv e information traps executed user mode, VMM intercept support virtual version sensitive information, guest OS expects. absence support, measures must taken. VMM must take special precautions locate problematic instructions ensure behave correctly executed guest OS, thereby increasing complexity VMM reducing performance running VM. Protection Instruction Set Architecture Protection joint effort architecture operating system s, architects modify awkward details existing instruction set architectures virtual memory became popul ar. example, x86 instruction POPF loads flag registers 817from top stack memory. One flags Interrupt Enable (IE) flag. run POPF instruction user mode, rather trap it, simply changes flags except IE. system mode, change IE. Since guest OS runs user mode inside VM, problem, expects see changed IE. Historically, IBM mainframe hardware VMM took three steps improve performance virtual machines: 1. Reduce cost processor virtualization. 2. Reduce interrupt overhead cost due virtualization. 3. Reduce interrupt cost steering interrupts proper V without invoking VMM. AMD Intel tried address first point 2006 reducing cost processor virtualization. interesting ee many generations architecture VMM modifications take address three points, long virtual machines 21st century x86 efficient IBM mainframes VMMs 1970s. Elaboration RISC-V traps privileged instructions running user mode, supports classical virtualization , wherein guest OS runs user mode VMM runs supervisor mode. 5.7 Virtual Memory … system devised make core drum combination appear programmer single level store, requisite transfers taking place automatically. Kilburn et al., One-level storage system, 1962 earlier sections, saw caches provided fast access recently-used portions program’s code data. Similarly, main memory act “cache” secondary storage, traditionally implemented magnetic disks. technique called virtual memory . Historically, two major motivations virtual memory: allow efficient safe sharing 818of memory among several programs, memory needed multiple virtual machines Cloud computing, remove programming burdens small, limited amount main memory. Five decades invention, it’s former reason reigns today. virtual memory technique uses main memory “cache” secondary storage. course, allow multiple virtual machines share memory, must able protect virtual machines other, ensuring program read write portions main memory assigned it. Main memory need contain active portions many virtual machines, cache contains active portion one program. Thus, principle locality enables virtual memory well caches, virtual memory allows us share processor efficiently well main memory. cannot know virtual machines share memory virtual machines compile them. fact, virtual machines sharing memory change dynamically running. dynamic interaction, would like compile program address space —a separate range memory locations accessible program. Virtual memory implements translation program’s address space physical addresses . translation process enforces protection program’s address space virtual machines. physical address address main memory. protection set mechanisms ensuring multiple processes sharin g processor, memory, I/O devices cannot interfere, intentionally unintentionally, one another reading writing other’s data. mechanisms also isolate 819operating system user process. second motivation virtual memory allow single- user program exceed size primary memory. Formerly, program became large memory, programmer make fit. Programmers divided programs pieces identified pieces mutually exclus ive. overlays loaded unloaded user program control execution, programmer ensuring program time tried access overlay loaded overlays loaded never exceeded total size memory. Overlays traditionally organized modules, containing code data. Calls procedures different modules would lead overlaying one module another. well imagine, responsibility substantial burden programmers. Virtual memory, invented relieve programmers difficulty, automatically manages two levels memory hierarchy represented main memory (sometimes called physical memory distinguish virtual memory) secondary storage. Although concepts work virtual memory caches same, differing historical roots led use different terminology. virtual memory block called page, virtual memory miss called page fault . virtual memory, processor produces virtual address , translated combination hardware software physical address , turn used access main memory. Figure 5.25 shows virtually addressed memory pages mapped main memory. process called address mapping address translation . Today, two memory hierarchy levels controlled virtual memory usually DRAMs flash memory personal mobile devices DRAMs magnetic disks servers (see Section 5.2 ). return library analogy, think virtual address title book physical address location book library, might given Library Congress call number. 820page fault event occurs accessed page present main memory. virtual address address corresponds location virtual space translated address mapping physical address memory accessed. address translation Also called address mapping . process virtual address mapped address used access memory. FIGURE 5.25 virtual memory, blocks memory (called pages ) mapped one set addresses (called virtual addresses ) another set (called physical addresses ). processor generates virtual addresses memory accessed using physical addresses. virtual memory physical memory broken pages, virtual page mapped physical page. course, also possible 821virtual page absent main memory mapped physical address; case, page resides disk. Physical pages shared two virtual addresses point physical address. capability used allow two different programs share data code. Virtual memory also simplifies loading program executio n providing relocation . Relocation maps virtual addresses used program different physical addresses addresses used access memory. relocation allows us load program anywhere main memory. Furthermore, virtual memory systems use today relocate program set fixed- size blocks (pages), thereby eliminating need find contiguous block memory allocate program; instead, operating system needs find enough pages main memory. virtual memory, address broken virtual page number page offset . Figure 5.26 shows translation virtual page number physical page number . RISC-V 64-bit address, upper 16 bits used, address mapped 48 bits. figure assumes physical memory 1 TiB, 240 bytes, needs 40-bit address. physical page number constitutes upper portion physical address, wh ile page offset, changed, constitutes lower porti on. number bits page offset field determines page ize. number pages addressable virtual address different number pages addressable physical address. larger number virtual pages physical pages basis illusion essentially unbounded amount virtual memory. 822FIGURE 5.26 Mapping virtual physical address. page size 212 = 4 KiB. number physical pages allowed memory 228, since physical page number 28 bits it. Thus, main memory 1 TiB, virtual address space 256 TiB. RISC-V allows physical memory 1 PiB; chose 1 TiB ample many computers 2016. Many design choices virtual memory systems motivated high cost page fault. page fault disk take millions clock cycles process. (The table page 372 shows main memory latency 100,000 times quicker disk.) enormous miss penalty, dominated time get first wor typical page sizes, leads several key decisions designing virtual memory systems: Pages large enough try amortize high access time. Sizes 4 KiB 64 KiB typical today. New desktop server systems developed support 32 KiB 64 KiB pages, new embedded systems going direction, 1 KiB pages. Organizations reduce page fault rate attractive. 823primary technique used allow fully associative placement pages memory. Page faults handled software overhead small compared disk access time. addition, software afford use clever algorithms choosing place pages even little reductions miss rate pay cost algorithms. Write-through work virtual memory, since writes take long. Instead, virtual memory systems use write-back. next subsections address factors virtual memor design. Elaboration present motivation virtual memory many virtual machines sharing memory, virtual memory originally invented many programs could share computer part timesharing system. Since many readers today experience time-sharing systems, use virtual machines motivate section. Elaboration RISC-V supports variety virtual memory configurations. addition 48-bit virtual address scheme, good fit large servers 2016, architecture support 39-bit 57- bit virtual address spaces. configurations use page size 4 Kibibytes. Elaboration servers even PCs, 32-bit address processors problematic. Although normally think virtual addresses much larger physical addresses, opposite occur processor address size small relative state memory technology. single program virtual machine benefit, collection programs virtual machines running time benefit swapped main memory running parallel processors. 824 Elaboration discussion virtual memory book focuses paging, uses fixed-size blocks. also variable-size block scheme called segmentation . segmentation, address consists two parts: segment number segment offset. segment number mapped physical address, offset added find actual physical address. segment vary size, bounds check also needed make sure offset within segment. major use segmentation support powerful methods protection sharing address space. operating system textbooks contain extensive discussions segmentation compared paging use segmentation share address space logically. major disadvantage segmentation splits address space int logically separate pieces must manipulated two-part address: segment number offset. Paging, contrast, makes boundary page number offset invisible programmers compilers. Segments also used method extend address space without changing word size computer. attempts unsuccessful awkwardness performance penalties inherent two-part address, programmers compilers must aware. Many architectures divide address space large fixed-size blocks simplify protection operating system user programs increase efficiency implementing paging . Although divisions often called “segments,” mechanism much simpler variable block size segmentation visible user programs; discuss detail shortly. segmentation variable-size address mapping scheme address consists two parts: segment number, mapped physical address, segment offset. Placing Page Finding 825Because incredibly high penalty page fault, designers reduce page fault frequency optimizing page placement. allow virtual page mapped physical page, operating system choose replace page wants page fault occurs. example, operating system use sophisticated algorithm complex data structures track page usage try choose page needed long time. ability use clever flexible replacement scheme reduces page fault rate simplifies use fully associative placement pages. mentioned Section 5.4 , difficulty using fully associative placement locating entry, since anywhere upper level hierarchy. full search impractical. virtual memory systems, locate pages using table indexes main memory; structure called page table , resides main memory. page table indexed page number virtual address discover corresponding physical page number. program page table, maps virtual address space program main memory. library analogy, page table corresponds mapping book titles library locations. card catalog may contain entries books another library campus rather local branch library, see page table may contain entries pages present memory. indicate location page table memory, hardware includes register points start page table; call page table register . Assume page table fixed contiguous area memory. page table table containing virtual physical address translations n virtual memory system. table, stored memory, typically indexed virtual page number; entry table contains physical page number virtual page page currently memory. Hardware/Software Interface page table, together program counter 826registers, specifies state virtual machine. want allow another virtual machine use processor, must save state. Later, restoring state, virtual machine continue execution. often refer state process . process considered active possession processor; otherwise, considered inactive . operating system make process active loading process’s state, including program counter, initiate execution value saved program counter. process’s address space, hence data access memory, defined page table, resides memory. Rather save entire page table, operating system simply loads page table register point page table process wants make active. process page table, since different processes use virtual addresses. operating system responsible allocating physical memo ry updating page tables, virtual address spaces distinct processes collide. see shortly, u se separate page tables also provides protection one process another. Figure 5.27 uses page table register, virtual address, indicated page table show hardware form physical address. valid bit used page table entry, cache. bit off, page present main memory page fault occurs. bit on, page memory entry contains physical page number. 827FIGURE 5.27 page table indexed virtual page number obtain corresponding portion physical address. assume 48-bit address. page table pointer gives starting address page table. figure, page size 212 bytes, 4 KiB. virtual address space 248 bytes, 256 TiB, physical address space 240 bytes, allows main memory 1 TiB. RISC-V used single page table shown figure, number entries page table would 236, 64 billion entries. (We’ll see RISC-V reduce number entries shortly.) valid bit entry indicates whether mapping legal. off, page present memory. Although page table entry shown need 29 bits wide, would typically rounded power 2 bits ease indexing. page table entries RISC-V 64 bit s. extra bits would used store additional information needs kept per-page basis, 828such protection. page table contains mapping every possible virtual page, tags required. cache terminology, index used access page table consists full block address, case virtual page number. Page Faults valid bit virtual page off, page fault occurs. operating system must given control. transfer done wi th exception mechanism, saw Chapter 4 discuss later section. operating system gets control, must find page next level hierarchy (usually flash memory magnetic disk) decide place requested page main memory. virtual address alone immediately tell us page secondary memory. Returning library analogy, cannot find location library book shelves knowing title. Instead, go catalog look book, obtaining address location shelves, Library Congress call number. Likewise, virtual memory system, must keep track location secondary memory page virtual address space. know ahead time page memory replaced, operating system usually creates space flash memory disk pages process creates process. space called swap space . time, also creates data structure record virtual page stored disk. data structure may part page table may auxiliary data structure indexed way page table. Figure 5.28 shows organization single table holds either physical page number secondary memory address. swap space space disk reserved full virtual memory space process. 829FIGURE 5.28 page table maps page virtual memory either page main memory page stored disk, next level hierarchy. virtual page number used index page table. valid bit on, page table supplies physical page number (i.e., starting address page memory) corresponding virtual page. valid bit off, page currently resides disk, specified disk address. many systems, table physical page addresses disk page addresses, logically one table, stored two separate data structures. Dual tables justified part must keep disk addresses pages, even currently main memory. Remember pages main memory pages disk size. operating system also creates data structure tracks processes virtual addresses use physical page. page fault occurs, pages main memory use, operating system must choose page replace. want minimize number page faults, operating 830systems try choose page hypothesize needed soon. Using past predict future, operating systems follow least recently used (LRU) replacement scheme, mentioned Section 5.4 . operating system searches least recently used page, assuming page used long time less likely needed recently accessed page. replaced pages written swap space secondary memory. case wondering, operating system another process, tables controlling memory memory; details seeming contradiction explained shortly. Hardware/Software Interface Implementing completely accurate LRU scheme expensive , since requires updating data structure every memory reference. Thus, operating systems approximate LRU keeping track pages pages recently used. help operating system estimate LRU pages, RISC-V computers provide reference bit , sometimes called use bit access bit , set whenever page accessed. operating system periodically clears reference bits later records determine pages touched particular time period. usage information, operating system select page among least recently referenced (detected reference bi off). bit provided hardware, operating system must find another way estimate pages accessed. reference bit Also called use bit access bit . field set whenever page accessed used implement LRU replacement schemes. Virtual Memory Large Virtual Addresses caption Figure 5.27 points single level page table 48-bit address 4 KiB pages, need 64 billion table 831entries. page table entry 8 bytes RISC-V, would require 0.5 TiB map virtual addresses physical addresses! Moreover, could hundreds processes running, page table. much memory translation would unaffordable even largest systems. range techniques used reduce amount storage required page table. five techniques aim reducing total maximum storage required well minimizing main memory dedicated page tables: 1. simplest technique keep limit register restric ts size page table given process. virtual page number becomes larger contents limit register, entries mu st added page table. technique allows page table grow process consumes space. Thus, page table large process using many pages virtual address space. technique requires address space expand jus one direction. 2. Allowing growth one direction sufficient, since languages require two areas whose size expandable: one area holds stack, area holds heap. duality, convenient divide page table let gr ow highest address down, well lowest address up. means two separate page tables two separate limits. use two page tables breaks address space two segments. high-order bit address usually determines segment thus page table use address. Since high-order address bit specifies seg ment, segment large one-half address space. limit register segment specifies current size segment, grows units pages. Unlike type segmentation discussed second elaboration page 423, form segmentation invisible application program, although operating system. major disadvantage scheme work well address space used sparse fashion rather contiguous set virtual addresses. 3. Another approach reducing page table size apply hashing function virtual address page table need size number physical pages main memory. 832Such structure called inverted page table . course, lookup process slightly complex inverted page table, longer index page table. 4. reduce actual main memory tied page tables, modern systems also allow page tables paged. Although sounds tricky, works using basic ideas virtual memory simply allowing page tables reside virtual address space. addition, small critical problems, never-ending series page faults, must avoided. problems overcome detailed typically highly processor-specific. brief, probl ems avoided placing page tables address space operating system placing least page tables operating system portion main memory physically addressed always present thus never secondary memory. 5. Multiple levels page tables also used reduce total amount page table storage, solution RISC-V uses reduce memory footprint address translation. Figure 5.29 shows four levels address translation go 48-bit virtual address 40-bit physical address 4 KiB page. Address translation happens first looking level 0 table, using highest-order bits address. address thi table valid, next set high-order bits used index page table indicated segment table entry, on. Thus, level 0 table maps virtual address 512 GB (239 bytes) region. level 1 table turn maps virtual address 1 GB (230) region. next level maps 2 MB (221) region. final table maps virtual address 4 KiB (212) memory page. scheme allows address space used sparse fashion (multiple noncontiguous segments active) withou allocate entire page table. schemes particularly useful large address spaces software systems require noncontiguous allocation. primary disadvantage multi-level mapping complex process address translation. Writes? 833The difference access time cache main memory tens hundreds cycles, write-through scheme used, although need write buffer hide latency write processor. virtual memory system, writes next level hierarchy (disk) take millions proce ssor clock cycles; therefore, building write buffer allow syst em write-through disk would completely impractical. Inst ead, virtual memory systems must use write-back, performing individual writes page memory, copying page back secondary memory replaced main memory. Hardware/Software Interface write-back scheme another major advantage virtual memory system. disk transfer time small compared access time, copying back entire page much efficient writing individual words back disk. write - back operation, although faster transferring separate words, still costly. Thus, would like know whether page needs copied back choose replace it. track whether page written since read memory, dirty bit added page table. dirty bit set word page written. operating system chooses replace pag e, dirty bit indicates whether page needs written location memory given another page. Hence, modified page often called dirty page. Making Address Translation Fast: TLB Since page tables stored main memory, every memory access program take least twice long: one memory access obtain physical address second access get data. key improving access performance rely locality reference page table. translation virtual page number used, probably needed soon, references words page temporal spatial locality. Accordingly, modern processors include special cache keeps track recently used translations. special address translation cache traditionally referred translation- 834lookaside buffer (TLB) , although would accurate call translation cache. TLB corresponds little piece paper typically use record location set books look card catalog; rather continually searching entire catalog, record location several books use scrap paper cache Library Congress call numbers. translation-lookaside buffer (TLB) cache keeps track recently used address mappings try avoid access page table. Figure 5.30 shows tag entry TLB holds portion virtual page number, data entry TLB holds physical page number. access TLB instead page table every reference, TLB need include status bits, dirty reference bits. Although Figure 5.30 shows single page table, TLBs work fine multi-level page tables well. TLB simply loads physical address protection tags last level page table. FIGURE 5.29 RISC-V uses four levels tables translate 48-bit virtual address 40-bit physical address. Rather needing 64 billion page table entries fo r single page table Figure 5.27 , hierarchical approach needs tiny fraction. step translation uses 9 bits virtual address find 835next level table, upper 36 bits virtua l address mapped physical address desired 4 KiB page. RISC-V page table entry 8 bytes, 512 entries table fill single 4 KiB page. Supervisor Page Table Base Register (SPTBR) gives starting address first page table. 836FIGURE 5.30 TLB acts cache page table entries map physical pages only. TLB contains subset virtual-to-physical page mappings page table. TLB mappings shown color. TLB cache, must tag field. matching entry TLB page, page table must examined. page table either supplies physical page number page (which used build TLB entry) indicates page resides disk, case page fault occurs. Since page table entry every virtual page, tag field needed; words, unlike TLB, page table cache. every reference, look virtual page number TLB. get hit, physical page number used form address, corresponding reference bit turned on. processor performing write, dirty bit also turned on. f miss TLB occurs, must determine whether page fault merely TLB miss. page exists memory, TLB miss indicates translation missing. 837cases, processor handle TLB miss loading translation (last-level) page table TLB trying reference again. page present memory, TLB miss indicates true page fault. case, processor invokes operating system using exception. Bec ause TLB many fewer entries number pages main memory, TLB misses much frequent true page faults. TLB misses handled either hardware software. practice, care little performance difference two approaches, basic operations either case. TLB miss occurs missing translation retrieved page table, need select TLB entry replace. reference dirty bits contained TLB entry, need copy bits back page table entry replace entry. bits portion TLB entry changed. Using write-back—that is, copying entries back miss time rather written— efficient, since expect TLB miss rate small. systems use techniques approximate reference dirty bits, eliminating need write TLB except load new table entry miss. typical values TLB might TLB size: 16–512 entries Block size: 1–2 page table entries (typically 4–8 bytes each) Hit time: 0.5–1 clock cycle Miss penalty: 10–100 clock cycles Miss rate: 0.01%–1% Designers used wide variety associativities TLBs. systems use small, fully associative TLBs fully associative mapping lower miss rate; furthermore, since TLB small, cost fully associative mapping high. systems use large TLBs, often small associativity. fully associative mapping, choosing entry replace becomes tricky since implementing hardware LRU scheme expensive. Furthermore, since TLB misses much frequen page faults thus must handled cheaply, cannot afford expensive software algorithm, page 838faults. result, many systems provide support randomly choosing entry replace. We’ll examine replacement schemes little detail Section 5.8 . Intrinsity FastMATH TLB see ideas real processor, let’s take closer look TLB Intrinsity FastMATH. memory system uses 4 KiB pages 32-bit address space; thus, virtual page number 20 bits long. physical address size virtual address. TLB contains 16 entries, fully associative, shared instruction data references. entry 64 bits wide contains 20-bit tag (which virtual page number TLB entry), corresponding physical page number (also 20 bits), valid bit, dirty bit, bookkeeping bits. Like MIPS systems, uses software handle TLB misses. Figure 5.31 shows TLB one caches, Figure 5.32 shows steps processing read write request. TLB miss occurs, hardware saves page number reference special register generates exception. exception invokes operating system, handles miss n software. find physical address missing page, TLB miss indexes page table using page number virtual address page table register, indicates starting address active process page table. Using special set system instructions update TLB, operating system places physical address page table TLB. TLB miss takes 13 clock cycles, assuming code page table entry instruction cache data cache, respectively. true page fault occurs page table entry valid physical address. hardware maintains index indicates recommended entry replace; chosen randoml y. 839FIGURE 5.31 TLB cache implement process going virtual address data item Intrinsity FastMATH. figure shows organization TLB data cache, assuming 4 KiB page size. Note address size computer 32 bits. diagram focuses read; Figure 5.32 describes handle writes. Note unlike Figure 5.12 , tag data RAMs split. addressing long narrow data RAM cache index concatenated block offset, select desired word block without 16:1 multiplexor. cache direct mapped, TLB fully associative. Implementing fully associative TLB requires every TLB tag compared virtual page number, since entry interest anywhere 840the TLB. (See content addressable memories Elaboration page 400.) valid bit matching entry on, access TLB hit, bits physical page number together bits page offset form index used access cache. 841FIGURE 5.32 Processing read write-through Intrinsity FastMATH TLB cache. TLB generates hit, cache accessed resulting physical address. read, cache generates hit miss supplies data causes stall data brought memory. operation write, portion cache en try overwritten hit data sent write buffer assume write-through. write miss like read miss except block modified read memory. Write-back requires writes set dirty bit cache block, write buffer loaded whole block read miss write miss block replaced dirty. Notice TLB hit cache hit independent events, cache hit occur TLB hit occurs, means data must present memory. relationship TLB misses cache misses examined following example exercises end chapter. Note 842the address size computer 32 bits. extra complication write requests: namely, write access bit TLB must checked. bit prevents program writing pages read access. program attempts write write access bit off, exception generated. write access bit forms part protection mechanism, discuss shortly. Integrating Virtual Memory, TLBs, Caches virtual memory cache systems work together hierarchy, data cannot cache unless present main memory. operating system helps maintain hierarchy flushing contents page cache decides migrate page secondary memory. time, OS modifies page tables TLB, attempt access data migrated page generate page fault. best circumstances, virtual address translated TLB sent cache appropriate data found, retrieved, sent back processor. worst case, reference miss three components memory hierarchy: TLB, page table, cache. following example illustrates interactions detail. Overall Operation Memory Hierarchy Example memory hierarchy like Figure 5.31 , includes TLB cache organized shown, memory reference encounter three different types misses: TLB miss, page fault, cache miss. Consider combinations three events one occurring (seven possibilities). e ach possibility, state whether event actually occur circumstances. Answer Figure 5.33 shows combinations whether possible 843practice. FIGURE 5.33 possible combinations events TLB, virtual memory system, cache. Three combinations impossible, one possible (TLB hit, page table hit, cache miss) never detected. Elaboration Figure 5.33 assumes memory addresses translated physical addresses cache accessed. organization, cache physically indexed physically tagged (both cache index tag physical, rather virtual, addresses). system, amount time access memory, assuming cache hit, must accommodate TLB access cache access; course, accesses pipelined . 844Alternatively, processor index cache address completely partially virtual. called virtually addressed cache , uses tags virtual addresses; hence, cache virtually indexed virtually tagged . caches, address translation hardware (TLB) unused normal cache access, since cache accessed virtual address translated physical address. takes TLB critical path, reducing cache latency. cache miss occurs, however, processor needs translate address physical address fetch cache block main memory. virtually addressed cache cache accessed virtual address rather physical address. cache accessed virtual address pages shared processes (which may access different virtual addresses), possibility aliasing . Aliasing occurs object two names—in case, two virtual addresses page. ambiguity creates problem, word page may cached two 845different locations, corresponding distinct virtual addresses. ambiguity would allow one program write data without program aware data changed. Completely virtually addressed caches either introduc e design limitations cache TLB reduce aliases require operating system, possibly user, take steps ensure aliases occur. 846 aliasing situation two addresses access object; occur virtual memory two virtual addresses physical page. common compromise two design points caches virtually indexed—sometimes using page- offset portion address, really physical address since translated—but use physical tags. designs, virtually indexed physically tagged , attempt achieve performance advantages virtually indexed caches architecturally simpler advantages physically addressed cache . example, alias problem case. Figure 5.31 assumed 4 KiB page size, it’s really 16 KiB, Intrinsity FastMATH use trick. pull off, must careful coordination minimum page size, cache size, associativity. RISC-V requires caches behave though physically tagged indexed, mandate implementation. example, virtually indexed, physically tagged data caches could use additional logic ensure software cannot tell difference. physically addressed cache cache addressed physical address. Implementing Protection Virtual Memory Perhaps important function virtual memory today allow sharing single main memory multiple processes, providing memory protection among processes operating system. protection mechanism must ensure although multiple processes sharing main memory, one renegade process cannot write address space another user process operating system either intentionally unintentionally. write access bit TL B protect page written. Without level protecti on, 847computer viruses would even widespread. Hardware/Software Interface enable operating system implement protection virtual memory system, hardware must provide least three basic capabilities summarized below. Note first two requirements needed virtual machines ( Section 5.6). 1. Support least two modes indicate whether running process user process operating system process, variously called supervisor process, kernel process, executive process. 2. Provide portion processor state user process read write. state includes user/supervisor mod e bit, dictates whether processor user supervis mode, page table pointer, TLB. write elements, operating system uses special instructions ar e available supervisor mode. 3. Provide mechanisms whereby processor go user mode supervisor mode vice versa. first direction typically accomplished system call exception, implemented special instruction ( ecall RISC-V instruction set) transfers control dedicated location supervisor code spac e. exception, program counter point system call saved supervisor exception program counter (SEPC), processor placed supervisor mode. return user mode exception, use supervisor exception return (sret ) instruction, resets user mode jumps address SEPC. using mechanisms storing page tables operating system’s address space, operating system change page tables preventing user process changing them, ensuring user process access storage provided operating system. supervisor mode Also called kernel mode . mode indicating running process operating system process. 848 system call special instruction transfers control user mode dedicated location supervisor code space, invoking exception mechanism process. also want prevent process reading data another process. example, wouldn’t want student program read teacher’s grades processor’s memory. begin sharing main memory, must provide ability process protect data reading writing another process; otherwise, sharing main memory mixed blessing! Remember process virtual address space. Thus, operating system keeps page tables organized independent virtual pages map disjoint physical pages, one process able access another’s data. course, also requires user process unable change page table mapping. operating system assure safety prevents user process modifying page tables. However, operating system must able modify page tables. Placing page tables protected address space operating system satisfies requirements. processes want share information limited way, operating system must assist them, since accessing informatio n another process requires changing page table accessi ng process. write access bit used restrict sharing read sharing, and, like rest page table, bit changed operating system. allow another process, say, P1, read page owned process P2, P2 would ask operating system create page table entry virtual page P1’s address space points physical page P2 wants share. operating system could use write protecti bit prevent P1 writing data, P2’s wish. bits determine access rights page must included page table TLB, page table accessed TLB miss. Elaboration 849When operating system decides change running process P1 running process P2 (called context switch process switch ), must ensure P2 cannot get access page tables P1 would compromise protection. TLB, suffices change page table register point P2’ page table (rather P1’s); TLB, must clear TLB entries belong P1—both protect data P1 force TLB load entries P2. process switch rate high, could quite inefficient. example, P2 might load TLB entries operating system switched back P1. Unfortunately, P1 would find TLB entries gone would pay TLB misses reload them. problem arises virtual addresses used P1 P2 same, must clear TLB avoid confusing addresses. common alternative extend virtual address space adding process identifier task identifier . Intrinsity FastMATH 8-bit address space ID (ASID) field purpose. small field identifies currently running process; ke pt register loaded operating system switches process es. RISC-V also offers ASID reduce TLB flushes context switc hes. process identifier concatenated tag portion TLB, TLB hit occurs page number process identifier match. combination eliminates need clear TLB, except rare occasions. Similar problems occur cache, since process switch, cache contain data running process. problems arise different ways physically addressed virtually addressed caches, variety solutions, process identifiers, used ensure process gets data. context switch changing internal state processor allow differe nt process use processor includes saving state needed return currently executing process. Handling TLB Misses Page Faults 850Although translation virtual physical addresses TLB straightforward get TLB hit, saw earlier, handling TLB misses page faults complex. TLB miss occurs entry TLB matches virtual address. Recall TLB miss indicate one two possibilities: 1. page present memory, need create missing TLB entry. 2. page present memory, need transfer control operating system deal page fault. Handling TLB miss page fault requires using exception mechanism interrupt active process, transferring control operating system, later resuming execution interrupted process. page fault recognized sometime clock cycle used access memory. restart instruction page fault handled, program counter instruction caused page fault must saved. supervisor exception program counter (SEPC) register used hold value. addition, TLB miss page fault exception must asserted end clock cycle memory access occurs, next clock cycle begin exception processing rat continue normal instruction execution. page fault recognized clock cycle, load instruction could overwrite register, could disastrous try restart instruction. example, consider instruction lb x10, 0(x10) : computer must able prevent write pipeline stage occurring; otherwise, could properly restart instruction, since contents x10 would destroyed. similar complication arises stores. must prevent write memory actually completing page fault; usually done deasserting write control line memory. Hardware/Software Interface time begin executing exception handler operating system time operating system saved state process, operating system particularly vulnerable. instance, another exception occurred 851were processing first exception operating system, th e control unit would overwrite exception link register, mak ing impossible return instruction caused page fault ! avoid disaster providing ability disable enable exceptions . exception first occurs, processor sets bit disables exceptions; could happen time processor sets supervisor mode bit. operating system save enough state allow recover another exception occurs—namely, supervisor exception rogram counter (SEPC) supervisor exception cause (SCAUSE) registers, saw Chapter 4 records reason exception. SEPC SCAUSE RISC-V two special control registers help exceptions, TLB mi sses, page faults. operating system re-enable exceptions. steps make sure exceptions cause processor lose state thereby unable restart execution interrupting instruction. exception enable Also called interrupt enable. signal action controls whether process responds exception not; necessary preventing occurrence exceptions intervals bef ore processor safely saved state needed restart. operating system knows virtual address caused page fault, must complete three steps: 1. Look page table entry using virtual address find location referenced page secondary memory. 2. Choose physical page replace; chosen page dirty, must written secondary memory bring new virtual page physical page. 3. Start read bring referenced page secondary memory chosen physical page. course, last step take millions processor clock cycles disks (so second replaced page dirt y); accordingly, operating system usually select another process execute processor disk access comp letes. operating system saved state process, 852can freely give control processor another process. read page secondary memory complete, operating system restore state process originally caused page fault execute instruction returns exception. instruction reset p rocessor kernel user mode, well restore program counter. user process re-executes instruction faulte d, accesses requested page successfully, continues execut ion. Page fault exceptions data accesses difficult implement properly processor combination three characteristics: 1. occur middle instructions, unlike instruction p age faults. 2. instruction cannot completed handling exception. 3. handling exception, instruction must restarted nothing occurred. Making instructions restartable , exception handled instruction later continued, relatively easy architecture like RISC-V. instruction writes onl one data item write occurs end instruction cycle, simply prevent instruction completing (b writing) restart instruction beginning. restartable instruction instruction resume execution exception resolved without exception’s affecting result instruction. Elaboration processors complex instructions touch many memory locations write many data items, making instructions restartable much harder. Processing one instruct ion may generate number page faults middle instruction. example, x86 processors block move instructions touch thousands data words. processors, instructions often cannot restarted beginning, RISC-V instructions. Instead, 853instruction must interrupted later continued midstream execution. Resuming instruction middle execution usually requires saving special state, processing exception, restoring special state. Making work properly requires careful detailed coordination th e exception-handling code operating system hardware. Elaboration Rather pay extra level indirection every memory access, Virtual Memory Monitor ( Section 5.6 ) maintains shadow page table maps directly guest virtual address space physical address space hardware. detecting modifications guest’s page table, VMM ensure shadow page table entries used hardware translations correspond guest OS environment, wit h exception correct physical pages substituted real pages guest tables. Hence, VMM must trap attempt guest OS change page table access page table pointer. commonly done write protecting guest p age tables trapping access page table pointer guest OS. noted above, latter happens naturally accessing page table pointer privileged operation. Elaboration final portion architecture virtualize I/O. far difficult part system virtualization increasing number I/O devices attached computer expanding diversity I/O device types. Another difficulty sharing real device among multiple VMs, yet another comes supporting myriad device drivers required, especially different guest OSes supported VM system. VM illusion maintained giving VM generic versions type I/O device driver, leaving VMM handle real I/O. Elaboration 854In addition virtualizing instruction set virtual machin e, another challenge virtualization virtual memory, guest OS every virtual machine manages set page tables. make work, VMM separates notions real physical memory (which often treated synonymously), makes real memory separate, intermediate level virtual memory physical memory. (Some use terms virtual memory, physical memory , machine memory name three levels.) guest OS maps virtual memory real memory via page tables, VMM page tables map guest’s real memory physical memory. virtual memory architecture typically specified via page tables, IBM VM/370, x86, RISC-V. Summary Virtual memory name level memory hierarchy manages caching main memory secondary memory. Virtual memory allows single program expand address space beyond limits main memory. importantly, virtual memory supports sharing main memory among multiple, simultaneously active processes, protected manner. Managing memory hierarchy main memory disk challenging high cost page faults. Several techniques used reduce miss rate: 1. Pages made large take advantage spatial locality reduce miss rate. 2. mapping virtual addresses physical addresses, implemented page table, made fully associative virtual page placed anywhere main memory. 3. operating system uses techniques, LRU reference bit, choose pages replace. Writes secondary memory expensive, virtual memory uses write-back scheme also tracks whether page unchanged (using dirty bit) avoid writing clean pages. virtual memory mechanism provides address translation virtual address used program physical address space used accessing memory. address translation allows protected sharing main memory provides several 855additional benefits, simplifying memory allocation. Ensuring processes protected requires th operating system change address translations, implemented preventing user programs alterin g page tables. Controlled sharing pages processes implemented help operating system access bits page table indicate whether user program read write access page. processor access page table resident memory translate every access, virtual memory would expensive, caches would pointless! Instead, TLB acts cache translations page table. Addresses translated virtual physical using translations TLB. Caches, virtual memory, TLBs rely common set principles policies. next section discusses comm framework. Understanding Program Performance Although virtual memory invented enable small memory act large one, performance difference secondary memory main memory means program routinely accesses virtual memory physical memory, run slowly. program would continuously swapping pages main memory secondary memory, called thrashing . Thrashing disaster occurs, rare. program thrashes, easiest solution run computer memory buy memory computer. complex choice re-examine algorithm data structures see change locality thereby reduc e number pages program uses simultaneously. set popular pages informally called working set . common performance problem TLB misses. Since TLB might handle 32–64 page entries time, program could easily see high TLB miss rate, processor may access less quarter mebibyte directly: 64 × 4 KiB =0.25 MiB. example, TLB misses often challenge Radix Sort. try alleviate problem, computer architectures offer support larger page sizes. instance, addition 856minimum 4 KiB page, RISC-V hardware supports 2 MiB 1 GiB pages. Hence, program uses large page sizes, access memory directly without TLB misses. practical challenge getting operating system allow programs select larger page sizes. again, complex solution reducing TLB misses re-examine algorithm data structures reduce working set pages; given importance memory accesses performance frequency TLB misses, programs large working sets redesigned goal. Elaboration RISC-V supports larger page sizes via multi-level page tabl e Figure 5.29 . addition pointing next level page table levels 1 2, allows superpage translation map virtual address 1 GiB physical address (if block translation level 1) 2 MiB physical address (if block translation level 2). 5.8 Common Framework Memory Hierarchy now, you’ve recognized different types memory hierarchies great deal common. Although many aspects memory hierarchies differ quantitatively, many policies features determine hierarchy functions similar qualitatively. Figure 5.34 shows quantitative characteristics memory hierarchies differ. rest section, discuss common operational alternatives memory hierarchies, determine behavior. examine policies series four questions apply two levels memory hierarchy, although simplicity, primarily use terminology caches. 857FIGURE 5.34 key quantitative design parameters characterize major elements memory hierarchy computer. typical values levels 2012. Although range values wide, partially many values shifted time related; example, caches become larger overcome larger miss penalties, block sizes also grow. shown, server microprocessors today also L3 caches, 2 8 MiB contain many blocks L2 caches. L3 caches lower L2 miss penalty 30 40 clock cycles. Question 1: Block Placed? seen block placement upper level hierarchy use range schemes, direct mapped set associative fully associative. mentioned above, entire range schemes thought variations set- associative scheme number sets number blocks per set varies: advantage increasing degree associativity usually decreases miss rate. improvement miss rate comes reducing misses compete location. examine detail shortly. First, let’s look much improvement gained. Figure 5.35 shows miss rates several cache sizes associativity varies direct mapped 858eight-way set associative. largest gains obtained going direct mapped two-way set associative, yields 20% 30% reduction miss rate. cache sizes grow, relative improvement associativity increases onl slightly; since overall miss rate larger cache lower, opportunity improving miss rate decreases absolut e improvement miss rate associativity shrinks significantly. potential disadvantages associativity, mentioned earlier, increased cost slower access time. FIGURE 5.35 data cache miss rates eight cache sizes improve associativity increases. benefit going one-way (direct mapped) two-way set associative significant, benefits associativity smaller (e.g., 1– 10% improvement going two-way four-way versus 20–30% improvement going one-way two-way). even less improvement going four-way eight-way set associative, which, turn, comes close miss rates fully associative cache. Smaller caches obtain significantly larger absolute benefit associativity 859because base miss rate small cache larger. Figure 5.16 explains data collected. Question 2: Block Found? choice locate block depends block placement scheme, since dictates number possible locations. summarize schemes follows: choice among direct-mapped, set-associative, fully associative mapping memory hierarchy depend cost miss versus cost implementing associativity, bot h time extra hardware. Including L2 cache chip enables much higher associativity, hit times critical designer rely standard SRAM chips building blocks. Fully associative caches prohibi tive except small sizes, cost comparators overwhelming absolute miss rate improvements greatest. virtual memory systems, separate mapping table—the page table—is kept index memory. addition storage needed table, using index table requires extra memory access. choice full associativity page placement extra table motivated facts: 1. Full associativity beneficial, since misses expensiv e. 2. Full associativity allows software use sophisticated replacement schemes designed reduce miss rate. 3. full map easily indexed extra hardware searching required. Therefore, virtual memory systems almost always use fully associative placement. 860Set-associative placement often used caches TLBs, access combines indexing search small set. systems used direct-mapped caches advantage access time simplicity. advantage access time occurs finding requested block depe nd comparison. design choices depend many details implementation, whether cache on-chip, technology used implementing cache, critical role cache access time determining processor cycle time. Question 3: Block Replaced Cache Miss? miss occurs associative cache, must decide block replace. fully associative cache, blocks candidates replacement. cache set associative, must choose among blocks set. course, replacement easy direct-mapped cache one candidate. two primary strategies replacement set- associative fully associative caches: Random: Candidate blocks randomly selected, possibly using hardware assistance. Least recently used (LRU): block replaced one unused longest time. practice, LRU costly implement hierarchies small degree associativity (two four, typically), since tracking usage information expensive. Even four- way set associativity, LRU often approximated—for example, keeping track pair blocks LRU (which requires 1 bit) , tracking block pair LRU (which requires 1 bit per pair). larger associativity, either LRU approximated random replacement used. caches, replacement algorithm hardware, means scheme easy implement. Random replacement simple build hardware, two-way set-associative cache, random replacement miss rate 1.1 times higher LRU replacement. caches become larger, miss rate replacement strategie falls, absolute difference becomes small. fact, random 861replacement sometimes better simple LRU approximations easily implemented hardware. virtual memory, form LRU always approximated, since even tiny reduction miss rate important cost miss enormous. Reference bits equivalent functionality often provided make easier operati ng system track set less recently used pages. misses expensive relatively infrequent, approximating information primarily software acceptable. Question 4: Happens Write? key characteristic memory hierarchy deals writes. already seen two basic options: Write-through: information written block cache block lower level memory hierarchy (main memory cache). caches Section 5.3 used scheme. Write-back: information written block cache. modified block written lower level hierarchy replaced. Virtual memory systems always use write-back, reasons discussed Section 5.7 . write-back write-through advantages. key advantages write-back following: Individual words written processor rate cache, rather memory, accept them. Multiple writes within block require one write lo wer level hierarchy. blocks written back, system make effective use high-bandwidth transfer, since entire block written. Write-through advantages: Misses simpler cheaper never require block written back lower level. Write-through easier implement write-back, although realistic, write-through cache still need use write buffer. BIG Picture Caches, TLBs, virtual memory may initially look 862different, rely two principles locality, understood answers four questions: Question 1:Where block placed? Answer: One place (direct mapped), places (set associative), place ( fully associative). Question 2:How block found? Answer: four methods: indexing (as direct-mapped cache), limi ted search (as set-associative cache), full search (as fully associative cache), separate lookup table (as page table). Question 3:What block replaced miss? Answer: Typically, either least recently used random block. Question 4:How writes handled? Answer: level hierarchy use either write-through writ e-back. virtual memory systems, write-back policy practical long latency write lower level hierarchy. rate writes generated processor generally exceeds rate memory system process them, even allowing physically logically wider memories burst modes DRAM. Consequently, today lowest-level caches typically use write-back. Three Cs: Intuitive Model Understanding Behavior Memory Hierarchies subsection, look model provides insight sources misses memory hierarchy misses affected changes hierarchy. explain ideas terms caches, although ideas carry directly level hierarchy. model, misses classified one three categories (the three Cs ): three Cs model cache model cache misses classified one three categories: compulsory misses, capacity misses, conflic misses. 863 Compulsory misses : cache misses caused first access block never cache. also called cold-start misses . Capacity misses : cache misses caused cache cannot contain blocks needed execution program. Capacity misses occur blocks replaced later retrieved. Conflict misses : cache misses occur set- associative direct-mapped caches multiple blocks compete set. Conflict misses misses direct-mapped set-associative cache eliminated fully associative cache size. cache misses also called collision misses . compulsory miss Also called cold-start miss . cache miss caused first access block never cache. capacity miss cache miss occurs cache, even full associativity, cannot contain blocks needed satisfy request. conflict miss Also called collision miss . cache miss occurs set- associative direct-mapped cache multiple blocks compet e set eliminated fully associative cache size. Figure 5.36 shows miss rate divides three sources. sources misses directly attacked changing aspect cache design. Since conflict misses arise straight contention cache block, increasing associativity reduces conflict misses. Associativity, however , may slow access time, leading lower overall performance. 864FIGURE 5.36 miss rate broken three sources misses. graph shows total miss rate components range cache sizes. data SPEC CPU2000 integer floating-point benchmarks source data Figure 5.35 . compulsory miss component 0.006% cannot seen graph. next component capacity miss rate, depends cache size. conflict portion, depends associativity cache size, shown range associativities one-way eight-way. case, labeled section corresponds increase miss rate occurs associativity changed next higher degree labeled degree associativity. example, section labeled two-way indicates additional misses arising cache associativity two rather four. Thus, difference miss rate incurred direct-mapped cache versus fully associative cache size given sum sections marked four-way, two-way , one- way. difference eight-way four-way small difficult see graph. Capacity misses easily reduced enlarging cache; 865indeed, second-level caches growing steadily bigger f many years. course, make cache larger, must also careful increasing access time, could lead lower overall performance. Thus, first-level caches growing slowly, all. compulsory misses generated first reference block, primary way cache system reduce number compulsory misses increase block size. red uce number references required touch block program once, program consist fewer cache blocks. mentioned above, increasing block size much negative effect performance increase miss penalty. BIG Picture challenge designing memory hierarchies every change potentially improves miss rate also negatively affect overall performance, Figure 5.37 summarizes. combination positive negative effects makes design memory hierarchy interesting. FIGURE 5.37 Memory hierarchy design challenges. decomposition misses three Cs useful qualitative model. real cache designs, many design choices interact, changing one cache characteristic often affect several components miss rate. Despite shortcomings, model useful way gain insight performance cache designs. 866Check following statements (if any) generally true? 1. way reduce compulsory misses. 2. Fully associative caches conflict misses. 3. reducing misses, associativity important capacity. 5.9 Using Finite-State Machine Control Simple Cache build control cache, implemented control single-cycle pipelined datapaths Chapter 4 . section starts definition simple cache description finite-state machines (FSMs). finishes FSM controller simple cache. Section 5.12 goes depth, showing cache controller new hardware description language. Simple Cache We’re going design controller straightforward cache. key characteristics cache: Direct-mapped cache Write-back using write allocate Block size four words (16 bytes 128 bits) Cache size 16 KiB, holds 1024 blocks 32-bit addresses cache includes valid bit dirty bit per block Section 5.3 , calculate fields address cache: Cache index 10 bits Block offset 4 bits Tag size 32−(10+4) 18 bits signals processor cache 1-bit Read Write signal 1-bit Valid signal, saying whether cache operation 32-bit address 32-bit data processor cache 867 32-bit data cache processor 1-bit Ready signal, saying cache operation complete interface memory cache fields processor cache, except data fields 128 bits wide. extra memory width generally found microprocessors today, deal either 32-bit 64- bit words processor DRAM controller often 128 bits. Making cache block match width DRAM simplified design. signals: 1-bit Read Write signal 1-bit Valid signal, saying whether memory operation 32-bit address 128-bit data cache memory 128-bit data memory cache 1-bit Ready signal, saying memory operation complete Note interface memory fixed number cycles. assume memory controller notify cache via Ready signal memory read write finished. describing cache controller, need review fini te- state machines, allow us control operation take multiple clock cycles. Finite-State Machines design control unit single-cycle datapath, used truth tables specified setting control signals base instruction class. cache, control complex operation series steps. control cache must specify signals set step next step sequence. common multistep control method based finite- state machines , usually represented graphically. finite-state machine consists set states directions change states. directions defined next-state function , maps current state inputs new state. use finite-state machine control, state also specifies set outputs asserted machine state. implementation finite-state machine usually 868assumes outputs explicitly asserted deasserted. Similarly, correct operation datapath depend fact signal explicitly asserted deasserted , rather acting don’t care. finite-state machine sequential logic function consisting set inputs outputs, next-state function maps current state inputs new state, output function maps current state possibly inputs set asserted outputs. next-state function combinational function that, given inputs current state, determines next state finite-state machine. Multiplexor controls slightly different, since select one inputs, whether 0 1. Thus, finite-state machine, always specify setting multiplexor controls th care about. implement finite-state machine logic, setting control 0 may default therefore may require gates. simple example finite-state machine appears Appendix , unfamiliar concept finite-state machine, may want examine Appendix proceeding. finite-state machine implemented temporary register holds current state block combinational logic determines data-path signals asserted next state. Figure 5.38 shows implementation might look. Appendix C describes detail finite-state machine implemented using structure. Section A.3 , combinational control logic finite-state machine implemented either ROM ( read-only memory ) PLA (programmable logic array ). (Also see Appendix description logic elements.) Elaboration 869Note simple design called blocking cache, processor must wait cache finished request. Section 5.12 describes alternative, called nonblocking cache. 870FIGURE 5.38 Finite-state machine controllers typically implemented using block combinational logic register hold current state. outputs combinational logic next- state number control signals asserted current state. inputs combinational logi c current state inputs used determine next state. Notice finite-state machine used chapter, outputs depend current state, inputs. use color indicate control lines logic versus data lines logic. Elaboration explains detail. Elaboration style finite-state machine book called Moore machine, Edward Moore. identifying characteristic 871the output depends current state. Moore machine, box labeled combinational control logic split two pieces. One piece control output state input, next-state output. alternative style machine Mealy machine, named George Mealy. Mealy machine allows input current state used determine output. Moore machines potential implementation advantages speed size control unit. speed advantages arise control outputs, needed early clock cycle, depend inputs, current state. Appendix , implementation finite-state machine taken logic gates, size advantage clearly seen. potential disadvantage Moore machine may require additional states. example, situations one-state difference two sequences states, Mealy machine may unify states making outputs depend inputs. FSM Simple Cache Controller Figure 5.39 shows four states simple cache controller: Idle: state waits valid read write request processor, moves FSM Compare Tag state. Compare Tag: name suggests, state tests see requested read write hit miss. index portion address selects tag compared. data cache block referred index portion address valid, tag portion address matches tag, hit. Either data read selected word load written selected word store. Cache Ready signal set. write, dirty bit set 1. Note write hit also sets valid bit tag field; seems unnecessary, included tag single memory, change dirty bit likewise need change valid tag fields. hit block valid, FSM returns idle state. miss first updates cache tag goes either Write-Back state, block location dirty bit value 1, Allocate state 0. Write-Back: state writes 128-bit block memory using 872address composed tag cache index. remain state waiting Ready signal memory. memory write complete, FSM goes Allocate state. Allocate: new block fetched memory. remain state waiting Ready signal memory. memory read complete, FSM goes Compare Tag state. Although could gone new state complete operation instead reusing Compare Tag state, good deal overlap, including update appropriate word block access write. 873FIGURE 5.39 Four states simple controller. simple model could easily extended states try improve performance. example, Compare Tag state compare read write cache data single clock cycle. Often compare cache access done separate states try improve clock cycle time. Another optimization would add write buffer could save dirty block read new block first processor doesn’t wait two memory accesses dirty miss. cache would next write dirty block write buffer processor operating requested data. Section 5.12 goes detail FSM, showing full controller hardware description language block diagram simple cache. 8745.10 Parallelism Memory Hierarchy: Cache Coherence Given multicore multiprocessor means multiple processo rs single chip, processors likely share common physical address space. Caching shared data introduces new problem, view memory held two different processors individual caches, which, without additional precautions, could end seeing two distinct values. Figure 5.40 illustrates problem shows two different processors two different values location. difficulty generally referred cache coherence problem . FIGURE 5.40 cache coherence problem single memory location (X), read written two processors (A B). initially assume neither cache contains variable X value 0. also assume write-through cache; write-back cache adds additional similar complications. value X written A, A’s cache memory contain new value, B’s cache not, B reads value X, receive 0! Informally, could say memory system coherent read data item returns recently written value data item. definition, although intuitively appealing, vague simplistic; reality much complex. simple definition contains two different aspects memory system behavior, critical writing correct shared memory programs. first aspect, called coherence , defines values returned read. second aspect, called 875consistency , determines written value returned read. Let’s look coherence first. memory system coherent 1. read processor P location X follows write P X, writes X another processor occurring write read P, always returns value written P. Thus, Figure 5.40 , CPU read X time step 3, see value 1. 2. read processor location X follows write another processor X returns written value read write sufficiently separated time writes X occur two accesses. Thus, Figure 5.40 , need mechanism value 0 cache CPU B replaced value 1 CPU stores 1 memory address X time step 3. 3. Writes location serialized ; is, two writes location two processors seen order processors. example, CPU B stores 2 memory address X time step 3, processors never read value location X 2 later read 1. first property simply preserves program order—we certainl expect property true uniprocessors, instance. second property defines notion means coherent view memory: processor could continuously read old data value, would clearly say memory incoherent. need write serialization subtle, equally important. Suppose serialize writes, processor P1 writes location X followed P2 writing location X. Serializing writes ensures every processor see write done P2 point. serialize writes, might case processor could see write P2 first see th e write P1, maintaining value written P1 indefinitely. simplest way avoid difficulties ensure writ es location seen identical order, call write serialization . Basic Schemes Enforcing Coherence cache coherent multiprocessor, caches provide 876migration replication shared data items: Migration: data item moved local cache used transparent fashion. Migration reduces latency access shared data item allocated remotely bandwidth demand shared memory. Replication: shared data simultaneously read, caches make copy data item local cache. Replication reduces latency access contention read shared data item. Supporting migration replication critical performance accessing shared data, many multiprocessors introduce hardware protocol maintain coherent caches. protocols maintain coherence multiple processors called cache coherence protocols . Key implementing cache coherence protocol tracking state sharing data block. popular cache coherence protocol snooping . Every cache copy data block physical memory also copy sharing status block, centralized state kept. caches accessible via broadcast medium (a bus network), cache controllers monitor snoop medium determine whether copy block requested bus switch access. following section explain snooping-based cache coherence implemented shared bus, communication medium broadcasts cache misses processors used implement snooping-based coherence scheme. broadcasting caches makes snooping protocols simple implement also limits scalability. Snooping Protocols One method enforcing coherence ensure processor h exclusive access data item writes item. styl e protocol called write invalidate protocol invalidates copies caches write. Exclusive access ensures readable writable copies item exist write occurs: cached copies item invalidated. Figure 5.41 shows example invalidation protocol snooping bus write-back caches action. see 877protocol ensures coherence, consider write followed read b another processor: since write requires exclusive access, copy held reading processor must invalidated (hence protocol name). Thus, read occurs, misses cache, cache forced fetch new copy data. write, require writing processor exclusive access, preventing processor able write simultaneously. two processors attempt write data time, one wins race, causing processor’s copy invalidated. processor complete write, must obtain new copy data, must contain updated value. Therefore, protocol also enforces write serialization. Hardware/Software Interface One insight block size plays important role cache coherency. example, take case snooping cache block size eight words, single word alternatively writte n read two processors. protocols exchange full blocks processors, thereby increasing coherency bandwidth demands. Large blocks also cause called false sharing : two unrelated shared variables located cache block, whole block exchanged processors even though processors accessing different variables. Programmers compilers lay data carefully avoid false sharing. false sharing two unrelated shared variables located cache block full block exchanged processors even though processors accessing different variables. Elaboration Although three properties page 455 sufficient ensure coherence, question written value seen also important. see why, observe cannot require read X Figure 5.40 instantaneously sees value written X 878some processor. If, example, write X one processor precedes read X another processor shortly beforehand, may impossible ensure read returns value data written, since written data may even left processor point. issue exactly written value must seen reader defined memory consistency model . make following two assumptions. First, write complete (and allow next write occur) processors seen effect write. Second, processor change order write respect memory access. two conditions mean processor writes location X followed location Y, processor sees new value must also see new value X. restrictions allow processor reorder reads, force processor finish write program order. Elaboration Since input change memory behind caches, since output could need latest value write back cache, also cache coherency problem I/O caches single processor well caches multiple processors . cache coherence problem multiprocessors I/O (see Chapter 6 ), although similar origin, different characteristics affect appropriate solution. Unlike I/O, multiple data copies rare event—one avoided whenever possible —a program running multiple processors normally copies data several caches. Elaboration addition snooping cache coherence protocol status shared blocks distributed, directory-based cache coherence protocol keeps sharing status block physic al memory one location, called directory . Directory-based coherence slightly higher implementation overhead snooping, reduce traffic caches thus scale larger processor counts. 879FIGURE 5.41 example invalidation protocol working snooping bus single cache block (X) write-back caches. assume neither cache initially holds X value X memory 0. CPU memory contents show value processor bus activity completed. blank indicates activity copy cached. second miss B occurs, CPU responds value canceling response memory. addition, contents B’s cache memory contents X updated. update memory, occurs block becomes shared, simplifies protocol, possible track ownership force write-back block replaced. requires introduction additional state called “owner, ” indicates block may shared, owning processor responsible updating processors memory changes block replaces it. Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks 880This online section describes using many disks conjunct ion offer much higher throughput, original inspiration Redundant Arrays Inexpensive Disks (RAID). real popularity RAID, however, due considerably greater dependability offered including modest number redundant disks. section explains differences performance, cost, dependability RAID levels. 5.11 Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks Amdahl’s law Chapter 1 reminds us neglecting I/O parallel revolution foolhardy. simple example demonstrates this. Impact I/O System Performance Example Suppose benchmark executes 100 seconds elapsed time, 90 seconds CPU time, rest I/O time. Suppose number processors doubles every 2 years, processors remain speed, I/O time doesn’t improve. much faster program run end 6 years? Answer 881We know new CPU times resulting elapsed times computed following table. improvement CPU performance 6 years However, improvement elapsed time I/O time increased 10% 47% elapsed time. Hence, parallel revolution needs come I/O well computation, effort spent parallelizing could squandered whenever programs I/O, must do. Accelerating I/O performance original motivation disk arrays. late 1980s, high-performance storage choice 882large, expensive disks. argument replacing big disks many small disks, performance would improve would read heads. shift good match multiple processors well, since many read/write heads mean storage system could support many independent accesse well large transfers spread across many disks. is, could get high I/Os per second high data transfer rates. addition higher performance, could advantages cost, power, floor space, since smaller disks generally efficient per gigabyte larger disks. flaw argument disk arrays could make reliability much worse. smaller, inexpensive drives lo wer MTTF ratings large drives, importantly, replacing single drive with, say, 50 small drives, failure rate would go least factor 50. solution add redundancy system could cope disk failures without losing information. many little disks, cost extra redundancy improve dependabil ity small, relative solutions large disks. Thus, dependability affordable constructed redundant array inexpensive disks. observation led name: redundant arrays inexpensive disks , abbreviated RAID . redundant arrays inexpensive disks (RAID) organization disks uses array small inexpensive disks increase performance reliability. 883In retrospect, although invention motivated performance, dependability key reason widespread popularity RAID. parallel revolution resurfaced original performance side argument RAID. rest thi section surveys options dependability impacts n cost performance. much redundancy need? need extra information find faults? matter organize data additional check information disks? paper coined term gave evolutionary answer questions, starting simplest expensive solut ion. Figure e5.11.1 shows evolution example cost number extra check disks. keep track evolution, authors numbered stages RAID, still used today. 884FIGURE E5.11.1 RAID example four data disks showing extra check disks per RAID level companies use level. Figures e5.11.2 e5.11.3 explain difference RAID 3, RAID 4, RAID 5. Redundancy (RAID 0) Simply spreading data multiple disks, called striping , automatically forces accesses several disks. Striping across se disks makes collection appear software single large disk, simplifies storage management. also improves performance large accesses, since many disks operate once. Video-editing systems, example, frequently stripe thei r data may worry dependability much as, say, databases. 885 striping Allocation logically sequential blocks separate disks ow higher performance single disk deliver. RAID 0 something misnomer, redundancy. However, RAID levels often left operator set creating storage system, RAID 0 often listed one options. Hence, term RAID 0 become widely used. Mirroring (RAID 1) traditional scheme tolerating disk failure, called mirroring shadowing , uses twice many disks RAID 0. Whenever data written one disk, data also written redundant disk, always two copies information. disk fails, system goes “mirror” reads contents get desired information. Mirroring expensive RAID solution, since requires disk s. mirroring Writing identical data multiple disks increase data availability. Error Detecting Correcting Code (RAID 2) RAID 2 borrows error detection correction scheme often used memories (see Section 5.5 ). Since RAID 2 fallen disuse, we’ll describe here. Bit-Interleaved Parity (RAID 3) cost higher availability reduced 1/ n, n number disks protection group . Rather complete copy original data disk, need add enough redundant information restore lost information failure. Reads writes go disks group, one extra disk hold check information case failure. RAID 3 886is popular applications large data sets, multimedia scientific codes. protection group group data disks blocks share common check disk block. Parity one scheme. Readers unfamiliar parity think redundant disk sum data disks. disk fails, subtract data good disks parity disk; remaining information must b e missing information. Parity simply sum modulo two. Unlike RAID 1, many disks must read determine missing data. assumption behind technique taking longer recover failure spending less redundant storage good tradeoff. Block-Interleaved Parity (RAID 4) RAID 4 uses ratio data disks check disks RAID 3, access data differently. parity stored blocks associated set data blocks. RAID 3, every access went disks. However, applications prefer smaller accesses, allowing independent acces ses occur parallel. purpose RAID levels 4 7. Since error detection information sector checked reads see data correct, “small reads” disk occur independently long minimum access one sector. n RAID context, small access goes one disk protection group large access goes disks protection group. Writes another matter. would seem small write would demand disks accessed read rest information needed recalculate new parity, left Figure e5.11.2 . “small write” would require reading old data old parity, adding new information, writing new parity parity disk new data data disk. 887FIGURE E5.11.2 Small write update RAID 4. optimization small writes reduces number disk accesses well number disks occupied. figure assumes four blocks data one block parity. naive RAID 4 parity calculation left figure reads blocks D1, D2 , D3 adding block D0? calculate new parity P? (In case wondering, new data D0? comes directly CPU, disks involved reading it.) RAID 4 shortcut rig ht reads old value D0 compares new value D0? see bits change. next read old parity P change corresponding bits form P? logical function exclusive exactly want. example replaces three disk reads (D1, D2, D3) two disk writes (D0?, P?) involving disks two disk reads (D0, P) two disk writes (D0?, P?), involve two disks. Enlarging size parity group increases savings shortcut. RAID 5 uses shortcut. key insight reduce overhead parity simply sum information; watching bits change write new information, need change corresponding bits parity disk. right Figure e5.11.2 shows shortcut. must read old data disk written, compare old data new data see bits change, read old parity, change corresponding bits, write new data new parity. Thus, small write involves four disk accesses two disks instead accessing disks. organization RAID 4. 888Distributed Block-Interleaved Parity (RAID 5) RAID 4 efficiently supports mixture large reads, large writes, small reads, plus allows small writes. One drawback system parity disk must updated every write, parity disk bottleneck back-to-back writes. fix parity-write bottleneck, parity information spread throughout disks single bottlenec k writes. distributed parity organization RAID 5. Figure e5.11.3 shows data distributed RAID 4 versus RAID 5. organization right shows, RAID 5 parity associated row data blocks longer restricted single disk. organization allows multiple wri tes occur simultaneously long parity blocks located disk. example, write block 8 right must also access parity block P2, thereby occupying first thi rd disks. second write block 5 right, implying update parity block P1, accesses second fourth disks thus could occur concurrently write block 8. writes organization left result changes blocks P1 P2, fifth disk, bottleneck. FIGURE E5.11.3 Block-interleaved parity (RAID 4) versus distributed block-interleaved parity (RAID 5). distributing parity blocks disks, small writes performed parallel. 889P+Q Redundancy (RAID 6) Parity-based schemes protect single self-identifying failure. single failure correction sufficient, parity c generalized second calculation data another check disk information. second check block allows recovery second failure. Thus, storage overhead twice RAID 5. small write shortcut Figure e5.11.2 works well, except six disk accesses instead four update P Q information. RAID Summary RAID 1 RAID 5 widely used servers; one estimate 80% disks servers found RAID organization. One weakness RAID systems repair. First, avoid making data unavailable repair, array must designed allow failed disks replaced without turn system. RAIDs enough redundancy allow continuous operation, hot-swapping disks place demands physical electrical design array disk interfaces. Second, another failure could occur repair, repair time affects chances losing data: longer repair time, greater chances another failure lose data. Rather wait operator bring good disk, systems include standby spares data reconstructed instantly upon discovery failure. oper ator replace failed disks leisurely fashion. Note human operator ultimately determines disks remove. Operators human, occasionally remove good disk instead broken disk, leading unrecoverable disk failure. hot-swapping Replacing hardware component system running. standby spares 890Reserve hardware resources immediately take place failed component. addition designing RAID system repair, questions disk technology changes time. Althoug h disk manufacturers quote high MTTF products, tho se numbers nominal conditions. particular disk array subject temperature cycles due to, say, failure th e air-conditioning system, shaking due poor rack design, construction, installation, failure rates three six times higher (see fallacy page 470). calculation RAID reliability assumes independence disk failures, disk failures could correlated, damage due environment would likely happen disks array. Another concern since disk bandwidth growing slowly disk capacity, time repair disk RAID system increasing, turn enhances chances second failure. example, 3-TB disk could take almost 9 hours read sequentially, assuming interference. Given damaged RAID likely continue serve data, reconstruction could stretched considerably. Besides increasing time, another concern reading much data reconstruction means increasing chance uncorrectable read media failure, would result data loss. arguments concern simultaneous multiple failures th e increasing number disks arrays use higher-capacity disks. Hence, trends led growing interest protecting one failure, RAID 6 increasingly offered option used field. Check following true RAID levels 1, 3, 4, 5, 6? 1. RAID systems rely redundancy achieve high availability. 2. RAID 1 (mirroring) highest check disk overhead. 3. small writes, RAID 3 (bit-interleaved parity) worst throughput. 4. large writes, RAID 3, 4, 5 throughput. 891 Elaboration One issue mirroring interacts striping. Suppose had, say, four disks’ worth data store eight physical disks use. Would create four pairs disks—each organized RAID 1—and stripe data across four RAID 1 pairs? Alternatively, would create two sets four disks—each organized RAID 0—and mirror writes RAID 0 sets? RAID terminology evolved call former RAID 1+0 RAID 10 (“striped mirrors”) latter RAID 0+1 RAID 01 (“mirrored stripes”). Advanced Material: Implementing Cache Controllers online section shows implement control cache, implemented control single-cycle pipeli ned datapaths Chapter 4 . section starts description finite-state machines implementation cache controlle r simple data cache, including description cache controller hardware description language. goes details example cache coherence protocol difficulti es implementing protocol. 5.12 Advanced Material: Implementing Cache Controllers section starts SystemVerilog cache control ler Section 5.9 eight figures. goes details example cache coherency protocol difficulties 892implementing protocol. SystemVerilog Simple Cache Controller hardware description language using section SystemVerilog. biggest change prior versions Veri log borrows structures C make code easier read. Figures e5.12.1 e5.12.8 show SystemVerilog description cache controller. 893FIGURE E5.12.1 Type declarations SystemVerilog cache tags data. tag field 18 bits wide index field 10 bits wide, 2-bit field (bits 3–2) used dex block select word block. rest type declaration found following figu re. 894FIGURE E5.12.2 Type declarations SystemVerilog CPU-cache cache-memory interfaces. nearly identical except data 32 bits wide CPU cache 128 bits wide cache memory. 895FIGURE E5.12.3 Block diagram simple cache using Verilog names. shown write enables cache tag memory cache data memory, control signals multiplexors supply data Data Write variable. Rather separate write enables every word cache data block, Verilog reads old value block Data Writ e updates word variable write . writes whole 128-bit block. 896FIGURE E5.12.4 Cache data tag modules SystemVerilog. nearly identical except data 32 bits wide CPU cache 128 bits wide cache memory. write positive clock edges write enable set. 897FIGURE E5.12.5 FSM SystemVerilog, part I. modules instantiate memories according type definitions previous figure. 898FIGURE E5.12.6 FSM SystemVerilog, part II. section describes default value signals. following figures set values one clock cycle, Verilog reset values following clock cycle. 899FIGURE E5.12.7 FSM SystemVerilog, part III. Actual FSM states via case statement figure next. figure Idle state Compare Tag state. 900FIGURE E5.12.8 FSM SystemVerilog, part IV. Actual FSM states via case statement prior figure one. figure last part Compare Tag state, plus Allocate Write-Back states. 901Figures e5.12.1 e5.12.2 declare structures used definition cache following figures. example , cache tag structure ( cache_tag_type ) contains valid bit ( valid ), dirty bit ( dirty ), 18-bit tag field ([ TAGMSB:TAGLSB] tag ). Figure e5.12.3 shows block diagram cache using names Verilog description. Figure e5.12.4 defines modules cache data ( dm_cache_data ) cache tag ( dm_cache_tag ). memories read time, writes occur positive clock edge (posedge(clk) ) write enable 1 ( data_req.we tag_req.we ). Figure e5.12.5 defines inputs, outputs, states FSM. inputs requests CPU ( cpu_req ) responses memory ( mem_data ), outputs responses CPU (cpu_res ) requests memory ( mem_req ). figure also declares internal variables needed FSM. example, current state next state registers FSM rstate vstate , respectively. Figure e5.12.6 lists default values control signals, including word read written block, setting cache write enables 0, on. values set every clock cycle, write enable portion cache—for example, tag_req.we —would set 1 one clock cycle figures would reset 0 according Verilog figure. last two figures show FSM large case statement (case(rstate) ), four states split across two figures. Figure e5.12.7 starts Idle state ( idle ), simply goes Compare Tag state ( compare_tag ) CPU makes valid request. describes Compare Tag state. Compare Tag state checks see tags match entry valid. so, first sets Cache Ready signal (v_cpu_res.ready ). request write, sets tag field, valid bit, dirty bit. next state Idle. miss, state prepares change tag entry valid dirty bits. block replaced clean invalid, next state Allocat e. Figure e5.12.8 continues Compare Tag state. block replaced dirty, next state Write-Back. figure show 902the Allocate state ( allocate ) next, simply reads new block. keeps looping memory ready; is, go es Compare Tag state. followed figure Write-Back state ( write_back ). figure shows, Write-Back state merely writes dirty block memory, looping memory ready. memory ready, indicating write complete, go Allocate state. code end sets current state next state resets FSM Idle state next clock edge, dependin g reset signal ( rst). online material includes Test Case module useful check code figures. SystemVerilog c ould used create cache cache controller FPGA. Basic Coherent Cache Implementation Techniques key implementing invalidate protocol use bus, another broadcast medium, perform invalidates. invalidate, processor simply acquires bus access broadcasts address invalidated bus. processors continuously snoop bus, watching addresses. processors check whether address bus cach e. so, corresponding data cache invalidated. write block shared occurs, writing processor must acquire bus access broadcast invalidation. two processors try write shared blocks time, attempts broadcast invalidate operation serialized arbitrate bus. first processor obtain bus access cause copies block writing invalidated. processors attempting write block, serialization enforced bus also serializes writes. One implication scheme write shared data item cannot actually complete obtains bus access. coherence schemes require method serializing accesses cache block, serializing access either communication medium another shared structure. addition invalidating outstanding copies cache block written into, also need locate data item 903cache miss occurs. write-through cache, easy find recent value data item, since written data unfailingly sent memory, most-recent value data item always fetched. design adequate memory bandwidth support write traffic processors, usi ng write-through simplifies implementation cache cohere nce. write-back cache, finding most-recent data value difficult, since recent value data item cache rather memory. Happily, write-back caches use snooping scheme cache misses writes: processor snoops addresses placed bus. processor finds dirty copy requested cache blo ck, provides cache block response read request causes memory access aborted. increased complexity comes retrieve cache block processor’s cache, often take longer retrieving shared memory processors separate chips. Since write - back caches generate lower requirements memory bandwidth, support larger numbers faster processors approach chosen multiprocessors, despite additional complexity maintaining coherence. Therefore, wi examine implementation coherence write-back caches. normal cache tags used implement process snooping, valid bit block makes invalidation easy implement. Read misses, whether generated invalidation event, also straightforward, since simply rely snooping capability. writes, we’d like know whether copies block cached, cached copies, write need placed bus write-back cache. sending write reduces time taken write required bandwidth. track whether cache block shared, add extra state bit associated cache block, valid bit dirty bit. adding bit indicating whether block shared, decide whether write must generate invalidate. write block shared state occurs, cache generates invalidation bus marks block exclusive . invalidations sent processor block. processor sole copy cache block 904normally called owner cache block. invalidation sent, state owner’s cache block changed shared unshared (or exclusive). another processor later requests cache block, state must made shared again. Since snooping cache also sees misses, knows exclusive cache block requested another processor, state made shared. Every bus transaction must check cache-address tags, could potentially interfere processor cache accesses. One way reduce interference duplicate tags. interf erence also reduced multilevel cache directing snoop requests L2 cache, processor uses miss L1 cache. scheme work, every entry L1 cache must present L2 cache, property called inclusion property . snoop gets hit L2 cache, must arbitrate L1 cache update state possibly retrieve data, usually requires stall processor. Sometimes may even useful duplicate tags secondary cache decrease contention processor snooping activity. Example Cache Coherency Protocol snooping coherence protocol usually implemented incorporating finite-state controller node. contr oller responds requests processor bus (or oth er broadcast medium), changing state selected cache block, well using bus access data invalidate it. Logically, think separate controller associated block; is, snooping operations cache requests differe nt blocks proceed independently. actual implementations, single controller allows multiple operations distinct blo cks proceed interleaved fashion (that is, one operation may initiated another completed, even though one cach e access one bus access allowed time). Also, remember although refer bus following description, interconnection network supports broadcast coherence controllers associated caches used implement snooping. 905The simple protocol consider three states: invalid, shared, modified. shared state indicates block potentially shared, modified state indicates blo ck updated cache; note modified state implies block exclusive. Figure e5.12.9 shows requests generated processor-cache module node (in first n ine rows table) well coming bus (in last five rows table). protocol write-back cache, easily changed work write-through cache reinterpreting modified state exclusive state updati ng cache writes normal fashion write-through cache. common extension basic protocol addition exclusive state, describes block unmodified held one cache; caption Figure e5.12.9 describes state addition detail. FIGURE E5.12.9 cache coherence mechanism receives requests processor bus responds based type request, whether hits misses cache, state cache block specified request. fourth column describes type cache action normal hit miss (the uniprocessor 906cache would see), replacement (a uniprocessor cache replacement miss), coherence (required maintain cache coherence); normal replacement action may cause coherence action depending state block caches. read misses, write misses, invalidates snooped bus, action required read write addresses match block cache block valid. protocols also introduce state designate block exclusively one cache yet written. state arise write access broken two pieces: getting block exclusively one cache subsequently updating it; protocol “exclusive unmodified state” transient, ending soon write completed. protocols use maintain exclusive state unmodified block. snooping protocol, state entered processor reads block resident cache. subsequent accesses snooped, possible maintain accuracy state. particular, another processor issues read miss, state changed exclusive shared. advantage adding state subsequent write block exclusive state processor need acquire bus access generate invalidate, since block known exclusively cache; processor merely changes state modified. state easily added using bit encodes coherent state exclusive state using dirty bit indicate block modified. popular MESI protocol, named four states includes (modified, exclusive, shared, invalid), uses structure. MOESI protocol introduces another extension: “owned” state. invalidate write miss placed bus, processors copies cache block invalidate it. write - cache, data write miss always retrieved memory. write miss writeback cache, block exclusive one cache, cache also writes back block; otherwise, data read memory. Figure e5.12.10 shows finite-state transition diagram single cache block using write invalidation protocol write-back 907cache. simplicity, three states protocol duplic ated represent transitions based processor requests (on l eft, corresponds top half table Figure e5.12.9 ), contrary transitions based bus requests (on right, corresponds last five rows table Figure e5.12.9 ). Boldface type used distinguish bus actions, contrast conditions state transition depends. state node represents state selected cache block specified b processor bus request. FIGURE E5.12.10 write-invalidate, cache- coherence protocol write-back cache, showing states state transitions block cache. cache states shown circles, access permitted processor without state transition shown parentheses name state. stimulus causing state change shown transition arcs regular type, bus actions generated part state transition shown transition arc bold. stimulus actions apply block cache, specific address cache. Hence, read miss block shared state miss cache block different address. left side diagram shows state transitions based actions processor associated cache; right side shows transitions based operations bus. read 908miss exclusive shared state write miss exclusive state occur address requested processor match address cache block. miss standard cache replacement miss. attempt write block shared state generates invalidate. Whenever bus transaction occurs, caches contain cache block specified bus transaction take action dictated right half diagram. protocol assumes memory provides data read miss block clean caches. actual implementations, two sets state diagrams combined. practice, many subtle variations invalidate protocols, including introduction exclusive unmodified state, whether processor memory provides data miss. states cache protocol would needed uniprocessor cache, would correspond invalid, valid (and clean), dirty states. state changes indicated arcs left half Figure e5.12.10 would needed write-back uniprocessor cache, exception invalidate write hit shared block. state changes represented arcs right half Figure e5.12.10 needed coherence would appear uniprocessor cache controller. mentioned earlier, one finite-state machine per cache, stimuli coming either attached processor bus. Figure e5.12.11 shows state transitions right half Figure e5.12.10 combined left half figure form single state diagram cache block. 909FIGURE E5.12.11 Cache coherence state diagram state transitions induced local processor shown black bus activities shown gray. Figure e5.12.10 , activities transition shown bold. understand protocol works, observe valid cache block either shared state one caches exclusive state exactly one cache. transition exclusive state (which required processor write block) requires invalidate write miss placed bus , causing caches make block invalid. addition, cache block exclusive state, cache generate write back, supplies block containing desired 910address. Finally, read miss occurs bus block exclusive state, cache exclusive copy changes st ate shared. actions gray Figure e5.12.11 , handle read write misses bus, essentially snooping component protocol. One property preserved prot ocol, protocols, memory block shared state always date memory, simplifies implementation. Although simple cache protocol correct, omits number complications make implementation much trickier. important protocol assumes operation atomic —that is, operation done way intervening operation occur. example, protocol described assumes write misses detected, acquire bus, receive response single atomic action. reality, true. Similarly, used switch, recent multiprocessors do, even read misses would also atomic. Nonatomic actions introduce possibility protocol c deadlock , meaning reaches state cannot continue. next page, discuss protocols implemented without bus. Constructing small-scale (two four processors) multiproc essors become easy. example, Intel Nehalem AMD Opteron processors designed use cache-coherent multiprocessors external interface supports snooping allows two four processors directly connected. also larger on-chip caches reduce bus utilization. case Opteron processors, support interconnecting multiple processors integrated onto pr ocessor chip, memory interfaces. case Intel design, two-processor system built additional external chips interface memory system I/O. Although designs cannot easily scaled larger processor counts, offer extremely cost-effective solution tw four processors. devil details. 911Classic proverb. Implementing Snoopy Cache Coherence said earlier, major complication actually implementing snooping coherence protocol described write upgrade misses atomic recent multiprocessor. steps detecting write upgrade miss; communicating processors memory; getting most-recent value write miss ensuring invalidates processed; updating cache cannot done took single cycle. simple single-bus system, steps made effectively atomic arbitrating bus first (before changing cache state) releasing bus actions complete. processor know invalidates complete? bus-based multiprocessors, single line used signal w hen necessary invalidates received processed. Following signal, processor generated th e miss release bus, knowing required actions completed activity related next miss. holdin g bus exclusively steps, processor effect ively makes individual steps atomic. system without bus, must find method making steps miss atomic. particular, must ensure two processors attempt write block time, situation called race, strictly ordered: one write processed next begun. matter wh ich two writes race wins race, single winner whose coherence actions completed first. snoopy system, ensuring race one winner accomplished using broadcast misses, well basic properties interconnection network. properties, together ability restart miss handling loser race, key implementing snoopy cache coherence without bus. 5.13 Real Stuff: ARM Cortex-A53 Intel Core i7 Memory Hierarchies 912In section, look memory hierarchy two microprocessors described Chapter 4 : ARM Cortex-A53 Intel Core i7. section part based Section 2.6 Computer Architecture: Quantitative Approach , 5th edition. Figure 5.42 summarizes address sizes TLBs two processors. Note Cortex-A53 two 10-entry fully associative micro-TLBs backed shared 512-entry four-way set associative main TLB 48-bit virtual address space 40- bit physical address space. Core i7 three TLBs 48-bit virtual address 44-bit physical address. Although 64-bit registers processors could hold larger virtual addres s, software need large space, 48-bit virtual addresses shrinks page table memory footprint TLB hardware. 913FIGURE 5.42 Address translation TLB hardware ARM Cortex-A53 Intel Core i7 920. processors provide support large pages, used things like operating system mapping frame buffer. large-page scheme avoids using large number entries map single object always present. Figure 5.43 shows caches. Cortex-A53 one four processors cores Core i7 fixed four. Cortex-A53 16 64 KiB, two-way L1 instruction cache (per core) Core i7 32 KiB, four-way set associative, L1 instruction cache (per core). use 64 byte blocks. Cortex- A53 increases associativity four-way data cache, variables remain same. Similarly, Core i7 keeps everything except associativity, increases eight-way. Core i7 provides 256 KiB, eight-way set associative unified L2 cache (per core) 64 byte blocks. contrast, Cortex-A53 provides L2 cache shared one four cores. cache 16-way set associative 64 byte blocks 128 KiB 2 MiB size. Core i7 used servers, also offers L3 cache shared cores chip. size varies 914depending number cores. four cores, case, size 8 MiB. 915FIGURE 5.43 Caches ARM Cortex-A53 Intel Core i7 920. significant challenge facing cache designers support processors like Cortex-A53 Core i7 execute one memory instruction per clock cycle. popular technique break cache banks allow multiple, independent, parallel accesses, provided accesses different banks. technique similar interleaved DRAM 916banks (see Section 5.2 ). Cortex-A53 Core i7 additional optimizations allow reduce miss penalty. first th e return requested word first miss. also continue execute instructions access data cache cache miss. Designers attempting hide cache miss latency commonly use technique, called nonblocking cache. implement two flavors nonblocking. Hit miss allows additional cache hits miss, miss miss allows multiple outstanding cache misses. aim first wo hiding miss latency work, aim second overlapping latency two different misses. nonblocking cache cache allows processor make references cache cache handling earlier miss. Overlapping large fraction miss times multiple outstanding misses requires high-bandwidth memory system capable handling multiple misses parallel. personal mobile device, memory system often pipeline, merge, reorder, prioritize requests appropriately. Large serve rs 917and multiprocessors typically memory systems capable handling several outstanding misses parallel. Cortex-A53 Core i7 prefetch mechanisms data accesses. look pattern data misses uses information try predict next address start fetching data miss occurs. techniques generally work best accessing arrays loops. sophisticated memory hierarchies chips large fraction dies dedicated caches TLBs show significant design effort expended try close gap betwe en processor cycle times memory latency. Performance Cortex-A53 Core i7 Memory Hierarchies memory hierarchy Cortex-A53 measured using 32 KiB two-way set associative L1 instruction cache, 32 KiB four-way set associative L1 data cache, 1 MiB 16-way set associative L2 cache running integer SPEC2006 benchmarks. Cortex-A53 instruction cache miss rates benchmarks small. Figure 5.44 shows data cache results Cortex-A53, significant L1 L2 miss rates. L1 data cache miss rates go 0.5% 37.3%, mean 6.4% median 2.4%. (global) L2 cache miss rates vary 0.1% 9.0%, mean 1.3% median 0.3%. L1 miss penalty 1 GHz Cortex-A53 12 clock cycles, L2 miss penalty 124 clock cycles. Using miss penalties, Figure 5.45 shows average miss penalty per data access. low miss rates multiplied high miss penalties, ou see represent significant fraction CPI 5 12 SPEC2006 programs. 918FIGURE 5.44 Data cache miss rates ARM Cortex-A53 running SPEC2006int. Applications larger memory footprints tend higher miss rates L1 L2. Note L2 rate global miss rate; is, counting references, including hit L1. (See Elaboration Section 5.4 .) mcf known cache buster. Note figure systems benchmarks Figure 4.74 Chapter 4 . FIGURE 5.45 average memory access penalty 919in clock cycles per data memory reference coming L1 L2 shown ARM processor running SPEC2006int. Although miss rates L1 significantly higher, L2 miss penalty, five times higher, means L2 misses contribute significantly. Figure 5.46 shows miss rates caches Core i7 using SPEC2006 benchmarks. L1 instruction cache miss rate varies 0.1% 1.8%, averaging 0.4%. rate keeping studies instruction cache behavior th e SPECCPU2006 benchmarks, show low instruction cache miss rates. L1 data cache miss rates running 5% 10%, sometimes higher, importance L2 L3 caches obvious. Since cost miss memory 100 cycles, average data miss rate L2 4%, L3 obviously critical. Assuming half instructions loads stores, without L3 L2 cache misses could add two cycles per instruction CPI ! comparison, average L3 data miss rate 1% still significant four times lower L2 miss rate six times less L1 miss rate. Elaboration speculation may sometimes wrong (see Chapter 4 ), references L1 data cache correspond loads stores eventually complete execution. data Figure 5.44 measured data requests, including cancelled. miss rate measured completed data accesses 1.6 times higher (an average 9.5% versus 5.9% L1 Dcache misses). 920FIGURE 5.46 L1, L2, L3 data cache miss rates Intel Core i7 920 running full integer SPECCPU2006 benchmarks. 5.14 Real Stuff: Rest RISC- V System Special Instructions Figure 5.48 lists 13 remaining RISC-V instructions special purpose systems category. fence instructions provide synchronization barriers instructions ( fence.i ), data ( fence ), address translations (sfence.vma ). first, fence.i , informs processor software modified instruction memory, guarantee instruction fetch reflect updated instructions. se cond, fence , affects data memory access ordering multiprocessing I/O. third, sfence.vma , informs processor software modified page tables, guarantee address translations reflect updates. six control status register (CSR) access instructions move data general-purpose registers CSRs. csrrwi instruction (CSR read/write immediate) copies CSR integer 921register, overwrites CSR immediate. csrrsi (CSR read/set immediate) copies CSR integer register, overwrites CSR bitwise CSR immediate. csrrci (CSR read/clear) like csrrsi , clears bits instead setting them. csrrw , csrrs , csrrc instructions use register operand instead immediate, otherwise thing. Two instructions’ purpose generate exceptions: ecall generates environment call exception invoke OS, ebreak generates breakpoint exception invoke debugger. supervisor exception-return instruction ( sret ), naturally enough, allows program return exception handler. Finally, wait-for-interrupt instruction, wfi, informs processor may enter idle state interrupt occurs. 5.15 Going Faster: Cache Blocking Matrix Multiply next step continuing saga improving performance DGEMM tailoring underlying hardware add cache blocking subword parallelism instruction level parallelism optimizations Chapters 3 4. Figure 5.47 shows blocked version DGEMM Figure 4.78 . changes made earlier going unoptimized DGEMM Figure 3.22 blocked DGEMM Figure 5.21 above. time take unrolled version DGEMM Chapter 4 invoke many times submatrices A, B, C. Indeed, lines 28–34 lines 7–8 Figure 5.47 mirror lines 14–20 lines 5–6 Figure 5.21 , except incrementing loop line 7 amount unrolled. 922FIGURE 5.47 Optimized C version DGEMM Figure 4.78 using cache blocking. changes ones found Figure 5.21. assembly language produced compiler do_block function nearly identical Figure 4.79 . again, overhead call do_block compiler inlines function call. Unlike earlier chapters, show resulting x86 code inner loop code nearly identical Figure 4.79 , blocking affect computation, order accesses data memory. change bookkeeping integer instructions implement loops. expands 14 923instructions inner loop eight loop Figure 4.78 40 28 instructions respectively bookkeeping code generated Figure 5.47 . Nevertheless, extra instructions executed pale comparison performance improvement reducing cache misses. Figure 5.49 compares unoptimized optimized subword parallelism, instruction level parallelis m, caches. Blocking improves performance unrolled AVX code factors 2 2.5 larger matrices. compare unoptimized code code three optimizations, performance improvement factors 8 15, largest increase largest matrix. Elaboration mentioned Elaboration Section 3.9 , results Turbo mode turned off. Chapters 3 4, turn on, improve results temporary increase clock rate 3.3/2.6 =1.27. Turbo mode works particularly well case using single core eight-core chi p. However, want run fast use cores, we’ll see Chapter 6 . 924FIGURE 5.48 list assembly language instructions systems special operations full RISC-V instruction set. FIGURE 5.49 Performance four versions DGEMM matrix dimensions 32 ×32 960 ×960. fully optimized code largest matrix almost 15 times fast unoptimized version Figure 3.22 Chapter 3 . 5.16 Fallacies Pitfalls one naturally quantitative aspects computer architecture, memory hierarchy would seem less vulnerable fallacies pitfalls. many fallacies propagated pitfalls encountered, led major negative outcomes. start pitfall often traps students exercises exams. Pitfall: Ignoring memory system behavior writing programs generating code compiler. could rewritten fallacy: “Programmers ignore 925memory hierarchies writing code.” evaluation sort Figure 5.19 cache blocking Section 5.14 demonstrate programmers easily double performance factor behavior memory system design algorithm s. Pitfall: Forgetting account byte addressing cache block size simulating cache. simulating cache (by hand computer), need make sure account effect byte addressing multiword blocks determining cache block given address maps. example, 32-byte direct-mapped cache block size 4 bytes, byte address 36 maps block 1 cache, since byte address 36 block address 9 (9 modulo 8)=1. hand, address 36 word address, maps block (36 mod 8)=4. Make sure problem clearly states base address. like fashion, must account block size. Suppose cache 256 bytes block size 32 bytes. block byte address 300 fall? break address 300 fields, see answer: Byte address 300 block address number blocks cache 926Block number 9 falls cache block number (9 modulo 8)=1. mistake catches many people, including authors (in earlier drafts) instructors forget whether intend ed addresses doublewords, words, bytes, block numbers. Remember pitfall tackle exercises. Pitfall: less set associativity shared cache number cores threads sharing cache. Without extra care, parallel program running 2n processors threads easily allocate data structures addresses would map set shared L2 cache. cache least 2n-way associative, accidental conflicts hidden hardware program. not, programmers could face apparently mysterious performance bugs—actually due L2 conflict misses—when migrating from, say, 16-core design 32- core design use 16-way associative L2 caches. Pitfall: Using average memory access time evaluate memory hierarchy out-of-order processor. 927If processor stalls cache miss, separately calculate memory-stall time processor execution tim e, hence evaluate memory hierarchy independently using average memory access time (see page 391). processor continues execute instructions, may even sustain cache misses cache miss, accurate assessment memory hierarchy simulate - of-order processor along memory hierarchy. Pitfall: Extending address space adding segments top unsegmented address space. 1970s, many programs grew large code data could addressed 16-bit address. Computers revised offer 32-bit addresses, either unsegmented 32-bit address space (also called flat address space ) adding 16 bits segment existing 16-bit address. marketing point view, adding segments programmer-visible forced programmer compiler decompose programs segments could solve addressing problem. Unfortunately, trouble time programming language wants address larger one segment, indices large arrays, unrestricted pointers, reference parameters. Moreover, adding segments turn every address two words—one segment number one segment offset—causing problems use addresses registers. Fallacy: Disk failure rates field match specifications . Two recent studies evaluated large collections disks chec k relationship results field compared specifications. One study almost 100,000 disks quoted MTTF 1,000,000 1,500,000 hours, AFR 0.6% 0.8%. found AFRs 2% 4% common, often three five times higher specified rates [Schroeder Gibso n, 2007]. second study 100,000 disks Google, quoted AFR 1.5%, saw failure rates 1.7% drives first year rise 8.6% drives third year, abou 928five six times declared rate [Pinheiro, Weber, Barroso, 2007]. Fallacy: Operating systems best place schedule disk accesses. mentioned Section 5.2 , higher-level disk interfaces offer logical block addresses host operating system. Given high-level abstraction, best OS try help performance sort logical block addresses increasi ng order. However, since disk knows actual mapping logical addresses onto physical geometry sectors, tracks, surfaces, reduce rotational seek latencies rescheduling. example, suppose workload four reads [Anderson, 2003]: Operation Starting LBA Length Read 724 8 Read 100 16 Read 9987 1 Read 26 128 host might reorder four reads logical block order: Operation Starting LBA Length Read 26 128 Read 100 16 Read 724 8 Read 9987 1 Depending relative location data disk, reordering could make worse, Figure 5.50 shows. disk- scheduled reads complete three-quarters disk revolutio n, OS-scheduled reads take three revolutions. Pitfall: Implementing virtual machine monitor instructio n set architecture wasn’t designed virtualizable. 929FIGURE 5.50 Example showing OS versus disk schedule accesses, labeled host-ordered versus drive-ordered. former takes three revolutions complete four reads, latter completes three- fourths revolution. Anderson [2003]. Many architects 1970s 1980s weren’t careful make sure instructions reading writing information related hardware resource information privileged. laissez-faire attitude causes problems VMMs architectures , including x86, use example. Figure 5.51 describes 18 instructions cause problems virtualization [Robin Irvine, 2000]. two broad classes instructions Read control registers user mode reveals guest operating system running virtual machine (such POPF, mentioned earlier) Check protection required segmented architecture bu assume operating system running highest privilege level 930FIGURE 5.51 Summary 18 x86 instructions cause problems virtualization [Robin Irvine, 2000]. first five instructions top group allow program user mode read control register, descriptor table registers, without causing trap. pop flags instruction modifies control register sensitive information fails silently user mode. protection checking segmented architecture x86 downfall bottom group, instructions checks privilege level implicitly part instruction execution reading control register. checking assumes OS must highest privilege level, case guest VMs. Move segment register tries modify control state, protection checking foils well. simplify implementations VMMs x86, AMD Intel proposed extensions architecture via new mode. Intel’s VT-x provides new execution mode running VMs, architected definition VM state, instructions swap VMs rapidly, large set parameters select circumstances VMM must invoked. Altogether, VT-x adds 11 new instructions x86. AMD’s Pacifica makes similar proposals. alternative modifying hardware make small 931changes operating system avoid using troublesome pieces architecture. technique called paravirtualization , open source Xen VMM good example. Xen VMM provides guest OS virtual machine abstraction uses easy-to-virtualize parts physical x86 hardware VMM runs. 5.17 Concluding Remarks difficulty building memory system keep pace faste r processors underscored fact raw material main memory, DRAMs, essentially fastest computers slowest cheapest. principle locality gives us chance overcome long latency memory access—and soundness strategy demonstrated levels memory hierarchy . Although levels hierarchy look quite different quantitative terms, follow similar strategies oper ation exploit properties locality. Multilevel caches make possible use cache optimizations easily two reasons. First, design parameters lower-level cache different first-level 932cache. example, lower-level cache much larger, possible use bigger block sizes. Second, lower-le vel cache constantly used processor, first-leve l cache is. allows us consider lower-level cache something idle may useful preventing future misses. Another trend seek software help. Efficiently managing memory hierarchy using variety program transformations hardware facilities major focus compiler enhancements. Two different ideas explored. One idea reorganize program enhance spatial temporal locality. approach focuses loop-oriented programs use sizable arrays major data structure; large linear algebra problems typical example, DGEMM. restructuring loops access arrays, substantially improved locality—and, therefore, cache performance—can obtained. Another approach prefetching . prefetching, block data brought cache actually referenced. Many 933microprocessors use hardware prefetching try predict accesses may difficult software notice. prefetching technique data blocks needed future brought cache early using special instructions specify address block. third approach special cache-aware instructions optimize memory transfer. example, microprocessors Section 6.10 Chapter 6 use optimization fetch contents block memory write miss program going write full block. optimization significantly reduces memory traffic one kernel. see Chapter 6 , memory systems central design issue parallel processors. growing significance memory hierarchy determining system performance means important area continue focus designers researchers years come. Historical Perspective Reading section, appears online, gives overview memory technologies, mercury delay lines DRAM, invention memory hierarchy, protection mechanisms, virtual machines, concludes brief history operating systems, including CTSS, MULTICS, UNIX, BSD UNIX, MS-DOS, Windows, Linux. 9345.18 Historical Perspective Reading …the one single development put computers feet invention reliable form memory, namely, core memory.… cost reasonable, reliable and, reli able, could due course made large. Maurice Wilkes, Memoirs Computer Pioneer, 1985 history section gives overview memory technologies , mercury delay lines DRAM, invention memory hierarchy, protection mechanisms, concludes brief history operating systems, including CTSS, MULTICS, UNIX, BSD UNIX, MS-DOS, Windows, Linux. developments concepts chapter driven revolutionary advances technology use f memory. discuss memory hierarchies evolved , let’s take brief tour development memory technology . ENIAC small number registers (about 20) storage implemented basic vacuum tube technology used building logic circuitry. Howev er, vacuum tube technology far expensive used build larger memory capacity. Eckert came idea developing new technology based mercury delay lines. technology, electrical signals converted vibrations sent tube mercury, reaching end, read recirculated. One mercury delay line could store 0.5 Kbits. Although bits accessed serially, th e mercury delay line hundred times cost-effective vacuum tube memory. first known working mercury delay lines developed Cambridge EDSAC. Figure e5.17.1 shows mercury delay lines EDSAC, 32 tanks 512 36-bit words. 935FIGURE E5.17.1 mercury delay lines EDSAC. technology made possible build first stored-program computer. young engineer photograph none Maurice Wilkes, lead architect EDSAC. Despite tremendous advance offered mercury delay lines, terribly unreliable still rather expensive. Th e 936breakthrough came invention core memory J. Forrester MIT part Whirlwind project early 1950s (see Figure e5.17.2 ). Core memory uses ferrite core, magnetized, magnetized, acts store (just magnetic recording tape stores information). set wires runnin g center core, dimension 0.1–1.0 millimeters, makes possible read value stored ferr ite core. Whirlwind eventually included core memory 2048 16-bit words, 32 Kbits. Core memory tremendous advance: cheaper, faster, considerably reliable, higher density. Core memory much better alternatives became dominant memory technology years invention remained nearly 20 years. 937FIGURE E5.17.2 core memory plane Whirlwind containing 256 cores arranged 16×16 array. Core memory invented Whirlwind, used air defense problems, display Smithsonian. (Incidentally, Ken Olsen, founder Digital president 20 years, built computer tested core memories; first computer.) technology replaced core memory one use logic memory: integrated circuit. registers built transistorized memory 1960s, IBM computers used transistorized memory microcode store caches 1970, building main memory transistors remained prohibitively expensive deve lopment integrated circuit. integrated circuit, became possible build DRAM (dynamic random access memory—see Appendix description). first DRAMs built Intel 1970, computers using DRAM memories (as high-speed 938option core) came shortly thereafter; used 1 Kbit DRAMs. fact, computer folklore says Intel developed microproc essor partly help sell DRAM. Figure e5.17.3 shows early DRAM board. late 1970s, core memory become historical curiosity. core memory technology allowed tremendous expansion memory size, DRAM technology allowed comparable expansion. 1990s, many personal computers much memory largest computers using core memory ever had. 939FIGURE E5.17.3 early DRAM board. board uses 18 Kbit chips. Nowadays, DRAMs typically packaged multiple chips little board called DIMM (dual inline memory module). SIMM (single inline memory module) shown Figure e5.17.4 contains total 1 MB sold $5 1997. 2004, DIMMs available 1024 MB sold $100. DRAMs remain dominant memory technology time come, innovations packaging DRAMs provide higher bandwidth greater density ongoing. 940FIGURE E5.17.4 1 MB SIMM, built 1986, using 1 Mbit chips. SIMM sold $5/MB 1997. 2006, main memory packed DIMMs similar this, though using much higher-density memory chips (1 Gbit). Development Memory Hierarchies Although pioneers computing foresaw need memory hierarchy coined term, automatic management two levels first proposed Kilburn colleagues demonstrated University Manchester Atlas computer, implemented virtual memory. year IBM 360 announced. IBM planned include virtual memory next generation (System/370), OS/360 operating system wasn’t challenge 1970. Virtual memory announced 370 family 1972, computer term translation-lookaside buffer coined. embedded computers use virtual memory today. 941The problems inadequate address space plagued designers repeatedly. architects PDP-11 identified smal l address space architectural mistake difficult recover. PDP-11 designed, core memory densities increasing slow rate, competition 100 minicomputer companies meant DEC might cost-competitive product every address go 16-bit datapath twice—hence, decision add 4 address bits predecessor PDP-11, 16 12. architects IBM 360 aware importance address size planned architecture extend 32 bits address. 24 bits used IBM 360, however, low- end 360 models would even slower larger addresses. Unfortunately, expansion effort greatly complicated programmers stored extra information upper 8 “unused” address bits. wider address lasted 2000, IBM expanded architecture 64 bits z-series. Running address space often cause death architecture, architectures managed make transition larger address space. example, PDP-11, 16-bit computer, replaced 32-bit VAX. 80386 extended 80286 architecture segmented 24-bit address space flat 32-bit address space 1985. 1990s, several RISC instruction sets made transition 32-bit addressing 64-bit addressing providing compatible 942extension instruction sets. MIPS first . decade later, Intel HP announced IA-64 large part provide 64-bit address successor 32-bit Intel IA-32 HP Precision architectures. evolutionary AMD64 battle versus revolutionary IA-64, thousand 64-bit address computers Intel based x86. Many early ideas memory hierarchies originated England. years Atlas paper, Wilkes [1965] published first paper describing concept cache, calling “slave”: use discussed fast core memory of, say, 32,000 words slave slower core memory of, say, one million words way practical cases effective access time nearer fast memory slow memory. two-page paper describes direct-mapped cache. Although first publication caches, first implementation probably direct-mapped instruction cache built University Cambridge Scarrott described 1965 IFIP Congress. based tunnel diode memory, fastest form memory available time. Subsequent publication, IBM started project led first commercial computer cache, IBM 360/85. Gibson IBM recognized memory-accessing behavior would significant impact performance. described measure program behavior cache behavior showed miss rate varies programs. Using sample 20 programs (each 3 million references—an incredible number time), Gibson analyzed effectiveness caches using average memory access time metric. Conti, Gibson, Pitowsky described resulting performance 360/85 first paper use term cache 1968. Since early work, become clear caches one important ideas computer architecture software systems well. idea caching found applications operating systems, networking systems, databases, compilers, name few. thousands papers topic caching, continues popular area research. 943One first papers nonblocking caches Kroft 1981, may coined term. later explained first design computer cache Control Data Corporation, using old concepts new mechanisms, hit upon idea allowing two-ported cache continue service accesses miss. Multilevel caches inevitable resolution lack improvement main memory latency higher clock rates microprocessors. field surprise size second- third-level caches, larger main memories past machines. surprise number levels continually increasing, even single-chip microprocessor. Disk Storage 1956, IBM developed first disk storage system moving heads multiple disk surfaces San Jose, helping seed birth magnetic storage industry southern en Silicon Valley. Reynold B. Johnson led development IBM 305 RAMAC (Random Access Method Accounting Control). could store 5 million characters (5 MB) data 50 disks, 24 inches diameter. RAMAC shown Figures e5.17.5 e5.17.6 . Although disk pioneers would amazed size, cost, capacity modern disks, basic mechanical design RAMAC. 944FIGURE E5.17.5 magnetic drum made Digital Development Corporation 1960s used CDC machine. electronics supporting read/write heads seen outside drum. 945FIGURE E5.17.6 RAMAC disk drive IBM, made 1956, first disk drive moving head first multiple platters. IBM storage technology Web site discussion IBM’s major contributions storage technology. Moving-head disks quickly became dominant high-speed magnetic storage, though high cost meant magnetic tape continued used extensively 1970s. next key milestone hard disks removable hard disk drive developed IBM 1962; made possible share expensive drive electronics helped disks overtake tapes preferred storage medium. Figure e5.17.7 shows removable disk drive multiplatter disk used drive. IBM also invente floppy disk drive 1970, originally hold microcode IBM 370 series. Floppy disks became popular PC 10 years later. 946FIGURE E5.17.7 DEC disk drive removable pack. disks became popular starting mid-1960s dominated disk technology Winchester drives late 1970s. drive made mid- 1970s; disk pack drive could hold 80 MB. sealed Winchester disk, developed IBM 1973, completely dominates disk technology today. Winchester disks benefited two related properties. First, reductions cost disk electronics made unnecessary share electronics thus made nonremovable disks economical. Since disk fixed could sealed enclosure, environmental control problems greatly reduced, allowing significant gains density. first disk IBM shipped two spindles, 30 MB disk; moniker “30-30” disk led name Winchester. Winchester disks grew rapidly popularity 1980s, completely replacing removable disks middle decade. historic role IBM disk industry came end 2002, IBM sold disk storage division Hitachi. IBM continues make storage subsystems, purchases disk drives others. 947A Brief History Flash Memory Flash memory invented researchers Toshiba 1980s. invented NOR-based Flash memory 1984 denser NAND-based Flash memory 1989. first use digital cameras, starting CompactFlash form factor Flash memory SmartMedia form factor NAND Flash memory. Today, digital cameras, cell phones, music players, tablets rely Flash memory, increasing fraction laptops use flash memory instead disk. Brief History Databases Although data stores punch cards later magnetic tapes, emergence magnetic disk led modern databases. 1961, Charles Bachman General Electric created pioneering database management system called Integrated Data Store (IDS) take advantage new magnetic disks. 1971, Bachman others published standards manage databases using Cobol programs, named Codasyl approach standards committee served. Many companies offered Codasyl-compatible databases, IBM. IBM introduced IMS 1968, derived IBM’s work NASA Apollo project. Codasyl databases IMS classified navigational databases programs navigate data. Ted Codd, researcher IBM, thought navigational approach wrong-headed. recalled people didn’t write programs dealing old punch card databases. Instead, set data flows series punch card machines would perform simple functions like copy sort. card machines set up, pushed cards get results. view, users declare type ata looking leave computers process it. n 1970, published new way organize access data called relational model. based set theory; data independent implementation users described lookin g declarative, nonprocedural language. paper led considerable controversy within IBM, 948already database product. Codd even arranged public debate Bachman, led internal criticism IBM Codd undermining IMS. good news debate led researchers IBM U.C. Berkeley try demonstrate viability relational databases building Syst em R Ingres. System R 1974–79 demonstrated feasibility and, perhaps importantly, created Structured Query Language (SQL) still widely used today. However, results sufficient convince IBM, researchers left IBM build relational databases companies. Mike Stonebraker Gene Wong interested geographic data systems, 1973 decided pursue relational databases. Rather build IBM mainframes, Ingres project built DEC minicomputers Unix. Ingres important led company tried commercialize ideas, 1000 copies source code openly distributed, trained generation database developers researchers. code people led many companies, including Sybase. Larry Ellison started Oracle first reading papers System R Ingres groups hiring people worked projects. Microsoft later purchased copy Sybase sources became foundation SQL Server product. Relational databases matured 1980s, IBM developing relational databases, including DB2. 1990s saw development object-oriented databases, address impedance mismatch databases programming, evolution parallel databases analytic processing data mining. ACM showered awards community. ACM Turing Award went Charles Bachman 1973 contributions via IDS Codasyl group. Codd 1980 relational model. 1988, developers System R (Donald Chamberlin, Jim Gray, Raymond Lorie, Gianfranco Putzolu, Patricia Selinger, Irving Traiger) shared ACM Systems Software Award developers Ingres (Gerald Held, Michael Stonebraker, Eugene Wong). Jim Gray Turing Award 1998 contributions transaction processing databases. Stonebrake r 949won 2014 contributions concepts practices underlying modern database systems. Finally, first two ACM SIGMOD Innovations Awards went Stonebraker Gray, 2002 2003 editions went Selinger Chamberlin. RAID small-form-factor hard disks PCs mid-1980s led group Berkeley propose redundant arrays inexpensive dis ks (RAID). group worked reduced instruction set computer effort expected much faster processors beco available. two questions were: could done small disks accompanied PCs? could done area I/O keep much faster processors? argued replace one large mainframe drive 50 small drives, could get much greater performance many independent arms. many small drives even offered savings power consumption floor space. downside many disks much lower MTTF. Hence, reasoned advantages redundant disks rotating parity address get greater performance many small drives yet reliability high single mainframe disk. problem experienced explaining ideas researchers heard disk arrays form redundancy, didn’t understand Berkeley proposal. Hence, first RAID paper [Patterson, Gibson, Katz 1987] case arrays small-form-factor disk drives, also something tutorial classification existing work dis k arrays. Mirroring (RAID 1) long used fault-tolerant computers sold Tandem. Thinking Machines arrays 32 data disks seven check disks using ECC correction (RAID 2) 1987, Honeywell Bull RAID 2 product even earlier. Also, disk arrays single parity disk used scientific computers time frame (RAID 3). paper described single parity disk support sector accesses (RAID 4) rotated parity (RAID 5). Chen et al. [1994] survey original RAID ideas, commercial products, developments. 950Unknown Berkeley group, engineers IBM working AS/400 computer also came rotated parity give greater reliability collection large disks. IBM filed pate nt RAID 5 shortly Berkeley group submitted pape r. Patents RAID 1, RAID 2, RAID 3 several companies predate IBM RAID 5 patent, led plenty courtroom action. EMC supplier DRAM boards IBM computers, around 1988 new policies IBM made nearly impossible EMC continue sell IBM memory boards. Berkeley paper crossed desks EMC executives, decided go market dominated IBM disk storage products. paper advocated, model use many small drives compete mainframe drives, EMC announced RAID product 1990. relied mirroring (RAID 1) reliability; RAID 5 products came much later EMC. next year, Micropolis offered RAID 3 product; Compaq offered RAID 4 product; Data General, IBM, NCR offered RAID 5 products. RAID ideas soon spread rest workstation server industry. article explaining RAID Byte magazine led RAID products offered desktop PCs, something surprise Berkeley group. focused n performance good availability, higher availability attractive PC market. Another surprise cost disk arrays. redundant power supplies fans, ability “hot-swap” disk drive, RAID hardware controller itself, redundant disks, on, first disk arrays cost many times cost disks. Perhaps result, “inexpensive” RAID morphed “independent.” Many marketing departments technical writers today know RAID “redundant arrays independent disks.” 2004, 80% nondesktop drive sales found RAIDs. recognition role, 1999 Garth Gibson, Randy Katz, David Patterson received IEEE Reynold B. Johnson Information Storage Award “for development Redundant Arrays Inexpensive Disks (RAID).” Protection Mechanisms 951Architectural support protection varied greatly past 20 years. early computers, virtual memory, protection simple best. 1960s, sophisticated mechanisms supported different protection levels (called rings ) invented. late 1970s early 1980s, elaborate mechanisms protection devised later built; mechanisms supported variety powerful protection schemes allowed controlled instances sharing, way process could share data controlling exactly done data. powerful method, called capabilities , created data object described access rights portion memory. capabilities could passed process es, thus granting access object described capability. Supporting sophisticated protection mechanism complex costly, creation, copying, manipulation capabilities required combination operating system hardware support. Recent computers support simpler protection scheme based virtual memory, similar discussed Section 5.7 . Given current concerns computer security due costs worms viruses, perhaps see renaissance protection research, potentially renewing inter est 20-year-old publications. mentioned text, system virtual machines pioneered IBM part investigation virtual memory. IBM’s first computer virtual memory IBM 360/67, introduced 1967. IBM researchers wrote program CP-67, created illusion several independent 360 computers. wrote interactive, single-user operating system cal led CMS ran virtual machines. CP-67 led product VM/370, today IBM sells z/VM mainframe computers. Brief History Modern Operating Systems MIT developed first timesharing system, CTSS (Compatible Time-Sharing System), 1961. John McCarthy generally given credit idea timesharing, Fernando Corbato systems person realized concept form CTSS. CTSS allowed three people share machine, response time minutes seconds dramatic improvement 952batch processing system replaced. Moreover, demonstrated value interactive computing. Flush success first system, group launch ed second system, MULTICS (Multiplexed Information Computing Service). included many innovations, strong protection, controlled sharing, dynamic libraries. However, suffered “second system effect.” Fred Brooks, Jr. described second system effect classic book lessons learned developing operating system IBM mainframe, Mythical Man Month: one designing successor relatively small, elegant, successful system, tendency become grandiose one’s success design elephantine feature-laden monstrosit y. MULTICS took sharing logical extreme discover issues, including extreme. MIT, General Electric, later Bell Labs tried build economical useful system. Despite great deal time money, failed. UC Berkeley building timesharing system, Cal TSS. (“Cal” nickname University California.) people leading project included Peter Deutsch, Butler Lampson, Chuck Thacker, Ken Thompson. added paging virtual memory hardware SDS 920 wrote operating system it. SDS sold computer SDS-930, first commercially available timesharing system operational hardware software. Thompson graduated joined Bell Labs. others founded Berkeley Computer Corporation (BCC), goal selling time-sharing hardware software. We’ll pick BCC later story, let’s follow Thompson. Bell Labs 1971, Thompson led development simple timesharing system good ideas MULTICS left many complex features. demonstrate contrast, first called UNICS. joined others Bell Labs burned MULTICS experience, renamed UNIX, x coming Phoenix, legendary bird rose ashes. result elegant operating system ever built. Forced live 16-bit address space DEC 953minicomputers, amazing amount functionality per line code. Major contributions pipes, uniform file system, uniform process model, shell user interface allowed users connect programs together using pipes files. Dennis Ritchie joined UNIX team 1973 MIT, experience MULTICS, written high-level language. Like prior operating systems, UNIX written assembly language. Ritchie designed language system implementation called C, used make UNIX portable. 1971 1976, Bell released six editions UNIX timesharing system. Thompson took sabbatical alma mater brought UNIX him. Berkeley many universities began use UNIX popular PDP-11 minicomputer. DEC announced VAX, 32-bit virtual address successor PDP-11, question arose operating system run. UNIX became first operating system migrated different computer ported VAX. Students Berkeley one first VAXes, soon adding features UNIX VAX, paging efficient implementation TCP/IP protocol. Berk eley implementation TCP/IP notable fast. essentially implementation TCP/IP years, since early implementations operating systems consisted copying Berkeley code verbatim, minimal changes integrate local system. Advanced Research Project Agency (ARPA), funded computer science research, asked Stanford professor, Forrest Basket, recommend system academic community use: DEC operating system VMS, led David Cutler, Berkeley version UNIX, led graduate student named Bill Joy. recommended latter, Berkeley UNIX soon became academic standard bearer. Berkeley Software Distribution (BSD) UNIX, first release 1978, essentially one first open source movements. sources shipped tapes, systems developers around world learned craft studying UNIX code. BSD also first split UNIX, AT&T Bell Labs continued develop UNIX own. eventually led forest UNIXes, company compiled UNIX source code 954for architecture. Bill Joy graduated Berkeley helped found Sun Microsystems, naturally Sun OS based BSD UNIX. Among many UNIX flavors Santa Cruz Operation UNIX, HP-UX, IBM’s AIX. AT&T Sun attempted unify UNIX striking deal whereby AT&T Sun would combine forces jointly develop AT&T UNIX. led adverse reaction HP, IBM, others, want competitor supplying code, created Open Sourc e Foundation competing organization. addition UNIX variants companies, public domain versions also proliferated. BSD team Berkeley rewrote substantial portions UNIX could distribute needing license AT&T. eventually led lawsuit, Berkeley won. BSD UNIX soon split FreeBSD, NetBSD, OpenBSD, provided competing camps developers. Apple’s current operating system, OS X, based Free BSD. Let’s go back Berkeley Computer Corporation. Alas, effort commercially viable. time BCC getting trouble, Xerox hired Robert Taylor build comp uter science division new Xerox Palo Alto Research Center (PARC) 1970. returned tour duty ARPA, funded Berkeley research. recruited Deutsch, Lampson, Thacker BCC form core PARC’s team: 11 initial 20 employees BCC, decided build small computers individuals rather large computers groups. first personal computer, called Alto, built technology minicomputers, keyboard, mouse, graphical display, windows. popularized windows led many inventions, including client-server computing, th e Ethernet, print servers. directly inspired Macintosh, successor popular Apple II. IBM long interested selling home, success Apple II led IBM start competing project. contrast tradition, project IBM designed everyt hing components outside company. selected new 16-bit microprocessor Intel, 8086. (To lower costs, started version 8-bit bus, called 8088.) visited Microsoft see small company would willing sell popular Basic interpreter asked recommendatio ns 955for operating system. Gates volunteered Microsoft could deliver interpreter operating system, long paid royalty fee $10 $50 copy rather flat fee. IBM agreed, provided Microsoft could meet deadlines. Microsoft didn’t operating system, time resources build one, Gates knew Seattle company developed operating system Intel 8086. Microsoft purchased QDOS (Quick Dirty Operating System) $15,000, made small change relabeled MS-DOS. MS-DOS simple operating system without modern features—no protection, processes, virtual memory—in part believed wasn’t necessary personal computer. Announced 1980, IBM PC became tremendous success IBM companies relied upon. Microsoft sold 500,000 copies MS-DOS 1983, $10 million income allowed Microsoft start new software projects. seeing version Macintosh development, Microsoft hired people PARC lead reply. Macintosh announced 1984, Windows available PCs following year. originally application ran top DOS, later integrated DOS renamed Windows 2.0. Microsoft hired Cutler DEC lead development Windows NT, new operating system. NT modern operating system protection, processors, much common DEC’s VMS. Today’s PC operating systems sophisticated timesharing systems 20 years ago, yet still suffer need maintain compatibility crippled first PC operating systems suc h MS-DOS. popularity PC led desire UNIX ran it. Many tried develop one, successful written scratch 1991 Linus Torvalds. addition making source code available, like BSD, allowed everyone make changes submit inclusion next release. Linux popularized open source development know today, software getting hundreds volunteers test releases add new features. Many people story awards roles development modern operating systems. McCarthy received 956ACM Turing Award 1971 part contributions timesharing. 1983, Thompson Ritchie received one UNIX. announcement said “the genius UNIX system framework, enables programmers stand work others.” 1990, Corbato received Turing Award contributions CTSS MULTICS. Two years later, Lampson part work personal computing operating systems. Reading Brooks, F.P. [1975]. mythical man-month . Reading: Addison- Wesley classic book explains challenge software engineering using IBM OS development example . Cantin, J.F. M.D. Hill [2001]. “Cache performance selected SPEC CPU2000 benchmarks”, SIGARCH Computer Architecture News 29:4 þ (September), 13–18. reference paper cache miss rates many cache sizes SPEC2000 benchmarks . Chen, P.M., E.K. Lee, G.A. Gibson, R.H. Katz, D.A. Patterson [1994]. “RAID: High-performance, reliable secondary storage”, ACM Computing Surveys 26:2 (June) 145–88. tutorial covering disk arrays advantages organization . Conti, C., D.H. Gibson, S.H. Pitowsky [1968]. “Structural aspects System/360 Model 85, part I: General organization”, IBM Systems J. 7:1, 2–14. classic paper describes first commercial computer use cache resulting performance . Hennessy, J. D. Patterson [2003]. Chapter 5 Computer Architecture: Quantitative Approach , third edition, Morgan Kaufmann Publishers, San Francisco. in-depth coverage variety topics including protection, cache performance out-of-order processors, virtually addressed ca ches, multilevel caches, compiler optimizations, additional latency tole rance mechanisms, cache coherency . Kilburn, T., D.B.G. Edwards, M.J. Lanigan, F.H. Sumner [1962]. “One-level storage system”, IRE Transactions Electronic 957Computers EC-11(April), 223–335. Also appears D.P. Siewiorek, C.G. Bell, A. Newell [1982], Computer Structures: Principles Examples, McGraw-Hill, New York, 135–48. classic paper first proposal virtual memory . LaMarca, A. R.E. Ladner [1996]. “The influence caches performance heaps”, ACM J. Experimental Algorithmics , Vol. 1. paper shows difference complexity analysis algorithm, instruction count performance, memory hierarchy fo ur sorting algorithms . McCalpin, J.D. [1995]. “STREAM: Sustainable Memory Bandwidth High Performance Computers” , https://www.cs.virginia.edu/stream/ . widely used microbenchmark measures performance memory system behind caches . Patterson, D., G. Gibson, R. Katz [1988]. “A case redundant arrays inexpensive disks (RAID)” , SIGMOD Conference, 109–116. classic paper advocates arrays smaller disks introduces RAID levels . Przybylski, S.A. [1990]. Cache Memory Hierarchy Design: Performance-Directed Approach , Morgan Kaufmann Publishers, San Francisco. thorough exploration multilevel memory hierarchies performance . Ritchie, D. [1984]. “The evolution UNIX time-sharing system”, AT&T Bell Laboratories Technical Journal 1984, 1577–1593. history UNIX one inventors . Ritchie, D.M. K. Thompson [1978]. “The UNIX time-sharing system”, Bell System Technical Journal (August) , 1991–2019. paper describing elegant operating system ever invented . Silberschatz, A., P. Galvin, G. Grange [2003]. Operating System Concepts , sixth edition, Addison-Wesley, Reading, MA. operating systems textbook thorough discussion virtual memory, processes process management, protection issues . Smith, A.J. [1982]. “Cache memories,” Computing Surveys 14:3 (September), 473–530. classic survey paper caches. paper defined terminology field served reference many computer designers . Smith, D.K. R.C. Alexander [1988]. Fumbling Future: 958Xerox Invented, Ignored, First Personal Computer , Morrow, New York. popular book explains role Xerox PARC laying foundation today’s computing, Xerox substantially benefit . Tanenbaum, A. [2001]. Modern Operating Systems , second edition, Upper Saddle River, Prentice Hall, NJ. operating system textbook good discussion virtual memory . Wilkes, M. [1965]. “Slave memories dynamic storage allocation”, IEEE Trans. Electronic Computers EC-14:2 (April), 270–71. first classic paper caches . 5.19 Exercises Assume memory byte addressable words 64 bits, unless specified otherwise. 5.1 exercise look memory locality properties matrix computation. following code written C, elements within row stored contiguously. Assume word 64-bit integer. (I=0; I<8; I++) (J=0; J<8000; J++) A[I][J]=B[I][0]+A[J][I]; 5.1.1 [5] <§5.1> many 64-bit integers stored 16- byte cache block? 5.1.2 [5] <§5.1> variable references exhibit temporal locality? 5.1.3 [5] <§5.1> variable references exhibit spatial locality? Locality affected reference order data layout. computation also written Matlab, differs C stores matrix elements within column contiguously memory. I=1:8 J=1:8000 A(I,J)=B(I,0)+A(J,I); end end 5.1.4 [5] <§5.1> variable references exhibit temporal 959locality? 5.1.5 [5] <§5.1> variable references exhibit spatial locality? 5.1.6 [15] <§5.1> many 16-byte cache blocks needed store 64-bit matrix elements referenced using Matlab’s matrix storage? many using C’s matrix storage? (Assume row contains one element.) 5.2 Caches important providing high-performance memory hierarchy processors. list 64-bit memory address references, given word addresses. 0x03, 0xb4, 0x2b, 0x02, 0xbf, 0x58, 0xbe, 0x0e, 0xb 5, 0x2c, 0xba, 0xfd 5.2.1 [10] <§5.3> references, identify binary word address, tag, index given direct-mapped cache 16 one-word blocks. Also list whether reference hit miss, assuming cache initially empty. 5.2.2 [10] <§5.3> references, identify binary word address, tag, index, offset given direct- mapped cache two-word blocks total size eight blocks. Also list reference hit miss, assuming cache initially empty. 5.2.3 [20] <§§5.3, 5.4> asked optimize cache design given references. three direct-mapped cache designs possible, total eight words data: C1 1-word blocks, C2 2-word blocks, C3 4-word blocks. 5.3 convention, cache named according amount data contains (i.e., 4 KiB cache hold 4 KiB data); however, caches also require SRAM store metadata tags valid bits. exercise, examine cache’s configuration affects total amount SRAM needed implement well performance cache. parts, assume caches byte addressable, addresses words 64 bits. 5.3.1 [10] <§5.3> Calculate total number bits required implement 32 KiB cache two-word blocks. 5.3.2 [10] <§5.3> Calculate total number bits required implement 64 KiB cache 16-word blocks. much 960bigger cache 32 KiB cache described Exercise 5.3.1 ? (Notice that, changing block size, doubled amount data without doubling total size cache.) 5.3.3 [5] <§5.3> Explain 64 KiB cache, despite larger data size, might provide slower performance first cache. 5.3.4 [10] <§§5.3, 5.4> Generate series read requests lower miss rate 32 KiB two-way set associative cache cache described Exercise 5.3.1 . 5.4 [15] <§5.3> Section 5.3 shows typical method index direct-mapped cache, specifically (Block address) modulo (Number blocks cache). Assuming 64-bit address 1024 blocks cache, consider different indexing function, specifically (Block address[63:54] XOR Block address[53:44]). possible use index direct-mapped cache? so, explain discuss changes might need made cache. possible, explain why. 5.5 direct-mapped cache design 64-bit address, following bits address used access cache. Tag Index Offset 63–10 9–5 4–0 5.5.1 [5] <§5.3> cache block size (in words)? 5.5.2 [5] <§5.3> many blocks cache have? 5.5.3 [5] <§5.3> ratio total bits required cache implementation data storage bits? Beginning power on, following byte-addressed cache references recorded. 5.5.4 [20] <§5.3> reference, list (1) tag, index, offset, (2) whether hit miss, (3) bytes replaced (if any). 5.5.5 [5] <§5.3> hit ratio? 5.5.6 [5] <§5.3> List final state cache, valid 961entry represented record <index, tag, data>. example, <0, 3, Mem[0xC00]-Mem[0xC1F]> 5.6 Recall two write policies two write allocation policies, combinations implemented either L1 L2 cache. Assume following choices L1 L2 caches: L1 L2 Write through, non-write allocate Write back, write allocate 5.6.1 [5] <§§5.3, 5.8> Buffers employed different levels memory hierarchy reduce access latency. given configuration, list possible buffers needed L1 L2 caches, well L2 cache memory. 5.6.2 [20] <§§5.3, 5.8> Describe procedure handling L1 write-miss, considering components involved possibility replacing dirty block. 5.6.3 [20] <§§5.3, 5.8> multilevel exclusive cache configuration (a block reside one L1 L2 caches), describe procedures handling L1 write-miss L1 read-miss, considering components involved possibility replacing dirty block. 5.7 Consider following program cache behaviors. 5.7.1 [10] <§§5.3, 5.8> Suppose CPU write-through, write- allocate cache achieves CPI 2. read write bandwidths (measured bytes per cycle) RAM cache? (Assume miss generates request one block.) 5.7.2 [10] <§§5.3, 5.8> write-back, write-allocate cache, assuming 30% replaced data cache blocks dirty, read write bandwidths needed CPI 2? 5.8 Media applications play audio video files part class workloads called “streaming” workloads (i.e., bring large amounts data reuse much it). Consider video streaming workload accesses 512 KiB working set 962sequentially following word address stream: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9 … 5.8.1 [10] <§§5.4, 5.8> Assume 64 KiB direct-mapped cache 32-byte block. miss rate address stream above? miss rate sensitive size cache working set? would categorize misses workload experiencing, based 3C model? 5.8.2 [5] <§§5.1, 5.8> Re-compute miss rate cache block size 16 bytes, 64 bytes, 128 bytes. kind locality workload exploiting? 5.8.3 [10] <§5.13> “Prefetching” technique leverages predictable address patterns speculatively bring additional cache blocks particular cache block accessed. One example prefetching stream buffer prefetches sequentially adjacent cache blocks separate buffer particular cache block brought in. data found prefetch buffer, considered hit, moved cache, next cache block prefetched. Assume two-entry stream buffer; and, assume cache latency cache block loaded computation previous cache block completed. miss rate address stream above? 5.9 Cache block size (B) affect miss rate miss latency. Assuming machine base CPI 1, average 1.35 references (both instruction data) per instruction, find block size minimizes total miss latency given following miss rates various block sizes. 5.9.1 [10] <§5.3> optimal block size miss latency 20 ×B cycles? 5.9.2 [10] <§5.3> optimal block size miss latency 24 +B cycles? 5.9.3 [10] <§5.3> constant miss latency, optimal block size? 5.10 exercise, look different ways capacity affects overall performance. general, cache access time 963proportional capacity. Assume main memory accesses take 70 ns 36% instructions access data memory. following table shows data L1 caches attached two processors, P1 P2. 5.10.1 [5] <§5.4> Assuming L1 hit time determines cycle times P1 P2, respective clock rates? 5.10.2 [10] <§5.4> Average Memory Access Time P1 P2 (in cycles)? 5.10.3 [5] <§5.4> Assuming base CPI 1.0 without memory stalls, total CPI P1 P2? processor faster? (When say “base CPI 1.0”, mean instructions complete one cycle, unless either instruc tion access data access causes cache miss.) next three problems, consider addition L2 cache P1 (to presumably make limited L1 cache capacity). Use L1 cache capacities hit times previous table solving problems. L2 miss rate indicated local miss rate. L2 Size L2 Miss Rate L2 Hit Time 1 MiB 95% 5.62 ns 5.10.4 [10] <§5.4> AMAT P1 addition L2 cache? AMAT better worse L2 cache? 5.10.5 [5] <§5.4> Assuming base CPI 1.0 without memory stalls, total CPI P1 addition L2 cache? 5.10.6 [10] <§5.4> would L2 miss rate need order P1 L2 cache faster P1 without L2 cache? 9645.10.7 [15] <§5.4> would L2 miss rate need order P1 L2 cache faster P2 without L2 cache? 5.11 exercise examines effect different cache designs, specifically comparing associative caches direct-mapped caches Section 5.4 . exercises, refer sequence word address shown below. 0x03, 0xb4, 0x2b, 0x02, 0xbe, 0x58, 0xbf, 0x0e, 0x1 f, 0xb5, 0xbf, 0xba, 0x2e, 0xce 5.11.1 [10] <§5.4> Sketch organization three-way set associative cache two-word blocks total size 48 words. sketch style similar Figure 5.18 , clearly show width tag data fields. 5.11.2 [10] <§5.4> Trace behavior cache Exercise 5.11.1 . Assume true LRU replacement policy. reference, identify binary word address, tag, index, offset whether reference hit miss, tags way cache reference handled. 5.11.3 [5] <§5.4> Sketch organization fully associative cache one-word blocks total size eight words. sketch style similar Figure 5.18 , clearly show width tag data fields. 5.11.4 [10] <§5.4> Trace behavior cache Exercise 5.11.3 . Assume true LRU replacement policy. reference, identify binary word address, tag, index, offset, whether reference hit miss, contents cache reference handled. 5.11.5 [5] <§5.4> Sketch organization fully associative cache two-word blocks total size eight words. 965Your sketch style similar Figure 5.18 , clearly show width tag data fields. 5.11.6 [10] <§5.4> Trace behavior cache Exercise 5.11.5 . Assume LRU replacement policy. reference, identify binary word address, tag, index, offset, whether reference hit miss, contents cache reference handled. 5.11.7 [10] <§5.4> Repeat Exercise 5.11.6 using MRU ( recently used) replacement. 5.11.8 [15] <§5.4> Repeat Exercise 5.11.6 using optimal replacement policy (i.e., one gives lowest miss rate). 5.12 Multilevel caching important technique overcome limited amount space first-level cache provide still maintaining speed. Consider processor following parameters: **First Level Cache miss rate per instruction. Assume tal number L1 cache misses (instruction data combined) equal 7% n umber instructions. 5.12.1 [10] <§5.4> Calculate CPI processor table using: 1) first-level cache, 2) second-level direct- mapped cache, 3) second-level eight-way set associative cache. numbers change main memory access time doubles? (Give change absolute CPI percent change.) Notice extent L2 cache hide effects slow memory. 5.12.2 [10] <§5.4> possible even greater cache hierarchy two levels? Given processor second-level, direct-mapped cache, designer wants add third-level cache takes 50 cycles access 13% miss rate. Would provide better performance? general, advantages disadvantages adding 966a third-level cache? 5.12.3 [20] <§5.4> older processors, Intel Pentium Alpha 21264, second level cache external (located different chip) main processor first-level cache. allowed large second-level caches, latency access cache much higher, bandwidth typically lower second-level cache ran lower frequency. Assume 512 KiB off-chip second- level cache miss rate 4%. additional 512 KiB cache lowered miss rates 0.7%, cache total access time 50 cycles, big would cache match performance second-level direct-mapped cache listed above? 5.13 Mean time failures (MTBF), mean time replacement (MTTR), mean time failure (MTTF) useful metrics evaluating reliability availability storage resource. Explore concepts answering questions devic e following metrics: MTTF MTTR 3 Years 1 Day 5.13.1 [5] <§5.5> Calculate MTBF device. 5.13.2 [5] <§5.5> Calculate availability device. 5.13.3 [5] <§5.5> happens availability MTTR approaches 0? realistic situation? 5.13.4 [5] <§5.5> happens availability MTTR gets high, i.e., device difficult repair? imply device low availability? 5.14 exercise examines single error correcting, double error detecting (SEC/DED) Hamming code. 5.14.1 [5] <§5.5> minimum number parity bits required protect 128-bit word using SEC/DED code? 5.14.2 [5] <§5.5> Section 5.5 states modern server memory modules (DIMMs) employ SEC/DED ECC protect 64 bits 8 parity bits. Compute cost/performance ratio code code Exercise 5.14.1 . case, cost relative number parity bits needed performance relative number errors corrected. better? 9675.14.3 [5] <§5.5> Consider SEC code protects 8 bit words 4 parity bits. read value 0x375, error? so, correct error. 5.15 high-performance system B-tree index database, page size determined mainly data size disk performance. Assume that, average, B-tree index page 70% full fix-sized entries. utility page B-tre e depth, calculated log2(entries). following table shows 16-byte entries, 10-year-old disk 10 ms latency 10 MB/s transfer rate, optimal page size 16 K. 5.15.1 [10] <§5.7> best page size entries become 128 bytes? 5.15.2 [10] <§5.7> Based Exercise 5.15.1 , best page size pages half full? 5.15.3 [20] <§5.7> Based Exercise 5.15.2 , best page size using modern disk 3 ms latency 100 MB/s transfer rate? Explain future servers likely larger pages. Keeping “frequently used” (or “hot”) pages DRAM save disk accesses, determine exact meaning “frequently used” given system? Data engineers use cost ratio DRAM disk access quantify reuse time threshold hot pages. cost disk access $Disk/accesses_per_sec, cost keep page DRAM $DRAM_MiB/page_size. typical DRAM disk costs typical database page sizes several time points listed below: 9685.15.4 [20] <§5.7> factors changed keep using page size (thus avoiding software rewrite)? Discuss likeliness current technology cost trends. 5.16 described Section 5.7 , virtual memory uses page table track mapping virtual addresses physical addresses. exercise shows table must updated addresses accessed. following data constitute stream virtual byte addresses seen system. Assume 4 KiB pages, four- entry fully associative TLB, true LRU replacement. pages must brought disk, increment next largest page number. TLB Page table Index Valid Physical Page Disk 9690 1 5 1 0 Disk 2 0 Disk 3 1 6 4 1 9 5 1 11 6 0 Disk 7 1 4 8 0 Disk 9 0 Disk 1 3 b 1 12 5.16.1 [10] <§5.7> access shown above, list whether access hit miss TLB, whether access hit miss page table, whether access page fault, updated state TLB. 5.16.2 [15] <§5.7> Repeat Exercise 5.16.1 , time use 16 KiB pages instead 4 KiB pages. would advantages larger page size? disadvantages? 5.16.3 [15] <§5.7> Repeat Exercise 5.16.1 , time use 4 KiB pages two-way set associative TLB. 5.16.4 [15] <§5.7> Repeat Exercise 5.16.1 , time use 4 KiB pages direct mapped TLB. 5.16.5 [10] <§§5.4, 5.7> Discuss CPU must TLB high performance. would virtual memory accesses handled TLB? 5.17 several parameters affect overall size page table. Listed key page table parameters. Virtual Address Size Page Size Page Table Entry Size 32 bits 8 KiB 4 bytes 5.17.1 [5] <§5.7> Given parameters shown above, calculate maximum possible page table size system running five processes. 5.17.2 [10] <§5.7> Given parameters shown above, calculate total page table size system running five applications utilize half virtual memory available, given two-level page table approach 256 entries 1st level. Assume entry main page table 6 bytes. 970Calculate minimum maximum amount memory required page table. 5.17.3 [10] <§5.7> cache designer wants increase size 4 KiB virtually indexed, physically tagged cache. Given page size shown above, possible make 16 KiB direct- mapped cache, assuming two 64-bit words per block? would designer increase data size cache? 5.18 exercise, examine space/time optimizations page tables. following list provides parameters virtual memory system. 5.18.1 [10] <§5.7> single-level page table, many page table entries (PTEs) needed? much physical memory needed storing page table? 5.18.2 [10] <§5.7> Using multi-level page table reduce physical memory consumption page tables keeping active PTEs physical memory. many levels page tables needed segment tables (the upper-level page tables) allowed unlimited size? many memory references needed address translation missing TLB? 5.18.3 [10] <§5.7> Suppose segments limited 4 KiB page size (so paged). 4 bytes large enough page table entries (including segment tables? 5.18.4 [10] <§5.7> many levels page tables needed segments limited 4 KiB page size? 5.18.5 [15] <§5.7> inverted page table used optimize space time. many PTEs needed store page table? Assuming hash table implementation, common case worst case numbers memory references needed servicing TLB miss? 5.19 following table shows contents four-entry TLB. 9715.19.1 [5] <§5.7> scenarios would entry 3’s valid bit set zero? 5.19.2 [5] <§5.7> happens instruction writes VA page 30? would software managed TLB faster hardware managed TLB? 5.19.3 [5] <§5.7> happens instruction writes VA page 200? 5.20 exercise, examine replacement policies affect miss rate. Assume two-way set associative cache four one-word blocks. Consider following word address sequence: 0, 1, 2, 3, 4, 2, 3, 4, 5, 6, 7, 0, 1, 2, 3, 4, 5, 6, 7, 0. Consider following address sequence: 0, 2, 4, 8, 10, 12, 14, 16, 0 5.20.1 [5] <§§5.4, 5.8> Assuming LRU replacement policy, accesses hits? 5.20.2 [5] <§§5.4, 5.8> Assuming MRU ( recently used ) replacement policy, accesses hits? 5.20.3 [5] <§§5.4, 5.8> Simulate random replacement policy flipping coin. example, “heads” means evict first block set “tails” means evict second block set. many hits address sequence exhibit? 5.20.4 [10] <§§5.4, 5.8> Describe optimal replacement policy sequence. accesses hits using policy? 5.20.5 [10] <§§5.4, 5.8> Describe difficult implement cache replacement policy optimal address sequences. 5.20.6 [10] <§§5.4, 5.8> Assume could make decision upon memory reference whether want requested 972address cached. effect could miss rate? 5.21 One biggest impediments widespread use virtual machines performance overhead incurred running virtual machine. Listed various performance parameters application behavior. 5.21.1 [10] <§5.6> Calculate CPI system listed assuming accesses I/O. CPI VMM overhead doubles? cut half? virtual machine software company wishes limit performance degradation 10%, longest possible penalty trap VMM? 5.21.2 [15] <§5.6> I/O accesses often large effect overall system performance. Calculate CPI machine using performance characteristics above, assuming non-virtualized system. Calculate CPI again, time using virtualized system. CPIs change system half I/O accesses? 5.22 [15] <§§5.6, 5.7> Compare contrast ideas virtual memory virtual machines. goals compare? pros cons each? List cases virtual memory desired, cases virtual machines desired. 5.23 [10] <§5.6> Section 5.6 discusses virtualization assumption virtualized system running ISA underlying hardware. However, one possible use virtualization emulate non-native ISAs. example QEMU, emulates variety ISAs MIPS, SPARC, PowerPC. difficulties involved kind virtualization? possible emulated system run faster native ISA? 5.24 exercise, explore control unit cache controller processor write buffer. Use finite st ate machine found Figure 5.39 starting point designing finite state machines. Assume cache controller simple direct-mapped cache described page 453 (Figure 5.39 Section 5.9 ), add write buffer 973capacity one block. Recall purpose write buffer serve temporary storage processor doesn’t wait two memory accesses dirty miss. Rather writing back dirty block reading new block, buffers dirty block immediately begins reading new block. dirty block written main memory processor working. 5.24.1 [10] <§§5.8, 5.9> happen processor issues request hits cache block written back main memory write buffer? 5.24.2 [10] <§§5.8, 5.9> happen processor issues request misses cache block written back main memory write buffer? 5.24.3 [30] <§§5.8, 5.9> Design finite state machine enable use write buffer. 5.25 Cache coherence concerns views multiple processors given cache block. following data show two processors read/write operations two different words cache block X (initially X[0]=X[1]=0). P1 P2 X[0] ++; X[1] = 3; X[0] = 5; X[1] +=2; 5.25.1 [15] <§5.10> List possible values given cache block correct cache coherence protocol implementation. List least one possible value block protocol doesn’t ensure cache coherency. 5.25.2 [15] <§5.10> snooping protocol, list valid operation sequence processor/cache finish read/write operations. 5.25.3 [10] <§5.10> best-case worst-case numbers cache misses needed execute listed read/write instructions? Memory consistency concerns views multiple data items. following data show two processors read/write operations different cache blocks (A B initially 0). P1 P2 = 1; B = 2; A+=2; B++; C = B; = A; 5.25.4 [15] <§5.10> List possible values C 974implementations ensure consistency assumptions page 455. 5.25.5 [15] <§5.10> List least one possible pair values C assumptions maintained. 5.25.6 [15] <§§5.3, 5.10> various combinations write policies write allocation policies, combinations make protocol implementation simpler? 5.26 Chip multiprocessors (CMPs) multiple cores caches single chip. CMP on-chip L2 cache design interesting trade-offs. following table shows miss rates hit latencies two benchmarks private vs. shared L2 cache designs. Assume L1 cache 3% miss rate 1- cycle access time. Private Shared Benchmark miss rate 10% 4% Benchmark B miss rate 2% 1% Assume following hit latencies: Private Cache Shared Cache Memory 5 20 180 5.26.1 [15] <§5.13> cache design better benchmarks? Use data support conclusion. 5.26.2 [15] <§5.13> Off-chip bandwidth becomes bottleneck number CMP cores increases. bottleneck affect private shared cache systems differently? Choose best design latency first off-chip link doubles. 5.26.3 [10] <§5.13> Discuss pros cons shared vs. private L2 caches single-threaded, multi-threaded, multiprogrammed workloads, reconsider on-chip L3 caches. 5.26.4 [10] <§5.13> Would non-blocking L2 cache produce improvement CMP shared L2 cache private L2 cache? Why? 5.26.5 [10] <§5.13> Assume new generations processors double number cores every 18 months. maintain level per-core performance, much off-chip memory bandwidth needed processor released three 975years? 5.26.6 [15] <§5.13> Consider entire memory hierarchy. kinds optimizations improve number concurrent misses? 5.27 exercise show definition web server log examine code optimizations improve log processing speed. data structure log defined follows: struct entry { int srcIP; // remote IP address char URL[128]; // request URL (e.g., “GET index.htm l”) long long refTime; // reference time int status; // connection status char browser[64]; // client browser name } log [NUM_ENTRIES]; Assume following processing function log: topK_sourceIP (int hour); function determines frequently observed source IPs given hour. 5.27.1 [5] <§5.15> fields log entry accessed given log processing function? Assuming 64-byte cache blocks prefetching, many cache misses per entry given function incur average? 5.27.2 [5] <§5.15> reorganize data structure improve cache utilization access locality? 5.27.3 [10] <§5.15> Give example another log processing function would prefer different data structure layout. functions important, would rewrite program improve overall performance? Supplement discussion code snippet data. 5.28 problems below, use data “Cache Performance SPEC CPU2000 Benchmarks” (http://www.cs.wisc.edu/multifacet/misc/spec2000cache-data/ ) pairs benchmarks shown following table. a.Mesa/gcc b.mcf/swim 5.28.1 [10] <§5.15> 64 KiB data caches varying set associativities, miss rates broken miss types (cold, capacity, conflict misses) benchmark? 9765.28.2 [10] <§5.15> Select set associativity used 64 KiB L1 data cache shared benchmarks. L1 cache directly mapped, select set associativity 1 MiB L2 cache. 5.28.3 [20] <§5.15> Give example miss rate table higher set associativity actually increases miss rate. Construct cache configuration reference stream demonstrate this. 5.29 support multiple virtual machines, two levels memory virtualization needed. virtual machine still controls mapping virtual address (VA) physical address (PA), hypervisor maps physical address (PA) virtual machine actual machine address (MA). accelerate mappings, software approach called “shadow paging” duplicates virtual machine’s page tables hypervisor, intercepts VA PA mapping changes keep copies consistent. remove complexity shadow page tables, hardware approach called nested page table (NPT) explicitly supports two classes page tables (VA ⇒PA PA ⇒MA) walk tables purely hardware. Consider following sequence operations: (1) Create process; (2) TLB miss; (3) page fault; (4) context switch; 5.29.1 [10] <§§5.6, 5.7> would happen given operation sequence shadow page table nested page table, respectively? 5.29.2 [10] <§§5.6, 5.7> Assuming x86-based four-level page table guest nested page table, many memory references needed service TLB miss native vs. nested page table? 5.29.3 [15] <§§5.6, 5.7> Among TLB miss rate, TLB miss latency, page fault rate, page fault handler latency, metrics important shadow page table? important nested page table? Assume following parameters shadow paging system. 5.29.4 [10] <§5.6> benchmark native execution CPI 1, CPI numbers using shadow page tables vs. NPT (assuming page table virtualization overhead)? 9775.29.5 [10] <§5.6> techniques used reduce page table shadowing induced overhead? 5.29.6 [10] <§5.6> techniques used reduce NPT induced overhead? Answers Check §5.1, page 369: 1 4. (3 false cost memory hierarchy varies per computer, 2016 highest cost usually DRAM.) §5.3, page 390: 1 4: lower miss penalty enable smaller blocks, since don’t much latency amortize, yet higher memory bandwidth usually leads larger blocks, since miss penalty slightly larger. §5.4, page 409: 1. §5.8, page 449: 2. (Both large block sizes prefetching may reduce compulsory misses, 1 false.) 9786 Parallel Processors Client Cloud Abstract chapter explains computer architects long sought create powerful computers simply connecting many existing smaller ones. shows multiprocessor software must design ed work variable number processors. One consequence th energy become overriding issue microprocessors datacenters. Replacing large inefficient proce ssors many smaller, efficient processors deliver better perfo rmance per joule large small, software efficiently use them. Improved energy efficiency joins scalable performance making case use multiprocessors. Th parallel revolution hardware/software interface perhaps greatest challenge facing field. chapter explains revolution provide many new research business prospec ts inside outside field, companies dominate multicore era may ones dominated uniprocessor era. emphasizes understanding underlyin g hardware trends learning adapt software innovation technical advances occur years ahead. Keywords multicore uniprocessor; parallel parallelism; parallel processing pro grams; multiprocessor; uniprocessor; shared memory multiprocessors; cluster “I swing big, everything I’ve got. hit big miss big. like live big can.” 979Babe Ruth American baseball player OUTLINE 6.1 Introduction 492 6.2 Difficulty Creating Parallel Processing Progr ams 494 6.3 SISD, MIMD, SIMD, SPMD, Vector 499 6.4 Hardware Multithreading 506 6.5 Multicore Shared Memory Multiprocessors 509 6.6 Introduction Graphics Processing Units 514 6.7 Clusters, Warehouse Scale Computers, Message - Passing Multiprocessors 521 6.8 Introduction Multiprocessor Network Topologies 526 6.9 Communicating Outside World: Cluster Networking 529 6.10 Multiprocessor Benchmarks Performance Models 530 6.11 Real Stuff: Benchmarking Rooflines Int el Core i7 960 NVIDIA Tesla GPU 540 6.12 Going Faster: Multiple Processors Matrix Multipl 545 6.13 Fallacies Pitfalls 548 6.14 Concluding Remarks 550 6.15 Historical Perspective Reading 553 6.16 Exercises 553 980Multiprocessor Cluster Organization 6.1 Introduction Mountains Moon, Valley Shadow, Ride, boldly ride shade replied—If seek El Dorado! Edgar Allan Poe, “El Dorado,” stanza 4, 1849 Computer architects long sought “The City Gold” (El Dorado) computer design: create powerful computers simpl connecting many existing smaller ones. golden vision fountainhead multiprocessors . Ideally, customers order many processors afford receive commensurate amount performance. Thus, multiprocessor software must designed work variable number processors. mentioned Chapter 1 , energy become overriding issue microprocessors datacenters. Replacing large inefficie nt processors many smaller, efficient processors deliver better performance per joule large small, software efficiently use them. Therefore, improved energy efficiency joins scalable performance case multiprocessors. multiprocessor computer system least two processors. computer contrast uniprocessor, one, increasingly 981hard find today. Since multiprocessor software scale, designs support operation presence broken hardware; is, single processor fails multiprocessor n processors, systems would continue provide service n – 1 processors. Hence, multiprocessors also improve availability (see Chapter 5). High performance mean greater throughput independent tasks, called task-level parallelism process-level parallelism . tasks independent single-threaded applications, important popular use multiple processors. approach contrasts running single job multiple processors. use term parallel processing program refer single program runs multiple processors simultaneously. task-level parallelism process-level parallelism Utilizing multiple processors running independent progr ams simultaneously. 982 parallel processing program single program runs multiple processors simultaneousl y. long scientific problems needed much faster computers, class problems used justify many novel parallel computers decades. problems handled simply today, using cluster composed microprocessors housed many independent servers (see Section 6.7 ). addition, clusters serve equally demanding applications outside sciences, search engines, Web servers, email servers, databases. cluster set computers connected local area network function single large multiprocessor. described Chapter 1 , multiprocessors shoved spotlight energy problem means future increases performance primarily come explicit hardware parallelism rather much higher clock rates vastly improved CPI. said Chapter 1 , called multicore microprocessors instead multiprocessor microprocessors, presumably avoid redundancy naming. Hence, processors often called cores multicore chip. number cores expected increase Moore’s Law . multicores almost always Shared Memory Processors (SMPs) , usually share single physical address space. We’ll see SMPs Section 6.5 . multicore microprocessor microprocessor containing multiple processors (“cores”) single integrated circuit. Virtually microprocessors today desktops servers multicore. shared memory multiprocessor (SMP) parallel processor single physical address space. 983The state technology today means programmers care performance must become parallel programmers, sequential code means slow code. tall challenge facing industry create hardware software make easy write correct parallel processing programs execute efficiently performance energy number cores per chip scales. abrupt shift microprocessor design caught many guard, great deal confusion terminology means. Figure 6.1 tries clarify terms serial, parallel, sequential, concurrent. columns figure represent software, either inherently sequential concurrent. rows figure represent hardware, either serial parallel. example, programmers compiler think sequential programs: steps include parsing, code generation, optimization, on. contrast, programmers operating systems normally think concurrent programs: cooperating processes handling I/O events due independent jobs running computer. 984FIGURE 6.1 Hardware/software categorization examples application perspective concurrency versus hardware perspective parallelism. point two axes Figure 6.1 concurrent software run serial hardware, operating systems Intel Pentium 4 uniprocessor, parallel hardware, OS recent Intel Core i7. true sequential software. example, MATLAB programmer writes matrix multiply thinking sequentially, could run serially Pentium 4 parallel Intel Core i7. might guess challenge parallel revolutio n figuring make naturally sequential software high performance parallel hardware, also make concurrent programs high performance multiprocessors number processors increases. distinction made, rest chapter use parallel processing program parallel software mean either sequential concurrent software running parallel hardware. next section chapter describes hard create efficient parallel processing programs. proceeding path parallelism, don’t forget initial incursions earlier chapters: Chapter 2 , Section 2.11 : Parallelism Instructions: Synchronization Chapter 3 , Section 3.6 : Parallelism Computer Arithmetic: Subword Parallelism Chapter 4 , Section 4.10 : Parallelism via Instructions Chapter 5 , Section 5.10 : Parallelism Memory Hierarchy: Cache Coherence Check True false: benefit multiprocessor, application must concurrent. 9856.2 Difficulty Creating Parallel Processing Programs difficulty parallelism hardware; fe w important application programs rewritten complete tasks sooner multiprocessors. difficult write softw uses multiple processors complete one task faster, problem gets worse number processors increases. so? parallel processing programs much harder develop sequential programs? first reason must get better performance better energy efficiency parallel processing program multiprocessor; otherwise, would use sequential prog ram uniprocessor, sequential programming simpler. fact, uniprocessor design techniques, superscalar out-of- order execution, take advantage instruction-level parallelism ( see Chapter 4 ), normally without involvement programmer. innovations reduced demand rewriting programs multiprocessors, since programmers could nothing yet th eir sequential programs would run faster new computers. difficult write parallel processing programs fast, especially number processors increases? Chapter 1 , used analogy eight reporters trying write single sto ry hopes work eight times faster. succeed, task must broken eight equal-sized pieces, otherwise reporters would idle waiting ones lar ger pieces finish. Another speed-up obstacle could reporters would spend much time communicating instead writing pieces story. analogy parallel programming, challenges include scheduling, partitioning work parallel pieces, balancing load evenly workers, time synchronize, overhead communication parties. challenge stiffer reporters newspaper story th e processors parallel programming. discussion Chapter 1 reveals another obstacle, namely Amdahl’s Law. reminds us even small parts program must parallelized program make good use many 986cores. Speed-up Challenge Example Suppose want achieve speed-up 90 times faster 100 processors. percentage original computation sequential? Answer Amdahl’s Law ( Chapter 1 ) says reformulate Amdahl’s Law terms speed-up versus initial execution time: formula usually rewritten assuming execution time 1 unit time, execution time affected improvement considered fraction orig inal execution time: Substituting 90 speed-up 100 amount improvement formula above: 987Then simplifying formula solving fraction time affected: Thus, achieve speed-up 90 100 processors, sequential percentage 0.1%. However, applications plenty parallelism, shall see next. Speed-up Challenge: Bigger Problem Example Suppose want perform two sums: one sum 10 scalar variables, one matrix sum pair two-dimensional arrays, dimensions 10 10. let’s assume matrix sum parallelizable; we’ll see soon parallelize scalar sums. speed-up get 10 versus 40 processors? Next, calculate speed-ups assuming matrices grow 20 20. Answer assume performance function time addition, t, 10 additions benefit parallel processors 100 additions do. time single processor 110 t, execution time 10 processors 988so speed-up 10 processors 110 t/20t = 5.5. execution time 40 processors speed-up 40 processors 110 t/12.5 = 8.8. Thus, problem size, get 55% potential speed-up 10 processors, 22% 40. Look happens increase matrix. sequential program takes 10 + 400 = 410 t. execution time 10 processors speed-up 10 processors 410 t/50t = 8.2. execution time 40 processors speed-up 40 processors 410 t/20t = 20.5. Thus, larger problem size, get 82% potential speed-up 10 processors 51% 40. examples show getting good speed-up multiprocessor keeping problem size fixed harder getting good speed-up increasing size problem. insight allows us introduce two terms describe ways sc ale up. Strong scaling means measuring speed-up keeping problem size fixed. Weak scaling means problem size grows proportionally increase number processor s. 989Let’s assume size problem, M, working set main memory, P processors. memory per processor strong scaling approximately M/P, weak scaling, M. strong scaling Speed-up achieved multiprocessor without increasing si ze problem. weak scaling Speed-up achieved multiprocessor increasing size problem proportionally increase number processors. Note memory hierarchy interfere conventional wisdom weak scaling easier strong scaling. example, weakly scaled dataset longer fits last level cache multicore microprocessor, resulting performance could much worse using strong scaling. Depending application, argue either scaling approach. example, TPC-C debit-credit database benchmark 990requires scale number customer accounts proportion higher transactions per minute. argument it’s nonsensical think given customer base suddenly going start using ATMs 100 times day bank gets faster computer. Instead, you’re going demonstrate system perform 100 times numbers transactions per minute, run experiment 100 times many customers. Bigger problems often need data, argument weak scaling. final example shows importance load balancing. Speed-up Challenge: Balancing Load Example achieve speed-up 20.5 previous larger problem 40 processors, assumed load perfectly balanced. is, 40 processors 2.5% work do. Instead, show impact speed-up one processor’s load higher rest. Calculate twice load (5%) five times load (12.5%) hardest working processor. well utilized rest processors? Answer one processor 5% parallel load, must 5%×400 20 additions, 39 share remaining 380. Since operating simultaneously, calculate execution time maximum speed-up drops 20.5 410 t/30t = 14. remaining 39 processors utilized less half time: waiting 20 hardest working processor finish, compute 380t/39 =9.7 t. one processor 12.5% load, must perform 50 additions. formula is: 991The speed-up drops even 410 t/60t = 7. rest processors utilized less 20% time (9 t/50t). example demonstrates importance balancing load, single processor twice load others cuts speed- third, five times load one processor reduces speed-up almost factor three. better understand goals challenges parallel processing, give overview rest chapter. Section 6.3 describes much older classification scheme Figure 6.1 . addition, describes two styles instruction set architectures support running sequential applications n parallel hardware, namely SIMD vector . Section 6.4 describes multithreading , term often confused multiprocessing, part relies upon similar concurr ency programs. Section 6.5 describes first two alternatives fundamental parallel hardware characteristic, whether processors systems rely upon single physical address space. mentioned above, two popular versions alternatives called shared memory multiprocessors (SMPs) clusters , section covers former. Section 6.6 describes relatively new style computer graphics hardware community, called graphics-processing unit (GPU) also assumes single physical address. ( Appendix B describes GPUs even detail.) Section 6.7 describes clusters, popular example computer multiple physical address spaces. Section 6.8 shows typical topologies used connect many processors together, either server nodes cluster cores n microprocessor. Section 6.9 describes hardware software communicating nodes cluster using Ethernet. shows optimize performance using custom software hardware. next discuss difficulty finding parallel benchmarks Section 6.10 . section also includes simple, yet insightful performance model helps desi gn applications well architectures. use model well 992parallel benchmarks Section 6.11 compare multicore computer GPU. Section 6.12 divulges final largest step journey accelerating matrix multiply. matrices don’t fit cache, parallel processing uses 16 cores improve performance factor 14. close fallacies pitfalls conclusions parallelism. next section, introduce acronyms probably already seen identify different types parallel computers . Check True false: Strong scaling bound Amdahl’s Law. 6.3 SISD, MIMD, SIMD, SPMD, Vector One categorization parallel hardware proposed 1960s still used today. based number instruction streams number data streams. Figure 6.2 shows categories. Thus, conventional uniprocessor single instruction stream single data stream, conventional multiprocessor multiple instruction streams multiple data streams. two categories abbreviated SISD MIMD , respectively. SISD Single Instruction stream, Single Data stream. uniprocessor. MIMD Multiple Instruction streams, Multiple Data streams. multiprocessor. 993FIGURE 6.2 Hardware categorization examples based number instruction streams data streams: SISD, SIMD, MISD, MIMD. possible write separate programs run different processors MIMD computer yet work together grander, coordinated goal, programmers normally write single program runs processors MIMD computer, relying conditional statements different processors hould execute distinct sections code. style called Single Program Multiple Data (SPMD) , normal way program MIMD computer. SPMD Single Program, Multiple Data streams. conventional MIMD programming model, single program runs across processors. closest come multiple instruction streams single data stream (MISD) processor might “stream processor” would perform series computations single data stream pipelined fashion: parse input network, decrypt data, decompress it, search match, on. inverse MISD much popular. SIMD computers operate vectors data. example, single SIMD instruction might add 64 numbers sending 64 data streams 64 ALUs form 64 sums within single clock cycle. subword parallel instructi ons saw Sections 3.6 3.7 another example SIMD; indeed, middle letter Intel’s SSE acronym stands SIMD . SIMD Single Instruction stream, Multiple Data streams. instruction applied many data streams, vector processor. virtues SIMD parallel execution units synchronized, respond single instruction emanates single program counter (PC). programmer’s 994perspective, close already familiar SISD. Although every unit executing instruction, execution unit address registers, unit different data addresses. Thus, terms Figure 6.1 , sequential application might compiled run serial hardware organized SISD parallel hardware organized SIMD. original motivation behind SIMD amortize cost control unit dozens execution units. Another advantage reduced instruction bandwidth space—SIMD needs one copy code simultaneously executed, whil e message-passing MIMDs may need copy every processor shared memory MIMD need multiple instruction caches. SIMD works best dealing arrays loops. Hence, parallelism work SIMD, must great deal identically structured data, called data-level parallelism . SIMD weakest case switch statements, execution unit must perform different operation data, depending data has. Execution units wrong data must disabled units proper data may continue. n cases, situations, SIMD processors essentially run 1/ nth peak performance. so-called array processors inspired SIMD category faded history (see Section 6.15 online), two current interpretations SIMD remain active today. data-level parallelism Parallelism achieved performing operation independent data. SIMD x86: Multimedia Extensions described Chapter 3 , subword parallelism narrow integer data original inspiration Multimedia Extension (MMX) instructions x86 1996. Moore ’s Law continued, instructions added, leading first Streaming SIMD Extensions (SSE) Advanced Vector Extensions (AVX). AVX supports simultaneous execution four 64-bit floating-point numbers. width operation registers encoded opcode f 995these multimedia instructions. data width register operations grew, number opcodes multimedia instructions exploded, hundreds SSE AVX instructions (see Chapter 3 ). Vector older and, shall see, elegant interpretation SIMD called vector architecture , closely identified computers designed Seymour Cray starting 1970s. also great match problems lots data-level parallelism. Rather 64 ALUs perform 64 additions simultaneously, like old array processors, vector architectures pipelin ed ALU get good performance lower cost. basic philosophy vector architecture collect data elements memory , put order large set registers, operate sequentially registers using pipelined execution units , write results back memory. key feature vector architectures therefore set vector registers. Thus, vecto r architecture might 32 vector registers, 64 64-bit elements. 996 Comparing Vector Conventional Code Example Suppose extend RISC-V instruction set architecture vector instructions vector registers. Vector operations us e names RISC-V operations, suffix “V” appended. example, fadd.d.v adds two double-precision vectors. Let’s also add 32 vector registers, v0—v31, sixty- four 64-bit elements. vector instructions take input either pair vector (V) registers ( fadd.d.v ) vector register scalar register ( fadd.d.vs ). latter case, value scalar register used input operations—the operation fadd.d.vs add contents scalar register element vector register. names fld.v fsd.v denote vector load vector store, load store entire vector double- precision data. One operand vector register loaded stored; operand, RISC-V general-purpose register, starting address vector memory. Given short description, show conventional RISC-V cod e versus vector RISC-V code 997where X vectors 64 double precision floating-point numbers, initially resident memory, scalar double precision variable. (This example so-called DAXPY loop forms inner loop Linpack benchmark; DAXPY stands double precision a×X plus Y.) Assume starting addresses X x19 x20, respectively. Answer conventional RISC-V code DAXPY: fld f0, a(x3) // load scalar addi x5, x19, 512 // end array X loop: fld f1, 0(x19) // load x[i] fmul.d f1, f1, f0 // * x[i] fld f2, 0(x20) // load y[i] fadd.d f2, f2, f1 // * x[i] + y[i] fsd f2, 0(x20) // store y[i] addi x19, x19, 8 // increment index x addi x20, x20, 8 // increment index bltu x19, x5, loop // repeat done hypothetical RISC-V vector code DAXPY: fld f0, a(x3) // load scalar fld.v v0, 0(x19) // load vector x fmul.d.vs v0, v0, f0 // vector-scalar multiply fld.v v1, 0(x20) // load vector fadd.d.v v1, v1, v0 // vector-vector add fsd.v v1, 0(x20) // store vector interesting comparisons two code segments example. dramatic vector processor greatly reduces dynamic instruction bandwidth, executing six instructions versus 500 basline RI SC- V architecture. reduction occurs vector operations work 64 elements time overhead instructions constitute nearly half loop RISC-V present vector code. might expect, reduction n instructions fetched executed saves energy. Another important difference frequency pipeline 998hazards ( Chapter 4 ). straightforward RISC-V code, every fadd.d must wait fmul.d , every fsd must wait fadd.d , every fadd.d fmul.d must wait fld. vector processor, vector instruction stall firs element vector, subsequent elements flow smoothly pipeline. Thus, pipeline stalls required p er vector operation , rather per vector element . example, pipeline stall frequency RISC-V 64 times higher vector version RISC-V. pipe line stalls eliminated RISC-V unrolling loop (see Chapter 4 ). However, large difference instruction bandwidth cannot reduced. Since vector elements independent, operated parallel, much like subword parallelism Intel x86 AVX instructions. modern vector computers vector functi onal units multiple parallel pipelines (called vector lanes ; see Figures 6.2 6.3) produce two results per clock cycle. Elaboration 999The loop example exactly matched vector length. loops shorter, vector architectures use register reduces length vector operations. loops larger, add bookkeeping code iterate full-length vector operations handle leftovers. latter process called strip mining . FIGURE 6.3 Using multiple functional units improve performance single vector add instruction, C =A +B. vector processor (a) left single add pipeline complete one addition per cycle. vector processor (b) right four add pipelines lanes complete four additions per cycle. elements within single vector add instruction interleaved across four lanes. Vector versus Scalar Vector instructions several important properties compare conventional instruction set architectures, called scalar architectures context: 1000 single vector instruction specifies great deal work—it equivalent executing entire loop. instruction fetch decode bandwidth needed dramatically reduced. using vector instruction, compiler programmer indicates computation result vector independent computation results vector, hardware check data hazards within vector instruction. Vector architectures compilers reputation making much easier using MIMD multiprocessors write efficient applications contain data-level parallelism. Hardware need check data hazards two vector instructions per vector operand, every element within vectors. Reduced checking save energy well time. Vector instructions access memory known access pattern. vector’s elements adjacent, fetching vector set heavily interleaved memory banks works well. Thus, cost latency main memory seen entire vector, rather word vector. complete loop replaced vector instruction whose behavior predetermined, control hazards would normally arise loop branch nonexistent. savings instruction bandwidth hazard checking plus efficient use memory bandwidth give vector architectur es advantages power energy versus scalar architectures. reasons, vector operations made faster sequence scalar operations number data items, designers motivated include vector units application domain often use them. Vector versus Multimedia Extensions Like multimedia extensions found x86 AVX instructions, vector instruction specifies multiple operations. However, multimedia extensions typically denote operations vector specifies dozens operations. Unlike multimedia extensions, number elements vector operation n 1001the opcode separate register. distinction means different versions vector architecture implemen ted different number elements changing content register hence retain binary compatibility. contrast, new large set opcodes added time “vector” length changes multimedia extension architecture x86: MMX, SSE, SSE2, AVX, AVX2, …. Also, unlike multimedia extensions, data transfers need contiguous. Vectors support strided accesses, th e hardware loads every nth data element memory, indexed accesses, hardware finds addresses items loaded vector register. Indexed accesses also called gather- scatter , indexed loads gather elements main memory contiguous vector elements, indexed stores scatter ctor elements across main memory. Like multimedia extensions, vector architectures easily captur e flexibility data widths, easy make vector operatio n work 32 64-bit data elements 64 32-bit data elements 128 16-bit data elements 256 8-bit data elements. parallel semantics vector instruction allows implementation execute operations using deeply pipelined functional unit, array parallel functional units, combination parallel pipelined functional units. Figure 6.3 illustrates improve vector performance using parallel pipelines exec ute vector add instruction. 1002Vector arithmetic instructions usually allow element N one vector register take part operations element N vector registers. dramatically simplifies const ruction highly parallel vector unit, structured multiple parallel vector lanes . traffic highway, increase peak throughput vector unit adding lanes. Figure 6.4 shows structure four-lane vector unit. Thus, going four lanes one lane reduces number clocks per vector instruction roughly factor four. multiple lanes advantageous, applications architecture must support long vectors. Otherwise, execute quickly you’ll run instructions, requiring instruction level parallel techniques like Chapter 4 supply enough vector instructions. vector lane One vector functional units portion vector register file. Inspired lanes highways increase traffic speed, multiple lanes execute vector operations simultaneously . 10031004FIGURE 6.4 Structure vector unit containing four lanes. vector-register storage divided across lanes, lane holding every fourth element vector register. figure shows three vector functional units: FP add, FP multiply, loa d- store unit. vector arithmetic units contains four execution pipelines, one per lane, acts concert complete single vector instruction. Note section vector-register file needs provide enough read write ports (see Chapter 4 ) functional units local lane. Generally, vector architectures efficient way execut e data parallel processing programs; better matches compiler technology multimedia extensions; easier evolve time multimedia extensions x86 architecture. Given classic categories, next see exploit parallel streams instructions improve performance single 1005processor, reuse multiple processors. Check True false: exemplified x86, multimedia extensions thought vector architecture short vectors support contiguous vector data transfers. Elaboration Given advantages vector, aren’t popular outside high-performance computing? concerns larger state vector registers increasing context switch time difficulty handling page faults vector loads stores, SIMD instructions achieved benefits vector instructions. addition, long advances instructio n- level parallelism could deliver performance promise Moore’s Law , little reason take chance changing architecture styles. Elaboration Another advantage vector multimedia extensions 1006relatively easy extend scalar instruction set architecture wi th instructions improve performance data parallel operations. Elaboration Haswell-generation x86 processors Intel support AVX2, gather operation scatter operation. 6.4 Hardware Multithreading related concept MIMD, especially programmer’s perspective, hardware multithreading . MIMD relies multiple processes threads try keep many processors busy, hardware multithreading allows multiple threads share functional units single processor overlapping fashion try utilize hardware resources efficiently. permit thi sharing, processor must duplicate independent state thread. example, thread would separate copy register file program counter. memory shared virtual memory mechanisms, already support multi-programming. addition, hardware must support ability change different thread relatively quic kly. particular, thread switch much efficient process switch, typically requires hundreds thousands f processor cycles thread switch instantaneous. hardware multithreading Increasing utilization processor switching another thre ad one thread stalled. thread thread includes program counter, register state, stack. lightweight process; whereas threads commonly share single address space, processes don’t. process 1007A process includes one threads, address space, operating system state. Hence, process switch usually invokes operating system, thread switch. two main approaches hardware multithreading. Fine-grained multithreading switches threads instruction, resulting interleaved execution multiple th reads. interleaving often done round-robin fashion, skippin g threads stalled clock cycle. make fine-grained multithreading practical, processor must able switch threads every clock cycle. One advantage fine-grained multithreading hide throughput losses arise short long stalls, since instructions threads executed one thread stalls. primary disadvantage fine-grained multithreading slows execution individual threads, since thread ready execute without stalls delayed instructions threads. fine-grained multithreading version hardware multithreading implies switching threads every instruction. Coarse-grained multithreading invented alternative fine-grained multithreading. Coarse-grained multithreading switches threads expensive stalls, last-level cache misses. change relieves need thread switching extremely fast much less likely slow executio n individual thread, since instructions threads nly issued thread encounters costly stall. Coarse-grained multithreading suffers, however, major drawback: limited ability overcome throughput losses, especial ly shorter stalls. limitation arises pipeline start-up costs coarse-grained multithreading. processor coarse- grained multithreading issues instructions single thread , stall occurs, pipeline must emptied frozen. new thread begins executing stall must fill pipeline instructions able complete. Due st art- 1008up overhead, coarse-grained multithreading much useful reducing penalty high-cost stalls, pipeline fill negligible compared stall time. coarse-grained multithreading version hardware multithreading implies switching threads significant events, last-level cache miss. Simultaneous multithreading (SMT) variation hardware multithreading uses resources multiple-issue, dynamically scheduled pipelined processor exploit thread-level parallelism time exploits instruction-level paralle lism (see Chapter 4 ). key insight motivates SMT multiple-issue processors often functional unit parallelism available single threads effectively use. Furthermore, register renaming dynamic scheduling (see Chapter 4 ), multiple instructions independent threads issued without regard dependences among them; resolution dependences handled dynamic scheduling capability. 1009 simultaneous multithreading (SMT) version multithreading lowers cost multithread ing utilizing resources needed multiple issue, dynamicall scheduled microarchitecture. Since SMT relies existing dynamic mechanisms, switch resources every cycle. Instead, SMT always executing instructions multiple threads, leaving hardware associate instruction slots renamed registers prop er threads. Figure 6.5 conceptually illustrates differences processor’s ability exploit superscalar resources following pro cessor configurations. top portion shows four threads would execute independently superscalar multithreading support. bottom portion shows four threads could combined execute processor efficiently using hree multithreading options: superscalar coarse-grained multithreading superscalar fine-grained multithreading superscalar simultaneous multithreading 10101011FIGURE 6.5 four threads use issue slots superscalar processor different approaches. four threads top show would execute running alone standard superscalar processor without multithreading support. three examples bottom show would execute running together three multithreading options. Th e horizontal dimension represents instruction issue capability clock cycle. vertical dimension represents sequence clock cycles. empty (white) box indicates corresponding issue slot unused clock cycle. shades gray color correspond four different threads multithreading processors. additional pipeline start-up effects coarse multithreading, illustrated figure, would lead loss throughput coarse multithreading. 1012In superscalar without hardware multithreading support, use issue slots limited lack instruction-level parallelism . addition, major stall, instruction cache miss, leave entire processor idle. coarse-grained multithreaded superscalar, long stalls partially hidden switching another thread uses resources processor. Although reduces number completely idle clock cycles, pipeline start-up overhead till leads idle cycles, limitations ILP mean issue slots wil l used. fine-grained case, interleaving threads mostly eliminates idle clock cycles. single thread issues instructions given clock cycle, however, limitations instruction-level parallelism still lead idle slots within clock cycles. SMT case, thread-level parallelism instruction-level parallelism exploited, multiple threads using issue slots single clock cycle. Ideally, issue slot usage limited imbalances resource needs resource availability multiple threads. practice, factors restrict many slots used. Although Figure 6.5 greatly simplifies real operation processors, illu strate potential performance advantages multithreading general 1013and SMT particular. Figure 6.6 plots performance energy benefits multithreading single processor Intel Core i7 960, hardware support two threads. average speed-up 1.31, bad given modest extra resources hardware multithreading. average improvement energy efficiency 1.07, excellent. general, you’d happy performance speed-up energy neutral. 1014FIGURE 6.6 speed-up using multithreading one core i7 processor averages 1.31 PARSEC benchmarks (see Section 6.9 ) energy efficiency improvement 1.07. data collected analyzed Esmaeilzadeh et al. [2011]. seen multiple threads utilize resources single processor effectively, next show use exploit multiple processors. 1015 Check 1. True false: multithreading multicore rely parallelism get efficiency chip. 2. True false: Simultaneous multithreading (SMT) uses threads improve resource utilization dynamically scheduled, out-of- order processor. 6.5 Multicore Shared Memory Multiprocessors hardware multithreading improved efficiency processors modest cost, big challenge last decade deliver performance potential Moore’s Law efficiently programming increasing number processors pe r chip. Given difficulty rewriting old programs run well parallel hardware, natural question is: computer designers simplify task? One answer provide single physical address space processors share, programs need concern data are, 1016merely programs may executed parallel. approach, variables program made available time processor. alternative separate address space per processor requires sharing must explicit; we’ll desc ribe option Section 6.7 . physical address space common hardware typically provides cache coherence give consistent view shared memory (see Section 5.8 ). mentioned above, shared memory multiprocessor (SMP) one offers programmer single physical address space across processors—which nearly always case multicore chips— although accurate term would shared- address multiprocessor. Processors communicate shared variable memory, processors capable accessing memory location via loads stores. Figure 6.7 shows classic organization SMP. Note systems still run independent jobs virtual address spaces, even share physical address space. FIGURE 6.7 Classic organization shared memory multiprocessor. Single address space multiprocessors come two styles. first style, latency word memory depend processor asks it. machines called uniform memory access (UMA) multiprocessors. second style, memory accesses much faster others, depending 1017processor asks word, typically main memory divided attached different microprocessors differe nt memory controllers chip. machines called nonuniform memory access (NUMA) multiprocessors. might expect, programming challenges harder NUMA multiprocessor UMA multiprocessor, NUMA machines scale larger sizes, NUMAs lower latency nearby memory. uniform memory access (UMA) multiprocessor latency word main memory matter processor requests access. nonuniform memory access (NUMA) type single address space multiprocessor memory accesses much faster others depending processor asks word. processors operating parallel normally share data, also need coordinate operating shared data; otherwise, one processor could start working data another finished it. coordination called synchronization , saw Chapter 2 . sharing supported single address space, must separate mechanism synchronization. One approach uses lock shared variable. one processor time acquire lock, processors interested shared data must wait original processor unlocks variable. Section 2.11 Chapter 2 describes instructions locking RISC-V instruction set. synchronization process coordinating behavior two process es, may running different processors. lock synchronization device allows access data one processor time. 1018 Simple Parallel Processing Program Shared Address Space Example Suppose want sum 64,000 numbers shared memory multiprocessor computer uniform memory access time. Let’ assume 64 processors. Answer first step ensure balanced load per processor, spli set numbers subsets size. allocate subsets different memory space, since single memory space machine; give different starting addresses processor. Pn number identifies processor, 0 63. processors start program running loop sums subset numbers: sum[Pn] = 0; (i = 1000*Pn; < 1000*(Pn+1); += 1) sum[Pn] += A[i]; /*sum assigned areas*/ (Note C code += 1 shorter way say = + 1.) next step add 64 partial sums. step called reduction , divide conquer. Half processors add pairs partial sums, quarter add pairs new partial sums, single, final sum. Figure 6.8 illustrates hierarchical nature reduction. reduction function processes data structure returns single value. 1019FIGURE 6.8 last four levels reduction sums results processor, bottom top. processors whose number less half, add sum produced processor number (i +half) sum. example, two processors must synchronize “consumer” processor tries read result memory location written “producer” processor; otherwise, consumer may read old value data. want processor version loop counter variable i, must indicate “private” variable. code (half private also): half = 64; /*64 processors multiprocessor*/ synch(); /*wait partial sum completion*/ (half%2 != 0 && Pn == 0) sum[0] += sum[half–1]; /*Conditional sum needed half odd; Processor0 gets missing element */ half = half/2; /*dividing line sums */ (Pn < half) sum[Pn] += sum[Pn+half]; (half > 1); /*exit final sum Sum[0] * / 1020Hardware/Software Interface Given long-term interest parallel programming, hundreds attempts build parallel programming systems. limited popular example OpenMP . Application Programmer Interface (API) along set compiler directives, environment variables, runtime library routines extend standard programming languages. offers portable, scalable, simple programming model shared memory multiprocessors. primary goal parallelize loops perform reductions. OpenMP API shared memory multiprocessing C, C++, Fortran runs UNIX Microsoft platforms. includes compiler directives, library, runtime directives. C compilers already support OpenMP. command use OpenMP API UNIX C compiler just: cc –fopenmp foo.c OpenMP extends C using pragmas , commands C macro preprocessor like #define #include . set number processors want use 64, wanted example above, use command #define P 64 /* define constant we’ll use times */ #pragma omp parallel num_threads(P) is, runtime libraries use 64 parallel threads. turn sequential loop parallel loop divides work equally threads told use, write (assuming sum initialized 0) #pragma omp parallel (Pn = 0; Pn < P; Pn += 1) (i = 0; 1000*Pn; < 1000*(Pn+1); += 1) sum[Pn] += A[i]; /*sum assigned areas*/ perform reduction, use another command tells OpenMP reduction operator variable need use place result reduction. #pragma omp parallel reduction(+ : FinalSum) (i = 0; < P; += 1) 1021 FinalSum += sum[i]; /* Reduce single number */ Note OpenMP library find efficient code sum 64 numbers efficiently using 64 processors. OpenMP makes easy write simple parallel code, helpful debugging, many programmers use sophisticated parallel programming systems OpenMP, many programmers today use productive languages C. Given tour classic MIMD hardware software, next path exotic tour type MIMD architecture different heritage thus different perspective parallel programming challenge. Check True false: Shared memory multiprocessors cannot take advantage task-level parallelism. Elaboration writers repurposed acronym SMP mean symmetric multiprocessor , indicate latency processor memory processors. shift done contrast large-scale NUMA multiprocessors, classes used single address space. clusters proved much popular large-scale NUMA multiprocessors, book restore SMP original meaning, use contrast use multiple address spaces, clusters. Elaboration alternative sharing physical address space would separate physical address spaces share common virtual address space, leaving operating system handle communication. approach tried, high overhead offer practical shared memory abstraction performance-oriented programmer. 6.6 Introduction Graphics 1022Processing Units original justification adding SIMD instructions exis ting architectures many microprocessors connected graphics displays PCs workstations, increasing fraction processing time used graphics. Moore’s Law increased number transistors available microprocessor s, therefore made sense improve graphics processing. major driving force improving graphics processing computer game industry, PCs dedicated game consoles Sony PlayStation. rapidly growing game market encouraged many companies make increasing investments developing faster graphics hardware, positive feedback loop led graphics processing improve quicker rate general-purpose processing mainstream microprocessors. Given graphics game community different goals microprocessor development community, evolved style processing terminology. graphics processors increased power, earned name Graphics Processing Units GPUs distinguish CPUs. hundred dollars, anyone buy GPU today 1023hundreds parallel floating-point units, makes high- performance computing accessible. interest GPU computing blossomed potential combined programming language made GPUs easier program. Hence, many programmers scientific multimedia applications today pondering whether use GPUs CPUs. (This section concentrates using GPUs computing. see GPU computing combines traditional role graphics acceleration, see Appendix B .) key characteristics GPUs vary CPUs: GPUs accelerators supplement CPU, need able perform tasks CPU. role allows dedicate resources graphics. It’s fine GPUs perform tasks poorly all, given system CPU GPU, CPU needed. GPU problem sizes typically hundreds megabytes gigabytes, hundreds gigabytes terabytes. differences led different styles architecture: Perhaps biggest difference GPUs rely multilevel caches overcome long latency memory, CPUs. Instead, GPUs rely hardware multithreading ( Section 6.4) hide latency memory. is, time memory request time data arrive, GPU executes hundreds thousands threads independent request. GPU memory thus oriented toward bandwidth rather latency. even special graphics DRAM chips GPUs wider higher bandwidth DRAM chips CPUs. addition, GPU memories traditionally smaller main memories conventional microprocessors. 2013, GPUs typically 4 6 GiB less, CPUs 32 256 GiB. Finally, keep mind general-purpose computation, must include time transfer data CPU memory GPU memory, since GPU coprocessor. Given reliance many threads deliver good memory bandwidth, GPUs accommodate many parallel processors (MIMD) well many threads. Hence, GPU processor 1024more highly multithreaded typical CPU, plus processors. Hardware/Software Interface Although GPUs designed narrower set applications, programmers wondered could specify applications form would let tap high potential performance GPUs. tiring trying specify problems using graphics APIs languages, developed C-inspired programming languages allow write programs directly GPUs. example NVIDIA’s CUDA (Compute Unified Device Architecture ), enables programmer write C programs execute GPUs, albeit restrictions. Appendix B gives examples CUDA code. (OpenCL multi-company initiative develop portable programming language provides many benefits CUDA.) NVIDIA decided unifying theme forms parallelism CUDA Thread . Using lowest level parallelism programming primitive, compiler hardware gang thousands CUDA threads together utilize various styles parallelism within GPU: multithreading, MIMD, SIMD, instruction-level parallelism. threads blocked together executed groups 32 time. multithreaded processor inside GPU executes blocks threads, GPU consists 8 32 multithreaded processors. Introduction NVIDIA GPU Architecture use NVIDIA systems example representative GPU architectures. Specifically, follow terminology CUDA parallel programming language use Fermi architecture example. Like vector architectures, GPUs work well data-level parallel problems. styles gather-scatter data transfers, 1025and GPU processors even registers vector processors. Unlike vector architectures, GPUs also rely hardware multithreading within single multithreaded SIMD processor hide memory latency (see Section 6.4 ). multithreaded SIMD processor similar vector processor , former many parallel functional units instead deeply pipelined, latter. mentioned above, GPU contains collection multithreaded SIMD processors; is, GPU MIMD composed multithreaded SIMD processors. example, NVIDIA four implementations Fermi architecture different price points 7, 11, 14, 15 multithreaded SIMD processors. provide transparent scalability across models GPUs differing number multithreaded SIMD processors , Thread Block Scheduler hardware assigns blocks threads multithreaded SIMD processors. Figure 6.9 shows simplified block diagram multithreaded SIMD processor. FIGURE 6.9 Simplified block diagram datapath multithreaded SIMD Processor. 16 SIMD lanes. SIMD Thread Scheduler many independent SIMD threads chooses run processor. Dropping one level detail, machine object hardware creates, manages, schedules, executes thread 1026of SIMD instructions , also call SIMD thread . traditional thread, contains exclusively SIMD instructio ns. SIMD threads program counters, run multithreaded SIMD processor. SIMD Thread Scheduler includes controller lets know threads SIMD instructions ready run, sends dispatch unit run multithreaded SIMD processor. identical hardware thread scheduler traditional multithreaded processor (see Section 6.4 ), except scheduling threads SIMD instructions. Thus, GPU hardware two levels hardware schedulers: 1. Thread Block Scheduler assigns blocks threads multithreaded SIMD processors, 2. SIMD Thread Scheduler within SIMD processor, schedules SIMD threads run. SIMD instructions threads 32 wide, thread SIMD instructions would compute 32 elements computation. Since thread consists SIMD instruction s, SIMD processor must parallel functional units perform operation. call SIMD Lanes , quite similar Vector Lanes Section 6.3 . Elaboration number lanes per SIMD processor varies across GPU generations. Fermi, 32-wide thread SIMD instructions mapped 16 SIMD lanes, SIMD instruction thread SIMD instructions takes two clock cycles complete. thread SIMD instructions executed lock step. Staying wi th analogy SIMD processor vector processor, could say 16 lanes, vector length would 32. wide shallow nature use term SIMD processor instead vector processor, intuitive. Since definition threads SIMD instructions independent, SIMD Thread Scheduler pick whatever thread SIMD instructions ready, need stick next SIMD instruction sequence within single thread. Th us, using terminology Section 6.4 , uses fine-grained multithreading. 1027To hold memory elements, Fermi SIMD processor impressive 32,768 32-bit registers. like vector processor, registers divided logically across vector lanes or, case, SIMD lanes. SIMD thread limited 64 registers, might think SIMD thread 64 vector registers, vector register 32 elements element 32 bits wide. Since Fermi 16 SIMD lanes, contains 2048 registers. CUDA thread gets one element vector registers . Note CUDA thread vertical cut thread SIMD instructions, corresponding one element executed one SI MD lane. Beware CUDA threads different POSIX threads; can’t make arbitrary system calls synchronize arbitrarily CUDA thread. NVIDIA GPU Memory Structures Figure 6.10 shows memory structures NVIDIA GPU. call on-chip memory local multithreaded SIMD processor Local Memory . shared SIMD lanes within multithreaded SIMD processor, memory shared multithreaded SIMD processors. call off-chip DRAM shared whole GPU thread blocks GPU Memory . 1028FIGURE 6.10 GPU Memory structures. GPU Memory shared vectorized loops. threads SIMD instructions within thread block share Local Memory. Rather rely large caches contain entire working sets application, GPUs traditionally use smaller streaming caches rely extensive multithreading threads SIMD instructions hide long latency DRAM, since wor king sets hundreds megabytes. Thus, fit last-level cache multicore microprocessor. Given use hardware multithreading hide DRAM latency, chip area used caches system processors spent instead computing resources large number registers hold state f many threads SIMD instructions. Elaboration 1029While hiding memory latency underlying philosophy, note latest GPUs vector processors added caches. example, recent Fermi architecture added caches, thought either bandwidth filters reduce demands GPU Memory accelerators variables whose latency cannot hidden multithreading. Local memory stack frames, function calls, register spilling good match caches, since latency matters calling function. Caches also save energy, since on-chip cache accesses take much less energy accesses multiple, external DRAM chips. Putting GPUs Perspective high level, multicore computers SIMD instruction extensions share similarities GPUs. Figure 6.11 summarizes similarities differences. MIMDs whose processo rs use multiple SIMD lanes, although GPUs processors many lanes. use hardware multithreading improve processor utilization, although GPUs hardware support many threads. use caches, although GPUs use smaller streaming caches multicore computers use large multilevel caches try contain whole working sets completely. use 64-bit address space, although physical main memory much smaller GPUs. GPUs support memory protection page level, yet support demand paging. FIGURE 6.11 Similarities differences multicore Multimedia SIMD extensions recent GPUs. 1030SIMD processors also similar vector processors. multiple SIMD processors GPUs act independent MIMD cores, many vector computers multiple vector processors. view would consider Fermi GTX 580 16-core machine hardware support multithreading, core 16 lanes. biggest difference multithreading, fundamental GPUs missing vector processors. GPUs CPUs go back computer architecture genealogy shared ancestor; Missing Link explains both. result uncommon heritage, GPUs used terms common computer architecture community, led confusion GPUs work. help resolve confusion, Figure 6.12 (from left right) lists descriptive term used sectio n, closest term mainstream computing, official NVIDIA GPU term case interested, short description term. “GPU Rosetta Stone” may help relate section ideas conventional GPU descriptions, found Appendix B . 1031FIGURE 6.12 Quick guide GPU terms. use first column hardware terms. Four groups cluster 12 terms. top bottom: Program Abstractions, Machine Objects, Processing Hardware, Memory Hardware. GPUs moving toward mainstream computing, can’t abandon responsibility continue excel graphi cs. Thus, design GPUs may make sense architects ask, given hardware invested graphics well, supplement improve performance wider range applications? covered two different styles MIMD shared address space, next introduce parallel processors processor private address space, makes considerably easier build much larger systems. Internet 1032services use every day depend large-scale systems. Elaboration GPU introduced separate memory CPU, AMD Intel announced “fused” products combine GPUs CPUs share single memory. challenge maintain high bandwidth memory fused architecture foundation GPUs. Check True false: GPUs rely graphics DRAM chips reduce memory latency thereby increase performance graphics applications. 6.7 Clusters, Warehouse Scale Computers, Message- Passing Multiprocessors alternative approach sharing address space processors private physical address space. Figure 6.13 shows classic organization multiprocessor multiple private address spaces. alternative multiprocessor must communicate via explicit message passing , traditionally name style computers. Provided system routines send receive messages , coordination built message passing, since one processor knows message sent, receiving processor knows message arrives. sender needs confirmation message arrived, receiving processor send acknowledgment message back sender. message passing Communicating multiple processors explicitly se nding receiving information. 1033 send message routine routine used processor machines private memories pass message another processor. receive message routine routine used processor machines private memories accept message another processor. 1034FIGURE 6.13 Classic organization multiprocessor multiple private address spaces, traditionally called message-passing multiprocessor. Note unlike SMP Figure 6.7 , interconnection network caches memory instead processor-memory nodes. several attempts build large-scale computers based high-performance message-passing networks, offer better absolute communication performance clusters built using local area networks. Indeed, many supercomputers today use custom networks. problem much expensive local area networks like Ethernet. applications today outside high-performance computing justify hi gher communication performance, given much higher costs. Hardware/Software Interface Computers rely message passing communication rather cache coherent shared memory much easier hardware designers build (see Section 5.8 ). advantage programmers well, communication explicit, means fewer performance surprises implic communication cache-coherent shared memory computers. downside programmers it’s harder port sequential 1035program message-passing computer, since every communication must identified advance program doesn’t work. Cache-coherent shared memory allows hardware figure data need communicated, makes porting easier. differences opinion th e shortest path high performance, given pros cons implicit communication, confusion marketplace today. Multicore microprocessors use shared physical memory nodes cluster communicate using message passing. concurrent applications run well parallel hardware, independent whether offers shared addresses message passing. particular, task-level parallelism applications little communication—like Web search, mail servers, file servers—do require shared addressing run well. result, clusters become widespread example today message-passing parallel computer. Given separate memories, node cluster runs distinct copy operating system . contrast, cores inside microprocessor connected usin g high-speed network inside chip, multichip shared- memory system uses memory interconnect communication. memory interconnect higher bandwidth lower latency, allowing much better communication performance shared memory multiprocessors. clusters Collections computers connected via I/O standard netwo rk switches form message-passing multiprocessor. weakness separate memories user memory parallel programming perspective turns strength syste dependability (see Section 5.5 ). Since cluster consists independent computers connected local area network, much easier replace computer without bringing system cluster shared memory multiprocessor. Fundamentally, shared address means difficult isolate processor replace without heroic work 1036operating system physical design server. als easy clusters scale gracefully server fails, thereby improving dependability . Since cluster software layer runs top local operating systems running computer, much easier disconnect replace broken computer. Given clusters constructed whole computers independent, scalable networks, isolation also makes easier expand system without bringing application run top cluster. lower cost, higher availability, rapid, incremental expandability make clusters attractive service Internet prov iders, despite poorer communication performance compared large-scale shared-memory multiprocessors. search engines hundreds millions us use every day depend upon technology. Amazon, Facebook, Google, Microsoft, others multiple datacenters clusters tens thousands f servers. Clearly, use multiple processors Internet ser vice companies hugely successful. Warehouse-Scale Computers Internet services, described above, necessitated construction new buildings house, power, cool 100,000 servers. Although may classified large clusters, th eir architecture operation sophisticated. act one giant computer cost order $150M building, electrical cooling infrastructure, servers, networking equipment connects houses 50,000 100,000 1037servers. consider new class computer, called Warehouse-Scale Computers (WSC). Anyone build fast CPU. trick build fast system. Seymour Cray, considered father supercomputer. Hardware/Software Interface popular framework batch processing WSC MapReduce [Dean, 2008] open-source twin Hadoop. Inspired Lisp functions name, Map first applies programmer-supplied function logical input record. Map runs thousands servers produce intermediate result key-value pairs. Reduce collects output distribute tasks collapses using another programmer-defined function. appropriate software support, highly parallel yet easy understand use. Within 30 minutes, novice programmer run MapReduce task thousands servers. example, one MapReduce program calculates number occurrences every English word large collection documents. simplified version program, shows inner loop assumes one occurrence English words found document: map(String key, String value): // key: document name // value: document contents word w value: EmitIntermediate(w, “1”); // Produce list wo rds reduce(String key, Iterator values): // key: word // values: list counts int result = 0; v values: result += ParseInt(v); // get integer key-valu e pair Emit(AsString(result)); function EmitIntermediate used Map function emits word document value one. Reduce function sums values per word document using 1038ParseInt() get number occurrences per word documents. MapReduce runtime environment schedules map tasks reduce tasks servers WSC. extreme scale, requires innovation power distribution, cooling, monitoring, operations, WSC modern descendant 1970s supercomputers—making Seymour Cray godfather today’s WSC architects. extreme computers handled computations could done nowhere else , expensive companies could afford them. time target providing information technology fo r world instead high-performance computing scientist engineers. Hence, WSCs surely play important societal role today Cray’s supercomputers past. share common goals servers, WSCs three major distinctions: 1. Ample, easy parallelism : concern server architect whether applications targeted marketplace enough parallelism justify amount parallel hardware whether cost high sufficient communication hardware exploit parallelism. WSC architect concern. First, batch applications like MapReduce benefit large number independent data sets need independent processing, billions Web pages Web crawl. Second, interactive Internet service applications, also known Software Service (SaaS) , benefit millions independent users interactive Internet services. Reads writes rarely depend ent SaaS, SaaS rarely needs synchronize. example, search uses read-only index email normally reading writing independent information. call type easy parallelism Request-Level Parallelism , many independent efforts proceed parallel naturally little need communication synchronization. software service (SaaS) Rather selling software installed run customers’ computers, software run remote site made available Internet typically via Web interface customers. SaaS customers charged based use versus ownership. 10392. Operational Costs Count : Traditionally, server architects design systems peak performance within cost budget worry energy make sure don’t exceed cooling capacity enclosure. usually ignored operational cost server, assuming pale comparison purchase costs. WSCs longer lifetimes—the building electrical cooling infrastructure often amortized 10 years— operational costs add up: energy, power distribution, cooling represent 30% costs WSC 10 years. 3. Scale Opportunities/Problems Associated Scale : construct single WSC, must purchase 100,000 servers along supporting infrastructure, means volume discoun ts. Hence, WSCs massive internally get economy scale even WSCs. economies scale led cloud computing , lower per unit costs WSC meant cloud companies could rent servers profitable rate still costs outsiders themselves. flip side economic opportunity scale need cope failure frequency scale. Even server Mean Time Failure amazing 25 years (200,000 hours), WSC architect would need design five server failures every day. Section 5.15 mentioned annualized disk failure rate (AFR) measured 1040Google 2% 4%. four disks per server annual failure rate 2%, WSC architect expect see one disk fail every hour . Thus, fault tolerance even important WSC architect server architect. economies scale uncovered WSC realized long dreamed goal computing utility. Cloud computing means anyone anywhere good ideas, business model, credit card tap thousands servers deliver vision almost instantly around world. course, important obstacles could limit growth cloud computing—such security , privacy, standards, rate growth Internet bandwidth— foresee addressed WSCs cloud computing flourish. put growth rate cloud computing perspective, 2012 Amazon Web Services announced adds enough new server capacity every day support Amazon’s global infrastructure 2003, Amazon $5.2Bn annual revenue enterprise 6000 employees. understand importance message-passing multiprocessors, especially cloud computing, next cove r ways connect nodes WSC together. Thanks Moore’s Law increasing number cores per chip, need networks inside chip well, topologies important n small well large. 1041 Elaboration MapReduce framework shuffles sorts key-value pairs end Map phase produce groups share key. groups next passed Reduce phase. Elaboration Another form large-scale computing grid computing , computers spread across large areas, programs run across must communicate via long haul networks. popular unique form grid computing pioneered SETI@home project. millions PCs idle one time nothing useful, could harvested put good use someone developed software could run computers gave PC independent piece problem work on. first example Search ExtraTerrestrial Intelligence (SETI), launched UC Berkeley 1999. 5 million computer users 200 countries signed SETI@home, 50% outside US. end 2011, average performance SETI@home grid 3.5 PetaFLOPS. 1042Check 1. True false: Like SMPs, message-passing computers rely locks synchronization. 2. True false: Clusters separate memories thus need many copies operating system. 6.8 Introduction Multiprocessor Network Topologies Multicore chips require on-chip networks connect cores tog ether, clusters require local area networks connect servers toge ther. section reviews pros cons different interconne ction network topologies. Network costs include number switches, number links switch connect network, width (number bits) per link, length links network mapped silicon. example, cores servers may adjacent others may side chip side datacenter. Network performance multifaceted well. includes latency unloaded network send receive message, throughput terms maximum number messages transmitted given time period, delays caused contention portion network, variable performance depending pattern communication. Another obligation network may fault tolerance, since systems may required operate presence broken components. Finally, era energy-limited systems, energy effici ency different organizations may trump concerns. Networks normally drawn graphs, edge graph representing link communication network. figures section, processor-memory node shown black square switch shown colored circle. assume links bidirectional ; is, information flow either direction. networks consist switches whose links go processor-memory nodes switches. first network connects sequence nodes together: 1043This topology called ring. Since nodes directly connected, messages hop along intermediate nodes arrive final destination. Unlike bus—a shared set wires allows broadcasting connected devices—a ring capable many simultaneous transfers. numerous topologies choose from, performance metrics needed distinguish designs. wo popular. first total network bandwidth , bandwidth link multiplied number links. represents peak bandwidth. ring network above, P processors, total network bandwidth would P times bandwidth one link; total network bandwidth bus bandwidth bus. network bandwidth Informally, peak transfer rate network; refer speed single link collective transfer rate links n network. balance best bandwidth case, include another metric closer worst case: bisection bandwidth . metric calculated dividing machine two halves. sum bandwidth links cross imaginary dividing line. bisection bandwidth ring two times link bandwidth. one times link bandwidth bus. single link fast bus, ring twice fast bus worst case, P times faster best case. bisection bandwidth bandwidth two equal parts multiprocessor. measure worst-case split multiprocessor. Since network topologies symmetric, question 1044arises draw imaginary line bisecting machine. Bisection bandwidth worst-case metric, answer choose division yields pessimistic net work performance. Stated alternatively, calculate possible bisectio n bandwidths pick smallest. take pessimistic view parallel programs often limited weakest link communication chain. extreme ring fully connected network , every processor bidirectional link every processor. fully connected networks, total network bandwidth P×(P−1)/2, bisection bandwidth ( P/2)2. fully connected network network connects processor-memory nodes supplying dedicated communication link every node. tremendous improvement performance fully connecte networks offset tremendous increase cost. consequence inspires engineers invent new topologies th cost rings performance fully connecte networks. evaluation success depends large part nature communication workload parallel programs run computer. number different topologies discussed publications would difficult count, used commercial parallel processors. Figure 6.14 illustrates two popular topologies. 1045FIGURE 6.14 Network topologies appeared commercial parallel processors. colored circles represent switches black squares represent processor-memory nodes. Even though switch many links, generally one goes processor. Boolean n-cube topology n-dimensional interconnect 2n nodes, requiring n links per switch (plus one processor) thus n nearest-neighbor nodes. Frequently, basic topologies supplemented extra arcs improve performance reliability. alternative placing processor every node network leave switch nodes. switches smaller processor-memory-switch nodes, thus may packed densely, thereby lessening distance increasing performance. networks frequently called multistage networks reflect multiple steps message may travel. Types multistage networks numerous single-stage networks; Figure 6.15 illustrates two popular multistage organizations. fully connected crossbar network allows node communicate node one pass network. Omega network uses less hardware crossbar network (2 n log2 n versus n2 switches), contention occur messages, depending pattern communication. example, Omega network Figure 6.15 cannot send message P0 P6 time sends message P1 P4. 1046 multistage network network supplies small switch node. crossbar network network allows node communicate node one pass network. 1047FIGURE 6.15 Popular multistage network topologies eight nodes. switches drawings simpler earlier drawings links unidirectional; data come left exit right link. switch box c pass C B B C D. crossbar uses n2 switches, n number processors, Omega network uses 2 n log2n large switch boxes, logically composed four smaller switches. case, crossbar uses 64 switches versus 12 switch boxes, 48 switches, Omega network. crossbar, however, support combination messages processors, Omega network cannot. Implementing Network Topologies 1048This simple analysis networks section ignores important practical considerations construction networ k. distance link affects cost communicating high clock rate—generally, longer distance, expensive run high clock rate. Shorter distances also make easier assign wires link, power drive many wires less wires short. Shorter wires also cheaper longe r wires. Another practical limitation three-dimensional drawings must mapped onto chips essentially two- dimensional media. final concern energy. Energy concerns may force multicore chips rely simple grid topologies, fo r example. bottom line topologies appear elegant sketched blackboard may impractical constructed silicon datacenter. understand importance clusters seen topologies follow connect together, next look hardware software interface network processor. Check True false: ring P nodes, ratio total network bandwidth bisection bandwidth P/2. Communicating Outside World: Cluster Networking online section describes networking hardware software used connect nodes cluster together. example 10 gigabit/second Ethernet connected computer using Peripheral Component Interconnect Express (PCIe). shows 1049software hardware optimizations improve network performance, including zero copy messaging, user space communication, using polling instead I/O interrupts, hardware calculation checksums. example networking, techniques section apply storage controllers I/O devices well. covering performance network low level detail online section, next section shows benchmark multiprocessors kinds much higher-level programs . 6.9 Communicating Outside World: Cluster Networking online section describes networking hardware software used connect nodes cluster together. ther e whole books courses networking, section introduces main terms concepts. example networking, techniques describe apply storage contro llers I/O devices well. Ethernet dominated local-area networks decades, surprising clusters primarily rely Ethernet cl uster interconnect. became commercially popular 10 Megabits per second link speed 1980s, today 1 Gigabit per second Ethernet standard 10 Gigabit per second deployed datacenters. Figure e6.9.1 shows network interface card (NIC) 10 Gigabit Ethernet. 1050FIGURE E6.9.1 NetFPGA 10-Gigabit Ethernet card (see http://netfpga.org/ ), connects four 10-Gigabit/sec Ethernet links. FPGA- based open platform network research classroom experimentation. DMA engine four “MAC chips” Figure e6.9.2 portions Xilinx Virtex FPGA middle board. four PHY chips Figure e6.9.2 four black squares right four white rectangles left edge board, Ethernet cables plugged in. Computers offer high-speed links plug fast I/O devices li ke NIC. used separate chips connect microprocessor memory high-speed I/O devices, thank Moore’s Law functions absorbed main chip recent offerings like Intel’s Sandy Bridge. popular hig h- speed link today PCIe , stands Peripheral Component Interconnect Express . called link basic building block, called serial lane , consists four wires: two receiving data two transmitting data. small number contrasts earlier version PCI consisted 64 wires , called parallel bus . PCIe allows anywhere one 32 lanes used connect I/O devices, depending needs. NIC uses PCI 1.1, lane transfers 2 Gigabits/second. 1051The NIC Figure e6.9.1 connects host computer eight-lane PCIe link, offers 16 Gigabits/second directions. communicate, NIC must send transmit messages receive them, often abbreviated TX RX, respectively. NIC, 10 G link uses separate transmit receive queues, store two full-length Ethernet packets, used Ethernet links NIC. Figure e6.9.2 block diagram NIC showing TX RX queues. NIC also two 32-entry queues transmitting receiving host computer NIC. FIGURE E6.9.2 Block diagram NetFPGA Ethernet card Figure e6.9.1 showing control 1052paths data paths. control path allows DMA engine read status queues, empty vs. on-empty, content next available queue entry. DMA engine also controls port multiplexing. data path simply passes DMA block TX/RX queues main memory. “MAC chips” described below. PHY chips, refer physical layer, connect “MAC chips” physical networking medium, copper wire optical fiber. give command NIC, processor must able address device supply one command words. memory-mapped I/O , portions address space assigned I/O devices. initialization (at boot time), PCIe devices request assigned address region specified length. subsequent processor reads writes address region forwarded PCIe device. Reads writes addresses interpreted commands I/O device. memory-mapped I/O I/O scheme portions address space assigned I/O devices, reads writes addresses interpreted commands I/O device. example, write operation used send data network interface data interpreted command. processor issues address data, memory system ignores operation address indicates portion memory space used I/O. NIC, however, sees operation records data. User programs prevented issuing I/O operations directly, OS provide access address space assigned I/O devices, thus addresses protected address translation. Memory-mapped I/O also used transmit data writing reading select addresses. device uses address determine type command, data may provided write obtained read. event, address encodes device identity type transmission 1053processor device. processor could transfer data user space I/O space itself, overhead transferring data fro high-speed network could intolerable, since could consume large fraction processor. Thus, computer designer long ago invented mechanism offloading processor device controller transfer data directly th e memory without involving processor. mechanism call ed direct memory access (DMA). direct memory access (DMA) mechanism provides device controller ability transfer data directly memory without involving processor. DMA implemented specialized controller transfers data network interface memory independent processor, case DMA engine inside NIC. notify operating system (and eventually application receive packet) transfer complete, DMA sends I/O interrupt . interrupt-driven I/O I/O scheme employs interrupts indicate proces sor I/O device needs attention. I/O interrupt like exceptions saw Chapters 4 5, two important distinctions: 1. I/O interrupt asynchronous respect instructio n execution. is, interrupt associated instruction prevent instruction completion, different either page fault exceptions exception arithmetic overflow. control unit needs check pending I/O interrupt time starts new instruction. 2. addition fact I/O interrupt occurred, would like convey information, identity f device generating interrupt. Furthermore, interrupts represent devices may different priorities whose 1054interrupt requests different urgencies associated m. communicate information processor, identity device raising interrupt, system use eit vectored interrupts exception identification register , called supervisor exception cause (SCAUSE) register RISC-V (see Section 4.9). processor recognizes interrupt, device send either vector address status field place Cause register. result, OS gets control, knows identit device caused interrupt immediately interrogate device. interrupt mechanism eliminates nee processor keep checking device instead allows th e processor focus executing programs. Role Operating System Networking operating system acts interface hardware program requests I/O. network responsibilities operating system arise three characteristics network s: 1. Multiple programs using processor share network. 2. Networks often use interrupts communicate information operations. interrupts cause transfer kernel supervisor mode, must handled operating system (OS). 3. low-level control network complex, requir es managing set concurrent events requirements correct device control often detailed. Hardware/Software Interface three characteristics networks specifically I/O sys tems general lead several different functions OS must provide : OS guarantees user’s program accesses portions I/O device user rights. example, OS must allow program read write file disk owner file granted access program. system shared I/O devices, protection could provided user programs could perform I/O directly. OS provides abstractions accessing devices supplyin g 1055routines handle low-level device operations. OS handles interrupts generated I/O devices, handles exceptions generated program. OS tries provide equitable access shared I/O resources, well schedule accesses enhance system throughput. software inside operating system interfaces specific I/O device like NIC called device driver . driver NIC follows five steps transmitting receiving message. Figure e6.9.3 shows relationship steps Ethernet packet sent one node cluster received another node cluster. device driver program controls I/O device attached computer. 1056FIGURE E6.9.3 Relationship five steps driver transmitting Ethernet packet one node receiving packet another node. First, transmit steps: 1. driver first prepares packet buffer host memory. copie 1057a packet user address space buffer allocates operating system address space. 2. Next, “talks” NIC. driver writes I/O descriptor appropriate NIC register gives address buffer length. 3. DMA NIC next copies outgoing Ethernet packet host buffer PCIe. 4. transmission complete, DMA interrupts processor notify processor packet successfully transmitted. 5. Finally, driver de-allocates transmit buffer. Next, receive steps: 1. First, driver prepares packet buffer host memory, allocating new buffer place received packet. 2. Next, “talks” NIC. driver writes I/O descriptor appropriate NIC register gives address buffer length. 3. DMA NIC next copies incoming Ethernet packet PCIe allocated host buffer. 4. transmission complete, DMA interrupts processor notify host newly received packet size. 5. Finally, driver copies received packet user address space. see Figure e6.9.3 , first three steps time- critical transmitting packet (since last two occur packet sent), last three steps time-critical receiving packet (since first two occur packet arriv es). However, non-critical steps must completed individual nodes run resources, memory space. Failure negatively affects network performance. Improving Network Performance importance networking clusters means certainly worthwhile try improve performance. show software hardware techniques. Starting software optimizations, one performance target reducing number times packet copied, may 1058have noticed happening repeatedly five steps driver above. zero-copy optimization allows DMA engine get message directly user program data space transmission placed user wants message received, rather go intermediary buffers operating system along way. second software optimization cut operating system almost entirely moving communication user addre ss space. invoking operating system causing context switch, reduce software overhead considerably. radical scenario, third step would drop interrupts. One reason modern processors normally go int lower power mode waiting interrupt, takes time come low power service interrupt well disruption pipeline, increases latency. alternativ e interrupts processor periodically check status bits see I/O operation complete, called polling . Hence, require user program poll NIC continuously see DMA unit delivered message, side effect processor go low-power mode. polling process periodically checking status I/O device determine need service device. Looking hardware optimizations, one potential target improvement calculating values fields Ethernet packet. 48-bit Ethernet address, called Media Access Control address MAC address , unique number assigned Ethernet NIC. improve performance, “MAC chip”—actually portion FPGA NIC—calculates value preamble fields CRC field (see Section 5.5 ). driver left placing MAC destination address, MAC source address, message type, data payload, padding needed. (Ethernet requires minimum packet, including header CRC fields preamble, 64 bytes.) Note even least expensive Ethernet NICs CRC calculation hardware today. second hardware optimization, available recent 1059Intel processors Ivy Bridge, improves performance f NIC respect memory hierarchy. Direct Data IO (DDIO) allowing 10% last-level cache used fast scratchpad DMA engine. Data copied directly last-level cache rather DRAM DMA, written DRAM upon eviction cache. optimization helps latency, also bandwidth; memory regions used control might written NIC repeatedly, writes longer need go DRAM. Thus, DDIO offers benefits similar write back cache versus write cache (Chapter 5 ). Let’s look object store follows client-server architecture uses optimizations above: zero copy messaging, user space communication, polling instead interrupts, hardware calculation preamble CRC. driver operates user address space library application invokes. grants application exclusive direc access NIC. I/O register space NIC mapped application, driver state kept application. OS kernel doesn’t even see NIC such, avoids overheads context switching, standard kernel network software stack, interrupts. Figure e6.9.4 shows time send object one node another. varies 9.5 12.5 microseconds, depending size object. time step microseconds: 0.7 – client “driver” (library) make request (Driver TX Figure e6.9.4 ). 6.4 8.7 – NIC hardware transmit client’s request PCIe bus Ethernet, depending size object (NIC TX) . 0.02 – send object 10 G Ethernet (Time Flight). time flight limited speed light 5 ns per meter. three-meter cables used measurement mean time flight 15 ns, small clearly visible figure . 1.8 2.5 – NIC hardware receive object, depending size (NIC RX) . 0.6 – server “driver” transmit message requested object app (Driver RX) . 10601061FIGURE E6.9.4 Time send object broken transmit driver NIC hardware time vs. receive driver NIC hardware time. NIC transmit time much larger NIC receive time transmit requires PCIe round-trips. NIC PCIe reads read descriptor data, receive NIC PCIe writes data, length data, interrupt. PCIe reads incur rou nd trip latency NIC waits reply, PCIe writes require response PCIe reliable, PCIe writes sent back-to-back. seen measure performance network low level detail, let’s raise perspective see benchmark multiprocessors kinds much higher - level programs. Elaboration three versions PCIe. NIC uses PCIe 1.1, transfers 2 gigabits per second per lane, NIC transfers 16 gigabits per second direction. PCIe 2.0, found PC motherboards today, doubles lane 1062bandwidth 4 gigabits per second. PCIe 3.0 doubles 8 gigabits per second, starting found motherboards. applaud standard committee’s logical rate bandwidth improvement, 2version number gigabits/second. limitations Virtex 5 FPGA prevented NIC using faster versions PCIe. Elaboration Ethernet foundation cluster communication, clust ers commonly use higher-level protocols reliable communicati on. Transmission Control Protocol Internet Protocol (TCP/IP), although invented planet-wide communication, often used inside warehouse-scale computer, due part dependability. IP makes delivery guarantees protocol, TCP does. sender keeps packet sent get acknowledgment message back received correctly receiver. receiver knows message corrupted along way, double-checking contents th e TCP CRC field. ensure IP delivers right destination , IP header includes checksum make sure destination number remains unchanged. success Internet due large part elegance popularity TCP/IP, allows independent local-area networks communicate dependably. Given importance Internet clusters, many accelerated TCP/IP, using techniques like listed ection [Regnier, 2004]. Elaboration Adding DMA another path memory system—one go address translation mechanism cache hierarchy. difference generates problems n virtual memory caches. problems usually solved combination hardware techniques software support. difficulties DMA virtual memory system arise pages physical virtual address. DMA also creates problems systems caches, two copies data item: one cache one memory. DMA issues memory requests directly memory 1063rather processor cache, value memory location seen DMA unit processor may differ. Consider read NIC DMA unit places directly memory. locations DMA writes cache, processor receive old value read. Similarly, cache write-back, DMA may read value directly memory newer value cache, value written back. called stale data problem coherence problem (see Chapter 5 ). Similar solutions coherence used DMA. Elaboration Virtual Machine support clearly negatively impact networking performance. result, microprocessor designers adding hardware reduce performance overhead virtual machines networking particular I/O general. Intel offers Virtualization Technology Directed I/O (VT-d ) help virtualize I/O. I/O memory management unit enables guest virtual machines directly use I/O devices, Ether net. supports DMA remapping , allows DMA read write data directly I/O buffers guest virtual machine, rather host I/O buffers copy guest I/O buffers. also supports interrupt remapping , lets virtual machine monitor route interrupt requests directly proper virtual machine. Check Two options networking using interrupts polling, using DMA using processor via load store instructions. 1. want lowest latency small packets, combination likely best? 2. want lowest latency large packets, combination likely best? 6.10 Multiprocessor Benchmarks Performance Models 1064As saw Chapter 1 , benchmarking systems always sensitive topic, highly visible way try deter mine system better. results affect sales commercial systems, also reputation designers systems. Hence, participants want win competition, also want sure someone else wins, deserve genuinely better system. desi leads rules ensure benchmark results simply engineering tricks benchmark, instead advances improve performance real applications. avoid possible tricks, typical rule can’t change benchmark. source code data sets fixed, single proper answer. deviation rules makes results invalid. Many multiprocessor benchmarks follow traditions. common exception able increase size problem run benchmark systems widely different number processors. is, many benchmarks allow weak scaling rather require strong scaling, even though must take care comparing results programs running different problem sizes. Figure 6.16 gives summary several parallel benchmarks, also described below: Linpack collection linear algebra routines, routines performing Gaussian elimination constitute known Linpack benchmark. DGEMM routine example page 209 represents small fraction source code Linpack benchmark, accounts execution time benchmark. allows weak scaling, letting user pick size problem. Moreover, allows user rewrite Linpack almost form language, long computes proper result performs number floating point operations given problem size. Twice year, 500 computers fastest Linpack performance published www.top500.org . first list considered press world’s fastest computer. SPECrate throughput metric based SPEC CPU benchmarks, SPEC CPU 2006 (see Chapter 1 ). Rather report performance individual programs, SPECrate runs 1065many copies program simultaneously. Thus, measures task-level parallelism, communication tasks. run many copies programs want, form weak scaling. SPLASH SPLASH 2 (Stanford Parallel Applications Shared Memory) efforts researchers Stanford University 1990s put together parallel benchmark suite similar goals SPEC CPU benchmark suite. includes kernels applications, including many high-performance computing community. benchmark requires strong scaling, although comes two data sets. NAS (NASA Advanced Supercomputing) parallel benchmarks another attempt 1990s benchmark multiprocessors. Taken computational fluid dynamics, consist five kernels. allow weak scaling defining data sets. Like Linpack, benchmarks rewritten, rules require programming language C Fortran. recent PARSEC (Princeton Application Repository Shared Memory Computers) benchmark suite consists multithreaded programs use Pthreads (POSIX threads) OpenMP ( Open MultiProcessing ; see Section 6.5 ). focus emerging computational domains consist nine applications three kernels. Eight rely data parallelism, three rely pipelined parallelism, one unstructured parallelism. cloud front, goal Yahoo! Cloud Serving Benchmark (YCSB) compare performance cloud data services. offers framework makes easy client benchmark new data services, using Cassandra HBase representative examples [Cooper, 2010]. Pthreads UNIX API creating manipulating threads. structured library. 1066FIGURE 6.16 Examples parallel benchmarks. downside traditional restrictions benchmarks innovation chiefly limited architecture compile r. Better data structures, algorithms, programming languages, often cannot used, since would give misleading result. system could win of, say, algorithm, hardware compiler. guidelines understandable foundations computing relatively stable—as 1990s first half decade—they undesirable programming revolution. revolution succeed, nee encourage innovation levels. 1067Researchers University California Berkeley advocated one approach. identified 13 design patterns claim part applications future. Frameworks kernels implement design patterns. Examples sparse matrices, structured grids, finite-state machines, map reduce, graph traversal. keeping definitions high level, hope encourage innovations level system. Thus, system fastest sparse matrix solver welcome use data structure, algorithm, programming language, addition novel architectures compilers. Performance Models topic related benchmarks performance models. seen increasing architectural diversity chapte r— multithreading, SIMD, GPUs—it would especially helpful simple model offered insights performance different architectures. need perfect, insightfu l. 3Cs cache performance Chapter 5 example performance model. perfect performance model, since ignores potentially important factors like block size, block allocation policy, block replacement policy. Moreover, quirks. example, miss ascribed due capacity one design, conflict miss another cache size. Yet 3Cs model popular 25 years, offers insight behavior programs, helping architects programmers improve creations based insights model. find model parallel computers, let’s start small kernels, like 13 Berkeley design patterns Figure 6.16. versions different data types kernels, floating point popular several implementations. Hence, peak floating-point performance limit speed kernels given computer. multicore chips, peak floating-point performance collective peak performance f cores chip. multiple microprocessors n system, would multiply peak per chip total number chips. demands memory system estimated 1068dividing peak floating-point performance average number floating-point operations per byte accessed: ratio floating-point operations per byte memory accessed called arithmetic intensity . calculated taking total number floating-point operations program divided total number data bytes transferred main memory program execution. Figure 6.17 shows arithmetic intensity several Berkeley design pattern Figure 6.16 . arithmetic intensity ratio floating-point operations program number data bytes accessed program main memory. FIGURE 6.17 Arithmetic intensity, specified number floating-point operations run program divided number bytes accessed main memory [Williams, Waterman, Patterson, 2009]. kernels arithmetic intensity scales problem size, Dense Matrix, many kernels arithmetic intensities independent problem size. kernels former case, weak scaling lead different results, since puts much 1069less demand memory system. Roofline Model simple model ties floating-point performance, arithmetic intensity, memory performance together two-dimensional graph [Williams, Waterman, Patterson, 2009]. Peak floating- point performance found using hardware specifications mentioned above. working sets kernels consider fit on-chip caches, peak memory performance may defined memory system behind caches. One way find peak memory performance Stream benchmark. (See Elaboration page 373 Chapter 5 .) Figure 6.18 shows model, done computer, kernel. vertical Y-axis achievable floating-poi nt performance 0.5 64.0 GFLOPs/second. horizontal X-axis arithmetic intensity, varying 1/8 FLOPs/DRAM byte accessed 16 FLOPs/DRAM byte accessed. Note graph log-log scale. 1070FIGURE 6.18 Roofline Model [Williams, Waterman, Patterson, 2009]. example peak floating-point performance 16 GFLOPS/sec peak memory bandwidth 16 GB/sec Stream benchmark. (Since Stream actually four measurements, line average four.) dotted vertical line color left represents Kernel 1, arithmetic intensity 0.5 FLOPs/byte. limited memory bandwidth 8 GFLOPS/sec Opteron X2. dotted vertical line right represents Kernel 2, arithmetic intensity 4 FLOPs/byte. limited computationally 16 GFLOPS/s. (These data based AMD Opteron X2 (Revision F) using dual cores running 2 GHz dual socket system.) given kernel, find point X-axis based arithmetic intensity. draw vertical line point , 1071the performance kernel computer must lie somewhe along line. plot horizontal line showing peak floating- point performance computer. Obviously, actual floating- point performance higher horizontal line, since hardware limit. could plot peak memory performance, measured bytes/second? Since X-axis FLOPs/byte Y-axis FLOPs/second, bytes/second diagonal line 45- degree angle figure. Hence, plot third line gives maximum floating-point performance memory system computer support given arithmetic intensity. express limits formula plot line graph Figure 6.18 : horizontal diagonal lines give simple model name indicate value. “roofline” sets upper bound performance kernel depending arithmetic intensity. Given roofline computer, apply repeatedly, since doesn’t vary kernel. think arithmetic intensity pole hits roof, either hits slanted part roof, means performanc e ultimately limited memory bandwidth, hits flat part roof, means performance computationally limited. Figure 6.18 , kernel 1 example former, kernel 2 example latter. Note “ridge point,” diagonal horizontal roofs meet, offers interesting insight computer. far right, kernels high arithmetic intensit achieve maximum performance computer. far left, almost kernel potentially hit maximum performance. Comparing Two Generations Opterons AMD Opteron X4 (Barcelona) four cores successor Opteron X2 two cores. simplify board design, 1072use socket. Hence, DRAM channels thus peak memory bandwidth. addition doubling number cores, Opteron X4 also twice peak floating-point performance per core: Opteron X4 cores issue two floating-point SSE2 instructions per clock cycle, wh ile Opteron X2 cores issue one. two systems we’re comparing similar clock rates—2.2 GHz Opteron X2 versus 2.3 GHz Opteron X4—the Opteron X4 four times peak floating-point performance Opteron X2 DRAM bandwidth. Opteron X4 also 2MiB L3 cache, found Opteron X2. Figure 6.19 roofline models systems compared. would expect, ridge point moves right, 1 Opteron X2 5 Opteron X4. Hence, see performance gain next generation, kernels need arithmetic intensity higher 1, working sets must fit caches Opteron X4. 1073FIGURE 6.19 Roofline models two generations Opterons. Opteron X2 roofline, Figure 6.18 , black, Opteron X4 roofline color. bigger ridge point Opteron X4 means kernels computationally bound Opteron X2 could memory-performance bound Opteron X4. roofline model gives upper bound performance. Suppose program far bound. optimizations perform, order? reduce computational bottlenecks, following two optimizations help almost kernel: 10741. Floating-point operation mix . Peak floating-point performance computer typically requires equal number nearly simultaneous additions multiplications. balance necessary either computer supports fused multiply - add instruction (see Elaboration page 214 Chapter 3 ) floating-point unit equal number floating- point adders floating-point multipliers. best performan ce also requires significant fraction instruction mix floating-point operations integer instructions. 2. Improve instruction-level parallelism apply SIMD . modern archi-tectures, highest performance comes fetching, executing, committing three four instructions per clock cycle (see Section 4.10 ). goal step improve code compiler increase ILP. One way unrolling loops, saw Section 4.12 . x86 architectures, single AVX instruction operate four double precision operands, used whenever possible (see Sections 3.7 3.8). reduce memory bottlenecks, following two optimizations help: 1. Software prefetching . Usually highest performance requires keeping many memory operations flight, easier performing predicting accesses via software prefetch instructions rather waiting data required computation. 10752. Memory affinity . Microprocessors today include memory controller chip microprocessor, improves performance memory hierarchy . system multiple chips, means addresses go DRAM local one chip, rest require accesses chip interconnect access DRAM local another chip. split results non-uniform memory accesses, descri bed Section 6.5 . Accessing memory another chip lowers performance. second optimization tries allocate data threads tasked operate data memory- processor pair, processors rarely access memory chips. 1076The roofline model help decide two optimizations perform order perform them. think optimizations “ceiling” appropriate roofline, meaning cannot break ceiling without performing associated optimization. computational roofline found manuals, memory roofline found running Stream benchmark. computational ceilings, floating-point balance, also come manuals computer. memory ceiling, memory affinity, requires running experiments computer determine gap them. good news process need done per computer, someone characterizes computer’s ceilings, everyone use results prioritize optimizations f computer. Figure 6.20 adds ceilings roofline model Figure 6.18 , showing computational ceilings top graph memory bandwidth ceilings bottom graph. Although higher ceilings labeled optimizations, implied figure; break highest ceiling, yo u need already broken ones below. 10771078FIGURE 6.20 Roofline model ceilings. top graph shows computational “ceilings” 8 GFLOPs/sec floating-point operation mix imbalanced 2 GFLOPs/sec optimizations increase ILP SIMD also missing. bottom graph shows memory bandwidth ceilings 11 GB/sec without software prefetching 4.8 GB/sec memory affinity optimizations also missing. width gap ceiling next higher limit reward trying optimization. Thus, Figure 6.20 suggests optimization 2, improves ILP, large benefit improving computation computer, optimization 4, improves memory affinity, large benefit improving memory bandwidth computer. Figure 6.21 combines ceilings Figure 6.20 single graph. arithmetic intensity kernel determines optimization region, turn suggests optimizations try. Note computational optimizations memory bandwidth optimizations overlap much arithmetic intensity. Three regions shaded differently Figure 6.21 indicate different optimization strategies. example, Kern el 2 falls blue trapezoid right, suggests working computational optimizations. Kernel 1 falls blue- gray parallelogram middle, suggests trying type optimizations. Moreover, suggests starting optimizatio ns 2 4. Note Kernel 1 vertical lines fall floating- point imbalance optimization, optimization 1 may unnecessary. kernel fell gray triangle lower left , would suggest trying memory optimizations. 1079FIGURE 6.21 Roofline model ceilings, overlapping areas shaded, two kernels Figure 6.18 . Kernels whose arithmetic intensity land blue trapezoid right focus computation optimizations, kernels whose arithmetic intensity land gray triangle lower left fo cus memory bandwidth optimizations. land blue-gray parallelogram middle need worry both. Kernel 1 falls parallelogr middle, try optimizing ILP SIMD, memory affinity, software prefetching. Kernel 2 falls trapezoid right, try optimizing ILP SIMD balance floating-point operations. Thus far, assuming arithmetic intensity fixed, really case. First, kernels arithmetic intensity increases problem size, 1080Dense Matrix N-body problems (see Figure 6.17 ). Indeed, reason programmers success weak scaling strong scaling. Second, effectiveness memory hierarchy affects number accesses go memory, optimizations improve cache performance also improve arithmetic intensity. One example improving tempor al locality unrolling loops grouping together stateme nts similar addresses. Many computers special cache instructions allocate data cache first fill dat memory address, since soon over-written. optimizations reduce memory traffic, thereby moving arithmetic intensity pole right factor of, say, 1.5. shift right could put kernel different optimization reg ion. examples show help programmers improve performance, architects also use model decide optimize hardware improve performance kernels think important. next section uses roofline model demonstrate performance difference multicore microprocessor GPU see whether differences reflect performance real programs. 1081 Elaboration ceilings ordered lower ceilings easier optimize. Clearly, programmer optimize order, following sequence reduces chances wasting effort optimization benefit due constraints. Like 3Cs model, long roofline model delivers insights, model assumptions may prove optimistic. example, roofline assumes load balanced processors. Elaboration alternative Stream benchmark use raw DRAM bandwidth roofline. raw bandwidth definitely hard upper bound, actual memory performance often far boundary it’s useful. is, program go close bound. downside using Stream careful programming may exceed Stream results, memory roofline may hard limit computational roofline. stick Stream programmers able deliver memory bandwidth Stream discovers. Elaboration Although roofline model shown multicore processor s, clearly would work uniprocessor well. Check True false: main drawback conventional approaches benchmarks parallel computers rules ensure fairness also slow software innovation. 6.11 Real Stuff: Benchmarking Rooflines Intel Core i7 960 NVIDIA Tesla GPU group Intel researchers published paper [Lee et al., 2010] 1082comparing quad-core Intel Core i7 960 multimedia SIMD extensions previous generation GPU, NVIDIA Tesla GTX 280. Figure 6.22 lists characteristics two systems. products purchased Fall 2009. Core i7 Intel’s 45- nanometer semiconductor technology GPU TSMC’ 65-nanometer technology. Although might fairer comparison neutral party interested parties, purpose section determine much faster one product another, try understand relative value features two contrasting architecture styles. FIGURE 6.22 Intel Core i7-960, NVIDIA GTX 280, GTX 480 specifications. rightmost columns show ratios Tesla GTX 280 Fermi GTX 480 Core i7. Although case study Tesla 280 i7, include Fermi 480 show relationship Tesla 280 since described chapter. Note memory bandwidths higher Figure 6.23 DRAM pin bandwidths Figure 6.23 processors measured benchmark program. (From Table 2 Lee et al. [2010].) rooflines Core i7 960 GTX 280 Figure 6.23 illustrate differences computers. GTX 1083280 much higher memory bandwidth double-precision floating-point performance, also double-precision rid ge point considerably left. double-precision ridge point 0.6 GTX 280 versus 3.1 Core i7. mentioned above, much easier hit peak computational performance ridge point roofline left. single-pre cision performance, ridge point moves far right computers, it’s considerably harder hit roof single- precision performance. Note arithmetic intensity kernel based bytes go main memory, bytes go cache memory. Thus, mentioned above, caching change arithmetic intensity kernel particular compute r, references really go cache. Note also bandwidth unit-stride accesses architectures. Real gather-scatter addresses slower GTX 280 Core i7, shall see. 1084FIGURE 6.23 Roofline model [Williams, Waterman, Patterson, 2009]. rooflines show double-precision floating-point performance top row single-precision performance bottom row. (The DP FP performance ceiling also bottom row give perspective.) Core i7 960 left peak DP FP performance 51.2 GFLOPs/sec, SP FP peak 102.4 GFLOPs/sec, peak memory bandwidth 16.4 GBytes/sec. NVIDIA GTX 280 DP FP peak 78 GFLOPs/sec, SP FP peak 624 GFLOPs/sec, 127 GBytes/sec memory bandwidth. dashed vertical line left represents arithmetic intensity 0.5 FLOP/byte. limited memory bandwidth 8 DP GFLOPs/sec 8 SP GFLOPs/sec Core i7. dashed vertical line right arithmetic intensity 4 FLOP/byte. limited computationally 51.2 DP GFLOPs/sec 102.4 SP 1085GFLOPs/sec Core i7 78 DP GFLOPs/sec 624 SP GFLOPs/sec GTX 280. hit highest computation rate Core i7 need use four cores SSE instructions equal number multiplies adds. GTX 280, need use fused multiply-add instructions multithreaded SIMD processors. researchers selected benchmark programs analyzing computational memory characteristics four recently proposed benchmark suites “formulated set throughput computing kernels capture characteristics.” Figure 6.24 shows performance results, larger numbers meaning faster. Rooflines help explain relative performanc e case study. 1086FIGURE 6.24 Raw relative performance measured two platforms. study, SAXPY used measure memory bandwidth, right unit GBytes/sec GFLOP/sec. (Based Table 3 [Lee et al., 2010].) Given raw performance specifications GTX 280 vary 2.5×slower (clock rate) 7.5×faster (cores per chip) performance varies 2.0× slower (Solv) 15.2× faster (GJK), Intel researchers decided find reasons diffe rences: Memory bandwidth . GPU 4.4× memory bandwidth, helps explain LBM SAXPY run 5.0 5.3× faster; working sets hundreds megabytes hence don’t fit Core i7 cache. (So access memory intensively, purposely use cache blocking Chapter 5 .) Hence, slope rooflines explains performance. SpMV also large working set, runs 1.9× faster double-precision floating point GTX 1087280 1.5× fast Core i7. Compute bandwidth . Five remaining kernels compute bound: SGEMM, Conv, FFT, MC, Bilat. GTX faster 3.9, 2.8, 3.0, 1.8, 5.7×, respectively. first three use single-precision floating-point arithmetic, GTX 280 single precision 3 6× faster. MC uses double precision, explains it’s 1.8× faster since DP performance 1.5× faster. Bilat uses transcendental functions, GTX 280 supports directly. Core i7 spends two-thirds time calculating transcendental functions Bilat, GTX 280 5.7× faster. observation helps point value hardware support operations occur workload: double-precision floating point perhaps even transcendentals. Cache benefits . Ray casting (RC) 1.6× faster GTX cache blocking Core i7 caches prevents becoming memory bandwidth bound (see Sections 5.4 5.14), GPUs. Cache blocking help Search, too. index trees small fit cache, Core i7 twice fast. Larger index trees make memory bandwidth bound. Overall, GTX 280 runs search 1.8× faster. Cache blocking also helps Sort. programmers wouldn’t run Sort SIMD processor, written 1-bit Sort primitive called split. However, split algorithm executes many instructions scalar sort does. result, Core i7 runs 1.25× fast GTX 280. Note caches also help kernels Core i7, since cache blocking allows SGEMM, FFT, SpMV become compute bound. observation re- emphasizes importance cache blocking optimizations Chapter 5 . Gather-Scatter . multimedia SIMD extensions little help data scattered throughout main memory; optimal performance comes accesses data aligned 16- byte boundaries. Thus, GJK gets little benefit SIMD Core i7. mentioned above, GPUs offer gather-scatter addressing found vector architecture omitted SIMD extensions. memory controller even batches accesses DRAM page together (see Section 5.2 ). combination means GTX 280 runs GJK startling 15.2× fast 1088as Core i7, larger single physical parameter Figure 6.22 . observation reinforces importance gather-scatter vector GPU architectures missing SIMD extensions. Synchronization . performance synchronization limited atomic updates, responsible 28% total runtime Core i7 despite hardware fetch-and- increment instruction. Thus, Hist 1.7× faster GTX 280. Solv solves batch independent constraints small amount computation followed barrier synchronization. Core i7 benefits atomic instructions memory consistency model ensures right results even previous accesses memory hierarchy completed. Without memory consistency model, GTX 280 version launches batches system processor, leads GTX 280 running 0.5× fast Core i7. observation points synchronization performance important data parallel problems. striking often weaknesses Tesla GTX 280 uncovered kernels selected Intel researchers already addressed successor architecture Tesla: Fermi faster double-precision floating-point performance , faster atomic operations, caches. also interesting gather-scatter support vector architectures predate SIMD instructions decades important effective useful ness SIMD extensions, predicted comparison. Intel researchers noted six 14 kernels would exploit SIMD better efficient gather-scatter support Core i7. study certainly establishes importance cache blocking well. seen wide range results benchmarking different multiprocessors, let’s return DGEMM example see detail much change C code exploit multiple processors. 6.12 Going Faster: Multiple Processors Matrix Multiply 1089This section final largest step incremental performance journey adapting DGEMM underlying hardware Intel Core i7 (Sandy Bridge). Core i7 eight cores, computer using two Core i7s. Thus, 16 cores run DGEMM. Figure 6.25 shows OpenMP version DGEMM utilizes cores. Note line 30 single line added Figure 5.48 make code run multiple processors: OpenMP pragma tells compiler use multiple threads outermos loop. tells computer spread work outermost loop across threads. 1090FIGURE 6.25 OpenMP version DGEMM Figure 5.48 . Line 30 OpenMP code, making outermost loop operate parallel. line difference Figure 5.48 . Figure 6.26 plots classic multiprocessor speed-up graph, showing performance improvement versus single thread number threads increase. graph makes easy see challenges strong scaling versus weak scaling. everything fits first-level data cache, case 32 ×32 matrices, adding threads actually hurts performance. 16-threaded version DGEMM almost half fast single-threaded version case. contrast, two largest matrices get 14 × 1091speedup 16 threads, hence classic two “up right” lines Figure 6.26 . 1092FIGURE 6.26 Performance improvements relative single thread number threads increase. honest way present graphs make performance relative best version single processor program, did. plot relative performance code Figure 5.48 without including OpenMP pragmas. Figure 6.27 shows absolute performance increase increase number threads one 16. DGEMM operates 174 GLOPS 960 ×960 matrices. unoptimized C version DGEMM Figure 3.22 ran code 0.8 GFLOPS, optimizations Chapters 3 6 tailor code underlying hardware result speed-up 200 times! 1093FIGURE 6.27 DGEMM performance versus number threads four matrix sizes. performance improvement compared unoptimized code Figure 3.22 960 ×960 matrix 16 threads astounding 212 times faster! Next warnings fallacies pitfalls multiprocessing. computer architecture graveyard fille parallel processing projects ignored them. Elaboration results Turbo mode turned off. using dual chip system system, surprisingly, get ful l Turbo speed-up (3.3/2.6 =1.27) either one thread (only one core one chips) two threads (one core per chip). increase number threads hence number active cores, benefit Turbo mode decreases, less power budget spend active cores. four threads average Turbo speed-up 1.23, eight 1.13, 16 1.11. Elaboration Although Sandy Bridge supports two hardware threads per 1094core, get performance 32 threads. reason single AVX hardware shared two threads multiplexed onto one core, assigning two threads per core actually hurts performance due multiplexing overhead. 6.13 Fallacies Pitfalls decade prophets voiced contention organization single computer reached limits trul significant advances made interconnection multiplicity computers manner permit cooperative solution. …Demonstration made continued validity single processor approach … Gene Amdahl, “Validity single processor approach achieving large scale computing capabilities,” Spring Joint Computer Conf erence, 1967 many assaults parallel processing uncovered numerous fallacies pitfalls. cover four here. Fallacy: Amdahl’s Law doesn’t apply parallel computers. 1987, head research organization claimed multiprocessor machine broken Amdahl’s Law. try understand basis media reports, let’s see quote gave us Amdahl’s Law [1967, p. 483]: fairly obvious conclusion drawn point effort expended achieving high parallel processing rates wasted unless accompanied achievements sequential processing rates nearly magnitude. statement must still true; neglected portion program must limit performance. One interpretation law leads following lemma: portions every program must sequential, must economic upper bound number processors—say, 100. showing linear speed-up 1000 processors, lemma disproved; hence claim Amdahl’s Law broken. 1095The approach researchers use weak scaling: rather going 1000 times faster data set, computed 1000 times work comparable time. algorithm, sequential portion program constant, independent size input, rest fully parallel —hence, linear speed-up 1000 processors. Amdahl’s Law obviously applies parallel processors. research point one main uses faster computers run larger problems. sure users reall care problems versus justification buying expensive computer finding problem simply keeps lots processors busy. Fallacy: Peak performance tracks observed performance. supercomputer industry used metric marketin g, fallacy exacerbated parallel machines. marketers using nearly unattainable peak performance uniprocessor node, also multiplying otal number processors, assuming perfect speed-up! Amdahl’s Law suggests difficult reach either peak; multiplying two together multiplies sins. roofline model helps put peak performance perspective. Pitfall: developing software take advantage of, optimize for, multiprocessor architecture. long history parallel software lagging behind parallel hardware, possibly software problems much harder. give one example show subtlety issues, many examples could choose! One frequently encountered problem occurs software designed uniprocessor adapted multiprocessor environment. example, Silicon Graphics operating system originally protected page table single lock, assuming page allocation infrequent. uniprocessor, represent performance problem. multiprocessor, become major performance bottleneck programs. Consider program uses large number pages 1096initialized start-up, UNIX statically allocated pages. Suppose program parallelized multiple processes allocate pages. page allocation requires use page table, locked whenever use, even OS kernel allows multiple threads OS serialized f processes try allocate pages (which exactl might expect initialization time!). page table serialization eliminates parallelism initialization significant impact overall parallel performance. performance bottleneck persists even task - level parallelism. example, suppose split parallel processing program apart separate jobs run them, one job per processor, sharing jobs. (This exactly one user did, since reasonably believed performance problem due unintended sharing interference application.) Unfortunately, lock still serializes jobs—so even independent job performance poor. pitfall indicates kind subtle significant performance bugs arise software runs multiprocessors. Like many key software components, algorithms data structures must rethought multiprocessor context. Placing locks smaller portions th e page table effectively eliminated problem. Fallacy: get good vector performance without providing memory bandwidth. saw Roofline model, memory bandwidth quite important architectures. DAXPY requires 1.5 memory references per floating-point operation, ratio typic al many scientific codes. Even floating-point operations ok time, Cray-1 could increase DAXPY performance vector sequence used, since memory limited. Cray-1 performance Linpack jumped compiler used blocking change computation values could kept vector registers. approach lowered number memory references per FLOP improved performance nearly factor two! Thus, memory bandwidth Cray-1 became 1097sufficient loop formerly required bandwidth, whi ch Roofline model would predict. 6.14 Concluding Remarks dedicating future product development multicore designs. believe key inflection point industry. … race. sea change computing… Paul Otellini, Intel President, Intel Developers Forum , 2004 dream building computers simply aggregating processo rs around since earliest days computing. Progress building using effective efficient parallel processors, however, slow. rate progress limited difficult software problems well long process evolvi ng architecture multiprocessors enhance usability improve efficiency. discussed many software challenges chapter, including difficulty writing programs obtain good speed-up due Amdahl’s Law. wide variety different architectural approaches limited success short life many parallel architectures past compounded software difficulties. discuss history development multiprocessors onli ne Section 6.15 . go even greater depth topics chapter, see Chapter 4 Computer Architecture: Quantitative Approach, Fifth Edition GPUs comparisons GPUs CPUs Chapter 6 WSCs. said Chapter 1 , despite long checkered past, information technology industry tied future parallel computing. Although easy make case effort fail like many past, reasons hopeful: Clearly, software service (SaaS) growing importance, clusters proven successful way deliver services. providing redundancy higher level, including geographically distributed datacenters, services delivered 24 ×7 ×365 availability customers around world. believe Warehouse-Scale Computers changing 1098goals principles server design, needs mobile clients changing goals principles microprocessor design. revolutionizing software industry well. Performance per dollar performance per joule drive mobile client hardware WSC hardware, parallelism key delivering sets goals. SIMD vector operations good match multimedia applications, playing larger role post-PC era. share advantage easier programmer classic parallel MIMD programming energy-efficient MIMD. put perspective importance SIMD versus MIMD, Figure 6.28 plots number cores MIMD versus number 32-bit 64-bit operations per clock cycle SIMD mode x86 computers time. x86 computers, expect see two additional cores per chip every 2 years SIMD width double every 4 years. Given assumptions, next decade potential speed-up SIMD parallelism twice MIMD parallelism. Given effectiveness SIMD multimedia increasing importance post-PC era, emphasis may appropriate. Hence, it’s least important understand SIMD parallelism MIMD parallelism, even though latter received much attention. use parallel processing domains scientific engineering computation popular. application domain almost limitless thirst computation. also many applications lots natural concurrency. again, clusters dominate application area. example, using 2012 Top 500 report, clusters responsible 80% 500 fastest Linpack results. desktop server microprocessor manufacturers building multiprocessors achieve higher performance, so, unlike past, easy path higher performance sequential applications. said earlier, sequential programs slow programs. Hence, programmers need higher performance must parallelize codes write new parallel processing programs. past, microprocessors multiprocessors subject different definitions success. scaling uniprocessor 1099performance, microprocessor architects happy single thread performance went square root increased silicon area. Thus, pleased sublinear performance terms resources. Multiprocessor success used defi ned linear speed-up function number processors, assuming cost purchase cost administration n processors n times much one processor. parallelism happening on-chip via multicore, use traditional microprocessor metric successful sublinear performance improvement. success just-in-time runtime compilation autotunin g makes feasible think software adapting take advantage increasing number cores per chip, provides flexibility available limited static compilers. Unlike past, open source movement become critical portion software industry. movement meritocracy, better engineering solutions win mind share developers legacy concerns. also embraces innovation, inviting change old software welcoming new languages software products. open culture could extremely helpful time rapid change. 1100FIGURE 6.28 Potential speed-up via parallelism MIMD, SIMD, MIMD SIMD time x86 computers. figure assumes two cores per chip MIMD added every 2 years number operations SIMD double every 4 years. motivate readers embrace revolution, demonstrated potential parallelism concretely matrix multiply Intel Core i7 (Sandy Bridge) Going Faster sections Chapters 3 6: Data-level parallelism Chapter 3 improved performance factor 3.85 executing four 64-bit floating-point operations parallel using 256-bit operands AVX instructions, 1101demonstrating value SIMD. Instruction-level parallelism Chapter 4 pushed performance another factor 2.3 unrolling loops four times give out-of-order execution hardware instructions schedu le. Cache optimizations Chapter 5 improved performance matrices didn’t fit L1 data cache another factor 2.0 2.5 using cache blocking reduce cache misses. Thread-level parallelism chapter improved performance matrices don’t fit single L1 data cache another factor 4 14 utilizing 16 cores multicore chips, demonstrating value MIMD. adding single line using OpenMP pragma. Using ideas book tailoring software computer added 24 lines code DGEMM. matrix sizes 32 ×32, 160 ×160, 480 ×480, 960 ×960, overall performance speed-up ideas realized two-dozen lines co de factors 8, 39, 129, 212! parallel revolution hardware/software interface perhaps greatest challenge facing field last 60 years. also think outstanding opportunity, Going Faster sections demonstrate. revolution provide many new research business prospects inside outside fi eld, companies dominate multicore era may ones dominated uniprocessor era. understanding underlying hardware trends learning adapt software them, perhaps one innovators seize opportunities certain appear uncertain times ahead. look forward benefiting inventions! Historical Perspective 1102and Reading section online gives rich often disastrous history f multiprocessors last 50 years. References B. F. Cooper, A. Silberstein, E. Tam, R. Ramakrishnan, R. Sears. Benchmarking cloud serving systems YCSB, In: Proceedings 1st ACM Symposium Cloud computing, June 10–11, 2010, Indianapolis, Indiana, USA, doi:10.1145/1807128.1807152. G. Regnier, S. Makineni, R. Illikkal, R. Iyer, D. Minturn, R. Huggahalli, D. Newell, L. Cline, A. Foong. TCP onloading data center servers. IEEE Computer, 37(11):48–58, 2004. 6.15 Historical Perspective Reading tremendous amount history multiprocessors; thi section, divide discussion time period architecture. start SIMD approach Illiac IV. turn short discussion early experimental multiprocessors progress discussion gre debates parallel processing. Next describe historical r oots present multiprocessors conclude discussing recent advances. 1103SIMD Computers: Attractive Idea, Many Attempts, Lasting Successes cost general multiprocessor is, however, high design options considered would decrease cost without seriously degrading power efficiency system. options consist recentralizing one three major componen ts.… Centralizing [control unit] gives rise basic organizati [an] … array processor Illiac IV. Bouknight et al. [1972] SIMD model one earliest models parallel computing, dating back first large-scale multiprocessor, Illiac IV. key idea multiprocessor, recent SIMD multiprocessors, single instruction operate many data items once, using many functional units (see Figure e6.15.1 ). 1104FIGURE E6.15.1 Illiac IV control unit followed 64 processing elements. perhaps infamous supercomputers. project started 1965 ran first real application 1976. 64 processors used 13-MHz clock, combined main memory size 1 MB: 64 ×16 KB. Illiac IV first machine teach us software parallel machines dominates hardware issues. Photo courtesy NASA Ames Research Center. Although successful pushing several technologies prove useful later projects, failed computer. Costs escalated fro $8 million estimate 1966 $31 million 1972, despite construction quarter planned multiprocessor. Actual performance best 15 MFLOPS, versus initial predictions 1000 MFLOPS full system [Hord, 1982]. 1105Delivered NASA Ames Research 1972, computer required three years engineering usable. events slowed investigation SIMD, Danny Hillis [1989] resuscitating style Connection Machine, 65,636 1-bit processors. Real SIMD computers need mixture SISD SIMD instructions. SISD host computer perform operati ons branches address calculations need parallel operation. SIMD instructions broadcast executio n units, set registers. flexibility , individual execution units disabled SIMD instruction. addition, massively parallel SIMD multiprocesso rs rely interconnection communication networks exchange data processing elements. SIMD works best dealing arrays loops. Hence, opportunity massive parallelism SIMD, must massive amounts data, data parallelism. SIMD weakest case statements, execution unit must perform different operation data, depending data has. execution units wrong data disabled proper units continue. situations essentially run 1/nth performance, n number cases. basic tradeoff SIMD multiprocessors performance processor versus number processors. Recent multiproc essors emphasize large degree parallelism performance individual processors. Connection Multiprocessor 2, example, offered 65,536 single-bit-wide processors, Illiac IV 64 64-bit processors. resurrected 1980s, originally Thinking Machines MasPar, SIMD model put bed general-purpose multiprocessor architectur e, two main reasons. First, inflexible. number important problems cannot use style multiprocessor, architecture scale competitive fashion; is, small-scale SIMD multiprocessors often worse cost performance alternatives. Second, SIMD cannot take advantage tremendous performance cost advantages microprocessor technology. Instead leveraging thi low-cost technology, designers SIMD multiprocessors mus build 1106custom processors multiprocessors. Although SIMD computers departed scene general-purpose alternatives, style architecture c ontinue role special-purpose designs. Many special-purpose tasks highly data parallel require limited set functional units. Thus, designers build support certain operation s, well hardwired interconnection paths among functional units. organizations often called array processors, useful tasks like image signal processing. Multimedia Extensions SIMD Extensions Instruction Sets Many recent architectures laid claim first er multimedia extensions, set new instructions takes advantage single wide ALU partitioned act several narrower ALUs operating parallel. It’s unlikel appeared 1957, however, Lincoln Lab’s TX-2 computer offered instructions operated ALU either one 36-bit operation, two 18-bit operations, four 9-bit operations. Ivan Sutherland, considered Father Computer Graphics, built historic Sketchpad system TX-2. Sketchpad did, fact, take advantage SIMD instructions, despite TX-2 appearing invention term SIMD. Early Experiments difficult distinguish first MIMD multiprocessor . Surprisingly, first computer Eckert-Mauchly Corporation, example, duplicate units improve availability . 1107Two best-documented multiprocessor projects undertaken 1970s Carnegie Mellon University. first C.mmp, consisted 16 PDP-11s connected crossbar switch 16 memory units. among first multiprocessors processors, shared memory programming model. Much focus research C.mmp project software, especially OS area. later multiprocessor, Cm*, cluster-based multiprocessor distributed memory nonuniform access time. absence caches long remote access latency made data placement critical. Many ideas multiprocessors would reused 1980s, microprocessor made much cheaper build multiprocessors. Great Debates Parallel Processing turning away conventional organization came middle 1960s, law diminishing returns began take effect effort increase operational speed computer .… Electronic circuits ultimately limited speed opera tion speed light … many circuits already operating nanosecond range. W. Jack Bouknight et al. Illiac IV System [1972] … sequential computers approaching fundamental physical limit potential computational power. limit spee light … Angel L. DeCegama Technology Parallel Processing, Volume [1989] 1108… today’s multiprocessors … nearing impasse technologies approach speed light. Even components sequential processor could made work fast, best could expected million instructions per second. David Mitchell Transputer: Time [1989] quotes give classic arguments abandoning current form computing, Amdahl [1967] gave classic reply support continued focus IBM 360 architecture. Arguments advantages parallel execution traced back 19th century [Menabrea, 1842]! Despite this, effectiveness multiprocessor reducing latency individual important programs still explored. Aside fro debates advantages limitations parallelism, several hot debates focused build multiprocessors . today’s perspective, clear speed light brick wall; brick wall was, instead, power consumption CMOS clock rates increased. It’s hard predict future, yet 1989 Gordon Bell made two predictions 1995. included predictions first edition book, outcome completely unclear. discuss section, together assessment accuracy prediction. first computer capable sustaining tera FLOPS —one million MFLOPS—would constructed 1995, using either multicomputer 4K 32K nodes Connection Multiprocessor several million processing elements. put prediction perspective, year Gordon Bell Prize acknowledges advances parallelism, including fastest real program (highest MFLOPS). 1989, winner used eight- processor Cray Y-MP run 1680 MFLOPS. basis numbers, multiprocessors programs would improved factor 3.6 year fastest program achieve 1 TFLOPS 1995. 1999, first Gordon Bell prize winner crossed 1 TFLOPS bar. Using 5832-processor IBM RS/6000 SST system designed specially Livermore Laboratories, achieved 1.18 TFLOPS shock wave simulation. ratio represents year-to-year improvement 1.93, still quite 1109impressive. recognized since 1990s although may technology build TFLOPS multiprocessor, clear machine cost-effective, except perhaps fe w specialized critically important applications related national security. estimated 1990 achieving 1 TFLOPS would require machine 5000 processors would cost $100 million. 5832-processor IBM system Livermore cost $110 million. might expected, improvements performance individual micro-processors cost performance directly affect cost performance large-sc ale multiprocessors, 5000-processor system cost 5000 times price desktop system using processor. Since time, much faster multiprocessors built, major improvements increasingly come processors past 5 years, rather fundamental breakthroughs parallel architecture. second Bell prediction concerned number data streams super- computers shipped 1995. Danny Hillis believed although supercomputers small number data streams might best sellers, biggest multiprocessors would multiprocessors many data streams, would perform bulk computations. Bell bet Hillis last quarte r calendar year 1995, sustained MFLOPS would shipped multiprocessors using data streams (<100) rather many data streams (>1000). bet concerned supercomputers, defined multiprocessors costing $1 million used scientific applications. Sustained MFLOPS defined th bet number floating-point operations per month, availability multiprocessors affects rating. 1989, bet made, totally unclear would win. 1995, survey current publicly known supercomputers showed six multiprocessors existenc e world 1000 data streams, Bell’s prediction clear winner. fact, 1995, much smaller microprocessor-based multiprocessors (<20 processors) becoming dominant. 1995, survey 500 highest-performance multiprocessors use (based Linpack ratings), called Top 500, showed largest number multiprocessors bus-based shared 1110memory multiprocessors! 2005, various clusters multicomputers played large role. example, top 25 systems, 11 custom clusters, IBM Blue Gene system Cray XT3, 10 clusters shared memory multiprocessors (both using distributed centralized memor y), remaining four clusters built using PCs off- the-shelf interconnect. Recent Advances Developments primary exception parallel vector multiprocessor recently IBM Blue Gene design, modern MIMD computers built off-the-shelf microprocessors using bus logically central memory interconnection network distributed memory. number experimental multiprocessors built 1980s refined enhanced concepts form basis many today’s multiprocessors. Development Bus-Based Coherent Multiprocessors Although large mainframes built multiple processors 1960s 1970s, multiprocessors become highly successful 1980s. Bell [1985] suggests key smaller size microprocessor allowed memory bu replace interconnection network hardware portable operating systems meant multiprocessor projects longer required invention new operating system. paper, Bell defined terms multiprocessor multicomputer et stage two different approaches building larger-scale multiprocessors. first bus-based multiprocessor sn ooping caches Synapse N +1 1984. early 1990s saw beginning expansion systems use wide, high-speed buses (the SGI Challenge system used 256-bit, packet-oriented bus supporting eight processor boards 32 processors) later use multiple buses crossbar interconnects, example, Sun SPARCCenter Enterprise systems. 2001, Sun Enterprise servers represented primary example large-scale (>16 1111processors), symmetric multiprocessors active use. Toward Large-Scale Multiprocessors effort build large-scale multiprocessors, two differ ent directions explored: message-passing multicomputers scalable shared memory multiprocessors. Although many attempts build mesh hypercube-connected multiprocessors, one first multiprocessors succes sfully bring together pieces Cosmic Cube built Caltech [Seitz, 1985]. introduced important advances routing interconnect technology substantially reduced cost interconnect, helped make multicomputer viable. Intel iPSC 860, hypercube-connected collection i860s, based ideas. recent multiprocessors, Intel Paragon, used networks lower dimensionality higher individual links. Paragon also employed separate i860 communications controller node, although number users found better use i860 processors computation well communication. Thinking Multiprocessors CM-5 made use off-the-shelf microprocess ors. provided user-level access communication channel, significantly improving communication latency. 1995, two multiprocessors represented state art message-pas sing multicomputers. Clusters Clusters probably “invented” 1960s customers could fit work one computer, needed backup machine case failure primary machine [Pfister, 1998]. Tandem introduced 16-node cluster 1975. Digital followed VAX clusters, introduced 1984. originally independent computers shared I/O devices, requiring distributed operating system coordinate activit y. Soon communication links computers, part computers could geographically distributed incre ase availability case disaster single site. Users log cluster unaware machine using. DEC (now HP) sold 25,000 clusters 1993. early 1112companies Tandem (now HP) IBM (still IBM). Today, virtually every company cluster products. products aimed availability, performance scaling secondary benefit. Scientific computing clusters emerged competitor MPPs. 1993, Beowulf project started goal fulfilling NASA’s desire 1-GFLOPS computer less $50,000. 1994, 16-node cluster built off-the-shelf PCs using 80486s achieved goal. emphasis led variety software interfaces make easier submit, coordinate, debug large programs large number independent programs. Efforts made reduce latency communication cluster well increase bandwidth, several research projects worked problem. (One commercial result low-latenc research VI interface standard, embraced Infiniband, discussed below.) Low latency proved useful applications. example, 1997 cluster 100 UltraSPARC desktop computers U.C. Berkeley, connected 160 MB/sec per link Myrinet switches, used set world records database sort (sorting 8.6 GB data originally disk 1 minute) cracking encrypted message (taking 3.5 hours decipher 40-bit DES key). research project, called Network Workstations, also developed Inktomi search engine, led start-up company name. Google followed example Inktomi build search engines clusters desktop comp uters rather large-scale SMPs, strategy leading search engine, Alta Vista, Google took over. 2013, virtually Internet services rely clusters serve million customers. Clusters also popular scientists. One reason thei r low cost, enables individual scientists small groups cluster dedicated programs. clusters get results faster waiting long job queues shared MPPs supercomputer centers, stretch weeks. interested learning more, Pfister [1998] written entertaining book clusters. Recent Trends Large-Scale Multiprocessors 1113In mid-to-late 1990s, became clear hoped-for growth market ultralarge-scale parallel computing unlikely occur. Without market growth, became increasingly obvious high-end parallel computing market small support costs highly customized hardware software designed small market. Perhaps important trend come observation clustering would used reach highest levels performance. three general classes large-scale multiprocessors: 1. Clusters integrate standard desktop motherboards using interconnection technology, Myrinet Infiniban 2. Multicomputers built standard microprocessors configu red processing elements connected custom interconn ect, IBM Blue Gene 3. Clusters small-scale shared memory computers, possibly vector support, including Earth Simulator IBM Blue Gene interesting designs, sin ce rationale parallels underlying causes recent trend toward multicore uniprocessor architectures. Blue Gene star ted research project within IBM aimed protein sequencing folding problem. Blue Gene designers observed powe r becoming increasing concern large-scale multiprocesso rs performance/watt processors embedded space much better high-end uniprocessor space. parallelism route high performance, start efficient building block simply them? Thus, Blue Gene constructed using custom chip includes embedded PowerPC microprocessor offering half performance high-end PowerPC, much smaller fraction area power. allows system functions, including global interconnect, integrated onto sam e die. result highly replicable efficient building bl ock, allowing Blue Gene reach much larger processor counts efficiently. Instead using stand-alone microprocessors standard desktop boards building blocks, Blue Gene uses processor cores. doubt approach provides much greater efficiency. Whether market support cost customized design special software remains open question. 1114In 2006, Blue Gene processor Lawrence Livermore 32K processors held factor 2.6 lead Linpack performance third-place system, consisted 20 SGI Altix 512-processor systems interconnected Infiniband cluster. Blue Gene’s predecessor experimental machine, QCDOD, pioneered concept machine using lower-power embedded microprocessor tightly integrated interconnect drive cost power consumption node. Looking almost unbounded amount information multiprocessors multicomputers: conferences, journal paper s, even books seem appear faster single person absorb ideas. doubt many papers go unnoticed —not unlike past. major architecture conferences contain papers multiprocessors. annual conference, Supercomputing XY (where X last two digits year), brings together users, architects, software developers, vendors publishes proceedings book, CD-ROM, online (see www.scXY.org ) form. Two major journals, Journal Parallel Distributed Computing IEEE Transactions Parallel Distributed Systems , contain papers aspects parallel processing. Several books focusing parallel processi ng included following references, Culler et al. [1998] recent, large-scale effort. years, Eugene Miya NASA Ames collected online bibliography parallel processing papers. bibliography, contains 35,000 entries, available online at: www.ira.uka.de/bibliography/Parallel/Eugene/index.html . Asanovic et al. [2006] surveyed wide-ranging challenges industry multicore challenge. report may help ful understanding depth various challenges. addition documenting discovery concepts used practice, references also provide descriptions many deas explored found wanting, well ideas whose time yet come. Given move toward multicore multiprocessors future high-performance computer architecture, expect many new approaches explored 1115in years ahead. manage solve hardware software problems key using multiprocessing past 40 years! Reading Almasi, G. S. A. Gottlieb [1989]. Highly Parallel Computing , Benjamin/Cummings, Redwood City, CA. textbook covering parallel computers . Amdahl, G. M. [1967]. “Validity single processor approach achieving large scale computing capabilities,” Proc. AFIPS Spring Joint Computer Conf ., Atlantic City, NJ (April), 483–85. Written response claims Illiac IV, three-page art icle describes Amdahl’s law gives classic reply arguments abandoning current form computing . Andrews, G. R. [1991]. Concurrent Programming: Principles Practice , Benjamin/Cummings, Redwood City, CA. text gives principles parallel programming . Archibald, J. J. -L. Baer [1986]. “Cache coherence protocols: Evaluation using multiprocessor simulation model”, ACM Trans. Computer Systems 4 4 (November), 273–98. Classic survey paper shared-bus cache coherence protocols . Arpaci-Dusseau, A., R. Arpaci-Dusseau, D. Culler, J. Hellerstein, D. Patterson [1997]. “High-performance sorting networks workstations,” Proc. ACM SIG MOD/PODS Conference Management Data, Tucson, AZ (May), 12–15. world record sort performed cluster, including architecture critique workstation network interfac e. April 1, 1997, pushed record 8.6 GB 1 minute 2.2 seconds sort 100 MB . Asanovic, K., R. Bodik, B. C. Catanzaro, J. J. Gebis, P. Husbands, K. Keutzer, D. A. Patterson, W. L. Plishker, J. Shalf, S. W. Williams, K. A. Yelick. [2006]. “The landscape parallel computing research: view Berkeley.” Tech. Rep. UCB/EECS-2006-183, EECS Department, University California, Berkeley (December 18). Nicknamed “Berkeley View,” report lays landscape multicore challenge . Bailey, D. H., E. Barszcz, J. T. Barton, D. S. Browning, R. L. Carter, 1116L. Dagum, R. A. Fatoohi, P. O. Frederickson, T. A. Lasinski, R. S. Schreiber, H. D. Simon, V. Venkatakrishnan, S. K. Weeratunga. [1991]. “The NAS parallel benchmarks—summary preliminary results,” Proceedings 1991 ACM/IEEE conference Super- computing (August). Describes NAS parallel benchmarks . Bell, C. G. [1985]. “Multis: new class multiprocessor computers”, Science 228(April 26), 462–467. Distinguishes shared address nonshared address multiprocessors based micro processors . Bienia, C., S. Kumar, J. P. Singh, K. Li [2008]. “The PARSEC benchmark suite: characterization architectural implications,” Princeton University Technical Report TR-81 1-008 (January). Describes PARSEC parallel benchmarks. Also see http://parsec.cs.princeton.edu/ . Bouknight, W. J., Denenberg, S. A., McIntyre, D. E., Randall, J. M., Sameh, A. H., Slotnick, D. L. [1972]. Illiac IV system, Proceedings IEEE , 60(4), 369–388. describes infamous SIMD supercomputer . Culler, D. E. J. P. Singh, A. Gupta [1998]. Parallel Computer Architecture , Morgan Kaufmann, San Francisco. textbook parallel computers . Dongarra, J. J., J. R. Bunch, G. B. Moler, G. W. Stewart [1979]. LINPACK Users’ Guide , Society Industrial Mathematics. original document describing Linpack, became widely used parallel bench mark . Falk, H. [1976]. “Reaching gigaflop”, IEEE Spectrum 13: 10 (October), 65–70. Chronicles sad story Illiac IV: four times cost less han one-tenth performance original goals . Flynn, M. J. [1966]. “Very high-speed computing systems”, Proc. IEEE 54 12 (December), 1901–09. Classic article showing SISD/SIMD/MISD/MIMD classificatio ns. Hennessy, J. D. Patterson [2003]. Chapters 6 8 Computer Architecture: Quantitative Approach , third edition, Morgan Kaufmann Publishers, San Francisco. in-depth coverage variety multiprocessor cluster topics, including programs measurements . Henning, J. L. [2007]. “SPEC CPU suite growth: historical 1117perspective”, Computer Architecture News Vol. 35, no. 1 (March). Gives history SPEC, including use SPECrate measure performance independent jobs, used parallel benchmark . Hillis, W. D. [1989]. connection machine . MIT Press. PhD Dissertation makes case 1-bit SIMD computer . Hord, R. M. [1982]. Illiac-IV, First Supercomputer , Computer Science Press, Rockville, MD. historical accounting Illiac IV project . Hwang, K. [1993]. Advanced Computer Architecture Parallel Programming , McGraw-Hill, New York. Another textbook covering parallel computers . Kozyrakis, C. D. Patterson [2003]. “Scalable vector processors embedded systems”, IEEE Micro 23:6 (November–December), 36–45. Examination vector architecture MIPS instruction se media signal processing . Menabrea, L. F. [1842]. “Sketch analytical engine invented Charles Babbage”, Bibliothèque Universelle de Genève (October). Certainly earliest reference multiprocessors, mathem atician made comment translating papers Babbage’s mechanical computer . Pfister, G. F. [1998]. Search Clusters: Coming Battle Lowly Parallel Computing , second edition, Prentice Hall, Upper Saddle River, NJ. entertaining book advocates clusters critical NU multiprocessors . Regnier, G., S. Makineni, I. Illikkal, R. Iyer, D. Minturn, R. Huggahalli, A. Foong [2004]. TCP onloading data center servers. Computer , 37(11), 48–58. paper describing benefits TCP/IP inside servers vs. external hardware . Seitz, C. [1985]. “The Cosmic Cube”, Comm . ACM 28 1 (January), 22–31. tutorial article parallel processor connected via hypertree . Cosmic Cube ancestor Intel supercomputers . Slotnick, D. L. [1982]. “The conception development parallel processors—a personal memoir”, Annals History Computing 4: 1 (January), 20–30. 1118Recollections beginnings parallel processing archite ct Illiac V . Williams, S., J. Carter, L. Oliker, J. Shalf, K. Yelick [2008]. “Lattice Boltzmann simulation optimization leading multicore platforms,” International Parallel & Distributed Processing Symposium (IPDPS) . Paper containing results four multicores LBMHD . Williams, S., L. Oliker, R. Vuduc, J. Shalf, K. Yelick, J. Demmel [2007]. “Optimization sparse matrix-vector multiplication emerging multicore platforms,” Supercomputing (SC). Paper containing results four multicores SPmV . Williams, S. [2008]. Autotuning Performance Multicore Computers , Ph.D. Dissertation, U.C. Berkeley. Dissertation containing roofline model . Woo, S. C., M. Ohara, E. Torrie, J. P. Singh, A. Gupta. “The SPLASH-2 programs: characterization methodological considerations,” Proceedings 22nd Annual International Symposium Computer Architecture (ISCA ’95) , May, 24–36. Paper describing second version Stanford parallel benchmark s. 6.16 Exercises 6.1 First, write list daily activities typically weekday. instance, might get bed, take shower, get dressed, eat breakfast, dry hair, brush teeth. Make sure break list minimum 10 activities. 6.1.1 [5] <§6.2> consider activities already exploiting form parallelism (e.g., brushing multiple teeth time, versus one time, carrying one book time school, versus loading backpack carry “in parallel”). activities, discuss already working parallel, not, not. 6.1.2 [5] <§6.2> Next, consider activities could carried concurrently (e.g., eating breakfast listening news). activities, describe activity could paired activity. 11196.1.3 [5] <§6.2> Exercise 6.1.2 , could change current systems (e.g., showers, clothes, TVs, cars) could perform tasks parallel? 6.1.4 [5] <§6.2> Estimate much shorter time would take carry activities tried carry many tasks parallel possible. 6.2 trying bake three blueberry pound cakes. Cake ingredients follows: 1 cup butter, softened 1 cup sugar 4 large eggs 1 teaspoon vanilla extract 1/2 teaspoon salt 1/4 teaspoon nutmeg 1 1/2 cups flour 1 cup blueberries recipe single cake follows: Step 1: Preheat oven 325°F (160°C). Grease flour cake pan. Step 2: large bowl, beat together mixer butter sugar medium speed light fluffy. Add eggs, vanilla, salt nutmeg. Beat thoroughly blended. Reduce mixer speed low add flour, 1/2 cup time, beating blended. Step 3: Gently fold blueberries. Spread evenly prepared baking pan. Bake 60 minutes. 6.2.1 [5] <§6.2> job cook three cakes efficiently possible. Assuming one oven large enough hold one cake, one large bowl, one cake pan, one mixer, come schedule make three cakes quickly possible. Identify bottlenecks completing task. 6.2.2 [5] <§6.2> Assume three bowls, three cake pans three mixers. much faster process additional resources? 6.2.3 [5] <§6.2> Assume two friends help cook, large oven accommodate three cakes. change schedule arrived Exercise 6.2.1 above? 6.2.4 [5] <§6.2> Compare cake-making task computing 1120three iterations loop parallel computer. Identify data- level parallelism task-level parallelism cake-making loop. 6.3 Many computer applications involve searching set data sorting data. number efficient searching sorting algorithms devised order reduce runtime tedious tasks. problem consider best parallelize tasks. 6.3.1 [10] <§6.2> Consider following binary search algorithm (a classic divide conquer algorithm) searches value X sorted N-element array returns index matched entry: BinarySearch(A[0..N−1], X) { low = 0 high = N −1 (low <= high) { mid = (low + high) / 2 (A[mid] >X) high = mid −1 else (A[mid] <X) low = mid + 1 else return mid // found } return −1 // found } Assume cores multi-core processor run BinarySearch. Assuming much smaller N, express speed-up factor might expect obtain values N. Plot graph. 6.3.2 [5] <§6.2> Next, assume equal N. would affect conclusions previous answer? tasked obtaining best speed-up factor possible (i.e., strong scaling), explain might change code obtain it. 6.4 Consider following piece C code: (j=2;j<=1000;j++) D[j] = D[j−1]+D[j−2]; RISC-V code corresponding fragment is: 1121 li x5, 8000 add x12, x10, x5 addi x11, x10, 16 LOOP: fld f0, -16(x11) fld f1, -8(x11) fadd.d f2, f0, f1 fsd f2, 0(x11) addi x11, x11, 8 ble x11, x12, LOOP latency instruction number cycles must come instruction instruction using resul t. Assume floating point instructions following associated latencies (in cycles): fadd.d fld fsd 4 6 1 6.4.1 [10] <§6.2> many cycles take execute code? 6.4.2 [10] <§6.2> Re-order code reduce stalls. Now, many cycles take execute code? (Hint: remove additional stalls changing offset fsd instruction.) 6.4.3 [10] <§6.2> instruction later iteration loop depends upon data value produced earlier iteration loop, say loop-carried dependence iterations loop. Identify loop-carried dependences code. Identify dependent program variable assembly-level registers. ignore loop induction variable j. 6.4.4 [15] <§6.2> Rewrite code using registers carry data iterations loop (as opposed storing re-loading data main memory). Show code stalls calculate number cycles required execute. Note problem need use assembler pseudo-instruction “ fmv.d rd, rs1 ”, writes value floating-point register rs1 floating-point register rd. Assume fmv.d executes single cycle. 6.4.5 [10] <§6.2> Loop unrolling described Chapter 4 . Unroll optimize loop unrolled loop 1122handles three iterations original loop. Show code stalls calculate number cycles required execute. 6.4.6 [10] <§6.2> unrolling Exercise 6.4.5 . works nicely happen want multiple three iterations. happens number iterations known compile time? efficiently handle number iterations isn’t multiple number iterations per unrolled loop? 6.4.7 [15] <§6.2> Consider running code two-node distributed memory message passing system. Assume going use message passing described Section 6.7 , introduce new operation send ( x, y) sends node x value y, operation receive( ) waits value sent it. Assume send operations take one cycle issue (i.e., later instructions node proceed next cycle), take several cycles received receiving node. Receive instructions stall execution node executed receive message. use system speed code exercise? so, maximum latency receiving information tolerated? not, not? 6.5 Consider following recursive mergesort algorithm (anothe r classic divide conquer algorithm). Mergesort first described John Von Neumann 1945. basic idea divide unsorted list x elements two sublists half size original list. Repeat operation sublist, continue lists size 1 length. starting sublists length 1, “merge” two sublists single sorted list. Mergesort(m) var list left, right, result length(m) ≤ 1 return else var middle = length(m) / 2 x middle add x left x middle 1123 add x right left = Mergesort(left) right = Mergesort(right) result = Merge(left, right) return result merge step carried following code: Merge(left,right) var list result length(left) >0 length(right) > 0 first(left) ≤ first(right) append first(left) result left = rest(left) else append first(right) result right = rest(right) length(left) >0 append rest(left) result length(right) >0 append rest(right) result return result 6.5.1 [10] <§6.2> Assume cores multicore processor run Mergesort. Assuming much smaller length (m), express speed-up factor might expect obtain values length (m). Plot graph. 6.5.2 [10] <§6.2> Next, assume equal length (m). would affect conclusions previous answer? tasked obtaining best speed-up factor possible (i.e., strong scaling), explain might change code obtain it. 6.6 Matrix multiplication plays important role number applications. Two matrices multiplied number columns first matrix equal number rows second. Let’s assume m×n matrix want multiply n×p matrix B. express product m×p matrix denoted AB (or A·B). assign C=AB, ci,j denotes entry C position ( i, j), eac element j 11241≤i≤m 1≤ j≤p . want see parallelize computation C. Assume matrices laid memory sequentially follows: a1,1, a2,1, a3,1, a4,1, …, etc. 6.6.1 [10] <§6.5> Assume going compute C single-core shared-memory machine four-core shared- memory machine. Compute speed-up would expect obtain four-core machine, ignoring memory issues. 6.6.2 [10] <§6.5> Repeat Exercise 6.6.1 , assuming updates C incur cache miss due false sharing consecutive elements row (i.e., index i) updated. 6.6.3 [10] <§6.5> would fix false sharing issue occur? 6.7 Consider following portions two different programs running time four processors symmetric multicore processor (SMP). Assume code run, x 0. Core 1: x = 2 ; Core 2: = 2 ; Core 3: w = x + + 1 ; Core 4: z = x + ; 6.7.1 [10] <§6.5> possible resulting values w,x,y, z? possible outcome, explain might arrive values. need examine possible interleavings instructions. 6.7.2 [5] <§6.5> could make execution deterministic one set values possible? 6.8 dining philosopher’s problem classic problem synchronization concurrency. general problem stated philosophers sitting round table one two things: eating thinking. eating, thinking, thinking, eating. bowl pasta center. fork placed philosopher. result philosopher one fork left one fork right. Given nature eating pasta, philosopher needs two forks eat, use forks immediate left right. philosophers speak one another. 6.8.1 [10] <§6.7> Describe scenario none 1125philosophers ever eats (i.e., starvation). sequence events happen lead problem? 6.8.2 [10] <§6.7> Describe solve problem introducing concept priority. guarantee treat philosophers fairly? Explain. assume hire waiter charge assigning forks philosophers. Nobody pick fork waiter says can. waiter global knowledge forks. Further, impose policy philosophers always request pick left fork requesting pick right fork, guarantee avoid deadlock. 6.8.3 [10] <§6.7> implement requests waiter either queue requests periodic retry request. queue, requests handled order received. problem using queue may always able service philosopher whose request head queue (due unavailability resources). Describe scenario five philosophers queue provided, service granted even though forks available another philosopher (whose request deeper queue) eat. 6.8.4 [10] <§6.7> implement requests waiter periodically repeating request resources becom e available, solve problem described Exercise 6.8.3 ? Explain. 6.9 Consider following three CPU organizations: CPU SS: two-core superscalar microprocessor provides out-of-order issue capabilities two function units (FUs). single thread run core time. CPU MT: fine-grained multithreaded processor allows instructions two threads run concurrently (i.e., two functional units), though instructions singl e thread issued cycle. CPU SMT: SMT processor allows instructions two threads run concurrently (i.e., two functional units), instructions either threads issue run cycle. Assume two threads X run CPUs 1126include following operations: Thread X Thread A1 – takes three cycles execute B1 – take two cycles execute A2 – dependences B2 – conflicts functional unit B1 A3 – conflicts functional unit A1B3 – depends result B2 A4 – depends result A3 B4 – dependences takes two cycle execute Assume instructions take single cycle execute unless noted otherwise encounter hazard. 6.9.1 [10] <§6.4> Assume one SS CPU. many cycles take execute two threads? many issue slots wasted due hazards? 6.9.2 [10] <§6.4> assume two SS CPUs. many cycles take execute two threads? many issue slots wasted due hazards? 6.9.3 [10] <§6.4> Assume one MT CPU. many cycles take execute two threads? many issue slots wasted due hazards? 6.9.4 [10] <§6.4> Assume one SMT CPU. many cycles take execute two threads? many issue slots wasted due hazards? 6.10 Virtualization software aggressively deployed reduce costs managing today’s high-performance servers. Companies like VMWare, Microsoft, IBM developed range virtualization products. general concept, described Chapter 5 , hypervisor layer introduced hardware operating system allow multiple operating systems share physical hardware. hypervisor layer responsible allocating CPU memory resources, well handling services typically handled operating system (e.g., I/O). Virtualization provides abstract view underlying hardware hosted operating system application software. require us rethink multi-core multiprocessor systems designed future sup port sharing CPUs memories number operating 1127systems concurrently. 6.10.1 [30] <§6.4> Select two hypervisors market today, compare contrast virtualize manage underlying hardware (CPUs memory). 6.10.2 [15] <§6.4> Discuss changes may necessary future multi-core CPU platforms order better match resource demands placed systems. instance, multithreading play effective role alleviating competition computing resources? 6.11 would like execute loop efficiently possible. two different machines, MIMD machine SIMD machine. (i=0; i<2000; i++) (j=0; j<3000; j++) X_array[i][j] = Y_array[j][i] + 200; 6.11.1 [10] <§6.3> four CPU MIMD machine, show sequence RISC-V instructions would execute CPU. speed-up MIMD machine? 6.11.2 [20] <§6.3> eight-wide SIMD machine (i.e., eight parallel SIMD functional units), write assembly program using SIMD extensions RISC-V execute loop. Compare number instructions executed SIMD machine MIMD machine. 6.12 systolic array example MISD machine. systolic array pipeline network “wavefront” data processing elements. elements need program counter since execution triggered arrival data. Clocked systolic arrays compute “lock-step” processor undertaking alternate compute communication phases. 6.12.1 [10] <§6.3> Consider proposed implementations systolic array (you find Internet technical publications). attempt program loop provided Exercise 6.11 using MISD model. Discuss difficulties encounter. 6.12.2 [10] <§6.3> Discuss similarities differences MISD SIMD machines. Answer question terms data-level parallelism. 6.13 Assume want execute DAXPY loop shown page 1128501 RISC-V vector assembly NVIDIA 8800 GTX GPU described chapter. problem, assume math operations performed single-precision floating-po int numbers (we rename loop SAXPY). Assume instructions take following number cycles execute. 6.13.1 [20] <§6.6> Describe constructs warps SAXPY loop exploit eight cores provided single multiprocessor. 6.14 Download CUDA Toolkit SDK https://developer.nvidia.com/cuda-toolkit . Make sure use “emurelease” (Emulation Mode) version code. (You need actual NVIDIA hardware assignment.) Build example programs provided SDK, confirm run emulator. 6.14.1 [90] <§6.6> Using “template” SDK sample starting point, write CUDA program perform following vector operations: 1) − b (vector-vector subtraction) 2) · b (vector dot product) dot product two vectors a=[a1, a2, …, an] b=[b1, b2, …, bn] defined as: Submit code program demonstrates operation verifies correctness results. 11296.14.2 [90] <§6.6> GPU hardware available, complete performance analysis program, examining computation time GPU CPU version program range vector sizes. Explain results see. 6.15 AMD recently announced integrating graphics processing unit x86 cores single package (though different clocks cores). example heterogeneous multiprocessor system. One key design points allow fast data communication CPU GPU. AMD’s Fusion architecture, communications needed discrete CPU GPU chips. Presently, plan use multiple (at least 16) PCI express channels facilitate intercommunication. 6.15.1 [25] <§6.6> Compare bandwidth latency associated two interconnect technologies. 6.16 Refer Figure 6.14b , shows n-cube interconnect topology order 3 interconnects eight nodes. One attractiv e feature n-cube interconnection network topology ability sustain broken links still provide connectivity . 6.16.1 [10] <§6.8> Develop equation computes many links n-cube (where n order cube) fail still guarantee unbroken link exist connect node n-cube. 6.16.2 [10] <§6.8> Compare resiliency failure n-cube fully connected interconnection network. Plot comparison reliability function added number links two topologies. 6.17 Benchmarking field study involves identifying representative workloads run specific computing platform order able objectively compare performance one system another. exercise compare two classes benchmarks: Whetstone CPU benchmark PARSEC Benchmark suite. Select one program PARSEC. programs freely available Internet. Consider running multiple copies Whetstone versus running PARSEC Benchmark systems described Section 6.11. 6.17.1 [60] <§6.10> inherently different two 1130classes workload run multi-core systems? 6.17.2 [60] <§6.10> terms Roofline Model, dependent results obtain running benchmarks amount sharing synchronization present workload used? 6.18 performing computations sparse matrices, latency memory hierarchy becomes much factor. Sparse matrices lack spatial locality datastream typically found matrix operations. result, new matrix representations proposed. One earliest sparse matrix representations Yale Sparse Matrix Format. stores initial sparse m×n matrix, row form using three one-dimensional arrays. Let R number nonzero entries M. construct array length R contains nonzero entries (in left-to-right top-to-bottom order). also construct second array IA length m+1 (i.e., one entry per row, plus one). IA(i) contains index first nonzero element row i. Row original matrix extends A(IA(i)) A(IA(i+1)−1). third array, JA, contains column index element A, also length R. 6.18.1 [15] <§6.10> Consider sparse matrix X write C code would store code Yale Sparse Matrix Format. Row 1 [1, 2, 0, 0, 0, 0] Row 2 [0, 0, 1, 1, 0, 0] Row 3 [0, 0, 0, 0, 9, 0] Row 4 [2, 0, 0, 0, 0, 2] Row 5 [0, 0, 3, 3, 0, 7] Row 6 [1, 3, 0, 0, 0, 1] 6.18.2 [10] <§6.10> terms storage space, assuming element matrix X single-precision floating point, compute amount storage used store matrix Yale Sparse Matrix Format. 6.18.3 [15] <§6.10> Perform matrix multiplication matrix X matrix shown below. [2, 4, 1, 99, 7, 2] Put computation loop, time execution. Make sure increase number times loop executed ge good resolution timing measurement. Compare 1131runtime using naïve representation matrix, Yale Sparse Matrix Format. 6.18.4 [15] <§6.10> find efficient sparse matrix representation (in terms space computational overhead)? 6.19 future systems, expect see heterogeneous computing platforms constructed heterogeneous CPUs. begun see appear embedded processing market systems contain floating-point DSPs microcontroller CPUs multichip module package. Assume three classes CPU: CPU A—A moderate-speed multi-core CPU (with floating- point unit) execute multiple instructions per cycle. CPU B—A fast single-core integer CPU (i.e., floating-point unit) execute single instruction per cycle. CPU C—A slow vector CPU (with floating-point capability) execute multiple copies instruction per cycle. Assume processors run following frequencies: CPU CPU B CPU C 1 GHz 3 GHz 250 MHz CPU execute two instructions per cycle, CPU B execute one instruction per cycle, CPU C execute eight instructions (through instruction) per cycle. Assume operations complete execution single cycle latency without hazards. three CPUs ability perform integer arithmetic, though CPU B cannot perform floating point arithmetic. CPU B instruction set similar RISC-V processor. CPU C perform floating point add subtract operations, well memory loads stores. Assume CPUs access shared memory synchronization zero cost. task hand compare two matrices X contain 1024 ×1024 floating-point elements. output count number indices value X larger equal value Y. 6.19.1 [10] <§6.11> Describe would partition problem three different CPUs obtain best performance. 6.19.2 [10] <§6.11> kind instruction would add vector CPU C obtain better performance? 6.20 question looks amount queuing occurring system given maximum transaction processing rate, 1132the latency observed average transaction. latency includes service time (which computed maximum rate) queue time. Assume quad-core computer system process database queries steady state maximum rate rate requests per second. Also assume transaction takes, average, lat ms process. pairs table, answer following questions: Average Transaction Latency Maximum transaction processing rate 1 ms 5000/sec 2 ms 5000/sec 1 ms 10,000/sec 2 ms 10,000/sec pairs table, answer following questions: 6.20.1 [10] <§6.11> average, many requests processed given instant? 6.20.2 [10] <§6.11> move eight-core system, ideally, happen system throughput (i.e., many queries/second computer process)? 6.20.3 [10] <§6.11> Discuss rarely obtain kind speed-up simply increasing number cores. Answers Check §6.1, page 494: False. Task-level parallelism help sequential applications sequential applications made run parallel hardware, although challenging. §6.2, page 499: False. Weak scaling compensate serial portion program would otherwise limit scalability, bu strong scaling. §6.3, page 504: True, missing useful vector features like gather-scatter vector length registers improve efficiency vector architectures. (As elaboration sec tion mentions, AVX2 SIMD extensions offers indexed loads via gather operation scatter indexed stores. Haswell generation x86 microprocessor first support AVX2.) §6.4, page 509: 1. True. 2. True. 1133§6.5, page 513: False. Since shared address physical address, multiple tasks virtual address spaces run well shared memory multiprocessor. §6.6, page 521: False. Graphics DRAM chips prized higher bandwidth. §6.7, page 526: 1. False. Sending receiving message implicit synchronization, well way share data. 2. True. §6.8, page 528: True. §6.10, page 540: True. likely need innovation levels hardware software stack parallel computing succeed. 1134Appendix OUTLINE Appendix Basics Logic Design Appendix B Graphics Computing GPUs Appendix C Mapping Control Hardware Appendix Survey RISC Architectures Desktop, Serve r, Embedded Computers 1135APPENDIX Basics Logic Design always loved word, Boolean. Claude Shannon IEEE Spectrum, April 1992 (Shannon’s master’s thesis showed algebra invented George Boole th e 1800s could represent workings electrical switches.) A.1 Introduction A-3 A.2 Gates, Truth Tables, Logic Equations A-4 A.3 Combinational Logic A-9 A.4 Using Hardware Description Language A-20 A.5 Constructing Basic Arithmetic Logic Unit A-26 A.6 Faster Addition: Carry Lookahead A-37 A.7 Clocks A-47 A.8 Memory Elements: Flip-Flops, Latches, Registers A-49 A.9 Memory Elements: SRAMs DRAMs A-57 A.10 Finite-State Machines A-66 A.11 Timing Methodologies A-71 A.12 Field Programmable Devices A-77 A.13 Concluding Remarks A-78 A.14 Exercises A-79 A.1 Introduction appendix provides brief discussion basics logic design. replace course logic design, enable design significant working logic systems. hav e 1136little exposure logic design, however, appendix wil l provide sufficient background understand material th book. addition, looking understand motivation behind computers implemented, material serve useful introduction. curiosity aroused sated appendix, references end provide sever al additional sources information. Section A.2 introduces basic building blocks logic, namely, gates . Section A.3 uses building blocks construct simple combinational logic systems, contain memory. exposure logic digital systems, probably b e familiar material first two sections. Section A.5 shows use concepts Sections A.2 A.3 design ALU RISC-V processor. Section A.6 shows make fast adder, may safely skipped interested topic. Section A.7 short introduction topic clocking, necessary discuss memory elements work. Section A.8 introduces memory elements, Section A.9 extends focus random access memories; describes characteristics important understanding used, discussed Chapter 4 , background motivates many aspects memory hierarchy design discussed Chapter 5 . Section A.10 describes design use finite-state machines, sequential logic blocks. intend read Appendix C , thoroughly understand material Sections A.2 A.10 . intend read material control Chapter 4 , skim appendices; however, familiarity material except Section A.11 . Section A.11 intended want deeper understanding clocking methodologies timing. explains basics edge-triggered clocking works, introduces another clocking scheme, briefly describes problem synchronizing asynchronous inputs. Throughout appendix, appropriate, also include segments demonstrate logic represented n Verilog, introduce Section A.4 . extensive complete Verilog tutorial available online Companion Web site book. 1137A.2 Gates, Truth Tables, Logic Equations electronics inside modern computer digital . Digital electronics operate two voltage levels interest: h igh voltage low voltage. voltage values temporary occur transitioning values. (As discuss later section, possible pitfall digital design samplin g signal clearly either high low.) fact computers digital also key reason use binary numbers, since binary system matches underlying abstraction inherent electronics. various logic families, values relationships two voltage values differ. Thus, rather refer voltage levels, talk signals (logically) true, 1, asserted ; signals (logically) false, 0, deasserted . values 0 1 called complements inverses one another. asserted signal signal (logically) true, 1. deasserted signal signal (logically) false, 0. Logic blocks categorized one two types, depending whether contain memory. Blocks without memory called combinational ; output combinational block depends current input. blocks memory, outputs depend inputs value stored memory, called state logic block. section next, focus combinational logic . introducing different memory elements Section A.8 , describe sequential logic , logic including state, designed. combinational logic logic system whose blocks contain memory hence compute output given input. 1138 sequential logic group logic elements contain memory hence whose value depends inputs well current contents memory. Truth Tables combinational logic block contains memory, completely specified defining values outputs fo r possible set input values. description normally given truth table . logic block n inputs, 2n entries truth table, since many possible combinations input values. entry specifies value outputs particular input combination. Truth Tables Example Consider logic function three inputs, A, B, C, three outputs, D, E, F. function defined follows: true least one input true, E true exactly two inputs true, F true three inputs true. Show truth table function. Answer truth table contain 23 = 8 entries. is: 1139Truth tables completely describe combinational logic function; however, grow size quickly may easy understand. Sometimes want construct logic function 1140that 0 many input combinations, use shorthand specifying truth table entries nonzero outpu ts. approach used Chapter 4 Appendix C . Boolean Algebra Another approach express logic function logic equations. done use Boolean algebra (named Boole, 19th-century mathematician). Boolean algebra, variables values 0 1 and, typical formulations, three operators: operator written +, A+B. result operator 1 either variables 1. operation also called logical sum , since result 1 either operand 1. operator written · , · B. result operator 1 inputs 1. operator also called logical product , since result 1 operands 1. unary operator written . result operator 1 input 0. Applying operator logical value results inversion negation value (i.e., input 0 output 1, vice versa). several laws Boolean algebra helpful manipulating logic equations. Identity law: A+0=A A·1=A Zero One laws: A+1=1 A·0=0 Inverse laws: Commutative laws: A+B=B+A A·B=B·A Associative laws: A+(B+C)=(A+B)+C A·(B·C)=(A·B)·C Distributive laws: A·(B+C)=(A·B)+(A·C) A+(B·C)=(A+B)·(A+C) addition, two useful theorems, called DeMorgan’s laws, discussed depth exercises. set logic functions written series equations output left-hand side equation formula consisting variables three operators right - hand side. Logic Equations 1141Example Show logic equations logic functions, D, E, F, described previous example. Answer Here’s equation D: F equally simple: E little tricky. Think two parts: must true E true (two three inputs must true), cannot true (all three cannot true). Thus write E also derive E realizing E true exactly two inputs true. write E three possible terms two true inputs one false input : Proving two expressions equivalent explored exercises. Verilog, describe combinational logic whenever possible using assign statement, described beginning page A-23. write definition E using Verilog exclusive- operator assign E = (A & (B ^ C)) | (B & C & ~A) , yet another way describe function. F even simpler representations, like correspondin g C code: assign = | B | C assign F = & B & C . 1142Gates Logic blocks built gates implement basic logic functions. example, gate implements function, gate implements function. Since commutative associative, gate multiple inputs, output equal inputs. logical function implemented inverter always single input. standard representation three logic building blocks shown n Figure A.2.1 . gate device implements basic logic functions, OR. FIGURE A.2.1 Standard drawing gate, gate, inverter, shown left right. signals left symbol inputs, output appears right. gates two inputs. Inverters single input. Rather draw inverters explicitly, common practice add “bubbles” inputs outputs gate cause logic value input line output line inverted. example , Figure A.2.2 shows logic diagram function , using explicit inverters left bubbled inputs outputs n right. FIGURE A.2.2 Logic gate implementation 1143using explicit inverts left bubbled inputs outputs right. logic function simplified Verilog, & ~ B . logical function constructed using gates, gates, inversion; several exercises give opportunity try implementing common logic functions gates. next section, we’ll see implementation logic function constructed using knowledge. fact, logic functions constructed single gate type, gate inverting. two common inverting gates called NAND correspond inverted gates, respectively. NAND gates called universal , since logic function built using one gate type. exercises explore concept further. gate inverted gate. NAND gate inverted gate. Check following two logical expressions equivalent? not, f ind setting variables show not: A.3 Combinational Logic section, look couple larger logic building blocks use heavily, discuss design structured logic automatically implemented logic equation truth table translation program. Last, discuss notion 1144an array logic blocks. Decoders One logic block use building larger components decoder . common type decoder n-bit input 2n outputs, one output asserted input combination. decoder translates n-bit input signal corresponds binary value n-bit input. outputs thus usually numbered, say, Out0, Out1, …, Out2n −1. value input i, true outputs false. Figure A.3.1 shows 3-bit decoder truth table. decoder called 3-to-8 decoder since three inputs eight (23) outputs. also logic element called encoder performs inverse function decoder, taking 2n inputs producing n-bit output. decoder logic block n-bit input 2 n outputs, one output asserted input combination. FIGURE A.3.1 3-bit decoder three inputs, called 12, 11, 10, 23 = 8 outputs, called Out0 Out7. output corresponding binary value input true, shown truth table. la bel 3 input decoder says input signal 3 bits wide. 1145Multiplexors One basic logic function use quite often Chapter 4 multiplexor . multiplexor might properly called selector , since output one inputs selected contro l. Consider two-input multiplexor. left side Figure A.3.2 shows multiplexor three inputs: two data values selector (or control ) value . selector value determines inputs becomes output. represent logic functi computed two-input multiplexor, shown gate form right side Figure A.3.2 , . selector value Also called control value . control signal used select one input values multiplexor output multiplexor. FIGURE A.3.2 two-input multiplexor left implementation gates right. multiplexor two data inputs ( B), labeled 0 1, one selector input ( S), well output C. Implementing multiplexors Verilog requires little work, especially wider two inputs. show beginning page A-23. Multiplexors created arbitrary number data inputs. two inputs, selector single signal selects one inputs true (1) false (0). n data inputs, need 1146selector inputs. case, multiplexor basically consist three parts: 1. decoder generates n signals, indicating different input value 2. array n gates, combining one inputs signal decoder 3. single large gate incorporates outputs gates associate inputs selector values, often label data inputs numerically (i.e., 0, 1, 2, 3, …, n −1) interpret data selector inputs binary number. Sometimes, make use multiplexor undecoded selector signals. Multiplexors easily represented combinationally Verilo g using expressions. larger multiplexors, case statements convenient, care must taken synthesize combinational logic. Two-Level Logic PLAs pointed previous section, logic function implemented AND, OR, functions. fact, much stronger result true. logic function written canonical form, every input either true complemented variable two levels gates—one OR—with possible inversion final output. representation called two-level representation , two forms, called sum products product sums . sum- of-products representation logical sum (OR) products (t erms using operator); product sums opposite. earlier example, two equations output E: second equation sum-of-products form: two 1147levels logic inversions individual variables. first equation three levels logic. sum products form logical representation employs logical sum (OR) products (terms joined using operator). Elaboration also write E product sums: derive form, need use DeMorgan’s theorems , discussed exercises. text, use sum-of-products form. easy see logic function represented sum products constructing representation truth table function. truth table entry function true corresponds product term. product term consists logical product inputs complements input s, depending whether entry truth table 0 1 corresponding variable. logic function logical sum product terms function true. easi ly seen example. Sum Products Example Show sum-of-products representation following ruth table D. 11481149Answer four product terms, since function true (1) f different input combinations. are: Thus, write function sum terms: Note truth table entries function true generate terms equation. use relationship truth table two- level representation generate gate-level implementation set logic functions. set logic functions corresponds truth table multiple output columns, saw example page A-5. output column represents different logic functio n, may directly constructed truth table. sum-of-products representation corresponds common structured-logic implementation called programmable logic array (PLA) . PLA set inputs corresponding input complements (which implemented set inverters) , two stages logic. first stage array gates form set product terms (sometimes called minterms ); product term consist inputs complement s. second stage array gates, forms logical sum number product terms. Figure A.3.3 shows basic form PLA. programmable logic array (PLA) structured-logic element composed set inputs 1150corresponding input complements two stages logic: fi rst generates product terms inputs input complements, second generates sum terms product terms. Hence, PLAs implement logic functions sum products. minterms Also called product terms . set logic inputs joined conjunction (AND operations); product terms form firs logic stage programmable logic array (PLA). FIGURE A.3.3 basic form PLA consists array gates followed array gates. entry gate array product term consisting number inputs inverted inputs. entry gate array sum term consisting number product terms. PLA directly implement truth table set logic functions multiple inputs outputs. Since entry wher e 1151the output true requires product term, corresponding row PLA. output corresponds potential row gates second stage. number gates corresponds number truth table entries output true. total size PLA, shown Figure A.3.3 , equal sum size gate array (called plane ) size gate array (called plane ). Looking Figure A.3.3 , see size gate array equal number inputs times number different product terms, size gate array number outputs times number product terms. PLA two characteristics help make efficient way implement set logic functions. First, truth table entries produce true value least one output logic gates associated them. Second, different product term one entry PLA, even product term used multiple outputs. Let’s look example. PLAs Example Consider set logic functions defined example pag e A-5. Show PLA implementation example D, E, F. Answer truth table constructed earlier: 1152Since seven unique product terms least one true value output section, seven columns plane. number rows plane three (since three inputs), also three rows 1153plane (since three outputs). Figure A.3.4 shows resulting PLA, product terms corresponding tr uth table entries top bottom. FIGURE A.3.4 PLA implementing logic function described example. Rather drawing gates, Figure A.3.4 , designers often show position gates gates. Dots used intersection product term signal line input line output line corresponding gate gate required. Figure A.3.5 shows PLA Figure A.3.4 would look drawn way. contents PLA fixed PLA created, although also forms PLA-like structures, called PLAs , programmed electronically designer ready use them. 1154FIGURE A.3.5 PLA drawn using dots indicate components product terms sum terms array. Rather use inverters gates, usually inputs run width plane tr ue complement forms. dot plane indicates input, inverse, occurs product term. dot plane indicates corresponding product term appears corresponding output. ROMs Another form structured logic used implement set logic functions read-only memory (ROM) . ROM called memory set locations read; however, contents locations fixed, usually time ROM manufactured. also programmable ROMs (PROMs) programmed electronically, designer knows contents. also erasable PROMs; devices require slow erasure process using ultraviolet light, thus used read-only memories, except design 1155and debugging process. read-only memory (ROM) memory whose contents designated creation time, contents read. ROM used structured logic implement set logic functions using terms logic functions address inputs outputs bits word memory. programmable ROM (PROM) form read-only memory programmed designer knows contents. ROM set input address lines set outputs. number addressable entries ROM determines number address lines: ROM contains 2m addressable entries, called height , input lines. number bits addressable entry equal number output bits sometimes called width ROM. total number bits ROM equal height times width. height width sometimes collectively referred shape ROM. ROM encode collection logic functions directly truth table. example, n functions inputs, need ROM address lines (and 2m entries), entry n bits wide. entries input portion truth table represent addresses entries ROM, contents output portion truth table constitute th e contents ROM. truth table organized sequence entries input portion constitutes sequen ce binary numbers (as truth tables shown far), output portion gives ROM contents order well. example starting page A-13, three inputs three outputs. leads ROM 23 = 8 entries, 3 bits wide. contents entries increasing order addre ss directly given output portion truth table appears page A-14. ROMs PLAs closely related. ROM fully decoded: 1156contains full output word every possible input combinatio n. PLA partially decoded. means ROM always contain entries. earlier truth table page A-14, ROM contains entries eight possible inputs, whereas PL contains seven active product terms. number inputs grows, number entries ROM grows exponentially. contrast, real logic functions, num ber product terms grows much slowly (see examples Appendix C ). difference makes PLAs generally efficient implementing combinational logic functions. ROM advantage able implement logic function matching number inputs outputs. advantage makes easier change ROM contents logic function changes, since size ROM need change. addition ROMs PLAs, modern logic synthesis systems also translate small blocks combinational logic collection gates placed wired automatically. Although small collections gates usually area- efficient, small logic functions less overhead rigid structure ROM PLA preferred. designing logic outside custom semicustom integrate circuit, common choice field programming device; describe devices Section A.12 . Don’t Cares Often implementing combinational logic, situations care value output is, either another output true subset input combinations determines values outputs. situations referred don’t cares . Don’t cares important make easier optimize implementation logic function. two types don’t cares: output don’t cares input don’t cares, represented truth table. Output don’t cares arise don’t care value output input combination. appear Xs output portion truth table. output don’t care 1157some input combination, designer logic optimization program free make output true false input combination. Input don’t cares arise output depends inputs, also shown Xs, though input portion truth table. Don’t Cares Example Consider logic function inputs A, B, C defined follows: C true, output true, whatever value B. B true, output E true, whatever value C. Output F true exactly one inputs true, although don’t care value F, whenever E true. Show full truth table function truth table using don’t cares. many product terms required PLA these? Answer Here’s full truth table, without don’t cares: 1158This requires seven product terms without optimization. truth table written output don’t cares looks like this: 1159If also use input don’t cares, truth table simplified yield following: 1160This simplified truth table requires PLA four minterms, r implemented discrete gates one two-input gate three gates (two three inputs one two inputs). compares original truth table seven minterms would required four gates. Logic minimization critical achieving efficient implementations. One tool useful hand minimization random logic Karnaugh maps . Karnaugh maps represent truth table graphically, product terms may combined easily seen. Nevertheless, hand optimization significant logic functi ons using Karnaugh maps impractical, size maps complexity. Fortunately, process logic minimization highly mechanical performed design 1161tools. process minimization, tools take advantage don’t cares, specifying important. textbook references end appendix provide discussi logic minimization, Karnaugh maps, theory behind minimization algorithms. Arrays Logic Elements Many combinational operations performed data done entire word (64 bits) data. Thus often want build array logic elements, represent simply showing given operation happen entire collection inputs. Inside machine, much time want select pair buses . bus collection data lines treated together single logical signal. (The term bus also used indicate shared collection lines multiple sour ces uses.) bus logic design, collection data lines treated together single logical signal; also, shared collection lines multip le sources uses. example, RISC-V instruction set, result instruction written register come one tw sources. multiplexor used choose two buses (each 64 bits wide) written Result register. 1-b multiplexor, showed earlier, need replicated 64 times. indicate signal bus rather single 1-bit line showing thicker line figure. buses 64 bits wide; explicitly labeled width. show logic unit whose inputs outputs buses, means unit must replicated sufficient number times accommodate width input. Figure A.3.6 shows draw multiplexor selects pair 64-bit buses expands terms 1-bit-wide multiplexors. Sometimes need construct array logic elements inputs elements array outputs earlier 1162elements. example, multibit-wide ALU constructed. cases, must explicitly show create wider arrays, since individual elements array longer independent, case 64-bit-wide multiplexor. Check Parity function output depends number 1s input. even parity function, output 1 input even number ones. Suppose ROM used implement even parity function 4-bit input. A, B, C, represents contents ROM? 116311641165FIGURE A.3.6 multiplexor arrayed 64 times perform selection two 64-bit inputs. Note still one data selection signal used 64 1-bit multiplexors. A.4 Using Hardware Description Language Today digital design processors related hardware systems done using hardware description language . language serves two purposes. First, provides abstract description hardware simulate debug design. Second, use logic synthesis hardware compilation tools, description compiled hardware implementation. 1166 hardware description language programming language describing hardware, used generating simulations hardware design also input synthesis tools generate actual hardware. section, introduce hardware description language Verilog show used combinational design. rest appendix, expand use Verilog include design sequential logic. optional sections Chapter 4 appear online, use Verilog describe processor implementations. optional section Chapter 5 appears online, use system Verilog describe cache controlle r implementations. System Verilog adds structures useful features Verilog. Verilog one two primary hardware description languages; VHDL . Verilog somewhat heavily used industry based C, opposed VHDL, based Ada. reader generally familiar C find basics Verilog, use appendix, easy follow. Readers already familiar VHDL find concepts simple, provided exposed syntax C. Verilog One two common hardware description languages. VHDL One two common hardware description languages. Verilog specify behavioral structural definition digital system. behavioral specification describes digital system functionally operates. structural specification describes detailed organization digital system, usually using hierarchical description. structural specification b e used describe hardware system terms hierarchy basic elements gates switches. Thus, could use Verilog describe exact contents truth tables datapath last section. 1167 behavioral specification Describes digital system operates functionally. structural specification Describes digital system organized terms hierarchical connection elements. arrival hardware synthesis tools , designers use Verilog VHDL structurally describe datapath, relying logic synthesis generate control behavioral description. addition, CAD systems provide extensive libraries standardized parts, ALUs, multiplexors, register files, memories, programmable logic blocks, well basic gates. hardware synthesis tools Computer-aided design software generate gate-level design based behavioral descriptions digital system. Obtaining acceptable result using libraries logic synthesi requires specification written eye toward eventual synthesis desired outcome. simple designs, primarily means making clear expect implemented combinational logic expect require sequential logic. examples use section remainder appendix, written Verilog eventual synthesis mind. Datatypes Operators Verilog two primary datatypes Verilog: 1. wire specifies combinational signal. 2. reg (register) holds value, vary time. reg need necessarily correspond actual register implementation, although often will. wire 1168In Verilog, specifies combinational signal. reg Verilog, register. register wire, named X, 64 bits wide declared array: reg [63:0] X wire [63:0] X , also sets index 0 designate least significant bit register. often want access subfield register wire, refer contiguous set bits register wire notation [starting bit: ending bit ], indices must constant values. array registers used structure like register file r memory. Thus, declaration reg [63:0] registerfile[0:31] specifies variable registerfile equivalent RISC-V registerfile, register 0 first. accessing array, refer single element, C, using notation registerfile[regnum] . possible values register wire Verilog 0 1, representing logical false true X, representing unknown, initial value given registers wire connected something Z, representing high-impedance state tristate gates, discuss appendix Constant values specified decimal numbers well binary, octal, hexadecimal. often want say exactly large constant field bits. done prefixing valu e decimal number specifying size bits. example: 4’b0100 specifies 4-bit binary constant value 4, 4’d4 . −8’h4 specifies 8-bit constant value −4 (in two’s complement representation) Values also concatenated placing within { } separated commas. notation {x{bitfield}} replicates bitfield x times. example: {32{2’b01}} creates 64-bit value pattern 0101 … 01. {A[31:16],B[15:0]} creates value whose upper 16 bits come 1169from whose lower 16 bits come B. Verilog provides full set unary binary operators C, including arithmetic operators (+, −, *. /), logical operators (&, |, ~), comparison operators (==, !=, >, <, <=, >=), shift operators (<<, >>), C’s conditional operator (?, used form condition ? expr1 :expr2 returns expr1 condition true expr2 false). Verilog adds set unary logic reduction operators (&, |, ^) yield single bit applying logical operator bits operand. example, &A returns value obtained ANDing bits together, ^A returns reduction obtained using exclusive bits A. Check following define exactly value? 1. 8’bimoooo 2. 8’hF0 3. 8’d240 4. {{4{1’b1}},{4{1’b0}}} 5. {4’b1,4’b0) Structure Verilog Program Verilog program structured set modules, may represent anything collection logic gates complete system. Modules similar classes C++, although nearly powerful. module specifies input output ports, describe incoming outgoing connections module. module may also declare additional variables. body module consists of: initial constructs, initialize reg variables Continuous assignments, define combinational logic always constructs, define either sequential combinational logic Instances modules, used implement module defined Representing Complex Combinational Logic 1170in Verilog continuous assignment, indicated keyword assign , acts like combinational logic function: output continuously assigned value, change input values reflected immediately output value. Wires may assigned values continuous assignments. Using continuous assignments, define module implements half-adder, Figure A.4.1 shows. FIGURE A.4.1 Verilog module defines half- adder using continuous assignments. Assign statements one sure way write Verilog generates combinational logic. complex structures, however, assign statements may awkward tedious use. also possible use always block module describe combinational logic element, although care must taken. Using always block allows inclusion Verilog control constructs, suc h if-then-else, case statements, statements, repeat statements, used. statements similar C small changes. always block specifies optional list signals block sensitive (in list starting @). always block re- evaluated listed signals changes value; list omitted, always block constantly re-evaluated. always block specifying combinational logic, sensitivity list include input signals. multiple Verilog statements executed always block, surrounded keywords begin end, take place { } C. always block thus looks like this: always @(list signals cause reevaluation) b egin Verilog statements including assignments 1171control statements end sensitivity list list signals specifies always block re- evaluated. Reg variables may assigned inside always block, using procedural assignment statement (as distinguished continuous assignment saw earlier). are, however, two different types procedural assignments. assignment operat =executes C; right-hand side evaluated, left-hand side assigned value. Furthermore, executes like normal C assignment statement: is, completed next statement executed. Hence, assignment operator =has name blocking assignment . blocking useful generation sequential logic, return shortly. form assignment ( nonblocking ) indicated <=. nonblocking assignment, right-hand sides assignments always group evaluated assignments done simultaneously. first example combinational logic implemented using always block, Figure A.4.2 shows implementation 4-to-1 multiplexor, uses case construct make easy write. case construct looks like C switch statement. Figure A.4.3 shows definition RISC-V ALU, also uses case statement. blocking assignment Verilog, assignment completes execution next statement. nonblocking assignment assignment continues evaluating right-hand side, assigning left-hand side value right-hand side evaluated. 1172FIGURE A.4.2 Verilog definition 4-to-1 multiplexor 64-bit inputs, using case statement. case statement acts like C switch statement, except Verilog code associated selected case executed (as case state break end) fall-through next statement. 1173FIGURE A.4.3 Verilog behavioral definition RISC-V ALU. could synthesized using module library containing basic arithmetic logical operations. Since reg variables may assigned inside always blocks, want describe combinational logic using always block, care must taken ensure reg synthesize register. variety pitfalls described elaborati below. Elaboration Continuous assignment statements always yield combinational logic, Verilog structures, even always blocks, yield unexpected results logic synthesis. co mmon problem creating sequential logic implying existence latch register, results implementation slower costly perhaps intended. ensure logic intend combinational synthesized way, make sure following: 1. Place combinational logic continuous assignment always block. 2. Make sure signals used inputs appear sensitivity list always block. 11743. Ensure every path always block assigns value exact set bits. last easiest overlook; read example Figure A.5.15 convince property adhered to. Check Assuming values initially zero, values B executing Verilog code inside always block? C = 1; <= C; B = C; A.5 Constructing Basic Arithmetic Logic Unit ALU n. [ Arthritic Logic Unit (rare) Arithmetic Logic Unit] random-number generator supplied standard computer systems. Stan Kelly-Bootle, Devil’s DP Dictionary, 1981 arithmetic logic unit (ALU) brawn computer, device per-forms arithmetic operations like addition subtraction logical operations like OR. section constructs ALU four hardware building blocks (AND gates, inverters, multiplexors) illustrates combinational logic works. next section, see addition sped clever designs. RISC-V registers 64 bits wide, need 64-bit- wide ALU. Let’s assume connect 64 1-bit ALUs create desired ALU. We’ll therefore start constructing 1-b ALU. 1-Bit ALU logical operations easiest, map directly onto hardware components Figure A.2.1 . 1175The 1-bit logical unit looks like Figure A.5.1 . multiplexor right selects b b, depending whether value Operation 0 1. line controls multiplexor shown color distinguish lines containing data. Notice renamed control output lines multiplexor give names reflect th e function ALU. FIGURE A.5.1 1-bit logical unit OR. next function include addition. adder must two inputs operands single-bit output sum. must second output pass carry, called CarryOut . Since CarryOut neighbor adder must included input, need third input. input called CarryIn . Figure A.5.2 shows inputs outputs 1-bit adder. Since know addition supposed do, specify outputs “black box” based inputs, Figure A.5.3 demonstrates. 1176FIGURE A.5.2 1-bit adder. adder called full adder; also called (3 ,2) adder three inputs two outputs. adder b inputs called (2,2) adder half-adder. 1177FIGURE A.5.3 Input output specification 1-bit adder. express output functions CarryOut Sum logical equations, equations turn implemented logic gates. Let’s CarryOut. Figure A.5.4 shows values inputs CarryOut 1. FIGURE A.5.4 Values inputs CarryOut 1. turn truth table logical equation: · b · CarryIn true, three terms must also true, leave last term corresponding fourth line table. thus simplify equation Figure A.5.5 shows hardware within adder black box CarryOut consists three gates one gate. three gates correspond exactly three parenthesized terms formula CarryOut, gate sums three terms. 1178FIGURE A.5.5 Adder hardware CarryOut signal. rest adder hardware logic Su output given equation page. Sum bit set exactly one input 1 three inputs 1. Sum results complex Boolean equation (recall means a): drawing logic Sum bit adder black box left exercise reader. Figure A.5.6 shows 1-bit ALU derived combining adder earlier components. Sometimes designers also want ALU perform simple operations, generating 0. easiest way add operation expand multiplexor 1179controlled Operation line and, example, connect 0 directly new input expanded multiplexor. FIGURE A.5.6 1-bit ALU performs AND, OR, addition (see Figure A.5.5 ). 64-Bit ALU completed 1-bit ALU, full 64-bit ALU created connecting adjacent “black boxes.” Using xi mean ith bit x, Figure A.5.7 shows 64-bit ALU. single stone cause ripples radiate shores quiet lake, single carry least significant bit (Result0) ripple way adder, causing carry significant bit 1180(Result63). Hence, adder created directly linking carries 1-bit adders called ripple carry adder. We’ll see faster way connect 1-bit adders starting page A-38. 1181FIGURE A.5.7 64-bit ALU constructed 64 1- bit ALUs. 1182CarryOut less significant bit connected CarryIn significant bit. organization called ripple carry. Subtraction adding negative version operand, adders perform subtraction. Recall shortcut negating two’s complement number invert bit (sometimes called one’s complement ) add 1. invert bit, simply add 2:1 multiplexor chooses b , Figure A.5.8 shows. FIGURE A.5.8 1-bit ALU performs AND, OR, addition b . selecting (Binvert=1) setting CarryIn 1 least significant bit ALU, get two’s comple-ment subtraction b instead addition b a. Suppose connect 64 1-bit ALUs, Figure A.5.7 . added multiplexor gives option b inverted 1183value, depending Binvert, one step negating two’s complement number. Notice least significant bit sti CarryIn signal, even though it’s unnecessary addition. happens set CarryIn 1 instead 0? adder calculate a+b+1. selecting inverted version b, get exactly want: simplicity hardware design two’s complement adder helps explain two’s complement representation become universal standard integer computer arithmetic. also wish add function. Instead adding separate gate NOR, reuse much hardware already ALU, like subtract. insight comes following truth NOR: is, (a b) equivalent b. fact called DeMorgan’s theorem explored exercises depth. Since b, need add ALU. Figure A.5.9 shows change. 1184FIGURE A.5.9 1-bit ALU performs AND, OR, addition b . selecting (Ainvert=1) (Binvert=1), get b instead b. Tailoring 64-Bit ALU RISC-V four operations—add, subtract, AND, OR—are found ALU almost every computer, operations RISC-V instructions performed ALU. design ALU incomplete. One instruction still needs support set less instruction ( slt). Recall operation produces 1 rs1 < rs2, 0 otherwise. Consequently, slt set least significant bit 0, least significant bit set according comparison. ALU perform slt, first need expand three-input multiplexor Figure A.5.9 add input slt result. call new input Less use slt. top drawing Figure A.5.10 shows new 1-bit ALU 1185the expanded multiplexor. description slt above, must connect 0 Less input upper 63 bits ALU, since bits always set 0. remains consider compare set least significant bit set less instructions. 11861187FIGURE A.5.10 (Top) 1-bit ALU performs AND, OR, addition b , (bottom) 1-bit ALU significant bit. top drawing includes direct input connected perform set less operation (see Figure A.5.11 ); bottom direct output adder less comparison called Set. (See Exercise A.24 end appendix see calculate overflow fewer inputs.) happens subtract b a? difference negative, < b since want least significant bit set less operation 1 < b; is, 1 − b negative 0 it’s positive. desired result corresponds exactly sign bit values: 1 means negative 0 means positive. Following line argument, need connect sign bit adder output least significant bit get set less than. (Alas, argument holds subtraction overflow; explore complete implementation exercises.) Unfortunately, Result output significant ALU bit top Figure A.5.10 slt operation output adder; ALU output slt operation obviously input value Less. Thus, need new 1-bit ALU significant bit extra output bit: adder output. bottom drawing Figure A.5.10 shows design, new adder output line called Set. long need special ALU significant bit, added overflow detection logic since also associat ed bit. Figure A.5.11 shows 64-bit ALU. 1188FIGURE A.5.11 64-bit ALU constructed 118963 copies 1-bit ALU top Figure A.5.10 one 1-bit ALU bottom figure. Less inputs connected 0 except least significant bit, connected Set output significant bit. ALU performs a−b select input 3 multiplexor Figure A.5.10 , Result=0 … 001 a<b, Result=0 … 000 otherwise. Notice every time want ALU subtract, set CarryIn Binvert 1. adds logical operations, want control lines 0. therefore simplify control ALU combining CarryIn Binvert single control line called Bnegate . tailor ALU RISC-V instruction set, must support conditional branch instructions Branch Equal (beq), branches two registers equal. easiest way test equality ALU subtract b test see result 0, since Thus, add hardware test result 0, test equality. simplest way outputs together send signal inverter: Figure A.5.12 shows revised 64-bit ALU. think combination 1-bit Ainvert line, 1-bit Bnegate line, 2-bit Operation lines 4-bit control lines ALU, telling perform add, subtract, AND, OR, NOR, set less than. Figure A.5.13 shows ALU control lines corresponding ALU operation. 1190FIGURE A.5.12 final 64-bit ALU. adds Zero detector Figure A.5.11 . 1191FIGURE A.5.13 values three ALU control lines, Ainvert, Bnegate, Operation, corresponding ALU operations. Finally, seen inside 64-bit ALU, use universal symbol complete ALU, shown Figure A.5.14 . 1192FIGURE A.5.14 symbol commonly used represent ALU, shown Figure A.5.12 . symbol also used represent adder, normally labeled either ALU Adder. Defining RISC-V ALU Verilog 1193Figure A.5.15 shows combinational RISC-V ALU might specified Verilog; specification would probably compiled using standard parts library provided adder, could instantiated. completeness, show ALU control RISC-V Figure A.5.16 , used Chapter 4 , build Verilog version RISC-V datapath. FIGURE A.5.15 Verilog behavioral definition RISC-V ALU. 1194FIGURE A.5.16 RISC-V ALU control: simple piece combinational control logic. next question is, “How quickly ALU add two 64-bit operands?” determine b inputs, CarryIn input depends operation adjacent 1-bit adder. trace way chain dependencies, connect significant bit least significant bit, significant bit sum must wait sequential evaluation 64 1-bit adders. sequential chain reaction slow used time-critical hardware. next section explores speed-up addition. topic crucial understanding th e rest appendix may skipped. Check Suppose wanted add operation (a b), called NAND. could ALU change support it? 1195 1. change. calculate NAND quickly using current ALU since already a, b, OR. 2. must expand big multiplexor add another input, add new logic calculate NAND. A.6 Faster Addition: Carry Lookahead key speeding addition determining carry high-order bits sooner. variety schemes anticipat e carry worst-case scenario function log2 number bits adder. anticipatory signals faste r go fewer gates sequence, takes many gates anticipate proper carry. key understanding fast-carry schemes remember that, unlike software, hardware executes parallel whenever inputs change. Fast Carry Using “Infinite” Hardware mentioned earlier, equation represented two levels logic. Since external inputs two operand CarryIn least significant bit adder, theory w e could calculate CarryIn values remaining bits adder two levels logic. example, CarryIn bit 2 adder exactly CarryOut bit 1, formula Similarly, CarryIn1 defined Using shorter traditional abbreviation c CarryIn i, rewrite formulas 1196Substituting definition c1 first equation resul ts formula: imagine equation expands get higher bits adder; grows rapidly number bits. complexity reflected cost hardware fast carry , making simple scheme prohibitively expensive wide adders. Fast Carry Using First Level Abstraction: Propagate Generate fast-carry schemes limit complexity equations simplify hardware, still making substantial speed improvements ripple carry. One scheme carry- lookahead adder . Chapter 1 , said computer systems cope complexity using levels abstraction. carry-lookahead adder relies levels abstraction implementation. Let’s factor original equation first step: rewrite equation c2 using formula, would see repeated patterns: Note repeated appearance (a · bi) (a + bi) formula above. two important factors traditionally called generate (gi) propagate (pi): 1197Using define ci+1, get see signals get names, suppose g 1. is, adder generates CarryOut (ci+1) independent value CarryIn (c i). suppose g 0 p 1. is, adder propagate CarryIn CarryOut. Putting two together, CarryIni+1 1 either g 1 p 1 CarryIn 1. analogy, imagine row dominoes set edge. end domino tipped pushing one far away, provided gaps two. Similarly, carry made true generate far away, provided propagates true. Relying definitions propagate generate first level abstraction, express CarryIn signals economically. Let’s show 4 bits: equations represent common sense: CarryIn 1 earlier adder generates carry intermediary adders 1198propagate carry. Figure A.6.1 uses plumbing try explain carry lookahead. 1199FIGURE A.6.1 plumbing analogy carry lookahead 1 bit, 2 bits, 4 bits using water pipes valves. wrenches turned open close valves. Water shown color. output pipe (ci+1) full either nearest generate value (g i) turned propagate value (p i) 1200is water upstream, either earlier generate propagate water behind it. CarryI n (c0) result carry without help generates, help propagates. Even simplified form leads large equations and, hence, considerable logic even 16-bit adder. Let’s try moving two levels abstraction. Fast Carry Using Second Level Abstraction First, consider 4-bit adder carry-lookahead logic single building block. connect ripple carry fashion form 16-bit adder, add faster original little hardware. go faster, we’ll need carry lookahead higher level. perform carry lookahead 4-bit adders, need propagate generate signals higher level. four 4-bi adder blocks: is, “super” propagate signal 4-bit abstraction (P i) true bits group propagate carry. “super” generate signal (G i), care carry significant bit 4-bit group. obviously occurs generate true significant b it; also occurs earlier generate true intermediate propagates, including significant bit, also true : 1201Figure A.6.2 updates plumbing analogy show P0 G0. 1202FIGURE A.6.2 plumbing analogy next-level carry-lookahead signals P0 G0. P0 open four propagates (p i) open, water flows G0 least one generate (gi) open propagates downstream generate open. 1203Then equations higher level abstraction carry 4-bit group 16-bit adder (C1, C2, C3, C4 Figure A.6.3 ) similar carry equations bit 4-bit adder (c1, c2, c3, c4) page A-40: 12041205FIGURE A.6.3 Four 4-bit ALUs using carry lookahead form 16-bit adder. Note carries come carry-lookahead unit, 4-bit ALUs. Figure A.6.3 shows 4-bit adders connected carry- lookahead unit. exercises explore speed differences carry schemes, different notations multibit propagate generate signals, design 64-bit adder. Levels Propagate Generate Example Determine g i, pi, Pi, G values two 16-bit numbers: a: 0001 1010 0011 0011two b: 1110 0101 1110 1011two Also, CarryOut15 (C4)? Answer Aligning bits makes easy see values generate g (ai·bi) propagate p (ai + bi): a: 0001 1010 0011 0011 b: 1110 0101 1110 1011 gi: 0000 0000 0010 0011 pi: 1111 1111 1111 1011 bits numbered 15 0 left right. Next, “super” propagates (P3, P2, P1, P0) simply lower-level propagates: “super” generates complex, use following equations: 1206Finally, CarryOut15 Hence, carry adding two 16-bit numbers. reason carry lookahead make carries faster logic begins evaluating moment clock cycle begins, result change output gate stops changing. taking shortcut going fewer gates send carry signal, output gates stop changing sooner, hence time adder less. appreciate importance carry lookahead, need calculate relative performance ripple carry adders. Speed Ripple Carry versus Carry Lookahead Example One simple way model time logic assume gate takes time signal pass it. Time estimated simply counting number gates along path piece logic. Compare number gate delays paths two 16-bit adders, one using ripple carry one using 1207two-level carry lookahead. Answer Figure A.5.5 page A-28 shows carry signal takes two gate delays per bit. number gate delays carry least significant bit carry significant 32×2=64. carry lookahead, carry significant bit C4, defined example. takes two levels logic specify C4 terms P G (the several terms). P specified one level logic (AND) using p i, G specified two levels using p g i, worst case next level abstraction two levels logic. p g one level logic, defined terms b i. assume one gate delay level logic equations, worst case 2+2+1=5 gate delays. Hence, path carry carry out, 16-bit addition carry-lookahead adder six times faster, using simple estimate hardware speed. Summary Carry lookahead offers faster path waiting carries ripple 32 1-bit adders. faster path paved two signals, generate propagate. former creates carry regardless carry input, latter passes carry along. Carry lookahead also gives another example abstraction important computer design cope complexity. Check Using simple estimate hardware speed gate delays, relative performance ripple carry 8-bit add versus 64-bit add using carry-lookahead logic? 1. 64-bit carry-lookahead adder three times faster: 8-bit adds 16 gate delays 64-bit adds seven gate delays. 2. speed, since 64-bit adds need levels logic 16-bit adder. 3. Eight-bit adds faster 64 bits, even carry lookahead. 1208 Elaboration accounted one arithmetic logical operations core RISC-V instruction set: ALU Figure A.5.14 omits support shift instructions. would possible widen ALU multiplexor include left shift 1 bit right shift 1 bit. hardware designers created circuit called barrel shifter , shift 1 63 bits time takes add two 64-bit numbers, shifting normally done outside ALU. Elaboration logic equation Sum output full adder page A- 28 expressed simply using powerful gate OR. exclusive gate true two operands disagree; is, technologies, exclusive efficient two levels gates. Using symbol represent exclusive OR, new equation: Also, drawn ALU traditional way, using gates. Computers designed today CMOS transistors, basically switches. CMOS ALU barrel shifters take advantage switches many fewer multiplexors shown designs, design principles similar. Elaboration Using lowercase uppercase distinguish hierarchy generate propagate symbols breaks two levels. alternate notation scales gi..j pi..j generate propagate signals bits j. Thus, g1..1 generated bit 1, g4..1 bits 4 1, g16..1 bits 16 1. 1209A.7 Clocks discuss memory elements sequential logic, useful discuss briefly topic clocks. short secti introduces topic similar discussion found Section 4.2 . details clocking timing methodologies presented Section A.11 . Clocks needed sequential logic decide element contains state updated. clock simply free- running signal fixed cycle time ; clock frequency simply inverse cycle time. shown Figure A.7.1 , clock cycle time clock period divided two portions: clock high clock low. text, use edge- triggered clocking . means state changes occur clock edge. use edge-triggered methodology simpler explain. Depending technology, may may best choice clocking methodology . edge-triggered clocking clocking scheme state changes occur clock edge. clocking methodology approach used determine data valid stable relative clock. FIGURE A.7.1 clock signal oscillates high low values. clock period time one full cycle. edge-triggered design, either rising falling e dge clock active causes state changed. edge-triggered methodology, either rising edge th e 1210falling edge clock active causes state changes occur. see next section, state elements edge- triggered design implemented contents stat e elements change active clock edge. choice whic h edge active influenced implementation technology affect concepts involved designing logic. state element memory element. clock edge acts sampling signal, causing value data input state element sampled stored state element. Using edge trigger means sampling process essentially instantaneous, eliminating problems could occu r signals sampled slightly different times. major constraint clocked system, also called synchronous system , signals written state elements must valid active clock edge occurs. signal valid stable (i.e., changing), value change inputs change. Since combinational circuits cannot feedback, inputs combinational logic unit changed, outputs eventually become valid. synchronous system memory system employs clocks data signals read clock indicates signal values stable. Figure A.7.2 shows relationship among state elements combinational logic blocks synchronous, sequential logic design. state elements, whose outputs change clock edge, provide valid inputs combinational logic bloc k. ensure values written state elements active clock edge valid, clock must long enough period signals combinational logic block stabilize, clock edge samples values storage state elements. constraint sets lower bound length clock period, must long enough state element inputs valid. 1211FIGURE A.7.2 inputs combinational logic block come state element, outputs written state element. clock edge determines contents state elements updated. rest appendix, well Chapter 4 , usually omit clock signal, since assuming state elements updated clock edge. state elements written every clock edge, others written certain conditions (such register updated). cases, explicit write signal state element. write signal must still gated clock update occurs clock edge write signal active. see done used next section. One advantage edge-triggered methodology possible state element used input output combinational logic block, shown Figure A.7.3 . practice, care must taken prevent races situations ensure clock period long enough; topic discussed Section A.11 . FIGURE A.7.3 edge-triggered methodology allows state element read written th e clock cycle without creating race could lead undetermined data values. course, clock cycle must still long enough input values stable active clock 1212edge occurs. discussed clocking used update state elements, discuss construct state elements. Elaboration Occasionally, designers find useful small number state elements change opposite clock edge majority state elements. requires extreme care, approach effects inputs outputs state element. would designers ever this? Consider case amount combinational logic state element small enough could operate one-half clock cycle, rather usual full clock cycle. state element written clock ed ge corresponding half clock cycle, since inputs outputs usable one-half clock cycle. One common place technique used register files , simply reading writing register file often done half normal clock cycle. Chapter 4 makes use idea reduce pipelining overhead. register file state element consists set registers read written supplying register number accessed. A.8 Memory Elements: Flip-Flops, Latches, Registers section next, discuss basic principles beh ind memory elements, starting flip-flops latches, moving register files, finishing memories. memory elemen ts store state: output memory element depends inputs value stored inside memory element. Thus logic blocks containing memory element contai n state sequential. simplest type memory elements unclocked ; is, 1213do clock input. Although use clocked memory elements text, unclocked latch simplest memory element, let’s look circuit first. Figure A.8.1 shows S-R latch (set-reset latch), built pair gates (OR gates inverted outputs). outputs Q represent value stored state complement. neither R asserted, cross-coupled gates act inverters store previous values Q . FIGURE A.8.1 pair cross-coupled gates store internal value. value stored output Q recycled inverting obtain inverting obtain Q. either R asserted, Q deasserted vice versa. example, output, Q, true, bottom inverter produces false output (which ), becomes input top inverter, produces true output, Q, on. asserted, output Q asserted deasserted, R asserted, output asserted Q deasserted. R deasserted, last values Q continue stored cross- 1214coupled structure. Asserting R simultaneously lead incorrect operation: depending R deasserted, latch may oscillate become metastable (this described detail Section A.11 ). cross-coupled structure basis complex memory elements allow us store data signals. elements contain additional gates used store signal values cause state updated conjunction clock. next section shows elements built. Flip-Flops Latches Flip-flops latches simplest memory elements. flip-flops latches, output equal value stor ed state inside element. Furthermore, unlike S-R latch described above, latches flip-flops use thi point clocked, means clock input change state triggered clock. difference flip-flop latch point clock causes state actually change. clocked latch, state changed whenever appropriate inputs change clock asserted, whereas flip-flop, state changed clock edge. Since throughout text use edge-triggered timing methodology state updated clock edges, need use flip-flops. Flip-flops often built latches, start describing operation simple clocked latch n discuss operation flip-flop constructed latch. flip-flop memory element output equal value stored state inside element internal state changed clock edge. latch memory element output equal value stored state inside element state changed whenever appropriate inputs change clock asserted. 1215 flip-flop flip-flop one data input stores value input signal internal memory clock edge occurs. computer applications, function flip-flops latches store signal. latch flip-flop stores value data input signal internal memory. Although many types latch flip-flop, type basic building block need. latch two inputs two outputs. inputs data value stored (called D) clock signal (called C) indicates latch read value input store it. outputs simply value internal state ( Q) complement ( ). clock input C asserted, latch said open , value output ( Q) becomes value input D. clock input C deasserted, latch said closed , value output ( Q) whatever value stored last time latch open. Figure A.8.2 shows latch implemented two additional gates added cross-coupled gates. Since latch open value Q changes changes, structure sometimes called transparent latch . Figure A.8.3 shows latch works, assuming output Q initially false changes first. 1216FIGURE A.8.2 latch implemented gates. gate acts inverter input 0. Thus, cross-coupled pair gates acts store state value unless clock input, C, asserted, case value input replaces value Q stored. value input must stable clock signal C changes asserted deasserted. FIGURE A.8.3 Operation latch, assuming output initially deasserted. clock, C, asserted, latch open Q output immediately assumes value input. 1217As mentioned earlier, use flip-flops basic building block, rather latches. Flip-flops transparent: outputs change clock edge. flip-flop built triggers either rising (positive) falling (neg ative) clock edge; designs use either type. Figure A.8.4 shows falling-edge flip-flop constructed pair latches. flip-flop, output stored clock edge occurs. Figure A.8.5 shows flip-flop operates. FIGURE A.8.4 flip-flop falling-edge trigger. first latch, called master, open follows input clock input, C, asserted. clock input, C, falls, first latch closed, second latch, called slave, open gets input output master latch. FIGURE A.8.5 Operation flip-flop falling-edge trigger, assuming output initia lly deasserted. 1218When clock input ( C) changes asserted deasserted, Q output stores value input. Compare behavior clocked latch shown Figure A.8.3 . clocked latch, stored value output, Q, change whenever C high, opposed C transitions. Verilog description module rising-edge flip- flop, assuming C clock input data input: module DFF(clock,D,Q,Qbar); input clock, D; output reg Q; output Qbar; assign Qbar= ~ Q; always @(posedge clock) Q=D; endmodule input sampled clock edge, must valid period time immediately immediately clock edge. minimum time input must valid clock edge called setup time ; minimum time must valid clock edge called hold time . Thus inputs flip-flop (or anything built using flip- flops) must valid window begins time tsetup clock edge ends thold clock edge, shown Figure A.8.6 . Section A.11 talks clocking timing constraints, including propagation delay flip-flop, n detail. setup time minimum time input memory device must valid clock edge. hold time minimum time input must valid clock edge. 1219FIGURE A.8.6 Setup hold time requirements flip-flop falling-edge trigger. input must stable period time clock edge, well clock edge. minimum time signal must stable clock edge called setup time, minimum time signal must stable clock edge called hold time. Failure meet minimum requirements result situation output flip-flop may predictable, described Section A.11 . Hold times usually either 0 small thus cause worry. use array flip-flops build register hold multibit datum, byte word. used registers throughout datapaths Chapter 4 . Register Files One structure central datapath register file . register file consists set registers read writ ten supplying register number accessed. register file implemented decoder read write port array registers built flip-flops. reading regis ter change state, need supply register number input, output data contained register. writing register need three inputs: regis ter number, data write, clock controls writing register. Chapter 4 , used register file two read ports one write port. register file drawn shown Figure A.8.7 . read ports implemented pair multiplexors, wide number bits register register file. Figure A.8.8 shows implementation two register read ports 64-bit-wide register file. 1220FIGURE A.8.7 register file two read ports one write port five inputs two outputs. control input Write shown color. 1221FIGURE A.8.8 implementation two read ports register file n registers done pair n-to-1 multiplexors, 64 bits wide. register read number signal used multiplexor selector signal. Figure A.8.9 shows write port implemented. Implementing write port slightly complex, since change contents designated register. using decoder generate signal used determine register write. Figure A.8.9 shows implement write port register file. important remember flip-flop changes state clock edge. n Chapter 4 , hooked write signals register file explicitly assumed clock shown Figure A.8.9 attached implicitly. 12221223FIGURE A.8.9 write port register file implemented decoder used write signal generate C input registers. three inputs (the register number, data, write signal) setup hold-time constraints ensure correct data written register file. happens register read written clock cycle? write register file occurs clock edge, register valid time read, saw earlier Figure A.7.2 . value returned value written earlier clock cycle. want read return value currently written, additional logic register file r outside needed. Chapter 4 makes extensive use logic. Specifying Sequential Logic Verilog specify sequential logic Verilog, must understand generate clock, describe value written 1224register, specify sequential control. Let us start specifying clock. clock predefined object Verilog ; instead, generate clock using Verilog notation #n statement; causes delay n simulation time steps execu-tion statement. Verilog simulators, als possible generate clock external input, allowing user specify simulation time number clock cycles run simulation. code Figure A.8.10 implements simple clock high low one simulation unit switches state. use delay capability blocking assignment implement clock. FIGURE A.8.10 specification clock. Next, must able specify operation edge- triggered register. Verilog, done using sensit ivity list always block specifying trigger either positive negative edge binary variable notation posedge negedge , respectively. Hence, following Verilog code causes register written value b positive edge clock: Throughout chapter Verilog sections Chapter 4 , assume positive edge-triggered design. Figure A.8.11 shows Verilog specification RISC-V register file assum es two reads one write, write clocked. Check Verilog register file Figure A.8.11 , output ports corresponding registers read assigned using continuous assignment, register written assig ned always block. following reason? a. special reason. simply convenient. b. Data1 Data2 output ports WriteData input port. 1225c. reading combinational event, writing sequential event. 1226FIGURE A.8.11 RISC-V register file written behavioral Verilog. register file writes rising clock edge. A.9 Memory Elements: SRAMs DRAMs Registers register files provide basic building block small memories, larger amounts memory built using either SRAMs (static random access memories) DRAMs (dynamic random access memories). first discuss SRAMs, somewhat simpler, turn DRAMs. static random access memory (SRAM) memory data stored statically (as flip-flops) rather dynamically (as DRAM). SRAMs faster DRAMs, 1227but less dense expensive per bit. SRAMs SRAMs simply integrated circuits memory arrays (usually) single access port provide either read write. SRAMs fixed access time datum, though read write access characteristics often differ. SRAM chip specific configuration terms number addressable locations, well width addressable location. example, 4M×8 SRAM provides 4M entries, 8 bits wide. Thus 22 address lines (since 4M=222), 8-bit data output line, 8-bit single data input line. ROMs, number addressable locations often called height , number bits per unit called width . variety technical reasons, newest fastest SRAMs typically available narrow configurations: ×1 ×4. Figure A.9.1 shows input output signals 2M×16 SRAM. FIGURE A.9.1 32K ×8 SRAM showing 21 address lines (32K =215) 16 data inputs, three control lines, 16 data outputs. initiate read write access, Chip select signal must made active. reads, must also activate Output enable signal controls whether datum selected 1228address actually driven pins. Output enable useful connecting multiple memories single-output bus usi ng Output enable determine memory drives bus. SRAM read access time usually specified delay time Output enable true address lines valid time data output lines. Typical read access times SRAMs 2004 varied 2–4 ns fastest CMOS parts, tend somewhat smaller narrower, 8–20 ns typical largest parts, 2004 32 million bits data. demand low-power SRAMs consumer products digital appliances grown greatly past 5 years; SRAMs much lower stand-by access power, usually 5–10 times slower. recently, synchronous SRAMs—similar synchronous DRAMs, discuss next section—have also developed. writes, must supply data written address, well signals cause write occur. Write enable Chip select true, data data input lines written cell specified address. ar e setup-time hold-time requirements address data lines, flip-flops latches. addition, th e Write enable signal clock edge pulse minimum width requirement. time complete write specified combination setup times, hold times, Write enable pulse width. Large SRAMs cannot built way build register file because, unlike register file 32-to-1 multiplexor mig ht practical, 64K-to-1 multiplexor would needed 64K×1 SRAM totally impractical. Rather use giant multiplexor, large memories implemented shared output line, called bit line , multiple memory cells memory array assert. allow multiple sources drive single line, three-state buffer (or tristate buffer ) used. three-state buffer two inputs—a data signal Output enable—and single output, one three states: asserted, deasserted, hig h impedance. output tristate buffer equal data input signal, either asserted deasserted, Output enable asserted, otherwise high-impedance state allows another three-state buffer whose Output enable asserted 1229determine value shared output. Figure A.9.2 shows set three-state buffers wired form multiplexor decoded input. critical Output enable one three-state buffers asserted; otherwis e, three-state buffers may try set output line differen tly. using three-state buffers individual cells SRAM, e ach cell corresponds particular output share output line. use set distributed three-state buffers efficient implementation large centralized multiplex or. three-state buffers incorporated flip-flops form basic cells SRAM. Figure A.9.3 shows small 4×2 SRAM might built, using latches input called Enable controls three-state output. 1230FIGURE A.9.2 Four three-state buffers used form multiplexor. one four Select inputs asserted. three-state buffer deasserted Output enable high-impedance output allows three-state buffer whose Output enable asserted drive shared output line. 1231FIGURE A.9.3 basic structure 4×2 SRAM consists decoder selects pair cells activate. activated cells use three-state output connected vertical bit lines supply requested data. address selects cell sent one set horizontal address lines, called word lines. simplicity, Output enable Chip select signals omitted, could easily added gates. design Figure A.9.3 eliminates need enormous multiplexor; however, still requires large decoder 1232correspondingly large number word lines. example, 4M×8 SRAM, would need 22-to-4M decoder 4M word lines (which lines used enable individual flip-flo ps)! circumvent problem, large memories organized rectangular arrays use two-step decoding process. Figure A.9.4 shows 4M×8 SRAM might organized internally using two-step decode. see, two-level decoding process quite important understanding DRAMs operate. FIGURE A.9.4 Typical organization 4M×8 SRAM array 4K×1024 arrays. first decoder generates addresses eight 4K×1024 arrays; set multiplexors used select 1 bit 1024-bit-wide array. much easier design single-level decode would need either enormous decoder gigantic multiplexor. practice, modern SRAM size would probably use even larger number blocks, somewhat smaller. Recently seen development synchronous SRAMs (SSRAMs) synchronous DRAMs (SDRAMs). key capability provided synchronous RAMs ability transfe r burst data series sequential addresses within array row. burst defined starting address, supplied usual fashion, burst length. speed advantage synchronous RAMs comes ability transfer bits burst without specify additional address bits. Instead, clock used transfer successive bits burst. elimination need specify address transfers 1233within burst significantly improves rate transferrin g block data. capability, synchronous SRAMs DRAMs rapidly becoming RAMs choice building memory systems computers. discuss use synchronous DRAMs memory system detail next section Chapter 5 . DRAMs static RAM (SRAM), value stored cell kept pair inverting gates, long power applied, value kept indefinitely. dynamic RAM (DRAM), value kept cell stored charge capacitor. single transistor used access stored charge, either read value overwrite charge stored there. DRAMs use single transistor per bit storage, much denser cheaper per bit. comparison, SRAMs require four six transistors per bit. DRAMs store charge capacitor, cannot kept indefinitely must periodically refreshed . memory structure called dynamic , opposed static storage SRAM cell. refresh cell, merely read contents write back. charge kept several milliseconds, might correspond close million clock cycles. Today, single-chip memory controllers often handle refresh function independently processor. every bit read DRAM written back individually, large DRAMs containing multiple megabytes, would constantly refreshi ng DRAM, leaving time accessing it. Fortunately, DRAMs also use two-level decoding structure, allows us refresh entire row (which shares word line) read cycle followed immediately write cycle. Typically, refresh operati ons consume 1% 2% active cycles DRAM, leaving remaining 98% 99% cycles available reading writing data. Elaboration DRAM read write signal stored cell? transistor inside cell switch, called pass transistor , 1234allows value stored capacitor accessed either reading writing. Figure A.9.5 shows single-transistor cell looks. pass transistor acts like switch: signal word line asserted, switch closed, connecting capacitor bit line. operation write, value written placed bit line. value 1, capacitor charged. value 0, capacitor discharged. Reading slightly complex, since DRAM must detect small charge stored capacitor. activating word line read, bit line charged voltage halfway low high voltage. Then, activating word line, charge capacitor read onto bit line. causes bit line move slightly toward high low direction, change detected sense amplifier, detect small changes voltage. FIGURE A.9.5 single-transistor DRAM cell contains capacitor stores cell contents transistor used access cell. DRAMs use two-level decoder consisting row access followed column access , shown Figure A.9.6 . row access chooses one number rows activates corresponding word line. contents columns 1235active row stored set latches. column access selects data column latches. save pins reduce package cost, address lines used row column address; pair signals called RAS ( Row Access Strobe ) CAS ( Column Access Strobe ) used signal DRAM either row column address supplied. Refresh performed simply reading columns column latches writing values back. Thus, entire row refreshed one cycle. two-level addressing scheme, combined internal circuitry, makes DRAM access times much longer (by factor 5–10) SRAM access times. 2004, typical DRAM access times ranged 45 65 ns; 256 Mbit DRAMs full production, first customer samples 1 GB DRAMs became available first quarter 2004. much lower cost per bit makes DRAM choice main memory, faster access time makes SRAM choice caches. FIGURE A.9.6 4M×1 DRAM built 2048×2048 array. row access uses 11 bits select row, latched 2048 1-bit latches. multiplexor 1236chooses output bit 2048 latches. RAS CAS signals control whether address lines sent row decoder column multiplexor. might observe 64M×4 DRAM actually accesses 8K bits every row access throws away four column access. DRAM designers used internal structure DRAM way provide higher bandwidth DRAM. done allowing column address change without changing row address, resulting access bits column latches. make process faster precise, address inputs clocked, leading dominant form DRAM use today: synchronous DRAM SDRAM. Since 1999, SDRAMs memory chip choice cache-based main memory systems. SDRAMs provide fast access series bits within row sequentially transferring bits burst control clock signal. 2004, DDRRAMs (Double Data Rate RAMs), called double data rate transfer data rising falling edge externally supplied clock, heavily used form SDRAMs. discuss Chapter 5 , high-speed transfers used boost bandwidth available main memory match needs processor caches. Error Correction potential data corruption large memories, mos computer systems use sort error-checking code det ect possible corruption data. One simple code heavily used parity code . parity code number 1s word counted; word odd parity number 1s odd even otherwise. word written memory, parity bit also written (1 odd, 0 even). Then, word read out, parity bit read checked. parity memory word stored parity bit match, error occurred. 1-bit parity scheme detect 1 bit error data item; 2 bits error, 1-bit parity scheme detect errors, since parity match data two errors. (Actually, 1-bit parity scheme detect odd number 1237of errors; however, probability three errors muc h lower probability two, so, practice, 1-bit parity code limited detecting single bit error.) cou rse, parity code cannot tell bit data item error. 1-bit parity scheme error detection code ; also error correction codes (ECC) detect allow correction error. large main memories, many systems use code allows detection 2 bits error correction single bit error. codes work using bits enco de data; example, typical codes used main memories require 7 8 bits every 128 bits data. error detection code code enables detection error data, precise location and, hence, correction error. Elaboration 1-bit parity code distance-2 code , means look data plus parity bit, 1-bit change sufficient generate another legal combination data plus parity. example, change bit data, parity wrong, vice versa. course, change 2 bits (any 2 data bits 1 data bit parity bit), parity match data error cannot detected. Hence, distance two legal combinations parity data. detect one error correct error, need distance-3 code , property legal combination bits error correction code data least 3 bits differing combination. Suppose code one error data. case, code plus data one bit away legal combination, correct data legal combination. two errors, recognize error, cannot correct errors. Let’s look example. data words distance-3 error correction code 4-bit data item. 1238To see works, let’s choose data word, say 0110, whose error correction code 011. four 1-bit error possibilities data: 1110, 0010, 0100, 0111. look data item code (011), entry value 0001. error correction decoder received one four possible data words error, would choose correcting 0110 0001. four words error 1 bit changed correct pattern 0110, 2 bits different alternate correction 0001. Hence, error correction mechanism easily choose correct 0110, since single error much higher probability. see two errors detected, simply notice combinations 2 bits changed different code. one reuse code 3 bits different, correct 2- bit error, correct wrong value, since decoder w ill 1239assume single error occurred. want correct 1-bit errors detect, erroneously correct, 2-bit error s, need distance-4 code. Although distinguished code data explanation, truth, error correction code treats combination code data single word larger code (7 bits example). Thus, deals errors code bits fashion errors data bits. example requires n −1 bits n bits data, number bits required grows slowly, distance-3 code , 64-bit word needs 7 bits 128-bit word needs 8. type code called Hamming code , R. Hamming, described method creating codes. A.10 Finite-State Machines saw earlier, digital logic systems classified combinational sequential. Sequential systems contain state stor ed memory elements internal system. behavior depend set inputs supplied contents internal memory, state system. Thus, sequential system cannot described truth table. Instead, sequential system described finite-state machine (or often state machine ). finite-state machine set states two functions, called next-state function output function . set states corresponds possible values internal storage. Th us, n bits storage, 2 n states. next-state function combinational function that, given inputs current state, determines next state system. outpu function produces set outputs current state inputs. Figure A.10.1 shows diagrammatically. finite-state machine sequential logic function consisting set inputs outputs, next-state function maps current state inputs new state, output function maps current state possibly inputs set asserted outputs. 1240 next-state function combinational function that, given inputs current state, determines next state finite-state machine. FIGURE A.10.1 state machine consists internal storage contains state two combinational functions: next-state function output function. Often, output function restricted take current state input; change capability sequential machine, affect internals. state machines discuss Chapter 4 synchronous . means state changes together clock cycle, new state computed every clock. Thus, state elements updated clock edge. use methodology section throughout Chapter 4 , usually show clock explicitly. use state machines throughout Chapter 4 control execution processor actions datapath. illustrate finite-state machine operates designed, let’s look simple classic example: controlling traffic light . (Chapters 4 5 contain detailed examples using finite- state machines control processor execution.) finite-st ate machine used controller, output function often restricted depend current state. finite-st ate 1241machine called Moore machine . type finite-state machine use throughout book. output function depend current state current input, machine called Mealy machine . two machines equivalent capabilities, one turned mechanically. basic advantage Moore machine faster, Mealy machine may smaller, since may need fewer states Moore machine. Chapter 5 , discuss differences detail show Verilog version finite- state control using Mealy machine. example concerns control traffic light intersection north-south route east-west route. simplicity, consider green red lights; addin g yellow light left exercise. want lights cycle faster 30 seconds direction, use 0.033- Hz clock machine cycles states faster every 30 seconds. two output signals: NSlite: signal asserted, light north-south road green; signal deasserted, light north-south road red. EWlite: signal asserted, light east-west road green; signal deasserted, light east- west road red. addition, two inputs: NScar: Indicates car detector placed roadbed front light north-south road (going nort h south). EWcar: Indicates car detector placed roadbed front light east-west road (going east west). traffic light change one direction car waiting go direction; otherwise, light continue show green direction last car crossed intersection. implement simple traffic light need two states: NSgreen: traffic light green north-south direction. EWgreen: traffic light green east-west direction. also need create next-state function, specified table: 1242Notice didn’t specify algorithm happens car approaches directions. case, next- state function given changes state ensure steady stream cars one direction cannot lock car direction. finite-state machine completed specifying outpu function. 1243Before examine implement finite-state machine, let’s look graphical representation, often used finite-state machines. representation, nodes used indicate states. Inside node place list outputs active state. Directed arcs used show next-state function, labels arcs specifying input condition logic functions. Figure A.10.2 shows graphical representation finite-state machine. 1244FIGURE A.10.2 graphical representation two-state traffic light controller. simplified logic functions state transitions. example, transition NSgreen EWgreen next-state table , equivalent EWcar. finite-state machine implemented register hold current state block combinational logic computes next-state function output function. Figure A.10.3 shows finite-state machine 4 bits state, thus 16 states, might look. implement finite-state machine way, must first assign state numbers states. process called state assignment . example, could assign NSgreen state 0 EWgreen state 1. state register would contain single bit. next-state function would given CurrentState contents state register (0 1) NextState output next-state function written state register end clock cycle. output function also simple: 12451246FIGURE A.10.3 finite-state machine implemented state register holds current state combinational logic block compute next state output functions. latter two functions often split apart implemented two separate blocks logic, may require fewer gates. combinational logic block often implemented using structured logic, PLA. PLA constructed automatically next-state output function tables. fact, computer-aided design (CAD) programs take either graphical textual representation finite-state machine produce optimized implementation automatically. Chapters 4 5, finite-state machines used control processor execution. Appendix C discusses detailed implementation 1247of controllers PLAs ROMs. show might write control Verilog, Figure A.10.4 shows Verilog version designed synthesis. Note thi simple control function, Mealy machine useful, style specification used Chapter 5 implement control function Mealy machine fewer states Moore machine controller. Check smallest number states Moore machine Mealy machine could fewer states? a. Two, since could one-state Mealy machine might thing. b. Three, since could simple Moore machine went one two different states always returned original state that. simple machine, two-state Mealy machine possible. c. need least four states exploit advantages Mealy machine Moore machine. 1248FIGURE A.10.4 Verilog version traffic light controller. A.11 Timing Methodologies Throughout appendix rest text, use edge-triggered timing methodology. timing methodology advantage simpler explain understand level-triggered methodology. section, explain ti ming methodology little detail also introduce level- sensitive clocking. conclude section briefly discuss ing issue asynchronous signals synchronizers, important problem digital designers. purpose section introduce major concepts clocking methodology. section makes important simplifying assumptions; interested understanding timing methodology detail, consult one referenc es listed end appendix. use edge-triggered timing methodology simpler explain fewer rules required correctness. n particular, assume clocks arrive time, 1249are guaranteed system edge-triggered registers betwee n blocks combinational logic operate correctly without races simply make clock long enough. race occurs contents state element depend relative speed differ ent logic elements. edge-triggered design, clock cycle mu st long enough accommodate path one flip-flop combinational logic another flip-flop must satis fy setup-time requirement. Figure A.11.1 shows requirement system using rising edge-triggered flip-flops. syst em clock period (or cycle time) must least large worst-case values three delays, defined follows: tprop time signal propagate flip-flop; also sometimes called clock-to- Q. tcombinational longest delay combinational logic (which definition surrounded two flip-flops). tsetup time rising clock edge input flip- flop must valid. FIGURE A.11.1 edge-triggered design, clock must long enough allow signals valid required setup time next clock edge. time flip-flop input propagate flip-flip outputs tprop; signal takes tcombinational travel combinational logic must valid tsetup next clock edge. 1250We make one simplifying assumption: hold-time requirements satisfied, almost never issue modern logic. One additional complication must considered edge- triggered designs clock skew . Clock skew difference absolute time two state elements see clock edge. Clock skew arises clock signal often use two different paths, slightly different delays, reach two dif ferent state elements. clock skew large enough, may possibl e state element change cause input another flip- flop change clock edge seen second flip-f lop. clock skew difference absolute time times two state elements see clock edge. Figure A.11.2 illustrates problem, ignoring setup time flip-flop propagation delay. avoid incorrect operation, clo ck period increased allow maximum clock skew. Thus, clock period must longer FIGURE A.11.2 Illustration clock skew cause race, leading incorrect operation. difference two flip-flops see clock, signal stored first fli p- flop race forward change input second flip-flop clock arrives second flip-flop. constraint clock period, two clocks also arrive opposite order, second clock arriving tskew earlier, circuit work correctly. Designers reduce cl ock- 1251skew problems carefully routing clock signal minimize difference arrival times. addition, smart designers also prov ide margin making clock little longer minimum; allows variation components well power supply. Since clock skew also affect hold-time requiremen ts, minimizing size clock skew important. Edge-triggered designs two drawbacks: require extra logic may sometimes slower. looking flip- flop versus level-sensitive latch used construc flip-flop shows edge-triggered design requires lo gic. alternative use level-sensitive clocking . state changes level-sensitive methodology instantaneous, level-sensitive scheme slightly complex require additional care make operate correctly. level-sensitive clocking timing methodology state changes occur either high low clock levels instantaneous changes edge-triggered designs. Level-Sensitive Timing level-sensitive timing, state changes occur either high low levels, instantaneous edge- triggered methodology. noninstantaneous change n state, races easily occur. ensure level-sensitive desig n also work correctly clock slow enough, designers use two-phase clocking . Two-phase clocking scheme makes use two nonoverlapping clock signals. Since two clocks, typical ly called ϕ1 ϕ2, nonoverlapping, one clock signals high given time, Figure A.11.3 shows. use two clocks build system contains level-sensi tive latches free race conditions, edge- triggered designs were. 1252FIGURE A.11.3 two-phase clocking scheme showing cycle clock nonoverlapping periods. One simple way design system alternate use latches open ϕ1 latches open ϕ2. clocks asserted time, race cannot occur. input combinational block ϕ1 clock, output latched ϕ2 clock, open ϕ2 input latch closed hence valid output. Figure A.11.4 shows system two-phase timing alternating latches operates. edge-triggered design, must pay attention clock skew, particularly two clock phases. increasing amount nonoverlap two phases, reduce potential margin error. Thus, system guaranteed operate correctly phase long enough large enough nonoverlap phases. FIGURE A.11.4 two-phase timing scheme alternating latches showing system operates clock phases. output latch stable opposite phase C input. Thus, first block combinational inputs stable input ϕ2, output latched ϕ2. second (rightmost) combinational block operates opposite fashion, stable inputs ϕ1. Thus, delays combinational blocks determine minimum time respective clocks must asserted. size nonoverlapping period determined maximum 1253clock skew minimum delay logic block. Asynchronous Inputs Synchronizers using single clock two-phase clock, eliminate race conditions clock-skew problems avoided. Unfortunately, impractical make entire system function single clock still keep clock skew small. CPU may use single clock, I/O devices probably clock. asynchronous device may communicate CPU series handshaking steps. translate asynchronous input synchronous signal used change state system, need use synchronizer , whose inputs asynchronous signal clock whose output signal synchronous input clock. first attempt build synchronizer uses edge-triggered flip-flop, whose input asynchronous signal, Figure A.11.5 shows. communicate handshaking protocol, matter whether detect asserted state asynchronous signal one clock next, since signal held asserted acknowledged. Thus, might think simple structure enough sample signal accurately, would case except one small problem. metastability situation occurs signal sampled stable required setup hold times, possibly causing sampled value fall indeterminate region high low value. FIGURE A.11.5 synchronizer built flip- flop used sample asynchronous signal produce output synchronous clock. 1254This “synchronizer” work properly! problem situation called metastability . Suppose asynchronous signal transitioning high low clock edge arrives. Clearly, possible know whether signal latched high low. problem could live with. Unfortunately, situation worse: signal sampled stable required setup hold times , flip-flop may go metastable state. state, output legitimate high low value, indeterminate region them. Furthermore, flip- flop guaranteed exit state bounded amount time. logic blocks look output flip-flop may see output 0, others may see 1. situation called synchronizer failure . synchronizer failure situation flip-flop enters metastable state logic blocks reading output flip-flop see 0 whil e others see 1. purely synchronous system, synchronizer failure avoided ensuring setup hold times flip-flop latch always met, impossible input asynchronous. Instead, solution possible wait long enough looking output flip-flop ensure hat output stable, exited metastable state, ever entered it. long long enough? Well, probability flip-flop stay metastable state decreases exponentially, short time probability f lip- flop metastable state low; however, probability never reaches 0! designers wait long enough probability synchronizer failure low, time failures years even thousands years. flip-flop designs, waiting period several times longer setup time makes probability synchronization failure low. clock rate longer th e potential metastability period (which likely), safe synchronizer built two flip-flops, Figure A.11.6 1255shows. interested reading problems, look references. Check Suppose design large clock skew—longer register propagation time . always possible design slow clock enough guarantee logic operates properly? a. Yes, clock slow enough signals always propagate design work, even skew large. b. No, since possible two registers see clock e dge far enough apart register triggered, outputs propagated seen second register clock edge. propagation time time required input flip-flop propagate outputs flip-flop. FIGURE A.11.6 synchronizer work correctly period metastability wi sh guard less clock period. Although output first flip-flop may metastable, seen logic element second clock, second flip- flop samples signal, time longer metastable state. A.12 Field Programmable Devices Within custom semicustom chip, designers make use flexibility underlying structure easily implement combinational sequential logic. designer 1256not want use custom semicustom IC implement complex piece logic taking advantage high levels integrat ion available? popular component used sequential combinational logic design outside custom semicustom IC field programmable device (FPD) . FPD integrated circuit containing combinational logic, possibly memory devices, configurable end user. field programmable devices (FPD) integrated circuit containing combinational logic, possib ly memory devices, configurable end user. FPDs generally fall two camps: programmable logic devices (PLDs) , purely combinational, field programmable gate arrays (FPGAs) , provide combinational logic flip-flops. PLDs consist two forms: simple PLDs (SPLDs) , usually either PLA programmable array logic (PAL) , complex PLDs, allow one logic block well configurable interconnections among blocks. speak PLA PLD, mean PLA user programmable and-plane or-plane. PAL like PLA, except or-plane fixed. programmable logic device (PLD) integrated circuit containing combinational logic whose function configured end user. field programmable gate array (FPGA) configurable integrated circuit containing combinational logic blocks flip-flops. simple programmable logic device (SPLD) Programmable logic device, usually containing either single PAL PLA. programmable array logic (PAL) 1257Contains programmable and-plane followed fixed or-plane. discuss FPGAs, useful talk FPDs configured. Configuration essentially question make break connections. Gate register structures static, connections configured. Notice configuring connections, user determines logic functions implemented. Consider configurable PLA: determining connections and-plane or-plane, user dictates logical functions computed PLA. Connections FPDs either permanent reconfigurable. Permanent connections involve creation destruction connection two wires. Current FPLDs use antifuse technology, allows connection built programming time permanent. way configure CMOS FPLDs SRAM. SRAM downloaded power-on, contents control setting switches, turn determines metal lines connected. use SRAM control advantage FPD reconfigured changing contents SRAM. disadvantages SRAM-based control two-fold: configuration volatile must reloaded power-on, use active transistors switches slightly increases resistance connections. antifuse structure integrated circuit programmed makes permanent connection two wires. FPGAs include logic memory devices, usually structured two-dimensional array corridors dividin g rows columns used global interconnect cells array. cell combination gates flip-flops programmed perform specific function. basically small, programmable RAMs, also called lookup tables (LUTs) . Newer FPGAs contain sophisticated building blocks pieces adders RAM blocks used build register files. FPGAs even contain 64-bit RISC-V cores! 1258 lookup tables (LUTs) field programmable device, name given cells consist small amount logic RAM. addition programming cell perform specific function, interconnections cells also programm able, allowing modern FPGAs hundreds blocks hundreds thousands gates used complex logic functions. Interconnect major challenge custom chips, even true FPGAs, cells represent natural units f decomposition structured design. many FPGAs, 90% area reserved interconnect 10% logic memory blocks. cannot design custom semicustom chip without CAD tools, also need FPDs. Logic synthesis tools developed target FPGAs, allowing generation system using FPGAs structural behavioral Verilog. A.13 Concluding Remarks appendix introduces basics logic design. digested material appendix, ready tackle material Chapters 4 5, use concepts discussed appendix extensively. Reading number good texts logic design. might like look into. Ciletti, M. D. [2002]. Advanced Digital Design Verilog HDL , Englewood Cliffs, NJ: Prentice Hall. thorough book logic design using Verilog. Katz, R. H. [2004]. Modern Logic Design , 2nd ed., Reading, MA: Addison-Wesley. general text logic design. Wakerly, J. F. [2000]. Digital Design: Principles Practices , 3rd ed., Englewood Cliffs, NJ: Prentice Hall. general text logic design. 1259A.14 Exercises A.1 [10] <§A.2>In addition basic laws discussed section, two important theorems, called DeMorgan’s theorems: Prove DeMorgan’s theorems truth table form A.2 [15] <§A.2>Prove two equations E example starting page A-7 equivalent using DeMorgan’s theorems axioms shown page A-7. A.3 [10] <§A.2>Show 2 n entries truth table function n inputs. A.4 [10] <§A.2>One logic function used variety purposes (including within adders compute parity) exclusive . output two-input exclusive function true exactly one inputs true. Show truth tabl e two-input exclusive function implement function using gates, gates, inverters. A.5 [15] <§A.2>Prove gate universal showing build AND, OR, functions using two-input gate. A.6 [15] <§A.2>Prove NAND gate universal showing 1260how build AND, OR, functions using two-input NAND gate. A.7 [10] <§§A.2, A.3>Construct truth table four-input odd- parity function (see page A-65 description parity). A.8 [10] <§§A.2, A.3>Implement four-input odd-parity function gates using bubbled inputs outputs. A.9 [10] <§§A.2, A.3>Implement four-input odd-parity function PLA. A.10 [15] <§§A.2, A.3>Prove two-input multiplexor also universal showing build NAND (or NOR) gate using multiplexor. A.11 [5] <§§4.2, A.2, A.3>Assume X consists 3 bits, x2 x1 x0. Write four logic functions true X contains one 0 X contains even number 0s X interpreted unsigned binary number less 4 X interpreted signed (two’s complement) number negative A.12 [5] <§§4.2, A.2, A.3>Implement four functions described Exercise A.11 using PLA. A.13 [5] <§§4.2, A.2, A.3>Assume X consists 3 bits, x2 x1 x0, consists 3 bits, y2 y1 y0. Write logic functions true X <Y, X thought unsigned binary numbers X <Y, X thought signed (two’s complement) numbers X =Y Use hierarchical approach extended larger numbers bits. Show extend 6-bit comparison. A.14 [5] <§§A.2, A.3>Implement switching network two data inputs ( B), two data outputs ( C D), control input ( S). equals 1, network pass-through mode, C equal A, equal B. equals 0, network crossing mode, C equal B, equal A. A.15 [15] <§§A.2, A.3>Derive product-of-sums representation 1261for E shown page A-11 starting sum-of-products representation. need use DeMorgan’s theorems. A.16 [30] <§§A.2, A.3>Give algorithm constructing sum- of-products representation arbitrary logic equation consisting AND, OR, NOT. algorithm recursive construct truth table proce ss. A.17 [5] <§§A.2, A.3>Show truth table multiplexor (inputs A, B, S; output C ), using don’t cares simplify table possible. A.18 [5] <§A.3>What function implemented following Verilog modules: module FUNC1 (I0, I1, S, out); input I0, I1; input S; output out; = S? I1: I0; endmodule module FUNC2 (out,ctl,clk,reset); output [7:0] out; input ctl, clk, reset; reg [7:0] out; always @(posedge clk) (reset) begin <= 8’b0 ; end else (ctl) begin <= + 1; end else begin <= - 1; end endmodule A.19 [5] <§A.4>The Verilog code page A-53 flip-flop. Show Verilog code latch. A.20 [10] <§§A.3, A.4>Write Verilog module implementation 2-to-4 decoder (and/or encoder). A.21 [10] <§§A.3, A.4>Given following logic diagram accumulator, write Verilog module implementation it. Assume positive edge-triggered register asynchronous 1262Rst. A.22 [20] <§§B3, A.4, A.5> Section 3.3 presents basic operation possible implementations multipliers. basic unit implementations shift-and-add unit. Show Verilog implementation unit. Show use unit build 32-bit multiplier. A.23 [20] <§§B3, A.4, A.5>Repeat Exercise A.22 , unsigned divider rather multiplier. A.24 [15] <§A.5>The ALU supported set less (slt) using sign bit adder. Let’s try set less operation using values −7ten 6ten. make simpler follow example, let’s limit binary representations 4 bits: 1001two 0110two. 1001two − 0110two = 1001two + 1010two = 0011two result would suggest −7>6, clearly wrong. Hence, must factor overflow decision. Modify 1- bit ALU Figure A.5.10 page A-33 handle slt correctly. Make changes photocopy figure save time. A.25 [20] <§A.6>A simple check overflow addition see CarryIn significant bit CarryOut significant bit. Prove check 1263same Figure 3.2 . A.26 [5] <§A.6>Rewrite equations page A-44 carry- lookahead logic 16-bit adder using new notation. First, use names CarryIn signals individual bits adder. is, use c4, c8, c12, … instead C1, C2, C3, …. addition, let P i,j; mean propagate signal bits j, G i,j; mean generate signal bits j. example, equation rewritten general notation useful creating wider adders. A.27 [15] <§A.6>Write equations carry-lookahead logic 64-bit adder using new notation Exercise A.26 using 16-bit adders building blocks. Include drawing similar Figure A.6.3 solution. A.28 [10] <§A.6>Now calculate relative performance adders. Assume hardware corresponding equation containing terms, equations p g page A-40, takes one time unit T. Equations consist several terms, equations c1, c2, c3, c4 page A-40, would thus take two time units, 2T. reason would take produce terms additional produce result OR. Calculate numbers performance ratio 4-bit adders ripple carry carry lookahead. terms equations defined equations, add appropriate delays intermediate equations, continue recursively actual input bits f adder used equation. Include drawing adder labeled calculated delays path worst-case delay highlighted. A.29 [15] <§A.6>This exercise similar Exercise A.28 , time calculate relative speeds 16-bit adder using ripple carry only, ripple carry 4-bit groups use carry lookahead, 1264and carry-lookahead scheme page A-39. A.30 [15] <§A.6>This exercise similar Exercises A.28 A.29, time calculate relative speeds 64-bit adder using ripple carry only, ripple carry 4-bit groups use carry lookahead, ripple carry 16-bit groups use carry lookahead, carry-lookahead scheme Exercise A.27 . A.31 [10] <§A.6>Instead thinking adder device adds two numbers links carries together, think adder hardware device add three inputs together (a i, bi, ci) produce two outputs (s, ci+1). adding two numbers together, little observation. adding two operands, possible reduce cost carry. idea form two independent sums, called S′ (sum bits) C′ (carry bits). end process, need add C′ S′ together using normal adder. technique delaying carry propagation end sum numbers called carry save addition . block drawing lower right Figure A.14.1 (see below) shows organization, two levels carry save adders connected single normal adder. Calculate delays add four 16-bit numbers using full carry- lookahead adders versus carry save carry-lookahead adder forming final sum. (The time unit Exercise A.28 same.) A.32 [20] <§A.6>Perhaps likely case adding many numbers computer would trying multiply quickly using many adders add many numbers single clock cycle. Compared multiply algorithm Chapter 3 , carry save scheme many adders could multiply 10 times faster. exercise estimates cost speed combinational multiplier multiply two positive 16- bit numbers. Assume 16 intermediate terms M15, M14, …, M0, called partial products , contain multiplicand ANDed multiplier bits m15, m14, …, m0. idea use carry save adders reduce n operands 2 n/3 parallel groups three, repeatedly get two large numbers add together traditional adder. First, show block organization 16-bit carry save adders add 16 terms, shown right Figure A.14.1 . 1265Then calculate delays add 16 numbers. Compare time iterative multiplication scheme Chapter 3 assume 16 iterations using 16-bit adder full carry lookahead whose speed calculated Exercise A.29 . A.33 [10] <§A.6>There times want add collection numbers together. Suppose wanted add four 4-bit numbers (A, B, E, F) using 1-bit full adders. Let’s ignore carry lookahead now. would likely connect 1-bit adders organization top Figure A.14.1 . traditional organization novel organization full adders. Try adding four numbers using organizations convince get answer. A.34 [5] <§A.6>First, show block organization 16-bit carry save adders add 16 terms, shown Figure A.14.1 . Assume time delay 1-bit adder 2T. Calculate time adding four 4-bit numbers organization top versus organization bottom Figure A.14.1 . A.35 [5] <§A.8>Quite often, would expect given timing diagram containing description changes take place data input clock input C (as Figures A.8.3 A.8.6 pages A-52 A-54, respectively), would differences output waveforms ( Q) latch flip-flop. sentence two, describe circumstances (e.g., nature inputs) would difference two output waveforms. A.36 [5] <§A.8> Figure A.8.8 page A-55 illustrates implementation register file RISC-V datapath. Pretend new register file built, two registers one read port, register 2 bits data. Redraw Figure A.8.8 every wire diagram corresponds 1 bit data (unlike diagram Figure A.8.8 , wires 5 bits wires 32 bits). Redraw registers using flip-flops. need show implement flip-flop multiplexor. A.37 [10] <§A.10>A friend would like build “electronic eye” use fake security device. device consists three lights lined row, controlled outputs Left, Middle, 1266and Right, which, asserted, indicate light on. one light time, light “moves” left right right left, thus scaring away thieves believe device monitoring activity. Draw graphical representation finite-state machine used specify electronic eye. Note rate eye’s movement controlled clock speed (which great) essentially inputs. A.38 [10] <§A.10>Assign state numbers states finite- state machine constructed Exercise A.37 write set logic equations outputs, including next-state bits. A.39 [15] <§§A.2, A.8, A.10>Construct 3-bit counter using three flip-flops selection gates. inputs consist signal resets counter 0, called reset , signal increment counter, called inc. outputs value counter. counter value 7 incremented, wrap around become 0. A.40 [20] <§A.10>A Gray code sequence binary numbers property 1 bit changes going one element sequence another. example, 3-bit binary Gray code: 000, 001, 011, 010, 110, 111, 101, 100. Using three flip-flops PLA, construct 3-bit Gray code counter two inputs: reset , sets counter 000, inc, makes counter go next value sequence. Note code cyclic, value 100 sequence 000. A.41 [25] <§A.10>We wish add yellow light traffic light example page A-68. changing clock run 0.25 Hz (a 4-second clock cycle time), duration yellow light. prevent green red lights cyclin g fast, add 30-second timer. timer single input, called TimerReset , restarts timer, single output, called TimerSignal , indicates 30-second period expired. Also, must redefine traffic signals include yellow. defining two output signals light: green yellow. output NSgreen asserted, green light on; output NSyellow asserted, yellow light on. signals off, red light on. assert 1267the green yellow signals time, since American drivers certainly confused, even European drivers understand means! Draw graphical representation finite-state machine improved controller. Choo se names states different names outputs. A.42 [15] <§A.10>Write next-state output-function tables traffic light controller described Exercise A.41 . A.43 [15] <§§A.2, A.10>Assign state numbers states traffic light example Exercise A.41 use tables Exercise A.42 write set logic equations outputs, including next-state outputs. A.44 [15] <§§A.3, A.10>Implement logic equations Exercise A.43 PLA. 1268FIGURE A.14.1 Traditional ripple carry carry save addition four 4-bit numbers. details shown left, individual signals lowercase, corresponding higher- level blocks right, collective signals upper case. Note sum four n-bit numbers take n + 2 bits. Answers Check §A.2, page A-8: No. = 1, C = 1, B = 0, first true, second false. §A.3, page A-20: C. §A.4, page A-22: exactly same. §A.4, page A-26: = 0, B = 1. §A.5, page A-37: 2. §A.6, page A-46: 1. 1269§A.8, page A-57: c. §A.10, page A-71: b. §A.11, page A-76: b. 1270APPENDIX B Graphics Computing GPUs John Nickolls, Director Architecture NVIDIA David Kirk, Chief Scientist NVIDIA B.1 Introduction B-3 B.2 GPU System Architectures B-7 B.3 Programming GPUs B-12 B.4 Multithreaded Multiprocessor Architecture B-25 B.5 Parallel Memory System B-36 B.6 Floating-point Arithmetic B-41 B.7 Real Stuff: NVIDIA GeForce 8800 B-46 B.8 Real Stuff: Mapping Applications GPUs B-55 B.9 Fallacies Pitfalls B-72 B.10 Concluding Remarks B-76 B.11 Historical Perspective Reading B-77 Imagination important knowledge. Albert Einstein Science, 1930s B.1 Introduction appendix focuses GPU —the ubiquitous graphics processing unit every PC, laptop, desktop computer, 1271workstation. basic form, GPU generates 2D 3D graphics, images, video enable Window-based operating systems, graphical user interfaces, video games, visual imaging applications, video. modern GPU describe highly parallel, highly multithreaded multiprocessor optimize visual computing . provide real-time visual interaction computed objects via graphics, images, video, GPU unified graphics computing architecture serves programmable graphics processor scalable parallel computing platform. PCs game consoles combine GPU CPU form heterogeneous systems . graphics processing unit (GPU) processor optimized 2D 3D graphics, video, visual computing, display. visual computing mix graphics processing computing lets visually interact computed objects via graphics, images, video. heterogeneous system system combining different processor types. PC heterogeneous CPU–GPU system. Brief History GPU Evolution Fifteen years ago, thing GPU. Graphics PC performed video graphics array (VGA) controller. VGA controller simply memory controller display generator connected DRAM. 1990s, semiconductor technology advanced sufficiently functions could added VGA controller. 1997, VGA controllers beginning incorporate three-dimensional (3D) acceleration functions, including hardware triangle setup rasterization (dicing triangles individual pixels) texture mapping shading (applying “decals” patterns pixels blending colors). 1272In 2000, single chip graphics processor incorporated almost every detail traditional high-end workstation graphics pipeline and, therefore, deserved new name beyond VGA controller. term GPU coined denote graphics device become processor. time, GPUs became programmable, programmable processors replaced fixed-function dedicated logic maintaining basic 3D graphics pipeline organization. addition, computations became precise time, progressi ng indexed arithmetic, integer fixed point, single- precision floating-point, recently double-precision f loating- point. GPUs become massively parallel programmable processors hundreds cores thousands threads. Recently, processor instructions memory hardware added support general purpose programming languages, programming environment created allow GPUs programmed using familiar languages, including C C++. innovation makes GPU fully general-purpose, programmable, manycore processor, albeit still special benefits limitations. GPU Graphics Trends GPUs associated drivers implement OpenGL DirectX models graphics processing. OpenGL open standard 3D graphics programming available computers. DirectX series Microsoft multimedia programming interfaces, including Direct3D 3D graphics. Since application programming interfaces (APIs) well- defined behavior, possible build effective hardware acceleration graphics processing functions defined th e APIs. one reasons (in addition increasing device density) new GPUs developed every 12 18 months double performance previous generation exist ing applications. application programming interface (API) set function data structure definitions providing interface library functions. 1273Frequent doubling GPU performance enables new applications previously possible. intersection graphics processing parallel computing invites new paradigm graphics, known visual computing. replaces large sections traditional sequential hardware graphics pipeline model programmable elements geometry, vertex, pixel programs. Visual computing modern GPU combines graphics processing parallel computing novel ways permit new graphics algorithms implemented, opens door entirely new parallel processing applications pervasive high-performance GPUs. Heterogeneous System Although GPU arguably parallel powerful processor typical PC, certainly processor. CPU, multicore soon manycore, complementary, primarily serial processor companion massively parallel manycore GPU. Together, two types processors comprise heterogeneous multiprocessor system. best performance many applications comes using CPU GPU. appendix help understand best split work two increasingly parallel processors. GPU Evolves Scalable Parallel Processor GPUs evolved functionally hardwired, limited capabilit VGA controllers programmable parallel processors. evolution proceeded changing logical (API-based) graphics pipeline incorporate programmable elements also making underlying hardware pipeline stages less specialized programmable. Eventually, made sense merge disparate programmable pipeline elements one unified array many programmable processors. GeForce 8-series generation GPUs, geometry, vertex, pixel processing run type processor. unification allows dramatic scalability. programmable 1274processor cores increase total system throughput. Unifyin g processors also delivers effective load balancing, since processing function use whole processor array. oth er end spectrum, processor array built processors, since functions run processors. CUDA GPU Computing? uniform scalable array processors invites new model programming GPU. large amount floating-point processing power GPU processor array attractive solving nongraphics problems. Given large degree parallelism range scalability processor array graphics applications, programming model general computing must express massive parallelism directly, ow scalable execution. GPU computing term coined using GPU computing via parallel programming language API, without using traditional graphics API graphics pipeline model. contrast earlier General Purpose computation GPU (GPGPU) approach, involves programming GPU using graphics API graphics pipeline perform nongraphics tasks. GPU computing Using GPU computing via parallel programming language API. GPGPU Using GPU general-purpose computation via traditional graphics API graphics pipeline. Compute Unifed Device Architecture (CUDA) scalable parallel programming model software platform GPU parallel processors allows programmer bypass graphics API graphics interfaces GPU simply program C C++. CUDA programming model 1275SPMD (single-program multiple data) software style, programmer writes program one thread instanced executed many threads parallel multiple processors GPU. fact, CUDA also provides facility programming multiple CPU cores well, CUDA environment writing parallel programs entire heterogeneous computer syste m. CUDA scalable parallel programming model language based C/C++. parallel programming platform GPUs multicore CPUs. GPU Unifes Graphics Computing addition CUDA GPU computing capabilities GPU, possible use GPU graphics processor computing processor time, combine uses visual computing applications. underlying processor architecture GPU exposed tw ways: first, implementing programmable graphics APIs, second, massively parallel processor array programmable C/C++ CUDA. Although underlying processors GPU unified, necessary SPMD thread programs same. GPU run graphics shader programs graphics aspect GPU, processing geometry, vertices, pixels, also run thread programs CUDA. GPU truly versatile multiprocessor architecture, supporting variety processing tasks. GPUs excellent graphics visual computing specifically designed applications. GPUs also excellent many general- purpose throughput applications “first cousins” graphics, perform lot parallel work, well lot regular problem structure. general, good match data-parallel problems (see Chapter 6 ), particularly large problems, less less regular, smaller problems. GPU Visual Computing Applications 1276Visual computing includes traditional types graphics applications plus many new applications. original purview GPU “anything pixels,” includes many problems without pixels regular computation and/or dat structure. GPUs effective 2D 3D graphics, since purpose designed. Failure deliver application performance would fatal. 2D 3D graphics use GPU “graphics mode,” accessing processing power GPU graphics APIs, OpenGL™, DirectX™. Games built 3D graphics processing capability. Beyond 2D 3D graphics, image processing video important applications GPUs. implemented using graphics APIs computational programs, using CUDA program GPU computing mode. Using CUDA, image processing simply another data-parallel array program. extent data access regular good locality, program efficient. practice, image processing good application GPUs. Video processing, especially encode decode (compression decompression according standard algorithms), quite efficient. greatest opportunity visual computing applications GPUs “break graphics pipeline.” Early GPUs implemented specific graphics APIs, albeit high performance. wonderful API supported operations wanted do. not, GPU could accelerate task, early GPU functionality immutable. Now, advent GPU computing CUDA, GPUs programmed implement different virtual pipeline simply writing CUDA program describe computation data flow desired. So, applications possible, stimulate new visual computing approaches. B.2 GPU System Architectures section, survey GPU system architectures common us e today. discuss system configurations, GPU functions services, standard programming interfaces, basic GPU internal architecture. 1277Heterogeneous CPU–GPU System Architecture heterogeneous computer system architecture using GPU CPU described high level two primary characteristics: first, many functional subsystems and/or chips used interconnection technologies topology; second, memory subsystems available functional subsystems. See Chapter 6 background PC I/O systems chip sets. Historical PC (circa 1990) Figure B.2.1 shows high-level block diagram legacy PC, circa 1990. north bridge (see Chapter 6 ) contains high-bandwidth interfaces, connecting CPU, memory, PCI bus. south bridge contains legacy interfaces devices: ISA bus (audio, LAN), interrupt controller; DMA controller; time/counter. system, display driven simple framebuffer subsystem known VGA ( video graphics array ) attached PCI bus. Graphics subsystems built-in processing elemen ts (GPUs) exist PC landscape 1990. 1278FIGURE B.2.1 Historical PC. VGA controller drives graphics display framebuffer memory. Figure B.2.2 illustrates two confgurations common use today. characterized separate GPU (discrete GPU) CPU respective memory subsystems. Figure B.2.2a , Intel CPU, see GPU attached via 16-lane PCI-Express 2.0 link provide peak 16 GB/s transfer rate (peak 8 GB/s direction). Similarly, Figure B.2.2b , AMD CPU, GPU attached chipset, also via PCI-Express available bandwidth. cases, GPUs CPUs may access other’s memory, albeit less available bandwidth access directly attached memories. case AMD system, north bridge memory controller integrated die CPU. PCI-Express (PCIe) standard system I/O interconnect uses point-to-point li nks. 1279Links configurable number lanes bandwidth. 1280FIGURE B.2.2 Contemporary PCs Intel 1281AMD CPUs. See Chapter 6 explanation components interconnects figure. low-cost variation systems, unified memory architecture (UMA) system, uses CPU system memory, omitting GPU memory system. systems relatively low-performance GPUs, since achieved performan ce limited available system memory bandwidth increased latency memory access, whereas dedicated GPU memory provides high bandwidth low latency. unified memory architecture (UMA) system architecture CPU GPU share common system memory. high-performance system variation uses multiple attached GPUs, typically two four working parallel, display daisy-chained. example NVIDIA SLI (scalable link interconnect) multi-GPU system, designed high-performanc e gaming workstations. next system category integrates GPU north bridge (Intel) chipset (AMD) without dedicated graphics memory. Chapter 5 explains caches maintain coherence shared address space. CPUs GPUs, multiple address spaces. GPUs access physical local memory CPU system’s physical memory using virtual addresses translated MMU GPU. operating system kernel manages GPU’s page tables. system physical page accessed using either coherent noncoherent PCI-Express transactions, determined attribute GPU’s page table. CPU access GPU’s local memory address range (also called aperture) PCI-Express address space. Game Consoles Console systems Sony PlayStation 3 Microsoft Xbox 360 resemble PC system architectures previously described. Console systems designed shipped 1282identical performance functionality lifespan last five years more. time, system may reimplemented many times exploit advanced silicon manufacturing processes thereby provide constant capabilit ever lower costs. Console systems need subsystems expanded upgraded way PC systems do, major internal system buses tend customized rather standardized. GPU Interfaces Drivers PC today, GPUs attached CPU via PCI-Express. Earlier generations used AGP . Graphics applications call OpenGL [ Segal Akeley, 2006 ] Direct3D [Microsoft DirectX Specifcation] API functions use GPU coprocessor. APIs send commands, programs, data GPU via graphics device driver optimized particular GPU. AGP extended version original PCI I/O bus, provided eight times bandwidth original PCI bus single card slot. primary purpose connect graphics subsystems PC systems. Graphics Logical Pipeline graphics logical pipeline described Section B.3 . Figure B.2.3 illustrates major processing stages, highlights important programmable stages (vertex, geometry, pixel shader stages). FIGURE B.2.3 Graphics logical pipeline. Programmable graphics shader stages blue, fixed-function blocks white. 1283Mapping Graphics Pipeline Unified GPU Processors Figure B.2.4 shows logical pipeline comprising separate independent programmable stages mapped onto physical distributed array processors. FIGURE B.2.4 Logical pipeline mapped physical processors. programmable shader stages execute array unified processors, logical graphics pipeline dataflow recirculates processors. Basic Unified GPU Architecture Unified GPU architectures based parallel array many programmable processors. unify vertex, geometry, pixel shader processing parallel computing processors, unlike earlier GPUs separate processors dedicated processing type. programmable processor array tightly integrated fixed function processors texture filte ring, rasterization, raster operations, anti-aliasing, compression, decompression, display, video decoding, high-definition vi deo processing. Although fixed-function processors signifi cantly outperform general programmable processors terms absolute performance constrained area, cost, power 1284budget, focus programmable processors here. Compared multicore CPUs, manycore GPUs different architectural design point, one focused executing many parallel threads efficiently many processor cores. using many simpler cores optimizing data-parallel behavior among groups threads, per-chip transistor budget devo ted computation, less on-chip caches overhead. Processor Array unified GPU processor array contains many processor cores, typically organized multithreaded multiprocessors. Figure B.2.5 shows GPU array 112 streaming processor (SP) cores, organized 14 multithreaded streaming multiprocessors (SMs). SP core highly multithreaded, managing 96 concurrent threads state hardware. processors connect four 64-bit-wide DRAM partitions via interconnection network. SM eight SP cores, two special function units (SFUs), instruction constant caches, multithreaded instruction unit, shared memory. basic Tesla architecture implemented NVIDIA GeForce 8800. unified architecture traditional graphics programs vertex, geometry, pixel shading run unified SMs SP cores, computing programs run processors. 1285FIGURE B.2.5 Basic unified GPU architecture. Example GPU 112 streaming processor (SP) cores organized 14 streaming multiprocessors (SMs); cores highly multithreaded. basic Tesla architecture NVIDIA GeForce 8800. processors connect four 64-bit-wide DRAM partitions via interconnection network. SM eight SP cores, two special function units (SFUs), instruction constant caches, multithreaded instruction unit, shared memory. processor array architecture scalable smaller larger GPU configurations scaling number multiprocessors number memory partitions. Figure B.2.5 shows seven clusters two SMs sharing texture unit texture L1 cache. texture unit delivers filtered results SM given set coordinates texture map. filter regions support often overlap successive texture requests, small streaming L1 texture cache effective reduce number requests memory system. processor array connects raster operation processors (ROPs), L2 texture caches, external DRAM memories, system memory via GPU-wide interconnection network. number processors number memories scale design balanced GPU systems different performance market segments. 1286B.3 Programming GPUs Programming multiprocessor GPUs qualitatively different han programming multiprocessors like multicore CPUs. GPUs provide two three orders magnitude thread data parallelism CPUs, scaling hundreds processor cores tens thousands concurrent threads. GPUs continue increas e parallelism, doubling every 12 18 months, enabled Moore’s law [ 1965 ] increasing integrated circuit density improving architectural efficiency. span wide price performance range different market segments, different GPU products implement widely varying numbers processors threads. Yet users expect games, graphics, imaging, computing applications work GPU, regardless many parallel threads executes many parallel processor cores has, expect expensive GPUs (with threads cores) run applications faster. result, GPU programming models application programs designed scale transparently wide range parallelism. driving force behind large number parallel threads cores GPU real-time graphics performance—the need render complex 3D scenes high resolution interactive frame rates, least 60 frames per second. Correspondingly, scalable programming models graphics shading languages Cg (C graphics) HLSL ( high-level shading language ) designed exploit large degrees parallelism via many independent parallel threads scale number processor cores. CUDA scalable parallel programming model similarly enables general parallel computing applications leverage large numbers parallel threads scale number parallel processor cores, transparently application. scalable programming models, programmer writes code single thread, GPU runs myriad thread instances parallel. Programs thus scale transparently wide range hardware parallelism. simple paradigm arose graphics APIs shading languages describe shade one vertex one pixel. remained effective paradigm GPUs rapidly increased parallelism performance since late 1990s. 1287This section briefly describes programming GPUs real-tim e graphics applications using graphics APIs programming languages. describes programming GPUs visual computing general parallel computing applications using C language CUDA programming model. Programming Real-Time Graphics APIs played important role rapid, successful development GPUs processors. two primary standard graphics APIs: OpenGL Direct3D , one Microsoft DirectX multimedia programming interfaces. OpenGL, open standard, originally proposed defined Silicon Graphics Incorporated. ongoing development extension OpenGL standard [ Segal Akeley, 2006 ; Kessenich, 2006 ] managed Khronos, industry consortium. Direct3D [ Blythe, 2006 ], de facto standard, defined evolved forward Microsoft partners. OpenGL Direct3D similarly structured, continue evolve rapidly GPU hardware advances. define logical graphics processing pipeline mapped onto GPU hardware processors, along programming models languages programmable pipeline stages. OpenGL open-standard graphics API. Direct3D graphics API defined Microsoft partners. Logical Graphics Pipeline Figure B.3.1 illustrates Direct3D 10 logical graphics pipeline. OpenGL similar graphics pipeline structure. API logical pipeline provide streaming dataflow infrastructure plumbing programmable shader stages, shown blue. 3D application sends GPU sequence vertices grouped geometric primitives—points, lines, triangles, polygons. 1288input assembler collects vertices primitives. vertex sh ader program executes per-vertex processing, including transform ing vertex 3D position screen position lighting ver tex determine color. geometry shader program executes pe r- primitive processing add drop primitives. setup rasterizer unit generates pixel fragments (fragments potenti al contributions pixels) covered geometric primitiv e. pixel shader program performs per-fragment processing, including interpolating per-fragment parameters, texturing, coloring. Pixel shaders make extensive use sampled filtered lookups large 1D, 2D, 3D arrays called textures , using interpolated floating-point coordinates. Shaders use texture accesses maps, functions, decals, images, data. raster operations processing (or output merger) stage performs Z-bu ffer depth testing stencil testing, may discard hidden pixel fragment replace pixel’s depth fragment’s depth , performs color blending operation combines fragment color pixel color writes pixel th e blended color. texture 1D, 2D, 3D array supports sampled filtered lookups interpolated coordinates. FIGURE B.3.1 Direct3D 10 graphics pipeline. logical pipeline stage maps GPU hardware GPU processor. Programmable shader stages blue, fixed-function blocks white, memory objects gray. stage processes vertex, geometric primitive, pixel streaming dataflow fashion. 1289The graphics API graphics pipeline provide input, output, memory objects, infrastructure shader programs process vertex, primitive, pixel fragment. Graphics Shader Programs Real-time graphics applications use many different shader programs model light interacts different materials render complex lighting shadows. Shading languages based dataflow streaming programming model corresponds logical graphics pipeline. Vertex shader programs map position triangle vertices onto screen, altering position, color, orientation. Typically vertex shader thread inputs floating-point (x, y, z, w) vertex position computes floating-point (x, y, z) screen position. Geometry shader programs operate geometric primitives (such lines triangles) defined multiple vertices, changing generating additional primitives. Pixel fragment shaders “shade” one pixel, computing floating-point red, green, blue, alpha (RGBA) color contribution rendered image pixel samp le (x, y) image position. Shaders (and GPUs) use floating-point arithmetic pixel color calculations eliminate visible artifacts computing extreme range pixel contribut ion values encountered rendering scenes complex light ing, shadows, high dynamic range. three types graphics shaders, many program instances run parallel, independent parallel threads, works independent data, produces independent results, side effects. Independent vertices, primitives, pixels enable th e graphics program run differently sized GPUs process different numbers vertices, primitives, pixels paralle l. Graphics programs thus scale transparently GPUs different amounts parallelism performance. shader program operates graphics data vertex pixel fragment. 1290 shading language graphics rendering language, usually dataflow streaming programming model. Users program three logical graphics threads common targeted high-level language. HLSL (high-level shading language) Cg (C graphics) commonly used. C-like syntax rich set library functions matrix operations, trigonometry, interpolation, texture access filtering, bu far general computing languages: currently lack general memory access, pointers, file I/O, recursion. HLSL Cg assume programs live within logical graphics pipeline, thus I/O implicit. example, pixel fragment shader may expect geometric normal multiple texture coordinates interpolated vertex values upstream fixed- function stages simply assign value COLOR output parameter pass downstream blended pixel implied (x, y) position. GPU hardware creates new independent thread execute vertex, geometry, pixel shader program every vertex, every primitive, every pixel fragment. video games, bulk threads execute pixel shader programs, typically 10 20 times pixel fragments vertices, complex lighting shadows require even larger ratios pixel vertex shader threads. graphics shader programming model drove GPU architecture efficiently execute thousands independent fine- grained threads many parallel processor cores. Pixel Shader Example Consider following Cg pixel shader program implements “environment mapping” rendering technique. pixel thread, shader passed five parameters, including 2D floating- point texture image coordinates needed sample surface col or, 3D floating-point vector giving refection view direction surface. three “uniform” parameters vary one pixel instance (thread) next. shader looks color two texture images: 2D texture access surface color, 3D texture access cube map (six images 1291corresponding faces cube) obtain external world color corresponding refection direction. fin al four- component (red, green, blue, alpha) floating-point color computed using weighted average called “lerp” linear interpolation function. void refection( float2 texCoord : TEXCOORD0, float3 refection_dir : TEXCOORD1, float4 color : COLOR, uniform float shiny, uniform sampler2D surfaceMap, uniform samplerCUBE envMap) { // Fetch surface color texture float4 surfaceColor = tex2D(surfaceMap, texCoord); // Fetch reflected color sampling cube map float4 reflectedColor = texCUBE(environmentMap, refection_dir); // Output weighted average two colors color = lerp(surfaceColor, refectedColor, shiny); } Although shader program three lines long, activates lot GPU hardware. texture fetch, GPU texture subsystem makes multiple memory accesses sample image colors vicinity sampling coordinates, interpolate final result floating-point filtering arithmetic. multithreaded GPU executes thousands lightweight Cg pixel shader threads parallel, deeply interleaving hide texture fetch memory latency. Cg focuses programmer’s view single vertex primitiv e pixel, GPU implements single thread; shader program transparently scales exploit thread parallelism available processors. application-specific, Cg provides ri ch set useful data types, library functions, language constructs express diverse rendering techniques. Figure B.3.2 shows skin rendered fragment pixel shader. Real skin appears quite different flesh-color paint light bounces around lot re-emerging. complex shader, three separate skin layers, unique subsurface scattering behavior, modeled give skin visual depth 1292translucency. Scattering modeled blurring convolution fattened “texture” space, red blurred green, blue blurred less. compiled Cg shader executes 1400 instructions compute color one skin pixel. 1293FIGURE B.3.2 GPU-rendered image. give skin visual depth translucency, pixel shader program models three separate skin layers, unique subsurface scattering behavior. executes 1400 instructions render red, green, blue, alpha color components 1294skin pixel fragment. GPUs evolved superior floating-point performance high streaming memory bandwidth real-time graphics, attracted highly parallel applications beyond traditional graphics. first, access power available couching application graphics-rendering algorithm, GPGPU approach often awkward limiting. recently, CUDA programming model provided far easier way exploit scalable high-performance floating-point memory bandwidth GPUs C programming language. Programming Parallel Computing Applications CUDA, Brook, CAL programming interfaces GPUs focused data parallel computation rather graphics. CAL ( Compute Abstraction Layer ) low-level assembler language interface AMD GPUs. Brook streaming language adapted GPUs Buck et al. [2004] . CUDA, developed NVIDIA [2007] , extension C C+ + languages scalable parallel programming manycore GPUs multicore CPUs. CUDA programming model described below, adapted article Nickolls et al. [2008] . new model GPU excels data parallel throughput computing, executing high-performance computin g applications well graphics applications. Data Parallel Problem Decomposition map large computing problems effectively highly parallel processing architecture, programmer compiler decompos es problem many small problems solved parallel. example, programmer partitions large result data array blocks partitions block elements, result blocks computed independently parallel, elements within block computed parallel. Figure B.3.3 shows decomposition result data array 3×2 grid blocks, block decomposed 5×3 array elements. two-level parallel decomposition 1295maps naturally GPU architecture: parallel multiprocessors compute result blocks, parallel threads compute result elements. 1296FIGURE B.3.3 Decomposing result data grid blocks elements computed parallel. programmer writes program computes sequence result data grids, partitioning result grid coarse-graine result blocks computed independently parallel. T0he program computes result block array fine-grained parallel threads, partitioning work among threads computes one result elements. Scalable Parallel Programming CUDA 1297The CUDA scalable parallel programming model extends C C++ languages exploit large degrees parallelism general applications highly parallel multiprocessors, particularly GP Us. Early experience CUDA shows many sophisticated programs readily expressed easily understood abstractions. Since NVIDIA released CUDA 2007, developers rapidly developed scalable parallel programs wide range applications, including seismic data processing, computational chemistry, linear algebra, sparse matrix solvers, sorting, searching , physics models, visual computing. applications scale transparently hundreds processor cores thousands concurrent threads. NVIDIA GPUs Tesla unified graphics computing architecture (described Sections B.4 B.7) run CUDA C programs, widely available laptops, PCs, workstations, servers. CUDA model also applicable shared memory parallel processing architectures, including multicore CPUs. CUDA provides three key abstractions—a hierarchy thread groups, shared memories , barrier synchronization —that provide clear parallel structure conventional C code one thread th e hierarchy. Multiple levels threads, memory, synchronization provide fine-grained data parallelism thread parallelism, nested within coarse-grained data parallelism task parallelism. abstractions guide programmer partition problem coarse subproblems solved independently parallel, finer pieces solved parallel. programming model scales transparently large numbers processor cores: compiled CUDA program executes number processors, runtime system needs know physical processor count. CUDA Paradigm CUDA minimal extension C C++ programming languages. programmer writes serial program calls parallel kernels , may simple functions full programs. kernel executes parallel across set parallel threads. programmer organizes threads hierarchy thread blocks grids thread blocks. thread block set concurrent threads cooperate among 1298barrier synchronization shared access memory space private block. grid set thread blocks may executed independently thus may execute parallel. kernel program function one thread, designed executed many threads. thread block set concurrent threads execute thread program may cooperate compute result. grid set thread blocks execute kernel program. invoking kernel, programmer specifies number threads per block number blocks comprising grid. thread given unique thread ID number threadIdx within thread block, numbered 0, 1, 2, …, blockDim-1 , thread block given unique block ID number blockIdx within grid. CUDA supports thread blocks containing 512 threads. convenience, thread blocks grids may one, two, three dimensions, accessed via .x, .y, .z index fields. simple example parallel programming, suppose given two vectors x n floating-point numbers wish compute result y=ax+y scalar value a. so-called SAXPY kernel defined BLAS linear algebra library. Figure B.3.4 shows C code performing computation serial processor parallel using CUDA. 1299FIGURE B.3.4 Sequential code (top) C versus parallel code (bottom) CUDA SAXPY (see Chapter 6 ). CUDA parallel threads replace C serial loop—each thread computes result one loop iteration. parallel code computes n results n threads organized blocks 256 threads. __global__ declaration specifier indicates procedure kernel entry point. CUDA programs launch parallel kernels extended function call syntax: kernel<<<dimGrid, dimBlock>>>(… parameter list …); dimGrid dimBlock three-element vectors type dim3 specify dimensions grid blocks dimensions blocks threads, respectively. Unspecified dimensions default one. Figure B.3.4 , launch grid n threads assigns one thread element vectors puts 256 threads block. individual thread computes element index thread block IDs performs desired calculation corresponding vector elements. Comparing serial 1300parallel versions code, see strikingly simi lar. represents fairly common pattern. serial code consists f loop iteration independent others. loops mechanically transformed parallel kernels: loop iteration becomes independent thread. assigning single thread output element, avoid need synchronization among threads writing results memory. text CUDA kernel simply C function one sequential thread. Thus, generally straightforward write typically simpler writing parallel code vector operat ions. Parallelism determined clearly explicitly specifying dimensions grid thread blocks launching kernel. Parallel execution thread management automatic. thread creation, scheduling, termination handled programmer underlying system. Indeed, Tesla architecture GPU performs thread management directly hardware. threads block execute concurrently may synchronize synchronization barrier calling __syncthreads() intrinsic. guarantees thread block proceed threads block reached barrier. passing barrier, threads also guaranteed see writes memory performed threads block barrier. Thu s, threads block may communicate writing reading per-block shared memory synchronization barrier. synchronization barrier Threads wait synchronization barrier threads thread block arrive barrier. Since threads block may share memory synchronize via barriers, reside together physical process multiprocessor. number thread blocks can, however, greatly exceed number processors. CUDA thread programming model virtualizes processors gives programmer flexibility parallelize whatever granularity conveni ent. Virtualization threads thread blocks allows intuitive problem decompositions, number blocks dictated size data processed rather number 1301processors system. also allows CUDA program scale widely varying numbers processor cores. manage processing element virtualization provide scalability, CUDA requires thread blocks able execute independently. must possible execute blocks ord er, parallel series. Different blocks means direct communication, although may coordinate activities using atomic memory operations global memory visible threads—by atomically incrementing queue pointers, example. independence requirement allows thread blocks scheduled order across number cores, making CUDA model scalable across arbitrary number cores well across variety parallel architectures. also helps avoid possibility deadlock. application may execute multiple grid either independently dependently. Independent grids may execute concurrently, given sufficient hardware resources. Dependent grids execute sequentially, implicit interke rnel barrier them, thus guaranteeing blocks first grid complete block second, dependent grid begins. atomic memory operation memory read, modify, write operation sequence completes without intervening access. Threads may access data multiple memory spaces execution. thread private local memory . CUDA uses local memory thread-private variables fit thread’s registers, well stack frames register spillin g. thread block shared memory , visible threads block, lifetime block. Finally, threads access global memory . Programs declare variables shared global memory __shared__ __device__ type qualifers. Tesla architecture GPU, memory spaces correspond physically separate memories: per- block shared memory low-latency on-chip RAM, global memory resides fast DRAM graphics board. 1302 local memory Per-thread local memory private thread. shared memory Per-block memory shared threads block. global memory Per-application memory shared threads. Shared memory expected low-latency memory near processor, much like L1 cache. therefore provide high-performance communication data sharing among threads thread block. Since lifetime corresponding thread block, kernel code typically initiali ze data shared variables, compute using shared variables, copy shared memory results global memory. Thread blocks sequentially dependent grids communicate via global memory, using read input write results. Figure B.3.5 shows diagrams nested levels threads, thread blocks, grids thread blocks. shows corresponding levels memory sharing: local, shared, global memories per-thread, per-thread-block, per-application data sharing. 1303FIGURE B.3.5 Nested granularity levels—thread, thread block, grid—have corresponding memory sharing levels—local, shared, global. Per-thread local memory private thread. Per- block shared memory shared threads block. Per-application global memory shared threads. program manages global memory space visible kernels calls CUDA runtime, cudaMalloc() cudaFree() . Kernels may execute physically separate device, case running kernels GPU. Consequently, application must use cudaMemcpy() copy data allocated space host system memory. CUDA programming model similar style familiar single- program multiple data (SPMD) model—it expresses parallelism explicitly, kernel executes fixed number threads. However, CUDA flexible realizations 1304of SPMD, kernel call dynamically creates new grid right number thread blocks threads application step. programmer use convenient degree parallelism kernel, rather design phases computation use number threads. Figure B.3.6 shows example SPMD-like CUDA code sequence. first instantiates kernelF 2D grid 3×2 blocks 2D thread block consists 5×3 threads. instantiates kernelG 1D grid four 1D thread blocks six threads each. kernelG depends results kernelF , separated interkernel synchronization barrier. single-program multiple data (SPMD) style parallel programming model threads execute program. SPMD threads typically coordinate barrier synchronization. 1305FIGURE B.3.6 Sequence kernel F instantiated 2D grid 2D thread blocks, interkernel synchronization barrier, followed kernel G 1D grid 1D thread blocks. concurrent threads thread block express fine-grained data parallelism thread parallelism. independent thread blocks grid express coarse-grained data parallelism. Independent grids express coarse-grained task parallelism. kern el simply C code one thread hierarchy. Restrictions efficiency, simplify implementation, CUDA 1306programming model restrictions. Threads thread blocks may created invoking parallel kernel, within parallel kernel. Together required independenc e thread blocks, makes possible execute CUDA programs simple scheduler introduces minimal runtime overhe ad. fact, Tesla GPU architecture implements hardware management scheduling threads thread blocks. Task parallelism expressed thread block level difficult express within thread block thread synchronization barriers operate threads block. enable CUDA programs run number processors, dependencies among thread blocks within kernel grid allowed—blocks must execute independently. Since CUDA requires thread blocks independent allows blocks executed order, combining results generated multiple blocks must general done launching second kernel new grid thread blocks (although thread blocks may coordinate activities using atomic memory operations global memory visible threads—by atomically incrementing queue pointers, example). Recursive function calls currently allowed CUDA kernels. Recursion unattractive massively parallel kernel, providing stack space tens thousands threads may active would require substantial amounts memory. Serial algorithms normally expressed using recursion, suc h quicksort, typically best implemented using nested data parallelism rather explicit recursion. support heterogeneous system architecture combining CPU GPU, memory system, CUDA programs must copy data results host memory device memory. overhead CPU–GPU interaction data transfers minimized using DMA block transfer engines fast interconnects. Compute-intensive problems large enough need GPU performance boost amortize overhead better small problems. Implications Architecture parallel programming models graphics computing 1307have driven GPU architecture different CPU architecture . key aspects GPU programs driving GPU processor architecture are: Extensive use fine-grained data parallelism: Shader programs describe process single pixel vertex, CUDA programs describe compute individual result. Highly threaded programming model: shader thread program processes single pixel vertex, CUDA thread program may generate single result. GPU must create execute millions thread programs per frame, 60 frames per second. Scalability: program must automatically increase performance provided additional processors, without recompiling. Intensive floating-point (or integer) computation . Support high-throughput computations . B.4 Multithreaded Multiprocessor Architecture address different market segments, GPUs implement scalable numbers multi-processors—in fact, GPUs multiprocessors composed multiprocessors. Furthermore, multiprocess highly multithreaded execute many fine-grained vertex pi xel shader threads efficiently. quality basic GPU two four multiprocessors, gaming enthusiast’s GPU computing platform dozens them. section looks architecture one multithreaded multiprocessor, simplified versio n NVIDIA Tesla streaming multiprocessor (SM) described Section B.7. use multiprocessor, rather several independent processors? parallelism within multiprocessor provi des localized high performance supports extensive multithreadin g fine-grained parallel programming models described Section B.3 . individual threads thread block execute together within multiprocessor share data. multithreaded multiprocessor design describe eight scalar proces sor cores tightly coupled architecture, executes 512 1308threads (the SM described Section B.7 executes 768 threads). area power efficiency, multiprocessor shares large complex units among eight processor cores, including instruction cache, multithreaded instruction unit, shared memory RAM. Massive Multithreading GPU processors highly multithreaded achieve several goals: Cover latency memory loads texture fetches DRAM Support fine-grained parallel graphics shader programming models Support fine-grained parallel computing programming models Virtualize physical processors threads thread blocks provide transparent scalability Simplify parallel programming model writing serial program one thread Memory texture fetch latency require hundreds processor clocks, GPUs typically small streaming caches rather large working-set caches like CPUs. fetch request generally requires full DRAM access latency plus interconnect buffering latency. Multithreading helps cover latency useful computing—while one thread waiting load texture fetch complete, processor execute anot thread. fine-grained parallel programming models provide literally thousands independent threads keep many processors busy despite long memory latency seen individual threads. graphics vertex pixel shader program program single thread processes vertex pixel. Similarly, CUDA program C program single thread computes result. Graphics computing programs instantiate many parallel threads render complex images compute large result arrays. dynamically balance shifting vertex pixel shader thread workloads, multiprocessor concurrently executes multip le different thread programs different types shader programs . support independent vertex, primitive, pixel programming model graphics shading languages single- 1309thread programming model CUDA C/C+ +, GPU thread private registers, private per-thread memory, program counter, thread execution state, execute independent code path. efficiently execute hundreds concurrent lightweight threads, GPU multiprocessor hardware multithreaded—it manages executes hundreds concurrent threads hardware without scheduling overhead. Concurrent threads within thread blocks synchronize barrier single instruction. Lightweight thread creation, zer o- overhead thread scheduling, fast barrier synchronization efficiently support fine-grained parallelism. Multiprocessor Architecture unified graphics computing multiprocessor executes ver tex, geometry, pixel fragment shader programs, parallel computing programs. Figure B.4.1 shows, example multiprocessor consists eight scalar processor (SP) cores large multithreaded register file (RF), two special function units (SFUs), multithreaded instruction unit, instruction cache, read-only constant cache, shared memory. 1310FIGURE B.4.1 Multithreaded multiprocessor eight scalar processor (SP) cores. eight SP cores large multithreaded register file (RF) share instruction cache, multithreaded instruction issue unit, constant cache, two special function units (SFUs), interconnection network, multibank shared memory. 16 KB shared memory holds graphics data buffers shared computing data. CUDA variables declared __shared__ reside shared memory. map logical graphics pipeline workload multiprocessor multiple times, shown Section B.2 , vertex, geometry, pixel threads independent input output buffers, workloads arrive depart independently thread execution. SP core contains scalar integer floating-point arithmetic units execute instructions. SP hardware multithreaded, supporting 64 threads. pipelined SP core executes one scalar instruction per thread per clock, range 1.2 GHz 1.6 GHz different GPU products. SP core large RF 1024 general-purpose 32-bit registers, partitioned among assigned threads. Programs declare register 1311demand, typically 16 64 scalar 32-bit registers per thread. SP concurrently run many threads use registers fewer threads use registers. compiler optimizes register allocation balance cost spilling registers versus co st fewer threads. Pixel shader programs often use 16 fewer registers, enabling SP run 64 pixel shader threads cover long-latency texture fetches. Compiled CUDA programs often need 32 registers per thread, limiting SP 32 threads, limits kernel program 256 threads per thread block example multiprocessor, rather maximum 512 threads. pipelined SFUs execute thread instructions compute special functions interpolate pixel attributes primiti vertex attributes. instructions execute concurrently wi th instructions SPs. SFU described later. multiprocessor executes texture fetch instructions texture unit via texture interface, uses memory inter face external memory load, store, atomic access instructions. instructions execute concurrently instructi ons SPs. Shared memory access uses low-latency interconnection network SP processors shared memory banks. Single-Instruction Multiple-Thread (SIMT) manage execute hundreds threads running several different programs efficiently, multiprocessor employs single- instruction multiple-thread (SIMT) architecture. creates, manages, schedules, executes concurrent threads groups parallel threads called warps . term warp originates weaving, first parallel thread technology. photograph Figure B.4.2 shows warp parallel threads emerging loom. example multiprocessor uses SIMT warp size 32 threads, executing four threads eight SP cores four clocks. Tesla SM multiprocessor described Section B.7 also uses warp size 32 parallel threads, executing four threads per SP core efficiency plentiful pixel threads computi ng threads. Thread blocks consist one warps. single-instruction multiple-thread (SIMT) 1312A processor architecture applies one instruction multi ple independent threads parallel. warp set parallel threads execute instruction together SIMT architecture. 13131314FIGURE B.4.2 SIMT multithreaded warp scheduling. scheduler selects ready warp issues instruction synchronously parallel threads composing warp. warps independent, scheduler may select different warp time. example SIMT multiprocessor manages pool 16 warps, total 512 threads. Individual parallel threads composing warp type start together program address, otherwise free branch execute independently. instruction issue time, SIMT multithreaded instructi unit selects warp ready execute next instruction, issues instruction active threads warp. SIMT instruction broadcast synchronously active parallel thr eads warp; individual threads may inactive due independent branching predication. multiprocessor, SP scalar processor core executes instruction four individual thr eads warp using four clocks, reflecting 4:1 ratio warp threads cores. SIMT processor architecture akin single-instruction multiple data (SIMD) design, applies one instruction multiple data lanes, differs SIMT applies one instruction multipl e independent threads parallel, multiple data lanes. instruction SIMD processor controls vector multiple ata lanes together, whereas instruction SIMT processor contro ls individual thread, SIMT instruction unit issues instruction warp independent parallel threads efficie ncy. SIMT processor finds data-level parallelism among threads runtime, analogous way superscalar processor finds instruction-level parallelism among instructions runtime. SIMT processor realizes full efficiency performance threads warp take execution path. threads warp diverge via data-dependent conditional branch, execution serializes branch path taken, paths complete, threads converge execution path. equal length paths, divergent if-else code block 50% efficient. multiprocessor uses branch synchronization stack manage independent threads diverge converge. Different warps 1315execute independently full speed regardless whether executing common disjoint code paths. result, SIMT GPUs dramatically efficient flexible branching code earlier GPUs, warps much narrower SIMD width prior GPUs. contrast SIMD vector architectures, SIMT enables programmers write thread-level parallel code individual independent threads, well data-parallel code many coordinated threads. program correctness, programmer essentially ignore SIMT execution attributes warps; howev er, substantial performance improvements realized taking care code seldom requires threads warp diverge. practice, analogous role cache lines traditional codes: cache line size safely ignored designing correctness must considered code structure whe n designing peak performance. SIMT Warp Execution Divergence SIMT approach scheduling independent warps flexible scheduling previous GPU architectures. w arp comprises parallel threads type: vertex, geometry, pixe l, compute. basic unit pixel fragment shader processing 2-by-2 pixel quad implemented four pixel shader threads. multiprocessor controller packs pixel quads warp. similarly groups vertices primitives warps, packs computing threads warp. thread block comprises one warps. SIMT design shares instruction fetch issue unit efficiently across parallel threads warp, requires ful l warp active threads get full performance efficiency. unified multiprocessor schedules executes multipl e warp types concurrently, allowing concurrently execute rtex pixel warps. warp scheduler operates less processor clock rate, four thread lanes per processor core. scheduling cycle, selects warp execute SIMT warp instruction, shown Figure B.4.2 . issued warp-instruction executes four sets eight threads ver four processor cycles throughput. processor pipeline uses several clocks latency complete instruction. num ber 1316of active warps times clocks per warp exceeds pipeline latency, programmer ignore pipeline latency. multiprocessor, round-robin schedule eight warps period 32 cycles successive instructions warp. program keep 256 threads active per multiprocessor, instruction latencies 32 cycles hidden individual sequential thread. However, active warps, processor pipeline depth becomes visible may cause processo rs stall. challenging design problem implementing zero-overhead warp scheduling dynamic mix different warp programs program types. instruction scheduler must select warp every four clocks issue one instruction per clock per thread, equi valent IPC 1.0 per processor core. warps independent, dependences among sequential instructions warp. scheduler uses register dependency scoreboard qualify warps whose active threads ready execute instruction. prioritizes ready warps selects highest priority one issue. Prioritization must consider w arp type, instruction type, desire fair active warps. Managing Threads Thread Blocks multiprocessor controller instruction unit manage threads thread blocks. controller accepts work requests input data arbitrates access shared resources, including texture unit, memory access path, I/O paths. graphics workloads, creates manages three types graphics threads concurrently: vertex, geometry, pixel. graphics work types independent input output paths. accumulates packs input work types SIMT warps parallel threads executing thread program. allocates free warp, allocates registers warp threads, starts warp execution multiprocessor. Every program declares per-thread register demand; controller starts warp allocate requested register count warp threads. threads warp exit, controller unpacks results frees warp registers resources. controller creates cooperative thread arrays (CTAs) 1317implement CUDA thread blocks one warps parallel threads. creates CTA create CTA warps allocate CTA resources. addition threads registers, CTA requires allocating shared memory barriers. program declares required capacities, controller waits allocate amounts launching CTA. creates CTA warps warp scheduling rate, CTA program starts executing immediately full multiprocessor performanc e. controller monitors threads CTA exited, frees CTA shared resources warp resources. cooperative thread array (CTA) set concurrent threads executes thread program may cooperate compute result. GPU CTA implements CUDA thread block. Thread Instructions SP thread processors execute scalar instructions indivi dual threads, unlike earlier GPU vector instruction architectures, wh ich executed four-component vector instructions verte x pixel shader program. Vertex programs generally compute (x, y, z, w) position vectors, pixel shader programs compute (red, green, blue, alpha) color vectors. However, shader programs becoming longer scalar, increasingly difficult fully occupy even two components legacy GPU four- component vector architecture. effect, SIMT architectur e parallelizes across 32 independent pixel threads, rather parallelizing four vector components within pixel. CUDA C/C++ programs predominantly scalar code per thread. Previous GPUs employed vector packing (e.g., combining subvectors work gain efficiency) complicated scheduling hardware well compiler. Scalar instructions simpler compiler-friendly. Texture instructions remain v ector- based, taking source coordinate vector returning filtered color vector. support multiple GPUs different binary microinstructi formats, high-level graphics computing language compilers generate intermediate assembler-level instructions (e.g., Dire ct3D 1318vector instructions PTX scalar instructions), optimized translated binary GPU microinstructions. NVIDIA PTX (parallel thread execution) instruction set definit ion [2007] provides stable target ISA compilers, provides compatibility several generations GPUs evolving binary microinstruction-set architectures. optimizer read ily expands Direct3D vector instructions multiple scalar binary microinstructions. PTX scalar instructions translate nearly one one scalar binary microinstructions, although PTX instructions expand multiple binary microinstructions, multiple PTX instructions may fold one binary microinstruction. intermediate assembler-level instructions use virtual registers, optimizer analyzes data dependencies allocates real registers. optimizer eliminate dead code, folds instructions together feasible, optimize SIMT branch diverge converge points. Instruction Set Architecture (ISA) thread ISA described simplified version Tes la architecture PTX ISA, register-based scalar instruction set comprising floating-point, integer, logical, conversion, special functions, flow control, memory access, texture operations. Figure B.4.3 lists basic PTX GPU thread instructions; see NVIDIA PTX specification [2007] details. instruction format is: 1319FIGURE B.4.3 Basic PTX GPU thread instructions. opcode.type d, a, b, c; destination operand, a, b, c source operands, .type one of: 1320Source operands scalar 32-bit 64-bit values registers, immediate value, constant; predicate operands 1-bit Boolean values. Destinations registers, except store memory. Instructions predicated prefixing @p @!p, p predicate register. Memory texture instructions transfer scalars vectors two four componen ts, 128 bits total. PTX instructions specify behavior one thread. PTX arithmetic instructions operate 32-bit 64-bit floating-point, signed integer, unsigned integer types. Rece nt GPUs support 64-bit double-precision floating-point; see Section B.6. current GPUs, PTX 64-bit integer logical instructions translated two binary microinstructions perfor 32-bit operations. GPU special function instructions limi ted 32-bit floating-point. thread control flow instructions conditional branch , function call return , thread exit , bar.sync (barrier synchronization). conditional branch instruction @p bra target uses predicate register p (or !p) previously set compare set predicate setp instruction determine whether thread takes branch not. instructions also predicated predicate register true false. Memory Access Instructions tex instruction fetches filters texture samples 1D, 2D, 3D texture arrays memory via texture subsystem. Texture fetches generally use interpolated floating-point coordinates address texture. graphics pixel shader thread computes pixel fragment color, raster operations processor blends pixel color assigned (x, y) pi xel position writes final color memory. support computing C/C++ language needs, Tesla PTX ISA implements memory load/store instructions. uses integ er byte addressing register plus offset address arithmetic facilitate conventional compiler code optimizations. Memory load/store instructions common processors, significant new capability Tesla architecture GPUs, prior GPUs provided texture pixel accesses required 1321graphics APIs. computing, load/store instructions access three read/w rite memory spaces implement corresponding CUDA memory spaces Section B.3 : Local memory per-thread private addressable temporary data (implemented external DRAM) Shared memory low-latency access data shared cooperating threads CTA/thread block (implemented on-chip SRAM) Global memory large data sets shared threads computing application (implemented external DRAM) memory load/store instructions ld.global , st.global , ld.shared , st.shared , ld.local , st.local access global, shared, local memory spaces. Computing programs use fast barrier synchronization instruction bar.sync synchronize threads within CTA/thread block communicate via shared global memory. improve memory bandwidth reduce overhead, local global load/store instructions coalesce individual parallel thread requests SIMT warp together single memory block request addresses fall block meet alignment criteria. Coalescing memory requests provides significant performance boost separate requests individual threads. multiprocessor’s large thread count, together support many outstanding load requests, helps cover load-to-use latency local global memory implemente external DRAM. latest Tesla architecture GPUs also provide efficient atomic memory operations memory atom.op.u32 instructions, including integer operations add, min, max, and, or, xor, exchange, cas (compare-and-swap) operations, facilitating parallel reductions parallel data structure management. Barrier Synchronization Thread Communication Fast barrier synchronization permits CUDA programs communicate frequently via shared memory global memory simply calling __syncthreads(); part interthread communication step. synchronization intrinsic function 1322generates single bar.sync instruction. However, implementing fast barrier synchronization among 512 threads per CUDA thread block challenge. Grouping threads SIMT warps 32 threads reduces synchronization difficulty factor 32. Threads wait barrier SIMT thread scheduler consume processor cycles waiting. thread executes bar.sync instruction, increments barrier’s thread arrival counter scheduler marks thread waiting barrier. CTA threads arrive, barrier counter matches expected terminal count, scheduler releases threads waiting barrier resumes executing threads. Streaming Processor (SP) multithreaded streaming processor (SP) core primary thread instruction processor multiprocessor. register file (RF) provides 1024 scalar 32-bit registers 64 threads. executes fundamental floating-point operations, includi ng add.f32 , mul.f32 , mad.f32 (floating multiply-add), min.f32 , max.f32 , setp.f32 (floating compare set predicate). floating- point add multiply operations compatible IEEE 754 standard single-precision FP numbers, including not-a- number (NaN) infinity values. SP core also implements 32-bit 64-bit integer arithmetic, comparison, conversion, logical PTX instructions shown Figure B.4.3 . floating-point add mul operations employ IEEE round-to- nearest-even default rounding mode. mad.f32 floating- point multiply-add operation performs multiplication truncation, followed addition round-to-nearest-even . SP flushes input denormal operands sign-preserved-zero . Results underflow target output exponent range flushed sign-preserved-zero rounding. Special Function Unit (SFU) Certain thread instructions execute SFUs, concurrent ly thread instructions executing SPs. SFU implements special function instructions Figure B.4.3 , 1323compute 32-bit floating-point approximations reciprocal, reciprocal square root, key transcendental functions. also implements 32-bit floating-point planar attribute interpolation fo r pixel shaders, providing accurate interpolation attributes su ch color, depth, texture coordinates. pipelined SFU generates one 32-bit floating-point special function result per cycle; two SFUs per multiprocessor ex ecute special function instructions quarter simple instructi rate eight SPs. SFUs also execute mul.f32 multiply instruction concurrently eight SPs, increasing pe ak computation rate 50% threads suitable instruction mixture. functional evaluation, Tesla architecture SFU employs quadratic interpolation based enhanced minimax approximations approximating reciprocal, reciprocal square - root, log2x, 2x, sin/cos functions. accuracy function estimates ranges 22 24 mantissa bits. See Section B.6 details SFU arithmetic. Comparing Multiprocessors Compared SIMD vector architectures x86 SSE, SIMT multiprocessor execute individual threads independen tly, rather always executing together synchronous groups. SIMT hardware finds data parallelism among independent threads, whereas SIMD hardware requires software express data parallelism explicitly vector instruction. SIMT machine executes warp 32 threads synchronously threads take execution path, yet execute thread independently diverge. advantage significant SIMT programs instructions simply describe behavior single independent thread, rather SIMD data vector four data lanes. Yet SIMT multiprocessor SIMD-like efficiency , spreading area cost one instruction unit across 32 threads warp across eight streaming processor cores. SIMT provides performance SIMD together productivity multithreading, avoiding need explicitl code SIMD vectors edge conditions partial divergence. SIMT multiprocessor imposes little overhead 1324hardware multithreaded hardware barrier synchronization. allows graphics shaders CUDA threads express fine-grained parallelism. Graphics CUDA programs use threads express fine-grained data parallelism per-thread program, rather forcing programmer express SIMD vector instructions. simpler productive deve lop scalar single-thread code vector code, SIMT multiprocessor executes code SIMD-like efficienc y. Coupling eight streaming processor cores together closely nto multiprocessor implementing scalable number multiprocessors makes two-level multiprocessor composed f multiprocessors. CUDA programming model exploits two - level hierarchy providing individual threads fine-grain ed parallel computations, providing grids thread blocks coarse-grained parallel operations. thread program provide fine-grained coarse-grained operations. contrast, CPUs SIMD vector instructions must use two different programming models provide fine-grained coarse - grained operations: coarse-grained parallel threads different cores, SIMD vector instructions fine-grained data parallelism. Multithreaded Multiprocessor Conclusion example GPU multiprocessor based Tesla architecture highly multithreaded, executing total 512 lightweight threads concurrently support fine-grained pixel shaders CUDA threads. uses variation SIMD architecture multithreading called SIMT ( single-instruction multiple-thread ) efficiently broadcast one instruction warp 32 parallel threads, permitting thread branch execute independentl y. thread executes instruction stream one eight streaming processor (SP) cores, multithreaded 64 threads. PTX ISA register-based load/store scalar ISA describes execution single thread. PTX instruct ions optimized translated binary microinstructions specific GPU, hardware instructions evolve rapidly witho ut disrupting compilers software tools generate PTX 1325instructions. B.5 Parallel Memory System Outside GPU itself, memory subsystem important determiner performance graphics system. Graphics workloads demand high transfer rates memory. Pixel write blend (read-modify-write) operations, depth buffer reads writes, texture map reads, well command object vertex attribute data reads, comprise majority memory traffic. Modern GPUs highly parallel, shown Figure B.2.5 . example, GeForce 8800 process 32 pixels per clock, 600 MHz. pixel typically requires color read write depth read write 4-byte pixel. Usually average two three texels four bytes read generate pixel’s co lor. typical case, demand 28 bytes times 32 pixels = 896 bytes per clock. Clearly bandwidth demand memory system enormous. supply requirements, GPU memory systems following characteristics: wide, meaning large number pins convey data GPU memory devices, memory array comprises many DRAM chips provide full total data bus width. fast, meaning aggressive signaling techniques used maximize data rate (bits/second) per pin. GPUs seek use every available cycle transfer data memory array. achieve this, GPUs specifically aim minimize latency memory system. High throughput (utilization efficiency) short latency fundamentally conflict. Compression techniques used, lossy, programmer must aware, lossless, invisible application opportunistic. Caches work coalescing structures used reduce amount off-chip traffic needed ensure cycles spent moving data used fully possible. 1326DRAM Considerations GPUs must take account unique characteristics DRAM. DRAM chips internally arranged multiple (typically four eight) banks, bank includes power-of-2 number rows (typically around 16,384), row contains power-of-2 number bits (typically 8192). DRAMs impose variety timing requirements controlling processor. example, dozens cycles required activate one row, activated, bits within row randomly accessible new column address every four clocks. Double-data rate (DDR) synchronous DRAMs transfer data rising falling edges interface clock (see Chapter 5 ). 1 GHz clocked DDR DRAM transfers data 2 gigabits per second per data pin. Graphics DDR DRAMs usually 32 bidirectional data pins, eight bytes read written DRAM per clock. GPUs internally large number generators memory traffic. Different stages logical graphics pipeline request streams: command vertex attribute fetch, shader texture fetch load/store, pixel depth color read- write. logical stage, often multiple independent units deliver parallel throughput. independent memory requestors. viewed memory system, enormous number uncorrelated requests flight. natural mismatch reference pattern preferre DRAMs. solution GPU’s memory controller maintain separate heaps traffic bound different DRAM banks, wait enough traffic particular DRAM row pending activating row transferring traffic once. Note accumulating pending requests, good DRAM row locality thus efficient use data bus, leads longer average latency seen requestors whose requests spend time waiting others. design must take care particular request waits long, otherwise processing un starve waiting data ultimately cause neighboring processors become idle. GPU memory subsystems arranged multiple memory partitions , comprises fully independent memory controller one two DRAM devices fully 1327exclusively owned partition. achieve best load balance therefore approach theoretical performance n partitions, addresses finely interleaved evenly across memory partitions. partition interleaving stride typical ly block hundred bytes. number memory partitions designed balance number processors memory requesters. Caches GPU workloads typically large working sets—on order hundreds megabytes generate single graphics frame. Unlike CPUs, practical construct caches chips large enough hold anything close full working se graphics application. Whereas CPUs assume high cache hit rates (99.9% more), GPUs experience hit rates closer 90% must therefore cope many misses flight. CPU reasonably designed halt waiting rare cache miss, GPU needs proceed misses hits intermingled. call streaming cache architecture . GPU caches must deliver high-bandwidth clients. Consider case texture cache. typical texture unit may evaluate two bilinear interpolations four pixels per c lock cycle, GPU may many texture units operating independently. bilinear interpolation requires four separ ate texels, texel might 64-bit value. Four 16-bit components typical. Thus, total bandwidth 2×4×4×64=2048 bits per clock. separate 64-bit texel independently addressed, cache needs handle 32 unique addresses per clock. naturally favors multibank and/or multiport arrangement SRAM arrays. MMU Modern GPUs capable translating virtual addresses physical addresses. GeForce 8800, processing units generate memory addresses 40-bit virtual address space. computing, load store thread instructions use 32-bit byte addresses, extended 40-bit virtual address adding 1328a 40-bit offset. memory management unit performs virtual physical address translation; hardware reads page tables local memory respond misses behalf hierarchy translation lookaside buffers spread among processors rendering engines. addition physical page bits, GPU page table entries specify compression algorithm page. Page sizes range 4 128 kilobytes. Memory Spaces introduced Section B.3 , CUDA exposes different memory spaces allow programmer store data values performance-optimal way. following discussion, NVIDIA Tesla architecture GPUs assumed. Global memory Global memory stored external DRAM; local one physical streaming multiprocessor (SM) meant communication among different CTAs (thread blocks) differen grids. fact, many CTAs reference location global memory may executing GPU time; design, CUDA programmer know relative order CTAs executed. address space evenly distributed among memory partitions, must read/write path streaming multiprocessor DRAM partition. Access global memory different threads (and different processors) guaranteed sequential consistency. Thread programs see relaxed memory ordering model. Within thread, order memory reads writes address preserved, order accesses different addresses ay preserved. Memory reads writes requested differen threads unordered. Within CTA, barrier synchronization instruction bar.sync used obtain strict memory ordering among threads CTA. membar thread instruction provides memory barrier/fence operation commits prior memory accesses makes visible threads proceeding. Threads also use atomic memory operations 1329described Section B.4 coordinate work memory share. Shared memory Per-CTA shared memory visible threads belong CTA, shared memory occupies storage time CTA created time terminates. Shared memory therefore reside on-chip. approach many benefits. First, shared memory traffc need compete limited off- chip bandwidth needed global memory references. Second, practical build high-bandwidth memory structures on-c hip support read/write demands streaming multiprocessor. fact, shared memory closely coupled streaming multiprocessor. streaming multiprocessor contains eight physical thread processors. one shared memory clock cycle, thread processor process two threads’ worth instructions, 16 threads’ worth shared memory requests must handled clock. thread generate addresses, addresses typically unique, shared memory built using 16 independently addressable SRAM banks. common access patterns, 16 banks sufficient maintain throughput, pathological cases possible; example, 16 threads might happen access different address one SRAM bank. must possible route request thread lane bank SRAM, 16-by-16 interconnection network required. Local Memory Per-thread local memory private memory visible singl e thread. Local memory architecturally larger thread’s register file, program compute addresses local memory. support large allocations local memory (recall total allocation per-thread allocation times number active threads), local memory allocated external DRAM. Although global per-thread local memory reside off-chip, well-suited cached on-chip. Constant Memory 1330Constant memory read-only program running SM (it written via commands GPU). stored external DRAM cached SM. commonly threads SIMT warp read address constant memory, single address lookup per clock sufficient. constant cache designed broadcast scalar values threads warp. Texture Memory Texture memory holds large read-only arrays data. Textures computing attributes capabilities textures used 3D graphics. Although textures commonly two- dimensional images (2D arrays pixel values), 1D (linear) 3D (volume) textures also available. compute program references texture using tex instruction. Operands include identifier name texture, one, two, three coordinates based texture dimensionality. float ing- point coordinates include fractional portion specifies sample location, often in-between texel locations. Noninteger coordinates invoke bilinear weighted interpolation four closest values (for 2D texture) result returned program. Texture fetches cached streaming cache hierarchy designed optimize throughput texture fetches thous ands concurrent threads. programs use texture fetches way cache global memory. Surfaces Surface generic term one-dimensional, two-dimensional, three-dimensional array pixel values associated format. variety formats defined; example, pixel may defined four 8-bit RGBA integer components, four 16-bit floating-poin components. program kernel need know surface type. tex instruction recasts result values floating-point, depending surface format. Load/Store Access 1331Load/store instructions integer byte addressing enable th e writing compiling programs conventional languages like C C++. CUDA programs use load/store instructions access memory. improve memory bandwidth reduce overhead, local global load/store instructions coalesce individual parallel thread requests warp together single memory block request addresses fall block meet alignment criteria. Coalescing individual small memory requests large block requests provides significant performance bo ost separate requests. large thread count, together support many outstanding load requests, helps cover load-to- use latency local global memory implemented external DRAM. ROP shown Figure B.2.5 , NVIDIA Tesla architecture GPUs comprise scalable streaming processor array (SPA), performs GPU’s programmable calculations, scalable memory system, comprises external DRAM control fixed functi Raster Operation Processors (ROPs) perform color depth framebuffer operations directly memory. ROP unit paired specific memory partition. ROP partitions fed SMs via interconnection network. ROP responsible depth stencil tests updates, well color blending. ROP memory controllers cooperate implemen lossless color depth compression (up 8:1) reduce extern al bandwidth demand. ROP units also perform atomic operations memory. B.6 Floating-point Arithmetic GPUs today perform arithmetic operations programmable processor cores using IEEE 754-compatible single precision 32-bit floating-point operations (see Chapter 3 ). fixed- point arithmetic early GPUs succeeded 16-bit, 24-bit, 32-bit floating-point, IEEE 754-compatible 32-bit floating-point . fixed-function logic within GPU, texture-filteri ng 1332hardware, continues use proprietary numeric formats. Recent GPUs also provide IEEE 754- compatible double-precision 64-bit floating-point instructions. Supported Formats IEEE 754 standard floating-point arithmetic specifies basic storage formats. GPUs use two basic formats computation, 32-bit 64-bit binary floating-point, commonly called single precision double precision. standard also specifies 16-bit binary storage floating-point format, half precision . GPUs Cg shading language employ narrow 16-bit half data format efficient data storage movement, maintaining high dynamic range. GPUs perform many texture filtering pixel blending computations half preci sion within texture filtering unit raster operations uni t. OpenEXR high dynamic-range image file format developed Industrial Light Magic [2003] uses identical half format color component values computer imaging motion picture applications. half precision 16-bit binary floating-point format, 1 sign bit, 5-bit exponent, 10-bit fraction, implied integer bit. Basic Arithmetic Common single-precision floating-point operations GPU programmable cores include addition, multiplication, multiply- add, minimum, maximum, compare, set predicate, conversions integer floating-point numbers. Floating-point instructions often provide source operand modifiers neg ation absolute value. multiply-add (MAD) single floating-point instruction performs compound operation: multiplication followed addition. 1333The floating-point addition multiplication operations mos GPUs today compatible IEEE 754 standard single precision FP numbers, including not-a-number (NaN) infinity values. FP addition multiplication operations use IEEE round-to-nearest-even default rounding mode. increase floating-point instruction throughput, GPUs often use compo und multiply-add instruction ( mad). multiply-add operation performs FP multiplication truncation, followed FP addition round-to-nearest-even. provides two floating -point operations one issuing cycle, without requiring instruc tion scheduler dispatch two separate instructions, computation fused truncates product addition. makes different fused multiply-add instruction discussed Chapter 3 later section. GPUs typically flush denormalized source operands sign-preserve zero, flush results underflow target output exponent range sign-preserved zero rounding. Specialized Arithmetic GPUs provide hardware accelerate special function computation , attribute interpolation, texture filtering. Special functio n instructions include cosine, sine, binary exponential, binary logarithm, reciprocal, reciprocal square root. Attribute interpolation instructions provide efficient generation pixel attributes, derived plane equation evaluation. special function unit (SFU) introduced Section B.4 computes special functions interpolates planar attributes [ Oberman Siu, 2005 ]. special function unit (SFU) hardware unit computes special functions interpolates planar attributes. Several methods exist evaluating special functions hardware. shown quadratic interpolation based Enhanced Minimax Approximations efficient method approximating functions hardware, including reciprocal, 1334reciprocal square-root, log2x, 2x, sin, cos. summarize method SFU quadratic interpolation. binary input operand X n-bit significand, significand divided two parts: Xu upper part containing bits, Xl lower part containing n-m bits. upper bits Xu used consult set three lookup tables return three fini te- word coefficients C0, C1, C2. function approximated requires unique set tables. coefficients used approximate given function f(X) range Xu≤X<Xu+2−m evaluating expression: accuracy function estimates ranges 22 24 significand bits. Example function statistics shown Figure B.6.1 . FIGURE B.6.1 Special function approximation statistics. NVIDIA GeForce 8800 special function unit (SFU). IEEE 754 standard specifies exact-rounding requirements division square root; however, many GPU applications, exact compliance required. Rather, applications, higher computational throughput important last-bit accuracy. SFU special functions, CUDA math library provides full accuracy function fast function SFU instruction accuracy. 1335Another specialized arithmetic operation GPU attribute interpolation. Key attributes usually specified vertices primitives make scene rendered. Example attributes color, depth, texture coordinates. attributes must interpolated (x,y) screen space needed determine values attributes pixel location. value given attribute U (x, y) plane expressed using plane equations form: A, B, C interpolation parameters associated attribute U. interpolation parameters A, B, C represented single-precision floating-point numbers. Given need function evaluator attribute interpolator pixel shader processor, single SFU perform functions efficiency designed. functions use sum products operation interpolate results, number terms summed functions similar. Texture Operations Texture mapping filtering another key set specialized floating-point arithmetic operations GPU. operations used texture mapping include: 1. Receive texture address (s, t) current screen pixel (x, y) , single-precision floating-point numbers. 2. Compute level detail identify correct texture MIP- map level. 3. Compute trilinear interpolation fraction. 4. Scale texture address (s, t) selected MIP-map level. 5. Access memory retrieve desired texels (texture elements ). 6. Perform filtering operation texels. MIP-map Latin phrase multum parvo , much small space. MIP- map contains precalculated images different resolutions, used increase rendering speed reduce artifacts. 1336Texture mapping requires significant amount floating-point computation full-speed operation, much done 16- bit half precision. example, GeForce 8800 Ultra delivers 500 GFLOPS proprietary format floating-point computation texture mapping instructions, addition conventional IEEE single-precision floating-point instructio ns. details texture mapping filtering, see Foley van Dam [1995] . Performance floating-point addition multiplication arithmetic hardware fully pipelined, latency optimized balance delay area. pipelined, throughput special functions le ss floating-point addition multiplication operations. Quarter-speed throughput special functions typical performance modern GPUs, one SFU shared four SP cores. contrast, CPUs typically significantly lower throughput similar functions, division square roo t, albeit accurate results. attribute interpolation hardware typically fully pipelined enable full-speed pixel shaders. Double precision Newer GPUs Tesla T10P also support IEEE 754 64-bit double-precision operations hardware. Standard floating-point arithmetic operations double precision include addition, multiplication, conversions different floating-p oint integer formats. 2008 IEEE 754 floating-point standard includes specification fused-multiply-add (FMA) operation, discussed Chapter 3 . FMA operation performs floating- point multiplication followed addition, single rounding. fused multiplication addition operations retai n full accuracy intermediate calculations. behavior enables accurate floating-point computations involving accumulation products, including dot products, matrix multiplication, polynomial evaluation. FMA instruction also enables efficient software implementations exactly roun ded 1337division square root, removing need hardware division square root unit. double-precision hardware FMA unit implements 64-bit addition, multiplication, conversions, FMA operation itsel f. architecture double-precision FMA unit enables full-sp eed denormalized number support inputs outputs. Figure B.6.2 shows block diagram FMA unit. 1338FIGURE B.6.2 Double-precision fused-multiply-add (FMA) unit. Hardware implement floating-point A×B+C double precision. shown Figure B.6.2 , significands B multiplied form 106-bit product, results left carry - save form. parallel, 53-bit addend C conditionally inverted aligned 106-bit product. sum carry results 106-bit product summed aligned addend 161-bit-wide carry-save adder (CSA). carry-save output 1339summed together carry-propagate adder produce unrounded result nonredundant, two’s complement form. result conditionally recomplemented, return result n sign-magnitude form. complemented result normalized, rounded fit within target format. B.7 Real Stuff: NVIDIA GeForce 8800 NVIDIA GeForce 8800 GPU, introduced November 2006, unified vertex pixel processor design also supports parall el computing applications written C using CUDA parallel programming model. first implementation Tesla unified graphics computing architecture described Section B.4 Lindholm et al. [2008] . family Tesla architecture GPUs addresses different needs laptops, desktops, workstations, servers. Streaming Processor Array (SPA) GeForce 8800 GPU shown Figure B.7.1 contains 128 streaming processor (SP) cores organized 16 streaming multiprocessors (SMs). Two SMs share texture unit texture/processor cluster (TPC). array eight TPCs makes streaming processor array (SPA), executes graphics shader programs computing programs. 1340FIGURE B.7.1 NVIDIA Tesla unified graphics computing GPU architecture. GeForce 8800 128 streaming processor (SP) cores 16 streaming multiprocessors (SMs), arranged eight texture/processor clusters (TPCs). processors connect six 64-bit-wide DRAM partitions via interconnection network. GPUs implementing Tesla architecture vary number SP cores, SMs, DRAM partitions, units. host interface unit communicates host CPU via PCI-Express bus, checks command consistency, performs context switching. input assembler collects geometric primitives (points, lines, triangles). work distribution b locks dispatch vertices, pixels, compute thread arrays TPCs SPA. TPCs execute vertex geometry shader programs computing programs. Output geometric data sent viewport/clip/setup/raster/zcull block rasterized pixel fragments redistributed back SPA execut e pixel shader programs. Shaded pixels sent across interconnection network processing ROP units. network also routes texture memory read requests SPA DRAM reads data DRAM level-2 cache back SPA. 1341Texture/Processor Cluster (TPC) TPC contains geometry controller, SMC, two SMs, texture unit shown Figure B.7.2 . 1342FIGURE B.7.2 Texture/processor cluster (TPC) streaming multiprocessor (SM). SM eight streaming processo r (SP) cores, two SFUs, shared memory. geometry controller maps logical graphics vertex pipel ine recir-culation physical SMs directing primit ive vertex attribute topology flow TPC. SMC controls multiple SMs, arbitrating shared texture unit, load/store path, I/O path. SMC serves three graphics workloads simultaneously: vertex, geometry, pixel. texture unit processes texture instruction one vert ex, geometry, pixel quad, four compute threads per cycle. Textur e instruction sources texture coordinates, outputs weighted samples, typically four-component (RGBA) floating- point color. texture unit deeply pipelined. Although contains streaming cache capture filtering locality, streams hits mixed misses without stalling. Streaming Multiprocessor (SM) SM unified graphics computing multiprocessor 1343executes vertex, geometry, pixel-fragment shader programs parallel computing programs. SM consists eight SP thread processor cores, two SFUs, multithreaded instruction fe tch issue unit (MT issue), instruction cache, read-only constant cache, 16 KB read/write shared memory. executes scalar instructions individual threads. GeForce 8800 Ultra clocks SP cores SFUs 1.5 GHz, peak 36 GFLOPS per SM. optimize power area efficiency, SM nondatapath units operate half SP clock rate. efficiently execute hundreds parallel threads runn ing several different programs, SM hardware multithreaded. manages executes 768 concurrent threads hardware zero scheduling overhead. thread thread execution state execute independent code path. warp consists 32 threads type—vertex, geometry, pixel, compute. SIMT design, previously described Section B.4 , shares SM instruction fetch issue unit efficiently across 32 threads requires full warp active threads full performance efficiency. SM schedules executes multiple warp types concurrently. issue cycle, scheduler selects one th e 24 warps execute SIMT warp instruction. issued warp instruction executes four sets eight threads four pro cessor cycles. SP SFU units execute instructions independently , issuing instructions alternate cycles, scheduler keep fully occupied. scoreboard qualifies e ach warp issue cycle. instruction scheduler prioritize ready warps selects one highest priority issue. Prioritization considers warp type, instruction type, “fairnes s” warps executing SM. SM executes cooperative thread arrays (CTAs) multiple concurrent warps access shared memory region allocated dynamically CTA. Instruction Set Threads execute scalar instructions, unlike previous GPU vecto r instruction architectures. Scalar instructions simpler 1344compiler-friendly. Texture instructions remain vector-based , taking source coordinate vector returning filtered color vector. register-based instruction set includes floating- point integer arithmetic, transcendental, logical, flow control, memory load/store, texture instructions listed PTX instruction table Figure B.4.3 . Memory load/store instructions use integer byte addressing register-plus-offset address ari thmetic. computing, load/store instructions access three read-wr ite memory spaces: local memory per-thread, private, temporary data; shared memory low-latency per-CTA data shared threads CTA; global memory data shared threads. Computing programs use fast barrier synchronization bar.sync instruction synchronize threads within CTA communicate via shared global memory. latest Tesla architecture GPUs implement PTX atomic memory operations, facilitate parallel reductions parallel data structure management. Streaming Processor (SP) multithreaded SP core primary thread processor, introduced Section B.4 . register file provides 1024 scalar 32-bit registers 96 threads (more threads example SP Section B.4 ). floating-point add multiply operations compatible IEEE 754 standard single-precision FP numbers, including not-a-number (NaN) infinity. add multiply operations use IEEE round-to-nearest-even defau lt rounding mode. SP core also implements 32-bit 64-bit integer arithmetic, comparison, conversion, logical PTX instructions Figure B.4.3 . processor fully pipelined, latency optimized balance delay area. Special Function Unit (SFU) SFU supports computation transcendental functions planar attribute interpolation. described Section B.6 , uses quadratic interpolation based enhanced minimax approximations approximate reciprocal, reciprocal square root, log2x, 2x, sin/cos functions one result per cycle. SFU 1345also supports pixel attribute interpolation color, depth , texture coordinates four samples per cycle. Rasterization Geometry primitives SMs go original round-r obin input order viewport/clip/setup/raster/zcull block. viewport clip units clip primitives view frustu enabled user clip planes, transform vertices screen (pixel) space. Surviving primitives go setup unit, generates edge equations rasterizer. coarse-rasterization stage generates pixel tiles least partially inside primitive. zcull unit maintains hierarchical z surface, rejecting pixel tiles conservatively known occluded previously drawn pixels. rejection rate 256 pixels per clock. Pixels survive zcull go fine-rasterization stag e generates detailed coverage information depth values. depth test update performed ahead fragment shader, after, depending current state. SMC assembles surviving pixels warps processed SM running current pixel shader. SMC sends surviving pixel associated data ROP. Raster Operations Processor (ROP) Memory System ROP paired specific memory partition. pixel fragment emitted pixel shader program, ROPs perform depth stencil testing updates, parallel, color blending updates. Lossless color compression (up 8:1) depth compression (up 8:1) used reduce DRAM bandwidth. ROP peak rate four pixels per clock supports 16-bit floating-point 32-bit floating-point HDR formats. ROPs support double-rate-depth processing color writes disabled. Antialiasing support includes 16×multisampling supersampling. coverage-sampling antialiasing (CSAA) algorithm computes stores Boolean coverage 16 samples compresses redundant color, depth, stencil information 1346memory footprint bandwidth four eight samples improved performance. DRAM memory data bus width 384 pins, arranged six independent partitions 64 pins each. partition supports double-data-rate DDR2 graphics-oriented GDDR3 protocols 1.0 GHz, yielding bandwidth 16 GB/s per partition, 96 GB/s. memory controllers support wide range DRAM clock rates, protocols, device densities, data bus widths. Texture load/store requests occur TPC memory partition, interconnection network routes requests responses. Scalability Tesla unified architecture designed scalability. Varyin g number SMs, TPCs, ROPs, caches, memory partitions provides right balance different performance cost targets GPU market segments. Scalable link interconnect (SLI) connects multiple GPUs, providing scalability. Performance GeForce 8800 Ultra clocks SP thread processor cores SFUs 1.5 GHz, theoretical operation peak 576 GFLOPS. GeForce 8800 GTX 1.35 GHz processor clock corresponding peak 518 GFLOPS. following three sections compare performance GeForce 8800 GPU multicore CPU three different applications—dense linear algebra, fast Fourier transforms, sorting. GPU programs libraries compiled CUDA C code. CPU code uses single-precision multithreaded Inte l MKL 10.0 library leverage SSE instructions multiple cores. Dense Linear Algebra Performance Dense linear algebra computations fundamental many applications. Volkov Demmel [2008] present GPU CPU performance results single-precision dense matrix-matrix multiplication (the SGEMM routine) LU, QR, Cholesky 1347matrix factorizations. Figure B.7.3 compares GFLOPS rates SGEMM dense matrix-matrix multiplication GeForce 8800 GTX GPU quad-core CPU. Figure B.7.4 compares GFLOPS rates matrix factorization GPU quad-core CPU. FIGURE B.7.3 SGEMM dense matrix-matrix multiplication performance rates. graph shows single-precision GFLOPS rates achieved multiplying square N×N matrices (solid lines) thin N ×64 64×N matrices (dashed lines). Adapted Figure 6 Volkov Demmel [2008] . black lines 1.35 GHz GeForce 8800 GTX using Volkov’s SGEMM code (now NVIDIA CUBLAS 2.0) matrices GPU memory. blue lines quad-core 2.4 GHz Intel Core2 Quad Q6600, 64-bit Linux, Intel MKL 10.0 matrices CPU memory. 1348FIGURE B.7.4 Dense matrix factorization performance rates. graph shows GFLOPS rates achieved matrix factorizations using GPU using CPU alone. Adapted Figure 7 Volkov Demmel [2008] . black lines 1.35 GHz NVIDIA GeForce 8800 GTX, CUDA 1.1, Windows XP attached 2.67 GHz Intel Core2 Duo E6700 Windows XP, including CPU–GPU data transfer times. blue lines quad-core 2.4 GHz Intel Core2 Quad Q6600, 64-bit Linux, Intel MKL 10.0. SGEMM matrix-matrix multiply similar BLAS3 routines bulk work matrix factorization, performance sets upper bound factorization rate. matrix order increases beyond 200 400, factorization problem becomes large enough SGEMM leverage GPU parallelism overcome CPU–GPU system copy overhead. Volkov’s SGEMM matrix-matrix multiply achieves 206 GFLOPS, 60% GeForce 8800 GTX peak multiply-add rate, QR factorization reached 192 GFLOPS, 4.3 times quad-core CPU. FFT Performance 1349Fast Fourier Transforms (FFTs) used many applications. Large transforms multidimensional transforms partitioned batches smaller 1D transforms. Figure B.7.5 compares in-place 1D complex single-precision FFT performance 1.35 GHz GeForce 8800 GTX (dating late 2006) 2.8 GHz quad-Core Intel Xeon E5462 series (code named “Harpertown,” dating late 2007). CPU performance measured using Intel Math Kernel Library (MKL) 10.0 FFT four threads. GPU performance measured using NVIDIA CUFFT 2.1 library batched 1D radix-16 decimation-in- frequency FFTs. CPU GPU throughput performance measured using batched FFTs; batch size 224/n, n transform size. Thus, workload every transform size 128 MB. determine GFLOPS rate, number operations per transform taken 5 n log2 n. FIGURE B.7.5 Fast Fourier transform throughput performance. graph compares performance batched one- dimensional in-place complex FFTs 1.35 GHz GeForce 8800 GTX quad-core 2.8 GHz Intel Xeon E5462 series (code named “Harpertown”), 6MB L2 Cache, 4GB Memory, 1600 FSB, Red Hat Linux, Intel MKL 10.0. 1350Sorting Performance contrast applications discussed, sort requires far substantial coordination among parallel threads, parallel scaling correspondingly harder obtain. Nevertheless, varie ty well-known sorting algorithms efficiently parallelized run well GPU. Satish et al. [2008] detail design sorting algorithms CUDA, results report radix sort summarized below. Figure B.7.6 compares parallel sorting performance GeForce 8800 Ultra 8-core Intel Clovertown system, date early 2007. CPU cores distributed two physical sockets. socket contains multichip module twin Core2 chips, chip 4MB L2 cache. sorting routines designed sort key-value pairs keys values 32-bit integers. primary algorithm studied radix sort, although quicksort-based parallel_sort() procedure provided Intel’s Threading Building Blocks also include comparison. two CPU-based radix sort codes, one implemented using scalar instruction set utilizes carefully hand-tuned assembly language routines take advantage SSE2 SIMD vector instructions. 1351FIGURE B.7.6 Parallel sorting performance. graph compares sorting rates parallel radix sort implementations 1.5 GHz GeForce 8800 Ultra 8-core 2.33 GHz Intel Core2 Xeon E5345 system. graph shows achieved sorting rate—defined number elements sorted divided time sort—for range sequence sizes. apparent graph GPU radix sort achieved highest sorting rate sequences 8K- elements larger. range, average 2.6 times faster quicksort-based routine roughly two times faster radix sort routines, using eight available CPU cores. CPU radix sort performance varies widely, likely due poor cache locality global permutations. B.8 Real Stuff: Mapping Applications GPUs advent multicore CPUs manycore GPUs means mainstream processor chips parallel systems. Furthermore , parallelism continues scale Moore’s law. challeng e develop mainstream visual computing high-performance 1352computing applications transparently scale parallelism leverage increasing number processor cores, much 3D graphics applications transparently scale parallelism GPU widely varying numbers cores. section presents examples mapping scalable parallel computing applications GPU using CUDA. Sparse Matrices wide variety parallel algorithms written CUDA fairly straightforward manner, even data structures involved simple regular grids. Sparse matrix-vector multiplication (SpMV) good example important numerical building block parallelized quite directly using abstractions provided CUDA. kernels discuss below, combined provided CUBLAS vector routines, make writing iterative solvers conjugate gradient method straightforward. sparse n×n matrix one number nonzero entries small fraction total. Sparse matrix representations seek store nonzero elements matrix . Since fairly typical sparse n×n matrix contain m=O(n) nonzero elements, represents substantial saving storage space processing time. One common representations general unstructured sparse matrices compressed sparse row (CSR) representation. nonzero elements matrix stored row-major order array Av. second array Aj records corresponding column index entry Av. Finally, array Ap n+1 elements records extent row previous arrays; entries row Aj Av extend index Ap[i] to, including, index Ap[i + 1] . implies Ap[0] always 0 Ap[n] always number nonzero elements matrix. Figure B.8.1 shows example CSR representation simple matrix. 1353FIGURE B.8.1 Compressed sparse row (CSR) matrix. Given matrix CSR form vector x, compute single row product y=Ax using multiply_row() procedure shown Figure B.8.2 . Computing full product simply matter looping rows computing result row using multiply_row() , serial C code shown Figure B.8.3 . FIGURE B.8.2 Serial C code single row sparse matrix-vector multiply. 1354FIGURE B.8.3 Serial code sparse matrix-vector multiply. algorithm translated parallel CUDA kernel quite easily. simply spread loop csrmul_serial() many parallel threads. thread compute exactly one row output vector y. code kernel shown Figure B.8.4 . Note looks extremely similar serial loop used csrmul_serial() procedure. really two points difference. First, row index thread computed block thread indices assigned thread, eliminating for-loop. Second, conditional evaluates row product row index within bounds matrix (this necessary since number rows n need multiple block size used launching kernel). 1355FIGURE B.8.4 CUDA version sparse matrix- vector multiply. Assuming matrix data structures already copied GPU device memory, launching kernel loo k like: unsigned int blocksize = 128; // size 512 unsigned int nblocks = (num_rows + blocksize - 1) / blocksize; csrmul_kernel<<<nblocks,blocksize>>>(Ap, Aj, Av, nu m_rows, x, y); pattern see common one. original serial algorithm loop whose iterations independent other. loops parallelized quite easily simply assigning one iterations loop parallel thread . programming model provided CUDA makes expressing type parallelism particularly straightforward. general strategy decomposing computations blocks independent work, specifically breaking independent loop iterations, unique CUDA. common approach used one form another various parallel programming systems, including OpenMP Intel’s Threading Building Block s. Caching Shared Memory SpMV algorithms outlined fairly simplistic. ar e number optimizations made CPU GPU codes improve performance, including loop 1356unrolling, matrix reordering, register blocking. parallel kernels also reimplemented terms data parallel scan operations presented Sengupta et al. [2007] . One important architectural features exposed CUDA presence per-block shared memory, small on-chip memory low latency. Taking advantage memory deliver substantial performance improvements. One common way use shared memory software-managed cache hold frequently reused data. Modifcations using shared memory shown Figure B.8.5 . 1357FIGURE B.8.5 Shared memory version sparse matrix-vector multiply. context sparse matrix multiplication, observe several rows may use particular array element x[i] . many common cases, particularly matrix 1358reordered, rows using x[i] rows near row i. therefore implement simple caching scheme expect achieve performance benefit. block threads processing rows j load x[i] x[j] shared memory. unroll multiply_row() loop fetch elements x cache whenever possible. resulting code shown Figure B.8.5 . Shared memory also used make optimizations, fetching Ap[row+1] adjacent thread rather refetching memory. Tesla architecture provides explicitly managed on-chip shared memory, rather implicitly active hardware cache, fairly common add sort optimization. Although impose additional development burden programmer, relatively minor, potential performance benefits substantial. example shown above, even fairly simple use shared memory returns roughly 20% performance improvement representative matrices derived fr om 3D surface meshes. availability explicitly managed memory lieu implicit cache also advantage caching prefetching policies specifically tailored th e application needs. fairly simple kernels whose purpose illustrate basic techniques writing CUDA programs, rather achieve maximal performance. Numerous possible avenues optimization available, several explored Williams et al. [2007] handful different multicore architectures. Nevertheless, still instructive examin e comparative performance even simplistic kernels. 2 GHz Intel Core2 Xeon E5335 processor, csrmul_serial() kernel runs roughly 202 million nonzeros processed per second, collection Laplacian matrices derived 3D triangulated surface meshes. Parallelizing kernel parallel_for construct provided Intel’s Threading Building Blocks prod uces parallel speed-ups 2.0, 2.1, 2.3 running two, four, eight cores machine, respectively. GeForce 8800 Ultra, csrmul_kernel() csrmul_cached() kernels achieve processing rates roughly 772 920 million nonzeros per second, corresponding parallel speed-ups 3.8 4.6 times serial performance single CPU core. 1359Scan Reduction Parallel scan, also known parallel prefix sum , one important building blocks data-parallel algorithms [ Blelloch, 1990 ]. Given sequence n elements: binary associative operator ⊕, scan function computes sequence: example, take ⊕ usual addition operator, applying scan input array produce sequence partial sums: scan operator inclusive scan, sense element output sequence incorporates element ai input. Incorporating previous elements would yield exclusive scan operator, also known prefix-sum operation. serial implementation operation extremely simple . simply loop iterates entire sequence, shown Figure B.8.6 . 1360FIGURE B.8.6 Template serial plus-scan. first glance, might appear operation inherently serial. However, actually implemented parallel efficiently. key observation addition associative, free change order elements added together. instance, imagine adding pairs consecutive elements parallel, adding partial sums, on. One simple scheme Hillis Steele [1989]. implementation algorithm CUDA shown Figure B.8.7 . assumes input array x[ ] contains exactly one element per thread thread block. performs log2 n iterations loop collecting partial sums together. 1361FIGURE B.8.7 CUDA template parallel plus- scan. understand action loop, consider Figure B.8.8 , illustrates simple case n=8 threads elements. level diagram represents one step loop. lines indicate location data fetched. element output (i.e., final row diagram) building summation tree input elements. edges highlighted blue show form summation tree final element. leaves tree initial elements. Tracing back output element shows incorporates input values including itself. FIGURE B.8.8 Tree-based parallel scan data references. simple, algorithm efficient would like. Examining serial implementation, see performs O(n) additions. parallel implementation, contrast, performs O(n log n) additions. reason, work efficient , since work serial implementation compute result. Fortunately, techniques implementin g scan work-efficient. Details efficient implementation techniques extension per-bloc k procedure multiblock arrays provided Sengupta et al. 1362[2007] . instances, may interested computing sum elements array, rather sequence prefix sums returned scan . parallel reduction problem. could simply use scan algorithm perform computation, reduction generally implemented efficiently th scan. Figure B.8.9 shows code computing reduction using addition. example, thread simply loads one element input sequence (i.e., initially sums subsequence length 1). end reduction, want thread 0 hold sum elements initially loaded threads block. loop kernel implicitly builds summation tree input eleme nts, much like scan algorithm above. 1363FIGURE B.8.9 CUDA implementation plus- reduction. end loop, thread 0 holds sum values loaded block. want final value location pointed total contain total elements array, must combine partial sums blocks grid. One strategy would block write partial sum second array launch reduction kernel again, repeating process reduced sequence singl e value. attractive alternative supported Tesla GPU architecture use atomicAdd() primitive, efficient atomic read-modify-write primitive supported memory subsys tem. eliminates need additional temporary arrays repeated kernel launches. Parallel reduction essential primitive parallel programming highlights importance per-block shared memory low-cost barriers making cooperation among threads efficient. degree data shuffling among threads 1364would prohibitively expensive done off-chip global memory. Radix Sort One important application scan primitives implementation sorting routines. code Figure B.8.10 implements radix sort integers across single thread block. accepts input array values containing one 32-bit integer thread block. efficiency, array stored per-block shared memory, required sort behave correctly. FIGURE B.8.10 CUDA code radix sort. fairly simple implementation radix sort. assumes availability procedure partition_by_bit() partition given array values 0 designated bit come values 1 bit. produce correct output, partitioning must stable. Implementing partitioning procedure simple application scan. Thread holds value xi must calculate correct output index write value. so, needs calculate (1) number threads j <i designated bit 1 (2) total number bits designated bit 0. CUDA code partition_by_bit() shown Figure B.8.11 . 1365FIGURE B.8.11 CUDA code partition data bit-by-bit basis, part radix sort. similar strategy applied implementing radix sort kernel sorts array large length, rather one- block array. fundamental step remains scan procedure, although computation partitioned across multiple kernels, must double-buffer array values rather partitioning place. Details performing radix sorts large arrays efficiently provided Satish et al. [2008] . N-Body Applications GPU1 Nyland et al. [2007] describe simple yet useful computational kernel excellent GPU performance—the all-pairs N-body algorithm. time-consuming component many scientific applications. N-body simulations calculate evolution 1366system bodies body continuously interacts every body. One example astrophysical simulation body represents individual star, bodies gravitationally attract other. examples protein folding, N-body simulation used calculate electrostat ic van der Waals forces; turbulent fluid flow simulation; global illumination computer graphics. all-pairs N-body algorithm calculates total force body system computing pair-wise force system, summing body. Many scientists consider method accurate, loss precision coming floating-point hardware operations. drawback O( n2) computational complexity, far large systems 10 bodies. overcome high cost, several simplifications proposed yield O( n log n) O(n) algorithms; examples Barnes-Hut algorithm, Fast Multipole Method Particle-Mesh-Ewald summation. fast methods still rely all-pairs method kernel accurate computation short-range forces; thus continues important. N-Body Mathematics gravitational simulation, calculate body-body force using elementary physics. two bodies indexed j, 3D force vector is: force magnitude calculated left term, direction computed right (unit vector pointing ne body other). Given list interacting bodies (an entire system subset), calculation simple: pairs interactions, compute th e force sum body. total forces calculated, used update body’s position velocity, based previous position velocity. calculation forces h complexity O( n2), update O( n). 1367The serial force-calculation code uses two nested for-loops iterating pairs bodies. outer loop selects body f total force calculated, inner loop iterates bodies. inner loop calls function computes pair-wise force, adds force running sum. compute forces parallel, assign one thread body, since calculation force body independent calculation bodies. forces computed, positions velocities bodies updated. code serial parallel versions shown Figure B.8.12 Figure B.8.13 . serial version two nested for- loops. conversion CUDA, like many examples, converts serial outer loop per-thread kernel thread computes total force single body. CUDA kernel computes global thread ID thread, replacing iterator variable serial outer loop. kernels finish storing total acceleration global array used compute new position velocity values subsequent step. outer loop replaced CUDA kernel grid launches N threads, one body. FIGURE B.8.12 Serial code compute pair-wise forces N bodies. 1368FIGURE B.8.13 CUDA thread code compute total force single body. Optimization GPU Execution CUDA code shown functionally correct, efficien t, ignores key architectural features. Better performance achieved three main optimizations. First, shared memory used avoid identical memory reads threads. Second, using multiple threads per body improves performance small values N. Third, loop unrolling reduces loop overhead. Using Shared Memory Shared memory hold subset body positions, much like cache, eliminating redundant global memory requests threads. optimize code shown p threads thread-block load one position shared memory (for total p positions). threads loaded value shared memory, ensured __syncthreads() , thread perform p interactions (using data shared memory). repeated N/p times complete force calculation body, reduces number requests memory factor p (typically range 32–128). function called accel_on_one_body() requires changes support optimization. modified code shown Figure B.8.14 . 1369FIGURE B.8.14 CUDA code compute total force body, using shared memory improve performance. loop formerly iterated bodies jumps block dimension p. iteration outer loop loads p successive positions shared memory (one position per th read). threads synchronize, p force calculations computed thread. second synchronization required ensure new values loaded shared memory prior threads completing force calculations current ata. Using shared memory reduces memory bandwidth required less 10% total bandwidth GPU sustain (using less 5 GB/s). optimization keeps application busy performing computation rather waiting memory accesses, would done without use shared memory. performance varying values N shown Figure B.8.15 . 1370FIGURE B.8.15 Performance measurements N-body application GeForce 8800 GTX GeForce 9600. 8800 128 stream processors 1.35 GHz, 9600 64 0.80 GHz (about 30% 8800). peak performance 242 GFLOPS. GPU processors, problem needs bigger achieve full performance (the 9600 peak around 2048 bodies, 8800 doesn’t reach peak 16,384 bodies). small N, one thread per body significantly improve performance, eventually incurs performance penalty N grows. Using Multiple Threads per Body Figure B.8.15 shows performance degradation problems small values N (N<4096) GeForce 8800 GTX. Many research efforts rely N-body calculations focus small N (for long simulation times), making target optimization efforts. presumption explain lower performance simply enough work keep GPU busy N small. solution allocate threads per body. change thread-block dimensions ( p, 1, 1) ( p, q, 1), q threads divide work single body equal parts. allocating additional threads within thread block, partial results stored shared memory. force calculations done, q partial results collected 1371summed compute final result. Using two four threads per body leads large improvements small N. example, performance 8800 GTX jumps 110% N=1024 (one thread achieves 90 GFLOPS, four achieve 190 GFLOPS). Performance degrades slightly large N, use optimization N smaller 4096. performance increases shown Figure B.8.15 GPU 128 processors smaller GPU 64 processors clocked two-thirds speed. Performance Comparison performance N-body code shown Figure B.8.15 Figure B.8.16 . Figure B.8.15 , performance high- medium- performance GPUs shown, along performance improvements achieved using multiple threads per body. performance faster GPU ranges 90 250 GFLOPS. FIGURE B.8.16 Performance measurements N-body code CPU. graph shows single precision N-body performance using Intel Core2 CPUs, denoted CPU model number. Note dramatic reduction GFLOPS performance (shown GFLOPS y-axis), demonstrating much faster GPU compared CPU. performance CPU generally independent problem size, except 1372anomalously low performance N=16,384 X9775 CPU. graph also shows results running CUDA version code (using CUDA-for-CPU compiler) single CPU core, outperforms C++ code 24%. programming language, CUDA exposes parallelism locality compiler exploit. Intel CPUs 3.2 GHz Extreme X9775 (code named “Penryn”), 2.66 GHz E8200 (code named “Wolfdale”), desktop, pre-Penryn CPU, 1.83 GHz T2400 (code named “Yonah”), 2007 laptop CPU. Penryn version Core 2 architecture particularly interesting N-body calculations 4-bit divider , allowing division square root operations execute four times faster previous Intel CPUs. Figure B.8.16 shows nearly identical code (C++ versus CUDA) running Intel Core2 CPUs. CPU performance 1% GPU, range 0.2 2 GFLOPS, remaining nearly constant wide range problem sizes. graph also shows results compiling CUDA version code CPU, performance improves 24%. CUDA, programming language, exposes parallelism, allowing compiler make better use SSE vector unit single core. CUDA version N-body code naturally maps multicore CPUs well (with grids blocks), achieves nearly perfect scaling eight-core system N=4096 (ratios 2.0, 3.97, 7.94 two, four, eight cores, respectively). Results modest effort, developed computational kernel improves GPU performance multicore CPUs factor 157. Execution time N-body code running recent CPU Intel (Penryn X9775 3.2 GHz, single core) took 3 seconds per frame run code runs 44 Hz frame rate GeForce 8800 GPU. pre-Penryn CPUs, code requires 6–16 seconds, older Core2 processors Pentium IV processor, time 25 seconds. must divide apparent increase performance half, CPU requires half many calculations compute result (using optimization forces pair bodies equal strength 1373and opposite direction). GPU speed code large amount? answer requires inspecting architectural details. pair-wi se force calculation requires 20 floating-point operations, compris ed mostly addition multiplication instructions (some whic h combined using multiply-add instruction), also division square root instructions vector normalizati on. Intel CPUs take many cycles single-precision division square root instructions,2 although improved latest Penryn CPU family faster 4-bit divider.3 Additionally, limitations register capacity lead many MOV instructions x86 code (presumably to/from L1 cache). contrast, GeForce 8800 executes reciprocal square-root thread instruction n four clocks; see Section B.6 special function accuracy. larger register file (per thread) shared memory accessed instruction operand. Finally, CUDA compiler emits 15 instructions one iteration loop, compared 40 instructions variety x86 CPU compilers. Greater parallelism, faster execution complex instructions, ore register space, efficient compiler combine explain th e dramatic performance improvement N-body code CPU GPU. GeForce 8800, all-pairs N-body algorithm delivers 240 GFLOPS performance, compared less 2 GFLOPS recent sequential processors. Compiling executing CUDA version code CPU demonstrates problem scales well multicore CPUs, still significantly slower single GPU. coupled GPU N-body simulation graphical display motion, interactively display 16K bodies interacting 44 frames per second. allows astrophysical biophysical events displayed navigated interactive rates. Additionally, parameterize many settings, noise reduction, damping, integration techniques, immediately displaying effects dynamics system. provides scientists stunning visual imagery, boosting th eir insights otherwise invisible systems (too large small, fast slow), allowing create better models physical phenomena. 1374Figure B.8.17 shows time-series display astrophysical simulation 16K bodies, body acting galaxy. initial configuration spherical shell bodies rotating abo ut z-axis. One phenomenon interest astrophysicists clustering occurs, along merging galaxies time . interested reader, CUDA code application available CUDA SDK www.nvidia.com/CUDA . 1375FIGURE B.8.17 Twelve images captured evolution N-body system 16,384 bodies. B.9 Fallacies Pitfalls GPUs evolved changed rapidly many fallacies pitfalls arisen. cover here. Fallacy GPUs SIMD vector multiprocessors. easy draw false conclusion GPUs simply SIMD vector multiprocessors. GPUs SPMD-style programming model, programmer write single program executed multiple thread instances multiple data. execution threads purely SIMD vector, however ; single-instruction multiple-thread (SIMT), described Section B.4 . GPU thread scalar registers, thread private memory, thread execution state, thread ID, independent execution branch path, effective program counter, address memory independently. Although group threads (e.g., warp 1376of 32 threads) executes efficiently PCs threads same, necessary. So, multiprocessors purely SIMD. thread execution model MIMD barrier synchronization SIMT optimizations. Execution efficient individual thread load/store memory accesses coalesced block accesses, well. However, strictl necessary. purely SIMD vector architecture, memory/registe r accesses different threads must aligned regular vector pattern. GPU restriction register memory accesses; however, execution efficient warps threads access local blocks data. departure pure SIMD model, SIMT GPU execute one warp threads concurrently. graphics applications, may multiple groups vertex programs, pixel programs, geometry programs running multiprocessor array concurrently. Computing programs may also execute different programs concurrently different warps. Fallacy GPU performance cannot grow faster Moore’s law. Moore’s law simply rate. “speed light” limit rate. Moore’s law describes expectation that, time, semiconductor technology advances transistors become smaller, manufacturing cost per transistor declin e exponentially. Put another way, given constant manufacturing cost, number transistors increase exponentially. Gordon Moore [1965] predicted progression would provide roughly two times number transistors manufacturing cost every year, later revised doubling every 2 years. Although Moore made initial prediction 1965 50 components per integrated circuit, proved remarkably consistent. reduction transistor size h historically benefits, lower power per transist faster clock speeds constant power. increasing bounty transistors used chip architect build processors, memory, components. time, CPU designers used extra transistors increase proces sor performance rate similar Moore’s law, much many people think processor performance growth two times eve ry 137718–24 months Moore’s law. fact, not. Microprocessor designers spend new transistors n processor cores, improving architecture design, pipelining clock speed. rest new transistors ar e used providing cache, make memory access faster. contrast, GPU designers use almost none new transistors provide cache; transistors used improving processor cores adding processor cores. GPUs get faster four mechanisms. First, GPU designers reap Moore’s law bounty directly applying exponentially transistors building parallel, thus faster, processors . Second, GPU designers improve architecture time, increasing efficiency processing. Third, Moore’s law assumes constant cost, Moore’s law rate clearly exceeded spending larger chips transisto rs. Fourth, GPU memory systems increased effective bandwidth pace nearly comparable processing rate, using faster memories, wider memories, data compression, better caches. combination four approaches historically allowed GPU performance double regularly, roughl every 12 18 months. rate, exceeding rate Moore’s law, demonstrated graphics applications approximately 10 years shows sign significant slowdown. challenging rate limiter appears memory system, competitive innovation advancing rapidly too. Fallacy GPUs render 3D graphics; can’t general computation. GPUs built render 3D graphics well 2D graphics video. meet demands graphics software developers expressed interfaces performance/feature requireme nts graphics APIs, GPUs become massively parallel programmable floating-point processors. graphics domain, processors programmed graphics APIs arcane graphics programming languages (GLSL, Cg, HLSL, OpenGL Direct3D). However, nothing preventing GPU architects exposing parallel processor cores programmers without graphics API arcane 1378graphics languages. fact, Tesla architecture family GPUs exposes processors software environment known CUDA, allows programmers develop general application programs using C language soon C++. GPUs Turing- complete processors, run program CPU run, although perhaps less well. perhaps faster. Fallacy GPUs cannot run double-precision floating-point programs fast. past, GPUs could run double-precision floating-poin programs all, except software emulation. that’s fast all. GPUs made progression indexed arithmetic representation (lookup tables colors) 8-bit ntegers per color component, fixed-point arithmetic, single-prec ision floating-point, recently added double precision. Modern GPU perform virtually calculations single-precision IEEE floati ng- point arithmetic, beginning use double precision addition. small additional cost, GPU support double-precision floating-point well single-precision floating-point. Tod ay, double-precision runs slowly single-precision speed, five ten times slower. incremental additional cost, double-precision performance increased relative sing le precision stages, applications demand it. Fallacy GPUs don’t floating-point correctly. GPUs, least Tesla architecture family processors, perform single-precision floating-point processing leve l prescribed IEEE 754 floating-point standard. So, terms accuracy, GPUs equal IEEE 754-compliant processors. Today, GPUs implement specific features described standard, handling denormalized numbers providing precise floating-point exceptions. However, recently introduced Tesla T10P GPU provides full IEEE rounding, fused-multiply-add, denormalized number support double 1379precision. Pitfall use threads cover longer memory latencies. CPU cores typically designed run single thread full speed. run full speed, every instruction data need available time instruction run. next instruction ready data required instructio n available, instruction cannot run processor stalls. External memory distant processor, takes many cycles wasted execution fetch data memory. Consequently, CPUs require large local caches keep running without stalling. Memory latency long, avoided striving run cache. point, program working set demands may larger cache. CPUs used multithreading tolerate latency, number threads per core generally limited small number. GPU strategy different. GPU cores designed run many threads concurrently, one instruction thread time. Another way say GPU runs thread slowly, aggregate runs threads efficiently. thread tolerate amount memory latency, threads run. downside multiple—many multiple threads— required cover memory latency. addition, memory accesses scattered correlated among threads, memory system get progressively slower responding individual request. Eventually, even multiple threads n ot able cover latency. So, pitfall “just use threads” strategy work covering latency, enough threads, threads well-behaved terms locality memory access. Fallacy O(n) algorithms difficult speed up. matter fast GPU processing data, steps transferring data device may limit performance algorithms O( n) complexity (with small amount work per datum). highest transfer rate PCIe bus approximately 48 GB/second DMA transfers used, 1380slightly less nonDMA transfers. CPU, contrast, typical access speeds 8–12 GB/second system memory. Example problems, vector addition, limited transfer inputs GPU returning output th e computation. three ways overcome cost transferring data. First, try leave data GPU long possible, instead moving data back forth different steps complicated algorithm. CUDA deliberately leaves data alone GPU launches support this. Second, GPU supports concurrent operations copy-in , copy-out computation, data streamed device computing. model useful dat stream processed arrives. Examples video processing, network routing, data compression/decompression , even simpler computations large vector mathematics. third suggestion use CPU GPU together, improving performance assigning subset work each, treating system heterogeneous computing platform. CUDA programming model supports allocation work one GPUs along continued use CPU without use threads (via asynchronous GPU functions), relatively simp le keep GPUs CPU working concurrently solve problems even faster. B.10 Concluding Remarks GPUs massively parallel processors become widely used, 3D graphics, also many applications. wide application made possible evolution graphics devices programmable processors. graphics application programming model GPUs usually API DirectX™ OpenGL™. general-purpose computing, CUDA programming model uses SPMD ( single- program multiple data ) style, executing program many parallel threads. GPU parallelism continue scale Moore’s law, mainly increasing number processors. parallel programming models readily scale hundreds 1381processor cores thousands threads successful supporting manycore GPUs CPUs. Also, applications many largely independent parallel tasks accelerated massively parallel manycore architectures. Parallel programming models GPUs becoming flexible, graphics parallel computing. example, CUDA evolving rapidly direction full C/C++ functionality. Graphics APIs programming models likely adapt parallel computing capabilities models CUDA. SPMD-style threading model scalable, convenient, succinct, easily learned model expressing large amounts parallelism. Driven changes programming models, GPU architecture turn becoming flexible programmable. GPU fixed-function units becoming accessible general programs, along lines CUDA programs already use texture intrinsic functions perform texture lo okups using GPU texture instruction texture unit. GPU architecture continue adapt usage patterns graphics application programmers. GPUs continue expand include processing power additional processor cores, well increasing thread memory bandwidth available programs. addition, programming models must evolve include programming heterogeneous manycore systems including GPUs CPUs. Acknowledgments appendix work several authors NVIDIA. gratefully acknowledge significant contributions Michae l Garland, John Montrym, Doug Voorhies, Lars Nyland, Erik Lindholm, Paulius Micikevicius, Massimiliano Fatica, Stuart Oberman, Vasily Volkov. B.11 Historical Perspective Reading Graphics Pipeline Evolution 13823D graphics pipeline hardware evolved large expensive systems early 1980s small workstations PC accelerators mid- late-1990s. period, three major transitions occurred: Performance-leading graphics subsystems declined price fr om $50,000 $200. Performance increased 50 million pixels per second 1 billion pixels per second 100,000 vertices per second 10 million vertices per second. Native hardware capabilities evolved wireframe (polygon outlines) flat shaded (constant color) filled polygons, smo oth shaded (interpolated color) filled polygons, full-scene ant i- aliasing texture mapping rudimentary multitexturing. Fixed-Function Graphics Pipelines Throughout period, graphics hardware configurable, programmable application developer. generation, incremental improvements offered. developers growing sophisticated asking new features could reasonably offered built-in fixed functions. NVIDIA GeForce 3, described Lindholm et al. [2001] , took first step toward true general shader programmability. exposed application developer private internal instruction set floating-point vertex engine. coincided release Microsoft’s Direct X 8 OpenGL’s vertex shader extensions. Later GPUs, time DirectX 9, extended general programmability floating point capability pixel fragment stage, made texture available vertex stage. ATI Radeon 9700, introduced 2002, featured programmable 24-bit floating-point pixel fragment processor programmed DirectX 9 OpenGL. GeForce FX added 32-bit floating-point pixel processors. part general trend toward unifying functionality different stages, least far application programmer concerned. NVIDIA’s GeForce 6800 7800 series built separate processor designs separate hardware dedicated vertex fragment processing. XBox 360 introduced early unified processor GPU 2005, allowing vertex pixel shaders 1383execute processor. Evolution Programmable Real-Time Graphics last 30 years, graphics architecture evolved simple pipeline drawing wireframe diagrams highly parallel design consisting several deep parallel pipelines cap able rendering complex interactive imagery appears three- dimensional. Concurrently, many calculations involved became far sophisticated user-programmable. graphics pipelines, certain stages great deal floating-point arithmetic completely independent data, transforming position triangle vertexes generating pix el colors. data independence key difference GPUs CPUs. single frame, rendered 1/60th second, might 1 million triangles 6 million pixels. opportunity use hardware parallelism exploit data independence tremendous. specific functions executed graphics pipeline stage vary rendering algorithms evolved programmable. Vertex programs map position triangle vertices screen, altering position, color, orientation. Typically vertex shader thread inputs floating-poi nt (x, y, z, w) vertex position computes floating-point (x, y, z) screen position. Geometry programs operate primitives defi ned multiple vertices, changing generating additional primitives. Pixel fragment shaders “shade” one pixel, computing floating-point red, green, blue, alpha (RGBA) color contribution rendered image pixel sample (x, y) image position. three types graphics shaders, program instance run parallel, works independent data, produces independent results, side effects. programmable graphics pipeline stages dozens fixed-function stages perform well-defined tasks far efficiently programmable processor could would benefit far less programmability. example, geometry processing stage pixel processing stage “rasterizer,” complex state machine determines exactly 1384pixels (and portions thereof) lie within geometric primi tive’s boundaries. Together, mix programmable fixed-function stages engineered balance extreme performance user control rendering algorithms. Common rendering algorithms perform single pass input primitives access memory resources highly coherent manner; algorithms provide excellent bandwidth utilization largely insensitive memory latency. Combined pixel shader workload usually compute-limited, characteristics guided GPUs along different evolutionary path CPUs. Whereas CPU die area dominated cache memory, GPUs dominated floating-point datapath fixed-function logic. GPU memory interfaces emphasize bandwidt h latency (since latency readily hidden high thread count); indeed, bandwidth typically many times higher CPU, exceeding 100 GB/second cases. far-higher number fine-grained lightweight threads effectively explo rich parallelism available. Beginning NVIDIA’s GeForce 8800 GPU 2006, three programmable graphics stages mapped array unified processors; logical graphics pipeline physically recirc ulating path visits processors three times, much fixed- function graphics logic visits. Since different rend ering algorithms present wildly different loads among three programmable stages, unification provides processor load balancing. Unified Graphics Computing Processors DirectX 10 generation, functionality vertex pixel fragment shaders made identical programmer, fact new logical stage introduced, geometry shader, process vertices primitive rather vertices isolation. GeForce 8800 designed DirectX 10 mind. Developers coming sophisticated shading algorithms, motivated sharp increase available shader operation rate, particularly floating-point operations. NVIDIA chose pursue processor design higher operatin g frequency standard-cell methodologies allowed, deliv er 1385the desired operation throughput area-efficiently possibl e. High-clock-speed design requires substantially engineer ing effort, favored designing one processor, rather two (o r three, given new geometry stage). became worthwhile take engineering challenges unified processor (load balanci ng recirculation logical pipeline onto threads process array) get benefits one processor design. GPGPU: Intermediate Step DirectX 9-capable GPUs became available, researchers took notice raw performance growth path GPUs began explore use GPUs solve complex parallel problems. DirectX 9 GPUs designed match features required graphics API. access computational resources, programmer cast problem native graphics operations. example, run many simultaneous instances pixel shader, triangle issued GPU (with clipping rectangle shape that’s desired). Shaders means perform arbitrary scatter operations memory. way write result memory emit pixel color value, configure framebuffer operation stage write (or blend, desired) result two- dimensional framebuffer. Furthermore, way get result one pass computation next write parallel results pixel framebuffer, use framebuffer textur e map input pixel fragment shader next stage computation. Mapping general computations GPU era quite awkward. Nevertheless, intrepid researchers demonstrated handful useful applications painstaking efforts. field called “GPGPU” general purpose computing GPUs. GPU Computing developing Tesla architecture GeForce 8800, NVIDIA realized potential usefulness would much greater f programmers could think GPU processor. NVIDIA selected programming approach programmers would 1386explicitly declare data-parallel aspects workload. DirectX 10 generation, NVIDIA already begun work high-efficiency floating-point integer processor co uld run variety simultaneous workloads support logical graphics pipeline. processor designed take advantage common case groups threads executing code path. NVIDIA added memory load store instructions integer byte addressing support requirements compi led C programs. introduced thread block (cooperative thread array) , grid thread blocks, barrier synchronization dispatch manage highly parallel computing work. Atomic memory operations added. NVIDIA developed CUDA C/C++ compiler, libraries, runtime software enable programmers readily access new data-parallel computation model develop applications. Scalable GPUs Scalability attractive feature graphics systems beginning. Workstation graphics systems gave customers choice pixel horsepower varying number pixel processor circuit boards installed. Prior mid-1990s PC graphics scaling almost nonexistent. one option— VGA controller. 3D-capable accelerators appeared, market room range offerings. 3dfx introduced multiboard scaling original SLI ( Scan Line Interleave ) Voodoo2, held performance crown time (1998). Also 1998, NVIDIA introduced distinct products variants single architecture Riva TNT Ultra (high- performance) Vanta (low-cost), first speed binning packaging, separate chip designs (GeForce 2 GTS & GeForce 2 MX). present, given architecture generation, four five separate GPU chip designs needed cover range desktop PC performance price points. addition, separate segments notebook workstation systems. acquiring 3dfx, NVIDIA continued multi-GPU SLI concept 2004, starting GeForce 6800—providing multi-GPU scalability transparently programmer user. Functional behavior identical across scaling range; one application 1387run unchanged implementation architectural family. CPUs scaling higher transistor counts increasing number constant-performance cores die, rather increasing performance single core. writing industry transitioning dual-core quad-core, eight - core far behind. Programmers forced find fourfold eightfold task parallelism fully utilize processors, applications using task parallelism must rewritten frequentl target successive doubling core count. contrast, highly multithreaded GPU encourages use many-fold data parallelism thread parallelism, readily scales thousands parallel threads many processors. GPU scalable parallel programming model graphics parallel computing designed transparent portable scalability. graphics program CUDA program written runs GPU number processors. shown Section B.1 , CUDA programmer explicitly states fine-grained coarse- grained parallelism thread program decomposing problem grids thread blocks—the program run efficiently GPUs CPUs size current future generations well. Recent Developments Academic industrial work applications using CUDA produced hundreds examples successful CUDA programs. Many programs run application tens hundreds times faster multicore CPUs capable running them. Examples include n-body simulation, molecular modeling, computational finance, oil gas exploration data processing. Although many use single-precision floating-point arithmetic, problems require double precision. recen arrival double-precision floating-point GPUs enables ev en broader range applications benefit GPU acceleration. comprehensive list examples current developments applications accelerated GPUs, visit CUDAZone: www.nvidia.com/CUDA . Future Trends 1388Naturally, number processor cores continue incre ase proportion increases available transistors silicon processes improve. addition, GPUs continue enjoy vigorous architectural evolution. Despite demonstrated high performance data-parallel applications, GPU core processors still relatively simple design. aggressive techniques introduced successive architecture increase ac tual utilization calculating units. scalable parallel computing GPUs new field, novel applications rapidly created. studying them, GPU designers discover implement new machine optimizations. Reading 1. Akeley, K. T. Jermoluk [1988]. “High-Performance Polygon Rendering,” Proc. SIGGRAPH 1988 (August), 239– 46. 2. Akeley, K. [1993]. “RealityEngine Graphics.” Proc. SIGGRAPH 1993 (August), 109–16. 3. Blelloch GB. Prefix Sums Applications. In: Reif John H, ed. Synthesis Parallel Algorithms . San Francisco: Morgan Kaufmann Publishers; 1990. 4. Blythe D. Direct3D 10 System. ACM Trans Graphics . 2006;Vol. 25(no. 3(July)):724–734. 5. Buck, I., T. Foley, D. Horn, J. Sugerman, K. Fatahlian, M. Houston, P. Hanrahan [2004]. “Brook GPUs: Stream Computing Graphics Hardware.” Proc. SIGGRAPH 2004 , 777–86, August. http://doi.acm.org/10.1145/1186562.1015800 . 6. Elder, G. [2002] “Radeon 9700.” Eurographics/SIGGRAPH Workshop Graphics Hardware, Hot3D Session, www.graphicshardware.org/previous/www_2002/presentations/Hot3D- RADEON9700.ppt . 7. Fernando R, Kilgard MJ. Cg Tutorial: Definitive Guide Programmable Real-Time Graphics Reading, MA: Addison- Wesley; 2003. 8. Fernando R, ed. GPU Gems: Programming Techniques, Tips, Tricks Real-Time Graphics . Reading, MA: Addison- Wesley; 2004; 1389https://developer.nvidia.com/gpugems/GPUGems/gpugems_pre f01.html 9. Foley J, van Dam A, Feiner S, Hughes J. Computer Graphics: Principles Practice, second edition C Reading, MA: Addison-Wesley; 1995. 10. Hillis WD, Steele GL. Data parallel algorithms. Commun ACM . 1986;29:1170–1183 http://doi.acm.org/10.1145/7902.7903 . 11. IEEE Std 754-2008 [2008]. IEEE Standard Floating-Point Arithmetic . ISBN 978-0-7381-5752-8, STD95802, http://ieeexplore.ieee.org/servlet/opac?punumber=4610933 (Aug. 29). 12. Industrial Light Magic [2003]. OpenEXR , www.openexr.com . 13. Intel Corporation [2007]. Intel 64 IA-32 Architectures Optimization Reference Manual . November. Order Number: 248966-016. http://www.intel.com/content/dam/www/public/us/en/docum ents/manuals/64- ia-32-architectures-optimization-manual.pdf . 14. Kessenich, J. [2006]. OpenGL Shading Language, Language Version 1.20, Sept . 2006 . www.opengl.org/documentation/specs/ . 15. Kirk, D. D. Voorhies [1990]. “The Rendering Architecture DN10000VS.” Proc. SIGGRAPH 1990 (August), 299–307. 16. Lindholm E., M.J. Kilgard, H. Moreton [2001]. “A User- Programmable Vertex Engine.” Proc. SIGGRAPH 2001 (August), 149–58. 17. Lindholm E, Nickolls J, Oberman S, Montrym J. NVIDIA Tesla: Unified Graphics Computing Architecture. IEEE Micro . 2008;Vol. 28(no. 2(March–April)):39–55. 18. Microsoft Corporation. Microsoft DirectX Specification, https://msdn.microsoft.com/en- us/library/windows/apps/hh452744.aspx . 19. Microsoft Corporation. Microsoft DirectX 9 Programmable Graphics Pipeline Redmond, WA: Microsoft Press; 2003. 20. Montrym, J., D. Baum, D. Dignam, C. Migdal [1997]. “InfiniteReality: Real-Time Graphics System.” Proc. SIGGRAPH 1997 (August), 293–301. 21. Montrym J, Moreton H. GeForce 6800. IEEE Micro . 13902005;Vol. 25(no. 2(March–April)):41–51. 22. Moore GE. Cramming components onto integrated circuits. Electronics . 1965;Vol. 38 (April 19). 23. Nguyen H, ed. GPU Gems 3 . Reading, MA: Addison-Wesley; 2008. 24. Nickolls J, Buck I, Garland M, Skadron K. Scalable Parallel Programming CUDA. ACM Queue . 2008;Vol. 6(no. 2(March–April)):40–53. 25. NVIDIA [2007]. CUDA Zone. http://www.nvidia.com/object/cuda_home_new.html . 26. NVIDIA [2007]. CUDA Programming Guide 1.1 . https://developer.nvidia.com/nvidia-gpu-programming- guide . 27. NVIDIA [2007]. PTX: Parallel Thread Execution ISA version 1.1. www.nvidia.com/object/io_1195170102263.html . 28. Nyland L, Harris M, Prins J. Fast N-Body Simulation CUDA. In: Nguyen H, ed. GPU Gems 3 . Reading, MA: Addison-Wesley; 2007. 29. Oberman, S. F. M. Y. Siu [2005]. “A High-Performance Area- Efficient Multifunction Interpolator,” Proc. Seventeenth IEEE Symp. Computer Arithmetic , 272–79. 30. Patterson DA, Hennessy JL. Computer Organization Design: Hardware/Software Inter face third edition San Francisco: Morgan Kaufmann Publishers; 2004. 31. Pharr M, ed. GPU Gems 2: Programming Techniques High- Performance Graphics General-Purpose Computation . Reading, MA: Addison-Wesley; 2005. 32. Satish, N., M. Harris, M. Garland [2008]. “Designing Efficient Sorting Algorithms Manycore GPUs,” NVIDIA Technical Report NVR-2008-001. 33. Segal, M. K. Akeley [2006]. OpenGL Graphics System: Specification, Version 2.1, Dec. 1, 2006 . www.opengl.org/documentation/specs/ . 34. Sengupta, S., M. Harris, Y. Zhang, J. D. Owens [2007]. “Scan Primitives GPU Computing.” Proc. Graphics Hardware 2007 (August), 97–106. 35. Volkov, V. J. Demmel [2008]. “LU, QR Cholesky Factorizations using Vector Capabilities GPUs,” Technical Report No. UCB/EECS-2008-49, 1–11. 1391http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS- 2008-49.pdf . 36. Williams, S., L. Oliker, R. Vuduc, J. Shalf, K. Yelick, J. Demmel [2007]. “Optimization sparse matrix-vector multiplication emerging multicore platforms,” Proc. Supercomputing 2007, November. 1Adapted Nyland et al. [2007] , “Fast N-Body Simulation CUDA,” Chapter 31 GPU Gems 3 . 2The x86 SSE instructions reciprocal-square-root (RSQRT*) reciprocal (RCP*) considered, accuracy low comparable. 3Intel Corporation, Intel 64 IA-32 Architectures Optimization Reference Manual . November 2007. Order Number: 248966-016. Also available www.intel.com/design/processor/manuals/248966.pdf . 1392APPENDIX C Mapping Control Hardware C.1 Introduction C-3 C.2 Implementing Combinational Control Units C-4 C.3 Implementing Finite-State Machine Control C-8 C.4 Implementing Next-State Function Sequencer C-22 C.5 Translating Microprogram Hardware C-28 C.6 Concluding Remarks C-32 C.7 Exercises C-33 custom format slave architecture hardware instruction set serves. format must strike proper compromise ROM size, ROM-output decoding, circuitry size, machine execution rate. Jim McKevit, et al.8086 design report, 1997 C.1 Introduction Control typically two parts: combinational part lacks state sequential control unit handles sequencing main control multicycle design. Combinational control units often used handle part decode control process. ALU control Chapter 4 example. single-cycle implementation like Chapter 4 also use combinational controller, since require multiple states. Section C.2 examines implementation two combinational units truth tables Chapter 4 . 1393Since sequential control units larger often complex, wider variety techniques implementing sequential control unit. usefulness techniques de pends complexity control, characteristics average number next states given state, implementation technology. straightforward way implement sequential control function block logic takes inputs current tate opcode field Instruction register produces outputs datapath control signals value next state. initial representation may either finite-state diagram microprogram. latter case, microinstruction represen ts state. implementation using finite-state controller, next-state function computed logic. Section C.3 constructs implementation ROM PLA. alternative method implementation computes next- state function using counter increments current st ate determine next state. next state doesn’t follow sequentially, logic used determine state. Section C.4 explores type implementation shows used implement finite-state control. Section C.5 , show microprogram representation sequential control translated control logic. C.2 Implementing Combinational Control Units section, show ALU control unit main control unit single clock design mapped gate level. modern computer-aided design (CAD) systems, process completely mechanical. examples illustrate CAD system takes advantage structure control function, includi ng presence don’t-care terms. Mapping ALU Control Function Gates Figure C.2.1 shows truth table ALU control function 1394was developed Chapter 4 , Section 4.4 . logic block implements ALU control function four distinct outputs (called Operation3, Operation2, Operation1, Operation0), corresponding one four bits ALU control last column Figure C.2.1 . logic function output constructed combining truth table entr ies set particular output. example, low-order bit ALU control (Operation0) set last two entries trut h table Figure C.2.1 . Thus, truth table Operation0 two entries. FIGURE C.2.1 truth table four ALU control bits (called Operation) function th e ALUOp function code field. table shown Figure 4.13 . Figure C.2.2 shows truth tables four ALU control bits. taken advantage common structure truth table incorporate additional don’t cares. example, five lines truth table Figure C.2.1 set Operation1 reduced two entries Figure C.2.2 . logic minimization program use don’t-care terms reduce number gates number inputs gate logic gate realization truth tables. 1395FIGURE C.2.2 truth tables three ALU control lines. entries output 1 shown. bits field numbered right lef starting 0; thus F5 significant bit function field, F0 least significant bit. Similarly, names signals corresponding 4-bit operation code supplied ALU Operation3, Operation2, Operation1, Operation0 (with last least significant bit). Thus th e truth table shows input combinations ALU control 0010, 0001, 0110, 0111 (the combinations used). ALUOp bits named ALUOp1 ALUOp0. three output values depend 2-bit ALUOp field and, field equal 10, 6-bit functi code instruction. Accordingly, ALUOp field equal 10, don’t care function code value (it represented X). truth table Operation3=1 always set 0 Figure C.2.1 . See Appendix background don’t cares. confusing aspect Figure C.2.2 logic function Opera-tion3. control line onl used operation, needed RISC-V 1396subset Figure 4.12 . simplified truth table Figure C.2.2 , generate logic shown Figure C.2.3 , call ALU control block. process straightforward done CAD program. example logic gates derived truth tables given legend Figure C.2.3 . FIGURE C.2.3 ALU control block generates four ALU control bits, based function code ALUOp bits. logic generated directly truth table Figure C.2.2 . 4 6 bits function code actually needed inputs, since upper 2 bits always don’t cares. Let’s examine logic relates truth table Figure C.2.2 . Consider Operation2 output, generated two lines truth table Operation2. second line two terms (F1 =1 ALUOp1 =1); top two-input gate corresponds term. term causes Operation2 asserted simply ALUOp0. two terms combined gate whose output Operation2. outputs Operation0 Operation1 derived similar fashion truth table. Since Operation3 always 0, connect signal complement inputs gate generate 0. ALU control logic simple three 1397outputs, possible input combinations need recognized. large number possible ALU function codes transformed ALU control signals, simple method would efficient. Instead, could use decoder, memory, structured array logic gates. techniques describe Appendix , see examples examine implementation multicycle controller Section C.3 . Elaboration general, logic equation truth table representation logic function equivalent. (We discuss detail Appendix ). However, truth table specifies entries result nonzero outputs, may completely describe logic function. full truth table completely indi cates don’t-care entries. example, encoding 11 ALUOp always generates don’t care output. Thus complete truth table would XXX output portion entries 11 ALUOp field. don’t-care entries allow us replace ALUOp field 10 01 1X X1, respectively. Incorporating don’t-care terms minimizing logic complex error-prone and, thus, better left program. Mapping Main Control Function Gates Implementing main control function unstructured collection gates, ALU control, reasonable control function neither complex large, see truth table shown Figure C.2.4 . However, 64 possible opcodes used many control lines, number gates would much larger gate could many inputs. 1398FIGURE C.2.4 control function simple one-clock implementation completely specified truth table. table shown Figure 4.22 . Since function computed two levels logic, another way implement logic function structured two- level logic array. Figure C.2.5 shows implementation. uses array gates followed array gates. structure called programmable logic array (PLA). PLA one common ways implement control function. return topic using structured logic elements imp lement control implement finite-state controller next section. 1399FIGURE C.2.5 structured implementation control function described truth table Figure C.2.4 . structure, called programmable logic array (PLA), uses array gates followed array gates. inputs gates function inputs inverses (bubbles indicate inversion signal). inputs gates outputs gates (or, degenerate case, function inputs inverses). output gates function outputs. C.3 Implementing Finite-State 1400Machine Control implement control finite-state machine, must first assign number 10 states; state could use number, use sequential numbering simplicity . Figure C.3.1 shows finite-state diagram. 10 states, need 4 bits encode state number, call state bits S3, S2, S1, S0. current-state number stored state register, shown Figure C.3.2 . states assigned sequentially, state encoded using state bits binary number i. example, state 6 encoded 0110two S3 =0, S2 =1, S1 =1, S0 =0, also written 1401FIGURE C.3.1 finite-state diagram multicycle control. 1402FIGURE C.3.2 control unit RISC-V consist control logic register hol state. state register written active clock edge stable clock cycle. control unit outputs specify next state. written state register clock edge become new state beginning next clock cycle following acti clock edge. name outputs NS3, NS2, NS1, NS0. 1403we determined number inputs, states, outputs, know basic outline control unit look like, show Figure C.3.2 . block labeled “control logic” Figure C.3.2 combinational logic. think big table giving value outputs terms inputs. logic block implemen ts two different parts finite-state machine. One part th e logic determines setting datapath control outputs , depend state bits. part control logic implements next-state function; equations dete rmine values next-state bits based current-state bits inputs (the 6-bit opcode). Figure C.3.3 shows logic equations: top portion shows outputs, bottom portion shows next-state function. values table determined state diagram Figure C.3.1 . Whenever control line active state, state entered second column table. Likewise, next-state entries made whenever one state successor another. 1404FIGURE C.3.3 logic equations control unit shown shorthand form. Remember “+” stands logic equations. state inputs NextState outputs must expanded using state encoding. blank entry don’t care. Figure C.3.3 , use abbreviation state N stand current state N. Thus, state N replaced term encodes state number N. use NextState N stand setting next-state outputs N. output implemented using next-state outputs (NS). NextState N active, bits NS[3–0] set corresponding binary version value N. course, since given next-state bit activated multiple next states, equation state bit terms th activate signal. Likewise, use term (Op =‘lw’), corresponds opcode inputs 1405specifies encoding opcode lw 6 bits, simple control unit previous section chapter . Translating entries Figure C.3.3 logic equations outputs straightforward. Logic Equations Next-State Outputs Example Give logic equation low-order next-state bit, NS0. Answer next-state bit NS0 active whenever next state NS0 =1 state encoding. true NextState1, NextState3, NextState5, NextState7, NextState9. entries states Figure C.3.3 supply conditions next- state values active. equation next states given below. first equation states next state 1 current state 0; current state 0 state input bits 0, rightmost product term indicate s. NS0 logical sum terms. seen, control function expressed logic equation output. set logic equations implemented two ways: corresponding complete truth table , corresponding two-level logic structure allows spar se encoding truth table. look implementations, let’s look truth table complete 1406control function. simplest break control function defined Figure C.3.3 two parts: next-state outputs, may depend inputs, control signal outputs, depend current-state bits. Figure C.3.4 shows truth tables datapath control signals. signals actually depend state bits (and opcode), entries table Figure C.3.4 actually represents 64 (=26) entries, 6 bits named Op possible values; is, Op bits don’t-care bits determining data path control outputs. Figure C.3.5 shows truth table next-state bits NS[3–0], depend state input bits instruction bits, supply opcode. Elaboration many opportunities simplify control function b observing similarities among two control signals using semantics implementation. example, signals PCWriteCond, PCSource0, ALUOp0 asserted exactly one state, state 8. three control signals replaced single signal. 1407FIGURE C.3.4 truth tables shown 16 datapath control signals depend current-state input bits, shown table. truth table row corresponds 64 entries: one possible value six Op bits. Notice outputs active nearly circumstances. example, case PCWriteCond, PCSource0, ALUOp0, signals active state 8 (see b, i, k). three signals could replaced one signal. opportunities reducing logic needed implement control function taking advantage similarities truth tables. 1408FIGURE C.3.5 four truth tables four next-state output bits (NS[3–0]). next-state outputs depend value Op[5-0], opcode field, current state, given S[3–0]. entries X don’t-care terms. entry don’t-care term corresponds two entries, one input 0 one input 1. Thus entry n don’t-care terms actually corresponds 2n truth table entries. ROM Implementation Probably simplest way implement control function encode truth tables read-only memory (ROM). number 1409of entries memory truth tables Figures C.3.4 C.3.5 equal possible values inputs (the 6 opcode bits plus 4 state bits), 2# inputs=210=1024. inputs control unit become address lines ROM, implements control logic block shown Figure C.3.2 . width entry (or word memory) 20 bits, since 16 datapath control outputs 4 next-state bits. means total size ROM 210×20=20 Kbits. setting bits word ROM depends outputs active word. look control words, need order bits within control input (the address) output words (the contents), respectively. number bits using order Figure C.3.2 , next-state bits low-order bits control word current- state input bits low-order bits address. means PCWrite output high-order bit (bit 19) memory word, NS0 low-order bit. high-order address bit given Op5, high-order bit th e instruction, low-order address bit given S0. construct ROM contents building entire truth table form row corresponds one 2n unique input combinations, set columns indicates outputs active input combination. don’t space show 1024 entries truth table. However, separating datapath control next-state outputs, do, since datapath control outputs depend current state. truth table datapath control outputs shown Figure C.3.6 . include encodings state inputs use (that is, values 0 9 corresponding 10 states state machine). 1410FIGURE C.3.6 truth table 16 datapath control outputs, depend state inputs. values determined Figure C.3.4 . Although 16 possible values 4-bit state field, 10 used shown here. 10 possible values shown top; column shows setting datapath control outputs state input value appears op column. example, state inputs 0011 (state 3), active datapath control outputs IorD MemRead. truth table Figure C.3.6 directly gives contents upper 16 bits word ROM. 4-bit input field gives low-order 4 address bits word, column gives contents word address. show full truth table datapath control bits state number opcode bits inputs, opcode inputs would don’t cares. construct ROM, cannot don’t cares, since addresses ROM must complete. Thus, datapath control outputs occur many times ROM, since part ROM whenever state bits identical, independent value opcode inputs. 1411 Control ROM Entries Example ROM addresses bit corresponding PCWrite, high bit control word, 1? Answer PCWrite high states 0 9; corresponds addresses 4 low-order bits either 0000 1001. bit high memory word independent inputs Op[5–0], addresses bit high 000000000, 0000001001, 0000010000, 0000011001, . . . , 1111110000, 1111111001. general form XXXXXX0000 XXXXXX1001, XXXXXX combination bits, corresponds 6-bit opcode output depend. show entire contents ROM two parts make easier show. Figure C.3.7 shows upper 16 bits control word; comes directly Figure C.3.6 . datapath control outputs depend state inputs, set words would duplicated 64 times full ROM, discussed above. entries corresponding input values 1010 1111 used, care contain. FIGURE C.3.7 contents upper 16 bits ROM depend state inputs. values Figure C.3.6 , simply rotated 90°. set control words would duplicated 64 times every possible value 1412upper six bits address. Figure C.3.8 shows lower four bits control word corresponding next-state outputs. last column table Figure C.3.8 corresponds possible values opcode match specified opcodes. state 0, next state always state 1, since instruction still fetche d. state 1, opcode field must valid. table indicates thi entries marked illegal; discuss deal exceptions interrupts opcodes Section 4.9 . FIGURE C.3.8 table contains lower 4 bits control word (the NS outputs), depend state inputs, S[3–0], opcode, Op[5–0], correspond instruction opcode. values determined Figure C.3.5 . opcode name shown encoding heading. four bits control word whose address given current-state bits Op bits shown entry. example, state input bits 0000, output always 0001, independent inputs; state two , next state don’t care three inputs, th ree lw, five sw. Together entries Figure C.3.7 , table specifies contents control unit ROM. example, word address 1000110001 obtained finding upper 16 bits table Figure C.3.7 using state input bits (0001) concatenating lower four bits found 1413using entire address (0001 find row 100011 find column). entry Figure C.3.7 yields 0000000000011000, appropriate entry table immediately 0010. Thus control word address 1000110001 00000000000110000010. column labeled “Any value” applies Op bits match one specified opcodes. representation two separate tables compact way show ROM contents; also efficient way implement ROM. majority outputs (16 20 bits) depends four 10 inputs. number bits total control implemented two separate ROMs 24 × 16 +210 × 4 =256 +4096 =4.3 Kbits, one-fifth size single ROM, requires 210 × 20 =20 Kbits. overhead associated structured-logic block, case additional overhead extra ROM would much smaller savings splitting single ROM. Although ROM encoding control function simple, wasteful, even divided two pieces. example, values Instruction register inputs often needed determine next state. Thus, next-state ROM many entries either duplicated don’t care. Consider case machine state 0: 26 entries ROM (since opcode field value), entries contents (namely, control word 0001). reason much ROM wasted ROM implements complete truth table, providing opportuni ty different output every combination inputs. combinations inputs either never happen redundant! PLA Implementation reduce amount control storage required cost using complex address decoding control inputs, wh ich encode input combinations needed. logic structure often used programmed logic array (PLA), mentioned earlier illustrated Figure C.2.5 . 1414In PLA, output logical one minterms. minterm , also called product term , simply logical one inputs. inputs thought address indexing PLA, minterms select possib le address combinations interesting. minterm corresponds single entry truth table, Figure C.3.4 , including possible don’t-care terms. output consists minterms, exactly corresponds complete truth table. However, unlike ROM, truth table entries produce active output needed, one copy minterm required, even minterm contains don’t cares. Figure C.3.9 shows PLA implements control function. 1415FIGURE C.3.9 PLA implements control function logic multicycle implementation. inputs control appear left outputs right. top half figure plane computes minterms. minterms carried plane vertical lines. colored dot corresponds signal 1416makes minterm carried line. sum terms computed minterms, gray dot representing presence intersecting minterm sum term. output consists single sum term. see PLA Figure C.3.9 , 17 unique minterms—10 depend current state seven others depend combination Op field current-state bits. total size PLA proportional (#inputs ×#product terms)+(#outputs ×#product terms), see symbolically figure. means total size PLA Figure C.3.9 proportional (10 ×17)+(20 ×17)=510. comparison, size single ROM proportional 20 Kb, even two-part ROM total 4.3 Kb. size PLA cell slightly larger size bit ROM, PLA much efficient implementation control unit. course, split ROM two, could split PLA two PLAs: one four inputs 10 minterms generates 16 control outputs, one 10 inputs seven minterms generates four next-state outputs. first P LA would size proportional (4 ×10)+(10 ×16)=200, second PLA would size proportional (10 ×7)+(4 ×7)=98. would yield total size proportional 298 PLA cells, 55% size single PLA. two PLAs considerably smaller implementation using two ROMs. details PLAs implementation, well references books logic design, see Appendix . C.4 Implementing Next-State Function Sequencer Let’s look carefully control unit built last secti on. examine ROMs implement control Figures C.3.7 C.3.8 , see much logic used specify next-state function. fact, implementation using two separate ROMs, 4096 4368 bits (94%) correspond next-state function! Furthermore, imagine control logi c 1417would look like instruction set many different instruction types, required many clocks implement. would many states finite-state machine. states, might branching large number different states depending instruction type (as n state 1 finite-state machine Figure C.3.1 ). However, many states would proceed sequential fashion, states 3 4 Figure C.3.1 . example, included floating point, would see sequence many states row implement multicycle floating-point instruction. Alternatively, consider co ntrol might look machine multiple memory operands per instruction. would require many states fetch mult iple memory operands. result would control log ic dominated encoding next-state function. Furthermore, much logic devoted sequences states one path look like states 2 4 Figure C.3.1 . instructions, sequences consist many sequentially numbered states simple subset. encode complex control functions efficiently , use control unit counter supply sequential next state. counter often eliminates need encode next-state function explicitly control unit. shown n Figure C.4.1 , adder used increment state, essentially turning counter. incremented state always state follows numerical order. However, finite-state machine sometimes “branches.” example, state 1 finite-state machine (see Figure C.3.1 ), four possible next states, one sequential next state. Thus, need able choose incremented state new state based inputs Instruction register current state. control word include control lines determine h ow next state chosen. 1418FIGURE C.4.1 control unit using explicit counter compute next state. control unit, next state computed using counter (at least states). comparison, Figure C.3.2 encodes next state control logic every state. control unit, signals labeled AddrCtl control next state determined. easy implement control output signal portion control word, since, use state numbers, portion control word look exactly like ROM contents sho wn Figure C.3.7 . However, method selecting next state differs next-state function finite-state machine . explicit counter providing sequential next state, control unit logic need specify choose state wh en 1419is sequentially following state. two methods this. first method already seen: namely, control unit explicitly encodes next-state function. difference control unit need set next-stat e lines designated next state state counter indicates. number states large next-state funct ion need encode mostly empty, may good choice, since resulting control unit lots empt redundant space. alternative approach use separate external logic specify next state counter n ot specify state. Many control units, especially implement large instruction sets, use approach, focus specifying control externally. Although nonsequential next state come external table, control unit needs specify occur find next state. two kinds “branching” must implement address select logic. First, must able jump one number states based opcode portion Instruction register. operation, called dispatch , usually implemented using set special ROMs PLAs included part address selection logic. additional set control outputs, call AddrCtl, indicates dispatch done. Looking finite-state diagram ( Figure C.3.1 ), see two states branch based portion opcode. Thus need two small dispatch tables. (Alternatively, could also use single dispatch table use control bits select table address bits choose portion dispatch table select address.) second type branching must implement consists branching back state 0, initiates execution next RISC-V instruction. Thus four possible ways choose next state (three types branches, plus incrementing curre nt- state number), encoded 2 bits. Let’s assume encoding follows: 1420If use encoding, address select logic contr ol unit implemented shown Figure C.4.2 . 1421FIGURE C.4.2 address select logic control unit Figure C.4.1 . complete control unit, need specify content dispatch ROMs values address-control lines state. already specified datapath control portion control word using ROM contents Figure C.3.7 (or corresponding portions PLA Figure C.3.9 ). next-state counter dispatch ROMs take place portion control unit computing next state, shown Figure C.3.8 . implementing portion instruction set, dispatch ROMs largely empty. Figure C.4.3 shows entries must assigned subset. 1422FIGURE C.4.3 dispatch ROMs 26 = 64 entries 4 bits wide, since number bits state encoding. figure shows entries ROM interest subset. first column table indicates value Op, address used access dispatch ROM. second column shows symbolic name opcode. third column indicates value address ROM. determine setting address selection lines (AddrCtl) control word. table Figure C.4.4 shows address control must set every state. information used specify setting AddrCtl field control word associated state. FIGURE C.4.4 values address-control lines set control word corresponds state. contents entire control ROM shown Figure C.4.5 . total storage required control quite small. ar e 10 control words, 18 bits wide, total 180 bits. addition, two dispatch tables 4 bits wide 64 entries, total 512 additional bits. total 692 bits beats implementation uses two ROMs next-state function encoded ROMs (which requires 4.3 Kbits). 1423FIGURE C.4.5 contents control memory implementation using explicit counter. first column shows state, second shows datapath control bits, last column shows address-control bits control word. Bits 17–2 identical Figure C.3.7 . course, dispatch tables sparse could efficiently implemented two small PLAs. control ROM could also replaced PLA. Optimizing Control Implementation reduce amount logic control unit two different techniques. first logic minimization , uses structure logic equations, including don’t-care te rms, reduce amount hardware required. success process depends many entries exist truth table, entries related. example, subset, lw sw opcodes active value signal Op5, replace two truth table entries test whether input lw sw single test bit; similarly, eliminate several bits used index dispatch ROM single bi used find lw sw first dispatch ROM. course, opcode space less sparse, opportunities optimization would difficult locate. However, choosing opcodes, architect provide additional opportunities choosing related opcodes instructions likely share states control. different sort optimization done assigning state numbers finite-state microcode implementation 1424minimize logic. optimization, called state assignment , tries choose state numbers resulting logic equati ons contain redundancy thus simplified. Let’s consider case finite-state machine encoded next-state contr ol first, since allows states assigned arbitrarily. example , notice finite-state machine, signal RegWrite acti states 4 7. encoded states 8 9, rather 4 7, could rewrite equation RegWrite simply test bit S3 (which states 8 9). renumbering allows us combine two truth table entries part (o) Figure C.3.4 replace single entry, eliminating one term control unit. course, would renumber existing states 8 9, perhaps 4 7. optimization applied implementation uses explicit program counter, though restricted. next-state number often computed incrementing current-state number, cannot arbitrarily assign states. However, keep states incremented state used next state order, reassign consecutive states block. implementation explicit next-state counter, state assignment may allow us simplify contents dispatch ROMs. look control unit Figure C.4.1 , looks remarkably like computer right. ROM PLA thought memory supplying instructions datapath. state thought instruction address. Hence origin name microcode microprogrammed control. control words thought microinstructions control datapath, State register called microprogram counter. Figure C.4.6 shows view control unit microcode . next section describes map microprogram microcode. 1425FIGURE C.4.6 control unit microcode. use word “micro” serves distinguish program counter datapath microprogram counter, microcode memory instruction memory. C.5 Translating Microprogram Hardware translate microprogram actual hardware, need specify field translates control signals. implement microprogram either finite-state control microcode implementation explicit sequencer. cho ose finite-state machine, need construct next-state funct ion microprogram. function known, map set truth table entries next-state outputs. se ction, show translate microprogram, assuming 1426next state specified sequencer. truth tables wi construct, would straightforward build next-state function finite-state machine. Assuming explicit sequencer, need two additional tasks translate microprogram: assign addresses microinstructions fill contents dispatch ROMs. process essentially process translating assembly language program machine instructions: fields assembly language microprogram instruction translated, labels instructions must resolved addresses. Figure C.5.1 shows various values microinstruction field controls datapath fields encoded control signals. field corresponding signal affects unit state (i.e., Memory, Memory register, ALU destination, PCWriteControl) blank, control signal active . field corresponding multiplexor control signal ALU operation control (i.e., ALUOp, SRC1, SRC2) blank, output unused, associated signals may set don’t care. 1427FIGURE C.5.1 microcode field translates set control signals set. 22 different values fields specify required combinations 18 control lines. Control lines set, correspond actions, 0 default. Multiplexor control lines set 0 output matters. multiplexor control line explicitly set, output don’t care used. sequencing field four values: Fetch (meaning go Fetch state), Dispatch 1, Dispatch 2, Seq. four values encoded set 2-bit address control Figure C.4.4 : Fetch =0, Dispatch 1 =1, Dispatch 2 =2, Seq =3. Finally, need specify contents dispatch tables relate dispatch entries sequence field symbolic labels n microprogram. use dispatch tables earlier Figure C.4.3 . 1428A microcode assembler would use encoding sequencing field, contents symbolic dispatch tables Figure C.5.2 , specification Figure C.5.1 , actual microprogram generate microinstructions. FIGURE C.5.2 two microcode dispatch ROMs showing contents symbolic form using labels microprogram. Since microprogram abstract representation control, great deal flexibility microprog ram translated. example, address assigned many microinstructions chosen arbitrarily; restricti ons imposed fact certain microinstructions must ccur sequential order (so incrementing State register generates address next instruction). Thus microco de assembler may reduce complexity control assigning microinstructions cleverly. Organizing Control Reduce Logic machine complex control, may great deal logic control unit. control ROM PLA may costly. Although simple implementation 18-bit microinstruction (assuming explicit sequencer), machines microinstructions hundreds bits wide . Clearly, designer would like reduce number microinstructions width. ideal approach reducing control store first write th e complete microprogram symbolic notation measure control lines set microinstruction. taking measurements able recognize control bits 1429encoded smaller field. example, one eight lines set simultaneously microinstruction, hen subset control lines encoded 3-bit field (lo g2 8 =3). change saves five bits every microinstruction hurt CPI, though mean extra hardware cost 3-to- 8 decoder needed generate eight control lines ar e required datapath. may also small clock cycle impact, since decoder signal path. However, shaving five bits control store width usually overcome co st decoder, cycle time impact probably small nonexistent. example, technique applied bits 13–6 microinstructions machine, since one even bits control word ever active (see Figure C.4.5 ). technique reducing field width called encoding . save space, control lines may encoded together occasionally set microinstruction; two microinstructions instead one required mus set. long doesn’t happen critical routines, narrower microinstruction may justify extra words contr ol store. Microinstructions made narrower still broken different formats given opcode format field distinguish them. format field gives unspecified co ntrol lines default values, change anything else machine, similar opcode instruction powerful instruction set. example, could use different format microinstructions memory accesses thos e register-register ALU operations, taking advantage fact memory access control lines needed microinstructions controlling ALU operations. Reducing hardware costs using format fields usually additional performance cost beyond requirement decoders. microprogram using single microinstruction form specify combination operations datapath take fewer clock cycles microprogram made restricted microinstructions cannot perform combination operations single microinstruction. However, full capability wider microprogram word heavily used, much control store wasted, machine 1430could made smaller faster restricting microinstruc tion capability. narrow, usually longer, approach often called vertical microcode , wide short approach called horizontal microcode. noted terms “vertical microcode” “horizontal microcode” universal definition—the designers 8086 considered 21-bit microinstruction horizontal single-chip computers time. related terms maximally encoded minimally encoded probably better vertical horizontal. C.6 Concluding Remarks began appendix looking translate finite-state diagram implementation using finite-state machine. looked explicit sequencers use different technique realizing next-state function. Although large microprograms ar e often targeted implementations using explicit next-st ate approach, also implement microprogram finite- state machine. saw, ROM PLA implementations logic functions possible. advantages explicit vers us encoded next state ROM versus PLA implementation summarized below. BIG Picture Independent whether control represented finite-stat e diagram microprogram, translation hardware control implementation similar. state microinstruction asserts set control outputs specifies choose next state . next-state function may implemented either encoding finite-state machine using explicit sequencer. explicit sequencer efficient number states large many sequences consecutive states without branching. control logic may implemented either ROMs PLAs (or even mix). PLAs efficient unless control function dense. ROMs may appropriate control stored separate memory, opposed within chip 1431as datapath. C.7 Exercises C.1 [10] <§C.2>Instead using four state bits implement finite-state machine Figure C.3.1 , use nine state bits, 1 finite-state machine particular state (e.g., S1 1 state 1, S2 1 state 2, etc.). Redraw PLA ( Figure C.3.9 ). C.2 [5] <§C.3>We wish add instruction jal (jump link). Make necessary changes datapath control signals needed. photocopy figures make faster show additions. many product terms required PLA implements control single-cycle datapath fo r jal? C.3 [5] <§C.3>Now wish add instruction addi (add immediate). Add necessary changes datapath control signals. many product terms required PLA implements control single-cycle datapath fo r addiu ? C.4 [10] <§C.3>Determine number product terms PLA implements finite-state machine addi . easiest way construct additions truth tables addi . C.5 [20] <§C.4>Implement finite-state machine using explicit counter determine next state. Fill new en tries additions Figure C.4.5 . Also, add entries needed dispatch ROMs Figure C.5.2 . C.6 [15] <§§C.3–C.6>Determine size PLAs needed implement multicycle machine, assuming next-state function implemented counter. Implement dispatch tables Figure C.5.2 using two PLAs contents main control unit Figure C.4.5 using another PLA. total size solution compare single PLA solutio n next state encoded? main PLAs approaches split two separate PLAs factoring next-state address select signals? 14321433APPENDIX Survey RISC Architectures Desktop, Server, Embedded Computers Steven Przybylskic, Designer Stanford MIPS D.1 Introduction D-3 D.2 Addressing Modes Instruction Formats D-5 D.3 Instructions: MIPS Core Subset D-9 D.4 Instructions: Multimedia Extensions Desktop/Server RISCs D-16 D.5 Instructions: Digital Signal-Processing Extensions Embedded RISCs D-19 D.6 Instructions: Common Extensions MIPS Core D-20 D.7 Instructions Unique MIPS-64 D-25 D.8 Instructions Unique Alpha D-27 D.9 Instructions Unique SPARC v9 D-29 D.10 Instructions Unique PowerPC D-32 D.11 Instructions Unique PA-RISC 2.0 D-34 D.12 Instructions Unique ARM D-36 1434D.13 Instructions Unique Thumb D-38 D.14 Instructions Unique SuperH D-39 D.15 Instructions Unique M32R D-40 D.16 Instructions Unique MIPS-16 D-40 D.17 Concluding Remarks D-43 RISC: computer announced 1985. D.1 Introduction cover two groups reduced instruction set computer (RISC) architectures appendix. first group desktop server RISCs: Digital Alpha Hewlett-Packard PA-RISC IBM Motorola PowerPC MIPS INC MIPS-64 Sun Microsystems SPARC second group embedded RISCs: Advanced RISC Machines ARM Advanced RISC Machines Thumb Hitachi SuperH Mitsubishi M32R MIPS INC MIPS-16 never another class computers similar. similarity allows presentation 10 architectures 50 pages. Characteristics desktop server RISCs found Figure D.1.1 embedded RISCs Figure D.1.2 . 1435FIGURE D.1.1 Summary first version five architectures desktops servers. Except number data address modes instruction set details, integer instruction sets architectures similar. Contrast Figure D.17.1 . Later versions architectures support flat, 64-bit address space. FIGURE D.1.2 Summary five architectures embedded applications. Except number data address modes instruction set details, integer instruction sets architectures similar. Contrast Figure D.17.1 . Notice embedded RISCs tend eight 16 general- purpose registers desktop/server RISCs 32, length instructions 16 32 bits embedded RISCs always 32 bits desktop/server RISCs. Although shown separate embedded instruction set architectures, Thumb MIPS-16 really optional modes ARM MIPS invoked call instructions. mode, execute subset native architecture using 16-bit-long 1436instructions. 16-bit instruction sets intended full architectures, enough encode procedures. Bot h machines expect procedures homogeneous, instructions either 16-bit mode 32-bit mode. Programs consist procedures 16-bit mode density 32-bit mode performance. One complication description older RISCs extended years. decided describe latest versions architectures: MIPS-64, Alpha version 3, PA-RISC 2.0, SPARC version 9 desktop/server; ARM version 4, Thumb version 1, Hitachi SuperH SH-3, M32R version 1, MIPS-16 version 1 embedded ones. remaining sections proceed follows: discussing addressing modes instruction formats RISC architectures, present survey instructions five steps: Instructions found MIPS core, defined Chapters 2 3 main text Multimedia extensions desktop/server RISCs Digital signal-processing extensions embedded RISCs Instructions found MIPS core found two architectures unique instructions characteristics 10 architectures give evolution instruction sets final secti conclude speculation future directions RISCs . D.2 Addressing Modes Instruction Formats Figure D.2.1 shows data addressing modes supported desktop architectures. Since one register always value 0 used address modes, absolute address mode limited range synthesized using zero base displacement addressing. (This register changed ALU operations PowerPC; always 0 machines.) Similarly, register indirect addressing synthesized usin g displacement addressing offset 0. Simplified addressing 1437modes one distinguishing feature RISC architectures. FIGURE D.2.1 Summary data addressing modes supported desktop architectures. PA-RISC also short address versions offset addressing modes. MIPS-64 indexed addressing floating-point loads stores. (These addressing modes described Figure 2.18 .) Figure D.2.2 shows data addressing modes supported embedded architectures. Unlike desktop RISCs, embedded machines reserve register contain 0. Although two three simple addressing modes, ARM SuperH several, including fairly complex calculations. ARM addressing mode shift one register amount, add registers form address, update one register new address. FIGURE D.2.2 Summary data addressing modes supported embedded architectures. SuperH M32R separate register indirect register + offset addressing modes rather putting 0 offset latter mode. incre ases use 16-bit instructions M32R, gives wider set address modes different data transfer instructions SuperH. get greater addressing range, ARM Thumb shift offset left one two bits data size halfword word. (These addressing modes described Figure 2.18 .) 1438References code normally PC-relative, although jump register indirect supported returning procedur es, case statements, pointer function calls. One variation PC- relative branch addresses shifted left 2 bits adde PC desktop RISCs, thereby increasing branch distance. works length instructions th e desktop RISCs 32 bits, instructions must aligned 32-bit words memory. Embedded architectures 16-bit-long instructions usually shift PC-relative address one imilar reasons. Figure D.2.3 shows format desktop RISC instructions, include size address. instruction set architecture uses four primary instruction formats. Figure D.2.4 shows six formats embedded RISC machines. desire smaller code size via 16-bit instructions leads mor e instruction formats. 1439FIGURE D.2.3 Instruction formats desktop/server RISC architectures. four formats found five architectures. (The superscrift notation figure means width field bits.) Although register fields located similar pieces instruction, aware destination two source fields scrambled. Op = main opcode, Opx = opcode extension, Rd = destination register, Rs1 = source register 1, Rs2 = source register 2, Const = 1440constant (used immediate address). Unlike RISCs, Alpha format immediates arithmetic logical operations different data transfer format shown here. provides 8-bit immediate bits 20 13 RR format, bits 12 5 remaining opcode extension. 14411442FIGURE D.2.4 Instruction formats embedded RISC architectures. six formats found five architectures. notation Figure D.2.3 . Note similarities branch, jump, call formats, diversity register-register, register-immediate, data transfer formats. differences result whether architecture eight 16 registers, whether two- three-operand format, whether instruction length 16 32 bits. Figures D.2.5 D.2.6 show variations extending constant fields full width registers. subtle point , RISCs similar identical. FIGURE D.2.5 Summary constant extension desktop RISCs. constants jump call instructions MIPS sign-extended, since replace lower 28 bits PC, leaving upper 4 bits unchanged. PA-RISC logical immediate instructions. FIGURE D.2.6 Summary constant extension embedded RISCs. 16-bit-length instructions much shorter immediates desktop RISCs, typically 5 8 bits. embedded RISCs, however, way get long address procedure calls 1443two sequencial halfwords. constants jump call instructions MIPS sign-extended, since replace lower 28 bits PC, leaving upper 4 bits unchanged. 8-bit immediates ARM rotated right even number bits 2 30, yielding large range immediate values. example, powers two immediates ARM. D.3 Instructions: MIPS Core Subset similarities architecture allow simultaneous descriptions, starting operations equivalent MI PS core. MIPS Core Instructions Almost every instruction found MIPS core found architectures, Figures D.3.1 D.3.5 show. (For reference, definitions MIPS instructions found MIPS Reference Data Card beginning book.) Instructions listed four categories: data transfer ( Figure D.3.1 ); arithmetic/logical ( Figure D.3.2 ); control ( Figure D.3.3 ); floating point ( Figure D.3.4 ). fifth category ( Figure D.3.5 ) shows conventions register usage pseudoinstructions architecture. MIPS core instruction requires short sequen ce instructions architectures, instructions se parated semicolons Figures D.3.1 D.3.5 . (To avoid confusion, destination register always leftmost operand th appendix, independent notation normally used architecture.) Figures D.3.6 D.3.9 show equivalent listing embedded RISCs. Note floating point generally defined embedded RISCs. 1444FIGURE D.3.1 Desktop RISC data transfer instructions equivalent MIPS core. sequence instructions synthesize MIPS instruction shown separated semicolons. several choices instructions equivalent MIPS core, separated commas. figure, halfword 16 bits word 32 bits. Note Alpha, LDS converts single-precision floating point double precision loads entire 64-bit register. FIGURE D.3.2 Desktop RISC arithmetic/logical instructions equivalent MIPS core. 1445Dashes mean operation available architecture, synthesized instructions. sequence instructions shown separated semicolons. several choices instructions equivalent MIPS core, separated commas. Note “Arithmetic/logical” category, machines SPARC use separate instruction mnemonics indicate immediate operand; SPARC offers immediate versions instructions uses single mnemonic. (Of course separate opcodes!) FIGURE D.3.3 Desktop RISC control instructions equivalent MIPS core. several choices instructions equivalent MIPS core, separated commas. FIGURE D.3.4 Desktop RISC floating-point instructions equivalent MIPS core. 1446Dashes mean operation available architecture, synthesized instructions. several choices instructions equivalent MIPS core, separated commas. FIGURE D.3.5 Conventions desktop RISC architectures equivalent MIPS core. FIGURE D.3.6 Embedded RISC data transfer instructions equivalent MIPS core. sequence instructions synthesize MIPS instruction shown separated semicolons. Note floating point generally defined embedded RISCs. Thumb MIPS-16 16-bit instruction subsets ARM MIPS architectures, machines switch modes execute full instruction set. use —1 show sequences available 32-bit mode 16- bit mode Thumb MIPS-16. 1447FIGURE D.3.7 Embedded RISC arithmetic/logical instructions equivalent MIPS core. Dashes mean operation available architecture, synthesized instructions. sequence instructions shown separated semicolons. several choices instructions equivalent MIPS core, separated commas. Thumb MIPS-16 16-bit instruction subsets ARM MIPS architectures, machines switch modes execute full instruction set. use —1 show sequences available 32-bit mode 16- bit mode Thumb MIPS-16. superscript 2 shows new instructions found 16-bit mode Thumb MIPS-16, CMP/I2. ARM includes shifts part every data operation instruction, shifts superscript 3 variation move instruction, LSR3. 1448FIGURE D.3.8 Embedded RISC control instructions equivalent MIPS core. Thumb MIPS-16 16-bit instruction subsets ARM MIPS architectures, machines switch modes execute full instruction set. use —1 show sequences available 32-bit mode 16-bit mode Thumb MIPS-16. superscript 2 shows new instructions found 16- bit mode Thumb MIPS-16, BTEQZ2. FIGURE D.3.9 Conventions embedded RISC instructions equivalent MIPS core. Every architecture must scheme compare conditional branch, despite similarities, architectures found different way perform operation. Compare Conditional Branch SPARC uses traditional four condition code bits stored program status word: negative, zero, carry , overflow . set arithmetic logical instruction; unlike earlier architectures, setting optional instruction. ex plicit option leads fewer problems pipelined implementation. Although condition codes set side effect operation, explicit compares synthesized subtract using r0 destination. SPARC conditional branches test condition codes determine possible unsigned signed relations. Floating poi nt uses separate condition codes encode IEEE 754 conditions, requiring floating-point compare instruction. Version 9 expande 1449SPARC branches four ways: separate set condition codes 64-bit operations; branch tests contents register branches value =, =, <, <=, >=, <= 0 (see MIPS below); three sets floating-point condition codes; branch instructions encode static branch prediction. PowerPC also uses four condition codes— less than, greater than, equal , summary overflow —but eight copies them. redundancy allows PowerPC instructions use different condition codes without conflict, essentially giving PowerPC eight extra 4-bit registers. eight condition codes target compare instruction, source conditional branch. integer instructions option bit th behaves integer op followed compare zero sets first condition “register.” PowerPC also lets second “register” optionally set floating-point instructions. Pow erPC provides logical operations among eight 4-bit condition c ode registers ( CRAND, CROR, CRXOR, CRNAND, CRNOR, CREQV ), allowing complex conditions tested single branch. MIPS uses contents registers evaluate conditional branches. two registers compared equality ( BEQ) inequality ( BNE), branch taken condition holds. set less instructions ( SLT, SLTI, SLTU, SLTIU ) compare two operands set destination register 1 less 0 otherwise. instructions enough synthesize full set relations. popularity comparisons 0, MIPS includes special compare branch instructions comparisons: greater equal zero ( BGEZ ), greater zero (BGTZ ), less equal zero ( BLEZ ), less zero ( BLTZ ). course, equal equal zero synthesized using r0 BEQ BNE. Like SPARC, MIPS uses condition code floating point separate floating-point compare branch instructions; MIPS IV expanded eight floating-point condition codes, floating point comparisons branch instructions specifying condition set test. Alpha compares ( CMPEQ , CMPLT , CMPLE , CMPULT , CMPULE ) test two registers set third 1 condition true 0 otherwise. Floating-point compares ( CMTEQ , CMTLT , CMTLE , CMTUN ) set result 2.0 condition holds 0 otherwise. branch instructions compare one register 0 ( BEQ, BGE, BGT, BLE, 1450BLT, BNE ) least significant bit 0 ( BLBC, BLBS ) branch condition holds. PA-RISC many branch options, we’ll see Section D.11 . straightforward compare branch instruction (COMB ), compares two registers, branches depending standard relations, tests least significant bit result comparison. ARM similar SPARC, provides four traditional condition codes optionally set. CMP subtracts one operand difference sets condition codes. Comp negative ( CMN) adds one operand other, sum sets condition codes. TST performs logical two operands set condition codes overflow, TEQ uses exclusive set first three condition codes. Like SPARC, conditional version ARM branch instruction tests condition codes determine possible unsigned signed relations. shall see Section D.12 , one unusual feature ARM every instruction option executing conditional ly depending condition codes. (This bears similarities annulling option PA-RISC, seen Section D.11 .) surprisingly, Thumb follows ARM. differences setting condition codes optional, TEQ instruction dropped, conditional execution instructions . Hitachi SuperH uses single T-bit condition set compare instructions. Two branch instructions decide branch f either bit 1 (BT) bit 0 (BF). two flavors branches allow fewer comparison instructions. Mitsubishi M32R also offers single condition code bit (C) used signed unsigned comparisons ( CMP, CMPI, CMPU, CMPUI ) see one register less not, similar MI PS set less instructions. Two branch instructions test se e C bit 1 0: BC BNC. M32R also includes instructions branch equality inequality registers ( BEQ BNE) relations register 0 ( BGEZ, BGTZ, BLEZ, BLTZ, BEQZ, BNEZ ). Unlike BC BNC, last instructions 32 bits wide. MIPS-16 keeps set less instructions ( SLT, SLTI, SLTU, SLTIU ), instead putting result one eight registers , placed special register named T. MIPS-16 always implemented machines also full 32-bit MIPS 1451instructions registers; hence, register really registe r 24 full MIPS architecture. MIPS-16 branch instructions test see register equal zero ( BEQZ BNEZ ). also instructions branch register equal zero (BTEQZ BTNEZ ). test two registers equal, MIPS added compare instructions ( CMP, CMPI ) compute exclusive two registers place result register T. Compare added since MIPS-16 left instructions compare branch registers equal ( BEQ BNE). Figures D.3.10 D.3.11 summarize schemes used conditional branches. FIGURE D.3.10 Summary five desktop RISC approaches conditional branches. Floating-point branch PA-RISC accomplished copying FP status register integer register using branch bit instruction test FP comparison bit. Integer compare SPARC synthesized arithmetic instruction sets condition codes using r0 destination. FIGURE D.3.11 Summary five embedded RISC approaches conditional branches. 1452D.4 Instructions: Multimedia Extensions Desktop/Server RISCs Since every desktop microprocessor definition graphical displays, transistor budgets increased inevitabl e support would added graphics operations. Many graphics systems use 8 bits represent three primary colors plus 8 bits location pixel. addition speakers microphones teleconferencing video games suggested support sound well. Audio samples need 8 bits precision, 16 bits sufficient. Every microprocessor special support bytes halfwords take less space stored memory, due infrequency arithmetic operations data sizes typical integer programs, little support beyond data transfers. architects Intel i860, justified graphical accelerator within company, recognized many graphics audio applications would perform operation vectors data. Although vector unit beyond transistor budget i860 1989, partitioning carry chains within 64-bit ALU, could perform simultaneous operations short vectors eight 8-bit operands, four 16-bit operands, two 32-bit operands. cost partitioned ALUs small. Applications lend support include MPEG (video), games like DOOM (3-D graphics), Adobe Photoshop (digital photography), teleconferencing (audio image processing). Like virus, time multimedia support spread nearly every desktop microprocessor. HP first successf ul desktop RISC include support. shall see, virus spread unevenly. PowerPC holdout, rumors “running fever.” extensions called subword parallelism, vector, SIMD (single-instruction, multiple data) (see Chapter 6 ). Since Intel marketing uses SIMD describe MMX extension 8086, become popular name. Figure D.4.1 summarizes 1453the support architecture. FIGURE D.4.1 Summary multimedia support desktop RISCs. B stands byte (8 bits), H half word (16 bits), nd W word (32 bits). Thus 8B means operation eight bytes single instruction. Pack unpack use notation 2*2W mean two operands two words. Note MDMX vector/scalar operations, scalar specified element one vector registers. table simplification full multimedia architectures, leaving many details. example, MIPS MDMX includes instructions multiplex two operands, HP MAX2 includes instruction calculate averages, SPARC VIS includes instructions set registers constants. Also, table include memory alignment operation MDMX, MAX, VIS. Figure D.4.1 , see that, general, MIPS MDMX works eight bytes four halfwords per instruction, HP PA- RISC MAX2 works four half-words, SPARC VIS works four halfwords two words, Alpha doesn’t much. Alpha MAX operations byte versions compare, min, max, absolute difference, leaving software isolate fields perform parallel adds, subtracts, multiplies bytes 1454halfwords. MIPS also added operations work two 32-bit floating-point operands per cycle, considered part f MIPS V simply multimedia extensions (see Section D.7 ). One feature generally found general-purpose microprocessors saturating operations. Saturation means calculation overflows, result set largest posi tive number negative number, rather modulo calculation two’s complement arithmetic. Commonly found digital signal processors (see next section), saturating oper ations helpful routines filtering. machines largely used existing register sets hold operands: integer registers Alpha HP PA-RISC floating- point registers MIPS Sun. Hence data transfers accomplished standard load store instructions. MIPS also added 192-bit (3*64) wide register act accumulator operations. three times native data width, partitioned accumulate either eight bytes 24 bits per fie ld four halfwords 48 bits per field. wide accumulator used add, subtract, multiply/ add instructions. MIPS claims performance advantages two four times accumulator. Perhaps surprising conclusion table lack consistency. operations found four logical operations (AND, OR, XOR), need partitioned ALU. leave frugal Alpha, common operations parallel adds subtracts four halfwords. manufacturer states instructions intended used hand-optimized subroutine libraries, intention likel followed, compiler works well multimedia extensions desktop RISCs would challenging. D.5 Instructions: Digital Signal- Processing Extensions Embedded RISCs One feature found every digital signal processor (DSP) architecture support integer multiply-accumulate. 1455multiplies tend shorter words regular integers, suc h 16 bits, accumulator tends longer words, 64 bits. reason multiply-accumulate efficiently implement digital filters, common DSP applications. Since Thumb MIPS-16 subset architectures, provide support. Instead, programmers use DSP multimedia extensions found 32-bit mode instructions ARM MIPS-64. Figure D.5.1 shows size multiply, size accumulator, operations instruction names embedded RISCs. Machines accumulator sizes greater 32 less 64 bits force upper bits remain sign bits, thereby “saturating” add set maximum minimum fixed-point values operations overflow. FIGURE D.5.1 Summary five embedded RISC approaches multiply-accumulate. D.6 Instructions: Common Extensions MIPS Core Figures D.6.1 D.6.7 list instructions found Figures D.3.5 D.3.11 four categories. Instructions put lists appear one standard architectures. instructions defined using hardware description language defined Figure D.6.8 . 1456FIGURE D.6.1 Data transfer instructions found MIPS core found two five desktop architectures. load linked/store conditional pair instructions gives Alpha MIPS atomic operations semaphores, allowing data read memory, modified, stored without fear interrupts ot machines accessing data multiprocessor (see Chapter 2 ). Prefetching Alpha external caches accomplished FETCH FETCH_M ; on-chip cache prefetches use LD_Q A, R31 , LD_Y A. F31 used Alpha 21164 (see Bhandarkar [1995] , p. 190). 1457FIGURE D.6.2 Arithmetic/logical instructions found MIPS core found two five desktop architectures. FIGURE D.6.3 Control instructions found MIPS core found two five desktop architectures. 1458FIGURE D.6.4 Floating-point instructions found MIPS core found two five desktop architectures. FIGURE D.6.5 Data transfer instructions found MIPS core found two five embedded architectures. use —1 show sequences available 32-bit mode 16-bit mode Thumb MIPS-16 . FIGURE D.6.6 Arithmetic/logical instructions found MIPS core found two five embedded architectures. use —1 show sequences available 145932-bit mode 16-bit mode Thumb MIPS - 16. superscript 2 shows new instructions found 16-bit mode Thumb MIPS-16, NEG2. FIGURE D.6.7 Control information five embedded architectures. FIGURE D.6.8 Hardware description notation (and standard C operators). Although categories self-explanatory, bear comment: “atomic swap” row means primitive exchange register memory without interruption. useful operating system semaphores uniprocessor well 1460multiprocessor synchronization (see Section 2.11 Chapter 2 ). 64-bit data transfer operation rows show MIPS, PowerPC, SPARC define 64-bit addressing integer operations. SPARC simply defines register addressing operations 64 bits, adding special instructions 64-bi shifts, data transfers, branches. MIPS includes extensions, plus adds separate 64-bit signed arithmetic instructions. PowerPC adds 64-bit right shift, load, store, divide, compare separate mode determining whether instructions interpreted 32- 64-bit operations; 64-bit operations work machine supports 32-bit mode. PA-RISC expanded 64-bit addressing operations version 2.0. “prefetch” instruction supplies address hint implementation data. Hints include whether data likely read written soon, likely read written once, likely read written many times. Prefetch cause exceptions. MIPS version adds two registers get address floating-point programs, unlik e nonfloating-point MIPS programs. “Endian” row, “Big/little” means bit program status register allows processor act either big endian little endian (see Appendix ). accomplished simply complementing least significant bits address data transfer instructions. “shared memory synchronization” helps cache-coherent multi-processors: loads stores executed instruction must complete loads stores start. (See Chapter 2 .) “coprocessor operations” row lists several categories allow processor extended special-purpose hardware. One difference needs longer explanation optimized branches. Figure D.6.9 shows options. Alpha PowerPC offer branches take effect immediately, like branches earlie r architectures. accelerate branches, machines use branch prediction (see Chapter 4 ). rest desktop RISCs offer delayed branches. embedded RISCs generally support delayed branch, exception SuperH, 1461option. FIGURE D.6.9 instruction following branch executed three types branches. three desktop RISCs provide version delayed branch makes easier fill delay slot. SPARC “annulling” branch executes instruction delay slot f branch taken; otherwise instruction annulled. means instruction target branch safely copied delay slot, since executed branch taken. restrictions target another branch target known compile time. (SPARC also offers nondelayed jump unconditional branch annul bit set execute following instruction.) Later versions MIPS architecture added branch likely instruction also annuls following instruction br anch taken. PA-RISC allows almost instruction annul next instruction, including branches. “nullifying” branch option execute next instruction depending direction th e branch whether taken (i.e., forward branch taken backward branch taken). Presumably choice made optimize loops, allowing instructions following exit branch looping branch execute common case. covered similarities, focus unique features architecture. first cover desktop/server RISCs, ordering length descriptio n unique features shortest longest, embedded RISCs. D.7 Instructions Unique MIPS-64 MIPS gone five generations instruction sets, thi evolution generally added features found architectur es. 1462Here salient unique features MIPS, first several found original instruction set. Nonaligned Data Transfers MIPS special instructions handle misaligned words memory. rare event programs, included supporting 16-bit minicomputer applications memcpy strcpy faster. Although RISCs trap try load word store word misaligned address, architectures misaligned words accessed without traps using four load byte instructions assembling result using shifts logical ORs. MIPS load store word left right instructions ( LWL, LWR, SWL, SWR ) allow done two instructions: LWL loads left portion register LWR loads right portion register. SWL SWR corresponding stores. Figure D.7.1 shows work. also 64-bit versions instructions. 1463FIGURE D.7.1 MIPS instructions unaligned word reads. figure assumes operation big-endian mode. Case 1 first loads three bytes 101, 102, 103 left R2, leaving least significant byte undisturbed. following LWR simply loads byte 104 least significant byte R2, leaving bytes register unchanged using LWL. Case 2 first loads byte 203 significant byte R4, following LWR loads three bytes R4 memory bytes 204, 205, 206. LWL reads word first byte memory, shifts left discard unneeded byte(s), changes bytes Rd. byte(s) transferred first byte lowest-order byte word. following LWR addresses last byte, right-shifts discard unneeded byte(s), finally changes bytes Rd. byte(s) transferred last byte highest-order byte word. Sto word left (SWL) simply inverse LWL, store word right (SWR) inverse LWR. Changing little-endian mode flips bytes selected discarded. (If big-little, left-right, load-store seem 1464confusing, don’t worry; work!) Remaining Instructions list remaining unique details MIPS-64 architecture: —This logical instruction calculates ~(Rs1 | Rs2). Constant shift amount —Nonvariable shifts use 5-bit constant field shown register-register format Figure D.2.3 . SYSCALL —This special trap instruction used invoke operating system. Move to/from control registers —CTCi CFCi move integer registers control registers. Jump/call PC-relative —The 26-bit address jumps calls added PC. shifted left two bits replaces lower 28 bits PC. would make difference program located near 256 MB boundary. TLB instructions —Translation-lookaside buffer (TLB) misses handled software MIPS I, instruction set also instructions manipulating registers TLB (see Chapter 5 TLBs). registers considered part “system coprocessor.” Since MIPS instructions dif fer among versions architecture; part implementations part instruction set architecture. Reciprocal reciprocal square root —These instructions, follow IEEE 754 guidelines proper rounding, included apparently applications value speed divide square root value accuracy. Conditional procedure call instructions —BGEZAL saves return address branches content Rs1 greater equal zero, BLTZAL less zero. purpose instructions get PC-relative call. (There “like ly” versions instructions well.) Parallel single-precision floating-point operations —As well extending architecture parallel integer operations MDMX, MIPS-64 also supports two parallel 32-bit floating-point operations 64-bit registers single instruction. “Paired single” operations include add ( ADD.PS ), subtract ( SUB.PS ), 1465compare ( C.__.PS ), convert ( CVT.PS.S, CVT.S.PL, CVT.S.PU ), negate ( NEG.PS ), absolute value ( ABS.PS ), move ( MOV.PS, MOVF.PS, MOVT.PS ), multiply ( MUL.PS ), multiply-add ( MADD.PS ), multiply-subtract ( MSUB.PS). specific provision MIPS architecture floating-point execution proceed parallel integer execution, MIPS implementations floating point allow happen checking see arithmetic interrupts possible early cycle. Normally, exception detection woul force serialization execution integer floating-point operations. D.8 Instructions Unique Alpha Alpha intended architecture made easy build high-performance implementations. Toward goal, architects originally made two controversial decisions: impreci se floating-point exceptions byte halfword data transfers. simplify pipelined execution, Alpha require exception act instructions past certain point executed point executed. supplies TRAPB instruction, stalls prior arithmetic instructions guaranteed complete without incurring arithmetic exceptions. conservative mode, placing one TRAPB per exception-causing instruction slows execution roughly five times provides precise exceptions (see Darcy Gay [1996] ). Code include TRAPB obey IEEE 754 floating-point standard. reason parts standard (NaNs, infinities, denormals) implemented software Alpha, many microprocessors. implement operations software, however, programs must find offending instruction operand values, cannot done imprecise interrupts! architecture developed, believed architects byte loads stores would slow data transfers. Byte loads require extra shifter data transfer path, byte stores require memory system perform 1466read-modify-write memory systems error correction codes, since new ECC value must recalculated. omission meant byte stores required sequence load word, replaced desired byte, stored word. (Inconsistently, floatin g- point loads go considerable byte swapping convert obtuse VAX floating-point formats canonical form.) reduce number instructions get desired data, Alpha includes elaborate set byte manipulation instructions: extract field zero rest register ( EXTxx ), insert field ( INSxx ), mask rest register ( MSKxx ), zero fields register ( ZAP), compare multiple bytes ( CMPGE ). Apparently, implementors bothered load store byte original architects. Beginning sh rink second version Alpha chip (21164A), architecture include loads stores bytes halfwords. Remaining Instructions list remaining unique instructions Alpha architecture: PAL code —To provide operations VAX performed microcode, Alpha provides mode runs privileges enabled, interrupts disabled, virtual memory mapping turned instructions. PAL (privileged architecture lib rary) code used TLB management, atomic memory operations, operating system primitives. PAL code called via CALL_PAL instruction. divide —Integer divide supported hardware. “Unaligned ” load-store —LDQ_U STQ_U load store 64-bit data using addresses ignore least significant three bits. Ext ract instructions select desired unaligned word using lower address bits. instructions similar LWL/R , SWL/R MIPS. Floating-point single precision represented double precision —Single- precision data kept conventional 32-bit formats memory converted 64-bit double-precision format register s. Floating-point register F31 fixed zero —To simplify comparisons zero. VAX floating-point formats —To maintain compatibility 1467VAX architecture, addition IEEE 754 single- double- precision formats called T, Alpha supports VAX single- double-precision formats called F G, VAX format D. (D narrow exponent field useful double precision replaced G VAX code.) Bit count instructions —Version 3 architecture added instructions count number leading zeros ( CTLZ ), count number trailing zeros ( CTTZ ), count number ones word ( CTPOP ). Originally found Cray computers, instructions help decryption. D.9 Instructions Unique SPARC v9 Several features unique SPARC. Register Windows primary unique feature SPARC register windows, optimization reducing register traffic procedure calls. everal banks registers used, new one allocated procedure call. Although could limit depth procedur e calls, limitation avoided operating banks circular buffer, providing unlimited depth. knee cost/performance curve seems six eight banks. SPARC two 32 windows, typically using eight registers globals, locals, incoming parameters, outgoing parameters. (Given window 16 unique registers, implementation SPARC 40 physical registers many 520, although 128 136, far.) Rather tie window changes call return instructions, SPARC separate instructions SAVE RESTORE. SAVE used “save” caller’s window pointing next window registers addition performing add instructio n. trick source registers caller’s wind ow addition operation, destination register callee’s window. SPARC compilers typically use instructio n changing stack pointer allocate local variables new stack frame. RESTORE inverse SAVE, bringing back caller’s window acting add instruction, source 1468registers callee’s window destination register caller’s window. automatically deallocates stack frame. Compilers also make use generating callee’s final return value. danger register windows larger number registers could slow clock rate. case early implementations. SPARC architecture (with register windows) MIPS R2000 architecture (without) built several technologies since 1987. several generations, SPARC clock rate slower MIPS clock rate implementations similar technologies, probably cache access times dominate register access times implementations. current-generation machines took differen implementation strategies—in order versus order—and it’s unlikely number registers determin ed clock rate either machine. Recently, architectures included register windows: Tensilica IA-64. Another data transfer feature alternate space option loads stores. simply allows memory system identify memory accesses input/ output devices, control regist ers devices cache memory management unit. Fast Traps Version 9 SPARC includes support make traps fast. expands single level traps least four levels, allowing windo w overflow underflow trap handlers interrupted. extra levels mean handler need check page faults misaligned stack pointers explicitly code, thereby makin g handler faster. Two new instructions added return multilevel handler: RETRY (which retries interrupted instruction) DONE (which not). support user-level traps, instruction RETURN return trap nonprivileged mode. Support LISP Smalltalk primary remaining arithmetic feature tagged addition subtraction. designers SPARC spent time thinking 1469about languages like LISP Smalltalk, influenced features SPARC already discussed: register windows, conditional trap instructions, calls 32-bit instruction addr esses, multiword arithmetic (see Taylor et al. [1986] Ungar et al. [1984] ). small amount support offered tagged data types operations addition, subtraction, and, hence, comparison. two least significant bits indicate whether operand integer (coded 00), TADDcc TSUBcc set overflow bit either operand tagged integer result large. subsequent conditional branch trap instruction decide do. (If operands integers, software recovers operands, checks types operands, invokes correct operation based types.) turns ou misaligned memory access trap also put use tagged data, since loading pointer wrong tag invalid access. Figure D.9.1 shows types tag support. FIGURE D.9.1 SPARC uses two least significant bits encode different data types tagged arithmetic instructions. a. Integer arithmetic takes single cycle long operands result integers. b. 1470misaligned trap used catch invalid memory accesses, trying use integer pointer. languages paired data like LISP, offset −3 used access even word pair (CAR) +1 used odd word pair (CDR). Overlapped Integer Floating-Point Operations SPARC allows floating-point instructions overlap execution integer instructions. recover interrupt suc h situation, SPARC queue pending floating-point instructions addresses. RDPR allows processor empty queue. second floating-point feature inclusion floating-p oint square root instructions FSQRTS , FSQRTD , FSQRTQ . Remaining Instructions remaining unique features SPARC follows: JMPL uses Rd specify return address register, specifying r31 makes similar JALR MIPS specifying r0 makes like JR. LDSTUB loads value byte Rd stores FF16 addressed byte. version 8 instruction used implement synchronization (see Chapter 2 ). CASA ( CASXA ) atomically compares value processor register 32-bit (64-bit) value memory; equal, swaps value memory value second processor register. version 9 instruction u sed construct wait-free synchronization algorithms require use locks. XNOR calculates exclusive complement second operand. BPcc, BPr , FBPcc include branch prediction bit compiler give hints machine whether branch likely taken not. ILLTRAP causes illegal instruction trap. Muchnick [1988] 1471explains used proper execution aggregate returning procedures C. POPC counts number bits set one operand, also found third version Alpha architecture. Nonfaulting loads allow compilers move load instructions ahead conditional control structures control use. Hence , nonfaulting loads executed speculatively. Quadruple-precision floating-point arithmetic data transfer allow floating-point registers act eight 128-bit registers floating-point operations data transfers. Multiple-precision floating-point results multiply mean two single- precision operands result double-precision product two double-precision operands result quadruple-precision product. instructions useful complex arithmetic models floating-point calculations. D.10 Instructions Unique PowerPC PowerPC result several generations IBM commercial RISC machines— IBM RT/PC, IBM Power1, IBM Power2—plus Motorola 8800. Branch Registers: Link Counter Rather dedicate one 32 general-purpose registers save return address procedure call, PowerPC puts address special register called link register . Since many procedures return without calling another procedure, link doesn’t always saved. Making return address special register makes return jump faster, since hardware need go register read pipeline stage return jumps. similar vein, PowerPC count register used loops program iterates fixed number times. using special register, branch hardware determine quickly whether branch based count register likely branch, since value register known early execution cy cle. Tests value count register branch instruction w ill automatically decrement count register. 1472Given count register link register already located hardware controls branches, one problems branch prediction getting target address earl pipeline, PowerPC architects decided make second use registers. Either register hold target address conditional branch. Thus, PowerPC supplements basic conditional branch two instructions get target addres registers ( BCLR , BCCTR ). Remaining Instructions Unlike RISC machines, register 0 hardwired value 0. cannot used base register—that is, generates 0 case—but base + index addressing used index. unique features PowerPC follows: Load multiple store multiple save restore 32 registers single instruction. LSW STSW permit fetching storing fixed- variable- length strings arbitrary alignment. Rotate mask instructions support bit field extraction insertion. One version rotates data performs logical mask ones, thereby extracting field. version rotates data places bits destination register corresponding 1 bit mask, thereby inserting field. Algebraic right shift sets carry bit ( CA) operand negative 1 bits shifted out. Thus, signed divide constant power two rounds toward 0 accomplished SRAWI followed ADDZE , adds CA register. CBTLZ count leading zeros. SUBFIC computes (immediate - RA), used develop one’s two’s complement. Logical shifted immediate instructions shift 16-bit immediate left 16 bits performing AND, OR, XOR. D.11 Instructions Unique PA-RISC 2.0 1473PA-RISC expanded slightly 1990 version 1.1 changed significantly 2.0 64-bit extensions 1996. PA-RISC perhaps unusual features desktop RISC machine. example, addressing modes instruction formats, and, shall see, several instructions really combination two simpler instructions. Nullification shown Figure D.6.9 , several RISC machines choose execute instruction following delayed branch improve utilization branch slot. called nullification PA-RISC, generalized apply arithmetic/logical instruction well branches. Thus, add instruction add two operands, store sum, cause following instruction skipped sum zero. Like conditional move instructions, nullification allows PA-RISC avoid branches cases one instruction part statement. Cornucopia Conditional Branches Given nullification, PA-RISC need separate conditional branch instructions. inventors could recommended nullifying instructions precede uncondit ional branches, thereby simplifying instruction set. Instead, PA-R ISC largest number conditional branches RISC machine. Figure D.11.1 shows conditional branches PA-RISC. see, several really combinations two instructions. FIGURE D.11.1 PA-RISC conditional branch 1474instructions. 12-bit offset called offset12 table, 5-bit immediate called imm5. 16 conditions =, <, < =, odd, signed overflow, unsigned overflow, zero overflow unsigned, never, respective complements. BB instruction selects one 32 bits register branches depending whether value 0 1. BVB selects bit branch using shift amount register, special-purpose register. subscript notation specifies bit field. Synthesized Multiply Divide PA-RISC provides several primitives multiply divide synthesized software. Instructions shift one operan 1, 2, 3 bits add, trapping overflow, useful multiplies. (Alpha also includes instructions multiply second operand adds subtracts 4 8: S4ADD , S8ADD , S4SUB , S8SUB .) divide step performs critical step nonrestoring divide, adding subtracting depending si gn prior result. Magen-heimer et al. [1988] measured size operands multiplies divides show well multiply step would work. Using data C programs, Muchnick [1988] found making special cases, average multiply constant takes six clock cycles multiply variables takes 24 clock cycles. PA- RISC ten instructions operations. original SPARC architecture used similar optimizations, increasing numbers transistors instruction set expanded include full multiply divide operations. PA-RIS C gives support along lines putting full 32-bit intege r multiply floating-point unit; however, integer data mu st first moved floating-point registers. Decimal Operations COBOL programs compute decimal values, stored 4 bits per digit, rather converting back forth binary decimal. PA-RISC instructions convert sum normal 32-bit add proper decimal digits. also provides 1475logical arithmetic operations set condition codes est carries digits, bytes, halfwords. operations also te st whether bytes halfwords zero. operations would useful arithmetic 8-bit ASCII characters. Five PA-RISC instructions provide decimal support. Remaining Instructions remaining PA-RISC instructions: Branch vectored shifts index register left 3 bits, adds base register, branches calculated address. used case statements. Extract deposit instructions allow arbitrary bit fields selected inserted registers. Variations include whether extracted field sign-extended, whether bit fi eld specified directly instruction indirectly anot register, whether rest register set zero lef unchanged. PA-RISC 12 instructions. simplify use 32-bit address constants, PA-RISC includes ADDIL , adds left-adjusted 21-bit constant register places result register 1. following data transfer instruction uses offset addressing add lower 11 bits address register 1. pair instructions allows PA-RISC add 32-bit constant base register, cost changing register 1. PA-RISC nine debug instructions set breakpoints instruction data addresses return trapped addresses. Load clear instructions provide semaphore lock reads value memory writes zero. Store bytes short optimizes unaligned data moves, moving either leftmost rightmost bytes word effective address, depending instruction options condition co de bits. Loads stores work well caches options give hints whether load data cache it’s already cache. example, load destination register 0 defined software-controlled cache prefetch. PA-RISC 2.0 extended cache hints stores indicate block copies, recommending processor load data 1476cache it’s already cache. also suggest loads stores, spatial locality prepare cache subsequent sequential accesses. PA-RISC 2.0 also provides optional branch target stack predict indirect jumps used subroutine returns. Software suggest addresses get placed removed branch target stack, hardware controls whether valid. Multiply/add multiply/subtract floating-point operations launch two independent floating-point operations single instruction addition fused multiply/add fus ed multiply/negate/add introduced version 2.0 PA-RISC. D.12 Instructions Unique ARM It’s hard pick unusual feature ARM, perhaps conditional execution instructions. Every instruction starts 4-bit field determines whether act nop real instruction, depending condition codes. Hence, conditional branches properly considered conditionally executing unconditional branch instruction. Conditional execution allows avoiding branch jump single instruction. takes less code space time simply condition ally execute one instruction. 12-bit immediate field novel interpretation. 8 least significant bits zero-extended 32-bit value, rotated right number bits specified first 4 bits fie ld multiplied two. Whether split actually catches immediates simple 12-bit field would interesting study. One advantage scheme represent powers two 32-bit word. Operand shifting limited immediates. second register arithmetic logical processing operations option shifted operated on. shift opti ons shift left logical, shift right logical, shift right arithmetic , rotate right. again, would interesting see often operations like rotate-and-add, shift-right-and-test, oc cur ARM programs. 1477Remaining Instructions list remaining unique instructions ARM architecture: Block loads stores —Under control 16-bit mask within instructions, 16 registers loaded stored memory single instruction. instructions save restore registers procedure entry return. instructions also used block memory copy—offering four times bandwidth single register load-store— today, block copies important use. Reverse subtract —RSB allows first register subtracted immediate shifted register. RSC thing, includes carry calculating difference. Long multiplies —Similarly MIPS, Hi Lo registers get 64- bit signed product (SMULL) 64-bit unsigned product (UMULL). divide —Like Alpha, integer divide supported hardware. Conditional trap —A common extension MIPS core found desktop RISCs ( Figures D.6.1 D.6.4 ), comes free conditional execution ARM instructions, including SWI . Coprocessor interface —Like many desktop RISCs, ARM defines full set coprocessor instructions: data transfer, move general- purpose coprocessor registers, coprocessor operations. Floating-point architecture —Using coprocessor interface, floating-point architecture defined ARM. implemented FPA10 coprocessor. Branch exchange instruction sets —The BX instruction transition ARM Thumb, using lower 31 bits register set PC significant bit determi ne mode ARM (1) Thumb (0). D.13 Instructions Unique Thumb ARM version 4 model, frequently executed procedures wil l use ARM instructions get maximum performance, less frequently executed ones using Thumb reduce overall co de 1478size program. Since typically procedures dominate execution time, hope hybrid gets best bot h worlds. Although Thumb instructions translated hardware conventional ARM instructions execution, several restrictions. First, conditional execution dropped alm ost instructions. Second, first eight registers easily available instructions, stack pointer, link register , program counter used implicitly instructions. Third, Thumb uses two-operand format save space. Fourth, unique shifted immediates shifted second operands disappeared replaced separate shift instructions. Fifth, addressing modes simplified. Finally, putting instructions 16 bits forces many instruction formats. many ways, simplified Thumb architecture conventional ARM. additional changes made ARM going Thumb: Drop immediate logical instructions —Logical immediates gone. Condition codes implicit —Rather condition codes set optionally, defined opcode. ALU instructions none data transfers set condition codes. Hi/Lo register access —The 16 ARM registers halved Lo registers Hi registers, eight Hi registers including stack pointer (SP), link register, PC. Lo registers available ALU operations. Variations ADD, BX, CMP, MOV also work combinations Lo Hi registers. SP PC registers also available variations data transfers add immediates. operations Hi registers require one MOV put value Lo register, perform operation there, transfer data back Hi register. Branch/call distance —Since instructions 16 bits wide, 8-bit conditional branch address shifted one instead two. Branch link specified two instructions, concatenating 11 bits instruction shifting left form 23-bit address load PC. Distance data transfer offsets —The offset 5 bits general-purpose registers 8 bits SP PC. 1479D.14 Instructions Unique SuperH Register 0 plays special role SuperH address modes. added another register form address indirect indexed addressing PC-relative addressing. R0 used load constants give larger addressing range easily fit 16- bit instructions SuperH. R0 also register operand immediate versions AND, CMP, OR, XOR. list remaining unique details SuperH architecture: Decrement test —DT decrements register sets bit 1 result 0. Optional delayed branch —Although embedded RISC machines generally use delayed branches (see Appendix A), SuperH offers optional delayed branch execution BT BF. Many multiplies —Depending whether operation signed unsigned, operands 16 bits 32 bits, product 32 bits 64 bits, proper multiply instruction MULS, MULU, DMULS, DMULU , MUL. product found MACL MACH registers. Zero sign extension —Byte halfwords either zero- extended ( EXTU ) sign-extended ( EXTS ) within 32-bit register. One-bit shift amounts —Perhaps attempt make fit within 16-bit instructions, shift instructions shift ingle bit time. Dynamic shift amount —These variable shifts test sign amount register determine whether shift left (positive) shift right (negative). logical ( SHLD ) arithmetic ( SHAD ) instructions supported. instructions help offset 1-bit constant shift amounts standard shifts. Rotate —SuperH offers rotations 1 bit left ( ROTL ) right ( ROTR ), set bit value rotated, also variations include bit rotations ( ROTCL ROTCR ). SWAP —This instruction swaps either high low bytes 32-bit word two bytes rightmost 16 bits. Extract word (XTRCT )—The middle 32 bits pair 32-bit registers placed another register. Negate carry —Like SUBC (Figure D.6.6 ), except first 1480operand 0. Cache prefetch —Like many desktop RISCs ( Figures D.6.1 D.6.4 ), SuperH instruction ( PREF ) prefetch data cache. Test-and-set —SuperH uses older test-and-set ( TAS) instruction perform atomic locks semaphores (see Chapter 2 ). TAS first loads byte memory. sets bit 1 byte 0 0 byte 0. Finally, sets significant bit byte 1 writes result back memory. D.15 Instructions Unique M32R unusual feature M32R slight VLIW approach pairs 16-bit instructions. bit reserved first instruction pair say whether instruction executed parallel next instruction— is, two instructions independent—or two must executed sequentially. (An earlier machine offered similar option Intel i860.) feature included future implementatio ns architecture. One surprise branch displacements shifted left 2 bits added PC, lower 2 bits PC set 0. Since instructions 16 bits long, shift means branch cannot go instruction program: branch instructions word boundaries. similar restriction placed return address branch-and-l ink jump-and-link instructions: return word boundary. Thus, slightly larger branch distance, software must ensure branch addresses return addresses aligned word boundary. M32R code space probably slightly larger, probably executes nop instructions would branch address shifted left 1 bit. However, VLIW feature means nop execute parallel another 16-bit instruction padding doesn’t take clock cycles. code size expansion depends ability compiler schedule code pair successiv e 16-bit instructions; Mitsubishi claims code size overall onl 7% larger Motorola 6800 architecture. 1481The last remaining novel feature result divide operation remainder instead quotient. D.16 Instructions Unique MIPS-16 MIPS-16 really separate instruction set 16-bit extension full 32-bit MIPS architecture. compatible wi th 32-bit address MIPS architectures (MIPS I, MIPS II) 64- bit architectures (MIPS III, IV, V). ISA mode bit determines th e width instructions: 0 means 32-bit-wide instructions 1 means 16-bit-wide instructions. new JALX instruction toggles ISA mode bit switch ISA. JR JALR redefined set ISA mode bit significant bit register containing branch address, bit considered part address. jump-and-link instructions save current mode bit significant bit return address. Hence, MIPS supports whole procedures containing either 16-bit 32-bit instructions, support mixing two lengths together single procedure. one exception th e JAL JALX : two instructions need 32 bits even 16-bit mode, presumably get large enough address branch far procedures. picking subset, MIPS decided include opcodes som e three-operand instructions keep 16 opcodes 64-bit operations. combination many opcodes operands 16 bits led architects provide eight easy-to-use regi sters —just like Thumb—whereas embedded RISCs offer 16 registers. Since hardware must include full 32 registers 32-bit ISA mode, MIPS-16 includes move instructions copy values eight MIPS-16 registers remaining 24 registers full MIPS architecture. reduce pressure eight visible registers, stack pointer considered separ ate register. MIPS-16 includes variety separate opcodes data transfers using SP base register increment SP: LWSP, LDSP, SWSP, SDSP, ADJSP, DADJSP, ADDIUSPD , DADDIUSP . fit within 16-bit limit, immediate fields generally shortened 5 8 bits. MIPS-16 provides way extend 1482shorter immediates full width immediates 32-bit mode. Borrowing trick Intel 8086, EXTEND instruction really 16-bit prefix prepended MIPS-16 instruction address immediate field. prefix suppl ies enough bits turn 5-bit field data transfers 5- 8-bit fields arithmetic immediates 16-bit constants. Alas, ar e two exceptions. ADDIU DADDIU start 4-bit immediate fields, since EXTEND supply 11 bits, wider immediate limited 15 bits. EXTEND also extends 3-bit shift fields 5- bit fields shifts. (In case wondering, EXTEND prefix need start 32-bit boundary.) address supply constants, MIPS-16 added new addressing mode! PC-relative addressing load word ( LWPC ) load double ( LDPC ) shifts 8-bit immediate field 2 3 bits, respectively, adding PC lower 2 3 bits cleared. constant word doubleword loaded register. Thus 32-bit 64-bit constants included MIPS-16 code, despite loss LIU set upper register bits. Given new addressing mode, also instruction ( ADDIUPC ) calculate PC-relative address place register. MIPS-16 differs embedded RISCs subset 64-bit address architecture. result 16-bit instruction-length versions 64-bit data operations: data transfer (LD, SD, LWU ), arithmetic operations ( DADDU/IU, DSUBU, DMULT/U, DDIV/U ), shifts ( DSLL/V, DSRA/V, DSRL/V ). Since MIPS plays prominent role book, show additional changes made MIPS core instructions going MIPS-16: Drop signed arithmetic instructions —Arithmetic instructions trap dropped save opcode space: ADD, ADDI, SUB, DADD, DADDI, DSUB . Drop immediate logical instructions —Logical immediates gone too: ANDI, ORI, XORI . Branch instructions pared —Comparing two registers branching fit, comparisons register zero. Hence instructions didn’t make either: BEQ, BNE, BGEZ, BGTZ, BLEZ , BLTZ . mentioned Section D.3, help compensate MIPS-16 includes compare instructions 1483to test two registers equal. Since compare set less set new register, branches added test register. Branch distance —Since instructions 16 bits wide, branch address shifted one instead two. Delayed branches disappear —The branches take effect next instruction. Jumps still one-slot delay. Extension distance data transfer offsets —The 5-bit 8-bit fields zero-extended instead sign-extended 32-bit mode . get greater range, immediate fields shifted left 1, 2, 3 bits depending whether data halfword, word, doubleword. EXTEND prefix prepended instructions, use conventional signed 16-bit immediate 32-bit mode. Extension arithmetic immediates —The 5-bit 8-bit fields zero-extended set less compare instructions, forming PC-relative address, adding SP placing result register ( ADDIUSP, DADDIUSP ). again, EXTEND prefix prepended instructions, use conventional signed 16-bit immediate 32-bit mode. still sign-extended general adds adding SP placing result back SP ( ADJSP, DADJSP ). Alas, code density orthogonality strange bedfellows MIPS-16! Redefining shift amount 0 —MIPS-16 defines value 0 3- bit shift field mean shift 8 bits. New instructions added due loss register 0 zero —Load immediate, negate, added, since operations could longer synthesized instructions using r 0 source. D.17 Concluding Remarks appendix covers addressing modes, instruction formats , instructions found 10 RISC architectures. Although later sections appendix concentrate differences, would possible cover 10 architectures pages many similarities. fact, would guess 90% instructions executed architectures would found Figures D.3.5 D.3.11 . 1484contrast homogeneity, Figure D.17.1 gives summary four architectures 1970s format similar shown Figure D.1.1 . (Imagine trying write single chapter style architectures!) history computing, never widespread agreement computer architecture. FIGURE D.17.1 Summary four 1970s architectures. Unlike architectures Figure D.1.1 , little agreement architectures category. style architecture cannot remain static, however. Like people, instruction sets tend get bigger get older. Figure D.17.2 shows genealogy instruction sets, Figure D.17.3 shows features added deleted generations desktop RISCs time. 1485FIGURE D.17.2 lineage RISC instruction sets. Commercial machines shown plain text research machines bold. CDC 6600 Cray-1 load-store machines register 0 fixed 0, separate integer floating-point register s. Instructions could cross word boundaries. early IBM research machine led 801 America research projects, 801 leading unsuccessful RT/PC America leading successful Power architecture. people worked 801 later joined Hewlett-Packard work PA-RISC. two university projects basis MIPS SPARC machines. According Furber [1996] , Berkeley RISC project inspiration ARM architecture. ARM1, ARM2, ARM3 names architectures chips, ARM version 4 name 1486architecture used ARM7, ARM8, StrongARM chips. (There ARMv4 ARM5 chips, ARM6 early ARM7 chips use ARM3 architecture.) DEC built RISC microprocessor 1988 introduce it. Instead, DEC shipped workstations using MIPS microprocessors 3 years brought RISC instruction set, Alpha 21064, similar MIPS III PRISM. Alpha architecture small extensions, formalized version numbers; used version 3 version reference manual. Alpha 21164A chip added byte halfword loads stores, Alpha 21264 includes MAX multimedia bit count instructions. Internally, Digit al names chips fabrication technology: EV4 (21064), EV45 (21064A), EV5 (21164), EV56 (21164A), EV6 (21264). “EV” stands “extended VAX.” FIGURE D.17.3 Features added desktop RISC machines. X means original machine, +means added later, ” means continued prior machine, — means removed architecture. Alpha included, added byte word loads stores, bit count 1487and multimedia extensions, version 3. MIPS V added MDMX instructions paired single floating-point operations. see, desktop RISC machines evolved 64-bit address architectures, done fairly painlessly. would like thank following people comments drafts appendix: Professor Steven B. Furber, University f Manchester; Dr. Dileep Bhandarkar, Intel Corporation; Dr. Earl Killian, Silicon Graphics/MIPS; Dr. Hiokazu Takata, Mitsubishi Electric Corporation. Reading 1. Bhandarkar DP. Alpha Architecture Implementations Newton, MA: Digital Press; 1995. 2. Darcy, J.D., D. Gay [1996]. “FLECKmarks: Measuring floating point performance using full IEEE compliant arithmetic benchmark,” CS 252 class project, U.C. Berkeley (see www.sonic.net/~jddarcy/Research/fleckmrk.pdf ). 3. Digital Semiconductor [1996]. Alpha Architecture Handbook , Version 3, Maynard, MA: Digital Press, Order number EC- QD2KB-TE (October). 4. Furber SB. ARM System Architecture Harlow, England: Addison-Wesley; 1996; (See http://www.pearsonhighered.com/pearsonhigheredus/educator/product/produ cts_detail.page? isbn=9780201675191&forced_logout=forced_logged_out#sthash.QX4WfErc 5. Hewlett-Packard [1994]. PA-RISC 2.0 Architecture Reference Manual , 3rd ed. 6. Hitachi [1997]. SuperH RISC Engine SH7700 Series Programming Manual . (See http://am.renesas.com/products/mpumcu/superh/sh7700/Documentation.j sp 7. IBM. PowerPC Architecture San Francisco: Morgan Kaufmann; 1994. 8. Kane G. PA-RISC 2.0 Architecture Upper Saddle River, NJ: Prentice Hall PTR; 1996. 9. Kane G, Heinrich J. MIPS RISC Architecture Englewood Cliffs, NJ: Prentice Hall; 1992. 148810. Kissell, K.D. [1997]. MIPS16: High-Density Embedded Market . 11. Magenheimer DJ, Peters L, Pettis KW, Zuras D. Integer multiplication division HP precision architecture. IEEE Trans Computers . 1988;37(8):980–990. 12. MIPS [1997]. MIPS16 Application Specific Extension Product Description . 13. Mitsubishi [1996]. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family Soft ware Manual (September). 14. Muchnick SS. Optimizing compilers SPARC. Sun Technology . 1988;1(3):64–77 (Summer). 15. Seal D. Arm Architecture Reference Manual 2nd ed Morgan Kaufmann 2000. 16. Silicon Graphics [1996]. MIPS V Instruction Set . 17. Sites RL, Witek R, eds. Alpha Architecture Reference Manual . 2nd ed Newton, MA: Digital Press; 1995. 18. Sloss AN, Symes D, Wright C. ARM System Developer’s Guide San Francisco: Elsevier Morgan Kaufmann; 2004. 19. Sun Microsystems [1989]. SPARC Architectural Manual , Version 8, Part No. 800-1399-09, August 25. 20. Sweetman D. See MIPS Run 2nd ed Morgan Kaufmann 2006. 21. Taylor, G., P. Hilfinger, J. Larus, D. Patterson, B. Zorn [1986]. “Evaluation SPUR LISP architecture,” Proc. 13th Symposium Computer Architecture (June), Tokyo. 22. Ungar, D., R. Blau, P. Foley, D. Samples, D. Patterson [1984]. “Architecture SOAR: Smalltalk RISC,” Proc. 11th Symposium Computer Architecture (June), Ann Arbor, MI, 188–97. 23. Weaver DL, Germond T. SPARC Architectural Manual, Version 9 Englewood Cliffs, NJ: Prentice Hall; 1994. 24. Weiss S, Smith JE. Power PowerPC San Francisco: Morgan Kaufmann; 1994. 1489Answers Check Chapter 1 §1.1, page 10: Discussion questions: many answers acceptable. §1.4, page 24: DRAM memory: volatile, short access time 50 70 nanoseconds, cost per GB $5 $10. Disk memory: nonvolatile, access times 100,000 400,000 times slower DRAM, cost per GB 100 times cheaper DRAM. Flash memory: nonvolatile, access times 100 1000 times slower DRAM, cost per GB 7 10 times cheaper DRAM. §1.5, page 28: 1, 3, 4 valid reasons. Answer 5 generally true high volume make extra investment reduce die size by, say, 10% good economic decision, doesn’t true. §1.6, page 33: 1. a: both, b: latency, c: neither. 7 seconds. §1.6, page 40: b. §1.10, page 51: a. Computer higher MIPS rating. b. Computer B faster. Chapter 2 §2.2, page 66: RISC-V, C, Java. §2.3, page 73: 2) slow. §2.4, page 80: 2) −8ten §2.5, page 89: 3) sub x11, x10, x9 1490§2.6, page 92: Both. mask pattern 1s leaves 0s everywhere desired field. Shifting left correc amount removes bits left field. Shifting right appropriate amount puts field right-most bit doubleword, 0s rest doubleword. Note leaves field originally, shift pair moves field rightmost part doubleword. §2.7, page 97: I. true. II. 1). §2.8, page 108: true. §2.9, page 113: I. 1) 2) II. 3). §2.10, page 121: I. 4) ±4 K. II. 4) ± 1 M. §2.11, page 124: true. §2.12, page 133: 4) Machine independence. Chapter 3 §3.2, page 177: 2. §3.5, page 215: 3. Chapter 4 §4.1, page 240: 3 5: Control, Datapath, Memory. Input Output missing. §4.2, page 243: false. Edge-triggered state elements make simultaneous reading writing possible unambiguous. §4.3, page 250: I. a. II. c. §4.4, page 262: Yes, Branch ALUOp0 identical. addition, use flexibility don’t care bits combine signals together. ALUSrc MemtoReg made setting two don’t care bits MemtoReg 1 0. ALUOp1 MemtoReg made inverses one another setting don’t care bit MemtoReg 1. don’t need inverter; simply use signal flip order inputs MemtoReg multiplexor! §4.5, page 275: 1. Stall due load-use data hazard ld result. 2. Avoid stalling third instruction read-after-writ e data hazard x11 forwarding add result. 3. need 1491stall, even without forwarding. §4.6, page 288: Statements 2 4 correct; rest incorrect. §4.8, page 314: 1. Predict taken. 2. Predict taken. 3. Dynamic prediction. §4.9, page 321: first instruction, since logically executed others. §4.10, page 334: 1. Both. 2. Both. 3. Software. 4. Hardware. 5. Hardware. 6. Hardware. 7. Both. 8. Hardware. 9. Both. §4.12, page 344: First two false last two true. Chapter 5 §5.1, page 369: 1 4. (3 false cost memory hierarchy varies per computer, 2016 highest cost usually DRAM.) §5.3, page 390: 1 4: lower miss penalty enable smaller blocks, since don’t much latency amortize, yet higher memory bandwidth usually leads larger blocks, since miss penalty slightly larger. §5.4, page 409: 1. §5.8, page 449: 2. (Both large block sizes prefetching may reduce compulsory misses, 1 false.) Chapter 6 §6.1, page 494: False. Task-level parallelism help sequential applications sequential applications made run parallel hardware, although challenging. §6.2, page 499: False. Weak scaling compensate serial portion program would otherwise limit scalability, strong scaling. §6.3, page 504: True, missing useful vector features like gather-scatter vector length registers improve efficiency vector architectures. (As elaboration section mentions, AVX2 SIMD extensions offers indexed loads via gather operation scatter indexed stores. Haswell generation x86 microprocessor first support AVX2.) 1492§6.4, page 509: 1. True. 2. True. §6.5, page 513: False. Since shared address physical address, multiple tasks virtual address spaces run well shared memory multiprocessor. §6.6, page 521: False. Graphics DRAM chips prized higher bandwidth. §6.7, page 526: 1. False. Sending receiving message implicit synchronization, well way share data. 2. True. §6.8, page 528: True. §6.10, page 540: True. likely need innovation levels hardware software stack parallel computing succeed. 1493Glossary absolute address variable’s routine’s actual address memory. abstraction model renders lower-level details computer systems temporarily invisible facilitate design sophisticated systems. access bit Also called use bit reference bit . field set whenever page accessed used implement LRU replacement schemes. acronym word constructed taking initial letters string words. example: RAM acronym Random Access Memory, CPU acronym Central Processing Unit. active matrix display liquid crystal display using transistor control transmission light individual pixel. address value used delineate location specific data element within memory array. address translation Also called address mapping . process virtual address mapped address used access memory. addressing mode One several addressing regimes delimited varied use operands and/or addresses. aliasing situation two addresses access object; occur virtual memory two virtual addresses physical page. alignment restriction requirement data aligned 1494memory natural boundaries. Amdahl’s Law rule stating performance enhancement possible given improvement limited amount improved feature used. quantitative version law diminishing returns. logical bit-by-bit operation two operands calculates 1 1 operands. antidependence Also called name dependence . ordering forced reuse name, typically register, rather true dependence carries value two instructions. antifuse structure integrated circuit programmed makes permanent connection two wires. application binary interface (ABI) user portion instruction set plus operating system interfaces used application programmers. defines standard binary portability across computers. architectural registers instruction set visible registers processor; example, RISC-V, 32 integer 32 floating-point registers. arithmetic intensity ratio floating-point operations program number data bytes accessed program main memory. arithmetic logic unit (ALU) Hardware performs addition, subtraction, usually logical operations OR. assembler program translates symbolic version instruction binary version. assembler directive operation tells assembler translate program produce machine instructions; always begins period. assembly language symbolic language translated 1495into binary machine language. asserted signal logically high true. asserted signal signal (logically) true, 1. backpatching method translating assembly language machine instructions assembler builds (possibly incomplete) binary representation every instruction e pass program returns fill previously undefined labels. basic block sequence instructions without branches (except possibly end) without branch targets branch labels (except possibly beginning). behavioral specification Describes digital system operates functionally. benchmark program selected use comparing computer performance. biased notation notation represents negative value 00…000two positive value 11…11two, 0 typically value 10…00two, thereby biasing number number plus bias nonnegative representation. binary digit Also called binary bit . One two numbers base 2, 0 1, components information. bisection bandwidth bandwidth two equal parts multiprocessor. measure worst-case split multiprocessor. block (or line) minimum unit information either present present cache. blocking assignment Verilog, assignment completes execution next statement. branch address table Also called branch table . table addresses alternative instruction sequences. branch-and-link instruction instruction branches 1496address simultaneously saves address following instruction register (usually x1 RISC-V). branch taken (untaken branch) branch branch condition false program counter (PC) becomes address instruction sequentially follows branch. branch prediction method resolving branch hazard assumes given outcome conditional branch proceeds assumption rather waiting ascertain actual outcome. branch prediction buffer Also called branch history table . small memory indexed lower portion address branch instruction contains one bits indicating whether branch recently taken not. branch taken branch branch condition satisfied program counter (PC) becomes branch target. unconditional branches taken branches. branch target address address specified branch, becomes new program counter (PC) branch taken. RISC-V architecture, branch target given sum immediate field instruction address branch. branch target buffer structure caches destination PC destination instruction branch. usually organized cache tags, making costly simple prediction buffer. bus logic design, collection data lines treated together single logical signal; also, shared collection lines multiple sources uses. cache memory small, fast memory acts buffer slower, larger memory. cache miss request data cache cannot filled data present cache. callee procedure executes series stored instructions based parameters provided caller returns 1497control caller. callee-saved register register saved routine making procedure call. caller program instigates procedure provides necessary parameter values. caller-saved register register saved routine called. capacity miss cache miss occurs cache, even full associativity, cannot contain blocks needed satisfy request. central processing unit (CPU) Also called central processor unit processor. active part computer, contains datapath control adds numbers, tests numbers, signals I/O devices activate, on. clock cycle Also called tick, clock tick , clock period , clock , cycle . time one clock period, usually processor clock. clock cycles per instruction (CPI) Average number clock cycles per instruction program program fragment. clock period length clock cycle. clock skew difference absolute time times two state elements see clock edge. clocking methodology approach used determine data valid stable relative clock. Cloud Computing refers large collections servers provide services Internet; providers rent dynamically varying numbers servers utility. cluster set computers connected local area network function single large multiprocessor. clusters Collections computers connected via I/O standard network switches form message-passing multiprocessor. coarse-grained multithreading version hardware multithreading implies switching threads 1498after significant events, last-level cache miss. combinational element operational element, gate ALU. combinational logic logic system whose blocks contain memory hence compute output given input. commit unit unit dynamic out-of-order execution pipeline decides safe release result operation programmer-visible registers memory. compiler program translates high-level language statements assembly language statements. compulsory miss Also called cold-start miss . cache miss caused first access block never cache. conditional branch instruction tests value, allows subsequent transfer control new address program based outcome test. conflict miss Also called collision miss . cache miss occurs set-associative direct-mapped cache multiple blocks compete set eliminated fully associative cache size. context switch changing internal state processor allow different process use processor includes saving state needed return currently executing process. control component processor commands datapath, memory, I/O devices according instructions program. control hazard Also called branch hazard . Arises proper instruction cannot execute proper pipeline clock cycle instruction fetched one needed; is, flow instruction addresses pipeline expected. control signal signal used multiplexor selection 1499directing operation functional unit; contrasts data signal , contains information operated functional unit. correlating predictor branch predictor combines local behavior particular branch global information behavior recent number executed branches. CPU execution time Also called CPU time . actual time CPU spends computing specific task. crossbar network network allows node communicate node one pass network. flip-flop flip-flop one data input stores value input signal internal memory clock edge occurs. data hazard Also called pipeline data hazard . planned instruction cannot execute proper clock cycle data needed execute instruction yet available. data race Two memory accesses forming data race different threads location, least one write, occur one another. data segment segment UNIX object executable file contains binary representation initialized data used program. data transfer instruction command moves data memory registers. data-level parallelism Parallelism achieved performing operation independent data. datapath component processor performs arithmetic operations. datapath element unit used operate hold data within processor. RISC-V implementation, datapath elements include instruction data memories, 1500register file, ALU, adders. deasserted signal logically low false. deasserted signal signal (logically) false, 0. decoder logic block n-bit input 2 n outputs, one output asserted input combination. defect microscopic flaw wafer patterning steps result failure die containing defect. delayed branch type branch instruction immediately following branch always executed, independent whether branch condition true false. die individual rectangular sections cut wafer, informally known chips . direct-mapped cache cache structure memory location mapped exactly one location cache. dividend number divided. divisor number dividend divided by. don’t-care term element logical function output depend values inputs. Don’t- care terms may specified different ways. double precision floating-point value represented 64-bit doubleword. doubleword Another natural unit access computer, usually group 64 bits; corresponds size register RISC-V architecture. dynamic branch prediction Prediction branches runtime using runtime information. dynamic multiple issue approach implementing multiple- issue processor many decisions made execution processor. dynamic pipeline scheduling Hardware support reordering order instruction execution avoid stalls. 1501dynamic random access memory (DRAM) Memory built integrated circuit; provides random access location. Access times 50 nanoseconds cost per gigabyte 2012 $5 $10. dynamically linked libraries (DLLs) Library routines linked program execution. edge-triggered clocking clocking scheme state changes occur clock edge. embedded computer computer inside another device used running one predetermined application collection software. EOR logical bit-by-bit operation two operands calculates exclusive two operands. is, calculates 1 values different two operands. error detection code code enables detection error data, precise location and, hence, correction error. exception Also called interrupt . unscheduled event disrupts program execution; used detect overflow. exception enable Also called interrupt enable. signal action controls whether process responds exception not; necessary preventing occurrence exceptions intervals processor safely saved state needed restart. executable file functional program format object file contains unresolved references. contain symbol tables debugging information. “stripped executable” contain information. Relocation information may included loader. exponent numerical representation system floating-point arithmetic, value placed exponent field. external label Also called global label . label referring object referenced files one 1502which defined. false sharing two unrelated shared variables located cache block full block exchanged processors even though processors accessing different variables. field programmable devices (FPD) integrated circuit containing combinational logic, possibly memory devices, configurable end user. field programmable gate array (FPGA) configurable integrated circuit containing combinational logic blocks flip- flops. fine-grained multithreading version hardware multithreading implies switching threads every instruction. finite-state machine sequential logic function consisting set inputs outputs, next-state function maps current state inputs new state, output function maps current state possibly inputs set asserted outputs. flash memory nonvolatile semiconductor memory. cheaper slower DRAM expensive per bit faster magnetic disks. Access times 5 50 microseconds cost per gigabyte 2012 $0.75 $1.00. flip-flop memory element output equal value stored state inside element internal state changed clock edge. floating point Computer arithmetic represents numbers binary point fixed. flush discard instructions pipeline, usually due unexpected event. formal parameter variable argument procedure macro; replaced argument macro expanded. forward reference label used defined. 1503forwarding Also called bypassing . method resolving data hazard retrieving missing data element internal buffers rather waiting arrive programmer- visible registers memory. fraction value, generally 0 1, placed fraction field. frame pointer value denoting location saved registers local variables given procedure. fully associative cache cache structure block placed location cache. fully connected network network connects processor- memory nodes supplying dedicated communication link every node. fused multiply add floating-point instruction performs multiply add, rounds add. gate device implements basic logic functions, OR. global miss rate fraction references miss levels multilevel cache. global pointer register reserved point static area. guard first two extra bits kept right intermediate calculations floating-point numbers; used improve rounding accuracy. handler Name software routine invoked “handle” exception interrupt. hardware description language programming language describing hardware, used generating simulations hardware design also input synthesis tools generate actual hardware. hardware multithreading Increasing utilization processor switching another thread one thread stalled. 1504hardware synthesis tools Computer-aided design software generate gate-level design based behavioral descriptions digital system. hexadecimal Numbers base 16. high-level programming language portable language C, C++, Java, Visual Basic composed words algebraic notation translated compiler assembly language. hit rate fraction memory accesses found level memory hierarchy. hit time time required access level memory hierarchy, including time needed determine whether access hit miss. hold time minimum time input must valid clock edge. implementation Hardware obeys architecture abstraction. imprecise interrupt Also called imprecise exception . Interrupts exceptions pipelined computers associated exact instruction cause interrupt exception. in-order commit commit results pipelined execution written programmer visible state order instructions fetched. input device mechanism computer fed information, microphone. instruction command computer hardware understands obeys. instruction count number instructions executed program. instruction format form representation instruction composed fields binary numbers. instruction latency inherent execution time instruction. 1505instruction-level parallelism parallelism among instructions. instruction mix measure dynamic frequency instructions across one many programs. instruction set architecture Also called architecture . abstract interface hardware lowest-level software encompasses information necessary write machine language program run correctly, including instructions, registers, memory access, I/O, on. integrated circuit Also called chip . device combining dozens millions transistors. interrupt exception comes outside processor. (Some architectures use term interrupt exceptions.) interrupt handler piece code run result exception interrupt. issue packet set instructions issues together one clock cycle; packet may determined statically compiler dynamically processor. issue slots positions instructions could issue given clock cycle; analogy, correspond positions starting blocks sprint. Java bytecode Instruction instruction set designed interpret Java programs. Time compiler (JIT) name commonly given compiler operates runtime, translating interpreted code segments native code computer. latch memory element output equal value stored state inside element state changed whenever appropriate inputs change clock asserted. latency (pipeline) number stages pipeline number stages two instructions execution. least recently used (LRU) replacement scheme block replaced one unused longest 1506time. least significant bit rightmost bit RISC-V doubleword. level-sensitive clocking timing methodology state changes occur either high low clock levels instantaneous, changes edge-triggered designs. linker Also called link editor . systems program combines independently assembled machine language programs resolves undefined labels executable file. liquid crystal display display technology using thin layer liquid polymers used transmit block light according whether charge applied. load-use data hazard specific form data hazard data loaded load instruction yet become available needed another instruction. loader systems program places object program main memory ready execute. local area network (LAN) network designed carry data within geographically confined area, typically within single building. local label label referring object used within file defined. local miss rate fraction references one level cache miss; used multilevel hierarchies. lock synchronization device allows access data one processor time. lookup tables (LUTs) field programmable device, name given cells consist small amount logic RAM. loop unrolling technique get performance loops access arrays, multiple copies loop body made instructions different iterations scheduled together. 1507machine language Binary representation used communication within computer system. macro pattern-matching replacement facility provides simple mechanism name frequently used sequence instructions. magnetic disk Also called hard disk . form nonvolatile secondary memory composed rotating platters coated magnetic recording material. rotating mechanical devices, access times 5 20 milliseconds cost per gigabyte 2012 $0.05 $0.10. main memory Also called primary memory . Memory used hold programs running; typically consists DRAM today’s computers. memory storage area programs kept running, contains data needed running programs. memory hierarchy structure uses multiple levels memories; distance processor increases, size memories access time increase. message passing Communicating multiple processors explicitly sending receiving information. metastability situation occurs signal sampled stable required setup hold times, possibly causing sampled value fall indeterminate region high low value. microarchitecture organization processor, including major functional units, interconnection, control. million instructions per second (MIPS) measurement program execution speed based number millions instructions. MIPS computed instruction count divid ed product execution time 106. MIMD Multiple Instruction streams, Multiple Data streams. multiprocessor. 1508minterms Also called product terms . set logic inputs joined conjunction (AND operations); product terms form first logic stage programmable logic array (PLA). miss penalty time required fetch block level memory hierarchy lower level, including time access block, transmit one level other, insert level experienced miss, pass block requestor. miss rate fraction memory accesses found level memory hierarchy. significant bit leftmost bit RISC-V doubleword. multicore microprocessor microprocessor containing multiple processors (“cores”) single integrated circuit. Virtually microprocessors today desktops servers multicore. multilevel cache memory hierarchy multiple levels caches, rather cache main memory. multiple issue scheme whereby multiple instructions launched one clock cycle. multiprocessor computer system least two processors. computer contrast uniprocessor, one, increasingly hard find today. multistage network network supplies small switch node. NAND gate inverted gate. network bandwidth Informally, peak transfer rate network; refer speed single link collective transfer rate links network. next-state function combinational function that, given inputs current state, determines next state finite-state machine. nonblocking assignment assignment continues evaluating right-hand side, assigning left-hand side value right-hand sides evaluated. 1509nonblocking cache cache allows processor make references cache cache handling earlier miss. nonuniform memory access (NUMA) type single address space multiprocessor memory accesses much faster others depending processor asks word. nonvolatile memory form memory retains data even absence power source used store programs runs. DVD disk nonvolatile. nop instruction operation change state. logical bit-by-bit operation two operands calculates two operands. is, calculates 1 0 operands. gate inverted gate. normalized number floating-point notation leading 0s. logical bit-by-bit operation one operand inverts bits; is, replaces every 1 0, every 0 1. object oriented language programming language oriented around objects rather actions, data versus logic. one’s complement notation represents negative value 10…000two positive value 01…11two, leaving equal number negatives positives ending two zeros, one positive (00…00two) one negative (11…11two). term also used mean inversion every bit pattern: 0 1 1 0. opcode field denotes operation format instruction. OpenMP API shared memory multiprocessing C, C++, Fortran runs UNIX Microsoft platforms. includes 1510compiler directives, library, runtime directives. logical bit-by-bit operation two operands calculates 1 1 either operand. out-of-order execution situation pipelined execution instruction blocked executing cause following instructions wait. output device mechanism conveys result computation, display, user another computer. page fault event occurs accessed page present main memory. page table table containing virtual physical address translations virtual memory system. table, stored memory, typically indexed virtual page number; entry table contains physical page number virtual page page currently memory. parallel processing program single program runs multiple processors simultaneously. PC-relative addressing addressing regime address sum program counter (PC) constant instruction. personal computer (PC) computer designed use individual, usually incorporating graphics display, keyboard, mouse. personal mobile devices (PMDs) Small wireless devices connect Internet; rely batteries power, software installed downloading apps. Conventional examples smart phones tablets. physical address address main memory. physically addressed cache cache addressed physical address. pipeline stall Also called bubble . stall initiated order resolve hazard. 1511pipelining implementation technique multiple instructions overlapped execution, much like assembly line. pixel smallest individual picture element. Screens composed hundreds thousands millions pixels, organized matrix. pop Remove element stack. precise interrupt Also called precise exception . interrupt exception always associated correct instruction pipelined computers. prefetching technique data blocks needed future brought cache early use special instructions specify address block. procedure stored subroutine performs specific task based parameters provided. procedure call frame block memory used hold values passed procedure arguments, save registers procedure may modify procedure’s caller want changed, provide space variables local procedure. procedure frame Also called activation record . segment stack containing procedure’s saved registers local variables. process Includes one threads, address space, operating system state. Hence, process switch usually invokes operating system, thread switch. program counter (PC) register containing address instruction program executed. programmable array logic (PAL) Contains programmable and- plane followed fixed or-plane. programmable logic array (PLA) structured-logic element composed set inputs corresponding input complements two stages logic, first generating 1512product terms inputs input complements, second generating sum terms product terms. Hence, PLAs implement logic functions sum products. programmable logic device (PLD) integrated circuit containing combinational logic whose function configured end user. programmable ROM (PROM) form read-only memory programmed designer knows contents. propagation time time required input flip-flop propagate outputs flip-flop. protection set mechanisms ensuring multiple processes sharing processor, memory, I/O devices cannot interfere, intentionally unintentionally, one another b reading writing other’s data. mechanisms also isolate operating system user process. pseudoinstruction common variation assembly language instructions often treated instruction right. Pthreads UNIX API creating manipulating threads. structured library. push Add element stack. quotient primary result division; number multiplied divisor added remainder produces dividend. read-only memory (ROM) memory whose contents designated creation time, contents read. ROM used structured logic implement set logic functions using terms logic functions address inputs outputs bits word memory. receive message routine routine used processor machines private memories accept message another processor. 1513recursive procedures Procedures call either directly indirectly chain calls. reduction function processes data structure returns single value. reference bit Also called use bit access bit . field set whenever page accessed used implement LRU replacement schemes. reg Verilog, register. register file state element consists set registers read written supplying register number accessed. register renaming renaming registers compiler hardware remove antidependences. register use convention Also called procedure call convention . software protocol governing use registers procedur es. relocation information segment UNIX object file identifies instructions data words depend absolute addresses. remainder secondary result division; number added product quotient divisor produces dividend. reorder buffer buffer holds results dynamically scheduled processor safe store results memory register. reservation station buffer within functional unit holds operands operation. response time Also called execution time . total time required computer complete task, including disk accesses, memory accesses, I/O activities, operating system overhead, CPU execution time, on. restartable instruction instruction resume execution exception resolved without exceptions affectin g result instruction. 1514return address link calling site allows procedure return proper address; RISC-V, usually stored register x1. rotational latency Also called rotational delay . time required desired sector disk rotate read/write head; usually assumed half rotation time. round Method make intermediate floating-point result fit floating-point format; goal typically find nearest number represented format. also name second two extra bits kept right intermediate floating-point calculations, improves rounding accuracy. scientific notation notation renders numbers single digit left decimal point. secondary memory Nonvolatile memory used store programs data runs; typically consists flash memory PMDs magnetic disks servers. sector One segments make track magnetic disk; sector smallest amount information read written disk. seek process positioning read/write head proper track disk. segmentation variable-size address mapping scheme address consists two parts: segment number, mapped physical address, segment offset. selector value Also called control value . control signal used select one input values multiplexor output multiplexor. semiconductor substance conduct electricity well. send message routine routine used processor machines private memories pass message another processor. sensitivity list list signals specifies always block re-evaluated. 1515separate compilation Splitting program across many files, compiled without knowledge files. sequential logic group logic elements contain memory hence whose value depends inputs well current contents memory. server computer used running larger programs multiple users, often simultaneously, typically accessed via network. set-associative cache cache fixed number locations (at least two) block placed. setup time minimum time input memory device must valid clock edge. shared memory multiprocessor (SMP) parallel processor single physical address space. sign-extend Increases size data item replicating high-order sign bit original data item high-order bits larger, destination data item. silicon natural element semiconductor. silicon crystal ingot rod composed silicon crystal 8 12 inches diameter 12 24 inches long. SIMD Single Instruction stream, Multiple Data streams. instruction applied many data streams, vector processor. simple programmable logic device (SPLD) Programmable logic device, usually containing either single PAL PLA. simultaneous multithreading (SMT) version multithreading lowers cost multithreading utilizing resource needed multiple issue, dynamically scheduled microarchitecture. single precision floating-point value represented 32-bit word. 1516single-cycle implementation Also called single clock cycle implementation . implementation instruction executed one clock cycle. easy understand, slow practical. SISD Single Instruction stream, Single Data stream. uniprocessor. Software Service (SaaS) delivers software data service Internet, usually via thin program browser runs local client devices, instead binary code must installed, runs wholly device. Examples include web search social networking. source language high-level language program originally written. spatial locality locality principle stating data location referenced, data locations nearby addresses tend referenced soon. speculation approach whereby compiler processor guesses outcome instruction remove dependence executing instructions. split cache scheme level memory hierarchy composed two independent caches operate parallel other, one handling instructions one handling data. SPMD Single Program, Multiple Data streams. conventional MIMD programming model, single program runs across processors. stack data structure spilling registers organized last-in- first-out queue. stack pointer value denoting recently allocated address stack shows registers spilled old register values found. RISC-V, register x2, also known sp. stack segment portion memory used program hold procedure call frames. 1517state element memory element, register memory. static data portion memory contains data whose size known compiler whose lifetime program’s entire execution. static multiple issue approach implementing multiple- issue processor many decisions made compiler execution. static random access memory (SRAM) memory data stored statically (as flip-flops) rather dynamically (as DRAM). SRAMs faster DRAMs, less dense expensive per bit. sticky bit bit used rounding addition guard round set whenever nonzero bits right round bit. stored-program concept idea instructions data many types stored memory numbers thus easy change, leading stored program computer. strong scaling Speed-up achieved multiprocessor without increasing size problem. structural hazard planned instruction cannot execute proper clock cycle hardware support combination instructions set execute. structural specification Describes digital system organized terms hierarchical connection elements. sum products form logical representation employs logical sum (OR) products (terms joined using operator). supercomputer class computers highest performance cost; configured servers typically cost tens hundreds millions dollars. superscalar advanced pipelining technique enables processor execute one instruction per clock cycl e selecting execution. 1518supervisor mode Also called kernel mode . mode indicating running process operating system process. swap space space disk reserved full virtual memory space process. symbol table table matches names labels addresses memory words instructions occupy. synchronization process coordinating behavior two processes, may running different processors. synchronizer failure situation flip-flop enters metastable state logic blocks reading output flip-flop see 0 others see 1. synchronous system memory system employs clocks data signals read clock indicates signal values stable. system call special instruction transfers control user mode dedicated location supervisor code space, invoking exception mechanism process. system CPU time CPU time spent operating system performing tasks behalf program. systems software Software provides services commonly useful, including operating systems, compilers, loaders, assemblers. tag field table used memory hierarchy contains address information required identify whether associated block hierarchy corresponds requested word. task-level parallelism process-level parallelism Utilizing multiple processors running independent programs simultaneously. temporal locality principle stating data location referenced, tend referenced soon. terabyte (TB) Originally 1,099,511,627,776 (240) bytes, although communications secondary storage systems developers 1519started using term mean 1,000,000,000,000 (1012) bytes. reduce confusion, use term tebibyte (TiB) 240 bytes, defining terabyte (TB) mean 1012 bytes. ( Figure 1.1 shows full range decimal binary values names.) text segment segment UNIX object file contains machine language code routines source file. thread thread includes program counter, register state, stack. lightweight process; whereas threads commonly share single address space, processes don’t. three Cs model cache model cache misses classified one three categories: compulsory misses, capacity misses, conflict misses. throughput Also called bandwidth . Another measure performance, number tasks completed per unit time. tournament branch predictor branch predictor multiple predictions branch selection mechanism chooses predictor enable given branch. track One thousands concentric circles makes surface magnetic disk. transistor on/off switch controlled electric signal. translation-lookaside buffer (TLB) cache keeps track recently used address mappings try avoid access page table. truth table logic, representation logical operation listing values inputs case showing resulting outputs be. underflow (floating-point) situation negative exponent becomes large fit exponent field. uniform memory access (UMA) multiprocessor latency word main memory matter processor requests access. units last place (ulp) number bits error least significant bits significand actual number 1520and number represented. unmapped portion address space cannot page faults. unresolved reference reference requires information outside source complete. use bit Also called reference bit access bit . field set whenever page accessed used implement LRU replacement schemes. use latency Number clock cycles load instruction instruction use result load without stalling pipeline. user CPU time CPU time spent program itself. valid bit field tables memory hierarchy indicates associated block hierarchy contains valid data. vector lane One vector functional units portion vector register file. Inspired lanes highways increase traffic speed, multiple lanes execute vector operation simultaneously. vectored interrupt interrupt address control transferred determined cause exception. Verilog One two common hardware description languages. very-large-scale integrated (VLSI) circuit device containing hundreds thousands millions transistors. long instruction word (VLIW) style instruction set architecture launches many operations defined independent single wide instruction, typically many separate opcode fields. VHDL One two common hardware description languages. virtual address address corresponds location 1521virtual space translated address mapping physical address memory accessed. virtual machine virtual computer appears nondelayed branches loads richer instruction set actual hardware. virtual memory technique uses main memory “cache” secondary storage. virtually addressed cache cache accessed virtual address rather physical address. volatile memory Storage, DRAM, retains data receiving power. wafer slice silicon ingot 0.1 inches thick, used create chips. weak scaling Speed-up achieved multiprocessor expanding size problem proportionally increase number processors. wide area network (WAN) network extended hundreds kilometers span continent. wire Verilog, specifies combinational signal. word natural unit access computer, usually group 32 bits. workload set programs run computer either actual collection applications run user constructed real programs approximate mix. typical workload specifies programs relative frequencies. write buffer queue holds data data waiting written memory. write-back scheme handles writes updating values block cache, writing modified block lower level hierarchy block replaced. write-through scheme writes always update 1522cache next lower level memory hierarchy, ensuring data always consistent two. yield percentage good dies total number dies wafer. 1523Further Reading Chapter 1 1. Barroso L, Hölzle U. case energy-proportional computing. IEEE Computer . 2007;December. plea change nature computer components use much less power lightly utilized . 2. Bell CG. Computer Pioneers Pioneer Computers ACM Computer Museum, videotapes 1996. Two videotapes history computing, produced Gordon Gwen Bell, including following machines inventors: Harvar Mark-I, ENIAC, EDSAC, IAS machine, many others . 3. Burks, A.W., H.H. Goldstine, J. von Neumann [1946]. “Preliminary discussion logical design electronic computing instrument,” Report U.S. Army Ordnance Department , p. 1; also appears Papers John von Neumann , W. Aspray A. Burks (Eds.), MIT Press, Cambridge, MA, Tomash Publishers, Los Angeles, 1987, 97–146. classic paper explaining computer hardware software first stored-program computer built. quote extensively Chapter 3. simultaneously explained computers world source controversy first draft give credit Eck ert Mauchly . 4. Campbell-Kelly M, Aspray W. Computer: History Information Machine New York: Basic Books; 1996. Two historians chronicle dramatic story. New York Times calls well written authoritative . 5. Ceruzzi PF. History Modern Computing Cambridge, MA: MIT Press; 1998. 1524Contains good description later history computing: integrated circuit impact, personal computers, UNIX, Internet . 6. Curnow HJ, Wichmann BA. synthetic benchmark. Computer J. 1976;19(1):80. Describes first major synthetic benchmark, Whetstone, created . 7. Flemming PJ, Wallace JJ. lie statistics: correct way summarize benchmark results. Commun ACM . 1986;29(3 (March)):218–221. Describes underlying principles using different means summarize performance results . 8. Goldstine HH. Computer: Pascal von Neumann Princeton, NJ: Princeton University Press; 1972. personal view computing one pioneers worked von Neumann . 9. Hayes B. Computing parallel universe. American Scientist . 2007;Vol. 95(November–December):476–480. overview parallel computing challenge written layman . 10. Hennessy JL, Patterson DA. Chapter 1 Computer Architecture: Quantitative Approach fifth edition Waltham, MA: Morgan Kaufmann Publishers; 2012. Section 1.5 goes detail power, Section 1.6 contains much detail cost integrated circuits explains reason difference price cost, Section 1.8 gives details n evaluating performance . 11. Lampson, B.W. [1986]. “Personal distributed computing; Alto Ethernet software.” ACM Conference History Personal Workstations (January). 12. Thacker, C.R. [1986]. “Personal distributed computing: Alto Ethernet hardware,” ACM Conference History Personal Workstations (January). two papers describe software hardware landmark Alto. 13. Metropolis N, Howlett J, Rota G-C, eds. History Computing Twentieth Century . New York: Academic Press; 1980. collection essays describe people, software, computers, nd laboratories involved first experimental commercial com puters. 1525Most authors personally involved projects. excellen bibliography early reports concludes interesting book . 14. Public Broadcasting System [1992]. Machine Changed World , videotapes. five 1-hour programs include rare footage interviews pioneers computer industry . 15. Slater R. Portraits Silicon Cambridge, MA: MIT Press; 1987. Short biographies 31 computer pioneers . 16. Stern N. invented first electronic digital computer? Annals History Computing . 1980;2(4 (October)):375– 376. historian perspective Atanasoff versus Eckert Mauchly . 17. Wilkes MV. Memoirs Computer Pioneer Cambridge, MA: MIT Press; 1985. personal view computing one pioneers . Chapter 2 1. Bayko, J. [1996]. “Great microprocessors past present,” search http://www.cpushack.com/CPU/cpu.html . personal view history representative unusual microprocessors, Intel 4004 Patriot Scientific ShBoom ! 2. Kane G, Heinrich J. MIPS RISC Architecture Englewood Cliffs, NJ: Prentice Hall; 1992. book describes MIPS architecture greater detail Appendix . 3. Levy H, Eckhouse R. Computer Programming Architecture: VAX Boston: Digital Press; 1989. book concentrates VAX, also includes descriptions f Intel 8086, IBM 360, CDC 6600 . 4. Morse S, Ravenal B, Mazor S, Pohlman W. Intel microprocessors—8080 8086. Computer . 1980;13. architecture history Intel 4004 8086, according people participated designs . 5. Wakerly J. Microcomputer Architecture Programming New York: Wiley; 1989. Motorola 6800 main focus book, covers Intel 8086, Motorola 6809, TI 9900, Zilog Z8000 . 15266. Waterman, A. Y. Lee, D. Patterson, K. Asanović [2016]. RISC-V Instruction Set Manual, Volume I: User-Level ISA, Version 2.1. canonical reference manual RISC-V instruction set architecture, technical report discusses rationale behind myriad tradeoffs ISA’s design. Download http://riscv.org/specifications/ . Chapter 3 interested learning floating point, two publications David Goldberg [1991, 2002] good starting points; abound pointers reading. Several th e stories told section come Kahan [1972, 1983]. latest word state art computer arithmetic often found Proceedings latest IEEE-sponsored Symposium Computer Arithmetic, held every 2 years; 16th held 2003. 1. Burks, A.W., H.H. Goldstine, J. von Neumann [1946]. “Preliminary discussion logical design electronic computing instrument,” Report U.S. Army Ordnance Dept ., p. 1; also Papers John von Neumann , W. Aspray A. Burks (Eds.), MIT Press, Cambridge, MA, Tomash Publishers, Los Angeles, 1987, 97–146. classic paper includes arguments floating-point hardwar e. 2. Goldberg, D. [2002]. “Computer arithmetic”. Appendix J Computer Architecture: Quantitative Approach , fifth edition, J. L. Hennessy D. A. Patterson, Morgan Kaufmann Publishers, Waltham, MA. advanced introduction integer floating-point arithmeti c, emphasis hardware. covers Sections 3.4–3.6 book 10 pages, leaving another 45 pages advanced topics . 3. Goldberg D. every computer scientist know floating-point arithmetic. ACM Computing Surveys . 1991;23(1):5–48. Another good introduction floating-point arithmetic author, time emphasis software . 4. Kahan W. survey error-analysis . Info Processing 71 (Proc. IFIP Congress 71 Ljubljana). Vol. 2 Amsterdam: North- Holland Publishing; 1972;1214–1239. 1527This survey source stories importance accurate arithmetic . 5. Kahan W. Mathematics written sand. Proc Amer Stat Assoc Joint Summer Meetings 1983, Statistical Computing Section 1983;12–26. title refers silicon another source stories illu strating importance accurate arithmetic . 6. Kahan W. “On advantage 8087’s stack,” unpublished course notes, Computer Science Division Berkeley: University California; 1990. 8087 floating-point architecture could . 7. Kahan, W. [1997]. Available http://www.cims.nyu.edu/~dbindel/class/cs279/87stack.pdf . collection memos related floating point, including “Beastly numbers” (another less famous Pentium bug), “Notes IEEE floating point arithmetic” (including comments feature atrophying), “The baleful effects computing benchmarks” (on unhealthy preoccupation speed versus correctness, accuracy, ease use, flexibility, …) . 8. Koren I. Computer Arithmetic Algorithms second edition Natick, MA: A. K. Peters; 2002. textbook aimed seniors first-year graduate students explains fundamental principles basic arithmetic, well com plex operations logarithmic trigonometric functions . 9. Wilkes MV. Memoirs Computer Pioneer Cambridge, MA: MIT Press; 1985. computer pioneer’s recollections include derivation standard hardware multiply divide developed von Neumann . Chapter 4 1. Bhandarkar D, Clark DW. Performance architecture: Comparing RISC CISC similar hardware organizations . Proc Fourth Conf Architectural Support Programming Languages Operating Systems Palo Alto, CA: IEEE/ACM (April); 1991;310–319. quantitative comparison RISC CISC written scholars argued CISCs well built them; conclude MIPS 2 4 times faster VAX built similar technology, 1528with mean 2.7 . 2. Fisher JA, Rau BR. Journal Supercomputing (January) Kluwer 1993. entire issue devoted topic exploiting ILP. contains papers architecture software wonderful source references . 3. Hennessy JL, Patterson DA. Computer Architecture: Quantitative Approach fifth edition Waltham, MA: Morgan Kaufmann; 2012. Chapter 3 Appendix C go considerably detail pipelined processors (almost 200 pages), including superscalar processor VLIW processors. Appendix G describes Itanium . 4. Jouppi NP, Wall DW. Available instruction-level parallelism superscalar superpipelined processors . Proc Third Conf Architectural Support Programming Languages Operating Systems Boston: IEEE/ACM (April); 1989;272–282. comparison deeply pipelined (also called superpipelined) superscalar systems . 5. Kogge PM. Architecture Pipelined Computers New York: McGraw-Hill; 1981. formal text pipelined control, emphasis underlying principles . 6. Russell RM. CRAY-1 computer system. Commun ACM . 1978;21(1 (January)):63–72. short summary classic computer uses vectors operation remove pipeline stalls . 7. Smith A, Lee J. Branch prediction strategies branch target buffer design. Computer . 1984;17(1 (January)):6–22. early survey branch prediction . 8. Smith JE, Plezkun AR. Implementing precise interrupts pipelined processors. IEEE Trans Computers . 1988;37(5 (May)):562–573. Covers difficulties interrupting pipelined computers . 9. Thornton JE. Design Computer: Control Data 6600 Glenview, IL: Scott, Foresman; 1970. classic book describing classic computer, considered first supercomputer . Chapter 5 15291. Cantin JF, Hill MD. Cache performance selected SPEC CPU2000 benchmarks. SIGARCH Computer Architecture News . 2001;29(4 (September)):13–18. reference paper cache miss rates many cache sizes SPEC2000 benchmarks . 2. Conti C, Gibson DH, Pitowsky SH. Structural aspects System/360 Model 85, part I: General organization. IBM Systems J. 1968;7(1):2–14. classic paper describes first commercial computer use cache resulting performance . 3. Hennessy J, Patterson D. Chapter 2 Appendix B Computer Architecture: Quantitative Approach fifth edition Waltham, MA: Morgan Kaufmann Publishers; 2012. in-depth coverage variety topics including protection, cache performance out-of-order processors, virtually addressed ca ches, multilevel caches, compiler optimizations, additional latency tole rance mechanisms, cache coherency . 4. Kilburn T, Edwards DBG, Lanigan MJ, Sumner FH. One- level storage system . IRE Transactions Electronic Computers 1962; EC-11 (April), 223–35. Also appears D. P. Siewiorek, C G Bell, A. Newell [1982], Computer Structures: Principles Examples , McGraw-Hill, New York, 135–148. classic paper first proposal virtual memory . 5. LaMarca A, Ladner RE. influence caches performance heaps. ACM J Experimental Algorithmics . 1996;Vol. 1. paper shows difference complexity analysis algorithm, instruction count performance, memory hierarchy fo ur sorting algorithms . 6. McCalpin, J.D. [1995]. “STREAM: Sustainable Memory Bandwidth High Performance Computers,” https://www.cs.virginia.edu/stream/ . widely used microbenchmark measures performance memory system behind caches . 7. Przybylski SA. Cache Memory Hierarchy Design: Performance-Directed Approach San Francisco: Morgan Kaufmann Publishers; 1990. thorough exploration multilevel memory hierarchies performance . 15308. Ritchie D. evolution UNIX time-sharing system. AT& Bell Laboratories Technical Journal . 1984;1984:1577– 1593. history UNIX one inventors . 9. Ritchie DM, Thompson K. UNIX time-sharing system. Bell System Technical Journal 1978;(August):1991–2019. paper describing elegant operating system ever invented . 10. Silberschatz A, Galvin P, Grange G. Operating System Concepts sixth edition Reading, MA: Addison-Wesley; 2003. operating systems textbook thorough discussion virtual memory processes process management, protection issues . 11. Smith AJ. Cache memories. Computing Surveys . 1982;14(3 (September)):473–530. classic survey paper caches. paper defined terminology field served reference many computer designers . 12. Smith DK, Alexander RC. Fumbling Future: Xerox Invented, Ignored, First Personal Computer New York: Morrow; 1988. popular book explains role Xerox PARC laying foundation today’s computing, Xerox substantially benefit . 13. Tanenbaum A. Modern Operating Systems second edition Upper Saddle River: Prentice Hall, NJ; 2001. operating system textbook good discussion virtual memory . 14. Waterman, A. Y. Lee, D. Patterson, K. Asanović [2016]. RISC-V Instruction Set Manual, Volume II: Privileged Architecture, Version 1.9.1. RISC-V Privileged Architecture manual discusses detail layered privilege mode design memory address-translation protection schemes described Chapter 5 . 15. Wilkes M. Slave memories dynamic storage allocation. IEEE Trans Electronic Computers EC . 1965;14(2 (April)):270– 271. first classic paper caches . Chapter 6 1. Almasi GS, Gottlieb A. Highly Parallel Computing Redwood 1531City, CA: Benjamin/Cummings; 1989. textbook covering parallel computers . 2. Amdahl, G.M. [1967]. “Validity single processor approach achieving large scale computing capabilities,” Proc. AFIPS Spring Joint Computer Conf ., Atlantic City, NJ (April), 483–85. Written response claims Illiac IV, three-page art icle describes Amdahl’s law gives classic reply arguments abandoning current form computing . 3. Andrews GR. Concurrent Programming: Principles Practice Redwood City, CA: Benjamin/Cummings; 1991. text gives principles parallel programming . 4. Archibald J, Baer J-L. Cache coherence protocols: Evaluation using multiprocessor simulation model. ACM Trans Computer Systems . 1986;4(4 (November)):273–298. Classic survey paper shared-bus cache coherence protocols . 5. Arpaci-Dusseau, A., R. Arpaci-Dusseau, D. Culler, J. Hellerstein, D. Patterson [1997]. “High-performance sorting networks workstations,” Proc. ACM SIG MOD/PODS Conference Management Data , Tucson, AZ (May), 12–15. world record sort performed cluster, including architecture critique workstation network interfac e. April 1, 1997, pushed record 8.6 GB 1 minute 2.2 seconds sort 100 MB. 6. Asanovic K, Bodik R, Catanzaro BC, et al. landscape parallel computing research: view Berkeley . Tech Rep UCB/EECS-2006-183 Berkeley: EECS Department, University California; 2006; (December 18). Nicknamed “Berkeley View,” report lays landscape multicore challenge . 7. Bailey, D.H., E. Barszcz, J.T. Barton, D.S. Browning, R.L. Carter, L. Dagum, R.A. Fatoohi, P.O. Frederickson, T.A. Lasinski, R.S. Schreiber, H.D. Simon, V. Venkatakrishnan, S.K. Weeratunga. [1991]. “The NAS parallel benchmarks—summary preliminary results,” Proceedings 1991 ACM/IEEE Conference Supercomputing (August). Describes NAS parallel benchmarks . 15328. Bell CG. Multis: new class multiprocessor computers. Science . 1985;228(April 26):462–467. Distinguishes shared address nonshared address multiprocessors based micro-processors . 9. Bienia, C., S. Kumar, J.P. Singh, K. Li [2008]. “The PARSEC benchmark suite: characterization architectural implications,” Princeton University Technical Report TR-81 1-008 (January). Describes PARSEC parallel benchmarks. Also see http://parsec.cs.princeton.edu/ . 10. Cooper, B.F., A. Silberstein, E. Tam, R. Ramakrishnan, R. Sears [2010]. Benchmarking cloud serving systems YCSB, In: Proceedings 1st ACM Symposium Cloud Computing , Indianapolis, Indiana, USA. http://dx.doi.org/10.1145/1807128.1807152 (June). Presents “Yahoo! Cloud Serving Benchmark” (YCSB) framework, goal facilitating performance comparisons new generation cloud data serving systems . 11. Culler DE, Singh JP, Gupta A. Parallel Computer Architecture San Francisco: Morgan Kaufmann; 1998. textbook parallel computers . 12. Dongarra JJ, Bunch JR, Moler GB, Stewart GW. LINPACK Users’ Guide Society Industrial Mathematics 1979. original document describing Linpack, became widely used parallel benchmark . 13. Falk H. Reaching gigaflop. IEEE Spectrum . 1976;13(10 (October)):65–70. Chronicles sad story Illiac IV: four times cost less han one-tenth performance original goals . 14. Flynn MJ. high-speed computing systems. Proc IEEE . 1966;54(12 (December)):1901–1909. Classic article showing SISD/SIMD/MISD/MIMD classificatio ns. 15. Hennessy J, Patterson D. Chapters 5 Appendices F . Computer Architecture: Quantitative Approach fifth edition Waltham, MA: Morgan Kaufmann Publishers; 2012. in-depth coverage variety multiprocessor cluster topics, including programs measurements . 16. Henning JL. SPEC CPU suite growth: historical perspective. Computer Architecture News . 2007;Vol. 35 1533(March). Gives history SPEC, including use SPECrate measure performance independent jobs, used parallel benchmark . 17. Hord RM. Illiac-IV, First Supercomputer Rockville, MD: Computer Science Press; 1982. historical accounting Illiac IV project . 18. Hwang K. Advanced Computer Architecture Parallel Programming New York: McGraw-Hill; 1993. Another textbook covering parallel computers . 19. Kozyrakis C, Patterson D. Scalable vector processors embedded systems. IEEE Micro . 2003;23(6 (November– December)):36–45. Examination vector architecture MIPS instruction se media signal processing . 20. Laprie, J.-C. [1985]. “Dependable computing fault tolerance: Concepts terminology,” 15th Annual Int’l Symposium Fault-Tolerant Computing FTCS 15 , Digest Papers, Ann Arbor, MI (June 19–21) 2–11. paper introduced standard definitions dependability, reliability, availability . 21. Menabrea LF. Sketch analytical engine invented Charles Babbage. Bibliothèque Universelle de Genève 1842; (October). Certainly earliest reference multiprocessors, mathem atician made comment translating papers Babbage’s mechanical computer . 22. Patterson, D., G. Gibson, R. Katz [1988]. “A case redundant arrays inexpensive disks (RAID),” SIGMOD Conference , 109–16. 23. Pfister GF. Search Clusters: Coming Battle Lowly Parallel Computing second edition Upper Saddle River, NJ: Prentice Hall; 1998. entertaining book advocates clusters critical NU multiprocessors . 24. Regnier G, Makineni S, Illikkal R, et al. TCP onloading data center servers. IEEE Computer . 2004;37(11):48–58. Describes work researchers Intel Labs, experimente alternative solutions improve server’s ability proce ss 1534TCP/IP packets efficiently high rates . 25. Seitz C. Cosmic Cube. Comm ACM . 1985;28(1 (January)):22–31. tutorial article parallel processor connected via hypertree . Cosmic Cube ancestor Intel supercomputers . 26. Slotnick DL. conception development parallel processors—a personal memoir. Annals History Computing . 1982;4(1 (January)):20–30. Recollections beginnings parallel processing archite ct Illiac IV . 27. Williams S, Waterman A, Patterson D. Roofline: insightful visual performance model multicore architectures. Communications ACM . 2009;52(4 (April)):65–76. 28. Williams, S., J. Carter, L. Oliker, J. Shalf, K. Yelick [2008]. “Lattice Boltzmann simulation optimization leading multicore platforms,” International Parallel & Distributed Processing Symposium (IPDPS) . Paper containing results four multicores LBMHD . 29. Williams S, Oliker L, Vuduc R, Shalf J, Yelick K, Demmel J. Optimization sparse matrix-vector multiplication emerging multicore platforms. Supercomputing (SC) 2007. Paper containing results four multicores SPmV . 30. Williams, S. [2008]. Autotuning Performance Multicore Computers , Ph.D. Dissertation, U.C. Berkeley. Dissertation containing roofline model . 31. Woo, S.C., M. Ohara, E. Torrie, J.P. Singh, A. Gupta. “The SPLASH-2 programs: characterization methodological considerations,” Proceedings 22nd Annual International Symposium Computer Architecture (ISCA ’95) , May, 24–36. Paper describing second version Stanford parallel benchmark s. Appendix number good texts logic design. might like look into. 1. Ashenden P. Digital Design: Embedded Systems Approach Using VHDL/Verilog Waltham, MA: Morgan Kaufmann; 15352007. 2. Ciletti MD. Advanced Digital Design Verilog HDL Englewood Cliff s, NJ: Prentice Hall; 2002. thorough book logic design using Verilog . 3. Harris D, Harris S. Digital Design Computer Architecture Waltham, MA: Morgan Kaufmann; 2012. unique modern approach digital design using VHDL SystemVerilog . 4. Katz RH. Modern Logic Design 2nd ed. Reading, MA: Addison-Wesley; 2004. general text logic design . 5. Wakerly JF. Digital Design: Principles Practices 3rd ed. Englewood Cliff s, NJ: Prentice Hall; 2000. general text logic design . Appendix B 1. Akeley, K. T. Jermoluk [1988]. “High-Performance Polygon Rendering,” Proc. SIGGRAPH 1988 (August), 239– 46. 2. Akeley, K. [1993]. “RealityEngine Graphics.” Proc. SIGGRAPH 1993 (August), 109–16. 3. Blelloch GB. Prefix Sums Applications. In: Reif John H, ed. Synthesis Parallel Algorithms . San Francisco: Morgan Kaufmann Publishers; 1990. 4. Blythe D. Direct3D 10 System. ACM Trans Graphics . 2006;Vol. 25 (July), 724–734. 5. Buck, I., T. Foley, D. Horn, J. Sugerman, K. Fatahlian, M. Houston, P. Hanrahan [2004]. “Brook GPUs: Stream Computing Graphics Hardware.” Proc. SIGGRAPH 2004 , 777–86, August. http://doi.acm.org/10.1145/1186562.1015800 . 6. Elder, G. [2002] “Radeon 9700.” Eurographics/SIGGRAPH Workshop Graphics Hardware, Hot3D Session. www.graphicshardware.org/previous/www_2002/presentations/Hot3D- RADEON9700.ppt . 7. Fernando R, Kilgard MJ. Cg Tutorial: Definitive Guide Programmable Real-Time Graphics Reading, MA: Addison- Wesley; 2003. 15368. Fernando R, ed. GPU Gems: Programming Techniques, Tips, Tricks Real-Time Graphics . Reading, MA: Addison- Wesley; 2004; https://developer.nvidia.com/gpugems/GPUGems/gpugems_pre f01.html 9. Foley J, van Dam A, Feiner S, Hughes J. Computer Graphics: Principles Practice, second edition C Reading, MA: Addison- Wesley; 1995. 10. Hillis WD, Steele GL. Data parallel algorithms. Commun ACM . 1986;29(12 (Dec.)):1170–1183 http://doi.acm.org/10.1145/7902.7903 . 11. IEEE 754R Working Group [2006]. DRAFT Standard Floating-Point Arithmetic P754 . www.validlab.com/754R/drafts/archive/2006-10-04.pdf . 12. Industrial Light Magic [2003]. OpenEXR , www.openexr.com . 13. Intel Corporation [2007]. Intel 64 IA-32 Architectures Optimization Reference Manual . November. Order Number: 248966-016. Also: http://www.intel.com/content/dam/www/public/us/en/documen ts/manuals/64- ia-32-architectures-optimization-manual.pdf . 14. Kessenich, J. [2006]. OpenGL Shading Language, Language Version 1.20, Sept. 2006 . www.opengl.org/documentation/specs/ . 15. Kirk, D. D. Voorhies [1990]. “The Rendering Architecture DN10000VS.” Proc. SIGGRAPH 1990 (August), 299–307. 16. Lindholm E., M.J. Kilgard, H. Moreton [2001]. “A User- Programmable Vertex Engine.” Proc. SIGGRAPH 2001 (August), 149–58. 17. Lindholm E, Nickolls J, Oberman S, Montrym J. NVIDIA Tesla: Unified Graphics Computing Architecture. IEEE Micro . 2008;Vol. 28 (March–April), 39–55. 18. Microsoft Corporation. Microsoft DirectX Specification , https://msdn.microsoft.com/en- us/library/windows/apps/hh452744.aspx . 19. Microsoft Corporation. Microsoft DirectX 9 Programmable Graphics Pipeline Redmond, WA: Microsoft Press; 2003. 20. Montrym, J., D. Baum, D. Dignam, C. Migdal [1997]. “InfiniteReality: Real-Time Graphics System.” Proc. 1537SIGGRAPH 1997 (August), 293–301. 21. Montrym J, Moreton H. GeForce 6800. IEEE Micro . 2005;Vol. 25 (March–April), 41–51. 22. Moore GE. Cramming components onto integrated circuits. Electronics . 1965;Vol. 38 (April 19). 23. Nguyen H, ed. GPU Gems 3 . Reading, MA: Addison-Wesley; 2008. 24. Nickolls J, Buck I, Garland M, Skadron K. Scalable Parallel Programming CUDA. ACM Queue . 2008;Vol. 6(no. 2 (March–April)):40–53. 25. NVIDIA [2007]. CUDA Zone. http://www.nvidia.com/object/cuda_home_new.html . 26. NVIDIA [2007]. CUDA Programming Guide 1.1 . https://developer.nvidia.com/nvidia-gpu-programming- guide . 27. NVIDIA [2007]. PTX: Parallel Thread Execution ISA version 1.1. www.nvidia.com/object/io_1195170102263.html . 28. Nyland, L., M. Harris, J. Prins [2007]. “Fast N-Body Simulation CUDA”. GPU Gems 3 , H. Nguyen (Ed.), Addison-Wesley, Reading, MA. 29. Oberman, S.F. M.Y. Siu [2005]. “A High-Performance Area-Efficient Multifunction Interpolator,” Proc. Seventeenth IEEE Symp. Computer Arithmetic , 272–79. 30. Pharr M, ed. GPU Gems 2: Programming Techniques High- Performance Graphics General-Purpose Computation . Reading, MA: Addison-Wesley; 2005. 31. Satish, N., M. Harris, M. Garland [2008]. “Designing Efficient Sorting Algorithms Manycore GPUs,” NVIDIA Technical Report NVR-2008-001. 32. Segal, M. K. Akeley [2006]. OpenGL Graphics System: Specification, Version 2.1, Dec. 1, 2006 . www.opengl.org/documentation/specs/ . 33. Sengupta, S., M. Harris, Y. Zhang, J.D. Owens [2007]. “Scan Primitives GPU Computing.” Proc. Graphics Hardware 2007 (August), 97–106. 34. Volkov, V. J. Demmel [2008]. “LU, QR Cholesky Factorizations using Vector Capabilities GPUs,” Technical Report No. UCB/EECS-2008-49, 1–11. http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS- 15382008-49.pdf . 35. Williams, S., L. Oliker, R. Vuduc, J. Shalf, K. Yelick, J. Demmel [2007]. “Optimization sparse matrix-vector multiplication emerging multicore platforms,” Proc. Supercomputing 2007 , November. Appendix 1. Bhandarkar DP. Alpha Architecture Implementations Newton, MA: Digital Press; 1995. 2. Darcy, J.D., D. Gay [1996]. “FLECKmarks: Measuring floating point performance using compliant arithmetic benchmark,” CS 252 class project, U.C Berkeley (see www.sonic.net/~jddarcy/Research/fleckmrk.pdf )). 3. Digital Semiconductor. Alpha Architecture Handbook, Version 3 Maynard, MA: Digital Press; 1996; Order number EC- QD2KB-TE (October). 4. Furber SB. ARM System Architecture Harlow, England: Addison-Wesley; 1996; (See http://www.pearsonhighered.com/pearsonhigheredus/educator/product/produ cts_detail.page? isbn=9780201675191&forced_logout=forced_logged_out#sthash.QX4WfErc 5. Hewlett-Packard [1994]. PA-RISC 2.0 Architecture Reference Manual , 3rd ed. 6. Hitachi [1997]. SuperH RISC Engine SH7700 Series Programming Manual . (See http://am.renesas.com/products/mpumcu/superh/sh7700/Documentation.j sp 7. IBM. PowerPC Architecture San Francisco: Morgan Kaufmann; 1994. 8. Kane G. PA-RISC 2.0 Architecture Upper Saddle River, NJ: Prentice Hall PTR; 1996. 9. Kane G, Heinrich J. MIPS RISC Architecture Englewood Cliffs, NJ: Prentice Hall; 1992. 10. Kissell, K.D. [1997]. MIPS16: High-Density Embedded Market . 11. Magenheimer DJ, Peters L, Pettis KW, Zuras D. Integer multiplication division HP precision architecture. IEEE Trans Computers . 1988;37(8):980–990. 12. MIPS [1997]. MIPS16 Application Specific Extension Product Description . 153913. Mitsubishi [1996]. Mitsubishi 32-Bit Single Chip Microcomputer M32R Family Software Manual (September). 14. Muchnick SS. Optimizing compilers SPARC. Sun Technology . 1988;1(3 (Summer)):64–77. 15. Seal, D. Arm Architecture Reference Manual , 2nd ed, Morgan Kaufmann, 2000. 16. Silicon Graphics [1996]. MIPS V Instruction Set. 17. Sites RL, Witek R, eds. Alpha Architecture Reference Manual . 2nd ed. Newton, MA: Digital Press; 1995. 18. Sloss, A. N., D. Symes, C. Wright, ARM System Developer’s Guide , San Francisco: Elsevier Morgan Kaufmann, 2004. 19. Sun Microsystems [1989]. SPARC Architectural Manual , Version 8, Part No. 800-1399-09, August 25. 20. Sweetman, D. See MIPS Run , 2nd ed, Morgan Kaufmann, 2006. 21. Taylor, G., P. Hilfinger, J. Larus, D. Patterson, B. Zorn [1986]. “Evaluation SPUR LISP architecture,” Proc. 13th Symposium Computer Architecture (June), Tokyo. 22. Ungar, D., R. Blau, P. Foley, D. Samples, D. Patterson [1984]. “Architecture SOAR: Smalltalk RISC,” Proc. 11th Symposium Computer Architecture (June), Ann Arbor, MI, 188–97. 23. Weaver DL, Germond T. SPARC Architectural Manual, Version 9 Englewood Cliffs, NJ: Prentice Hall; 1994. 24. Weiss S, Smith JE. Power PowerPC San Francisco: Morgan Kaufmann; 1994. 1540Index Note : Online information listed print page number period followed “e” online page number (54.e1). Page references preceded single letter hyphen refer appendices. Page references followed “ f,” “t,” “ b” refer figures, tables, boxes, respectively. 0-9, symbols 1-bit ALU, A-26–A-29 See also Arithmetic logic unit (ALU) adder, A-27 f CarryOut, A-28 significant bit, A-33 f illustrated, A-29 f logical unit AND/OR, A-27 f performing AND, OR, addition, A-31 , A-33 f 64-bit ALU, A-29–A-31 See also Arithmetic logic unit (ALU) 63 copies 1-bit ALU, A-34 f 64 1-bit ALUs, A-30 f defining Verilog, A-36–A-37 illustrated, A-35 f ripple carry adder, A-29 15417090/7094 hardware, 227.e6 Absolute references, 127 Abstractions hardware/software interface, 22 principle, 22 simplify design, 11 Accumulator architectures, 162.e1–162.e2 Acronyms, 9 Active matrix, 18 add (add), 64f addi (add immediate), 64f, 72, 84 Addition, 174–177 See also Arithmetic binary, 174b–175 b floating-point, 198–201 , 206 operands, 175, 175 significands, 197b–198 b speed, 177b Address interleaving, 372–373 Address select logic, C-24 , C-25 f Address space, 420, 423b extending, 469b flat, 469 ID (ASID), 438 inadequate, 473.e5–473.e6 shared, 509–510 1542single physical, 509, 509–510 virtual, 438 Address translation ARM cortex-A53, 460f defined, 420–421 fast, 430–432 Intel core i7, 460f TLB for, 430–432 Address-control lines, C-26 f Addresses base, 69 byte, 70 defined, 68 memory, 78b virtual, 420–421 , 440, 441b Addressing base, 118f branches, 115–117 displacement, 118 immediate, 118f PC-relative, 115–116 , 118f register, 118f RISC-V modes, 117–118 x86 modes, 151 Addressing modes desktop architectures, D-5–D-6 Advanced Vector Extensions (AVX), 218, 219 AGP, B-9–B-10 1543Algol-60, 162.e6 Aliasing, 436 Alignment restriction, 70 All-pairs N-body algorithm, B-65 Alpha architecture bit count instructions, D-29 floating-point instructions, D-28–D-29 instructions, D-27–D-29 divide, D-28 PAL code, D-28 unaligned load-store, D-28 VAX floating-point formats, D-29 ALU control, 251–253 See also Arithmetic logic unit (ALU) bits, 252–253 , 252f logic, C-6–C-7 mapping gates, C-4–C-7 truth tables, C-5f, C-5f ALU control block, 255 defined, C-4–C-6 generating ALU control bits, C-6f ALUOp, 252, C-6b–C-7 b bits, 252, 253 control signal, 255 Amazon Web Services (AWS), 417b AMD Opteron X4 (Barcelona), 535, 536f AMD64, 148, 148, 217, 162.e5 Amdahl’s law, 393, 495–496 1544corollary, 49 defined, 49 fallacy, 548 (and), 64f gates, A-12–A-13 , C-7 operation, 90, A-6 andi (and immediate), 64f Annual failure rate (AFR), 410–411 versus MTTF disks, 410b–411 b Antidependence, 327 Antifuse, A-77 Apple computer, 54.e6 Apple iPad 2 A1395, 20f logic board of, 20f processor integrated circuit of, 21f Application binary interface (ABI), 22 Application programming interfaces (APIs) defined, B-4 graphics, B-14 Architectural registers, 337–338 Arithmetic, 172 addition, 174–177 addition subtraction, 174–177 division, 183–191 fallacies pitfalls, 222–225 floating-point, 191–216 historical perspective, 227 multiplication, 177–183 1545parallelism and, 216–217 Streaming SIMD Extensions advanced vector extensions x86, 217–218 subtraction, 174–177 subword parallelism, 216–217 subword parallelism matrix multiply, 218–222 Arithmetic instructions See also Instructions desktop RISC, D-11 f, D-11 f embedded RISC, D-13 f logical, 243–244 operands, 67–74 Arithmetic intensity, 533–534 Arithmetic logic unit (ALU) See also ALU control , Control units 1-bit, A-26–A-29 64-bit, A-29–A-31 forwarding, 299f branch datapath, 246–247 hardware, 176 memory-reference instruction use, 237 register values, 244 R-format operations, 245f signed-immediate input, 302 ARM Cortex-A53, 236, 334–342 address translation for, 460f caches in, 461f data cache miss rates for, 462f memory hierarchies of, 459 1546performance of, 462–464 specification, 335f TLB hardware for, 460f ARPAnet, 54.e9 Arrays, 407f logic elements, A-18–A-20 multiple dimension, 212 pointers versus , 141–144 procedures setting zero, 141f ASCII binary numbers versus , 109b character representation, 108f defined, 108–109 symbols, 111 Assemblers, 125–127 defined, 14 function, 125–127 microcode, C-30 number acceptance, 126 object file, 126 Assembly language, 15f defined, 14, 125 floating-point, 207f illustrated, 15f programs, 125 RISC-V, 64f, 85b–86b translating machine language, 85b–86b Asserted signals, 242, A-4 1547Associativity caches, 397b–398 b degree, increasing, 396–398 , 444 increasing, 401–402 set, tag size versus , 401b–402 b Atomic compare swap, 123b Atomic exchange, 122 Atomic fetch-and-increment, 123b Atomic memory operation, B-21 Attribute interpolation, B-43–B-44 auipc’s effect, 156 Automobiles, computer application in, 4 Average memory access time (AMAT), 394 calculating, 394b B Bandwidth, 29–30 bisection, 527 external DRAM, 390 memory, 390 network, 525–526 Barrier synchronization, B-18 defined, B-20 thread communication, B-34 Base addressing, 69, 118 Base registers, 69 Basic block, 95b Benchmarks, 530–540 1548defined, 46 Linpack, 530, 227.e2–227.e3 , 227.e3 multiprocessor, 530–540 NAS parallel, 532 parallel, 531f PARSEC suite, 532 SPEC CPU, 46–48 SPEC power, 48–49 SPECrate, 530 Stream, 540b Biased notation, 81, 195 Binary numbers, 82 ASCII versus , 109b conversion decimal numbers, 77b defined, 74 Bisection bandwidth, 527 Bit maps defined, 18 goal, 18 storing, 18 Bit-Interleaved Parity (RAID 3), 458.e4 Bits ALUOp, 252, 253 defined, 14 dirty, 430b guard, 214 patterns, 214b–215 b reference, 428b 1549rounding, 214 sign, 75 state, C-8–C-10 sticky, 214 valid, 376–378 Blocking assignment, A-24 Blocking factor, 406 Block-Interleaved Parity (RAID 4), 458.e4–458.e5 Blocks combinational, A-4–A-5 defined, 367–368 finding, 444–445 flexible placement, 394–398 least recently used (LRU), 401 locating cache, 399–401 miss rate and, 383f multiword, mapping addresses to, 382b–383 b placement locations, 443 placement strategies, 396 replacement selection, 401 replacement strategies, 446 spatial locality exploitation, 383 state, A-4–A-5 valid data, 376–378 Bonding, 28 Boolean algebra, A-6–A-7 Bounds check shortcut, 96 Branch datapath 1550ALU, 246–247 operations, 246–247 Branch Equal (beq), A-32 Branch greater equal, unsigned (bgeu), 95–96 Branch less (blt) instruction, 95–96 Branch less than, unsigned (bltu), 95–96 Branch instructions pipeline impact, 308f Branch taken assumption, 307–308 defined, 246 Branch prediction buffers, 310 control hazard solution, 274 defined, 273–274 dynamic, 274, 310–314 static, 324 Branch predictors accuracy, 312 correlation, 312–313 information from, 312–313 tournament, 313–314 Branch table, 97–98 Branch taken cost reduction, 308–309 defined, 246 Branch target addresses, 246 1551buffers, 312 Branches See also Conditional branches addressing in, 115–117 compiler creation, 93–94 decision, moving up, 308–309 delayed, 274, 308–310 ending, 95b execution ID stage, 309 pipelined, 310b target address, 308–309 Branch-on-zero instruction, 260–261 Bubble Sort, 140 Bubbles, 305 Bus-based coherent multiprocessors, 553.e1 Buses, A-18–A-19 Bytes addressing, 70 order, 70 C C language assignment, compiling RISC-V, 65b compiling, 144–145 , 144.e1–144.e2 compiling assignment registers, 67b–68b compiling loops in, 94b–95b sort algorithms, 141f translation hierarchy, 124f 1552translation RISC-V assembly language, 65 variables, 104b C.mmp, 553.e3–553.e4 C++ language, 162.e7 , 144.e26 Cache blocking matrix multiply, 465–468 Cache coherence, 454–458 coherence, 454 consistency, 454 enforcement schemes, 456 implementation techniques, 459.e10–459.e11 migration, 456 problem, 454, 455f, 458b protocol example, 459.e11–459.e15 protocols, 456 replication, 456 snooping protocol, 456–458 snoopy, 459.e16 state diagram, 459.e15 f Cache coherency protocol, 459.e11–459.e15 finite-state transition diagram, 459.e14 f functioning, 459.e13 f mechanism, 459.e13 f state diagram, 459.e15 f states, 459.e12 write-back cache, 459.e14 f Cache controllers, 459 coherent cache implementation techniques, 459.e10–459.e11 implementing, 459.e1 1553snoopy cache coherence, 459.e16 SystemVerilog, 459.e1–459.e4 Cache hits, 460 Cache misses block replacement on, 445–446 capacity, 447, 448 compulsory, 447 conflict, 447 defined, 384 direct-mapped cache, 396 fully associative cache, 398 handling, 384–385 memory-stall clock cycles, 391 reducing flexible block placement, 394–398 set-associative cache, 397 steps, 385 write-through cache, 385 Cache performance, 390–410 calculating, 392b–393 b hit time and, 393–394 impact processor performance, 392–393 Cache-aware instructions, 472 Caches, 375–390 See also Blocks accessing, 378–384 ARM cortex-A53, 461f associativity in, 397b–398 b bits in, 382b 1554bits needed for, 382 contents illustration, 379f defined, 19–22 , 375–376 direct-mapped, 376, 377f, 382, 394 empty, 378 FSM controlling, 449–454 fully associative, 395 GPU, B-38 inconsistent, 385 index, 380 Intel Core i7, 461f Intrinsity FastMATH example, 387–389 locating blocks in, 399–401 locations, 377f multilevel, 390, 402–405 nonblocking, 460 physically addressed, 436–437 physically indexed, 436b–437 b physically tagged, 436b–437 b primary, 402, 409–410 secondary, 402, 409–410 set-associative, 395 simulating, 468b size, 381–383 split, 389b summary, 389–390 tag field, 380 tags, 459.e1 f, 459.e10–459.e11 , 459.e11 1555virtual memory TLB integration, 435–437 virtually addressed, 436 virtually indexed, 436 virtually tagged, 436 write-back, 386, 387, 446 write-through, 385, 387, 446 writes, 385–387 Callee, 99, 101 Caller, 99 Capabilities, 473.e12 Capacity misses, 447 Carry lookahead, A-37–A-47 4-bit ALUs using, A-43 f adder, A-38 fast, first level abstraction, A-38–A-40 fast, “infinite” hardware, A-38 fast, second level abstraction, A-40–A-45 plumbing analogy, A-41 f, A-42 f ripple carry speed versus , A-45 b summary, A-45–A-47 Carry save adders, 183 CDC 6600, 347.e2 , 54.e6 Cell phones, 6–7 Central processor unit (CPU) See also Processors classic performance equation, 36–40 defined, 19 execution time, 32, 33–34 1556performance, 33–35 system, time, 32 time, 391 time measurements, 33–34 user, time, 32 Cg pixel shader program, B-15 Characters ASCII representation, 108–109 Java, 111–113 Chips, 19, 25–26 , 26 manufacturing process, 26 Classes defined, 144.e14 packages, 144.e20 Clock cycles defined, 33 memory-stall, 391 number registers and, 67 worst-case delay and, 262 Clock cycles per instruction (CPI), 35–36 , 272 one level caching, 402 two levels caching, 402 Clock rate defined, 33 frequency switched function of, 41 power and, 40 Clocking methodology, 241–243 , A-47 edge-triggered, 241, A-47 , A-72–A-73 1557level-sensitive, A-73–A-74 , A-74–A-75 predictability, 241 Clocks, A-47–A-49 edge, A-47 , A-49 b edge-triggered design, A-72 f skew, A-73 specification, A-56 f synchronous system, A-47–A-48 Cloud computing, 524–525 defined, 7 Cluster networking, 529–530 , 529.e1 , 529.e3–529.e5 , 529.e6–529.e9 Clusters, 553.e7–553.e8 defined, 492, 522, 553.e7 isolation, 523 organization, 491 scientific computing on, 553.e7 Cm*, 553.e3–553.e4 CMOS (complementary metal oxide semiconductor), 41 Coarse-grained multithreading, 506–507 Cobol, 162.e6 Code generation, 144.e12 Code motion, 144.e6 Cold-start miss, 447 Collision misses, 447 Column major order, 405 Combinational blocks, A-4–A-5 Combinational control units, C-4–C-8 Combinational elements, 240 1558Combinational logic, 241, A-3–A-4 , A-9–A-20 arrays, A-18–A-19 decoders, A-9–A-10 defined, A-4–A-5 don’t cares, A-17–A-18 multiplexors, A-10 ROMs, A-14–A-16 two-level, A-11–A-14 Verilog, A-23–A-26 Commercial computer development, 54.e3–54.e9 Commit units buffer, 329 defined, 329 update control, 334b Common case fast, 11 Common subexpression elimination, 144.e5 Communication, 23–24 overhead, reducing, 44–45 thread, B-34 Compact code, 162.e3–162.e4 Compare branch zero, 309 Comparisons constant operands in, 72–74 signed versus unsigned, 95–96 Compilers, 125 branch creation, 94b brief history, 162.e7–162.e8 conservative, 144.e6 1559defined, 14 front end, 144.e2 function, 14, 125 high-level optimizations, 144.e3–144.e4 ILP exploitation, 347.e4–347.e5 Time (JIT), 133 optimization, 141, 162.e8 speculation, 323–324 structure, 144.e1 f Compiling C assignment statements, 65b C language, 94b–95b, 144–145 , 144.e1 , 144.e2 floating-point programs, 208b–209 b if-then-else, 93b Java, 144.e18–144.e19 procedures, 100b–101 b, 102b–103 b recursive procedures, 102b–103 b loops, 94b–95b Compressed sparse row (CSR) matrix, B-55 , B-56 Compulsory misses, 447, 448–449 Computer architects, 11–13 abstraction simplify design, 11 common case fast, 11 dependability via redundancy, 12 hierarchy memories, 12 Moore’s law, 11 parallelism, 12 pipelining, 12 1560prediction, 12 Computers application classes, traditional, 3 applications, 4 arithmetic for, 172 characteristics, 54.e12 f commercial development, 54.e3–54.e9 component organization, 17f components, 17f design measure, 53 desktop, 5 embedded, 5–6 first, 54.e2 information revolution, 4 instruction representation, 81–89 performance measurement, 54.e1–54.e3 post-PC era, 6–7 servers, 5 Condition codes/flags, 96 Conditional branches changing program counter with, 312b compiling if-then-else into, 93b defined, 92–93 desktop RISC, D-16 f embedded RISC, D-16 f implementation, 97b loops, 117 PA-RISC, D-34–D-36 , D-35 f 1561PC-relative addressing, 115–116 RISC, D-10–D-16 SPARC, D-10–D-12 Conditional move instructions, 313b–314 b Conflict misses, 447 Constant memory, B-40 Constant operands, 72–74 frequent occurrence, 72 Content Addressable Memory (CAM), 400b–401 b Context switch, 438b Control ALU, 251–253 challenge, 315 finalizing, 261 forwarding, 300 FSM, C-8–C-22 implementation, optimizing, C-27 mapping hardware, C-3–C-4 , C-4–C-8 , C-8–C-22 , C-22–C-28 , C- 28–C-32 , C-32–C-33 memory, C-26 f organizing, reduce logic, C-31–C-32 pipelined, 290–294 Control status register (CSR) access instructions, 464–465 Control flow graphs, 144.e8 , 144.e9 illustrated examples, 144.e8 f, 144.e9 f, 144.e11 f Control functions ALU, mapping gates, C-4–C-7 defining, 256 1562PLA, implementation, C-7, C-20 ROM, encoding, C-19 single-cycle implementation, 261–262 Control hazards, 271–274 , 307–315 branch delay reduction, 308–310 branch taken assumption, 307–308 branch prediction solution, 274 delayed decision approach, 274b dynamic branch prediction, 310–314 logic implementation Verilog, 345.e8 pipeline stalls solution, 272f pipeline summary, 314–315 solutions, 272f static multiple-issue processors and, 324 Control lines asserted, 256 datapath, 255f execution/address calculation, 291 final three stages, 293f instruction decode/register file read, 291 instruction fetch, 291 memory access, 291 setting of, 256 values, 291 write-back, 291 Control signals ALUOp, 255 defined, 242 1563effect of, 256f multi-bit, 256 pipelined datapaths with, 290–294 truth tables, C-14 f Control units, 239–240 See also Arithmetic logic unit (ALU) address select logic, C-24 , C-25 f combinational, implementing, C-4–C-8 explicit counter, C-23 f illustrated, 257f logic equations, C-11–C-12 main, designing, 253–256 microcode, C-28 f next-state outputs, C-10 , C-12 b–C-13 b output, 251–253 , C-10 RISC-V, C-10 f Cooperative thread arrays (CTAs), B-30 Coprocessors defined, 212b Core RISC-V instruction set abstract view, 238f desktop RISC, D-9f implementation, 236–237 implementation illustration, 239f overview, 237–240 subset, 236 Cores defined, 43 1564number per chip, 43 Correlation predictor, 312–313 Cosmic Cube, 553.e6–553.e7 CPU, 9 Cray computers, 227.e4 , 227.e5 Critical word first, 384 Crossbar networks, 527–528 CTSS (Compatible Time-Sharing System), 473.e13 CUDA programming environment, 515, B-5–B-6 barrier synchronization, B-18 , B-34 development, B-17 , B-17–B-18 hierarchy thread groups, B-18 kernels, B-19 , B-24 key abstractions, B-18 paradigm, B-19–B-22 parallel plus-scan template, B-61 f per-block shared memory, B-58 plus-reduction implementation, B-63 f programs, B-6, B-24 , B-24 scalable parallel programming with, B-17–B-23 shared memories, B-18 threads, B-36 Cyclic redundancy check, 415b–416 b Cylinder, 374 flip-flops, A-50–A-51 , A-52 latches, A-50–A-51 , A-51 1565Data bits, 413f Data flow analysis, 144.e8 Data hazards, 268–271 , 294–307 See also Hazards forwarding, 268–269 , 294–307 load-use, 269–271 , 308 stalls and, 303–307 Data parallel problem decomposition, B-17 , B-18 f Data race, 121 Data selectors, 237–238 Data transfer instructions See also Instructions defined, 68, 69 load, 69 offset, 69 store, 70–71 Datacenters, 7 Data-level parallelism, 500 Datapath elements defined, 243 sharing, 248–249 Datapaths branch, 246–247 building, 243–251 control signal truth tables, C-14 f control unit, 257f defined, 19 design, 243 1566exception handling, 318f fetching instructions, 245f hazard resolution via forwarding, 302f memory instructions, 247 operation branch-if-equal instruction, 260–261 operation load instruction, 259f operation R-type instruction, 258f operation of, 256–261 pipelined, 276–294 RISC-V architecture, 249 R-type instructions, 256–259 single, creating, 247–251 single-cycle, 275 static two-issue, 326f Deasserted signals, 242, A-4 DEC PDP-8, 162.e2 f Decimal numbers binary number conversion to, 77b defined, 74 Decision-making instructions, 92–98 Decoders, A-9–A-10 two-level, A-64 Decoding machine language, 118–120 Defect, 26–27 Delayed branches, 274 See also Branches control hazard solution, 274 embedded RISCs and, D-23 1567reducing, 308–310 Delayed decision, 274b DeMorgan’s theorems, A-11 Denormalized numbers, 216 Dependability via redundancy, 12 Dependable memory hierarchy, 410–416 failure, defining, 410–412 Dependences pipeline registers, 238–239 pipeline registers ALU inputs, 297–298 bubble insertion and, 305 detection, 297b name, 327 sequence, 295 Design compromises and, 84 datapath, 243 digital, 345 logic, 240–243 main control unit, 253–256 memory hierarchy, challenges, 449f pipelining instruction sets, 267 Desktop server RISCs See also Reduced instruction set computer (RISC) architectures addressing modes, D-6 architecture summary, D-4f, D-4f arithmetic/logical instructions, D-11 f conditional branches, D-16 1568constant extension summary, D-9f, D-9f control instructions, D-11 f conventions equivalent MIPS core, D-12 f data transfer instructions, D-10 f features added to, D-45 f floating-point instructions, D-12 f instruction formats, D-7f multimedia extensions, D-16–D-18 multimedia support, D-18 f Desktop computers, defined, 5 Device driver, 529.e4 DGEMM (Double precision General Matrix Multiply), 218–219 , 342, 344–345 , 405, 530 cache blocked version of, 407f optimized C version of, 220f, 342f, 466f performance, 344f, 408f Dicing, 27 Dies, 26–27 Digital design pipeline, 345 Digital signal-processing (DSP) extensions, D-19 DIMMs (dual inline memory modules), 473.e4 Direct Data IO (DDIO), 529.e6 Direct memory access (DMA), 529.e2 f, 529.e3 Direct3D, B-13 Direct-mapped caches See also Caches address portions, 399f choice of, 400–401 1569defined, 376, 394 illustrated, 377f memory block location, 395f misses, 397b–398 b single comparator, 399 total number bits, 382 Dirty bit, 430b Dirty pages, 430b Disk memory, 373–375 Displacement addressing, 118 Distributed Block-Interleaved Parity (RAID 5), 458.e5–458.e6 Divide algorithm, 186b Dividend, 184 Division, 183–191 algorithm, 185f dividend, 184 divisor, 184 Divisor, 184 divu (Divide Unsigned) See also Arithmetic faster, 188–189 floating-point, 206–212 hardware, 184–187 hardware, improved version, 187f operands, 184 quotient, 184 remainder, 184 RISC-V, 189 1570signed, 187–188 SRT, 189 Don’t cares, A-17–A-18 example, A-17 b–A-18 b term, 253 Double data rate (DDR), 371–372 Double Data Rate (DDR) SDRAM, 371–372 , A-64 Double precision See also Single precision defined, 193 FMA, B-45 , B-45–B-46 GPU, B-45 , B-74 b representation, 212–214 Doubleword, 67, 151 Dual inline memory modules (DIMMs), 373 Dynamic branch prediction, 310–314 See also Control hazards branch prediction buffer, 310 loops and, 312b Dynamic hardware predictors, 274 Dynamic multiple-issue processors, 322, 328–333 See also Multiple issue pipeline scheduling, 329–333 superscalar, 328 Dynamic pipeline scheduling, 329–333 commit unit, 329 concept, 329 hardware-based speculation, 331–333 primary units, 330f 1571reorder buffer, 334b reservation station, 329 Dynamic random access memory (DRAM), 370, 371–373 , A-62–A-64 bandwidth external to, 390 cost, 23 defined, 19, A-62 DIMM, 473.e4 Double Date Rate (DDR), 371–372 early board, 473.e4 f GPU, B-37–B-38 growth capacity, 25f history, 473.e1 internal organization of, 372f pass transistor, A-62 b–A-64 b SIMM, 473.e5 f, 473.e4 single-transistor, A-63 f size, 390 speed, 23–24 synchronous (SDRAM), 371–372 , A-59 , A-64 two-level decoder, A-64 Dynamically linked libraries (DLLs), 130–132 defined, 130 lazy procedure linkage version, 130 E Early restart, 384b Edge-triggered clocking methodology, 241, 242, A-47 , A-72–A-73 advantage, A-48 1572clocks, A-72–A-73 drawbacks, A-73–A-74 illustrated, A-49 f rising edge/falling edge, A-47 EDSAC (Electronic Delay Storage Automatic Calculator), 473.e2 f, 473.e1 , 54.e2 Eispack, 227.e2–227.e3 , 227.e3 Electrically erasable programmable read-only memory (EEPROM), 373 Elements combinational, 240 datapath, 243, 248–249 memory, A-49–A-57 state, 240, 242, 244f, A-47 , A-49 b Embedded computers, 5–6 application requirements, 6 design, 5 growth, 54.e11 Embedded Microprocessor Benchmark Consortium (EEMBC), 54.e11 Embedded RISCs See also Reduced instruction set computer (RISC) architectures addressing modes, D-6 architecture summary, D-4f, D-4f arithmetic/logical instructions, D-14 f conditional branches, D-16 constant extension summary, D-9f, D-9f control instructions, D-15 f data transfer instructions, D-13 f 1573delayed branch and, D-23 DSP extensions, D-19 general purpose registers, D-5 instruction conventions, D-15 f instruction formats, D-8f multiply-accumulate approaches, D-19 f Encoding defined, C-31 RISC-V instruction, 85f, 119f ROM control function, C-18 ROM logic function, A-15 x86 instruction, 153–154 ENIAC (Electronic Numerical Integrator Calculator), 473.e1 , 54.e2–54.e3 , 54.e2 , 54.e3 EPIC, 347.e4 Error correction, A-64–A-66 Error Detecting Correcting Code (RAID 2), 458.e4 Error detection, A-65–A-66 Error detection code, 412 Ethernet, 23–24 EX stage load instructions, 282f overflow exception detection, 317, 320f store instructions, 284f Exabyte, 6f Exception enable, 439b Exceptions, 315–321 association, 321b 1574datapath controls handling, 318f defined, 193, 315 detecting, 315 event types and, 315 imprecise, 321b interrupts versus , 315 pipelined computer example, 318b–319 b pipelined implementation, 317–321 precise, 321b reasons for, 316–317 result due overflow add instruction, 320f RISC-V architecture, 316–317 saving/restoring stage on, 440 Executable files defined, 127–129 Execute address calculation stage, 282 Execute/address calculation control line, 291 load instruction, 282 store instruction, 282 Execution time CPU, 32, 33–34 pipelining and, 276 valid performance measure, 50–51 Explicit counters, C-23–C-24 , C-26 f Exponents, 192 F 1575Failures, synchronizer, A-75–A-76 Fallacies See also Pitfalls Amdahl’s law, 548 arithmetic, 222 assembly language performance, 158b commercial binary compatibility importance, 158b defined, 49 GPUs, B-72 , B-75 low utilization uses little power, 50b peak performance, 548b pipelining, 345 powerful instructions mean higher performance, 157 right shift, 222b False sharing, 457 Fast carry first level abstraction, A-38–A-40 “infinite” hardware, A-38 second level abstraction, A-40–A-45 Fast Fourier Transforms (FFT), B-53 Fault avoidance, 411 Fault forecasting, 411 Fault tolerance, 411 Fermi architecture, 515, 544 Field programmable devices (FPDs), A-77–A-78 Field programmable gate arrays (FPGAs), A-77 Fields defined, 83 1576format, C-31 names, 83 RISC-V, 83–89 Files, register, 244, 249, A-49 b, A-53–A-55 Fine-grained multithreading, 506 Finite-state machines (FSMs), 449–454 , A-66–A-71 control, C-8–C-22 controllers, 452f multicycle control, C-9f simple cache controller, 453–454 implementation, 451, A-69 Mealy, 452 Moore, 452b–453 b next-state function, 451, A-66 output function, A-66 , A-68 state assignment, A-69 state register implementation, A-70 f style of, 452b–453 b synchronous, A-66 SystemVerilog, 459.e6 f traffic light example, A-67 Flash memory, 373 defined, 23 Flat address space, 469 Flip-flops flip-flops, A-50–A-51 , A-52 defined, A-50–A-51 Floating point, 191–216 1577assembly language, 207f backward step, 227.e3–227.e4 binary decimal conversion, 197b branch, 206 challenges, 226 diversity versus portability, 227.e2–227.e3 division, 206 first dispute, 227.e1–227.e2 form, 192 fused multiply add, 214b guard digits, 213b history, 227.e2 IEEE 754 standard, 193–198 intermediate calculations, 212–213 operands, 207f overflow, 192 packed format, 218 precision, 223 procedure two-dimensional matrices, 80b programs, compiling, 79b–80b registers, 212b representation, 192–193 RISC-V instruction frequency for, 226f RISC-V instructions, 206–212 rounding, 212–213 sign magnitude, 192 SSE2 architecture, 217, 217f subtraction, 206 1578underflow, 192 units, 213–214 x86, 217f Floating vectors, 227.e2 Floating-point addition, 198–201 arithmetic unit block diagram, 202f binary, 199b–201 b illustrated, 200f instructions, 206–212 steps, 198, 198, 198, 198–199 Floating-point arithmetic (GPUs), B-41–B-46 basic, B-42 double precision, B-45–B-46 , B-74 b performance, B-44 specialized, B-42–B-44 supported formats, B-42 texture operations, B-44 Floating-point control status register (fcsr), 193 Floating-point instructions desktop RISC, D-12 f SPARC, D-31–D-32 Floating-point multiplication, 201–206 binary, 205b–206 b illustrated, 204f instructions, 206 significands, 201–205 steps, 201–203 , 203, 203, 203, 203–205 Flow-sensitive information, 144.e13 b–144.e14 b 1579Flushing instructions, 308, 309–310 exceptions and, 319b loops, 142, 144.e25 inner, 144.e23 SIMD and, 553.e2 Format fields, C-31 Fortran, 162.e6 Forwarding, 294–307 ALU before, 299f control, 300 datapath hazard resolution, 302f defined, 268–269 graphical representation, 269f illustrations, 345.e20 multiple results and, 271 multiplexors, 300f pipeline registers before, 299f two instructions, 268b–269 b Verilog implementation, 345.e3 Fractions, 192, 193 Frame buffer, 18 Frame pointers, 104–105 Front end, 144.e2 Fully associative caches See also Caches block replacement strategies, 445–446 choice of, 445 defined, 395 1580memory block location, 395f misses, 398 Fully connected networks, 527 Fused-multiply-add (FMA) operation, 214b, B-45 G Game consoles, B-9 Gates, A-3–A-4 , A-4–A-9 AND, A-12–A-13 , C-7 delays, A-45 mapping ALU control function to, C-4–C-7 NAND, A-8–A-9 NOR, A-8–A-9 , A-49 f Gather-scatter, 503, 544 General Purpose GPUs (GPGPUs), B-5 General-purpose registers, 147 architectures, 162.e2 f embedded RISCs, D-5 Generate defined, A-39 example, A-44 b–A-45 b super, A-40 Gigabyte, 6f Global common subexpression elimination, 144.e5 Global memory, B-21 , B-39 Global miss rates, 408b Global optimization, 144.e4–144.e10 code, 144.e6 1581implementing, 144.e7 Global pointers, 104b GPU computing See also Graphics processing units (GPUs) defined, B-5–B-6 visual applications, B-6 GPU system architectures, B-7–B-12 graphics logical pipeline, B-10 heterogeneous, B-7–B-9 implications for, B-24–B-25 interfaces drivers, B-9–B-10 unified, B-10–B-11 Graph coloring, 144.e11 Graphics displays computer hardware support, 18 LCD, 18 Graphics logical pipeline, B-10 Graphics processing units (GPUs), 514–521 See also GPU computing accelerators, 514 attribute interpolation, B-43–B-44 defined, 46, 498–499 , B-3 evolution, B-5 fallacies pitfalls, B-72–B-75 floating-point arithmetic, B-16 , B-41–B-46 , B-74 GeForce 8-series generation, B-5 general computation, B-73 b General Purpose (GPGPUs), B-5 1582graphics mode, B-6 graphics trends, B-4 history, B-3–B-4 logical graphics pipeline, B-13–B-14 mapping applications to, B-55–B-72 memory, 514 multilevel caches and, 514 N-body applications, B-65–B-68 NVIDIA architecture, 515–517 parallel memory system, B-36–B-41 parallelism, 515, B-76 performance doubling, B-4 perspective, 519–521 programming, B-12–B-25 programming interfaces to, B-17 real-time graphics, B-13 Graphics shader programs, B-14–B-15 Gresham’s Law, 227, 227.e1 Grid computing, 525b–526 b Grids, B-19 GTX 280, 540–541 Guard digits defined, 212–213 rounding with, 213b H Half precision, B-42 Halfwords, 112 1583Hamming, Richard, 412 Hamming distance, 412 Hamming Error Correction Code (ECC), 412–413 calculating, 412 Hard disks access times, 23 defined, 23 Hardware hierarchical layer, 13f language of, 14–16 operations, 63–67 supporting procedures in, 98–108 synthesis, A-21 translating microprograms to, C-28–C-32 virtualizable, 418 Hardware description languages See also Verilog defined, A-20 using, A-20–A-26 VHDL, A-20–A-21 Hardware multithreading, 506–509 coarse-grained, 506–507 options, 507f simultaneous, 507 Hardware-based speculation, 331–333 Harvard architecture, 54.e3 Hazard detection units, 303 pipeline connections for, 306–307 1584Hazards See also Pipelining control, 271–274 , 307–315 data, 268–271 , 294–307 forwarding and, 302b structural, 267–268 , 284 Heap allocating space on, 104–105 defined, 105 Heterogeneous systems, B-4–B-5 architecture, B-7–B-12 defined, B-3 Hexadecimal numbers, 82 binary number conversion to, 82f, 83b Hierarchy memories, 12 High-level languages, 14–16 benefits, 16 computer architectures, 162.e4 importance, 16 High-level optimizations, 144.e3–144.e4 Hit rate, 368 Hit time cache performance and, 393–394 defined, 368–369 Hit miss, 460 Hold time, A-52–A-53 Horizontal microcode, C-32 Hot-swapping, 458.e6–458.e7 1585Human genome project, 4 I/O, 529.e1–529.e2 , 529.e2 , 529.e2 system performance, 458.e1 b–458.e2 b I/O benchmarks, See Benchmarks IBM 360/85, 473.e5 IBM 701, 54.e4 IBM 7030, 347.e1 IBM ALOG, 227.e6 IBM Blue Gene, 553.e8 , 553.e8–553.e9 IBM Personal Computer, 162.e5 , 54.e7 IBM System/360 computers, 54.e5 f, 347.e1 , 227.e5 , 227.e6 IBM z/VM, 473.e12 ID stage branch execution in, 309, 310 load instructions, 282f store instruction in, 281f IEEE 754 floating-point standard, 193–198 , 194f, 227.e7–227.e9 See also Floating point first chips, 227.e7–227.e9 GPU arithmetic, B-42 implementation, 227.e9 rounding modes, 213–214 today, 227.e9 statements, 115–116 If-then-else, 93b Imagination Technologies, 145 1586Immediate addressing, 118 Immediate instructions, 72 Imprecise interrupts, 321b, 347.e2–347.e3 Index-out-of-bounds check, 96 Induction variable elimination, 144.e6 Inheritance, 144.e14 In-order commit, 330–331 Input devices, 16–17 Inputs, 253 Instances, 144.e14 Instruction count, 36, 38 Instruction decode/register file read stage control line, 290–294 load instruction, 279 store instruction, 284 Instruction execution illustrations, 345.e13–345.e20 clock cycle 9, 345.e25 f clock cycles 1 2, 345.e21 f clock cycles 3 4, 345.e22 f clock cycles 5 6, 345.e23 f clock cycles 7 8, 345.e24 f examples, 345.e15–345.e20 forwarding, 345.e20 , 345.e20 hazard, 345.e15 pipelines stalls forwarding, 345.e20 Instruction fetch stage control line, 291 load instruction, 279 1587store instruction, 284 Instruction formats, 153 defined, 82 desktop/server RISC architectures, D-7f embedded RISC architectures, D-8f I-type, 84 MIPS, 146f RISC-V, 146f R-type, 84, 253–254 SB-type, 115 S-type, 84, 84–85 UJ-type, 115, 115 U-type, 113–114 x86, 153–154 Instruction latency, 346–347 Instruction mix, 39–40 , 54.e9 Instruction set architecture branch address calculation, 246 defined, 22, 52 history, 162 maintaining, 52 protection and, 419 thread, B-31–B-34 virtual machine support, 418 Instruction sets, B-49 MIPS-32, 146f RISC-V, 160 x86 growth, 162f 1588Instruction-level parallelism (ILP), 344–345 See also Parallelism compiler exploitation, 347.e4–347.e5 defined, 43b, 321–322 exploitation, increasing, 333 matrix multiply, 342–345 Instructions, 60, D-25–D-27 , D-40 , D-40–D-43 See also Arithmetic instructions , MIPS , Operands add immediate, 72–74 addition, 176 Alpha, D-27–D-29 arithmetic-logical, 243–244 ARM, D-36–D-38 assembly, 65 basic block, 95b cache-aware, 472 conditional branch, 92–93 , 93b conditional move, 313b–314 b data transfer, 68 decision-making, 92–98 defined, 14, 62 desktop RISC conventions, D-12 f electronic signals, 81–82 embedded RISC conventions, D-15 f encoding, 85f fetching, 245f floating-point, 206–212 floating-point (x86), 217f 1589flushing, 308, 309–310 immediate, 72 introduction to, 62–63 left-to-right flow, 277 load, 69 logical operations, 89–92 M32R, D-40 memory access, B-33–B-34 memory-reference, 237 multiplication, 183 nop, 304–305 PA-RISC, D-34–D-36 performance, 35–36 pipeline sequence, 304f PowerPC, D-12–D-13 , D-32–D-34 PTX, B-31 , B-32 f representation computer, 81–89 restartable, 440–441 resuming, 440b–441 b R-type, 243–244 , 248–249 SPARC, D-29–D-32 store, 71 store-conditional doubleword, 122–123 subtraction, 176 SuperH, D-39–D-40 thread, B-30–B-31 Thumb, D-38–D-39 vector, 500–502 1590as words, 62 x86, 146–155 Instructions per clock cycle (IPC), 322 Integrated circuits (ICs), 19 See also specific chips cost, 27 defined, 25 manufacturing process, 26 large-scale (VLSIs), 25 Intel Core i7, 46–49 , 236, 493, 540–545 address translation for, 460f architectural registers, 337–338 caches in, 461f memory hierarchies of, 459–464 microarchitecture, 337 performance of, 462 SPEC CPU benchmark, 46–48 SPEC power benchmark, 48–49 TLB hardware for, 460f Intel Core i7 920, 337–340 microarchitecture, 337 Intel Core i7 960 benchmarking rooflines of, 540–545 Intel Core i7 Pipelines, 334–342 , 337–340 memory components, 338f performance, 340–342 program performance, 341b specification, 335f 1591Intel IA-64 architecture, 162.e2 f Intel Paragon, 553.e6–553.e7 Intel Threading Building Blocks, B-60 Intel x86 microprocessors clock rate power for, 40f Interference graphs, 144.e10 Interleaving, 390 Interprocedural analysis, 144.e13 b–144.e14 b Interrupt enable, 439b Interrupt-driven I/O, 529.e3 Interrupts defined, 193, 315 event types and, 315 exceptions versus , 315 imprecise, 321b, 347.e2–347.e3 precise, 321b vectored, 316 Intrinsity FastMATH processor, 387–389 caches, 388f data miss rates, 389f, 399f read processing, 434f TLB, 432–435 write-through processing, 434f Inverted page tables, 429 Issue packets, 324–325 I-type, 87b J 1592Java bytecode, 132 bytecode architecture, 144.e10–144.e12 characters in, 111–113 compiling in, 144.e18–144.e19 goals, 132 interpreting, 132, 144–145 , 144.e14 keywords, 144.e20 method invocation in, 144.e20 pointers, 144.e25–144.e26 primitive types, 144.e25 programs, starting, 132–133 reference types, 144.e25 sort algorithms, 141f strings in, 111–113 translation hierarchy, 132f loop compilation in, 144.e17 b–144.e18 b Java Virtual Machine (JVM), 145, 144.e15 Jump-and-link register instruction (jalr), 97–98 , 99 Jump instructions, D-26 branch instruction versus , 250f control datapath for, 251 implementing, 237–240 instruction format, 250 Time (JIT) compilers, 133, 552 K Karnaugh maps, A-18 1593Kernel mode, 437 Kernels CUDA, B-19 , B-24 defined, B-19–B-22 Kilobyte, 6f L LAPACK, 223–224 Large-scale multiprocessors, 553.e6 , 553.e6–553.e7 Latches latch, A-50–A-51 , A-51 defined, A-50–A-51 Latency instruction, 346–347 memory, B-74 b pipeline, 276b use, 325–327 lb (load byte), 64f lbu (load byte, unsigned), 64f ld (load doubleword), 64f Leaf procedures See also Procedures defined, 102 example, 112f Least recently used (LRU) block replacement strategy, 445–446 defined, 401 pages, 426–428 1594Least significant bits defined, 74 SPARC, D-31 Left-to-right instruction flow, 277 Level-sensitive clocking, A-73–A-74 , A-74–A-75 defined, A-73–A-74 two-phase, A-74 lh (load halfword), 64f lhu (load halfword, unsigned), 64f Link, 529.e1–529.e2 Linkers, 127–129 defined, 127 executable files, 127–129 steps, 127 Linking object files, 128b–129 b Linpack, 530, 227.e2–227.e3 , 227.e3 Liquid crystal displays (LCDs), 18 LISP, SPARC support, D-30 Live range, 144.e10 Livermore Loops, 54.e10 Load balancing, 497b–498 b Load byte, 109 Load doubleword, 69, 71–72 Load instructions See also Store instructions access, B-41 base register, 254 compiling with, 71b 1595datapath operation for, 259f defined, 69 EX stage, 282f halfword unsigned, 112 ID stage, 281f stage, 281f load byte unsigned, 78 load half, 112 MEM stage, 283f pipelined datapath in, 286f signed, 78b unit implementing, 247f unsigned, 78b WB stage, 283f Loaders, 130 Load-reserved doubleword, 122–123 Load-store architectures, 162.e2 Load upper immediate, 113–114 Load-use data hazard, 269–271 , 308 Load-use stalls, 308 Load word, 113b Load word unsigned, 113b Local area networks (LANs), 24 See also Networks Local memory, B-21 , B-40 Local miss rates, 408b Local optimization, 144.e4 See also Optimization 1596implementing, 144.e7 Locality principle, 366–367 spatial, 366, 369b temporal, 366, 369b Lock synchronization, 121 Locks, 510–513 Logic address select, C-24 , C-25 f ALU control, C-6–C-7 combinational, 242, A-5, A-9–A-20 components, 241 control unit equations, C-11 f design, 240–243 equations, A-7b minimization, A-18 programmable array (PAL), A-77 sequential, A-4–A-5 , A-55–A-57 two-level, A-11–A-14 Logical operations, 89–92 AND, 90 desktop RISC, D-11 f, D-11 f embedded RISC, D-13 f NOT, 91 OR, 91 shifts, 90 xor, 91 Long instruction word (LIW), 347.e4 15971598Lookup tables (LUTs), A-77–A-78 Loop unrolling defined, 327–328 , 144.e3–144.e4 multiple-issue pipelines, 327b–328 b register renaming and, 327 Loops, 94–96 conditional branches in, 115–116 for, 142 prediction and, 312b test, 142, 143 while, compiling, 94b–95b lr.d (load reserved), 64f lui (load upper immediate), 64f lw (load word), 64f lwu (load word, unsigned), 64f M32R, D-15 , D-40 Machine code, 82 Machine instructions, 82 Machine language, 15f branch offset in, 116b–117 b decoding, 118–120 defined, 14, 82 illustrated, 15f RISC-V, 87–89 SRAM, 19–22 translating RISC-V assembly language into, 85b–86b 1599Main memory, 420 See also Memory defined, 23 page tables, 429 physical addresses, 420 Mapping applications, B-55–B-72 Mark computers, 54.e3 Matrix multiply, 218–222 , 545–548 Mealy machine, 452, A-67 , A-70–A-71 , A-71 b Mean time failure (MTTF), 410–411 versus AFR disks, 410b–411 b improving, 411–412 Media Access Control (MAC) address, 529.e6 Megabyte, 6f Memory addresses, 78b affinity, 538f atomic, B-21 bandwidth, 371–372 , 389b cache, 19–22 , 375–390 , 390–410 CAM, 400b–401 b constant, B-40 control, C-26 defined, 19 DRAM, 19, 371–373 , A-62–A-64 flash, 23 global, B-21 , B-39 GPU, 514 1600instructions, datapath for, 247 local, B-21 , B-40 main, 23 nonvolatile, 22–23 operands, 68–72 parallel system, B-36–B-41 read-only (ROM), A-14–A-16 SDRAM, 371–372 secondary, 23 shared, B-17 , B-39–B-40 spaces, B-39 SRAM, A-57–A-59 stalls, 392 technologies building, 24–28 texture, B-40 virtual, 419–443 volatile, 22–23 Memory access instructions, B-33–B-34 Memory access stage control line, 292f load instruction, 282f store instruction, 282 Memory bandwidth, 540–541 , 549b Memory consistency model, 458b Memory elements, A-49–A-57 clocked, A-50 flip-flop, A-50–A-51 , A-52 latch, A-51 1601DRAMs, A-62–A-64 flip-flop, A-50 hold time, A-52–A-53 latch, A-50 setup time, A-52–A-53 , A-53 f SRAMs, A-57–A-59 unclocked, A-50 Memory hierarchies, 537 ARM cortex-A53, 459–464 block (or line), 367–368 cache performance, 390–410 caches, 375–390 common framework, 443–449 defined, 367 design challenges, 449b development, 473.e5–473.e7 exploiting, 364 Intel Core i7, 459–464 level pairs, 368f multiple levels, 367 overall operation of, 435b–436 b parallelism and, 454–458 , 458.e1–458.e2 pitfalls, 468–472 program execution time and, 409 quantitative design parameters, 443f redundant arrays inexpensive disks, 458 reliance on, 369 structure, 367f 1602structure diagram, 370f variance, 409b virtual memory, 419–443 Memory rank, 373 Memory technologies, 370–375 disk memory, 373–375 DRAM technology, 370, 371–373 flash memory, 373 SRAM technology, 370, 371 Memory-mapped I/O, 529.e2 Memory-stall clock cycles, 391 Message passing defined, 521 multiprocessors, 521–526 Metastability, A-75–A-76 Methods defined, 144.e14 invoking Java, 144.e19–144.e20 Microarchitectures, 337 Intel Core i7 920, 337–340 Microcode assembler, C-30 control unit as, C-28 f defined, C-27 dispatch ROMs, C-30 , C-30 f horizontal, C-32 vertical, C-32 Microinstructions, C-31 1603Microprocessors design shift, 493 multicore, 8, 43, 492–493 Microprograms abstract control representation, C-30–C-31 field translation, C-28–C-29 translating hardware, C-28–C-32 Migration, 456 Million instructions per second (MIPS), 51 Minterms defined, A-12–A-13 , C-20 PLA implementation, C-20 MIP-map, B-44 MIPS RISC-V common features between, 145 MIPS-16 16-bit instruction set, D-41–D-42 immediate fields, D-41 instructions, D-40–D-43 MIPS core instruction changes, D-42–D-43 PC-relative addressing, D-41 MIPS-32 instruction set, 145 MIPS-64 instructions, 145, D-25–D-27 conditional procedure call instructions, D-27 constant shift amount, D-25 jump/call PC-relative, D-26 move to/from control registers, D-26 nonaligned data transfers, D-25 1604NOR, D-25 parallel single precision floating-point operations, D-27 reciprocal reciprocal square root, D-27 SYSCALL, D-25 TLB instructions, D-26–D-27 Mirroring, 458.e4 Miss penalty defined, 368–369 determination, 383–384 multilevel caches, reducing, 402–405 Miss rates block size versus , 383–384 data cache, 444f defined, 368 global, 408b improvement, 383–384 Intrinsity FastMATH processor, 389 local, 408b miss sources, 448 split cache, 389b Miss miss, 460 MMX (MultiMedia eXtension), 217 Moore machines, 452, A-67 , A-70–A-71 , A-71 b Moore’s law, 11, 371, 514, 529.e1–529.e2 , B-72 b significant bit 1-bit ALU for, A-33 f defined, 74 MS-DOS, 473.e15 , 473.e15 1605Multicore, 509–514 Multicore multiprocessors, 8, 43 defined, 8, 492–493 MULTICS (Multiplexed Information Computing Service), 473.e8 Multilevel caches See also Caches complications, 408b defined, 390, 408b miss penalty, reducing, 402–405 performance of, 402b–403 b summary, 409–410 Multimedia extensions desktop/server RISCs, D-16–D-18 SIMD extensions instruction sets, 553.e3 vector versus , 501b–502 b Multiple dimension arrays, 212 Multiple instruction multiple data (MIMD), 550–551 defined, 499, 500 first multiprocessor, 553.e3–553.e4 Multiple instruction single data (MISD), 499–500 Multiple issue, 322 code scheduling, 326b–327 b dynamic, 322, 328–333 issue packets, 324–325 loop unrolling and, 327b–328 b processors, 322 static, 322, 324–328 1606throughput and, 332b Multiple processors, 545–548 Multiple-clock-cycle pipeline diagrams, 286–287 five instructions, 288f illustrated, 287–290 Multiplexors, A-10 controls, 451 datapath, 255f defined, 237–238 forwarding, control values, 300f selector control, 251 two-input, A-10 Multiplicand, 178 Multiplication, 177–183 See also Arithmetic fast, hardware, 182 faster, 182–183 first algorithm, 180f floating-point, 201–206 hardware, 178–182 instructions, 183 operands, 183 product, 183 sequential version, 178–182 signed, 182 Multiplier, 178 Multiply algorithm, 178–182 Multiply-add (MAD), B-42 1607Multiprocessors benchmarks, 530–540 bus-based coherent, 553.e6 defined, 492 historical perspective, 553 large-scale, 553.e6 , 553.e6–553.e7 message-passing, 521–526 multithreaded architecture, B-26–B-27 , B-36 organization, 491, 521 performance, 549 shared memory, 492–493 , 509–514 software, 493f TFLOPS, 553.e5 UMA, 510 Multistage networks, 527–528 Multithreaded multiprocessor architecture, B-25–B-36 conclusion, B-36 ISA, B-31–B-34 massive multithreading, B-25–B-26 multiprocessor, B-26–B-27 multiprocessor comparison, B-35–B-36 SIMT, B-27–B-29 special function units (SFUs), B-35 streaming processor (SP), B-34 thread instructions, B-30–B-31 threads/thread blocks management, B-30 Multithreading, B-25–B-26 coarse-grained, 506–507 1608defined, 498–499 fine-grained, 506 hardware, 506–509 simultaneous (SMT), 507 Must-information, 144.e13 b–144.e14 b Mutual exclusion, 121 N Name dependence, 327 NAND gates, A-8–A-9 NAS (NASA Advanced Supercomputing), 532 N-body all-pairs algorithm, B-65 GPU simulation, B-71 mathematics, B-65–B-66 multiple threads per body, B-68–B-72 optimization, B-67 performance comparison, B-69–B-70 results, B-70–B-72 shared memory use, B-67–B-68 Negation shortcut, 78–79 Nested procedures, 102–104 compiling recursive procedure showing, 102b–103 b NetFPGA 10-Gigagit Ethernet card, 529.e1 f, 529.e2 f Network Workstations, 553.e7–553.e8 Network topologies, 526–529 implementing, 528–529 multistage, 529f 1609Networking, 529.e3–529.e4 operating system in, 529.e3–529.e5 performance improvement, 529.e6–529.e9 Networks, 23–24 advantages, 23 bandwidth, 527 crossbar, 527–528 fully connected, 527 local area (LANs), 23–24 multistage, 527–528 wide area (WANs), 23–24 Newton’s iteration, 212b Next state nonsequential, C-24 sequential, C-23–C-24 Next-state function, 451, A-66 defined, 451 implementing, sequencer, C-22–C-28 Next-state outputs, C-27 , C-12 b–C-13 b example, C-12 implementation, C-12–C-13 logic equations, C-12 b–C-13 b truth tables, C-13–C-15 Redundancy (RAID 0), 458.e3 write allocation, 386 Nonblocking assignment, A-24 Nonblocking caches, 334b, 460 Nonuniform memory access (NUMA), 510 1610Nonvolatile memory, 22–23 Nops, 304–305 gates, A-8–A-9 cross-coupled, A-49 f latch implemented with, A-51 f operation, D-25 operation, 91, A-6 Numbers binary, 74 computer versus real-world, 215 decimal, 74, 77b denormalized, 216 hexadecimal, 83 signed, 74–81 unsigned, 74–81 NVIDIA GeForce 8800, B-46–B-55 all-pairs N-body algorithm, B-71 dense linear algebra computations, B-51–B-53 FFT performance, B-53 instruction set, B-49 performance, B-51 rasterization, B-50 ROP, B-50–B-51 scalability, B-51 sorting performance, B-54–B-55 special function approximation statistics, B-43 f special function unit (SFU), B-50 streaming multiprocessor (SM), B-48–B-49 1611streaming processor, B-49–B-50 streaming processor array (SPA), B-46 texture/processor cluster (TPC), B-47 NVIDIA GPU architecture, 515–517 NVIDIA GTX 280, 541f, 542f NVIDIA Tesla GPU, 540–545 Object files, 128b–129 b debugging information, 127 header, 126 linking, 128b–129 b relocation information, 126 static data segment, 126 symbol table, 127 text segment, 126 Object-oriented languages See also Java brief history, 162.e7 defined, 145, 144.e14 One’s complement, 81, A-29 Opcodes control line setting and, 256 defined, 83, 254 OpenGL, B-13 OpenMP (Open MultiProcessing), 512b–513 b, 532 Operands, 67–74 See also Instructions 161232-bit immediate, 113–114 adding, 175 arithmetic instructions, 67 compiling assignment memory, 69b constant, 72–74 division, 183–191 floating-point, 207f memory, 68–72 multiplication, 177–183 RISC-V, 64f Operating systems brief history, 473.e8 defined, 13 encapsulation, 22 networking, 529.e3–529.e5 Operations atomic, implementing, 122 hardware, 63–67 logical, 89–92 x86 integer, 151–152 Optimization class explanation, 144.e13 f compiler, 141f control implementation, C-27 global, 144.e4–144.e10 high-level, 144.e3–144.e4 local, 144.e4–144.e10 , 144.e7 manual, 144 1613or (inclusive or), 64f operation, 176, A-6 ori (inclusive immediate), 64f Out-of-order execution defined, 330 performance complexity, 408b–409 b processors, 334b Output devices, 16–17 Overflow defined, 75, 192 detection, 176 exceptions, 318f floating-point, 193 occurrence, 175 saturation and, 177b subtraction, 175 P P+Q redundancy (RAID 6), 458.e6 Packed floating-point format, 218 Page faults, 426 See also Virtual memory data access, 461 defined, 420–421 handling, 422, 439–441 virtual address causing, 432–435 Page tables, 445 defined, 424–425 1614illustrated, 427f indexing, 424–425 inverted, 429 levels, 429 main memory, 429 register, 424–425 storage reduction techniques, 429 updating, 424 VMM, 441b Pages See also Virtual memory defined, 420–421 dirty, 430b finding, 424–425 LRU, 426–428 offset, 421 physical number, 421 placing, 424–425 size, 422f virtual number, 421 Parallel bus, 529.e1–529.e2 Parallel execution, 121 Parallel memory system, B-36–B-41 See also Graphics processing units (GPUs) caches, B-38 constant memory, B-40 DRAM considerations, B-37–B-38 global memory, B-39 1615load/store access, B-41 local memory, B-40 memory spaces, B-39 MMU, B-38–B-39 ROP, B-41 shared memory, B-39–B-40 surfaces, B-41 texture memory, B-40 Parallel processing programs, 494–499 creation difficulty, 494–499 defined, 492 message passing, 511b–513 b great debates in, 553.e4–553.e6 shared address space, 511b–513 b use of, 549 Parallel reduction, B-62 Parallel scan, B-60–B-63 CUDA template, B-61 f inclusive, B-60 tree-based, B-62 f Parallel software, 493 Parallelism, 12, 43b, 321–334 computers arithmetic, 216–217 data-level, 226, 500 debates, 553.e4–553.e6 GPUs and, 514, B-76 instruction-level, 43, 321–322 , 333 memory hierarchies and, 454–458 , 458.e1–458.e2 1616multicore and, 509b multiple issue, 322b multithreading and, 507 performance benefits, 44 process-level, 492 redundant arrays inexpensive disks, 458 subword, D-17 task, B-24 task-level, 492 thread, B-22 Paravirtualization, 472 PA-RISC, D-14 , D-17 branch vectored, D-35 conditional branches, D-34 , D-35 f debug instructions, D-36 decimal operations, D-35 extract deposit, D-35 instructions, D-34–D-36 load clear instructions, D-36 multiply/add multiply/subtract, D-36 nullification, D-34 nullifying branch option, D-25 store bytes short, D-36 synthesized multiply divide, D-34–D-35 Parity, 458.e4 bits, 412–413 code, 420, A-64–A-65 PARSEC (Princeton Application Repository Shared Memory 1617Computers), 532 Pass transistor, A-62 b–A-64 b PCI-Express (PCIe), 529, B-7–B-8 , 529.e1–529.e2 PC-relative addressing, 115–116 , 118 Peak floating-point performance, 534 Pentium bug morality play, 224f Performance, 28–40 assessing, 28 classic CPU equation, 36–40 components, 38f CPU, 33–35 defining, 29–32 equation, using, 36–40 improving, 34b–35b instruction, 35–36 measuring, 32–33 , 54.e9 program, 9–10 ratio, 31 relative, 31b response time, 30b sorting, B-49–B-50 throughput, 30b time measurement, 32 Personal computers (PCs), 7f defined, 5 Personal mobile device (PMD) defined, 6–7 Petabyte, 6f 1618Physical addresses, 420 mapping to, 420–421 space, 509, 511b–513 b Physically addressed caches, 436–437 Pipeline registers forwarding, 298–300 dependences, 297–298 , 298f forwarding unit selection, 302 Pipeline stalls, 270 avoiding code reordering, 270b–271 b data hazards and, 303–307 insertion, 305f load-use, 308 solution control hazards, 272f Pipelined branches, 310b Pipelined control, 290–294 See also Control control lines, 290–291 , 291 overview illustration, 306f specifying, 291 Pipelined datapaths, 276–294 connected control signals, 294f control signals, 290–294 corrected, 286f illustrated, 279f load instruction stages, 286f Pipelined dependencies, 296f Pipelines 1619branch instruction impact, 308f effectiveness, improving, 347.e3–347.e4 execute address calculation stage, 280, 282 five-stage, 264, 280, 288b–290 b graphic representation, 269f, 286–290 instruction decode register file read stage, 278f, 282 instruction fetch stage, 279f, 282 instructions sequence, 304f latency, 276b memory access stage, 280, 282 multiple-clock-cycle diagrams, 286–287 performance bottlenecks, 332–333 single-clock-cycle diagrams, 286–287 stages, 264–265 static two-issue, 325f write-back stage, 279, 284 Pipelining, 12, 262–276 advanced, 333–334 benefits, 262 control hazards, 271–274 data hazards, 268–271 exceptions and, 317–321 execution time and, 276b fallacies, 345–346 hazards, 267–271 instruction set design for, 267 laundry analogy, 263f overview, 262–276 1620paradox, 263–264 performance improvement, 267 pitfall, 345–346 simultaneous executing instructions, 276b speed-up formula, 265 structural hazards, 267–268 , 284 summary, 314–315 throughput and, 276b Pitfalls See also Fallacies address space extension, 384–385 arithmetic, 222–225 associativity, 469b defined, 49 GPUs, B-74 ignoring memory system behavior, 468b memory hierarchies, 468–472 out-of-order processor evaluation, 469b performance equation subset, 50b pipelining, 345–346 pointer automatic variables, 159b sequential word addresses, 159b simulating cache, 468 software development multiprocessors, 548b VMM implementation, 470–472 Pixel shader example, B-15–B-17 Pixels, 18 Pointers 1621arrays versus , 141–144 frame, 104–105 global, 104b incrementing, 143 Java, 144.e25–144.e26 stack, 99, 102–104 Polling, 529.e6 Pop, 99 Power clock rate and, 40 critical nature of, 53 efficiency, 333–334 relative, 41b–42b PowerPC algebraic right shift, D-33 branch registers, D-32–D-33 condition codes, D-12–D-13 instructions, D-12–D-13 instructions unique to, D-32–D-34 load multiple/store multiple, D-33 logical shifted immediate, D-33 rotate mask, D-33 Precise interrupts, 321b Prediction, 12 2-bit scheme, 312 accuracy, 312 dynamic branch, 310–314 loops and, 312b 1622steady-state, 312 Prefetching, 472, 536 Primitive types, 144.e25 Procedure calls preservation across, 104 Procedures, 98–108 compiling, 100b–101 b compiling, showing nested procedure linking, 100b–101 b execution steps, 98 frames, 104 leaf, 102 nested, 102b–103 b recursive, 107b setting arrays zero, 141f sort, 135–140 strcpy, 110b–111 b string copy, 110b–111 b swap, 134–135 Process identifiers, 438 Process-level parallelism, 492 Processors, 234 control, 19 cores, 43 datapath, 19 defined, 17b, 19 dynamic multiple-issue, 322 multiple-issue, 322 out-of-order execution, 334b, 408b–409 b 1623performance growth, 44f ROP, B-12 , B-41 speculation, 323–324 static multiple-issue, 322, 324–328 streaming, B-34 superscalar, 328, 507–508 , 347.e4 technologies building, 24–28 two-issue, 325–327 vector, 499–500 VLIW, 324 Product, 178 Product sums, A-11 Program counters (PCs), 243 changing conditional branch, 313b–314 b defined, 99, 243 exception, 437, 439 incrementing, 243, 245f instruction updates, 279 Program performance elements affecting, 39t understanding, 9 Programmable array logic (PAL), A-77 Programmable logic arrays (PLAs) component dots illustration, A-16 f control function implementation, C-7f, C-20 defined, A-12–A-13 example, A-13 b–A-14 b illustrated, A-13 f 1624ROMs and, A-15–A-16 size, C-20 truth table implementation, A-13 Programmable logic devices (PLDs), A-77 Programmable ROMs (PROMs), A-14 Programming languages See also specific languages brief history of, 162.e6–162.e7 object-oriented, 145 variables, 67 Programs assembly language, 125 Java, starting, 132–133 parallel processing, 492 starting, 124–133 translating, 124–133 Propagate defined, A-39 example, A-44 b–A-45 b super, A-40 Protected keywords, 144.e20 Protection defined, 420 implementing, 437–439 mechanisms, 473.e12 VMs for, 416–417 Protection group, 458.e4 Pseudoinstructions 1625defined, 125 summary, 126 Pthreads (POSIX threads), 532 PTX instructions, B-31 , B-32 f Public keywords, 144.e20 Push defined, 99 using, 102–104 Q Quad words, 151 Quicksort, 403b–405 b, 404f Quotient, 184 R Race, A-72–A-73 Radix sort, 403b–405 b, 404f, B-63–B-65 CUDA code, B-64 f implementation, B-63–B-65 RAID, See Redundant arrays inexpensive disks (RAID) RAM, 9 Raster operation (ROP) processors, B-12 , B-41 , B-50–B-51 fixed function, B-41 Raster refresh buffer, 18 Rasterization, B-50 Ray casting (RC), 544 Read-only memories (ROMs), A-14–A-16 control entries, C-16 b–C-18 b 1626control function encoding, C-19 dispatch, C-25 f implementation, C-15–C-19 logic function encoding, A-15 overhead, C-18 PLAs and, A-15–A-16 , A-16 programmable (PROM), A-14 total size, C-15–C-16 Read-stall cycles, 391 Read-write head, 373 Receive message routine, 521 Recursive procedures, 107b See also Procedures clone invocation, 102 Reduced instruction set computer (RISC) architectures, D-3–D-5 , D- 5–D-9 , D-9–D-16 , D-16–D-18 , D-19 , D-20–D-25 , D-25–D-27 , D-27– D-29 , D-29–D-32 , D-32–D-34 , D-34–D-36 , D-36–D-38 , D-38–D-39 , D-39–D-40 , D-40 , D-40–D-43 , D-43–D-45 , 347.e3 , 162.e4 See also Desktop server RISCs , Embedded RISCs group types, D-3–D-4 instruction set lineage, D-44 f Reduction, 511 Redundant arrays inexpensive disks (RAID), 458.e1–458.e2 history, 458.e6–458.e7 RAID 0, 458.e3 RAID 1, 458.e4 RAID 2, 458.e4 RAID 3, 458.e4 RAID 4, 458.e4–458.e5 1627RAID 5, 458.e5–458.e6 RAID 6, 458.e6 spread of, 458.e5 summary, 458.e6–458.e7 use statistics, 458.e6 f Reference bit, 428b References absolute, 127 types, 144.e25 Register addressing, 118f Register allocation, 144.e10–144.e12 Register files, A-49 b, A-53–A-55 behavioral Verilog, A-56 defined, 244, A-49 b, A-53 single, 249, 249 two read ports implementation, A-54 f two read ports/one write port, A-54 f write port implementation, A-55 f Register-memory architecture, 162.e2 Registers, 148, 149–151 architectural, 316, 337–338 base, 69 clock cycle time and, 67 compiling C assignment with, 67b–68b defined, 67 destination, 254 floating-point, 212b left half, 280 1628number specification, 244 page table, 424–425 pipeline, 297–298 , 297–298 , 298f, 302 primitives, 67 renaming, 327 right half, 280 RISC-V conventions, 255f spilling, 71 Status, 316 temporary, 68, 100 variables, 68 Relative performance, 31b Relative power, 41b–42b Reliability, 410–411 Remainder, defined, 184 Reorder buffers, 334b Replication, 456 Requested word first, 384 Request-level parallelism, 524 Reservation stations buffering operands in, 329 defined, 329 Response time, 30b Restartable instructions, 440–441 Return address, 99 R-format ALU operations, 245f Ripple carry 1629adder, A-29 carry lookahead speed versus , A-45 b RISC-V, 62, 85–87 architecture, 190f arithmetic instructions, 63 arithmetic/logical instructions in, D-21 f, D-23 f assembly instruction, mapping, 81b–82b common extensions to, D-20–D-25 compiling C assignment statements into, 65b compiling complex C assignment into, 66b control instructions in, D-21 f control registers, 439b control unit, C-10 data transfer instructions in, D-20 f, D-22 f divide in, 189 exceptions in, 316–317 fields, 83–89 floating-point instructions, 206–212 floating-point instructions in, D-22 f instruction classes, 157f instruction encoding, 85f, 119f instruction formats, 120, 146f instruction set, 62, 159–160 , 226, 236, D-9–D-16 machine language, 87–89 memory addresses, 70f memory allocation program data, 106f multiply in, 183 Pseudo, 226f 1630register conventions, 107f static multiple issue with, 324–328 Roofline model, 534–535 , 536f, 537 ceilings, 538f computational roofline, 535, 537 illustrated, 534f Opteron generations, 535 overlapping areas shaded, 539f peak floating-point performance, 538f peak memory performance, 542f two kernels, 539f Rotational delay, See Rotational latency Rotational latency, 375 Rounding, 212–213 accurate, 212–213 bits, 214 guard digits, 213b IEEE 754 modes, 213–214 Row-major order, 211b–212 b, 405 R-type, defined, 87b R-type instructions, 248b–249 b datapath for, 256–259 datapath operation for, 258f RV32, 73b RV64, 73b Saturation, 177b 1631sb (store byte), 64f SB-type instruction format, 115 sc.d (store conditional), 64f SCALAPAK, 223–224 Scaling strong, 497 weak, 497 Scientific notation adding numbers in, 199 defined, 191 reals, 191 sd (store doubleword), 64f Search engines, 4 Secondary memory, 23 Sectors, 373–374 Seek, 374 Segmentation, 423b Selector values, A-10 Semiconductors, 25–26 Send message routine, 521 Sensitivity list, A-23–A-24 Sequencers explicit, C-32 implementing next-state function with, C-22–C-28 Sequential logic, A-4 Servers, 458.e6 See also Desktop server RISCs cost capability, 5 1632Service accomplishment, 410–411 Service interruption, 410 Set-associative caches, 395 See also Caches address portions, 399f block replacement strategies, 445 choice of, 444 four-way, 396f, 399 memory-block location, 395f misses, 397b–398 b n-way, 395 two-way, 396f Set less instruction (slt), A-31 Setup time, A-52–A-53 , A-53 f sh (store halfword), 64f Shaders defined, B-14 floating-point arithmetic, B-14 graphics, B-14–B-15 pixel example, B-15–B-17 Shading languages, B-14 Shadowing, 458.e4 Shared memory See also Memory low-latency memory, B-21 caching in, B-58–B-60 CUDA, B-58 N-body and, B-66 f 1633per-CTA, B-39 SRAM banks, B-40 Shared memory multiprocessors (SMP), 509–514 defined, 492–493 , 509–510 single physical address space, 509 synchronization, 510–513 Shift left logical immediate (slli), 90 Shift right arithmetic (srai), 90 Shift right logical immediate (srli), 90 Sign magnitude, 192 Sign bit, 77 Sign extension, 246 defined, 78b shortcut, 78–79 Signals asserted, 242, A-4 control, 242, 255, 255, 255 deasserted, 242, A-4 Signed division, 187–188 Signed multiplication, 182 Signed numbers, 74–81 sign magnitude, 75 treating unsigned, 96 Significands, 193–194 addition, 198–199 multiplication, 201–205 Silicon, 25–26 key hardware technology, 53 1634crystal ingot, 26 defined, 25–26 wafers, 26 Silicon crystal ingot, 26 SIMD (Single Instruction Multiple Data), 498–499 , 550–551 computers, 553.e1–553.e3 data vector, B-35 extensions, 553.e3 loops and, 553.e2 massively parallel multiprocessors, 553.e1 small-scale, 553.e3 vector architecture, 500–502 x86, 500 SIMMs (single inline memory modules), 473.e5 f, 473.e4 Simple programmable logic devices (SPLDs), A-77 Simplicity, 65–67 Simultaneous multithreading (SMT), 507 support, 507f thread-level parallelism, 507 unused issue slots, 507f Single error correcting/Double error correcting (SEC/DEC ), 412–416 Single instruction single data (SISD), 500, 504–506 Single precision See also Double precision binary representation, 196b defined, 193 Single-clock-cycle pipeline diagrams, 287–290 illustrated, 289f 1635Single-cycle datapaths See also Datapaths illustrated, 277f instruction execution, 278f Single-cycle implementation control function for, 261 nonpipelined execution versus pipelined execution, 266f non-use of, 261–262 penalty, 262 pipelined performance versus , 264b–265 b Single-instruction multiple-thread (SIMT), B-27–B-29 overhead, B-35 multithreaded warp scheduling, B-28 f processor architecture, B-28–B-29 warp execution divergence, B-29–B-30 Single-program multiple data (SPMD), B-22 sll (shift left logical), 64f slli (shift left logical immediate), 64f Smalltalk-80, 162.e7 , 162.e7 Smart phones, 7 Snooping protocol, 456–458 Snoopy cache coherence, 459.e16 Software optimization via blocking, 405–409 Software layers, 13f multiprocessor, 492 parallel, 493 1636as service, 7, 524, 550 systems, 13 Sort algorithms, 141f Sort procedure, 135–140 See also Procedures code body, 136–138 full procedure, 139–140 passing parameters in, 138 preserving registers in, 138–139 procedure call, 138 register allocation for, 136 Sorting performance, B-54–B-55 Space allocation heap, 105–108 stack, 104–105 SPARC annulling branch, D-23–D-25 CASA, D-31–D-32 conditional branches, D-10–D-16 fast traps, D-30 floating-point operations, D-31 instructions, D-29–D-32 least significant bits, D-31 f multiple precision floating-point results, D-32 nonfaulting loads, D-32 overlapping integer operations, D-31 quadruple precision floating-point arithmetic, D-36 register windows, D-29–D-30 1637support LISP Smalltalk, D-30 Sparse matrices, B-55–B-58 Sparse Matrix-Vector multiply (SpMV), B-55 , B-57 f, B-58 CUDA version, B-57 f serial code, B-57 f shared memory version, B-59 f Spatial locality, 366 large block exploitation of, 383 tendency, 369 SPEC, 54.e10–54.e11 CPU benchmark, 46–48 power benchmark, 48–49 SPEC89, 54.e10 SPEC92, 54.e11 SPEC95, 54.e11 SPEC2000, 54.e11 SPEC2006, 54.e11 SPECrate, 530 SPECratio, 47–48 Special function units (SFUs), B-35 , B-50 defined, B-42–B-43 Speculation, 323–324 hardware-based, 331–333 implementation, 323 performance and, 323, 324 problems, 323 recovery mechanism, 323 Speed-up challenge 1638balancing load, 497b–498 b bigger problem, 496b–497 b Spilling registers, 71b–72b, 99 Split algorithm, 544 Split caches, 389b sra (shift right arithmetic), 64f srai (shift right arithmetic immediate), 64f srl (shift right logical), 64f srli (shift right logical immediate), 64f Stack architectures, 162.e3–162.e4 Stack pointers adjustment, 102–104 defined, 99 values, 101f Stacks allocating space on, 104–105 arguments, 99 defined, 99 pop, 99 push, 99, 102–104 Stalls, 270 avoiding code reordering, 270b–271 b behavioral Verilog detection, 345.e3–345.e8 data hazards and, 303–307 illustrations, 345.e20 insertion pipeline, 305f load-use, 308 memory, 391 1639as solution control hazard, 271 write-back scheme, 392 write buffer, 391 Standby spares, 458.e7 State 2-bit prediction scheme, 312 assignment, A-69 , C-27 bits, C-8–C-10 exception, saving/restoring, 440 logic components, 241 specification of, 424b State elements clock and, 241 combinational logic and, 241 defined, 240–241 , A-47 inputs, 241 register file, A-49 b storing/accessing instructions, 244f Static branch prediction, 324 Static data segment, 105 Static multiple-issue processors, 322, 324–328 See also Multiple issue control hazards and, 324–325 instruction sets, 324 RISC-V ISA, 324–328 Static random access memories (SRAMs), 370, 371, A-57–A-66 array organization, A-61 f 1640basic structure, A-60 f defined, 19–22 , A-57 fixed access time, A-57 large, A-58 read/write initiation, A-58 synchronous (SSRAMs), A-59 three-state buffers, A-58 , A-59 f Static variables, 104b Steady-state prediction, 312 Sticky bits, 214 Store buffers, 334b Store byte, 109 Store-conditional doubleword, 122–123 Store doubleword, 70–71 Store instructions See also Load instructions access, B-41 base register, 254 compiling with, 71 conditional, 122–123 defined, 71b EX stage, 284f ID stage, 281f stage, 281f instruction dependency, 302b MEM stage, 283f unit implementing, 247f WB stage, 283f 1641Store word, 113b Stored program concept, 63 computer principle, 88b illustrated, 88f principles, 159–160 Strcpy procedure, 110b–111 b See also Procedures leaf procedure, 111 pointers, 111 Stream benchmark, 540b Streaming multiprocessor (SM), B-13 Streaming processors, B-34 , B-49–B-50 array (SPA), B-41 , B-46 Streaming SIMD Extension 2 (SSE2) floating-point architecture, 217 Streaming SIMD Extensions (SSE) advanced vector extensions x86, 217 Stretch computer, 347.e1 f, 347.e1 Strings defined, 109–111 Java, 111–113 representation, 108f Strip mining, 502b Striping, 458.e3 Strong scaling, 497 Structural hazards, 267, 284 sub (subtract), 64f Subnormals, 216 Subtraction, 174–177 See also Arithmetic 1642binary, 174b–175 b floating-point, 206 negative number, 176 overflow, 176 Subword parallelism, 216–217 , 344f, D-17 matrix multiply, 218–222 Sum products, A-11 , A-12 b Supercomputers, 347.e2 defined, 5 SuperH, D-15 , D-39–D-40 Superscalars defined, 328, 347.e3–347.e4 dynamic pipeline scheduling, 328–329 multithreading options, 494 Supervisor Exception Cause Register (SCAUSE), 316 Supervisor exception program counter (SEPC), 316, 364, 439 address capture, 319–321 defined, 317–319 restart determination, 316 Supervisor exception return (sret), 437 Supervisor Page Table Base Register (SPTBR), 427f Supervisor Trap Vector (STVEC), 321b Surfaces, B-41 sw (store word), 64f Swap procedure, 134 See also Procedures body code, 134–135 full, 135, 139–140 1643register allocation, 134 Swap space, 426 Symbol tables, 126 Synchronization, 121–124 , 544 barrier, B-18 , B-20 , B-34 defined, 510–513 lock, 121 overhead, reducing, 44–45 unlock, 121 Synchronizers flip-flop, A-75 f defined, A-75 failure, A-75–A-76 Synchronous DRAM (SRAM), 371, A-59 , A-64 Synchronous SRAM (SSRAM), A-59 Synchronous system, A-47–A-48 Syntax tree, 144.e2 System calls, defined, 364 Systems software, 13 SystemVerilog cache controller, 459.e1–459.e4 cache data tag modules, 459.e16 FSM, 459.e6 f simple cache block diagram, 459.e3 f type declarations, 459.e1 f Tablets, 7f 1644Tags defined, 376 locating block, 399 page tables and, 426 size of, 401b–402 b Tail call, 107 Task identifiers, 438 Task parallelism, B-24 Task-level parallelism, 492 Tebibyte (TiB), 5 Telsa PTX ISA, B-31 arithmetic instructions, B-33 barrier synchronization, B-34 GPU thread instructions, B-32 f memory access instructions, 208 Temporal locality, 366 tendency, 369 Temporary registers, 68, 100 Terabyte (TB), 6f defined, 5 Texture memory, B-40 Texture/processor cluster (TPC), B-47 TFLOPS multiprocessor, 553.e4–553.e5 , 553.e5 Thrashing, 442 Thread blocks, 518f creation, B-23 defined, B-19 managing, B-30 1645memory sharing, B-20–B-21 synchronization, B-20–B-21 Thread parallelism, B-22 Threads creation, B-23 CUDA, B-36 ISA, B-31–B-34 managing, B-30 memory latencies and, B-74 b multiple, per body, B-68–B-72 warps, B-27–B-28 Three Cs model, 447b Three-state buffers, A-58 , A-59 f Throughput defined, 29–30 multiple issue and, 322 pipelining and, 264 Thumb, D-15 f, D-38–D-39 Timing asynchronous inputs, A-75–A-76 level-sensitive, A-74–A-75 methodologies, A-71–A-77 two-phase, A-74 f TLB misses, 431 See also Translation-lookaside buffer (TLB) handling, 439–441 occurrence, 439 problem, 442 1646Tomasulo’s algorithm, 347.e2 Touchscreen, 19 Tournament branch predicators, 313–314 Tracks, 373–374 Transfer time, 375 Transistors, 25 Translation-lookaside buffer (TLB), 430–432 , D-26–D-27 , 473.e5 See also TLB misses associativities, 432 illustrated, 431f integration, 435 Intrinsity FastMATH, 432–435 typical values, 432 Transmit driver NIC hardware time versus receive driver NIC hardware time, 529.e7 f Tree-based parallel scan, B-62 f Truth tables, A-5 ALU control lines, C-5f control bits, 253 datapath control outputs, C-17 f datapath control signals, C-14 f defined, 253 example, A-5b next-state output bits, C-15 f PLA implementation, A-13 Two’s complement representation, 76 advantage, 77 negation shortcut, 78b–79b 1647rule, 80b sign extension shortcut, 79b–80b Two-level logic, A-11–A-14 Two-phase clocking, A-74 , A-74 f TX-2 computer, 553.e3 U Unconditional branches, 93 Underflow, 192 Unicode alphabets, 111 defined, 111 example alphabets, 112f Unified GPU architecture, B-10–B-11 illustrated, B-11 f processor array, B-11–B-12 Uniform memory access (UMA), 510, B-9 multiprocessors, 510 Units commit, 329, 334b control, 239–240 , 251–253 , C-4–C-8 , C-10 f, C-12–C-13 defined, 213–214 floating point, 213–214 hazard detection, 303, 306–307 load/store implementation, 247f special function (SFUs), B-35 , B-42–B-43 , B-50 UNIVAC I, 54.e4 f, 54.e3–54.e4 UNIX, 162.e7 , 473.e10 , 473.e13 , 473.e13 , 473.e14 1648AT&T, 473.e14 Berkeley version (BSD), 473.e14 genius, 473.e16 history, 473.e13 , 473.e14 Unlock synchronization, 121 Unsigned numbers, 74–81 Use latency defined, 325–327 one-instruction, 325–327 V Vacuum tubes, 25f Valid bit, 376–378 Variables C language, 104b programming language, 67 register, 67 static, 104b storage class, 104b type, 104b VAX architecture, 162.e3 , 473.e6 Vector lanes, 502 Vector processors, 499–506 See also Processors conventional code comparison, 501b–502 b instructions, 501 multimedia extensions and, 500–502 scalar versus , 502–503 1649Vectored interrupts, 316 Verilog behavioral definition RISC-V ALU, A-25 f behavioral definition bypassing, 345.e4 f behavioral definition stalls loads, 345.e6 f behavioral specification, A-21 , 345.e1–345.e3 behavioral specification multicycle MIPS design, 345.e12 f behavioral specification simulation, 345.e1–345.e3 behavioral specification stall detection, 345.e3–345.e8 behavioral specification synthesis, 345.e8–345.e13 blocking assignment, A-24 branch hazard logic implementation, 345.e8 combinational logic, A-23–A-26 datatypes, A-21–A-23 defined, A-20–A-21 forwarding implementation, 345.e3 modules, A-23 f multicycle MIPS datapath, 345.e14 f nonblocking assignment, A-24 operators, A-22–A-23 program structure, A-23 reg, A-21 , A-21 RISC-V ALU definition in, A-36–A-37 sensitivity list, A-23–A-24 sequential logic specification, A-55–A-57 structural specification, A-21 wire, A-21 , A-21 , A-22 Vertical microcode, C-32 1650Very large-scale integrated (VLSI) circuits, 25 Long Instruction Word (VLIW) defined, 324 first generation computers, 347.e4 processors, 324 VHDL, A-20–A-21 Video graphics array (VGA) controllers, B-3–B-4 Virtual addresses causing page faults, 440 defined, 420–421 mapping from, 420–421 size, 422–423 Virtual machine monitors (VMMs) defined, 416 implementing, 470b laissez-faire attitude, 470 page tables, 441b performance improvement, 419 requirements, 418 Virtual machines (VMs), 416–419 benefits, 416–417 illusion, 441b instruction set architecture support, 419 performance improvement, 419 protection improvement, 416–417 Virtual memory, 419–443 See also Pages address translation, 420–421 , 430–432 1651integration, 435–437 large virtual addresses, 428–429 mechanism, 442 motivations, 419–420 page faults, 420–421 , 426 protection implementation, 437–439 segmentation, 423b summary, 441–443 virtualization of, 441b writes, 430 Virtualizable hardware, 418 Virtually addressed caches, 436 Visual computing, B-3 Volatile memory, 22 W Wafers, 26 defects, 26–27 dies, 27, 27, 27, 28 yield, 27 Warehouse Scale Computers (WSCs), 7, 521–526 , 550 Warps, B-27–B-28 Weak scaling, 497 Wear levelling, 373 loops, 94b–95b Whirlwind, 473.e1 Wide area networks (WANs), 24 See also Networks 1652Wide immediate operands, 113–114 Words accessing, 68 defined, 67 double, 151 load, 69, 71 quad, 151 store, 71b Working set, 442 World Wide Web, 4 Worst-case delay, 262 Write buffers defined, 387 stalls, 383 write-back cache, 387 Write invalidate protocols, 456 Write serialization, 455–456 Write-back caches See also Caches advantages, 446 cache coherency protocol, 459.e4 complexity, 387 defined, 386, 446 stalls, 391 write buffers, 387 Write-back stage control line, 292f load instruction, 282 1653store instruction, 284 Writes complications, 386b–387 b expense, 442 handling, 385–387 memory hierarchy handling of, 333–334 schemes, 386 virtual memory, 429 write-back cache, 386, 387 write-through cache, 386, 387 Write-stall cycles, 391 Write-through caches See also Caches advantages, 446 defined, 385, 446 tag mismatch, 386 X x86, 146–155 Advanced Vector Extensions in, 217–218 brief history, 162.e5–162.e6 conclusion, 154–155 data addressing modes, 149–151 evolution, 96 first address specifier encoding, 155f instruction encoding, 153–154 instruction formats, 154f instruction set growth, 162f 1654instruction types, 152f integer operations, 151–152 registers, 149–151 SIMD in, 498–499 Streaming SIMD Extensions in, 217–218 typical instructions/functions, 154f typical operations, 153f unique, D-36–D-38 Xerox Alto computer, 54.e7–54.e9 XMM, 217 xor (exclusive or), 64f xori (exclusive immediate), 64f Yahoo! Cloud Serving Benchmark (YCSB), 532 Yield, 27 YMM, 218 Z Zettabyte, 6f 1655RISC-V Reference Data Card (“Green Card”) 165616571658目录 Title page 2 Table Contents 6 Praise Computer Organization Design: Hardware/Software Interface16 Copyright 19 Dedication 22 Acknowledgments 23 Preface 24 Book 24 Book 25 RISC-V Edition? 26 Changes Fifth Edition 28 Instructor Support 32 Concluding Remarks 32 Acknowledgments 33 1. Computer Abstractions Technology 39 Abstract 39 1.1 Introduction 40 1.2 Eight Great Ideas Computer Architecture 52 1.3 Program 58 1.4 Covers 67 1.5 Technologies Building Processors Memory 84 1.6 Performance 91 1.7 Power Wall 107 1.8 Sea Change: Switch Uniprocessors Multiprocessors111 1.9 Real Stuff: Benchmarking Intel Core i7 120 1.10 Fallacies Pitfalls 124 1.11 Concluding Remarks 129 Historical Perspective Reading 134 1.12 Historical Perspective Reading 134 16591.13 Exercises 155 2. Instructions: Language Computer 163 Abstract 163 2.1 Introduction 166 2.2 Operations Computer Hardware 169 2.3 Operands Computer Hardware 172 2.4 Signed Unsigned Numbers 184 2.5 Representing Instructions Computer 195 2.6 Logical Operations 206 2.7 Instructions Making Decisions 210 2.8 Supporting Procedures Computer Hardware 218 2.9 Communicating People 233 2.10 RISC-V Addressing Wide Immediates Addresses 239 2.11 Parallelism Instructions: Synchronization 249 2.12 Translating Starting Program 254 2.13 C Sort Example Put Together 270 2.14 Arrays versus Pointers 279 Advanced Material: Compiling C Interpreting Java 284 2.15 Advanced Material: Compiling C Interpreting Java 284 2.16 Real Stuff: MIPS Instructions 320 2.17 Real Stuff: x86 Instructions 323 2.18 Real Stuff: Rest RISC-V Instruction Set 336 2.19 Fallacies Pitfalls 338 2.20 Concluding Remarks 341 Historical Perspective Reading 345 2.22 Historical Perspective Reading 346 2.22 Exercises 358 3. Arithmetic Computers 368 Abstract 368 3.1 Introduction 370 3.2 Addition Subtraction 371 3.3 Multiplication 375 3.4 Division 385 3.5 Floating Point 398 3.6 Parallelism Computer Arithmetic: Subword Parallelism 434 16603.7 Real Stuff: Streaming SIMD Extensions Advanced Vector Extensions x86435 3.8 Going Faster: Subword Parallelism Matrix Multiply 437 3.9 Fallacies Pitfalls 442 3.10 Concluding Remarks 447 Historical Perspective Reading 451 Historical Perspective Reading 451 3.12 Exercises 465 4. Processor 472 Abstract 472 4.1 Introduction 474 4.2 Logic Design Conventions 481 4.3 Building Datapath 486 4.4 Simple Implementation Scheme 500 4.5 Overview Pipelining 518 4.6 Pipelined Datapath Control 543 4.7 Data Hazards: Forwarding versus Stalling 569 4.8 Control Hazards 588 4.9 Exceptions 602 4.10 Parallelism via Instructions 612 4.11 Real Stuff: ARM Cortex-A53 Intel Core i7 Pipelines637 4.12 Going Faster: Instruction-Level Parallelism Matrix Multiply650 Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline Pipelining Illustrations655 4.13 Advanced Topic: Introduction Digital Design Using Hardware Design Language Describe Model Pipeline Pipelining Illustrations656 4.14 Fallacies Pitfalls 702 4.15 Concluding Remarks 704 Historical Perspective Reading 707 4.16 Historical Perspective Reading 707 4.17 Exercises 715 16615. Large Fast: Exploiting Memory Hierarchy 734 Abstract 734 5.1 Introduction 737 5.2 Memory Technologies 745 5.3 Basics Caches 754 5.4 Measuring Improving Cache Performance 776 5.5 Dependable Memory Hierarchy 804 5.6 Virtual Machines 813 5.7 Virtual Memory 818 5.8 Common Framework Memory Hierarchy 857 5.9 Using Finite-State Machine Control Simple Cache 867 5.10 Parallelism Memory Hierarchy: Cache Coherence 875 Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks880 5.11 Parallelism Memory Hierarchy: Redundant Arrays Inexpensive Disks881 Advanced Material: Implementing Cache Controllers 892 5.12 Advanced Material: Implementing Cache Controllers 892 5.13 Real Stuff: ARM Cortex-A53 Intel Core i7 Memory Hierarchies912 5.14 Real Stuff: Rest RISC-V System Special Instructions921 5.15 Going Faster: Cache Blocking Matrix Multiply 922 5.16 Fallacies Pitfalls 925 5.17 Concluding Remarks 932 Historical Perspective Reading 934 5.18 Historical Perspective Reading 935 5.19 Exercises 959 6. Parallel Processors Client Cloud 979 Abstract 979 6.1 Introduction 981 6.2 Difficulty Creating Parallel Processing Programs 986 6.3 SISD, MIMD, SIMD, SPMD, Vector 993 6.4 Hardware Multithreading 1007 6.5 Multicore Shared Memory Multiprocessors 1016 16626.6 Introduction Graphics Processing Units 1022 6.7 Clusters, Warehouse Scale Computers, Message- Passing Multiprocessors1033 6.8 Introduction Multiprocessor Network Topologies 1043 Communicating Outside World: Cluster Networking 1049 6.9 Communicating Outside World: Cluster Networking 1050 6.10 Multiprocessor Benchmarks Performance Models 1064 6.11 Real Stuff: Benchmarking Rooflines Intel Core i7 960 NVIDIA Tesla GPU1082 6.12 Going Faster: Multiple Processors Matrix Multiply 1089 6.13 Fallacies Pitfalls 1095 6.14 Concluding Remarks 1098 Historical Perspective Reading 1102 6.15 Historical Perspective Reading 1103 6.16 Exercises 1119 Appendix 1135 Appendix A. Basics Logic Design 1136 A.1 Introduction 1136 A.2 Gates, Truth Tables, Logic Equations 1138 A.3 Combinational Logic 1144 A.4 Using Hardware Description Language 1166 A.5 Constructing Basic Arithmetic Logic Unit 1175 A.6 Faster Addition: Carry Lookahead 1196 A.7 Clocks 1210 A.8 Memory Elements: Flip-Flops, Latches, Registers 1213 A.9 Memory Elements: SRAMs DRAMs 1227 A.10 Finite-State Machines 1240 A.11 Timing Methodologies 1249 A.12 Field Programmable Devices 1256 A.13 Concluding Remarks 1259 A.14 Exercises 1260 Appendix B. Graphics Computing GPUs 1271 B.1 Introduction 1271 B.2 GPU System Architectures 1277 B.3 Programming GPUs 1287 1663B.4 Multithreaded Multiprocessor Architecture 1308 B.5 Parallel Memory System 1326 B.6 Floating-point Arithmetic 1332 B.7 Real Stuff: NVIDIA GeForce 8800 1340 B.8 Real Stuff: Mapping Applications GPUs 1352 B.9 Fallacies Pitfalls 1376 B.10 Concluding Remarks 1381 B.11 Historical Perspective Reading 1382 Reading 1389 Appendix C. Mapping Control Hardware 1393 C.1 Introduction 1393 C.2 Implementing Combinational Control Units 1394 C.3 Implementing Finite-State Machine Control 1400 C.4 Implementing Next-State Function Sequencer 1417 C.5 Translating Microprogram Hardware 1426 C.6 Concluding Remarks 1431 C.7 Exercises 1432 Appendix D. Survey RISC Architectures Desktop, Server, Embedded Computers1434 D.1 Introduction 1435 D.2 Addressing Modes Instruction Formats 1437 D.3 Instructions: MIPS Core Subset 1444 D.4 Instructions: Multimedia Extensions Desktop/Server RISCs1453 D.5 Instructions: Digital Signal-Processing Extensions Embedded RISCs1455 D.6 Instructions: Common Extensions MIPS Core 1456 D.7 Instructions Unique MIPS-64 1462 D.8 Instructions Unique Alpha 1466 D.9 Instructions Unique SPARC v9 1468 D.10 Instructions Unique PowerPC 1472 D.11 Instructions Unique PA-RISC 2.0 1473 D.12 Instructions Unique ARM 1477 D.13 Instructions Unique Thumb 1478 D.14 Instructions Unique SuperH 1480 1664D.15 Instructions Unique M32R 1481 D.16 Instructions Unique MIPS-16 1482 D.17 Concluding Remarks 1484 Reading 1488 Answers Check 1490 Chapter 1 1490 Chapter 2 1490 Chapter 3 1491 Chapter 4 1491 Chapter 5 1492 Chapter 6 1492 Glossary 1494 Reading 1524 Index 1541 RISC-V Reference Data Card (“Green Card”) 1656 1665