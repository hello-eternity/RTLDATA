John Paul Shen John Paul Shen Director Intel's Microarchitecture Research Lab (MRL), providing leadership two- dozen highly skilled researchers lo cated Santa Clara, C A; Hillsboro, OR; ustin. TX. MRL responsible de- veloping innovative microarchitecture techniques potentially used future microproce ssor products Intel. MRL researchers collaborate closely microarchi- tects product teams joint advanced-development efforts. MRL frequently hosts visiting faculty Ph.D. interns conducts joint research projects academic research groups. Prior joining Intel 2000, John professor electrical computer engineering department Carnegie Mellon University, headed CMU Microarchitecture Research Team (CMuART). supervised total 16 Ph.D. students years CMU. Seven currently Intel, five fac ulty positions academia. multiple teaching awards CMU. NSF Presidential Young Inv estigator. IEEE Fellow served program committees ISCA, MICRO, HPCA, ASPLOS, PACT, ICCD, ITC, FTCS. published 100 research papers diverse areas, including fault- tolerant computing, built-in self-test, process defect fault analysis, concurrent error detection, application-specific processors, performan ce evaluation, compila- tion instruction-level parallelism, value loca lity prediction, analytical mod- eling superscalar processors, systematic microarchitecture test generation, per- formance simulator validation, precomputation-based prefetching, database workload analysis, user-le vel helper threads. John received M.S. Ph.D. degrees University Southern California, B.S. degree University Michigan, electrical engineering. attended Kimball High School Royal Oak, Michigan. happily married three daughters. family enjoys camping, road trips, reading Lord Rings. (continued back inside cover) Modern Processor Design Fundamentals Superscalar Processors John Paul Shen Intel Corporation Mikko H. Lipasti University Wisconsin Tata McGraw-Hill Publishing Company Limited NEW DELHI McGraw-Hill Offices New Delhi New York St Louis San Francisco Auckland Bogota Caracas Kuala Lumpur Lisbon London Madrid Mexico City Milan Montreal San Juan Santiago Singapore Sydney Tokyo TorontoOur parents: Paul Sue Shen Tarja Simo Lipasti spouses: Amy C. Shen Erica Ann Lipasti children: Priscilla S. Shen, Rachael S. Shen, Valentia C. Shen Emma Kristiina Lipasti Elias Joel Lipasti Tata Mc Graw-Hiil MODERN PROCESSOR DESIGN: FUNDAMENTALS SUPERSCALAR PROCESSORS Copyright © 2005 McGraw-Hill Companies, Inc., rights reserved. part publication may reproduced distributed form means, stored data base retrieval sy stem, without prior written consent McGraw-Hill Companies, Inc., including, limited to, network electronic storage tr ansmission, broadcast distance learning ancillaries, including electronic p rint com ponents, may available customers outside United States Tata McGraw-Hill Edition RZXQCRBIRQQDD Reprinted India arrangement McGraw-Hill Compa nies, Inc., New York Sales territories: India, Pakistan, Nepal, Bangladesh, Sri Lanka Bhutan ISBN 0-07-059033-8 Published Tata McGraw-Hill Publ ishing Company Limited, 7 West Patel Nagar, New Delhi 110 008, printed Shivam Printers, Delhi 110 032 McGraw-Hill Companies Table Contents Table Contents Additional Resources Preface 1 Processor Design 1.1 Evolution Microprocessors 1.2 Instruction Set Processor Design 1.2.1 Digital Systems Design 1.2.2 Architecture, Implementation, Realization 1.2.3 Instruction Set Architecture 1.2.4 Dynamic-Static Interface 1.3 Principles Processor Performance 1.3.1 Processor Performance Equation 1.3.2 Processor Performance Optimizations 1.3.3 Performance Evaluation Method 1.4 Instruction-Level Parallel Processing 1.4.1 Scalar Superscalar 1.4.2 Limits Instruction-Level Parallelism 1.4.3 Machines Instruction-Level Parallelism 1.5 Summary 2 Pipelined Processors 2.1 Pipelining Fundamentals 2.1.1 Pipelined Design 2.1.2 Arithmetic Pipeline Example 2.1.3 Pipelining Idealism 2.1.4 Instruction Pipelining 2.2 Pipelined Processor Design 2.2.1 Balancing Pipeline Stages 2.2.2 Unifyin g Instruction Types 2.2.3 Minimizing Pipeline Stalls 2.2.4 Commercial Pip elined Processors 2.3 Deeply Pipelined Processors 2.4 Summary 3 Memory I/O Systems 3.1 Introduction 3.2 Computer System Overview 3.3 Key Concepts: Latency BandwidthMODERN PROCESSOR DESIGN 3.4 Memory Hierarchy 110 3.4.1 Components Modem Memory Hierarchy 111 3.4.2 Temporal Spatial Locality 113 3.4.3 Caching Cache Memories 115 3.4.4 Main Memory 127 3.5 Virtua l Memory Systems 136 3.5.1 Demand Paging 138 3.5.2 Memory Protection 141 3.5.3 Page Table Architectures 142 3.6 Memory Hierarchy Implementation 145 3.7 Input/Output Systems 153 3.7.1 Types I/O Devices 154 3.7.2 Computer System Busses 161 3.7.3 Communication I/O Devices 165 3.7.4 Interaction I/O Devices Memory Hi erarchy 168 3.8 Summary 170 4 Superscalar Organization 177 4.1 Limitations Scalar Pipelines 178 4.1.1 Upper Bound Scalar Pipeline Throughput 178 4.1.2 Inefficient Unification Single Pipeline 179 4.1.3 Performance Lost Due Rigid Pipeline 179 4.2 Scalar Superscalar Pipelines 181 4.2.1 Parallel Pipelines 1814.2.2 Diversified Pipelines 184 4.2.3 Dynamic Pipelines 186 4.3 Superscalar Pipeline Overview 190 4.3.1 Instruction Fetching 191 4.3.2 Instruction Decoding 195 4.3.3 Instruction Dispatching 199 4.3.4 Instruction Execution 203 4.3.5 Instruction Complet ion Retiring 206 4.4 Summary 209 5 Superscalar Techniques 217 5.1 Instruction Flow Techniques 218 5.1.1 Program Contro l Flow Control Dependences 218 5.1.2 Performance Degradation Due Branches 219 5.1.3 Branc h Prediction Techniques 223 5.1.4 Branch Misprediction Recovery 228 5.1.5 Advanced Branch Prediction Techniques 231 5.1.6 Instructio n Flow Techniques 236 5.2 Register Data Flow Techniques 237 5.2.1 Register Reuse False Data Dependences 237 5.2.2 Register Renaming Techniques 239 5.2.3 True Data Dependences Data Flow Limit 244TABLE CONTENTS 5.2.4 Classic Tomasulo Algorithm 246 5.2.5 Dynamic Execution Core 254 5.2.6 Reservation Stations R eorder Buffer 256 5.2.7 Dynamic Instruction Scheduler 260 5.2.8 Register Dat Flow Techniques 261 5.3 Memory Data Flow Techniques 262 5.3.1 Memory Accessing Instructions 263 5.3.2 Ordering Memory Ac cesses 266 5.3.3 Load Bypassing Load Forwarding 267 5.3.4 Memory Data Flow Techniques 273 5.4 Summary 279 6 PowerPC 620 301 6.1 Introduction 302 6.2 Experimental Framework 305 6.3 Instruction Fetching 307 6.3.1 Branch Prediction 307 6.3.2 Fetching Speculation 309 6.4 Instruction Dispatching 311 6.4.1 Instruction Buffer 311 6.4.2 Dispatch Stalls 311 6.4.3 Dispatch Effectiveness 313 6.5 Instruction Execution 316 6.5.1 Issue Stalls 3166.5.2 Execution Parallelism 317 6.5.3 Execution Latency 317 6.6 Instruction Completion 318 6.6.1 Completion Parallelism 318 6.6.2 Cache Effects 318 6.7 Conclusions Observations 320 6.8 Bridging IBM POWER3 POWER4 > 322 6.9 Summary 324 7 Intel's P6 Microarchitecture 329 7.1 Introduction 330 7.1.1 Basics P6 Micro architecture 332 7.2 Pipelining 334 7.2.1 In-Order Front-End Pipeline 334 7.2.2 Out-of-Order Core Pipeline 336 7.2.3 Retirement Pipeline 337 7.3 In-Order Front End 338 7.3.1 Instruction Cache ITLB 3387.3.2 Branch Prediction 341 7.3.3 Instruction Decoder 343 7.3.4 Register Alias able 346 7.3.5 Allocator 353Vi MODERN PROCESSOR DESIGN 7.4 Out-of-Order Core 355 7.4.1 Reservation Station 355 7.5 Retirement 357 7.5.1 Reorder Buffer 357 7.6 Memory Subsystem 361 7.6.1 Memory Access Ordering 362 7.6.2 Load Memory Operations 363 7.6.3 Basic Store Memory Operations 363 7.6.4 Deferring Memory Operations 363 7.6.5 Page Faults 364 7.7 Summary 364 7.8 Acknowledgments 365 Surrey Superscalar Processors 369 8.1 Development Superscalar Processors 369 8.1.1 Early Adva nces Uniprocessor Parallelism: IBM Stretch 369 8.1.2 First Superscalar Design: IBM Advanced Computer System 372 8.1.3 Instruction-Level Parallelism Studies 377 8.1.4 By-Products DAE: First Multiple-Decoding Im plementations 378 8.1.5 IBM Cheetah, Panther, America 380 8.1.6 Decoupled Microarchitectures 380 8.1.7 Efforts 1980s 382 8.1.8 Wide Acceptance Superscalar 382 8.2 Classification Recent Designs 384 8.2.1 RISC CISC Retrofits 384 8.2.2 Speed Demons: Emphasis Clock Cycle Time 386 8.2.3 Brainiacs: Emphasis IPC 386 8.3 Processor Descriptions 387 8.3.1 Compaq/DEC Alpha 387 8.3.2 Hewlett-Packard PA-RISC Version 1.0 392 8.3.3 Hewlett-Packard PA-RISC Version 2.0 395 8.3.4 IBM POWER 397 8.3.5 Intel i960 402 8.3.6 Intel IA32—Native Approaches 405 8.3.7 Intel IA32—Decoupled Approaches 409 8.3.8 x86-64 417 8.3.9 MIPS 417 8.3.10 Motorola 422 8.3.11 PowerPC—32-bit Architecture 424 8.3.12 PowerPC—64-bit Architecture 429 8.3.13 PowerPC-AS 431 8.3.14 SPARC Version 8 432 8.345 SPARC Version 9 435TABLE CONTENTS 8.4 Verification Superscalar Processors 439 8.5 Acknowledgments 440 9 Advanced Instruction Flow Techniques 453 9.1 Introduction 453 9.2 Static Branch Prediction Techniques 454 9.2.1 Single-Direction Prediction 4559.2.2 Backwards Taken/Forwards Not-Taken 456 9.2.3 Ball/ Larus Heuristics 456 9.2.4 Profiling 457 9.3 Dynamic Branch Prediction Techniques 458 9.3.1 Basic Algorithms 4599.3.2 Interference-Reducing Predictors 472 9.3.3 Predicting wit h Alternative Contexts 482 9.4 Hybrid Branc h Predictors 491 9.4.1 Tournamen Predictor 491 9.4.2 Static Predictor Selection 4939.4.3 Branch Classification 4949.4.4 Multihybrid Predictor 495 9.4.5 Prediction Fusion 496 9.5 Instruction Flow Issues Techniques 497 9.5.1 Targe Prediction 497 9.5.2 Branch Confidence Prediction 5019.5.3 High-B andwidth Fetch Mechanisms 504 9.5.4 High-Fre quency Fetch Mechanisms 509 9.6 Summary 512 10 Advanced R egister Data Flow Techniques 519 10.1 Introduction 519 10.2 Value Locality Redundant Execution 523 10.2.1 Causes Value Locality 523 10.2.2 Quantifying Va lue Locality 525 10.3 Exploitin g Value Locality without Spe culation 527 10.3.1 Memoizatio n 527 10.3.2 Instruction Reuse 529 10.3.3 Basic Block Trace Reuse 533 10.3.4 Dat Flow Region Reuse 534 10.3.5 Conclu ding Remarks 535 10.4 Exploiting Value Localit Speculation 535 10.4.1 Weak Dependence Model 535 10.4.2 Value Prediction 536 10.4.3 Value Prediction Unit 537 10.4.4 Speculative Execution Using Predicted Values 542 10.4.5 Performance Value Prediction 55110.4.6 C oncluding Remarks 553 . 10.5 Summary 554Viii MODERN PROCESSOR DESIGN 11 Executing Multiple Threads 559 11.1 Introduction 559 11.2 Synchronizing Shared-Memory Threads 562 11.3 Introduction Multiprocessor Systems 565 11.3.1 Fully Shared Memory, Unit Latency, Lack Contention 566 11.3.2 Instantaneous Propagation Writes 567 11.3.3 Coherent Shared Memory 567 11.3.4 Implementing Cache Coherence 571 11.3.5 Multilevel Caches, Inclusion, Virtual Memory 574 11.3.6 Memory Consistency 576 11.3.7 Coherent Memory Interface 581 11.3.8 Concluding Remarks 583 11.4 Explicitly Multithreaded Processors 584 11.4.1 Chip Multiprocessors 584 11.4.2 Fine-Grained Multithreading 588 11.4.3 Coarse-Grained Multithreading 589 11.4.4 Simultaneous Multithreadin g 592 11.5 Implicitly Multithreaded Processors 600 11.5.1 Resolving Control Dependences 601 11.5.2 Resolving Register Data Dependences 605 11.5.3 Resolving Memory Data Dependences 607 11.5.4 Concluding Remarks 610 11.6 Executing Thread 610 11.6.1 Fault Detection 611 11.6.2 Prefetching 613 11.6.3 Branch Resolution 614 11.6.4 Concluding Remarks " 615 11.7 Summary 616 Index 623Additional Resources addition comprehensive coverage within book, number additional resources available Shen/Lipasti's MODERN PROCESSOR DESIGN book's website www.mhhe.coin/shen . ! ^H PW. p*Ur - Mfc*1 Website tatah ef Superscalar r SahaSftu! CaroMi" Mat** Unhw ilty Mtkk- Upaso, UnWtrelty ef Wlsceaski-Meelf walcai.^tatrwwabntafarMbdwnAn9M*vCW4..,A2003, ISBN -07-26296i-0. BMcMng raw beta edition John Carnegie W "Ion Untvan » ft. Intal Hikko Lipase Umwrtitt Wtscorwin-Madison. Dm ba_k tu <QI togeffierthenumarouirwcroartWtacturalttoV— lesfor h<rvistjng mora mstrucrxin-kva! parallelism (ILP) ad iy» batter processor parrormante nave proposed ano irnptamantad rsal mad—ie*. T*~t\m techniques, wall trie foundational prinaptes behind tham, art organized present-J w n dear framework a^-Mvs aasa comprehension. Tht» text intended advanced compUEar arcncaccura course court* tuparscelai processor design. written (aval appropriate sennr first year graduata leva! studertt, alto used proressii WaeraL.rap ^youtDa >(p)enib^5s«^rbr *>elplJresou/p^ Instructor Resources  Solutions Manual —A complete set solutions chapter-ending homework problems provided.  PowerPoint Slides —Two sets MS PowerPoint slides, Carnegie Mellon University University Wisconsin-Madison, down- loaded supplement lecture presentations.  Figures —A complete set figures book available eps format. f igures used create presentations.  Sample Homework Files —A set homework assignments answers Carnegi e Mellon University provided supplement assignments.  Sample Exams —A set e xams answers Carnegie Mellon Uni- versity also provided supplement exams.  Links www.simplescalar.com —We provide several links Simple- Scalar tool set, available free non-commercial academic use.Preface book emerged course Superscalar Processor Design, taught Carnegie Mellon University since 1995. Superscalar Processor Design mezzanine course targ eting seniors first-year graduate students. Quite aggressive juniors taken course spring semester jun- ior year. prerequisite course Introduction Compu ter Architecture course. objectives Super scalar Processor Design course include: (1) teach modem processor design skills microarchitecture level abstraction; (2) cover current microarchitecture techniques achieving high perfor mance via exploitation instruction-level parallelism (ILP); (3) impart insights hands-on expe rience effective design contemporary high-performance microprocessors mobile, desktop, serve r markets. addition covering contents book, course contains project component involves microarchitectural design future-generation superscalar microprocessor. decade 1990s many micro architectural techniques increas- ing clock frequency harvesting ILP achieve better processor perfor- mance proposed implemented real machines. book attempt codify large body knowledge systematic way. techn iques include deep pipelining, aggressive branch prediction, dyna mic register renaming, multiple instruction dispatching issuing, out-of-order execut ion, speculative load/store processing. Hundreds research papers published since early 1990s, many research ideas become reality commercial superscalar microprocessors. book, numerous techniques organized presented within clear framework f acilitates ease comprehension. foundational principles underlie plethora techniques highlighted. contents book would generally viewed graduate-level material, book intentionally written way would accessible undergraduate students. Significant effort spent making seemingly complex techniques appear quite straightforward appropriate abstrac- tion hiding details. priority convey clearly key concepts fundamental principles, giving enough details ensure understanding im- plementation issues withou massive dumping information quantitative data. hope body knowledge become widely possessed microarchitects processor designers B.S. M.S. students interests computer systems microprocessor design. brief summary chapters. Chapter 1: Processor Design chapter ntroduces art processor design, instruction set architecture (ISA) specification processor, microarchitecture imple- mentation processor. dynamic/static interface separates compile-timePREFACE software run-time hardware defined discussed. goal chapter revisit depth traditional issues regarding ISA design, erect proper framework understa nding modem processor design. Chapter 2: Pipelined Processors chapter focuses concept p ipelining, discusses instruction pipeline design, presents performa nce benefits pipelining. Pipelining usually in- troduced first computer architecture course. Pi pelining provide foundation modem superscalar echniques presented chapter fresh unique way. intentionally avoid massive dum ping bar charts graphs; instead, focus distil ling foundational principles instruction pipelining. Chapter 3: Memory I/O Systems chapter provides larger context remainder book including thorough grounding principles mechanisms modem memory I/O systems. Topics covered include memory h ierarchies, caching, main memory de- sign, virtual memory architecture, common input/output devices, processor-I/O in- teraction, bus design organization. Chapter 4: Superscalar Organization chapter troduces main concepts overall organization superscalar processors. provides "big picture" view reader leads smoothly detailed discussions next chapters specific superscalar techniques achiev- ing performance. chapter highlights key features superscalar processor organizations. Chapter 7 provides detailed survey features found real machines. Chapter 5: Superscalar Techniques chapter heart book presents major microarchitecture tech- niques designing contemporary superscalar processors achieving high perfor- mance. cl assifies pr esents specific techni ques enhancin g instruction flow, register data flow, memory data flow. chapter attempts orga nize plethora techniques systematic framework facilitates ease comprehension. Chapter 6: PowerPC 620 chapter presents detailed analysis PowerPC 620 microarchitecture uses case study examine many issues design tradeoffs intro- duced previous chapters. chapter contains extensive performance data aggressive out-of-order design. Chapter 7: Intel's P6 Microarchitecture case study chapter probably commercially successful contempo- rary superscalar microarchitecture. written Intel P6 design team led Bob Colwell presents depth P6 microarchitecture facilitated implemen- tation Pentium Pro, Pentium n, Pentium microprocessors. chapter offers readers opportunity peek mindset top-notch design team.xii MODERN PROCESSOR DESIGN Chapter 8: Survey Superscalar Processors chapter, compiled Prof. Mark Smotherman Clem son University, pro- vides historical chro nicle development superscalar machines survey existing superscalar microprocessors. chapter first completed 1998 continuously rev ised updated since then. contains fasci- nating information can't found elsewhere. Chapter 9: Advanced Instruction Flow Techniques chapter provides thorough overview issues related high-performance instruction fetching. topics covered include historical, currently used, pro- posed advanced future techniques branch prediction, well high-bandwidth high-frequency fetch architectures like trace caches. Though tech- niques yet adopted real machines, future designs likely incorpo- rate least form them. Chapter 10: Advanced Register Data Flow Techniques chapte r highlights emerging microarchitectural techniques increasing per- formance exploiting pr ogram characteristic value locality. program characteristic discovered recently, techniques ranging software memoization, instruction reuse, various forms value prediction described chapter. Though echniques yet adopted real machines, future designs likely incorporate least form them. Chapter 11 : Executing Multiple Threads chapter provides introduction thread-level parallelism (TLP), pro- vides basic introduction multiprocessing, cache coherence, hig h-perfor- mance implementations guarantee either sequentia l relaxed memory ordering across multiple processors. discusses single-chip techni ques like multi- threading on-chip multiprocessing also exploi thread-level parallelism. Finally, visits two emerging technologies—implicit multithreading preexecution—that attempt ex tract thread-level parallelism automatically single-threaded programs. summary, Chapters 1 5 cover f undamental concepts foundation- al techniques. C hapters 6 8 present case studies exte nsive survey actual commercial superscalar processors. Chapter 9 provides thorough overview advanced nstruction flow techniques, including recent developments ad- vanced branch predictors. Chapters 10 11 viewed advanced topics chapters highlight emerging techniques provide introduction multiprocessor systems. first edition book; earlier beta edition published 2002 intent collecting feedback help shape hone contents presen- tation first edition. course development book, large set homework exam problems created. subset problems included end chapter. Several problems suggest use thePREFAC Simplescalar simulation suite available Simplescalar website http ://www -simplescalar.com . companion website book contains additional support mate- rial instructor, including complete set lecture slides (www .mhhe.com/shen ). Acknowledgments Many people generousl contributed time, energy, support toward completion book. particular, grateful Bob Colwell, lead author Chapter 7, Intel's P6 Microarchitecture. also acknowledge coauthors, Dav e Papworth, Glenn Hinton, Mike Fetterman, Andy Glew, key members historic P6 team. chapter helps ground textbook practical, real-world considerations. also grateful Professor Mark Smotherman Clemson University, meticulously compiled au- thored Chapter 8, Survey Su perscalar Processors. chapter documents rich varied history superscalar processor design last 40 years. guest authors two chapters added certain rad iance textbook could possibly produced own. PowerPC 620 case study Chapter 6 based Trung Diep's Ph.D. thesis Carnegie Mellon University. Finally, thorough survey advanced instruction flow techniques Chapter 9 authored Gabriel Loh, lar gely based Ph.D. thesis Yale University. addition, want thank following professors detailed, in- sightful, thorough review original manuscript inputs reviews significantly imp roved first edition book.  David Andrews, University Arkansas  Angelos Bilas, University Toronto  Fred H. Carlin, University California Santa Barbara  Yinong Chen, Arizona State University  Lynn Choi, University California Irvine  Dan Connors. University Colorado  Karel Driesen, McGill University  Alan D. George, University Florida  Arthur Glaser, New Jersey Institute Technology  Rajiv Gupta, University Arizona  Vincent Hayward, McGill University  James Hoe, Carnegie Mellon University  Lizy Kurian John, University Texas Austin  Peter M. Kogge, University Notre Dame  Angkul Kongmunvattana, University Nevada Reno  Israel Koren, University Massachusetts Amherst  Ben Lee, Oregon State University  Francis Leung, Illinois Institute Technology  Walid Najjar, University California Riverside  Vojin G. Oklabdzija. University ofCali Davis  Soner Onder, Michigan Technological University  Parimal Patel, University Texas Antonio  Jih-Kwon Peir, University Florida  Gregory D. Peterson, University Tennessee  Amir Roth, University Pennsylvanii  Kevin Skadron, University Virginia  Mark Smothe rman, Clemson Universii  Miroslav N. Velev, Georgia Institute c Technology  Bin Wei, Rutgers University  Anthony S. Wojcik, Michigan State Uni  Ali Zaringhalam, Stevens Institute Technology  Xiaobo Zhou, University Colorado Colorado Springs civ MODERN PROCESSOR DESIGN book grew course Superscalar Processor Design Carnegie Mellon University. course taught CMU since 1995. Many teaching assis- tants course left indelible touch contents book. include Bryan Black, Scott Cape, Yuan Chou, Alex Dean, Trung Diep, John Faistl, Andrew Huang, Deepak L imaye, Chris Nelson, Chris Newbum, erek Noonburg, Kyle Oppenheim, Ryan Rakvic, Bob Rychlik. Hundreds students taken course CMU; many provided inputs also helped shape b ook. Since 2000, Professor James Hoe CMU taken course even further. indebted nurturing e xperienced CMU, hope book help perpetuate CMU's h istorical reputation producing best computer architects processor designers. draft version textbook als used University Wisconsin since 2000. problems end chapter actu- ally contributed students University Wisconsin. app reciate test driving book. John Paul Shen, Director, Microarchitecture Research, Intel Labs, Adjunct Professor, ECE Department, Carnegie Mellon University Mikko H. Lipasti, Assistant Professor, ECE Department, University Wisconsin June 2004 Soli Deo Gloria CHAPT 1 Processor Design CHAPTER OUTLINE 1.1 Evolution Microprocessors 1.2 Instruction Set Processor esigrv 13 Principles Processor P erformance 1.4" ' thstructtorf Level Paraltel Processing 15' Summary References Homework Problems Welcome contemporary microprocessor esign. relatively brief lifetime 30+ years, microprocessor tmdergone phenomenal advances. performance improved astounding rate doubling every 18 months. past three decades, microprocessors responsib le inspiring facilitating major innovations computer systems. Th ese innovations include embedded microcontrollers, personal computers, advanced workstations, handheld mobile devices, application file servers, web servers Internet, low-cost super- computers, large-scale co mputing clusters. C urrently 100 million microprocessors sold year mobile, desktop , server markets. Including embedded microprocessors microcontrollers, total number microprocessors shipped year well one billion units. Microprocessors instruction set processors (ISPs). ISP executes h v ^ structions predefined instruction set. microprocessor's functionality fully characterized instruction set capable executing. pro- grams run microprocessor encoded instruction set. pre- defined instruction set also called instruction set architecture (ISA). ISA serves interface software hardware, programs processors. terms processor design methodology, ISA specification 2 MODERN PROCESSOR DESIGN { design microprocessor ISP implementation design. forms engineering design, microprocessor design inherently creative process involves subtle tradeoffs requires good intuition clever insights. book focuses contemporary superscalar microprocessor design microarchitecture level. presents existing proposed microarchitectur e tech- niques systematic way imparts foundational principles insights, hope training new microarchitects contribute effective design future-generation microprocessors. 1.1 Evolution Microprocessors first microproc essor, Intel 4004, introduced 1971. 4004 4-bit proc essor consisting approximately 2300 transistors clock fre- quency 100 kilohertz (kHz). primary application building calculators. year 2001 marks thirtieth anniversary birth micropro- cessors. Hi gh-end microprocessors, containing 100 million transistors clock frequency reac hing 2 gigahertz (GHz), building blocks supercomputer systems powerful client server systems populate Internet. Within years microprocessors clocked close 10 GHz co ntain several hundred million transistors. three decades history microprocessors tell truly remarkable story technological advances computer industry; see Table 1.1. evo- lution microprocessor pretty much followed f amed Moore's law, observed Gordon Moore 1965, number evices inte- grated single piece silicon double roughly every 18 24 months. little 30 years, number transistors micropro cessor chip increased four orders magnitude. period , micropro- cessor performance increased five orders magnitude. past two decades, microprocessor performance doubling ever 18 months, increase factor 100 decade. phenome nal performance improvement unmatched industry. three decades existence, microprocessor played major roles critical advances th e computer industry. first decade, advent 4-bit microprocessor quickly led introduction Table 1.1 amazing decades evolution microprocessors 1970-1980 1980-1990 1990-2000 2000-2010 Transistor count 2K-100K 100K-1M 1M-100M 100M-2B Clock frequency 0.1-3 MHz 3-30 MHz 30MHz-lGHz 1-15 GHz Instructions/cycle 0.1 0.1-0.9 0.9-1.9 1.9-2.9PROCESSOR DESIGI 8-bit microprocessor. narrow bit-width microprocessors evolved self- contained microcontrollers produced huge volumes deployed numerous embedded applications ranging hing machines, elevators, jet engines. 8-bit microprocessor also became art new popular com- puting platform called personal computer (PC) ushered PC era computing. decade 1 980s witnessed ajor advances architecture microarchitecture 32-bit microprocessors. Instruction set design issues became focus academic industrial res earchers. Th e importance instruction set architecture facilitates efficient hardware implementation leverage compiler optimizations recognized. Instruction pipelin- ing fast cache memories became standard microarchitecture techniques. Pow- erful scientific engineering workstations based 32-bit microprocessors introduced. hese workstations turn became workhorses design subsequent generations even powerful microprocessors. decade 1990s, microprocessors became th e powerful popular form computers. clock frequency th e fastest micropro- cessors exceeded fastest supercomputers. Personal computers work- stations became ubiquitous essential tools productivity c ommunication. Extremely aggressive microarchitecture techniques devised achieve un- precedented levels microprocessor performance. Deeply pipelined machines capable achieving extremely high clock frequencies sustaining multiple instructions executed per c ycle became popular. Out-of-order execution instruc- tions aggre ssive branch prediction echniques introduced avoid reduce number pipeline stalls. end third decade microproces- sors, almost forms computing platforms ranging personal handheld devices mainstream desktop server computers powerful parallel clustered computers based building blocks microprocessors. heading fourth decade microprocessors, momentum shows sign abating. technologists agree Moore's law continue rule least 10 15 years more. 2010, expect microprocessors contain 1 billion transistors clocking frequen- cies greater 10 GHz. also expect new innovations number areas. current focus instruction-level parallelism (ILP) expand include thread-level parallelism (TLP) well memory-level parallelism (MLP). Architectural features historically belong large systems, exam- ple, multiprocessors memory hierarc hies, implemented single chip. Many traditional "macroarchitecture" issues become microarchi- tecture issues. Power consumption become dominant performance impedi- ment require new solutions levels design hierarchy, including fabrication process, circuit design, logic design, microarchitecture design, software run-time environment, order sustain rate performance improvements witnessed past three decades. objective book introduce fundamental principles micro- processor design microarchitecture level. Major techni ques beenMODERN PROCESSOR DESIGN developed deployed past three decades pres ented comprehensive way. book attempts codify large body knowledge systematic framework. Concepts techniques may appear quite complex difficult decipher distilled format intuitive insightful. number innovative techniques recently proposed researchers also highlighted. hope book play role producing new generation microprocessor designers help write history fourth decade microprocessors. 1.2 Instruction Set Processor Design focus book designing ins truction set processors. Critical instruction set processor instruction set architecture, specifies functionality must implemented instruction set processor. ISA plays several crucial roles instruction set processor des ign. 1.2.1 Digital Systems Design r engineering design starts specification objective obtaining \{ good design implementation. Specification behavioral description desired answers question "What do?" implementation structural description resultant design answers th e question "How constructed?" Typically design process involves tw fundamental tasks: syn- thesis analysis. Synthesis attempts find implementation based specification. Analysis examines implementation determine whether well meets specification. Synthesis creative task searches possible solutions performs various tradeoffs design optimi- zations arrive best solution. critical task analysis essential determining correctness effectiveness design; frequently employs , simulation tools perform design validation performance evaluation. typi- 1 cal design process require traversing analysis-synthesis cycle ^numerous times order arrive final best design; see Figure 1.1. digital systems design, specifications quite rigorous design optimi- zations rely th e use powerful software tools. Specification combina- tional logic cir cuit takes form boolean functions pecify relationship Figure 1.1 Engineering Design.PROCESSOR DESIGN input output variables. implementation typically optimized two-level AND-OR design multil evel network logic gates. optimiza- tion attempts reduce number logic gates number levels logic used designj|pr equential circuit design, specification form of~3 state machine descriptions include specification state variables well output next state functions. Optimization objectives include reduction number states complexity associated com bina- tional logic circuits. L ogic minimization state minimization software tools are,  % essential. Logic state machine imulation tools used assist analysis task. tools verify logic correctness design determine critical delay path hence maximum clocking rate state machine. design process microprocessor complex less straightfor- _ ward. specification microprocessor design th e instruction set architec -J^ ture, specifies set instructions microprocessor must able executeT^he implementation actual hardware design descri bed using hard -1 ware description language (HDL^The primitives HDL range logic" gates flip-flops, complex modules, decoders multiplexers, entire functional modules, adder multipliers. design described schematic, interconnected organization, primitives. process designing modem high -endjiiicroprocessnrjyp ically involves ^ two major steps: mi croarchitecture design logjc_design. Microarchitecture design involves developing defining key techniques achieving tar- geted performance. Usually performance model used analysis tool assess effect iveness techniques. Th e performance model accurately models behavior machine clock cycle granularity able quantify number machine cycles required execute benchmark program. end result mic roarchitecture design high-level description orga- nization th e microprocessor. description typically uses register transfer language (RTL) specify major modules machine ganization interactions modules. logic desjgnjtep, RTL description successively refined incorporation im plementation details eventually yield HDL description actual hardware des ign. RTL HDL descriptions potentially use description language. example, Verilog one language. primary focus book 'I microarchitecture design. . *. — 1.2.2 Archi tecture, Implementation, Realization classic textbook computer architecture Blaauw Brooks [1997] authors definecfthree fundamental distinct levels abstraction: architecture," implementation, realization. Arvtutecture. specifies functional behavior processor. Implementation logical structure ganization performs architecture. ReaiizgtianAs physical structure embodies implementation. _ ' Architecture also referred instruction set architecture. specifies instruction set characterizes fu nctional behavior instruction set processor?" software must mapped encoded instruction set in» MODERN PROCESSOR DESIGN AIMorder executed processor. Every program compiled sequence instructions instruction set. Examples well-known architectures areJBM 360, DEC VAX± Motorola^gK^oweiPQ^^rrMTA^ Attributes / associated" architecture include assembly language, instruction format, \ addressing modes, programming model. att ributes part ISA exposed soft ware perceived compiler programmer. implementation specific design architecture, alsor.±1EXAMP E rreferred microarchitecture. architecture many implementa- tions lifetime ISA. implementations architecture execute program encoded ISA. Examplesof ^messellrlcrMfflirurnrilementations above-listed architecture IBM 360/91^ VAXJ.1/780, Motorola 68040, PowerPC 604, and~IhteTP6. AhiibmesassocTated implementation include pipeline design, cache memories, branch predictors. Implementation microarchitecture features generally implemented hardware hidden  software. develop features job microprocesso r designer microarchitect. realization implementation specific physical embodiment design. microprocessor, physical embodiment usually chip multi- C chip pac kage?For given implementation, various realizations v implementation. realizations vary differ terms clock fre- quency, cache memory capacity, bus interface, fabrication technology, packaging, etc. Attributes associated realization include die ize, physical form factor, power, cooling, reliability. attributes concerns chip ^designer system designer uses chip. f~ primary focus book implementation modern micropro- cessors. Issues related architecture realization also important. Architecture serves specification fo r implementation. Attributes architecture significantly impact design complexity design effort implemen- tation. Attributes realization, die size power, must considered r\ design process used part design objectives. 1.2.3 Instruction Set Architecture Instruction set architecture plays crucial role defined con- tract softwar e hardware, program machine. ByTiaxingJlie-ISA contract, programs ajia^macbiries_can. devel- oped, independently. Programs de veloped target ISA without requiring knowledge actual machine implementation. Similarly,.machines designed implement ISA without concern programs run them. program written particular ISA able run machine implementing ISA. notion maintaining ISA across multiple implementations ISA first introduced IBM S/360 line computers [Amdahl et al., 1964]. CHaving ISA also ensures software por tability. program written par- ticular ISA run implementations ISATTypically given ISA, many implementations wil l developed lifetime ISA, orPROCESSOR DESIGN multiple implementations p rovide different levels cost performance simultaneously developed. program needs developed ISA, run implementations. program portability significantly reduces cost oftware development increases th e longevity software. Ujrfortunately benefit also makes migration aaiew ISA- difficult. uccessful ISAs, specifically ISAs large software instafledbase, tend stay around quite while. Two examples IBM 360/370 Intel IA32. Besides serving reference tar geted software developers compilers, ISA serves specification processor designers. Microprocessor design starts ISA produces icroarchitecture meets specification. Every new microarchitecture must validated ISA ensure per- forms functional requirements specified ISA. extremely impor- tant ensure existing ftware run correctly new microarchitecture. Since advent computers, wide variety ISAs developed used. differ ope rations operands specified. Typically ISA defines set ins tructions called assembly instructions. instruction specifies opera tion one oper ands. ISA uniquely defines_an_ assemblzJangugge. assembly language program constitutes sequence assembly instructions. ISAs beenjhfferentiated according tie _Jwmber _oi operands explicitly specified eacli Instruction, example two- add>ess~cFthree^lito use accumulator implicit operand. accumulator-based architecture, accumulator used implicit source operand destination . early ISAs assume oper- ands stored stack [last in, first (LIFO)] structure operations performed top one two entries stack. modemJSAs assume operands ar e stored Jn^ajnultifintry xegister_fi!e, jhjtfjdUuitlunetic logical operations performed operands stored th e registers. Special mstfiictions; sucITasToaTaHd store instmctions^ajre devisedjo^mpj'e^r^ands BefweeiTthe regTstei fileand main memory^ traditional ISA allow oper- ands come directly register file main memory. £~ ISAs tend evolve slowly due inerti recompiling rede- veloping software. Typically twofold performance increase needed software developers willing pay overhead recompile existing applications. new extensions existing ISA occur time time accommodate new emerging applications, introduction brand new ISA tall order. development effe ctive compilers operating systems new ISA take order UH- years. longer ISA exist- ence larger installed base software b ased ISA, diffi- cult replace ISA. One pos sible exceptio n might certain special application domains sp ecialized new ISA might able provide signif- r\ icant performance boost, order 10-fold. r~ Unlike glacial creep ISA innovations, significantly new microarchitectures j^can developed every 3 5 years. 1980s, widespread interests ISA design passionate debates constituted the8 MODERN PROCESSOR DESIGN best ISA features. However, since 1990s focus shifted implemen- tation innovative microarchitecture te chniques applicable most, all, ISAs. quite likely ISAs dominated micro- processor landscape past decades continue coming decade. hand, ca n expect see radicall different nnovative microarchitectures ISAs coming decade. 1.2.4 Dynamic-Static Interface far discussed two critical role played ISA. First, provides contract software hardware, facilitates independent development programs machines. Second, ISA serves specifica- tion microprocessor design. Al l implementations must meet requirements support functionality specified ISA. addition two critical roles, ISA third role. Inherent definition ofevery ISA assocP ated definition interface separates done statically aTcorfiple time ' versus done dynamically run time. interface called dynamic-static interface (DSI) Yale Part illustrated Figure 1.2 [Melvin Part, 1987]. ~~ | DSI direct consequence ISA serv e contract ' software hardware. (Jraditionatly, tasks optimizations done static domain compile time involve software compiler, considered DSI. Conversely, tasks optimizations done dynamic domain run time involve hardware considered DSITAII architecture features specified ISA therefore exposed software DSI static domain. hand, imple- mentation fe atures microarchitecture DSI operate dynamic domain run time; usually completely hidden software compiler static domain. stated earlier, software development take place DSI independent development th e microarchitecture features DSI. key issue design ISA placement DSI. application program written high-level language top actual hardware machine bot tom, different levels abstractions DSI potentially placed. placement DSI correlated Program (Software) 11 Compiler V complexity ArchitectureExposed software "Static" n Machine (Hardware) Figure 1.2 ^Qje Dynamic-Static Interface.jPSI) Hardware complexityHidden hardware"Dynamic"PROCESSOR DESI DEL -CISC -VLIW -RISCHLL Program Figure 1.3 Conceptual Illustration Possible Placements DSI ISA Design. decision p lace DSI place DSI. example, performance achieved optimizations carried DSI compiler well throug h optimizations per- formed DSI mi croarchitecture. Ideally DSI placed level achieves best synergy static echniques dynamic tech- niques, i.e., leveraging best combination compiler complexity hardware complexity obtain des ired performance. DSI placement becomes real challenge constantly evolving hardware technology compiler technology. history ISA design, number different placements DSI proposed led commerc ially successful ISAs. conceptual illustration possible placements DSI shown Figure 1.3. figure intended highly r igorous simply illustrate DSI placed different levels. Fo r example, Mike Flynn proposed placing DSI high doin g everything DSI, program written high-level language directly executed directly executable language machine [Flynn Hoevel, 1983]. complex instruction set computer (CISC) ISA places DSI traditional assembly language, macrocode, level. contrast, reduced instruction set computer (RISC) ISA lowers DSI expects perform optimizations DSI via compiler. lowering DSI effectively exposes elevates would con- sidered microarchitecture features CISC ISA ISA level. purpose reduce hardware complexity thus achieve much faster machine [ Colwell et al„ 1985]. DSI provides important separation architecture implemen- tation. Violation separation become problematic. ISA evolves extensions added, p lacement DSI implicitly shifted. lo wering DSI promoting former implementatio n feature architecture level effectively exposes part original microarchitecture software. facilitate optimizations compiler l ead reduced hardware complexity. However, hardware techno logy changes ra pidly implementations must adapt evolve take ad vantage technological changes. implementation styles techniques change , older techniques microarchitecture10 MODERN PROCESSOR DESIGN features may become ineffective even undesirable. older fea- tures promoted ISA level, become part ISA exist installed sof tware base legacy code containing features. Since future implementations must support entire ISA ensure portability existing code, unfortunate consequence future implementations must continue support ISA features promoted earlier, even ineffective even undesirable. mistakes made real ISAs. lesson learned mistakes strict separationpf-architec- ture microarchitecture must maintained disciplined f asluon^deally^the architecture ISA contain features necessary express function- ality semantics software algorithm, whereas features employed facilitate better program performance relegated imple- mentation th e microarchitecture domain. focus book ISA design microarchitecture tech- niques, almost exclusive emphasis performance. ISA features inflifc. ence design effort design complexity needed achieve high levels performance. However, view contemporary high-end microprocessor design, microarchitecture, ISA, dominant determi- nant microprocessor performance. Hence, focus bock microar- chitecture techniques achieving high performance. important design objectives, power, cost, reliability. However, historically per- formance received attention, large body knowledge techniques enhancing performance. bod knowledge book attempting codify. 1.3 Principles Processor Performance primary design objective new leading-edge microprocessors performance. new gen eration microarchitecture seeks significantly improve performance previous generation. recent years, reducing power consumption emerged another, potentially equall important design objective. However, demand greater performance alway there, processor performance wil l continue key design objective, 1.3.1 Processor Performance Equation 1980s several res earchers independently discovered formulated equation clearly define processor performance cleanly characterizes fundamental factors contribute processor performance. equation come known iron law processor performance, shown Equation (1.1). First, processor performance equation indicates proces- sor's performance measured terms long takes execute particular program (time/program). Second, measure time/program execution time formulated product three terms: instructions/program, cycles/ instruction, time/cycle. first term indicates JheJeuujijmbaMrfjri ^aTra instructions need executecTTor particular pr ogram; term alsoPROCESSOR DES referredtoas instructian-eeunt. The-second term indicate average (averag- ing overjhejentir^execution program) manymachine cycles con- sumed Ito execute Leach insjtoction; typically ffus tenms~dinoted"as CP/(cycIes per instruction). third term indicates length time machine cycle, namely, cycle time the. machine. ~—-r-—~—^ £K- t'"~i? fcT. &->">r c -tl cpU-C"*ie  1 /time instructions, ., c ycles ^'fime, lfo/1 IB-V Z~JC = r =f 'X : :—XJ fh (1.1) L CYl Performance program program/ instruction cycje shorter program's execution time, better performance. Looking Equation 1.1, conclude pr ocessor performance im proved reduc- ing magnitude one three terms equation. instruction count reduced, fewer instructions execute execution time reduced. CPI reduced, average instruction con- sume fewer machine cycles. cycle time reduced, cycle consume less time verall execution time reduced. might seem equation improving performance quite trivial. Unfor tunately, straightforward. three termsare notaU independent, J there^areromplex inter- actions between^ them. reduction one term potentially increase magnitude two terms. relationship three terms cannot easily characterized. Improving performance becomes real challenge involving subtle tradeoffs delicate balan cing acts. exactly challenge makes processor design fascinating times art science. Section 1.3.2 examine closely different ways improve processor performance. 1.3.2 Processor Performance Optimizations said performance optimization techniques boil reducing one three terms processor performance equation. SpmeJechz niques rnnjrfliirr rwMgrm _\yhilr .tanJTj£Jhr nthrr twn nnrhnn £od example, w^enj-compiler^erfbims_j ^mizatiojK. eliminate_redundant useless instructions object code, instruction count reduced without impact- ing CPI cycle time. another ex ample, fastgrdrciiit technology advanced fabrication process used reduces signal propagation delays, machine cycle time caTTpotentially reduced without impacting instruction count CPI. types performance optimization techniques always desirableand jihould employed cost isnot p rohibitive. techniques reducejone thetenns~can time increase one px-both-of-the-other terms. techniques, therejs perforjnance.^ain ifjhe rgduaiorjin pn^ermjsnot overwhelmed increase terms. examine techniques byTooking reduction three terms processo r performance equation. jare_a_numbei_of ways tn reducethe instmctionjwunt. First, instruc- tion set include mcje_jmiple *jii5tructiOTrs perform work per instruction. total number instructions executed decrease significantly. example, program RISC ISA require twice many instructions asMODERN PROCESSOR DESIGN one CISC ISA. instruction count may go down, complexity Jheexecution unit increase, leading ajgtfnt'al increas** cycle tirrjc^Jf, deeper pipelining used avoid increasing cycJejirnejJhenj^Mg^ misjsrj^ictiqn penalty result higher CPI. Second, cet^n^cQmpjle £C^niiza- tions result fewer instructions executed. e xample, unrollingijops caif reduce number loop elQsingJnstructjons executed. However, lead tefan 'n^gae uxZEe sfarjccode size, turn -impact instruction cache hit rate, wWchjan-in -tuHi increase CPI. Another similar example in-liningj>Truriction calls. eliminating-calls retains, fower insrtuo*iA»«^a«» executed, cod e size ca n significanti^expanA ^JJurd, recently researchers proposed dynamic Elimination nfredunrian computations :"ia- miooaiejUJteiaiuEJeclm^ hav e observed program exec ution, frequent repeated executions computation th e data set. Hence, result earlier computation J3e_biiffcrcd diroctly used. withou ^epe ^mg]ffija^?computation [Sodani Sohi, 1997] . computa- tion reuse techniques clm~reducethe instruction count, can_poteniially-incre £se complexity hardware implementation canjead ncrease cyctejime. lee decreasing instruction count potentially lead increasing CPI and/or c ycle time. desire reduce CPI inspired many architectural microarchitec- tural techniques. Onejafthe v ^^mntryatiriric fnr RISC war, tn rorinrx thr roirjnlrx ityof instruction order reduce number of^rnaeiiine cycles requited to. process instruction? already mentioned, comes the_oyejh_ head" increased instruction count. Another key technique tn reduce CP] \s instnicticai^r^elinirtg. pipelinecTprocessor overlap processing multi- ple instructions. Compared nonpipelined design assuming identical cycle times, pipelined design significantly reduce CPI. shallower pipeline, Ifaatjiua pipeline fewer pipestagpg <-an yield lower TPT deeper pipe- jinejbut expense ofincreased cycle time. use cachejnernorv reduce average memory access latency (in terms number clock cycles)VyiIl also reduce CPL conditional branch taken, stalled cycles result fetch next instruction nonsequential location. Branch predic- tion techniques reduce number stalled cycles, leadingto reduction oTCPI. However, addinr^nvnrjijnyrlirtnr^ pntpntinlly iccxease cycle timr duejra tte~^Ued_complexity die fetch pir^tage^OTeven increase_the_CPIJf_a deeperjijjeline required maintain tl^samecycle time. emergence superscalar processon^allows processor jiPjjJ!^jg-gl!THl!tgBgO^''1y process multiple instructions pipe stage. able sustain execution multiple instructions every macfirrrecycle, CPI significantly reduced. course, complexity pipe stage increase, leading potential increaseofcycje time pipelniedepth, canTnTum increase theCPL" key microarchitecture technique reducing^cycle_tirneTl ^i^im^rr£^ Pipelining effectively partitions thetask ofprocessing instruction multiple stages. latency (in terms signal propagation delay) ^ipe stage deter- mines the^nachine cycle time. employing deeper pipelines, thehrtencyTSFeacbTPROCESSOR DESIC pipe stage, hence cycle time^can^bj^educed. recent ears, aggressive ppelimhg~nas~been major technique used achieving phenomenal increases clock frequency high-end microprocessors. seen Table 1.1, dur- ing recent decade, performance increase due increase clock frequency. , downside increasing clock frequency th rough deeper pipelin- i/ ing. pipeline g ets deeper, CPI go three ways. First, front end pipeline gets deeper, number pipe stages fetch execute increases. increases number penalty cycles incurred branches mispredicted, resulting increase CPI. Second, pipeline deep primitive arithmetic-logic unit (ALU) operation requires multiple cycles, necessary latency two dependent instructions, even result- forwarding hardware, multiple cycles. Third, clock frequency increases deeper central processing unit (CPU) pipelines, latency mem- ory, terms number clock cycles, significantly increase. increase th e average latency memory operations thus increase overall CPI. Finally, hardware latency overhead pipelining lead diminishing returns performance gains. tech nique getting higher fre- quency via deeper pipelining served us well fo r decade. clear much push requisite complexity power consumption become pr ohibitive. concluded discussion, achieving performance improve- ment straightforward task. requires interesting tr adeoffs involving many sometimes subtle issues. talented microarchitects processor designers industry seem possess intuition insights enable make tradeoffs better others. goal, perhaps dream, book impart concepts techniques superscalar processor design also intuitions insights superb microarchitects. 1.3.3 Performance Evaluation Method modem microprocessor design, hardware prototyping infr sihto; design- ers use simulators performance projection ensure functional correctness design process. Typically two types sinuilators_are used: fun ctional simulators performance simulators. Funrtinnql emulators model machine architectur e (ISA) level used verify correct execution pro- gram. Functional simulators ac tually interpret execute instructions program. Performance simulators model microarchitecture design used measure th e number machine cycles required ex ecute program. Usually performance simulators concerned semantic correctness instruction execution, timing instruction execution. Performance simulators either trace-driven execution-driven; illus- trated Figure 1.4. Trace-driven performance simulators process pregenerated traces determine cycle count executing instructions traces. «A trace-eaptures dynamic seqj .iPnrf' "f instructions pypr;in^ri_s«idran begenerated three different ways; see Figure 1.4(a). One way via software instrumentation. MODERN PROCESSOR DESIGN Physical execution software instrumentation Physical execution hardware instrumentation Functional simulator Trace generationTrace storageCycle-based )—-performance f simulator Trace-driven timing simulation (a) Execution Functional simulator (instruction interpretation)trace Cycle-based performance simulatorFunctional simulator (instruction interpretation)Cycle-based performance simulatorFunctional simulator (instruction interpretation) CheckpointCycle-based performance simulator Functional simulation control Execution-driven timing simulation (b) Figure 1,4 Performance Simulation Methods: (a) Trace-Driven Simulation; (b) Execution-Driven Simulation. j inserts special instructions program prior run time Vy instrumented program executed physical system, inserted instructions produce dynamic execution trace . Another way via hardware instrumen- tation, involves putting special hardware probes monitor system bus record actual execution trace program executed system. Software instrumentation significantly increase code size program execution time. Hardware instrumentation requires monitorin g hardware seriously limited buffering capacity monitoring hardware. third trace generation method uses functional simulator simulate execution program. simulation, hooks embedded imulator record dynamic execution trace. three methods, traces generated, caa stored subsequent repeated use trace-driven performance simulators. ExecijtiQn^djjxenj^ormance simulators overcome limitations trace-driven performance simulators; see Figure 1.4(b). Instead us ing pregener- ated traces, execution-driven performanc^imulator interfaced functional simulator, two simulators work tandem. simulation, functional simulator executes instructions passes information associated exe- cuted instructions performance simulator. performance simulator tracks timing instructions movement pipeline stages. fhejjbililyjoisjsue directives thp, fiinct ionaljjirniilatnr checkpoint simulation state to.JateiresumeJronithe_checkpointed state. checkpoint capability allows simulation, . speculative instructions, instructionsPROCESSOR DESI following branch prediction. specifically, execution-driven simulation simulate mis-speculated instructions, instructions following mispredicted branch, going pipeline. trace-driven simulation, pre- generated trace contains actual (nonspeculative) instructions executed, trace-driven simulator cannot account instructions mis-speculated path potential contention resources (nonspeculative) instr uctions. Execution-driven simulators also alleviate need store long traces. mod- ern performance simulators employ execution-driven paradigm. advanced execution-driven performance simulators supported functional simulators capable performing full-system simulation, is, simula- tion application operating system instructions, memory hierarchy, jmd even input/output devices. actual implementation mic roarchitecture model performance simulator vary widely terms amount details machine resources explicitly modeled. Som e performance models merel cycle counters assume unlimited resources simply calculate total number cycles needed execution trace, taking account int er-instruction depen- dences. Others explicitly model organization machine com- ponent modules. performance models actually simulate movement instructions various pipeline tages, including allocation limited machine resources machine cycle. many per formance simulators claim "cycle-accurate," methods use model track activi- ties machine cycle quite different heavy reliance performance simulators early design stages microprocessor, validation accuracy performance simulators extremely difficult task. Typically th e performance model sim- ulator implemented early phase design used initial tradeoffs va rious microarchitecture features. phase refer- ence used validate performance model. As_the-design-progresscs - RTL rnpdelof design deyeloped^the RTL model beused -asAffit. erencejoj5lidateJhe_accjffiacy theperfbrmance model. H owever,, simulation using RTL model slow^ tnereTorTonlyrvery short traees-can-bo- used. entire design pr ocess, discipline essential concurrently evolve p erformance model RTL model ensure performance model tracking changes made RTL model. also important post-silicon validation performance model used good starting point next-generation design. performance simulators used academic research never validated. simulators quite complex and, like large pieces software, ca n contain many bugs di fficult eliminate. quite likely large fraction performance data pub- lished many research papers using unvalidated performance models com- pletely erroneous. Black argues convincingly rigorous validation processor simulators [Black Shen, 1998]. difficulty validating accuracy, another problem associated performance simulators extremely long simulation times ofteni MODERN PROCESSOR DESIGN required. contemporary performance evaluations involve simulation many benchmarks total tens hundreds billion instructions. early phase design, performance simulators used extensively support exploration numerous tradeoffs, require many simulation runs using differ- ent sets parameters simulation model. execu tion-dr iven performance simulators fairly detailed mo dels complex machine, slowdown factor four five orders magnitude rather common. words, simulate single machine cycle target machine, machine modeled, require execution 10,000 100,000 machine c ycles host machine. large set simulation runs sometimes take many days complete, even using large pool simulation machines. 1.4 Instruction-Level Parallel Processing Instruction-level parallel processing informally defined concurrent processing multiple instructions. Traditional sequential proce ssors execute one instruction time. leading instruction completed next instruction processed. certain extent, pipelined processors achieve form instruction- level parallel processing overlapping processing multiple instructions. many instructions pipeline stages concurrently flight onetime. Traditional equential (CISC) processors require average 10 machine cycles fo r processing ins truction, CPI = 10. pipelined (RIS£Xprpcessors,^yejiJhQugh instruction may stiU r equire multiple cyclesTo" complete, overlapping Hje pr ocessing multiple instructions pi peuneTtfie effective average CPI reducedtrJcToleTo ongtfVne^Tnkrucnon initio ated everxrnachinexycle. scalar pipelined processors, still limitation fetching initiating one instruction pipeline every machine cycle. hmitation, best possible CPI achieved one; inversely, best possible throughput scalar processor one instruction per cycle (IPC). aggressive form instruction-level parallel processing possible involves fetching initiating multiple instructions wider pipelined proces- sor every machine cycle. decade 1980s adopted CPI = 1 design objective single-chip microprocessors, goal decade 1990s reduce CPI one, achieve throughput IPC greater one. Processors capable IPC greater eare termed superscalar pro; cessors. section presents overview mstruction-level parallel processing provides bridge scalar pipelined processors natural descendants, superscalar processors. . 1.4.1 caiar Superscalar Sca/a?*processors pipelined processors a|e-degigned fetch issue one instruction every machine cycle. <SiuperscalapprocessoTS _are designed fetch issue multiple inslrurtians'every machine cycle. subsection presents basis motivation evolving scalar supersca- lar processor implementations.PROCESSOR DESIGN 1 A1.1 Pr ocessor Performance. Section 1.3.1 introduced iron law processor performance, shown Equation (1.1). equation actually repre- sents inverse performance product instruction count, average CPI, clock cycle time. rewrite equation directly represent performance product inverse instruction count, average IPC (IPC = 1/CPI), clock frequency, shown Equ ation (1.2). Looking equation, see performance incrpaspri hv ip^rfaiinc thfr IPCJncreasing frequency, decreasing instruction count. Performance = : 1 x instructions x 1 = IPC x frequency instruction count cycle cycle tim e instruction count (1-2) Instruction count riptprminpd hy thrpp rnntrihnting factors: instruction P ( . set architecture, thc_cojnpilerJMid theoperating system. ISA amount work encoded ins truction strongly infl uence total number instructions executed program. effectiveness compiler also strongly influence number ins tructions executed. operating system functions invoked application program effectively increase total number instructions executed carrying execution program. Average IPC (instructions per cycle) reflects th e average instruction throughput achieved processor key measure microarchitecture effectiveness. Historically, inverse IPC, is, CPI (cycles per instruction), used indicate average number machine cycles needed process instruction. use t~VI w p"f>"lar tl'i'rinp ffrr dm n^ ^"tar pjpcflpcd processors. per- formance penalties due variou forms pipeline stalls cl eanly stated dif- ferent CPI overheads. Back then, ultimate performance goal scalar pipelined processors reduce average CPI one. moyejnto the_sjiperscalar domam, hbecornes convenient use IPC npw pprfnrmanr-o ar.ai fr.r sur^rscalarjH ^li^rsjs greaterdranone. bulk microarchitecture techniques presented book target lmprovement IPC. Frequency strongly affected fabrication technology circuit tech- niques. Increasing number pipeline stages also facilitate higher clocking frequencies reducing number logic gate levels pipe stage. Tradi- tional pipelines 20 levels logic gates pipe stage; contemporary pipelines have^onlyJO^or fewer levels. achieve high IPC superscalardesigns, pipeline must made wider allow simultaneous pro- cessing multiple instructions pipe stage. widening pipeline increases hardware complexity signal propagation delay pipe stage. Hence, wider pipeline, order toTf^t^JBe e frequency evendeeper pipeline may required. isa complex tradeoff mak- ing pipelines wider makingthem deeper. 1.4.1.2 Parallel Processor Performance. consider parallel process- ing instructions increasing processor performance, insightful revisit classic observation parallel processing commonly referred Amdahl's law 18 MODERN PROCESSOR DESIGN 1-1 -h- Time Figure 1 .5 Scalar Vector Processing Traditional Supercomputer L[Amdahl, 1967]. Traditional supercomputers parallel processors perform scalar vecto r computations. scalar computation one processor used. vector computation N processors used perform operations array data. "The computation performed parallel machine depicted shown Figure 1.5, N number processors machine andjijs fraction timethe machine spends scalar computation. Conversely, 1 - h iTtheTraction time machine spends vector computation.nOne formulation Amdahl's law states ^fficiency _E parallel machine measured overall utilization N processors fraction timeTh^Vrjro^essors_areTusy;^ficTency E modilecTas f) E . h + Nx(l U N N(1.3) 'As the-numbetxif processors-JV hecomes large, efficiency E approaches . 1 - h, fraction time th e machine spends vector computation. N becomes large, amount time spent vector computation become smaller smaller approaches zero. Hence, TV becomes large, efficiency E approaches zero.This means almost ccarjputation time taken scalar computation, farther increase ofTV makes little impact r educing overall execution time. Another formulation principle based amount work done vector computatio n mode, vectorizability pro- gram. shown Figure 1.5»/ represents Jhe fraction program thaj_can.be parallehzedjojiin nTjyectpj computation mode. Therefore, 1 - / represents "traction program must executed sequentially. totaltime required run program, relative speedup represented s= L = (l-/) + (/W)(1.4) h e u f ( 1 -/), h e time required execute sequential part, f/N, time required execute parallelizable part program. N becomes large, second term sum approaches zero, totalPROCESSOR DESIGN 'execution time dictated amount time required execute sequential part. commonly referred sequential bottleneck; is, time spent m^wjMwvrifl] rixf-nri"n sraTarl^rr ^^fcDJjPr.o"1''' lirjiLi ^goggijJM 'yj^ffi performance improvement achieved via exploitation parallelism. N increases th e machine parallelism increases, performance become sensitive dictated sequential part program. efficiency parallel processor drops quickly number processors increased Furthermore, vectorizability, i.e., fraction program parallelized, program drops slightly 100%, efficiency drop-off rate increases. Similarly overall speedup drops quickly /, vectorizability program, drop even slightly 100%. Hence, overall performance improvement sensitive vectorizability program; state another way, overall speedup due parallel processing strongly dictated sequential part program Tfiemachine parallelism increases; 1.4.1.3 Pipelined Processor Performance. Harold Stone proposed per- formance model similar parallel processors developed pipe- lined processors [Stone, 1987]. typica l execution profile pipelined processor shown Figure 1.6(a). machine parallelism parameter N depth pipeline, is, number stages pipeline. three phases i-«  + > * Figure 1.6 Idealized Pipelined Execution Profile: (a) Actual; (b) Modeled.MODERN PROCESSOR DESIGN execution profile. fi rst phase pipeline f illing phase first sequence N instructions enters pipeline. second phase pipeline full phase, pipeline full represents steady state pipeline. assuming pipeline disruption, therefore represents perfect pipeline execution profile. third phase pipeline draining phase, new instruction entering pipeline Pipeline finishing instructions still present th e pipeline stages. \ modelin g purposes, modify execution profile Figure 1.6(a) execution profile Figure 1.6(b) moving work done pipeline filling phase pipeline draining phase. total amount work remains same; is, areas within two profi les equal. number pipeline stages N, fraction time N pipeline stages utilized g, 1 - g fraction time one pipeline stage utilized. Essentially 1 - g viewed fraction time one instruction moving pipeline; is, overlapping instructions pipeline. Unlike idealized pipeline ex ecution profile, realistic pipeline execu- tL^tion profile must account stalling cycles. done shown Figure 1.7(a). Instead remaining pipeline full phase duration entire execution, steady state interrupted pipeline stalls. stall effec-__ tively induces ne w pipeline draining phase new pipeline filling phase, shown Figure 1.7(a), du e break prpelme1fuirr£ase^ Similar modifica- tion r»T*erf6rmed execution profile resul modified profile (a) N SPipeline stall Pipeline stall i—1  ir ,-->r — - 1 i—1 1 |r - - 1 1 1 r—1 II— j I1 i1 Figure 1.7 Realistic Pipeline Execution Profile: (a) Actual; (b) Modeled.PROCESSOR DESIGN Figure 1.7(b) moving part work done two pipeline filling phases two pipeline draining phases. modified profile Figure 1.7(b) r esem- bles execution profile parallel processors shown Figure l.S. similarity execution profiles, borrow perfor- mance model parallel processors apply pipelined processors. Instead number processors, N number pipeline stages, maximum speedup possible. parameter g becomes fractioiiLpf time whenJhe r4pejine.is filled, parameter 1 - g represents fraction time pipeline stalled. speedup obtained ~~i  /] ?^SM \ s=(i-g)+(g,N) (L5) Note g, fraction time~when pipeline full, analogous to/, vec- torizability program parallel processor model. Therefore, Amdahl's law analogously applied pipelined processors. g drops slightly 100%, speedup performance pipelined processor drop offvery quickly. words, actual performance gain obtained pipelining strongly degraded small fraction stall cycles. degree pipelining N increases, traction stall cycles come increasjngjy_dpvasfa speedup te_achieyed pipeline processor. Stall cycles pipelined processors key adversary aricTare analogous sequential bottleneck parallel processors. Essentially, stall , cycles constitute pipelined proc essor's sequential bottleneck. 1 Equation (1.5) simple performance model fo r pipelined processors based Amdahl's law parallel processors. assumed model whenever pipeline talled, one instruction th e pipeline, ef fectively becomes seq uential nonpipelined processor. implication pipe- line stalled overlapping instructions allowed; effectively equiva- lent stalling pi peline N cycles allow instruction causing stall completely traverse pipelifieTWe know, however, clever design \ pipeline, use forwarding paths, resolve hazard causes pipeline stall, number penalty cycles incurred necessarily N likely less N. Based observation , refinement model Equation (1.5) possible. 5 = 1 (1.6) 1 2 N Equation (1.6) generalization Equation (1.5) provides refined model pipelined processo r performance. model, gj representsjhe fiaclLon_pf time instructions ijrithejrip^line. otherwords, gt represents fraction oftTme pipeline stalled (rV - i) penalty cycles. course, gN fraction time pipeline full. pipelined processor performance model illustrat ed applying six-stage TYP pipeline Ch apter 2. Note TYP pipeline load penaltyMODERN PROCESSOR DESIGN one cycle branch penalty four cycles. Based statistics IBM study presented Chapter 2, typical percentages loa branch instructions 25% 20%, respectively. Assuming TYP pipeline designed bias branch taken, 66.6% branch instructions, actually taken, incur branch penalty. Therefore, 13% instructions (branches) incur four-cycle penalty 25% instructions (loads) incur one-cycle p enalty. remaining instructions (62%) incur penalt cycles. performance TYP pipeline modeled shown Equation (1.7). Styp = 0.13 ~ 0.25 + 0.62 = 0.13 ~ 0.25 + 062 = 022 = 4'5 (L7) (6-4) (6-1) 6 2 5 6 resultant performance six-stage TYP pipeline processor factor 4.5 sequential nonpipelined processor. Note TYP six-stage pipeline th e theoretical peedup potential 6. actual speedup based model Equation (1.6) 4.5, shown Equation (1.7), viewed effective degree pipelining TYP pipeline. Essentially six-stage TYP processor behaves perfect pipeline 4.5 pipeline stages. difference 6 4.5 reflects difference potential (peak) pipeline parallelism achieved ( actual) pipeline parallelism. 1.4.1.4 Superscalar Proposal. restate Amdahl's law.that models performance parallel processor s~o ^rm (L8) model gives performance speedup parallel system nonparallel system. machine parallelismis measured hyW, number ofpxo- cessors_in machine, reflects maximum number tasks simultaneously performed system. parameter/, however, vector- izability program reflects program parallelism. formulation model influenced traditional supercomputers contain scalar unit vector unit. vector unit, consisting N processors, executes vector- izable portion program performing N tasks time. nonvectoriz- able portion program executed scalar unit sequential fashion. already observed oppressive tyranny nonvectorizable portion program overall performance obtained parallel processing. The. assumption nonvectorizable portion program must exe^ cuted sequentially overly pessimistic necessary. some, even low, level parallelism achieved nonvectorizable portion program, severe impact sequential bottleneck significantly moderated figure 1.8 illustrates principle. figure, taken IBM technical report coauthoredPROCESSOR DESIGN 1 ! 1 1 J— «=,ioo/ / [ | ! / h - .1 / ./*..,/ /* 'i * / V / " /- MA. Equation 1.10 / f\ / / .^^jr* ^-J£ . *r . .... _ ___-.—— 0.2 0.4 0.6 Vectorizability / 0.8 Figure 1.8 Easing Sequential Bottleneck Instruction-Level Parallelism Nonvectorizable Code. Source Agerwala Cocke, 1987. Agerwala Cocke [1987], plots speedup function /, vec torizabil- ity program, several values N, maximum parallelism machine. Take example case N = 6. speedup 1S = - (1.9) (l-/) + (//6) Examining curve Equation (1.9) figure 1.8, see speedup equal 6 if/is 100%, is, perfectly vectorizable. As/drops 100%, speedup drops quickly; /becomes 0%, speedup one; is, speedup obtained higher values N, speedup drop-off rate get signifi- cantly worse, as/approaches 0%, speedups approach one , regardless value N. assume minimum degree parallelism 2 achieved nonvectorizable portion program. speedup becomes 1S = (1-/)./ 2 6 (1.10) Examining curve Equation (1.10) Figure 1.8, see also starts speedup 6 when/is 100%, drops slowly curve Equa- tion (1.9) when/is lowered 100%. fact curve crosses curve Equation (1.8) N= 100 when/is approximately 75%. means cases with/less 75%, beneficial system maximum parallelism 6, thatls N~= 6T6ut minimum parallelism two non- vectorizable portion, system maximum parallelism N = 100 with± EXA J24 MODERN PROCESSOR DESIGN sequential ex ecutiarLi}f_the nonvectorizable portion.^ vectorizability pro- gram/isa complex function involvingtheapplication algorithm, programming language, compiler, architecture. tha n scientific applica- tions involving mostly numerical computatiqnSjjnost programs general-purpose computing teri3 veryTugn vectorizaMity. safe say general-purpose programs have/less 75%, many, significantly less. One primary motivation designing uperscalar processors develop general-purpose processors achieve (perhaps low relative vector- izing) level parallelism wide range application programs. goal ensure degree instruction-level parallelism achieved por- tions program moderate severe impact sequential bottle- neck. course, highly vectorizable programs continue achieve good speedup via parallelism. Note curve Equation (1.10) always higher Equation (1.9) even high values /, higher curves large values N lower values off. goal superscalar.processors isjo__ achieve generalized instruction-level parallelism consequent speedup types application programs, including necessarily vectorizable. T_J1.4.2 Li mits cf instruction-Level Parallelism Equation (1.10), parallelism degree 6 achieved the/fraction program parallelism degree 2 achieved remaining 1 -/ frac- tion program. speedup viewed aggregate degree par- allelism achieved entire progr example, parameter / 50% peak parallelism N 6, speedup aggregate degree parallelism 5 = 1 1 (W)+/ oj + PJ 2 6 2 6 = 3 (1.11) implication Equation (1.11) eff ectively overall aggregate degree parallelism 3 achieved entire program. Applyin g result instruction level, see Equation (1.11) indicates average three instructions simultaneously executed time. traditional vector computation, number operations simultaneously performed largely determined size vectors arrays, essentially data set size. general-purpose unstructu red programs, key question is, aggre- gate degree instruction-level parallelism potentially achieved? Instruction-level parallelism informally defined aggregate degree parallelism (measured number instructions) achieved th e concurrent execution multiple ins tructions. Possible limits ILP investigated almost three decades. Numerous experimental studies performed yield widely var ying results purported limits ILP. following table provides sample listing reported limits order increasing degrees ILP.Study "39LHIILP Limit Weiss Smith, 1984 I.S8 Soni vajapeyam, 1987 1.81 Tjaden Flynn, 1970 1.86 Tjaden Flynn, 1973 1.96 Uht Wedig, 1986 2.0 Smith etal., 1989 2.0 Jouppi Wall, 1989 2.4 Johnson, 1991 2.5 Acostaetal, 1986 2.79 Wedig, 1982 3.0 Butler etal., 1991 5.8 Melvin Part, 1991 6 Wall, 1991 7 Kuck etal, 1972 8 Riseman Foster, 1972 51 NiccJau F isher, 1984 90 listing certainly exhaustive, clearl illustrates diversity possible inconsistency research findings. limit studies making various idealized assumptions. real challenge achieve levels ILP realistic designs. purported limits also monotonic respect chronological order. decade 1990s debate lim- ILP replaced RISC vs. CISC debate 1980s [Colwell et al., 1985]. new debate limit ILP still ettled. 1 A2.1 Flynn's Bottleneck. One th e earliest studies done Stanford Univer- sity Tjadeaand Flynn inl970 concluded theJLP programs less ( 2^ limit informally referred Flynn's! bottleneck. JThis study focused instruction-level parallelism found within basic block boundaries. Since crossing basic block boundaries involves crossing control depen- \ dences, dependent run-time data, assumed basic blocks must executed sequentially. small size basic _J blocks, typically degree parallelism found less 2. result Flynn's bottleneck since confirmed sever al studies. One study 1972 confirmed result Riseman Foster [1972]. However, extended study examine th e degree ILP achieved somehow control dependences surmounted. study reported various degrees parallelism achieved various numbers control dependences overcome. number co ntel dependences ojercomejs unlimited, limit ILP around^ LJhjsjitudy high- lights srrpmjjjjfluenc^ol^^ limits ILP-MODERN PROCESSOR DESIGN 1.4.2.2 Fisher's Optimism. end spectrum study performed Nicolau Fisher 1984 Yale University. study hints-at almost unlim- r- ited amounts ILP many projgramsuThe benchmarks used study tend numerical, parallelisms measured due data parallelism .resulting array-type data ets. idealized machine model capable execut- ing many instructions simultaneously assumed. idealized assump- tions made study, present refreshing op timistic outlook amount ILP harvested agai nst pessimism due Flynn's botde- neck. W^jn&rjrnaUyj-efer purportedjimkon BLP Fisher's.optimisitL^ Initially optimism received great deal skepticism. number subsequent events somewhat vindicated study. First pr ototype machine model called VLIW (very lon g instruction word) processor developed along supporting compiler [Fisher, 1983]. Subsequently, commercial ven- ture (Multiflow, Inc.) formed develop realistic VLIW machine, resulted Multiflow TRACE computer. TRACE machines sup- ported powerful VLIW compiler employs trace scheduling (developed Josh Fisher et al.) extract instruction-level parallelism [Fisher, 1981] . Multiflow, Inc., reasonably successful eventually installed base 100 machines. importantly, short-lived commercial TRACE machines first g eneral-purpose uniprocessors achieve average IPC greater one. Although actual levels ILP achieved TRACE mach ines far less limits published earlier Nicolau Fisher 1984, substantiate claim significant amounts ILP harvested beyond previously accepted limit 2. 1.4.2.3 Contributing Factors. Many studies limits ED? employ different experimental approaches make different assumptions. Three key fac- tors contribute wide range experimental results: benchmark used, machine models assumed, compilation techniques employed. study adopts set benchmarks, frequently results ar e strongly influ- enced benchmarks chosen. Recently, th e Standard Performance Evaluation Corporation (SPEC) benchmark suites become widely adopted, manufacturers processors computing sys tems provide SPEC ratings systems. strict guidelines exist manufacturers report SPEC ratings products (see www.spec.org ), still quite nonuniform uses SPEC ratings researchers. also strong indications SPEC benchmark suites appropriate workstations running scientific engineering applications, relevan application domains commercial transaction processing embedded real-time computing. second key factor contributes confusion controversy limits ILP assumptions made various studies machine model. limit studies assume idealized machine models. example, cache memory usually considered assumed 100% hit rate one-cycle latency. models assume infinite-sized machines infinite register files. Usually one-cycle latency assume operation functionalPROCESSOR DESIGN unit types. studies em ploy realistic machine models, usually resulted pessimistic, possibly unnecessaril pessimistic, limits. course, also great deal nonuniformity th e instruction set architectures used. fictitious architectures, others use existing architectures. architectures used also tend strong influence ex perimental results.__ Finally, assumption made compilation techniques used quite diverse. Many studies include consideration com- piler; others assume infinitely powerful compilers. Fre quently, studies based dynamic traces collected real machines. Simulation results b ased traces dependent benchmarks architectures chosen, also strongly dep endent compilers used generate object code. potential contribution compilation techniques limits ILP ongoing area research. currently significant gap assumed capabilities all-powerful compilers capabilities existing commer- cially available compilers. Many anticipate many advancements expected compilation domain. Probably safesLQ?nd.usiCT d^ far ja. real fimitof ILP beyond chieved current-machines. room better research. Theji sjumptionof specific limit likely premature._As powerful efficient microarchitectures designed aggressive cor npilation techniques develo ped, previously made assumptions may changed previously purported limits may adjusted upward* . 1.4.3 Machines Instruction-Level Parallelism Instruction-level parallelismjfr referred fine-grained parallelism relative forms coarse-grained parallelism involving conc urrent processing multiple program fragments computing tasks. Machines designed exploit- ing general ILP referred ILP machines typically uniprocessors machine resource parallelisms functional unit level. classification ILP machines wa presented Norm Jouppi 1989 [J ouppi Wall, 1989]. ILP machines classified according number parameters.  Operation latency (OL). number machine cycles result instruction available use subsequent instruction. reference instruction used simple instruction tha typifies instructions instruction set. operation latency number machine cycles required th e execution instruction.  Machine parallelism (MP). maximum number simultaneously execut- ing instructions ifiaehine support Informally, maximum number ins tructions simultaneously flight pipeline one time.  IssuelatencyJlL). number machine cycles required issuing two consecutive instructions. reference instructions simpleMODERN PROCESSOR DESIGN instructions. present context, issuing means initiating new j instruction pipeline.  Issue parallelism (IP). maximum number instructions /. issued every machine cycle. Jouppi's classification, scalar pipelined" processor used baseline machine. classification also uses generic four-stage instruction pipeline illustration. stage 1. (instruction fetch) 2. DE (instruction decode) 3. EX (execute) 4. WB (write back) TheJjX stage used reference determination operation latency. scalar pipelined prc >CIAS^JJSE _Las thg. baseline machine, defineB _uXJ2g]a^ machme'with OL - 1 cycle 11 =1 cycle. baseline machine, instruc- tion processing proffie^iffiSrratgd inFigure 1.9, issue one new instruction frejMpehne inevery_cycje, aJyj?jcannsrmctiori requke&oae rnachmecydejor jts-ex«etttro!wThe ccOTespondm^JVlPi^egualjQ.t, th e number stages jnthe pipe- line; Figure 1.9 MP = 4. IP equal one instruction per cycle. Notice four par ameters static parameters machine take account dynamic behavior depends program executed. discuss performance speedup ILP machines, baseline, machine used reference. Earlier chapter referred peedup obtained pipelined processor sequential nonpipe- lined processor overlap processing multiple instructions. form speedup restricted comparison within domain scalar processors focuses increased throughput obtained (s calar) pipelined processor respect (s calar) nonpipelined processor. Beginning Chapter 3, DE EX WB 0 1 2 3 4 5 6 Time cycles (of baseline machine) ) Figure 1.9 Instruction Processing Profile Baseline Scalar Pipelined Machine.PROCESSOR DESIGN deals ILP machines, form speedup r eferred performance ILP machine compared scalar pipelined processor, used new reference machine. 1.43.1 Superpipelined Machines. superpipelined machine defined respect baseline machine machine higher degrees pipelining baseline machine. superpipelined machine, machine cycle time shorter baseline machine referred nurXoFcyde time. cycle time superpipelined machine 1/m baseluRrcycle time, equivalently minor cycles baseline cycle. superpipelined machine characterized OL = 1 cycle = minor cycles EL = 1 minor cycle. words, smipteuTstflKdiQnJStil^ pyrte, pqiial minor cycles, executioruJ )utjtfae^rnachine issue neaLinstmction evejyjninQLCjjc^ 1 instmction/minor cycle = instructions/ cycle, MP = x k. instruction processing profile superpipelined machine shown Figure 1.10. superpipelined machine pipelined machine whi ch degree pipe- lining beyond dictated operation latency simple instructions. Essentially superpipelining involves pipelining execution stage multi- ple stages. "underpipelined" machine cannot issue instructions fast executed. hand, superpipelined machine issues instructions faster executed. superpipelined machine degree m, is, one takes miner cycles execute simple operation, potentially achieve better performance baseline machine factor m. Techjiicjily, traditional pipelined computers require multiple cycles e xecuting simple operaticms_shquld classified ~aT5Up^ip1gJfflS3rFor example, latency per- forming fixed -point addltionisthree cycles CDC 6600 [Thornton, 1964] CRAY-1 [Russell, 1978], new instructions issued every cycle. Hence, really superpipelined chines. way, classification superpipelined machines somewhat artifi- cial, depends choice baseline cycle definition simple operation. key characteristic superpipelined machine 1$ 1 3L t~~~i < N R DE EX WB _l l_ Figure 1.10 Instruction Processing Profile Superpipelined Machine Degree = 3 30 MODERN PROCESSOR DESIGN resultof. inst ""*tinn available thff npxt m. - instructions. Hence, iuEerj^eiiried _p»x«s_oL^^ cessorjwith restrictions placement forwarding,paths. stan- dard pipelined processor, implicit assumption tntf sources destinations forwarding paths outputs inputs, respectively, pipeline stages. superpipelined machjneis viewed deeply pipelined machine x k stages, ouydtte stages can- aecessgd fOTw aidiiig inputs ,the _stages_cannot receive forwardedjdata. reason ^HraTsome ope rations require multiple minoFcycles multiple pipeline stages complete prim- itive operations, sense noninterruptible purpose data forwarding. really key dist inction pipelined superpipe- lined machines. book, outside section, special treatment superpipelined machines separate class processors distinct pipe- lined machines. 64-bit MIPS R4000 processor one first processors claimed "superpipelined" Internally, R4000 eight physical stages pipeline, shown Figure 1.11, physical machine cycle time 10 nanoseconds (ns) [Bashteen et al., 1991, Mirapuri etal., 1992]. However, chip requires 50- MHz clock input on-chip clock doubter. Consequently, R4000 uses 20 ns baseline cycle, considered superpipelined degree 2 respect four-stage baseline achine 20-ns cycle time. two miner cycles every baseline cycle. case R4000, multicycl e primitive operations cache access operations. example, first two physical stages (IF IS) required perform I-cache access, imilarly DF DS physical stages required D-cache access. noninterruptible operations; data for- warding involve buffers stages buffers DF DS stages. Cache accesses, considered "simple" operations, pipe- lined require operation latency two (minor) cycles. issue latency entire pipeline one (minor) cycle; is, one new instruction issued every One clock c ycle "(20 ns 50 MHz) Instruction fetch first (IF)Instruction fetch second (IS) Instruction cache Address translationRegister fetch (RF) r Decode Register fileExecute (EX) ALU Address additionData first (DF)Data second (DS) Data cache Address translationTag check (TC)Writeback (WB) Tag check| Register file | Figure 1.11 "Superpipelined" MIPS R4000 8-Stage PipelinePROCESSOR DESIGN 10 ns. Potentially R4000 achieve speedup baseline four-stage pipe- line factor 2. 1.43.2 Superscalar Mac hines. Superscalar machines extensions^of hasfilinejga}ar pipelined marJTingKjmrljrre^^ = 1 cycle,T17= T^cle^and TP = ji4nsrnicriorxs/cyj^je^TjwjTiaf^nj» cycle the, base- line cycle; minor cycles. simple operation executed one cycle. every cycle, multiple instructions issued. superscalar degree deter- mj g g j by_tJjejssua parallelism nj&» maximum number oflnsTfucuons issued every cycle. nstruction processing profile superscalar machine illustrated Figure 1.12. Compared scalar pipelined processor, supersca- lar machine o£degree_ttcaube_viewed hayirigji_pipelines pipeline n times wider sense able carry n instructions pipeline stage instead one. superscalar machine MP = n xk.lt shown superpipelined machine superscalar machine degree machine parallelism achieve roughly level performance. reason superscalar machine canno also superpipelined. issue latency reduced 1/m (baseline) cycle maintaining issue parallelism n instructions every (minor) cycle. total issue paral- lelism throughput n x instructions per (baseline) cycle. resultant machine parallelism become MP = n x X k, n superscalar degree, superpipelined degree, k degree pipelining baseline machine. Alternatively machine parallelism viewed MP = n x (m x k), representing superscal ar machine x k pipeline stages. machine equivalently viewed mor e deeply pipelined processor x k stages wit h superscalar degree n, without invoke tedious term "superscalar-superpipelined" machine; won't, 1.4.33 Very-Long-Instruction-Word Machines. Quite similar supersca- lar machines another class ILP machines called VLIW (very long instruction word) machines Josh Fisher [Fisher, 1983]. intent performance objec- tives similar fo r two classes machines; key difference lies 4 5 64 5 64 5 6 7 8 98 9 DE EX WB Figure 1.12 -V-t' Instruction Processing Profile Superscalar Machine Degree n= 3.MODERN PROCESSOR DESIGN w DE WB EX -tiit tFigure 1.13 Instruction Processing Profile VLIW Machine Degree n = 3. placement dynamic-static inte rface (DSI) partitionin g done run time vi hardware mechanisms done c ompile time via soft- ware means. instruction proce ssing profile VLIW mac hine illustrated Figure 1.13. 4T~" Unlike su perscalar machine, J ? DE stages VLIW machine need replicated su pport simultaneous processing, is, fetching decoding, n separate instructions. uperscalar machine, decision n instructions issued execute stage made run time. VLIW machine, instruction-issuing decision made compile time, n instructions simultaneously issued execute st age deter- mined compiler stored appropriately th e program memory Jong instru ction word. Superscalar VLIW machines represent two different approaches ultimate goal, achieving high processor performance via instruction-level parallel processing. two approaches evolved different historical paths different perspectives. suggested two approaches quit e synergistic, strong motivation pursuing potential integration two approaches. book focuses dynamic tech- niques implemented th e microarchitecture; hence, address depth VLIW features rely ag gressive compile-time techniques. 1.5 Summary Microprocessors unparalleled impact computer industry. changes taken place lifetime microprocessors (30+ years) phenomenal. Microprocessors entering fourth decade. fascinating speculate expect microprocessors coming decade. Although fad pasf decades, instruction set architecture (ISA) design longer ver interesting topic. learned great deal design elegant scalable ISA. However, code compatibility soft- ware installed base crucial determining longevity ISA. amply shown ISA deficiency overcome microarchitecturePROCESSOR DESIGN techniques. Furthermore, emergence portable bytecodes dynamic just-in-time (JIT) compilation, mea ning ISA consequent placement dynamic-static interface ( DSI) become quite blurred. corning decade, microarchitecture action is. chip integration density approaches 1 billion transistors die, many traditional (macro)archJtecture features, memory subsy stem, multiple processors, input/outpu subsystem , become on-die issues hence become effec- tively microarchitecture issues. Traditional system-level architecture become part chip-level design. expect see integration multiple proces- sors, cache memory hierarchy, main memory controller (arid possibly even main memory), input/output devices, network interface devices one chip. expect see many new innovative microarchitecture techniques. approach possibly exceed 10-GHz clocking speed, need rethink many fundamentals microarchitecture design. simple ALU operation may take multiple cycles. sophisticated branch predictor require 10 cycles . Main memory latency 1000+ cycles long. may take tens clock cycles traverse entire die. currently think aggressive pipelining viewed rather elementary. Future microprocessors become single-chip computing systems need exploit various forms parallelism. hese systems need go beyond instruction-level parallelism harvest thread-level parallelis (TLP) work- load. Perhaps important pursuit memory-level parallelism (MLP) able process many simultaneous memory acce sses. main mem- cry latency becomes three orders magnitude slower CPU c ycle time, need find clever ways trading memory bandwidth mitigate severe negative impact long memory lat ency overall performance. main c hallenge become mov ement data, operations p erformed data. REFERENCES Acosta, R., J. Kilestrup, H. Tomg: "An instruction issuing approach enhancing per- formance multiple f unctional unit processo rs," IEEE Trans, Computers, C35,9,1986, pp. 815-828. Agerwala, T., J. Cocke: "High performance reduced instruction set processors," Tech- nical report, D3M Computer Science, 1987. Amdahl, G.: "Validity single processor approach achieving large scale computing capabilities," AFIPS Con}. Proc, 1967, pp. 483-485. Amdahl, G., G. Blaauw, F. P. Brooks, Jr.: "Architecture IBM System/360," IBM Journal Research Development, 8,1964, pp. 87-101. Bashteen, A., I. Lui, J. Mullan: "A superpipeline approach MIPS architecture." Proc. COMPCON Spring 91, 1991, pp. 325 -333. Blaauw, G., F. P. Brooks, Jr.: Computer Architecture: Concepts Evolution. Read- ing, MA: Addison-Wesley, 1997. Black, B., J. P. Shen: "Calibration microprocessor performance models," Computer, 31, 5,1998, pp. 59-65.MODERN PROCESSOR DESIGN Butler, M., T.-Y. Yeh, Y. Patt, M. Alsup, H. Scales, M. Shebanow: "Instruction level parallelism greater two," Proc. 18th Int. Symposium Computer Architecture, 1991, pp. 276-286. Colwell, R.. C. Hitchcock, E. Jensen, H. B. Sprunt, C. Kollar: "Instructions sets beyond: computers, complexity, controversy," IEEE Computer, 18,9,1985, pp. 8-19. Fisher, J.: 'Trace scheduling: technique global microcode compaction. IEEE Trans, Computers." C-30,7, 1981, pp. 478-490. Fisher, J. A.: "Very long instruction word architectures ELI-512," Technical Report YLU 253, Yale University, 1983. Flynn, M, L. Hoevel: "Execution architecture: DELtran experiment," IEEE Trans, Computers, C-32,2,1983, pp. 156-175. Johnson, M: Superscalar Microprocessor Design. Englewood Cliffs, NJ: Prentice Hall, 1991. Jouppi, N. P., D. W. Wall: "Available instruction-level parallelism superscalar superpipelined machines," Proc. Third Int. Conf. Architectural Support Program- ming Languages Operating Systems (ASPLOS-llI), 1989, pp. 272-282. Kuck, D., Y. Muraoka, S. Chen: "On number operations simultaneously execut- able Fortran-like programs resulting speedup," IEEE Trans, Computers, C-21, 1972, pp. 1293-1310. Melvin, S., Y. Pan: "Exploiting f ine-grained parallelism combination hardware software techniques," Proc. 18th Int. Symposium Computer Architecture, 1991, pp. 287-296. Melvin, S. W., Y. Patt: "A clarification dynamic/static interface," Proc. 20m Annual Hawaii Int. Conf. System Sciences, 1987, pp. 218-226. Mirapuri, S., M. Woodacre, N. Vasseghi: "The MIPS R4000 processor," IEEE Micro, 12, 2, 1992, pp. 10-22. Nicolau, A., J. Fisher: "Measuring parallelism available fo r long instruction word architectures," IEEE Transactions Computers, C-33, 1984, pp. 968-976. Riseman, E. M., C. C. Foster: "The inhibition potential parallelism conditional jumps," IEEE Transactions Computers, 1972, pp. 1405-1411. Russell, R. M.: "The Cray-1 Computer System," Communications ACM, 21,1,1978, pp. 63-72. Smith, M. D., M. Johnson, M. A. Horowitz: "Limits multiple instruction issue," Proc. Third Int. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS-III), 1989, pp. 290-302. Sodani, A., G. S. Sohi: "Dynamic instruction reuse," Proc. 24th Annual Int. Sympo- sium Computer Architecture. 1997, pp. 194-205. Sohi, G, S. Vajapeyam: "Instruction issue logic high-performance, interrupt! ble pipelined processors," Proc. 14th Annual Int. Symposium Computer Architecture, 1987, pp. 27-34. Stone, H: High-Performance Computer Architecture. Reading, MA: Addison-Wesley, 1987. Thornton, J. E.: "Parallel operation Control Data 6600," AFIPS Proc. FJCC, part 2. 26, 1964, pp. 33-40. Tjaden, G, M. Flynn: "Representation concurrency ordering matrices," IEEE Trans, Computers, C-22, 8, 1973, pp. 752-761.PROCESSOR DESIGC Tjaden, G. S., M. J. Flynn: "Detection parallel execution inde pendent instruc- tions," IEEE Transactions Computers, C19,10,1970, pp. 889-895. Uht, A., R. Wedig: " Hardware extraction low-level concurrency serial instruction stream," Proc. Int. Conf. Parallel Processing, 1986, pp. 729—736. WaU, D.: "Limits instruction-level parallelism," Proc. 4th Int. Conf. Architectural Support Programming Languages Operating Systems, 1991, pp. 176-188. Wedig, R : Detection pf Co ncurrency Directly Executed Language Instruction treams. PhD thesis, Stan ford University, 1982. Weiss, S., J. Smith: "Instruction issue logic pipelined upercomputers," Proc. 11th Annual Symposium Computer Architecture, 1984, pp. 110-118. HOMEWORK PROBLEMS Pl.l Using resources World Wide Web, list top five reported benchmark results SPECINT2000, SPECFP2000, TPC-C. P1.2 Graph SPECTNT2000 vs. processor frequency two different pro- cessor families (e.g., AMD Athlon HP PA-RISC) many fre- quencies posted www.spec.org . Comment performance scaling wit h frequency, pointing anomalies suggesting possible explanations them. P1.3 Explain differences architecture, implementation, realization. Explain relates processor performance expressed Equation (1.1). P1.4 sili con technolog evolves, implementation constraints tradeoffs change, affect placement definition dynamic- static interface (DSI). Explain architecting branch delay slot [as millions instructions per second (MI PS) architecture] reasonable thing architecture introduced, less attr active today. P1.5 Many times, implementation issues particular generation end determining tradeoffs instructio n set architecture. Discuss least one historical implementation constraint explains CISC instruction sets sensible choice 1970s. P1.6 program's run time determined th e product instructions per program, cycles per instruction, clock frequency. Assume fol- lowing instruction mix MlPS-like RISC ins truction set: 15% stores, 25% loads, 15% branches, 35% integer arithmetic, 5% integer shift, 5% integer multiply. Given load instructions require tw cycles, branches require four cycles, integer ALU instmctions require one cycle, integer multiplies require ten cycles , compute overall CPI. P1.7 Given parameters Problem 6, consider strength-reducing opti- mization tha converts multiplies compile-time constant MODERN PROCESSOR DESIGN sequence shifts adds. instruction mix, 50% multiplies converted shift-add sequences average length three instructions. Assuming fixed frequency, compute change instruc- tions per program, cycles per instruction, overall program speedup. PI .8 Recent processors like Pentium 4 processors implement single- cycle shifts. Given scenario Problem 7, assume 5 = 50% additional instructions troduced strength reduction shifts, shifts take four cycles execute. Recompute cycles per instruction overall program speedup. strength reduction still good optimization? P1.9 Given assumptions Problem 8, solve break-even ratio (percentage additional instructions shifts). is, find value (if any) program performance identical baseline case without strength reduction (Problem 6). P1.10 Given assumptions Pro blem 8, assume designing shift unit Pentium 4 proc essor. concluded two possible implementation options shift unit: four-cycle shift latency frequency 2 GHz, two-cycle shift latency 1.9 GHz. Assume rest pipeline could run 2 GHz, hence two- cycle shifter would set entire processor's frequ ency 1.9 GHz. option provide better overall performance? Pl.ll Using Amdahl's law, compute speedups program 85% vec- torizable system 4, 8,16, 32 processors. would reasonable number processors build system running application? PI.12 Using Amd ahl's law, compute speedups program 98% vec- torizable system 16, 64, 256, 1024 proce ssors. would reasonable number processors build system running application? P1.13 Replot graph Figure 1.8 page 23 ILP limits shown list studies Section 1.4.2. conclusions draw graphs created? P1.14 Compare contrast two ILP limit studies reading rele- vant papers explaining limits different: Jouppi Wall [1989] vs. Wall [1991]. P1.15 1995, IBM AS/400 line computers tra nsitioned CISC instruction set RISC instruction set. simpler instruc- tion set, realizable clock frequency given technology generation CPI metric improved dramatically. H owever, rea- son, number instructions per program also increase noticeably. Given following parameters, compute total performancePROCESSOR DESIGN improvement occurred transition. Furthermore, compute break-even clock frequency, break-even cycles per instruction, break-even code expansion ratios trans ition, assuming two factors held constant. Performance FactorAS/400 CISC0MP1J (Actual)AS/400 RISC (PowerPC) (Actual)Actual RatioBrea var Rath Relative frequency 50MHZ 125 MHz 2.5 7 Cycles per instruction 7 3 0.43 7 Relative instructions per 1000 3300 3.3 7 program (dynamic count) P1.16 MIPS "(millions instructions per second) commonly used gauge computer system performance 1980s. Explain poor measure processor's perform ance. circumstances valid easure performance? so, describe circumstances. P1.17 MFLOPS (millions f loating-point operations per second) com- monly used gauge computer system performance 1980s. Explain poor measure processor's perfor- mance. circumstances valid measure performance? so, describe circumstances. Terms Buzzwords problems similar "Jeopardy Game" TV. answers shown provide best correct questions. answer may one appropriate question; need provide best one. P1.18 A: Instruction-level parallelism within basic block typically upper bounded 2. Q: ? P1.19 A: significantly reduce machine cycle time, increase branc h penalty. Q: ? P1.20 Describes speedup achievable fraction program execution parallelizable. Q: ? MODERN PROCESSOR DESIGN P1.21 A: widely used solution F lynn's bottleneck. Q: P1.22 A: best way describe computer system's performance. Q: . ? P1.23 A: specifies number registers, available ad dressing modes, instruction opcodes. Q: ? PI.24 A: determ ines processor's configuration number func- tional units. Q: ? P1.25 A: type processor relies heavily compiler stat- ically schedule independent instructions. Q: ? P1.26 A: type processor results instructions avail- able two cycles instruction begins execution. Q: ? P1.27 A: type processor attempts execute one instruction time. Q: ? P1.28 A: important study showed instruction-level parallelism abundant, control dependences could somehow overcome. Q: ? P1.29 A: type processor executes high-level languages with- aid compiler. Q: ? P1.30 A: approach processor simulation requires substantial storage space. Q: ? CHAPTI 1 Pipelined Processors CHAPTER OUTLINE 21 Pipelining Fundamentals 22 Pipelined Processor Design 23' Deeply Pipelined Proce ssors 24 Summary References ., ->* » Homework Problemst*r Pipelining powerful implementation technique enhancing system through- put without requiring massive replication hardware. wa first employed early 1960s design hig h-end mainframes. Instruction pipelining first introduced IBM 7030, nicknamed Stretch computer [Bloch, 1959, Bucholtz, 1962]. Later CDC 6600 inc orporated pipelining use multiple functional units [Thornton, 1964]. 1980s, pipelining became cornerstone RISC approach processor design. techniques cons tituted RISC approach directly indirectly related objective efficient pipelining. Since then, pipelining effectively applied CISC processors well. Intel i486 first pipelined implementation IA32 architecture [Crawford, 1990]. Pipelined versions Digital's VAX Motorola's M68K architectures also quit e successful commercially. Pipelining technique widely employed design instruc- tion set processors. chapter focuses design (scalar) pipelined proces- sors. Many approaches techniques related design pipelined processors, pipeline interlock mechanisms hazard detectio n resolu- tion, foundational design superscalar processors.MODERN PROCESSOR DESIGN J current trend toward deep pipelines. Pipeline depth increased \ less 10 20. Deep pipelines necessary achievin g \ high clock frequencies. effective means gaining greater \ processor performance. indications trend continue. 2.1 Pipelining Fundamentals section presents motivations fundamental principles pipelining. Historically there_ar£4wpjriajgrjtypesj^^ tionjnpelines. instruction p ipelines th e focus book, begin examining arithmetic pipeline example. Ar ithmetic pipelines readily illus- trate set idealized assumptions underlying principles pipelined designs.We term dealized assumptions pipelining idealism. dealing discrepancy idealized assumptions realisti c considerations instruction pipe lining makes pipelined processor design interesting. 2.1.1 Pipelined Design subsection introduces foundational notions pipelined design. moti- vations limitations pipelining presented. theoretical model, proposed Peter Kog ge, optimal pipelining hardware design perspective described [Kogge, 1981]. 2.1.1.1 Motivation*. r^f^ary_jnntivatifm- fnr pipg^i^j ^^jprrpajw ttw throughput system little increase hardware. throughput, band- widtnTorXsysteTnTsTneasured terms ffieTfiumber tasks performed per unit time, characterizes performance system. Egr system that-oner- atcK nn one task_at time, throughput P equalJp^L/D^where fijsdjeJateney of^tasJcjor delay associated wit h performance tas k system. throughput system increased pipelinin g many tasks require use system. actual latency task still remains may even increase slightly. Pjp^tinhogjnvoJyesjiartitipm^ syjrtern intp mujtir4e ^tages_wjthadded buffering stages/These stages interstage buffers constitute pipeline. computation carried original system decomposed k subcomputations, carried the_fcstages pipeline. new task start pipeline soon previous task traversed first stage. Hence, instead initiating_ane>g task every units time^ new task initiated every JD/it units time, k number stages the^rjipejine^and processing k caax^itBSai^hJao^S^e^'^^^Xi ^ pipeline. assumed original latency evenly partitioned intolt stages addi- tional delay introduce added buffers. Given total number tasks processed ver large, throughput pipelined system poten- tially approach k times nonpipelined system. Thi potential performance increase factor k simply adding new buffers it-stage pipeline primary attraction pipelined design. Figure 2.1 illustrates potential it-fold increase throughput ait-stage pipelined system.PIPELINED PROCESSORS Combinational logic n gate delay Gate delay — > - L *• | Gate delay —»- L I—» j Gate delay —»- j—» j Gate delay — - L I— - j Gate delay Figure 2.1 Potential fc-Fold Increase Throughput ((-Stage Pipelined System.BW 4 Figure 2.2 Earle Latch Incorporation Logic Without Incurring Additional Gate Delay: (a) Earle Latch Following Combinational Logic; (b) Earle Latch Integrated Combinational Logic. far hayej^umed thajLtte-additioaof interstage buffers intr oduce, anyadHftional delay/Ihis unrealistic. EarleTatch shown Figure 2.2(a) designed used IBM 360/91 buflermg r ietweenstages^of carry-save adders pipelined multiply unit. Earle latch, output Z follows input clock C = 1. clock goes low, value latched Z latching loop, output Z becomes insensitive changes D. Proper hold time required input ensure proper latching. middle gate ensures glitch-free operation; product term represented gate "covers" potential hazard. hazard spurious pulse caused42 MODERN PROCESSOR DESIGN rby race condition involving simultaneous change multiple signals. top bottom inputs gate potentially change simultaneously opposite directions. condition, gate middle (redun- dant) input, spurious pulse (the hazard) potentially appear output gate. Earle latch desirable glitch-free operation feature. Further- more, Earle latch integrated logic function incur additional gate delay. Figure 2.2(b) illustrates latching function merged last two AND-OR levels combinational logic circuit resulting additional gate delay addition latch. Th£jdrcuiLu^gure^2(b) performs logic function tha Figure 2.2(a) withoutJiicunirig-two additional gate delays latching. Tlieincrease_of gate fanrin by_one _jwrLslighfly^ increase delay gates. 2.1.13 Limitations. Since performance gained pipelined design pro- portional depth, is, number stages, pipeline, might seem best design is^lways maximize number stages apipelinerisys- terii. However, due clocking constraints, physical liirutations_to^how finely original computation partitioned rjhpelirie_stages. stage pipeline viewed piece combinational logic F fol- lowed set latches L. Signals must propagate F latched L. Let TM maximum propagation delay F, is, delay long- est signal path; let T„ minimum propagation delay F, is, delay sh ortest signal path. Let TL additional time needed proper clocking. Delay TL include necessary setup hold times ensure proper latching, well potential clock skews, is, worst-case disparity arrival times clock edge different latches. first set signals Xt applied inputs stage time Tu outputs F must valid T, + TM. proper latching L, signals outputs F must continue valid Tt + TM+ TL. second set signals X2 applied inputs F time T2, takes least T2 + Tm effects felt latches L. ensure second set signals overrun first set, required n+Tm>Tt + Tu + TL (2.1) means earliest possible arrival X2 latches must sooner time required proper latching Xt. inequality rewritten >TM-Tm + T, (2.2) T2 - T, effectively minimum clocking period T. Therefore, clock- ing period must ^ejreaterjhMiJ ^X^±I^-and maximum clocking rate cannoT exceed 1/7". Based foregoing analysis, two factors limit clocking rate. One difference maximum mmunmrijroj^atioirjlelays l^crSamely, TM- Tm. TneotHer additional time required properclocking,PIPELINED P ROCESSORS namely, TL. first factor eliminated signal propagation paths fenguiTThis accomplished bypijSSrng short paths. Hence, Tu — T„ close zero. second factor dictated need latch results pipe- line stages. Proper latehing requi res propagation signal feedback loop stabilizing signal value loop. Another contribution TL worst-case clock skew. clock signal may arrive different latches slightly different times due generation distribution clock signals latches. fully synchronous system, worst-case clock skew must accounted clockin g period. Ultimately, th e limit deeply synnhrr>nr »i<; system pipelined determined minimum time required [atoning uncertainty associated delays clock distribution networks J 2.1.13 Tradeoff. Clocking constraints determine th e ultimate physical limit thedepth pipelining. Aside thi limit, maximumpipeline"o^ °Ptlnlal^ejsh^when cos t^or pipelining overhead, considered. hard- ware design pipelined sys tem, tradeoff cost performance must considered cost/performance tr adeoff model pipelined design proposed Peter Kogge summarized [Kogge, 1 981]. Mo dels cost r^rformance ar e proposed. cost nonpipelined design denoted G. cost terms gate count, ransistor count, silicon real estate. cost C *-stage pipelined design equal C = G + k X L (2.3) k number stages pipeline,^, cost adding latch. and.61isjh^cpstjf^ Based cost model, pipeline cost C linear fu nction *, depth pipeline. Basically, cost pipeline goes linearly respect depth pipeline. Assume latency nonpipelined system T. performance ~1 nonpipelined design 1/7", computation rate. performance P pipelined design modeled 1/(77* + S), latency original nonpipelined design delay due addition latch. Assuming original latency evenly divided k stages, (77* + S) delay associated stage thus clocking period pipeline. Consequently, 1/(77* + S) equal clocking rate thro ughput pipelined design. Hence, performance pipelined design P~ (T/k + S) (2,4) Jf> Note P nonlinear function *. Given models cost performance, expression cost/ performance ratio <=p=<^± (2.5) . (.T/k + S) MODERN PROCESSOR DESIGN , xlu4 _l 1 L. 10 20 30 Pipeline depth k 40 50 Figure 2.3 Cost/Performance Tradeoff Model Pipelined Designs. expression rewritten X XA Mp E- = LT+GS + LSk + — P k (2.6) plotted Figure 2.3 two sets sample values G, L, T, S. Equation (2.6) expresses cost/performance ratio function k. first derivative taken set eq ual zero determine value k pro- duce minimal cost/performance ratio. value it, show n Equation (2.7), optimal pipelin ing depth terms parameters;, (2.7) Given expression optimal value it, pipelined design k < km considered underpipelined pip elining increasing pipeline depth beneficial increased cost justified increase performance. ther hand, k > kopl indicates overpipelined design diminishing return performance th e increased cost pipelining. foregoing tradeoff model based purely hardware design considerations; consideration dynamic behavior pipeline computations performed. take issues later.TJegirffling Section 2.Z 2.1 J2 Arithmeti c Pipeline Example two major types pipelines: arithmetic pipelines instru ction pipe- lines. Although instructio n pipeline design focus chapter, begin looking arithmetic pipeline example. Arithmetic pipelines clearlyPIPELINED PROCESSOR illustrate effectiveness pipelining without deal complex issue involved instruction pipeline design. complex issues addressed subsequ ent sections chapter. 2.1.2.1 Floating-Point Multiplication. design pipelined floating-point multiplier used th e example. "vintage" board-level design taken classic text Shlomo Waser Mike Flynn [Waser Flynn, 1982]. (Even though design assumes 1980 technology, nonetheless still serves effective vehicle illustrate arithmetic pipelining.) design ass umes 64-bit floating- point format uses excess-128 notation exponent e (8 bits) sign-magnitude fraction notation hidden bit mant issa (57 bits, including hidden bit). floating-point multiplication algorithm implemented design follows. 1. Check see operand zero. is, result immediately set zero. 2. Add two characteristics (phys ical bit patterns ex ponents) correct excess-128 bias, is, et + (e2 -128). 3. Perform fixed-point multiplication two mant issas m, m2. 4. Normalize product mantissas, whic h involves shifting left one bit decrementing exponent 1. (The normalized representation ofthe mantissa leading zeros.) 5. Round result adding 1 first guard bit (the bit immediately right least-significant bit mantissa). effectively rounding up. mantissa overflows, mantissa must shifted right one bit expon ent incremented 1 maintain normalized representation ntissa. Figure 2.4 llustrates functional block diagram nonpipelined design floating-point multiplier. input latches store two operands multiplied. next clock product two operands stored output latches. fixed-point mantissa multiplier represents complex module -in design consists three submodules partial product generation, partial product reduction, final duction. hardware complexity, terms number tegrated circuit'(IC) chips, propagation delay, nanoseconds, submodule obtained.  Partial product generation. Simultaneous generation partial products performed using 8 x 8 hardware multipliers. generate partial products, 34 8 x 8 multipliers n eeded. delay involved 125 ns.  Partial product reduction. partial products generated, must reduced summed summing circuit called (5, 5, 4) counter used reduce two columns 5 bits 4-bit sum. (5,5,4) counter implemented using 1K X 4 read-only memory (ROM) withMODERN PROCESSOR DESIGN \*l8 8 Add/Sub | 56 56 x-7Fixed-point mantissa multiplier Normalize Add/Sub RCTmrBng^j 1'1 8156 Figure 2.4 Nonpipelined Floating-Point Multiplier. Waser Flynn, 1982. delay 50 ns. Three levels (5, 5,4) counters needed reduce partial products. Hence total 72 u c h lKx4 ROMs needed, incurring total delay 150 ns.  Final reduction. partial products reduced two partial products final l evel reduction implemented using fast carry-lookahead (CLA) adders produce final result. Sixteen 4-bit adder chips CLA plus five 4-bit CL units needed fo r final reduction step. total 21IC chips 55-ns delay required. Two additional modules needed mantissa section, namely, shifter performing n ormalization (2 chips, 20-ns delay ) incrementer perform- ing rounding (15 chips, 50-ns delay). Add/Sub modules exponent sec- tion require another 4 chips; delays unimportant critical delay path. additional 17 10 chips needed implementing input output latches, respectively. total chip counts critical delays modules nonpipelined design summarized Table 2.1. ~ Based tabulation Table 2.1, nonpipelined design die floating- point multiplier r equires 175 chips clocked 2.5 MHz clock period 400 ns. Thi implies nonpipelined design achieve through- put 2.5 MFLOPS (mil lion floating-poin operations per second). p 2.133 Pipelined Floating-Point Multiplier. nonpipelined design L- floating-point multiplier pipelined increase throughput. example, assume pipelining within submodule; tha is, finest __granularity partitioning pipeline stages submodul e level. J examine delays associated (sub)modules critical delay path. delays shown th e third column Table 2.1. partial productPIPELINED PROCESS) Table 2.1 Chip counts critical delays modules nonpipelined floating-point multiplier de sign. Module Chip Count Delay, ns Partial product generation 34 125 Partial product reduction 72 150 Final reduction 21 55 Normalization 2 20 Rounding 15 50 Exponent section 4 Input latches 1 7 Output latches 10 Total 175 400 Source Waser Flynn, 1982. reduction submodule th e longest delay, 150 ns; delay det ermines delay stage pipeline. five (sub)modules critical path par- titioned three fairly even stages delays 125 ns (partial product genera- tion), 150 ns (partial product reduction), 125 ns (final reduction, normalization, rounding). resultant three-stage pipelined design shown Figure 2.5. determining actual clocking rate th e pipelined design, must con- sider clocking requirements. Assuming edge-triggered registers used buffering pipeline st ages, must add clock-edge-to-register-output delay 17 ns setup time 5 ns stage delay 150 ns. r esults minimum clocking period 172 ns. Therefore, instead clocking rate 2.5 MHz, new pipelined design clocked rate 5.8 MHz. represents factor 2.3 increase throughput. Note, however, f\ latency pe rforming multiplicatio n increased slightly, 400 | 516 ns. ^ \-J additional hardware required pipelined design edge- triggered register chips buffering pipeline stages. top original 175 IC chips, additional 82 IC ch ips required. Using chip count measure hardware complexity, total 257 IC chips represents increase 45% terms hardware complexity. 45% increase hardware cost resulted 130% increase pe rformance. Clearly, three-stage pipelined design floating-point multiplier win original nonpipelined design. example assumes board-level implementations nsing ff-the-shelf parts. Given today's chip technology , entire design easily implemented small module chip. board-level implementation floating-point multiplier may viewed outdated, purpose example succinctly illustrate effectiveness pipelining using published specific design actual latency hardware cost parameters. fact, per curve Figure 2.3 reflects parameters example.MODERN PROCESSOR DESIGN Add/Sub T"rH ** \ PP generation 1125 ns PP reduction 1150 ns Final reduction 55 ns Add/SubT Normalize 20 ns Rmmding ^^J 50 ns Clock Figure 2.5 Pipelined Floating-Point Multiplier. Source Waser Flynn, 1982. 2.1.3 Pipelining Idealism Recall motivation it- stage pipelined design achieve Mold increase throughput, illustrated Figure 2.1. However, forejjainft~~j example, th e three-stage pipelined floating-point multiplier achieved factor v 2.3 increase throughput. main reason falling short three-fold increase throughput it-fold increase throughput it-stage pipe- lined design represents ideal case based three idealized assumptions, referred pipelining idealism. understanding pipelining_J idealism crucial appreciation pipelined designs. unavoidable devi- ations idealism real pipelines make pipelined designs challenging. solutions dealing idealism-realism gap comprise interesting tech- niques pipelined designs. three points pipelining idealism 1. Uniform subcomputations. computation performed evenly partitioned uniform-latency subcomputations, 2. Identical computations. computation performe repeat- edly large number input data sets. 3. Independent computations. repetitions computation mutually independent.PIPELINED PROCESSORS 2.13.1 Uniform Subcomputations. first point pipelining idealism states computation pipelined evenly partitioned uni form- latency subcomputations. Thi means original design evenly partitioned k balanced (i.e., latency) pipeline stages. latency original computation, hence clocking period nonpipe- lined design, T, clocking period it-stage pipelined design exactly T/k, latency k stages. Given thi idealized assumption, Mold increase throughput achieved due it-fold increase clocking rate. idealized assumption may true actual pipelined design. may possible partition computation perfectly balanced stages. see floating-p oint multiplier example latency 400 ns original computation partitioned three stages latencies 123, ISO, 123 ns, respectively. Clearly original latency evenly partitioned three balanced stages!" Since clocking period pipelined design dic^ tated stage longest latency, stages shorter latencies effect incur inefficiency penalty.^n example, first third stages inefficiency 25 ns each; called inefficiency within pipe- line stages , internal fragmentation pipeline stages. ternal fragmentation,lhetotal latency required performing computation increase 7}, cloc king period pipelined design longer T/k Tf/k. example performance three subcomputa- tions require 450 ns instead original 400 ns, clocking period 133 ns (400/3 ns) 150 ns. secondary implicit assumption, namely, additional delay introduced introduction bu ffers pipeline stages addi- tional delay required ensuring proper clocking pipeline stages . Again, assumption may true actual designs. example, additional 22 ns required ensure proper clocking pipeline stages, resulted cycle time 172 ns three-stage pipelined design. ideal cycle time three-stage pipeline design would 133 ns. difference 172 133 ns clocking period accounts shortfall idealized three-fold increase throughput. first point pipelining idealism basically assumes two things: (1) inefficiency introduced due partitioning original computation multiple subcomputations; (2) additional delay caused intro- duction interstage buffers clocking requirements . chip-level design additional delay incurred proper pipeline clocking minimized employing latches similar Earle latch. partitioning computation balanced pipeline stages constitutes first challenge pipelined des ign. goal achieve stages balanced possible minimize internal fragmentation. Internal fragmentation due imperfectly balanced pipeline stages primary cause deviation first point pipelining idealism. deviation becomes form pipelining overhe ad leads shortfall idealized Mold increase throughput it-stage pipelined design.MODERN PROCESSOR DESIGN 2.13.2 Ident ical Com putations. second point pipelining idealism states many repetitions computation performed pipeline. computation repeated multiple sets input data; repetition requires sequence subcomputations provided pipeline stages. floating-point multiplier example, means many pairs floating- point numbers multiplied pair operands sent thre e pipeline stages. Basically assumption implies pipe- line stages used every repetition computation. certainly true example. assumptio n holds floating-point multiplier example pipeline performs one function, is, floating-point multiplication. pipeline designed perform multiple functions, assumption may hold. example, arithmetic pipeline designed perform addi- tion multiplication. multiple-function pipeline, pipeline stages may required functions supported pipeline. possible different subset pipeline stages required performing functions computation may require pipeline stages. Since sequence data sets traverses pipeline synchronous manner, data sets require pipeline stages effe ctively idling stages. unused idling pipeline stages introduce another form pipeline inefficiency ca n called external fragme ntation pipeline stages. Similar internal fragmentation, external fragmentation form pipelining overhead minimized multifunction pipe- lines. pipelined floating-point multiplier example, external fragmentation. second point pipelining idealism effectively assumes pipeline stages always ut ilized. Aside implication external frag- mentation, idealized assumption also implies many sets data processed. takes k cycles first data set reach last stage pipeline; cycles referred pipeline fill time. last data set entered first pipeline stage, additional k cycles needed drain pipeline. pipeline fill drain times, stages busy. main reason assumin g processing many sets input data pipeline fill drain times constitute small fr action total time. Hence, pipeline stages considered, practical purposes, always busy. fact, throughput 5.8 MFLOPS pipelined floating- point multiplier based assumption. 2.13.3 Independent Co mputations. third point pipelining idealism states repetitions computation, simply computations, pro- cessed pipeline independent. means computations concurrently resident pipeline stages ndependent, is, data co ntrol dependences pair computations. assumption permits pipeline operate "streaming" mode, later comp utationPIPELINED PROCE SSORS need wait completion earlier computation due dependence them. pipelined floating-point multiplier assumption holds. multiple pairs operands multiplied, multiplication pair operands depend result nother multiplication. hese pairs processed pipeline streaming mode. pipelines point may hold. late r computation may require result earlier computation. hese computations concurrently resident pipeline stages. later computation entered pipeline stage needs result earlier computation ached pipeline stage produces needed result, later computation must wait pipe- line stage. waitin g referred pipeline stall. computation stalled pipeline stage, subsequent computations may stalled well. Pipeline stalls effectively introduce idling pipeline stages, essentially dynamic form external fragmentation results reduction pipeline throughput. designing pipelines need process computations necessarily independent, goal produce pipeline design minimizes amount pipeline stalls. 2.1.4 Instruction Pip elining three points pipelining idealism three idealized assumptions pipe- lined designs. part, arithmetic pipelines reality far idealized assumptions . However, instruction pipelining gap realism idealism greater. br idging gap makes instruction pipelining interesting challenging. designing pipelined processors, three points become three ajor challenges. hese three challenges briefly introduced addressed depth Section 2.2 pipeline pro- cessor design. se three challenges also provide nice road map keeping track pipelined processor design techniques. 2.1 A1 Instruction Pipeline Design. three points pipelining idealism become objectives, desired goals, designing nstruction pipelines. processing instruction becomes computation pipelined. com- putation must partitioned equence fairly uniform subcomputations result fairly balanced pipeline stages. latency processing instruction referred instruction cycle; latency pipeline stage determines machine cycle. instruction cycle viewed logical concept specifies processing instruction. execution program many instructions involves repeated execution computation. machine cycle physical concept inv olves clocking storage elements digital logic circuits, essentially clocking period pipeline stages. view earlier floating-point multiplier example simple processor one instruction, namely, floatin g-point mul tiply. instructionMODERN PROCESSOR DESIGN ClockFloating-point multiply (a) Clock -^jPgeneratioiiJ 1 PP reduction 2 2j Final reduction normalize round Figure 2.6 Simple Illustration Instruction Cycle vs. MachineCyde. cycle involves performance floating-point multiply; se e Figure 2.6(a). computation naturally partitioned, based obvious functional unit boundaries, nto following five subcomputations. 1. Partial product generation (125 ns). 2. Partial product reduction (150 ns). 3. Final reduction (55 ns). 4. Normalization (20.ns). 5. Rounding (50 ns). purpose pipelining, grouped last three subcomputations one subcomputation. resulted three pipeline stages shown Figure 2.6(b). instruction cycle Figure 2.6(a) mapped three machine cycles Figure 2.6(b), resulting three-stage pipelined design. refer instruction cycle architected (logical) primitive specified instruction set architect ure, whereas machine cycle machine (physical) prim- itive specified microarc hitecture. pipelined design Figure 2.6(b) implementation architecture specified Figure 2.6(a). main task instruction pipelining stated mapping logi- cal instruction cycle physical machine cycles. words, computa- tion represented instruction cycle must partitioned sequence subcomputations carried pipelin e stages. perform mapping partitioning effectively, three points pipelining idealism must considered.PIPELINED PRO CESSORS Uniform Subcomputations. partitioning instruction cycle multi- ple machine cycles called stage quantization, shoul performed minimize internal fragmentation pipeline stages. care taken stage quantization, internal fragmentation introduced quickly undermine efficiency pipeline. first point p ipelining idealism leads first challenge instruction pipelining, namely, need balance pipeline stages. balanced pipeline stages are, less internal fragmentation. Identical Computations. Unlike single-function arithmetic pipeline, instruc- tion pipeline inherently multifunction pipeline, must able process different instruction types. Different instruction types require slightly different sequences subcomputations co nsequently differ ent hardware resources. second ch allenge instructio n pipelining involves efficient coalescing unify- ing different resource requirements different instruction types. pipeline must able support processing instruction types, minimizing unused idling pipeline stages instruction type. e ssentially equiva- lent minimizing external fragmentation. Independent Computations. Again, unlike arithme tic pipeline processes array data, instruction pipeline processes instructions necessarily independent one another. Hence, nstruction pipeline must built-in mechanisms detect th e occurrences dependences instructions ensure dependences violated. enfo rcing interinstruction dependences may incur penalties form pipelin e stalls. Recall pipeline stalls dynamic form external fragmentation reduces throughput pipeline. Therefore, third challenge instruction pi pelining mini- mizing pipeline stalls. 2.1.4.2 Instructi Set Architecture Impacts. address three major challenges instruction pipelining earnest, might enlightening briefly con- sider impacts instruction set architectures (ISAs) instruction pipelining. Again, three points p ipelining idealism considered turn. Uniform Subcomputations. first cha llenge balancing pipeline stages implies set uniform subcomputations must identified. Looking subcomputations involved processing instruction, one must identify one critical subcomputation requires longest latency can- easily partitioned nto multiple finer subcomputations. p ipelined processor design, one critical subcomputatio n accessing main mem- ory. disparity speed proce ssor main memory, memory accessing critical subcomputation. support efficient instruction pipelining, addres sing modes involve memory access minimized, fast cache memories keep processor speed employed.MODERN PROCESSOR DESIGN Identical Computations. second challenge unifying resource require- ments different instruction types one primary motivations RISC architectures. red ucing complexity diversity different instruction types, task unifying different struction types made easier. Complex addressing modes require additional accesses memory, also increase diversity resource requirements. unify resource requirements one inst ruction pipeline extremely difficult, resultant pipeline become inefficient many instructions less complex resource requirements. instructions would pay external fragmentation overhead underutilize stages pipeline. unifying instruction types pipelined implementation RISC architecture clean results efficient ins truction pipeline little external fragmentation. Independent Computations. third challenge minimizing pipeline stalls due interinstruction dependences probably fascinating area pipe- lined processor design. proper operation, instruction pipeline must detect enforce interinstruction depe ndences. Complex addr essing modes, es pecially involve memory accessing, make dependence detection diffi- cult due memory reference specifiers. general, register dependences easier check registers explic itly specified instruction. Clean symmetric instruction formats facilitate decoding instructions detection dependenc es. detection enforcement dependences done either statically compile time dynamically run time. decision compile time vs. run time involves defini- tion th e dynamic-static interface (DSI). placement DSI induces inter- esting subtle tradeoffs. tradeoffs highlight intimate relationship compilers (micro)architectures importance considering design processors. 2.2 Pipelined Processor Design designing nstruction pipelines pipelined processors, three points pipe- lining idealism manifest three primary design challenges. Dealing deviations idealized assump tions becomes primary task designing pipelined processors. three points pipelining idealism corresponding three primary challenges pipe lined processor design follows: 1. Uniform subcomputations => balancing pipeline stages 2. Identical computations => unifying instruction types 3. Independent computations => min imizing pipeline stalls three challenges addressed turn Subsections 2.2.1 2.2.3. three challenges provide nice framework presenting instruction pipelining techniques. pipelined processor design tec hniques viewed efforts addressing three challenges.PIPELINED PROCESSORS 2.2.1 Balancing Pipeline Stages pipelined processor design, computation pipelined work done eac h instruction cycle. typical instruction cycle functionally par- titioned f ollowing five generic subc omputations. 1. Instruction fetch (LF) 2. Instruction decode (ID) 3. Operand(s) fetch (OF) 4. Instruction execution (EX) 5. Operand store (OS) typical instruction cycle begins th e fetching next instruction executed, followed decoding th e instruction determine work performed instruction. Usually one operands speci- fied need fetched. operands reside registers mem- ory locations depending addressing modes used. Onc e necessary operands available, actual operation spe cified instruction per- formed. instruction cycle ends storing result produced specified operation. result stored register memory location, depending ad dressing mode specified. sequential processor, entire sequence subcomputations repeated next instruction. Dur- ing five generic subcomputations side effects also occur part execution instruction. Usually side effects take form certain modifications machin e state. changes machine state referred side effects effects necessarily explicitly specified instruction. implementation complexity resultant latency five generic subcomputations vary significantly depending actual ISA specified. 23.1.1 Stage Quantization. One natural partitioning instruction cycle pipelining based five generic subcomputations. five genericsubcomputations mapped pipeline stage, resulting five-stage instruction pipeline; see Figure 2.7. called example pipeline GENERIC (GNR) instruction pipeline. GNR pipeline, logical struction c ycle mapped five physical machine cycles. machin e cycles/instruction cycle ratio 5 reflects degree pipelinin g gives indication granu- larity pipeline stages. objective stage quantization partition instruction cycle bal- anced pipeline stages minimize internal fragmentation pipeline stages. Stage quantization begin th e natural functional partition instruction cycle, example, five generic subcomputations. Mult iple subcomputations short latencies grouped one new subcomputation achieve balanced stages. example, three subcomputations—final reduction, normal-ization, rounding—of floating-point multiplication computation grouped56 MODERN PROC ESSOR DESIGN 1. Instruction fetch 2. Instruction decode113 1 3. Operand fetch 4. Instruction execute 5. Operand storeOF EX OS Figure 2.7 Five-Stage GENERIC (GNR) Instruction Pipeline. T_Tinto one subcomputation pipelined design Figure 2.6(b). Similarly, five generic subcomputations typical instruction cycle grouped achieve balanced stages. example, instruction set architecture employs fixed instruction length, simple addr essing modes, rthogonal fields instruction format, ID subcomputations q uite str aight- forward relatively simple compared three subcomputations. two subcomputations potentially combined one new subcomputation, resulting four subcomputations balanced terms required latencies. Based four subcomputations four- stage instruction pipeline implemented; see Figure 2. 8(a). fact, combining ID subcompu- tations employed MIPS R2000/R3000 pipelined processors [Moussouris et al., 1986, Kane, 1987]. approach essentially uses subcomputation longest latency reference attempts group subcomputations shorter latencies new subcomputation comparable latency reference. result coarser-grained machine cycle lower degree pipelin ing. Instead combining subcomputations short latencies, opposite approach taken balance pipeline stages. given subcomputation extra-long latency partitioned multiple subcomputations sh orter latencies. approach uses subcomputation shortest latency ref- erence attempts subdivide long-latency subcomputations int many finer- grained subcomputations latencies comparable reference. result finer-grained machine cycle higher degree pipelining. example, ISA employs complex addressing modes may involv e accessing memory OS subcomputations, two subcomputations incur long latencies therefore subdivided multiple subcomputations.PIPELINED PROCESSORS ID EXro EX,rpl n=2 -n roi OFl OF2 OF3 ~r EXl EX2 osi OS29 10 (a)OS3 (b) Figure 2.8 (a) Four-iStJge Instruction Pipeline Example. (b) 11-Stage Instruction Pipeline Example. Additionally, operations performed EX subcomputation may quite complex subdivided multiple subcomputations well. Figure 2.8(b) illustrates instruction pipeline 11-stage esign. OS subcomputations mapped three pipeline stages, EX subcomputations mapped two pipeline stages. Essentially, ID subcomputation used reference achieve balanced stages. two methods presented stage quantization (1) merge multiple sub- computations one (2) subdivide subcomputation multiple subcom- putations. combination bot h methods also used design instruction pipeline. shown previous discussion, instruction set architecture significant impact stage quantization. cases, goal stage quantization minimize overall internal fragmentation. example, assume total latency five generi c subcomputations 280 ns resultant machine cycle times 4-stage design Figure 2.8(a) 11-stage design Figure 2.8(b) 80 30 ns, respectively. Consequently, the± 1 EXAMP -T1MODERN PROCESSOR DESIGN total latency 4-stage pipeline 320 ns (80 ns x 4) total latency 11-stage pipeline 330 ns (30 ns x 11). difference new total latency original total latency 280 ns represents internal fragmentation. Hence, internal fragmentation 4-stage design 40 ns (320 ns - 280 ns), internal fragmentation 11-stage design 50 ns (330 ns - 280 ns). concluded 4-stage design efficient 11-stage design terms incurring less overhead due internal fragmentation. course, 11-stage design yields throughput 9.3 (280 ns/30 ns) times non- pipelined design, 4-stage design's throughput 3.5 (280 ns/ 80 ns) times nonpipelined design. seen designs, internal fragmentation hind ered attainment idealized throughput increase factors 11 4 11-stage 4-stage pipelines, respectively. 2.2.1 J2 Hardware Requirements. realisti c engineering designs, goal simply achieve best possible performance, achieve best per- formance/cost ratio. Hence, addition simply maximizing throughput (performance) instruction pipeline, hardware requirements (cost) must con- sidered. general, higher degrees pipeli ning incur greater costs terms hardware req uirements. Clearly added cost due additional buffer- ing pipeline stages. already seen model presented Section 2.1.1.3 point beyond pipeli ning yields dimin- ishing returns due overhead buffering pipeline stag es. Besides buffering overhead, other, si gnificant , hardware require- ments highly pipelined designs. assessing hardware requirements instruction pipeline, first thing keep mind it-stage instruction pipeline, worst case, actually best case terms performance, k instructions concurrently present pipeline. instru ction resident pipeline stage, total instructions different phases instruction cycle. Hence, entir e pipeline must enough hardware support concurrent processing k instructions k pipeline stages. hardwar e requirements fall three categories: (1) logic required c ontrol data manipulation stage, (2) register-file ports support concu rrent register accessing multiple stages, (3) memory ports support concurrent memory accessing multiple stages. first examine four-st age instruction pipeline Figure 2.8(a). Assuming load/store architecture, typica l register-register instruction need read two register operands first stage store result back register fourth stage. load instruction need read memory second stage, store instruction need write memory fourth stage. Combining requirements four stages, register file two read ports one write port required, data memory interface capable performing one memory read one memory write every machine cycle required. addition, first stage needs read instruction memory every cycle instruction fetch. unified (i nstruction data) memory used, memory must able support two read accesses one write access every machine cy cle.PIPELINED PROCESSORS Similar analysis hardware requirements performed 11-stage instruction pipeline Figure 2.8(b). acco mmodate slow instruction memory, generic subcomputation subdivided mapped two pipeline stages, namely, IF1 IF2 stages. Instruction fetch initiated IF1 completes IF2. Even though instruction fetch takes two machine cycles, pipelined; is, f irst instructio n completing fetching IF2, second instruction begin fetching IF1. means instruction memory must able support two concurrent accesses, IF1 IF2 pipeline stages, every machine cycle. Similarly, mapping OS generic subcomputations three pipeline stages eac h implies one time could six instructions pip eline, process accessing data memory. Hence, data memory must able support six independent c oncurrent accesses with- conflict every machine cycle. potentially require six-ported data memory. Furthermore, instruction memory data memory unified one memory unit, eight-ported memory unit potentia lly required. multiported memory units extremely expensive implement. Less expensive lutions, using interleaved memory multiple banks, attempt simulate true multiported functionality usually cannot guarantee conflict- free concurrent ccesses times. degree pipelining, pipeline depth, increases, amount hardware resources needed sup port pipeline increases significantly. significant increases hardware resources additional ports register file(s) memory unit(s) needed support increase degree concurrent accesses data storage units. Furthermore, accommodate long memory access latency, memory access subcomputation must pipelined. However, physical pipelining memory accessing beyond tw machine cycles become quite complex, frequently conflict-free c oncurrent accesses must compromised. 2.2.1.3 Example instruction Pipelines. stage quantization two commer- cial pipelined processors presented pr ovide illustrations real instruction pipelines. MIPS R2OOO/R3O0O RISC processors employ five-stage instruc- tion pipeline, shown Figure 2.9(a). IPS architecture load/store architecture. ID generic subcomputations merged stage, require one memory (I-cache) read every chine cycle. generic subcomputation carried RD MEM stages. ALU instructions access register operands, operand fetch done RD stage requires reading two registers. load instructions, operand fetch also requi res accessing memory (D-cache) carried MEM stage, stage pipeline access D-cache. OS generic subcomputation carried MEM WB stages. Store instruc- tions must access D-cache done MEM stage. ALU load instructions write results back register file WB stage. MIPS processors normally employ separate instruction data caches. every machine cycle R2O00/R3O00 pipeline must support concurrent accesses of60 MODERN PROCESSOR DESIGN MIPS R2OOO/R30O0 ID EX osHIF RD ALU MEM WB (a)AMDAHL 470V/7 ID- EX os-41 PC GEN 1 1 r Cache Read 12T Cache Read 3 Decode | 4 Read REG35 Add GEN 6 Cache Read | 7 —r Cache Read ~|| 8 EX 1 J 9 EX 2 ^10 Check Result"^ 11  Write Result 12 1 ' (W Figure 2.9 Two Commercial Instruction Pipelines: (a) MIPS R2000/R3000 Five-Stage Pipeline; (b) AMDAHL 470V/7 12-Stage Pipeline. one I-cache read stage one D-cache read (for load nstruction) write (for store instruction) MEM stage. Note split cache con- figuration, I-cache D-cache need multiported. hartdr instructions data stored c ache, unified cache need dual-ported support pipeline. register file must provide adequate ports support two register reads RD stage one register write WB stage every machine cycle. Figure 2.9(b) illustrates 12-stage instruction pipeline AMDAHL 470V/7. generic subcomputation implemented first three stages. complex ad dressing modes that'must supported, generic subcomputation mapped four stages. EX OS generic subcompu- tations partitioned two pipeline stages. stage 1 12-stage pipeline, address next sequential instruction computed. Stage 2 initiates cache access read instruction; stage 3 loads instruction cache thePIPELINED PROCESSORS I-unit (i nstruction unit). Stage 4 decodes instruction. Two ge neral-purpose regis- ters read stage 5; regis ters used address registers. Stage 6 computes address operand memory. Stage 7 initiates cache access read memory operand; tage 8 loads operand cache I-unit also read register operands. Stages 9 10 two execute stages E-unit (execute unit). Stage 11 error checking performed computed result. final result stored destination register stage 12. 12-stage pipeline must support concurrent accesses two register reads stage 5 one register write stage 12 every machine cycle, along four cache memory reads stages 2, 3,7, 8 every machine cycle. memory su bsystem pipelined processor clearly much complicated MIPS R20O0/R3OO0 pipeline. current trend pipelined processor design toward higher degrees pipelining deeper pipeline depth. produces finer-grained pipelined stages clocked higher rates. four five stages common first- generation pipelined RISC processors, instruction pipelines ten stages becoming commonplace. also trend toward implementing multiple pipelines different numbers stages. subject superscalar processor design, addressed Chapter 4. 23.2 Unifying Instruction Types second point pi pelining idealism assumes computation performed repeatedly pipeline. instruction pipelines, idealized assumption repetition identical computations hold. instruc- tion pipeline repeatedly processes instructions, different types instruc- tions involved. Alth ough instruction cycle repeated over, repetitions instruction cycle may involve processing different instruction types. Different instruction types ifferent res ource requirements may require exact sequence subcomputations. instruction pipeline must able support different requirements must provide superset subcom- putations needed instruction types. instruction type may require pipeline stages instruction pip eline. instruction type, unnecessary pipeline stages become form inefficiency overhead instruction type; inefficiency overhead referred external frag- mentation pipeline Sect ion 2.1.3.2. Th e goal unifying instruction types, key challeng e resulting second point pipelining idealism, mini- mize external fragmentations al l instruction types. 2.2.2.1 Classification Instruction Types. perform computation, com- puter must three gen eric tasks: 1. Arithmetic operation 2. Data movement 3. Instruction sequencingMODERN PROCESSOR DESIGN three generic tasks carried processing instructions processor. arithmetic operation task involves performing arithmetic andlogical operations specified operands. obvious part perform- ing computation often equated computation. processor sup- port large variety arithmetic perations. data movement task responsible moving operands results storage locations. Typically hierarchy storage locations, ex plicit instructions used move data among th ese locations. instruction sequencing task responsible sequenc- ing instructions. Typically computation specified program consisting many instructions. performance computation involves processing sequence instructions. sequencing instructions, pro gram flow, explicitl specified instructions themselves. three generic tasks assigned various instructions ISA key component instruction set design. complex instruction specified actually performs three generic tasks. typical hori- zontally microcoded machine, every microinstruction fields used specify three generic tasks. traditional instruction set architec- tures known complex instruction set computer (CISC) architectures, many instructions c arry one three generic tasks. Influenced RISC research 1980s, recent instruction set architectures share common attributes. recent architectures include Hewlett-Packard's Precision architecture, IBM's P ower architecture, IBM/Motorola's PowerPC architecture, Digital's Alpha architecture. modern ISAs tend fixed-length instr uctions, symmetric instruction for- mats, load /store architectures, simple ddressing modes. attributes quite compatible inst ruction pipelining. part, book adopts assumes typical RISC architecture exa mples illustrations^ typical modem RISC architecture, ins truction set employs dedicated instruction type three generic tasks; instruction carries one three generic tasks. Based three generic tasks, instructions classified three types: 1. ALU instructions. performing arithmetic logical operations. 2. Load/store instructions. moving data registers memory locations. 3. Branch instructions. controlling instruction sequencing. ALU instructions perform ari thmetic logical operations strictly register operands. load store instructions access data memory. load/store branch instructions employ fairly simple addressing modes. Typically register-indirect offset ddressing mode supported. Often PC-relative addressing mode also supported branch instructions. following detailed speci fication three instruction types, use instruction cache (I-cache) data cache (D-cache) also assumed.PIPELINED PROCESSORS Table 2.2 Specification ALU instruction type ALU Instruction Type Generic Floating-Point Subcomputations integer Instruction Instruction Fetch instruction Fetch instruction (access I-cache). (access I-cache). ID Decode instruction. Decode instruction. 0F Access register file. Access FP register file." Ex Perform ALU operation. Perform FP operation. os Write back register file. Writeback FP r egister file. semantics three instruction types specified based sequence subcomputations performed tha instruction type. specifi- cation begin five generic subcomputations (Section 2.2.1) subse- quent refinements. Eventually, subcomputations specify sequence register transfers used hardware implementation. convenience, ALU instructions divided integer floating-point structions. semantics ALU instructions specified Table 2.2. load/store architecture, load store instructions instruc- tions access data memory. load instruction moves data memory location register; store instruction moves data register memory location. spe cification semantics load store instructions Table 2.3, assumed ad dressing mode register-i ndirect with,an offset. addressing mode computes effective address adding content register offset specified immediate field instruction. Comparing specifications Tables 2.2 2.3, observe sequences subcomputations required ALU lo ad/store instruction types similar exactly same. ALU instructions need generate memory addresses. hand, load/store instructions, generate effective address, perform explicit arithmetic logical opera- tions. simply move data reg isters memory locations. Even load store instructions subtle differences. load instruction, generic subcomputation expands three subcomputations involving accessing register file base address, generating effective address, accessing memory location. Similarly store instruction, OS generic subcomputation consists two subcomputations involving ge nerating effective ad dress storing register operand memory location. assumes base address register operand accessed register file generic subcomputation. Finally sequences subcomputations specify unconditional jump conditional branch instructions presented Tabl e 2.4. similarMODERN PROCESSOR DESIGN Table 23 Specification l oad/store instruction type Load/Store Instruction Type Generic - Subcomputations Load Instruction Store Instruction Fetch Instruction (access I-cache).Fetch instruction (access I-cache). ID Decode instruction. Decode instruction. Access register file (base address). Generate effective address (base + offset). Access (read) memory location (access D-cache).Access register file (register operand, base address). "EX OS Write back register file. Generate effective address (base + offset). Access (write) memory location (access D-cache). Table 2.4 Specification branch instruction type Generic Subcomputations ID EX OSBranch Instruction Type Jump (unconditional) Instruction Fetch instruction (access I-cache). Decode instruction. Access register file (base address). Generate effective address (base + offset). Update program counter target address.Conditional Branch Instruction Fetch instruction (access I-cache). Decode instruction. Access register file (base address). Generate effective address (base + offset). Evaluate branch condition. condition true, update program counter target address. addressing mode load/store instructions employed branch instructions. PC-relative addr essing mode also supported. address- ing mode, address target th e branch (or jump) instruction generated adding displacement current content program counter. TypicallyPIPELINED PROCESSOI displacement either p ositive negative value, fa cilitate for- ward backward branches. Examining specifications three major instruc tion types Tables 2.2 2.4, see initial subcomputations three types quite similar. However, differences later subcomputations. example, ALU instructions access data memory, hence memory address generation needed. hand, lo ad/store branch instruction types share required subcomput ation effective addr ess generation. Load/ store instructions must access data memory, branch instructions must provide address target instruction. also see conditional branch instruction, addition g enerating effective address, evaluation branch c ondition must performed. involve simply th e check- ing status bit generated earlier instruction, quire perfor- mance arithmetic operation register operand part processing branch instruction, involve checking value specified register. Based foregoing specifications instruction semantics, resource requirements three major instruction types determined. three instruction types hare commonality terms fetching decod- ing instructions, differences instruction types. differences instruction semantics lead differences resource requirements. 2333 Coalescing Resource Requirements. challenge unifying different instruction types involves efficient coalescing ifferent resource requirements one instruction pipeline accommodate instruction types. obj ective minimize total resources required pipeline time maximize utilization resources pipeline. procedure unifying different instruction types informally stated con- sisting following three steps. 1. Analyze sequence subcomputations instruction type, determine corresponding resource requirements. 2. Find commonality instruction types, merge common subcom- putations share pipeline stage. 3. there.exists flexibility, wit hout violating instruction semantics, shift reorder subcomputations facilitate merging.  procedure unifying instruction types illustrated applying instruction types specified Tables 2.2 2.4. simplicity c larity, floating- point instructions unconditional jumps c onsidered. Summary specifi- cations ALU, load, store, branch instruction types repeated Figure 2.10. four sequences subcomputations required four instruc- tion types taken Tables 2.2 2.4 summarized four columns left-hand side Figure 2.10. apply unifying procedure top down, examining four sequences^of subcomputations associatedMODERN PROCESSOR DESIGN ALU LOAD STORE BRANCH fetch inst. fetch inst. fetch inst. -fetch insi. \ update PC update PC update PC update PC / Figure 2.10 Unifying ALU, Load, Store, Branch Instruction Types Six-Stage Instruction Pipeline, Henceforth Identified TYPICAL (TYP) Instruction Pipeline. hardware resources required support them. procedure results definition stages instruction pipeline. four instruction types share common subcomputations ID. Hence, first two subcomputations four ins truction types eas- ily merged used define first two pipeline stages, labeled ID, instruction fetching instruction decoding. four instruction types also read register file generic subcomputation. AL U instructions access two register operands. Load branch instructions access register obtain base address. Store instructions access register obtai n register operand another register base address. four cases either one two registers read. similar sub- computations merged third stage th e pipeline, called RD, reading two registers register file. register file must capable supporting two independent concu rrent reads every machine cycle. ALU instructions require ALU functional unit perfor ming necessary arithmetic logical operations. load, store, branch instructions need perform operations, need generate effective address accessing memory. observed address generation task per- formed ALU functional unit. Hence, th ese subcomputations merged fourth stage pipeline, called ALU, consists primarily ALU functional unit performing arithmetic/logical operations effective address generation.PIPELINED PROCESSORS load store instruction types need access data memory. Hence pipeline stage must devoted subcomputation. fifth stage pipeline, labeled MEM, included purpose. ALU load instruction types must w rite result back register file last subcomputation. ALU instruction writes result opera- tion performed register operands destination register. load instruction loads destination register data fetched memory. memory access required ALU instruction; hence, writing back destination register theoretically take place immediately ALU stage. However, pur- pose unifying register write-back subcomputation load nstruction type, register write-back subcomputation ALU instructions delayed one pipeline stage takes place sixth pipeline stage, named WB. incurs one idle machine cycle ALU instructions MEM pipeline stage. form external fragmentation introduces inefficiency pipeline. conditional branch ins tructions, branch condition must determined prior updating program counter. Since ALU functional unit used perform effective address generation, cannot used perform branch con- dition evaluation. th e branch condition evaluation involves checking register determine equal zero, positive negative, simple comparator needed. comparator added, earliest pipeline stage added ALU stage, is, reference register read RD stage. Hence, earliest pipeline stage pro- gram counter u pdated branch target address, assuming condi- tional branch taken, MEM stage, is, target address computed branch condition determined ALU stage. foregoing co alescing resource requirements different instruction types resulted six-stage instruction pipeline shown right-hand side Figure 2.10. instruction pipeline dentified TYPICAL (TYP) instruction pipeline used remainder chapter illustra tion vehicle. one idling pipeline stage (MEM) ALU instructions, store branch instruction types also incur external fragmentation. store branch instructions need write back register idling WB stage. Overall six-stage instruction pipeline quite efficien Load instructions use six stages pipeline: three instruction types use five six stages. unifying different instruction types one instruction pipeline, three optimization objectives-. first minimize total resources required support instruction types. way, objective determine pipeline analogou least common multiple different resource require- ments. second objective maximize utiliza tion pipeline stages different instruction types, words, minimize idling stages incurred instruction type. Idling stages lead external fragmentation result inefficiency throughput penalty. third objective minimize overall latency instruction ty pes. Hence, idling stage unavoid- able part icular instruction type flexibility terms placement idling stage, always better place end pipeline. ThisMODERN PROCESSOR DESIGN allow instruction effectively complete earlier reduce overall latency instruction type. 2.233 Instruction Pip eline Implementation. six-stag e TYP instruction pipeline (Figure 2.10), potentially six ifferent instructions simulta- neously present "in flight" pipeline one time. six instructions going one pipeline stages. register file must sup- port two reads (by th e instruction RD stage) one write (by ins truction WB stage) every machine cycle. I-cache must support one read every machine cycle. Unless inte rrupted branch instruction, stage con- tinually increments program counter fetches next sequential instruction I-cache. D-cache must support one memory read memory write every machine cycle. MEM stage accesses th e D-cache; hence, time one instruction pipeline accessing data memory. pipeline diagram Figure 2.10 logical representation six- stage TYP instruction pipeline llustrates orderin g six pipeline stages. actual physical organization TYP instruction pipeline shown Figure 2.11, functional block diagram TYP pipelined proc essor Figure 2.11 Physical Organization th e Six-Stage TYP Instruction Pipeline.PIPELINED PROCESSORS implementation. diagram buffers pipeline stages explic- itly identified. logical buffer two particul ar pipeline stages actually involve multiple physical buffers distributed diagram. single logical path traverses six pipeline stages se quence actually involves multiple physical paths diagram. progre ssion instruction pipeline must traced along physical paths. physical organization six-stage TYP instruction pipeline Figure 2.11 looks complex really is. help digest it, first examine pipeline's interfaces register file memory subsystem. Assuming split cache organization, is, separate caches storing instruc- tions data, two single-ported caches, one I-cache one D-cache, needed. memory subsystem interface TYP pipeline quite simple efficient, resembles scalar pipelined processors. tage ccesses I-cache, MEM stage accesses D-cache, shown Figure 2.12. I-cache support fetch one instruction every machine cycle; miss I-cache stall pipeline. MEM stage pipeline, load (store) instruction performs read (write) (to) D-cache. Note assumed latency accessing D-cache, th e I-cache, within one machine cycle. caches become larger processor logic becomes deeply pipelined, maintaining one achine cycle latency caches become difficult. interface multiported register file hown Figure 2.13. RD WB stages access register file. every machine cycle, register ID | RDA r I-cache Memory ALU MEM WB TA r aD-cache Figure 2.12 Six-Stage TYP Instruction Pipeline's Interface Memory Subsystem.MODERN PROCESSOR DESIGN Figure 2.13 Six-Stage TYP Instruction Pipeline's Interface Multiported Register File. file must support potentially tw register reads RD stage one register write WB stage. Hence, multiported register file two read ports one write port required. register file illustrated Figure 2.13. three address ports, two data output ports, one data input port supporting two reads one write every machine cycle. instruction performing register write WB stage pr ecedes instruction th performing register reads RD stage hree machine cycles tervening instruc- tions. Consequently, three additional pipeline stage buffers register write address port ensure register write address specifying destination register written arrives register file write address port exactly time data written arriving input data port register file. Three-ported register files complex. However, number ports increases beyond three, hardware complexity increases rapidly. especially true increasing number write ports due circuit design limita- tions. Multiported register files 20 ports feasible found high-end microprocessors. look logical dia gram six-stage TYP instr uction pipeline Figure 2.10, appears every instruction flows single linear path six pipeline stages. However, different sets physical paths physical organization TYP instruction pipeline Figure 2.11 traversed different instruction types. flo w path segments labeled Figure 2.11 show pipeline stages associated with. Essentially pipeline stages physically distributed physical organization diagram pipeline.PIPELINED PROCESSORS six-stage TYP instruction pipeline quite similar two instruction pipelines, namely, MIPS R2000 /R3000 instructional DLX processor used popular textbook John Hennessy David Patterson [2003]. five-stage pipelines. MIPS pipeline combines ID stages TYP pipeline one pipeline stage. DLX pipeline combines ID RD stages TYP pipeline one pipeline stage. four stages essen- tially three pipelines. TYP pipeline used remainder chapter running example. 2 3 3 Minimizing Pipeline Stalls third point pipelinin g idealism sumes computations per- formed pipeline ar e mutually independent. fc-stage pipeline, k different computations going one time. instruction pipeline, k different instructions present flight pipeline one time. instructions may independent one another; fact, usu- ally dependences instructions flight. Ha ving independent instructions th e pipeline facilitates streaming pipeline; is, instruc- tions move th e pipeline without encountering pipeline stalls. inter-instruction dependences, must detected resolved. resolution dependences require stalling pipeline. chal- lenge design objective minimize pipeline stalls resultant throughput degradation. 23.3.1 Program Dependences Pipeline Hazards. ISA abstraction level, program specified sequence assembly language instructions. typical instruction specified function i: T<—SI op S2, domain instruction D(i) = {SL S2), range R(i) = {T), mapping domain range defined op, operation. Given two instructions j, j following lexical ordering two instruc- tions, data dependence ca n exist j, j data-dependent i, denoted ib j, one following three conditions exists. R(i)nD(j )*0 (2.8) RV)nD (i)*<3 (2.9) R(i)nR(j )*0 (2.10) first condition implies instruction j requires operand range instruction i. referred read-after-write (RAW) true data dependence denoted ibdj. implication true data dependence instruction j cannot begin execution instruction completes. second con- dition indicates operand required range j, instruction j modify variable operand i. referred write- after-read (WAR) anti data dependence denoted ibj. existence anti-dependence requires instruction j complete prior execution of72 MODERN PROCESSOR DESIGN instruction i; otherwise, instruction get wrong operand. third condi- tion indicates instructions j share common variable range, meaning modify variable. referred write- after-write (WAW) output data dependence denoted ibj. existence output dependence require instruction j complete com- pletion instruction i; otherwise, instructions subsequent j variable domains receive wrong operand. Clearly, read-after- read case involves instructions j accessing sam e operand harmless regardless relative order two accesses. three possible ways domains ranges two instructions over- lap induce three types possible dat dependences two instructions, namely, true (RAW), (WAR), output (WAW) data dependences. Since, assembly code, domains ranges instructions variables residing either registers memory locations, common variable dependence involve either register memory locat ion. refer register depen- dences memory dependences. chapter focus primarily register depen- dences. Figure 2.1 4 illustrates RAW, WAR, WAW register data dependences. data dependences, control dependence exist two instructions. Give n instructions j, j following i,j control-dependent i, denoted ibj, whether instruction j executed depends outcome execution instruction i. Control dependences consequences control flow structure pro gram. conditional branch instruction causes uncertainty instruction sequencing. Instructions following conditional branch control dependences branch instruction. asse mbly language program consists sequence instructions. semantics program assume depend th e sequential execution instructions. sequential listing instructions implies sequential prece- dence adjacent instructions. instruction followed instruction +1 program listing, assumed first instruction executed, instruction + 1 executed. uch sequential execution followed, semantic correctness program gua ranteed. prec ise, since instruction cycle nvolve multiple subcomputations , implicit assumption subcomputations instruction carried  True dependence K5 -  Anti-dependence R,  Output dependence fl3 «- fl, op R2 Rs «- R, op RtR, op R2 Read-after-write (RAW) "ft3 op R4 ,Rt op R2 Write-after-read (WAR) R4 op R5 cWrite-after-write (WAW) Rt op R1 Figure 2.14 Illustration RAW, WAR, WAW Data Dependences.PIPELINED PROCESSORS subcomputations instruction + 1 begin. called total sequential execution program; is, subcomputations sequence instructions carried sequentially. Given pipelined processor k pipeline stages, proc essing k instruc- tions overlapped pipeline. soon instruction finishes first sub- computation begins second subcomputation , instruction + 1 begins first subcomputation. k subcomputations, corresponding k pipeline stages, particular instruction overla pped subcomputations instructions. Hence, total sequential execution hold. total sequential execu- tion sufficient ensure semantic correctness, necessary requirement semantic correctness. total sequential execution mplied sequential listing instructions overspecification semantics program. essential requirement ensuring program semantics violated inter-instruction de pendences violated. words, exists dependence two instructions j, j following pro- gram listing, reading/writing common variable instructions j must occur original sequential order. pipelined processors, care taken, potential program dependences violated. Suc h poten- tial violations program dependences called pipeline hazards. pipeline hazards must detected resolved correct program execution. 23.33 Identification Pipeline Hazards. instruction types unified instruc tion pipeline functionality th e pipeline stages defined, analysis instruction pipeline performed identify pipeline hazards occur pipeline. Pipeline hazards consequences organization pipeline inter-instruction dependences. focus chapter scalar instruction pipelines. definiti on, scalar instruction pipeline single pipeline multiple pipeline stages organized linear sequential der. Instructions enter pipeline according sequential order specified program listing. Except pipeline stalls oc cur, instructions flow thr ough scalar instruction pipeline lockstep f ashion; is, instruction advances next pipeline stage every machine cycle. sca- lar instruction pipelines, necessary conditions pipeline organization occurrence pipeline hazards due data dependences determined. pipeline hazard potential violation program dependence. Pipeline hazards classified according type program dependence involved. WAW hazard potential violation output dependence. WAR hazard potential violation anti-dependence. RAW hazard potential viola- tion tru e data dependence. data dependence involves reading and/or writing common variable two instructions. hazard occur, must exist least two pipeline stages pipeline contain two instructions simultaneously access common variable. Figure 2.15 illustrates necessary conditions pipeline organization occurrence WAW, WAR, RAW hazards. necessary conditions apply hazards caused memory register data dependences (only registerMODERN PROCESSOR DESIGN 1 i: R„ Register write 11 Register write (a) WAW HazardRegisterwriteJ «— ^RegisterreadJ 7 1r Register read i: Rk «— Register write (b) WAR Hazard (c) RAW Hazard Figure 2.15 Necessary Conditions Pipeline Organization Occurrence (a) W W Hazards, (b) WAR Hazards, (c) RAW Hazards. dependences illustrated figure). order WAW hazard occur due output dependence i&J, must exist least two pipeline stages per- form two simultaneous writes common variable; see Figure 2.15(a). one stage pipeline write variable, hazard occur wri tes—in fact writes—to variable performe pipeline stage according original sequential order specified program listing. Figure 2.15(b) specifies order WAR hazard occur, must exist least two stages pipeline, earlier stage x later stage y, stage jc write variable stage read variable. order anti-dependence ibj violated, instruction j must perform write, is, reach stage x, prior instruction performing read reaching stage y. necessary condition hold, impossible instruction j, trailing inst ruc- tion, perform write prior instruction completing read. example, exists one pipeline stage perform read write variable, accesses variable done original sequential order, hence WAR hazard occur. case stage performing read earlier pipeline stage p erforming write, th e leading instruc- tion must complete read trailing instruction j possibly perform write later stage pipeline. Again, WAR hazard occur pipeline. actuality, necessary conditions presented Figure 2.15 also suf- ficient conditions considered c haracterizing conditions occur- rence WAW, WAR, RAW pipeline hazards. Figure 2.15(c) specifies order RAW hazard occur due true data dependence iSj, must exist two pipeline stages x y, x occur- ring earlier pipeline y, stage x perform read stage perform write common variable. pipeline organization, dependence ibj violated trai ling instruction j reaches stage x prior leading instruction reaching stage y. Arguments similar used WAR hazards applied show necessary condition hold, RAW hazard occur. example, one pipeline stage performs allPIPELINED PROCESSORS reads writes, effectively tota l sequential execution carried hazard occur. stage performing read positioned later pipeline stage p erforming write, RAW hazards never occur; reason writes leading instructions complete trailing instructions perform reads. Since pipeline hazards caused potential violations program depen- dences, systematic procedure identifying pipeline hazards occur instruction pipeline formulated considering dependence type turn. specific procedure employed chapter examines program dependences fo llowing order. 1. Memory data dependence a. Output dependence b. Anti-dependence c. True data dependence 2. Register data dependence a. Output dependence b. Anti-dependencec. True data dependence 3. Control dependence illustrate procedure applying ix-stage TYP instruction pipeline. First, memory data dependences conside red. memory data depen- dence involves common variable stored memory accessed (e ither read write) two instructions. Given load /store architecture , memory data depen- dences occur load/store instructions. determine whether pipeline hazards occur due memory data dependences, p rocessing load/store instructions pipeline must examined. Assuming split cache design, TYP pipeline, MEM tage access D-cache. Hence, accessing memory locations loa d/store instructions must occur MEM stage; one stage pipeline performs reads writes data memory. Based necessary conditio ns presented Figure 2.15 pipeline h azards due memory data de pendences occur TYP pipeline. Essentially, accesses data memory performed sequen- tially, processing load/store instructions done total sequen- tial execution mode. Therefore, TYP pipeline, pipeline hazards due memory data dep endences. Register data dependences considered next. determine pipeline hazards occur due register data dependences, pipeline stages access register file must identified. TYP pipeline, register reads occur RD stage register writes occur WB stage. output (WAW) dependence, denoted ib0j, indicates instruction subsequent instruc- tion j share destination register. enforce output dependence, instruction must write register first; instruction j write register. TYP pipeline, WB stage perform writes theMODERN PROCESSOR DESIGN register file. Consequently, register writes performed sequential order WB stage; according n ecessary condition Figure 2.15(a), pipe- line hazards due tp output dependences occur TYP pipeline. (WAR) dependence, denoted ibj, indicates instruction reading r egister destinatio n register subsequent instruction j. must ensured instruction reads register instruction j writes register. way anti-dependence cause pipeline hazar trailing instruction j perform register write earlier instruction per- form register read. impossibility TYP pipeline regis- ter reads occur RD stage, earlier pipeline WB stage, tage register writes occur. Hence, necessary cd nidition Figure 2.15(b) exist TYP pipeline. Consequendy, pipeline haz- ards due anti-dependences occur TYP pipeline. type register data dependences cause pipeline hazards TYP pipeline true data dependences. necessary condition Figure 2.15(c) exists TY P pipeline pipeline stage RD performs register reads positioned earlier pipeline WB stag e performs register writes. true data dependence, denoted involves instruction writing register trailing instruction j reading register. instruc- tion j immediately follow instruction i, j reaches RD stage, instru ction still ALU stage. Hence, j cannot read register operand result instruction reaches WB stage. enforce data dep endence, instruction j must prevented entering RD stage instruction completed WB stage. RAW pipeline hazards occur for, true data dependences trailing instruction reach register read stage pipeline prior leading ins truction completing register write stage pipeline. Finally, control dependences considered. Control dependences involve con- trol flow changing ins tructions, namely, conditional branch instructions. outcome conditional branch instruction determines whether next nstruction fetched next sequential ins truction target conditiona l branch instruc- tion. Essentially two candidate instructions follow conditional branch. instruction pipeline, normal operation, ins truction f etch stage uses content program counter fetch next instruction, incre- ments content program counter point next sequential instruction. task repeated every machine cycle instruction fetch stage keep pipeline filled. conditional branch instruction fetched, potential disruption sequential flow ca n occur. conditional branch taken, con- tinued fetching instruction fetch stage next sequential instruction cor- rect. However, conditional branch actually taken, th e fetching next sequential instruction th e instruction fetch stage incorrect. problem ambiguity cannot resolved th e condition branching known. control dependence viewed form register data (RAW) depen- dence involving program counter (PC). conditional branch struction writes PC, whereas fetching next instruction involves reading PC. conditional branch instruction updates PC address targetPIPELINED PROCESSORS instruction branch taken; otherwise, PC updated address next sequential instruction. TYP pipeline, updating PC target struction address performed MEM stage, whereas stage uses content PC fetch next instruction. Hence, stage per- forms reads PC register, MEM stage occurs later pipe- line performs writes PC register. ordering MEM stages, according Figure 2.15(c), satisfies necessary condition occurrence RAW hazards involving PC register. Therefore, control hazard exists TYP pipeline, viewed form RAW hazard involving PC. 2.2.3.3 Resolution Pipeline Hazards. Given organization TYP pipeline, type pipeline hazards due data dependences occur RAW hazards. addition, pipeline hazards due control dependences occur. hazards involve leading instruction writes reg ister (or PC) trailing instruction j reads r egister. presence pipeline hazards, mechanisms must provided resolve hazards, is, ensure corresponding dat dependences violated. regard RAW hazard TYP pipeline, must ensured read occurs write common register, hazard register. resolve RAW hazard, trailing ins truction j must prevented entering pipeline stage hazard register read j, leading instruction traversed pipeline stage hazard register written i. accomplished stalling earlier stages pipeline, namely stages p rior th e stage performing register read, thus preventing instruction j entering critical register read tage. number machine cycles instruction_/" must held back is, worst case, equal distance two critical stages pipeline, is, th e stages per forming read write hazard register. case TYP pipeline, leading instruction either ALU load instruction, critical register write stage WB stage critical r egister read stage fo r trailing instruction types RD stage. distance two critical stages three cycles; hence, worst-case penalty three cycles, shown Table 2.5. worst-case penalty incurred Table 2.5 Worst-case penalties due RAW hazards TYP pipeline .^mba otrr_w« .4Ufa Trailing instruction types (j) Hazard register Register write stage (/) Register read stage (J) RAW distance penaltyMl ALU ALU, Load/Store, Br. Int register (Ri) WB (stage 6) RD (stage 3) 3 cyclesLoad ALU, Load/Store, Br. Int. register (RD WB (stage 6) RD (stage 3) 3 cyclesBranch ALU, Load/Store, Br. PC E (stage 5) (stage 1) 4 cyclesMODERN PROCESSOR DESIGN instruction j immediately follows instruction original program listing; is, j equal + 1. case, instruction j must stalled three cycles ID stage allowed enter RD stage three cycles later instruction / exits WB stage. trailing instruction j immediately follow instruc- tion i, is, intervening instructions j, penalty less three cycles. assumed intervening instructions depend instruction i. actual number penalty cycles incurred thus equal 3 - s, 5 number intervening ins tructions. example, three instructions /' j, penalty cycle incurred. case, instruction j entering RD stage instruction exiting WB stage, st alling required satisfy RAW dependence. control hazards, leading instruction branch instruction, updates PC MEM stage. fetching trailing instruction j requires reading PC stage. distance two stages four cycles; hence, worst-case penalty four cycles. conditional branch instruction encountered, fetching instructions stopped st alling stage un til conditional branch instruction completes MEM stage PC updated branch target address. requi res stalling stage four c ycles. analysis reveals stalling necessary conditional branch actually taken. urns conditional branch taken, stage could continued fetching next sequen- tial instructions. fea ture included pipeline design, follow- ing conditional branch instruction, instruction fetching stalled. Effectively, pipeline assumes branch taken. event branch taken, PC updated branch target MEM stage instructions residing earlier pipeline stages deleted, flushed, next instruction fetched branch target. design, four-cycle penalty incurred conditional branch actually taken, penally cycle otherwise. Similar RAW hazards due register data dependence, fo ur-cycle pen- alty incurred control hazard viewed wor st-case penalty. instructions control-dependent instruction inserted instruction instruction j, control-dependent instruction, actual number penalty cycles incurred reduced number instructions inserted. concept delayed branches. Essentially penalty cycles filled useful instructions must execute regardless whether conditional branch taken. actual number penalty cycles 4 - 5, number control-independent instructions inserted instructions j. Delayed branches filling penalty cycles due branches makes difficult implement earlier technique assuming branch taken allowing stage fetch sequential p ath. reason mechanisms must provided distinguish filled instruc- tions actual normal sequential instructi ons. event branch actually taken, filled instructions need deleted, normal sequential instructions must deleted executed.PIPELINED PROCESSORS 233.4 Penalty Reduction via Forwarding Paths. far implicitly assumed mechanism available dealing hazard resolution stall dependent trailing instruction ensure writing reading hazard register done normal sequential order. ggressive techniques available actual implementation pipeline help reduce penalty cycles incurred pipeline hazards. One technique involves incorporation forwarding paths pipeline. respect pipeline haza rds, leading instruction instruction trailing instruction j depends. RAW hazards, ins truction j needs result instruction operand . Figure 2.16 illustrates pr ocessing ' leading instruction case ALU instruction load instruction. leading instruction ALU instruction, result needed instruction j actually produced ALU stag e available instruction com- pletes ALU stage. words, operand needed instr uction j actu- ally available output ALU stage instruction exits ALU stage, j need wait two cycles exit WB stage. output ALU stage made available input side ALU stage via physical forwarding path, trailing instruction j allowed enter ALU stage soon leading instruction leaves ALU stage. case, instruction j need access dependent operand reading register file RD stage; instead, ca n obtain dependent operand accessing output ALU stage. addition forwarding path associated control logic, th e worst-case penalty incurred zero cycles leading instruction ALU instruction. Even trailing instruction instruction + 1, stalling needed ins truction + 1 enter ALU stage instruc- tion leaves ALU stage normal pipeline operation. 1 cycle J JO cycle MEM WB14 cycles Branch Figure 2.16 Incorporation F orwarding Paths TYP Pipeline Reduce ALU Load Penalties.MODERN PROCESSOR DESIGN case leading instruction load instruction rather ALU instruction, similar forwarding path inc orporated reduce penalty cycles incurred due leading load instruction dependent trailing instruc- tion. Examining Figure 2.16 reveals leading instruction load instruc- tion, result load instruction, is, content memory location loaded register, available th e output MEM stage load instruction completes th e MEM stage. gain, forwarding path added output MEM stage input ALU stage support requirement trailing instruction. trailing instruction enter ALU stage soon leading load instruction completes MEM stage. effec- tively reduces wor st-case penalty due leading load struction three cycles one cycle. worst case, dependent instruction instruction + 1 , i.e.,y = + 1 . normal pipeline processing instruction ALU stage, instruction +1 RD stage. instruction advances MEM stage, instruction + 1 must held back RD stage via stalling earlier stages pipeline. ever, next cycl e instruc- tion exits MEM stage, forwarding path output MEM stage input ALU stage, instruction + 1 allowed enter ALU stage. effect, nstruction + 1 stalled one cycle RD stage; hence worst-case penalty one cycle. incorporation forward- ing paths worst-case penalties RAW hazards reduced shown Table 2.6. penalty due RAW hazard ALU instruction leading instruction ferred ALU penalty. Similarly, penalty due leading load instruction referred load penalty. TYP pipeline, for- warding paths added, ALU penalty zero cycles. effect, leading instruction ALU instruction, penalty incurred. Note th e source forwarding path output ALU stage, earliest point result instruction available. Th e destination forwarding path input ALU stage, latest point result instruction able 2.6 'orst-case penalties due RA W hazards TYP pipeline forwarding paths used LMdlfff Jnrtmctlon Type [i] ALU j Load Branch Trailing instruction types (j) ALU, Load/Store, Br. ALU, Load/Store, Br. ALU, Load/Store, Br. Hazard register Int. register (Ri) Int. register (Ri) PC Register write stage (/) WB (stage 6) WB ( stage 6) E (stage S) Register read stage (J) RD (stage 3) RD (stage 3) (stage 1) Forward outputs of: ALU, MEM. WB MEM, WB MEM Forward input of. ALU Penalty w/ forwarding paths 0 cyclesALU 1 cycle 4 cyclesPIPELINED PROCESSORS needed instruction j. forwarding path earliest point result available latest point result needed dependent instruction termed criti- cal forwarding path, represents best done terms reducing hazard penalty type leading instruction. addition critical* forwarding path, additional forwarding paths needed. example, forwarding paths needed start outputs MEM WB stages end input ALU stage. hese tw additional forwarding paths needed dependent instruction j could potentially instruction + 2 instruction + 3. j = + 2, instruction j ready enter ALU stage, instruction exiting MEM stage. Hence, result instruction i, still written back destination register needed instruction j, available output MEM stage must forwarded input ALU stage allow instruction j enter stage next cycle. Similarly, j = + 3, result instruction must forwarded output WB stage input ALU stage. case, although instruction completed write back destination register, instruction j already traversed RD stage ready enter ALU stage. course, case j = + 4, RAW dependence easily satisfied via normal reading register file j without requiring use forwarding path. time j reaches RD stage, completed WB stage. leading instruction load instruction, earliest point result instruction available output MEM stage, latest point result needed input ALU stage. Hence critical forwarding path leading load instruction output MEM stage input ALU stage. represents best done, case incurring one cycle penalty unavoidable. Again, another forward- ing path output WB stage input ALU stage needed case dependent trailing instruction ready enter ALU stage instruction exiting WB stage. Table 2.6 indicate forwarding path use reduce penalty due branch instruction. leading instruction branch instruction given addressing mode assumed TYP pipeline, earliest point result available'is output MEM stage. branch instructions, branch target address branch condition genera ted ALU stage. MEM stage th e branch condition checked target ddress branch loaded PC. Consequently, MEM stage PC used fetch branch ta rget. hand, PC must avail- able beginning LF stage allow fetching next instruction. Hence latest point w result needed beginning stage. result critical forwarding path, best done, current penalty path updating PC branch target stage start- ing fetching branch target next cycle branch taken. If, how- ever, branch co ndition generated early enough ALU stage allow updating PC branch target address toward end ALU stage, case branch penalty reduced four cycles three cycles.82 MODERN PROCESSOR DESIGN 2333 Implementation Pipeline Interlock. resolving pipeline haz- ards via hardware mechanisms refe rred pipeline interlock. Pipeline inter- lock hardware must detect pipeline hazards ensure dependences satisfied. Pipeline interlock involve talling certain stages pipeline well controlling forwarding data via forwarding paths. addition forwarding paths, scalar pipeline'is longer simple sequence pipeline stages data flowing first stage last stage. forwarding paths pr ovide potential feedback paths outputs later stages inputs earlier stages. example, three forwarding paths needed support lead ing ALU instruction involved pipeline hazard illustrated Figure 2.17. referred ALU forwarding paths. leading ALU instruction traverses pipeline stages, could multiple trail- ing instructions data ( RAW) dependent ins truction i. right side Figure 2.17 illustrates multiple dependent trailing instructions satisfied three consecu tive machin e cycles. cycle tl, instruction forwards result dependent ins truction + via forwarding path a. next cycle, t2 , instruction forwards result dependent instruction + 2 via forward- ing path b. instruction + 2 also requires result instruction + 1 , result also forwarded +2 +1 via forwarding path cycle. Dur- ing cycle t3, instruction forward result instruction + 3 via forwarding path c. Again, path path b also activated c ycle instruction + 3 also requ ires result + 2 + 1, respectively. ID RD ELi ALU forwarding pathsICycle tl: Cycle t2: Cycle 13: MEM | WB Ti + 1: « - R, + 2: *- R . + 3:*~ i:R, *- i+ 1 : — «. + 2: R. i:R, i+ 1 :- i:R,< (i —»- + 1) (i —*- 1 + 2) (i —*- + 3) Forwarding Forwarding writes Rt via path via path b + 3 reads/?, Figure 2.17 Forwarding Paths Supporting Pipeline Hazards Due ALU Leading Instruction.PIPELINED PROCESSORS Register file Figure 2.18 Implementation Pipeline Interlock RAW Hazards Involving Leading ALU Instruction. physical implementation logical di agram Figure 2.17 shown Figure 2.18. Note RAW hazards detected using comparators compare register specifiers consecutive instructions. Four 5 -bit (assuming 32 registers) comparators shown Figure 2.18. trailing instruction j currendy RD stage, is. attempting read two register operands, first two com- parators (to left) checking possible RAW dependences instruc- tion j instruction j — 1, ALU stage. tw comparators comparing two source register specifiers j destination register specifier j - 1. time two comparators (to right) checking possible RAW dependences j j - 2, MEM stage. two comparators comparing two source register specifiers ofj destinatio n register specifier j — 2. outputs four compar- ators used control signals next cycle activating appropriate for- warding paths dependences detected.MODERN PROCESSOR DESIGN Forwarding path activated first pair comparators RAW dependences detected instructions j j - 1. Similarly, forwarding path b activated outputs second pair comparators atisfying dependences instructions j ; - 2. paths simulta- neously activated j depends j - 1 j - 2. Forwarding path c Figure 2.17 hown Figure 2.18; reason forwarding path may necessary appropriate care taken design multiported register file. physical design three-ported (two reads one write ) register file pe rforms first write two reads cycle, third forwarding path necessary. Essentia lly instruction j read new, correct, value dependent register traverses RD stage. words, forwarding performed internally register file. need wait one cycle read dependent register forward output WB stage input ALU stage. reasonable design choice, reduce either penalty cycle one number forwarding paths one. actually imple mented MIPS R200G7R3000 pipe line. reduce penalty due pipeline hazards involve leading load instruc- tions, another set forwarding p aths needed. Figure 2.19 illustrates two for- warding paths needed leading instruction involved pipeline hazard load instruction. referred load forwarding paths. Forwarding path forwards output MEM stage input ALU stage, whereas path e forwards output WB stage input ALU stage. leading ID RD Load forwarding pathALU MEM | 1 WB | + 1: « - R, + 1: «- R , + 2: - *i i:R, «- MEM[] i+ 1:- *  i:R, *~ MEM[] i:R, «- MEM[] (l—+ I) Stall + 1 («"-*- + 1) Forwarding via path (i - * - + 2) writes /?]before + 2 reads /?, Figure 2.19 Forwarding Paths Supporting Pipeline Hazards Due Leading Load Instruction.PIPELINED PROCESSOR instruction reaches ALU stage, instruction + 1 dependent instruction i, must stalled RD stage one cycle. next cycle, instruction exiting MEM stage, result forwarde ALU stage via path allow instruction + 1 enter ALU stage. case instruction + 2 also depends instruction i, result forwarded next cycle via path e WB stage ALU stage allow instruction + 2 proceed ALU stage without incurring nother stall cycle. Again, multiported register file performs first write read, forwarding path e necessary. example, instruction + 2 read result instruction RD stage instruction simultaneously performing register write WB stage. physical implementation forwarding paths supporting pipeline hazards due ALU load leading instructions shown Figure 2.20. For- warding path e shown, assuming register file designed perform first write read cycle. Note ALU forwarding path b l CompRegister file  di ~ Stall IF, ID, RD3£D-cache Pathrf  Load ALU Figure 2.20 Implementation Pipeline Interlock RAW Hazards Involving ALU Load Instructions.MODERN PROCESSOR DESIGN load forwarding path shown Figure 2.17 Figure 2.19, respectively, going output MEM stage input ALU stage, actu- ally two different physical paths, shown Figure 2.20. two paths feed first pair multiplexers, oily one two selected depe nding whether leading instruction MEM stage ALU load instruction. Forwardi ng path b originates buffer MEM stage contains out- put ALU previous machine cycle. Forwarding path originates buffer MEM stage contains data accessed D-cache. two pairs comparators used detect register dependences regardless whether leading instruction ALU load instruction. Two pairs comparators required interlock hardware must detect pos- sible dependences instruct ions + 1 well instructions + 2. register file designed perform first write read cycle, dependence instructions + 3 automatically satis- fied traverse WB RD stages , respectively. output first pair comparators used along signal ID stage indicating leading instruction load produce control signal stalling first three stages pipeline one cycle dependence detected instructions + 1, instruction load. Pipeline interlock hardware TYP pipeline must also deal pipeline hazards due control dep endences. implementation interlock mecha- nism supporting control hazards involving leading branch instruction shown Figure 2.21. Normally, every cycle stage accesses I-cache [-cache Addr Data llR +4 XCondition Target addressALU Invalidate IF, ID, RD. ALU Figure 2.21 Implementation Pipeline Interlock Hazards Involving Branch Instruction.PIPELINED PROCESSO fetch next instruction time inc rements PC preparation fetching next sequential instruction. branch instruction fetched stage decoded instruction ID stage, stagecan stalled branch instruction traverses ALU stage, branch con dition target address generated. next cycle, corre- sponding MEM stage branch instruction, branch cond ition used load branch target ad dress PC via right side PC multi- plexer branch taken. results four-cycle penalty whenever branch instruction encountered. Alternatively, branch instruction assumed taken, stage continues fetch subsequent instruc- tions along sequential path. case branch instru ction deter- mined taken, PC updated branch target MEM stage branch instruction, sequential instructions IF, ID, RD, ALU stages invalidated flushed pipeline. case, four- cycle penalty incurred branch actually taken. branch turns taken, penalty cycle incurred. 2.2.4 Commercial Pipelined Processors Pipelined processor design become mature widely adopted technology. compatibility RISC philosophy instruction pipeli ning well known well exploited. Pipelining also successfully applied CISC architec- tures. subsection highlights two representative pipelined processors. MIPS R2000/R3O00 pipeline presented representative RISC pipeline pro- cessors [Moussouris et al., 1986; Kane, 1987]. Intel i486 prese nted repre- sentative CISC pipelined processors [Crawford, 1990]. Experimental data IBM study RISC pipelined processors done Tilak Agerwala John Cocke 1987 presented representative characteristics perfor- mance capabilities scalar pipelined processors [Agerwala Cocke, 1987]. 23.4.1 RISC Pipelined Processor Example. MIPS RISC architecture 32-bit instructio ns. three different nstruction formats shown Figure 2 .22. 31 26 25 2120 16 15 0 I-type (immediate) op rs rt immediate 31 26 25 °*. - -* ' J-type(jarap> op target 31 26 25 21 20 16 15 1110 6 5 0 R-type (register) op rs rt rd funct Figure 2.22 Instruction Formats Used MIPS Instruction Set Architecture.MODERN PROCESSOR DESIGN Instructions divided four types.  Computational instructions perform arithmetic, logical, shift opera- tions register operands. employ R-type format operands th e result registers, I-rype format one oper- ands specified immediate field instruction.  Load/store instructions move data memory registers. employ I-type format. addressing mode base register plus signed offset stored immediate field.  Jump branch instructions steer control flow program. Jumps unconditional use J-type format jump absolute address composed 26-bit target high-order 4 bits PC. Branches conditional use I-type format specify target address PC plus 16-bit offset immediate field.  instructions instruction set used perform operations coprocessors special system functions. Coprocessor 0 (CPU) system control coprocessor. CPO instructions manipulate th e memory management exception handling facilities. F loating-po int instructions implemented coprocessor instructions performed sepa- rate floating-point processor. MIPS R2000/R3000 pipeline five-stage instruction pipeline quite simi- lar TYP pipeline. However, pipeline stage div ided two separate pha ses, identified phase one (0 1) phase two (<|>2). functions per- formed five stages phases described Table 2.7. number interesting features five- stage pipeline. I-cache access, requires entire cycle, actually takes place 02 Table 2.7 Functionality MIPS R2000/R3000 fiv e-stage pipeline Stage Name Phase Function Performed 1. 01 Translate virtual instruction address using TLB. 02 Access I-cache using physical address. 2.RD 01 Return instructions I-cache; check tags parity. 02 Read register file; branch, generate target address. 3. ALU $1 Start ALU operation; branch, check branch condition. 4>2Finish ALU operation; load/store, translate virtual address. 4. MEM 01 Access D-cache. 02 Return data D-cache; check tags parity. 5.WB 01 Write register file. 02 —PIPELINED PROCESSORS 8 stage 01 RD stage. One translation lookaside buffer (TLB) nsed address translation I-cache D- cache. TLB accessed 0 1 stage, supporting I-cache access accessed < ] > 2 ALU stage, suppo rting D-cache acce ss, takes place MEM cycle. register file performs first write (0 1 WB stage), read ( < | > 2 RD stage) every machine cycle. pipeline requires three-ported (two reads one write) register file single-ported I-cache single-ported D-cache support MEM stages, respectively. forwarding paths outputs ALU MEM stages back input ALU stage, ALU leading hazards incur penalty cycle. load penalty, is, wo rst-case penalty incurred load leading hazard, one c ycle th e forwarding path output MEM stage input ALU stage. Th e branch penalty also one cycle. made possible due several features R2 O00/R3O00 pipeline. First, branch instructions use PC-relative addr essing mode. Unlike register must accessed RD stage, PC available stage. Hence, branch target address calculated, albeit using separate ad der, RD stage. second feature explicit condition code bit generated stored. branch con dition generate < | > 1 ALU stage com- paring contents refere nced register(s). Normally branch condi- tion generated ALU stage (stage 3) instruction fetch done stage (stage 1), expected penalty would two cycles. How- ever, pa rticular pipeline design I-cache access actually start 02 stage. Wit h branch condition available end < | > 1 ALU stage since I-cache access begin 02 stage, branch target address produced end RD stage teered branch condition PC prior tart I-cache access middle stage. Consequendy onl one-cycle penalty incurred branch instructions. Compared six-stage TYP pipeline, five-stage MIPS R20O0/R3O00 pipeline better design terms penaltie incurred due pipeline haz- ards. pipelines ALU load penalties zero cycles one cycle, respectively. However, due stated feat ures design, MIPS R2000/R3000 pipeline incurs one cycle, instead four cycles, branch penalty. Influenced b enefited RISC research done Stanford Univer sity, MIPS R2O00/R3O00 clean design highly efficient pipelined processor. 2.2.4.2 CISC Pipelined Processor Exam ple. 1978 Intel introduced one first 16-bit microprocessors, Intel 8086. Although preceded ea rlier 8-bit microprocessors Intel (8080 8085), 8086 began evolution would eventually result Intel IA32 family object code compatible microproces- sors. Intel IA32 CISC architecture variabl e-length instructions and' complex addressing modes, far dominant architecture today terms sales volume accompanying application software base. 1985, Intel 386, 32-bit version IA32 fa mily, introduced [Crawford, 1986]. first pipelined version IA 32 family, Intel 486, intr oduced 1989.MODERN PROCESSOR DESIGN Table 2.8 Funaionality Intel 486 five-stage pipeline Stage Name ".instruction fetch 2. Instruction decode-1 3. Instruction decode-2 4. Execute 5. Register write-backFunction Perfprmed Fetch" instruction 32-byte prefetch queue (prefetch unit fills flushes prefetch queue). Translate instruction control signals microcode address. Initiate address generation memory access. Access microcode memory. Output microinstruction execute unit. Execute ALU memory accessing operations. Write back result register. original 8086 chip less 30K transistors, 486 chip 1M transistors. 486 object code compatible previous members IA32 family, became popular microprocessor used personal computers early 1990s [Crawford, 1990]. 486 implemented five-stage instruction pipeline. functionality pipeline stages described Table 2.8 . instruction prefetch unit, via bus interface unit, prefetches 16-byte blocks instructions prefetch queue. th e instruction fetch tage, instruction fetched the- 32-byte prefetch queue. Instruction decoding performed two stages. Hard- wired control signals microinstructions produced instruction decoding. execute stage performs ALU operations well cache accesses. Address translation effective address generation carried dur- ing instruction decoding; memory accessing completed execute stage. Hence, memory load followed immediately use incur penalty cycle; output th e execute stage forwarded input. However, instruc- tion produces register result followed immediately another instruction uses registe r address generation, penalty cycle necessary address generation done instruction de coding. fifth stage pipeline performs register write-back. Floating-point operations carried on-chip floating-point unit incur multiple cycles execution. five-stage instruction pipeline, 486 execute many IA32 instruc- tions one cycle without using microcode. instructions require accessing micro-instructions multiple cycles. 48 6 clearly demonstrates perfor-« mance improvement obtained via instruction pi pelining. Based typical instruction mix execution times frequently used IA3 2 instructions, Intel 386 able achieve average cycles per instruction (CPI) 4.9* [Crawford, 1986]. pipelined Intel 486 achieve average CPI 1.95. represents speedup factor 2.5. terminology, five-stage i486 achieved effective degree pipelining 2.5. Clearly, significant pipelining overhead involved, primarily due complexity IA32 instruction setPIPELINED P ROCESSORS architecture burden ensuring object code compatibility. Nonetheless, CISC architecture, speedup obtained quite respectable. 486 clearly dem- onstrated feasibility pipelining CISC architecture. 2.2A3 Scalar Pipelined Processor Performance. report documenting IBM experience pipelined RISC machines Tilak Agerwala John Cocke 1987 provided assessment performance capability scalar pipelined RISC processors [Agerwala Cocke, 1987]. key obser- vations report presented here. thi study, assumed I-cache D-cache separate. I-cache supply one instruction per cycle processor . load/store instructions acces D-cache. study, hit rates caches assumed 100%. default latency caches one cycle. following characteristics statistics used study. 1. Dynamic instruction mix a. ALU: 40% (register-register) b. Loads: 25% c. Stores: 15% d. Branches: 20% 2. Dynamic branch instruction mix a. Unconditional: 33.3% (always taken) b. Conditional—taken: 33.3% c. Conditional—not taken: 33.3% 3. Load scheduling a. Cannot scheduled: 25% (n delay slot filled) b. moved back one two instructions: 65% (fill two delay slots) c. moved back one instruction: 10% (fill one delay slot) 4. Branch scheduling a. Unconditional: 100% schedulable (fill one delay slot) b. Conditional: 50% schedulable (fill one delay slot) performance processor estimated using average cycles per instruction. idealized goal scalar pipeline processor achieve CPI = 1. implies pipeline processing completing, average, one instruction every cycle. IBM study attempted quantify closely idealized goal ca n reached. Initially, assumed ALU penalty load branch penalties two cycles. Given dynamic instruction mix, CPI overheads due two penalties com puted.  Load penalty overhead: 0.25 x 2 = 0.5 CPI  Branch penalty overhead: "0.20 x 0.66 x 2 = 0.27 CPI  Resultant CPI: 1.0 + 0.5 + 0.27 = 1.77 CPIMODERN PROCESSOR DESIGN Since 25% dynamic instructions loads, assume load incurs two-cy cle penalty, CPI overhead 0.5. pipeline assumes branch instructions taken, biased taken, 66.6% branch instructions taken incur wo-cycle branch penalty. Taking account load branch penalties, expected CPI 1.77. far idealized goal CPI = 1. Assuming forwarding path added bypass register file load instructions, load penalty reduced two cycles one cycle. addition forwarding path, CPI reduced 1.0 + 0.25 + 0.27 = 1.52. addition, compiler employed schedule instructions load branch penalty slots. Assuming statistics presented preceding text, since 65% loads moved back one two instructions 10% loads moved back one instruction, total 75% load instructions scheduled, moved back, eliminate load penalty one cycle. 3 3.3% branch instructions unconditional, scheduled reduce branch penalty two cycles one cycle. Since pipeline biased taken branches, 33.3% branches conditional taken incur branch penalty. remaining 33.3% branches ar e conditional taken, assumption 50% schedulable, is, moved back one instruction. Hence 50% con- ditional branches taken incur one-cycl e penalty, ther 50% ncur normal two-cycle penalty. new CPI overheads resultant CPI shown here.  Load penalty ov erhead: 0.25 x 0.25 x 1 = 0.0625 CPI  Branch penalty head: 0.20 x [0.33 x 1 + 0.33 x 0.5 x 1 + 0.33 x 0.5x2] = 0.167 CPI  Resultant CPI: 1.0 + 0.063 + 0.167 = 1 .23 CPI scheduling load branch penalty slots, CPI overheads due load branch penalties significantly reduced. resultant CPI 1.23 approaching idealized goal CPI = 1. CPI overhead due branch penalty still significant. One way reduce overhead consider ways reduce branch penalty two cycles. IBM study, instead using register-indirect mode addr essing, 90% branches coded PC-relative . Using PC-relative addressing mode, branch target address generation done without access register file. sep- arate adder included ge nerate target address parallel register read stage. Hence, branch instructions employ PC-relative addressing, branch penalty reduced one cycle. 33.3% branches unconditional, 100% schedulable. Hence, branch penalty one cycle. 90% made PC-relative conse- quendy eliminate branch penalty, remaining 10% un con- ditional branches incur branch penalty one cycle. correspondingPIPELINED PROCE SSORS Table 2.9 Conditional branch penalties considering PC-relative addressing scheduling penalty slot PC-relative Addressing Schedulable Branch Penalty Yes (90%) Yes (50%) 0 cycles Yes (90%) (50%) 1 cycle (10%) Yes (50%) 1 cycle ' (10%) (50%) 2 cycles CPI overhead unconditional branches 0.20 x 0.33 x 0.10x1 = 0.0066 CPI. employment PC-relative addressing mode, fetch stage longer biased taken branches. Hence conditional branches treated way, regardless whether taken. Depending whether conditional branch made PC-relative whether scheduled, f possible cases. penalties four possible cases conditional branches shown Table 2.9. Including taken taken ones, 66.6% branches condi- tional. CPI overhead due conditional branches derived considering cases Table 2.9 equal 0.20 x 0.66 x{ [ 0 . 9x0.5x1]+ [0.1x0.5x1]+ [0.1x0.5x2]} = 0.079 CPI Combining CPI ovemeads due unconditional conditional branches results total CPI ove rhead due branch penalty 0. 0066 + 0.079 = 0.0856 CPI. Along original load penalty, new overheads resultant ove rall CPI shown here.  Load penalty ov erhead: 0.0625 CPI  Branch penalt overhead: 0.0856 CPI  Resultant CPI: 1.0 + 0.0625 + 0 .0856 = 1.149 CPI Therefore, series refinements, original CPI 1.77 reduced 1.15. quite close idealized goal CPI = 1. One way view CPI = 1 represents ideal instruction pipeline, new instruction entered pipeline every cycle. achievable third point pipelining idealism true, is, instructions inde- pendent. real programs inter-instruction depen dences. CPI = 1.15 indicates 15% overhead inefficiency incurred design realistic instruction pipeline deal inter-instruction depen- dences. q uite impressive reflects effectivenes instruction pipelining.MODERN PROCESSOR DESIGN 2.3 Deeply Pipelined Processors Pipelining effective means improving processor performance, strong motivations employing deep pipelines. deeper pipeline increases number pipeline stages reduces number logic gate levels pipeline stage. primary benefit deeper pipelines ability reduce machine cycle time hence increase clocking frequency. 1980s pipelined microprocessors four six pipeline stag es. Contemporary high-end microprocessors clocking frequencies multiple-gigahertz range, pipeline depths increased 20 pipeline stages. Pipelines gotten deeper, also wider, superscalar processors. pipelines get wider, increased complexity pipeline stage, increase delay eac h pipeline stage. maintain clocking fre- quency, wider pipeline need made even deeper. downside deeper pipelines. deeper pipeline penalties incurred pipeline hazard resolution become larger. Figure 2.23 illustrates happen ALU, load, branch penalties pipeline becomes wider much deeper. Comparing shallow deep pipelines, see ALU penalty increases zero cycles one cycle, load penalty increases one cycle four cycles, importandy, branch penalty goes three cycles eleven cycles. increa sed pipeline penalties, average CPI increases. potential performance gain due higher clocking frequency deeper pipeline ameliorated increase CPI. ensure overall performance improvement deeper pipeline, increase clocking frequency must exceed increase CPI. two approaches used mitigate negative impact increased branch penalty deep pipelines; see Figure 2.24. Among three pipeline penalties, branch penalty severe spans front-end pipeline stages. mispredicted branch, instructions front-end pipeline stages must flushed. first approach reduce branch Fetch Decode Dispatch Execute Memory Retire5 ' . ALU s* penalty V^.Hack.: --Decode- ["Dispatch, -Execute— j ! "Memory — Retire—Branch penalty | Load penalty Figure 2.23 Impact ALU, Load, Branch Penalties Increasing Pipeline Depth.PIPELINED PROCESSORS 95 - Decode- \ "Dispatch" rWtSoryt — Retire-Front-end contraction Back-end optimizationBranch penalty Figure 2.24 Mitigating Branch Penalty mpact Deep Pipelines. penalty reduce number pipeline stages front end. example, CISC architecture variable instruction length require complex instruc- tion decoding logic requ ire multiple pipeline stages. using RISC archi- tecture, decoding complexity reduced, resulting fewer front-end pipeline - stages. Another example use pre-decoding logic prior loading instructions I-cache. Pre-decoded instructions fetched I-cache require less decoding logic hence fewe r decode stages. second approach move front-end complexity back end pipeline, resulting shallower front end hence smaller branch penalty. active area research. sequence instructions repeatedly executed pipeline, front-end pipeline stages repeatedly per- form work fetching, decoding, dispatching instruc- tions. suggested result work done cached reused w ithout repeat work. example, block decoded instructions stored special cache. Subsequent fetching instructions done accessing cache, decoding pipeline stage(s) bypassed. caching decoded instructions, addi- tional optimization performed instructions, leading elim- ination need front-end pipeline stages. caching optimization implemented back end pipeline without impacting th e front-end depth associated branc h penalty. order deep pipelines ha rvest performance benefit higher clocking frequency, pipeline p enalties must kept co ntrol. di fferent forms tradeoffs involved designing deep pipelines. indicated Section 2.1, A-stage pipeline potentially achieve increase throughput factor k relative nonpipelined design. cost taken account, tradeoff involving cost performance. tradeoff dic- tates optimal value k arbitrarily large. Th illustrated Figure 2.3. form tradeoff deals hardware cost implement- ing pipeline, indicates pipeline depth beyond theMODERN PROCESSOR DESIGN additional cost pi pelining cannot justified diminishing return performance gain. another form tradeoff based foregoing analysis CPI impact induced deep pipelines. tradeoff involves increase clocking frequency versus increase CPI. According iron law processor per- formance (Sec. 1.3.1, Eq. 1.1), performance determined product c lock- ing frequency average IPC, frequency/CPI ratio. pipelines get deeper, frequency increases CPI. Increasing pipeline depth prof- itable long adde pipeline depth bring net increase perfor- mance. point beyond pipelining deeper lead little performance improvement. interesting question is, deep pipe- line go reach point diminishing returns? number recent studies focused determining optimum pipeline depth [Hartstein Puzak, 2002, 2 003; Sprangle Carmean, 2002; rinivasan et al., 2002] microprocessor. pipeline depth increases, frequency increased. However frequency increase linearly respect increase pipeline depth. sublinear increase frequency due overhead adding latches. pipeline depth increases, CP also increases due increas e branch load penalties. Combining frequency CPI behaviors yields overall performance. pipeline depth increased, overall performance tends increase due benefit increased frequency. However pipeline depth increased, reaches point CPI overhead overcomes benefit increased frequency; increase pipeline depth beyond point actually bring gra dual decrease overall performance. recent study, Hartstein Puzak [2003] showed, based ir performance model, point diminishing return, hence optimum pipeline depth, occurs around pipeline depth -25 stages. Using ggressive assumptions, Sprangle Carmean [2002] showed optimum pipeline depth actually around 50 stages. power consumption taken account, optimum pipeline depth significantly less 25 50 pi pe stages. higher frequency deeper pipeline leads significan increase power consumption. Power consump- tion become prohibitive render deep pipeline infeasible, even performance harvested. study, Hartstein Puzak [2003] developed new model optimum pipeline depth taking account power consumption addition performance. use model based BIPSAA3/W metric, BIPSAA3 billions instructions per second third power, W watt. model essentially favors performance (BEPS) power (W) ratio 3 1. Given model, optimum pipeline depth range 6-9 pipe stages. Assuming lower latching overhead increasing leakage power, showed optimum pipeline depth could potentially range 10-15 pipe stages. recent years witnessed relendess push towards ever higher clocking frequencies everPIPELINED PROCESSORS deeper pipelines, constraints due power consumption heat dissipation become se rious impediments relentless push. 2.4 Summary Pipelining microarchitecture technique applied ISA. true features RISC architectures make pipelining easier produce efficient pipeline designs. However, pipelining equally e ffective CISC architectures. Pipelining proved powerful technique increasing processor performance, terms pipeline depth still plenty head- room. expect much deeper pipelines. key impediment pipelined processor performance stal ling pipeline due inter-instruction dependences. branch penalty due control dependences biggest cu lprit. Dynamic branch prediction alleviate problem incur branch penalty branch misprediction occurs. branch correctly predicted, stalling pipeline; however, branch misprediction detected, pipeline must flushed. pipelines get deeper, branch penalty increases becomes key challenge. One trategy reduce branch penalty reducing depth front end pipeline, is, distance ins truction fetch stage stage branch instructions resolved. alternative increase accuracy dynamic branch prediction lgorithm fre- quency branch misprediction reduced; hence, fre quency incurring branch penalty also reduced. cover dynami c branch prediction chapter. important topic, chosen present branch pre- diction context superscalar p rocessors. get Chapter 5. Pipelined pro cessor design alters rel evance classic view CPU design. classic view partitions design processo r data path design control path design. Data path design focuses design ALU functional units well accessing registers. Control path design focuses design state machines decode instructions generate sequence control signals necessary appropriately manipulate th e data path. view longer relevant. pipelined processor partition longer obvious. Instructions decoded decode stage, decoded instruc- tions, including associated control signals, propagated pipeline used various subseque nt pipeline stages. pipeline stage simply uses appropriate fields decoded instru ction associ ated control signals. Essentially longer centralized control performed control path. Instead, form distributed control via propagation control sig- nals pipeline stages used. traditional sequencing multi- ple control path states proc ess instruction w replaced traversal various pipeline stages. Essentially, data path pipelined, also control pa th. Furthermore, traditional data path control path integrated pipeline.MODERN PROCESSOR DESIGN REFERENCES Agerwala, T., J. Cocke: "High performance reduced instruction set processors," Tech- nical report, IBM Computer Science, 1987. Bloch. E.: "The engineering design STRETCH computer," Proc. Fall Joint Computer Conf., 1959, pp. 48-59. Bucholtz, W.: Planning Computer System: Project Stretch. New York: McGraw-Hill, 1962. Crawford, J.: "Architecture Intel 80386," Proc. IEEE bit. Conf. Computer Design: VLSI Computers, 1986, pp. 155-160. Crawford, J.: "The execution pipeline Intel i486 CPU," Proc. COMPCON Spring '90, 1990, pp. 254-258. Hartstein, A., T. R. Puzak: "Optimum power/performance pipeline depth," Proc. 36th Annual International Symposium Microarchitecture (MICRO), Dec. 2003. Hartstein, A. , T. R. Puzak: "The optimum pipeline depth icroprocessor." Proc. 29th Annual International Symposium Computer Architecture (1SCA), June 2002. Hennessy, J-, D. Patterson: Computer Architecture: Quantitative Approach, 3rd ed., San Mateo, CA: Morgan Kaufmann Publishers, 2003. Kane, G.: MIPS R2000/R3000 RISC Architecture. Englewood Cliffs, NJ: Prentice Hall, 1987. Kogge, Vs.The Architecture Pipelined Computers. New York: McGraw-Hill, 1981. Moussouris, J., L. Crudele, D. Frietas, C. Hansen, E. Hudson, R. March, S. Przybylski, T. Riordan: "A CMOS RISC processor integrated system functions," Proc. COMPCON, 1986, pp. 126-131. Sprangle, E., D. Carmean: "Increasing processor performance implementing deeper pipelines," Proc. 29th Annual International Symposium Computer Architecture (ISCA), June 2002. Srinivasan, V., D. B rooks, M. Gschwind, P. Bose, V. Zyuban, P. N. Strenski, P. G. Emma: "Optimizing pipelines power performance," Proc. 35th Annual Inter- national Symposium Microarchitecture (MICRO), Dec. 2002. Thornton, J. E.: "Parallel operation Control Data 6600," AFIPS Proc. FJCCpart 2, vol. 26,1964, pp. 33-40. Waser, S., M. Flynn: bttroduction Arithmetic Digital Systems Designers. New York; Holt, Rinehart. Winston, 1982. HOMEWOR K PROBLEMS P2.1 Equation (2.4), relates performance ideal pipeline pipeline depth , looks similar Amdahl's law.'Des cribe relation- ship terms two equations, develop intuitive explanation two equations similar. P2.2 Using Equa tion (2.7), cost/performance optimal pipeline depth kop, computed using parameters G, T, L, S. Compute kBp, pipelined floating-point multiplier example Section 2.1 using thechip count cost terms (G = 175 chips L = 82/2 = 41 chipsPIPELINED PROCESSORS per interstage latch) delays shown (T - 400 ns, = 22 ns). different kBp, proposed pipel ined design? P2.3 Identify discuss two reasons Equation (2.4) useful naive approximations potential peedup pi pelining. P2.4 Consider would like add load-immediate instruction TYP instruction set pipeline. instruction extracts 16-bit immediate value instruction word, sign-extends immedi- ate value 32 bits, stores result destination register specified instruction word. Since extraction sig n-extension accomplished without ALU, colleague suggests instructions able write results register decode (ID) stage. Using hazard detection algorith described Figure 2.15, identify additional hazards change might introduce. P2.5 Ignoring pipeline interlock hardware (discussed Problem 6), additional pipeline resources chang e outlined Problem 4 require? Discuss resources heir cost. P2.6 Considering change outlined Problem 4, redraw th e pipeline interlock hardware shown Figure 2.18 correctly handle load- immediate instructions. P2.7 Consider would like add byte-wide ALU instructions TYP instruction set pipeline. instructions h ave semantics otherwise identical existing word-width ALU instructions, except source operands 1 byt e wide destination operand 1 byte wi de. byte-wid e operands stored registers word-wide instructions, l ow-order byte, register writes must affect low-order byte (i.e., high- order bytes must remain unchanged). Redraw RAW pipeline inter- lock detection hardware shown Figure 2.18 correcdy handle additional ALU instructions. P2.8 Consider adding store instruction indexed addressing mode TYP pipeline. store differs existing store register + immediate addressing mode computing effective add- ress sum two sourc e registers, is, stx r3,r4,r5 performs r3«—MEM[r4+r5]. Describe additional pipeline resources needed support instruction TYP pipeline. Discuss advan- tages disadvantages instruction. P2.9 Consider adding load-update instruction register + immediate postupdate addressing mode. addr essing mode, effec- tive address load computed register + immediate, resulting address written back base register. is, Iwur3,8(r4) performs r3«-MEM[r4+8]; r4«-r4+8. Describe the100 MODERN PROCESSOR DESIGN additional pipeline resources needed support instruction TYP pipeline. P2.10 Given change outlined Problem 9, redraw pipeline interlock hardware shown Figure 2.20 correctly handle load-update instruction. P2.ll Bypass network design: given following ID, EX, MEM, WB pipeline configuration, draw necessary MuxO Muxl bypass paths resolve RAW data hazards. Assume load instructions always separated least one independent instruction [possibly no-operation instruction (NOP)] instruction reads loaded register (hence never stall due RAW hazard). OP KMdRtgB | ReodRtgl ~ Wrrfeg j ID Stpaa j [ AIXtQBtEX fceadRegO ReadRegl "" WfReg~'"j EX MEM Q» | RwdRcgO RcadRegl - WrReg " MEM WB P2.12 Given forwarding paths Pr oblem 11, draw detailed design MuxO Muxl clearly identifies bypass paths selected control conditions. Identify input eachPIPELINED PROCESSORS mux name pipeline latch bypassing from. Spec- ify precisel boolean equations used control MuxO Muxl. Possible inputs boolean equations are:  ID.OP, EX.OP, MEM.OP = {'load', 's tore', 'alu', 'other'}  ID.ReadRegO, ID.ReadRegl = [0..31.32] 32.means r egister read instruction  EX.ReadRegO, etc., ID stage  MEM.ReadRegO, etc., ID stage  ID.WriteReg, EXWriteReg, MEM.WriteReg = [0..31.33] 33 means register written instruction  Draw MuxO Muxl labeled inputs; need show controls using gates. Simply write control equations using symbolic OP comparisons, etc. [e.g., C trll = (ID.op = 'load') & (ID.WriteReg=MEM.ReadRegO)]. T2.13 Given IBM experience outlined Section 2.2.4.3, compute CPI impact addition level-zero data cache able sup- ply data operand single cycle, 75% time. level-zero level-one caches accessed parallel, level-zero cache misses, level-one cache returns result next cycle, resulting one load-delay slot. Assume un iform distribu- tion level-zero hits across load-delay slots cannot filled. Show work. P2.14 Given assumptions Problem 13, compute CPI impact level-one cache accessed sequentially, level-zero cache misses, resulting two load-delay slots instead one. Show work. P2.15 IBM study pipelined processor performance assumed instruction mix based popular C programs use 1980s. Since then, object-oriented l anguages like C++ Java become much common. One effects langu ages object inheritance and.polymorphism used replace condi- tional branches virtual function ca lls. Given IBM ins truction mix CPI shown following table, perform following trans- formations reflect use C++ Java, recompute over- CPI speedup slowdown due c hange:  Replace 50% taken conditional branches load instruction followed jump r egister instruction (the loa jump register implement virtual function call).  Replace 25% not-taken branches loa instruction followed jump register instruction.2 MODERN PROCESSOR DESIGN Instruction fyp«Old Mix, %New Mix, Latency Old CPI Cycles % fait ructions Cycles NewCPf _oad 25.0 2 0.50 500 Store ISO 1"T"' 015 150 Arithmetic 30.0 0.30 300 Logical 10.0 1 0.10 100 Branch-T 8.0 3 0.24 240 Branch-NT 6.0 2 0.12 120 Jump 5.0 2 0.10 100 Jump register 1.0 3 6.03 30 Total 160.6"" 154""' 1540 P2.16 TYP-based pipeline design data cache, load instructions check tag array ca che hit parallel accessing data array read corresponding memory location. Pipelining stores cache difficult, since processor must check tag first, overwrites data array. Otherwise, case cache miss, wrong memory location may overwritten store. Design solution problem require sending store pipe twice, stalling pipe every store ins truction, dual-porting data cache. Referring Figure 2.15, new RAW, WAR, and/or WAW memory hazards? P2.17 MIPS pipeline shown Table 2.7 employs two-phase clocking scheme makes efficient use shared TLB, since instruction fetch accesses TLB phase one data fetch accesses phase two. However, resolving conditional branch, branch target address branch fall-through address need translated phase one—in parallel branch condition check phase one ALU stage—to enable instruction fetch either target fall-through phase two. seems imply dual-ported TLB. Suggest architected solution problem avoids dual-porting TLB. Problems 18 24: Instruction Pipeline Design problem explores pipeline design. discussed eadier, pipelining involves balancing pipe stages. Good pipeline implementations minimize bot h internalPIPELINED PROCESSORS 1 external fragmentation create simple balanced designs. nonpipe- lined implementation simple microprocessor executes ALU instruc- tions, data hazards: Ok1 Instruction cache (6 ns) 32 bits Register wi File (4 ns) WEN 1 control14 MODERN PROCESSOR DESIGN P2.18 Generate pipelined implement ation simple processor outlined figure rninirnizes internal fragmentation. subblock diagram primitive unit cannot partitioned smaller ones. original functionality must maintained pipelined implementation. Show diagram pipelined implementation. Pipeline registers following timing requirements:  0.5 -ns setup time  1-ns delay time (from clock output) P2.19 Compute latencies (in nanoseconds) instruction cycle nonpipelined pipelined implementations. P2.20 Compute machine cycle times (in nanoseconds) nonpipe- lined pipelined implementations. P2.21 Compute (potential) speedup pipelined implementation Problems 18 -20 original nonpipelined implementation. P2.22 microarchitectural echniques could used reduce machine cycle time pipel ined designs? Explain machine cycle time reduced. P2.23 Draw simplified diagram pipeline stages Problem 18; include necessary data forwardin g paths. diagram similar Figure 2.16. P2.24 Discuss impact data forwarding paths Problem 23 pipeline implementation Problem 18. wil l timing affected? pipeline remain balanced forwarding paths added? changes original pipeline organization Problem 18 might needed?Memory I/O SystemsCHAPTE CHAPTER OUTLINE 3.1 Introductiorv 3.2 Computer System Overview 33 Key Concepts; Latency Bandwidth 3A Memory Hie rarchy ' ^ " 35 virtual Memory Systems 3.6 Memory Hierarchy Implementation 3.7 Input/Output Systems 3.8 Summary References Homework Problems 3.1 Introduction primary focus book design advanced, high-performance proces- sors; chapter examines larger context computer systems incorporate processors. Basic components, memory systems, input output, virtual memory, ways interconnected described relative detail enable better understanding interactions high-performance processors peripheral devices connected to. Clearly, processors exist vacuum. Depending th eir intended appli- cation, processors interact c omponents internal computer system, devices external system, well humans external entities. speed interactions occur varies type communica- tion necessary, protocols used communicate them. Typically, interacting performance-critical entities memory subsystem accomplished via proprietary, high-speed interfaces, communication 1(106 MODERN PROCESSOR DESIGN peripheral external de vices accomplished across industry-standard interfaces sacrifice performance sake compatibility across multiple ven- dors. Usually interfaces balanced, providing symmetric bandwidth device. However, interacting physical beings (such humans) often leads un balanced bandwidth requirements. Even fastest human typist generate input rates kilobytes per second. contrast, human visual perception absorb 30 frames per second image data, image contains se veral megabytes pixel data , resulting output data rate 100 megabytes per second (Mbytes/s). ban dwidth requirements various components vary dramati- cally, latency characteristics diverse well. ex ample, studies shown subsecond response times (a response time defined inter- val user issuing command via keyboard observing response display) critical productivity human computer users, response times much less second provide rapidly dim inishing returns. Hence, low latency responding user input keyboard mouse critical. contrast, modern processors perate frequencies much higher main memory subsystems. e xample, state-of-the-art personal computer processor clocked 3 GHz today, synchronous main memory clocked 133 MHz. mismatch frequency cause processor starve instructions data waits memory supply them, hence motivating high-spee processor-to-memory interfaces opti- mized low latency. Section 3.2 presents overview modem computer systems. numerous interesting architectural tradeoffs design hardware subsystems, interfaces, protocols satisfy put/output requirements vary dramati- cally. Section 3.3, define fundamental metrics bandw idth latency discuss tradeoffs involved designing interfaces meet require- ments metrics . Section 3.4, introduce concept memory hierar- chy discuss components used build modem memory hierarchy well key tradeoffs met rics used design process. Section 3.5 ntroduces notion virtual memory, critically important modem systems tha time- share physical execution resources. Finally, Section 3.7 discusses various input/ output devices, key characteristics, interconnects used modern systems allow communicate processor. 3.2 Computer System Overview illustrated Figure 3.1, typical computer system consists proce ssor CPU, main memory, inp ut/output (I/O) bridge connected processor bus, peripheral devices network interface, disk controller driving one disk drives, display adapter driving display, input devices keyboard mouse, conn ected I/O bus. main memory pro- vides volatile storage programs data computer powered up. design efficient, high -performance memory systems using hierarchicalMEMORY I/O SYSTEMS 1l Memory Figure 3.1 Typical Computer System. approach tha exploits temporal spatial locality discussed detail Sec- tion 3.4. contrast volatile main memory, disk drive provides persistent storage survives even system powered down. Disks also used transparently increase effective memory ca pacity use virtual memory, described Section 3.5. network interface provides physical connection communicating across local area wide area networks ( LANs WANs) computer systems; systems without local disks also use network interface access remote persistent storage file servers. display subsystem used render textual graphical user interface display device cathode-ray tube (CRT) liquid-crystal display (LCD). Input devices enable user operator enter data issue commands computer system. discuss types peripheral devices (disks, network interfaces, display subsystems, inpu devices) Section 3.7.1. Finally, computer system must provide means interconnecting devices, well interface communicating them. discuss vari- ous types busses used interconnect peripheral devices Section 3.7.2 describe polling, interrupt-driven, pr ogrammed means communication I/O devices Section 3.7.3. 3.3 Key Concepts: Latency Bandwidth two fundamental metrics commonly used characterize va rious subsystems, peripheral devices, interconn ections computer systems. two metrics latency, measured unit time, bandwidth, measured quantity per unit time. metrics important understandin g behavior system, provide definitions brief introduction section.108 MODERN PROCESSOR DESIGN Latency defined elapse time betwee n issuing request command particular subsystem receiving response reply. measured either units time (seconds, microseconds, milliseconds, etc.) cycles, trivially translated time given cycle time frequency. Latency provides mea- surement responsiveness p articular system critical metric subsystem sa tisfies time-critical requests. example system memory subsystem, must pr ovide processor instructions data; latency critical processors usually stall memory sub- system respond rapidly. Latency also sometimes called response time decomposed inherent delay device subsystem, called service time, forms lower bound time required satisfy quest, queueing time, results waiting particular res ource become available. Queueing time greater zero onl multiple concurrent requ ests competing access resource, one requests must delay waiting another complete. Bandwidth defined throughput sub system; is, rate satisfy requests. Bandwidth measured quantity per unit time, quantity measured varies based type request. simplest, bandwidth expressed number requ ests per unit time. request corresponds fixed number bytes data, example, bandwidth ca n also expressed number tes per unit time. Naively, bandwidth defined inverse latency. is, device responds single request latency bandwidth e qual less 1/Z, n c e c n accept respond one request every units time. However, naive definition precludes concurrency handling requests. high-performance subsystem frequently overlap multiple requests increase bandwidth without affecting latency particular request. Hence, bandwidth generally defined rate subsystem able satisfy requests. bandwidth greater 1/Z, w e c n nfer subsystem supports multiple concurrent req uests able overlap latencies other. Mos high-performance interfaces, including process or-to-memory interconnects, standard inpu t/output busses like peripheral component interfaces (PCls), device interfaces like small computer systems interface (SCSI), support multiple concurrent requests bandwidth significantly higher h n 1/Z. Quite often, manufacturers also report raw peak bandwidth numbers, usually derived directly hardware parameters particular interface. example, synchronous dynamic random-access memory (DRAM) interface 8 bytes wide clocked 133 MH z may reported peak bandwidth 1 Gbyte/s. se peak numbers usually substantially higher sustainable bandwidth, since account request response transaction overheads bottlenecks might limit ach ievable bandwidth . Sustainable bandwidth realistic measure represents bandwidth sub- system actually deliver. Nevertheless, even sustainable bandwidth might unrealistically optimistic, since may account real-life access patterns andMEMORY I/O SY STEMS 109 system c omponents may cause additional queueing delays, increase overhead, reduce delivered bandwidth. general, bandwidth largely driven product-cost constraints rather fundamental limitations give n technology. example, bus always made wider increase number bytes transmitted per cycle, hence increas- ing bandwidth interface. increase cost, since chip pin count backplane trace count bus may doubl e, peak bandwidth may double, effective sustained bandwidth may increase much smaller factor. However, generally true system performance-limited due insufficient bandwidth either poorly engineered constrained cost factors; cost object, would usually possible provide adequate bandwidth. Latency fundamentally difficult improve, since often minated limitations particular technology, possibly even laws physics. example, electrical characteristics given signaling technology used mul- tidrop backplane bus determine maximum frequency bus operate. follows minimum latency transaction across bus bounded cycle time corresponding max imum frequency. common strategy improving latency, short transitioning newer, faster, technology, decompose latency portions due various subcomponents attempt maximize concurrency components. example, modern multiprocessor system like IBM pSeries 69 0 exposes concurrency handling proce ssor cache misses fetching missing block DRAM main memory parallel checking processors' caches try find newer, modified copy block. less aggressive approach would first chec k processors' caches fetch block DRAM pr ocessor modified copy. latter approach serializes two events, leading increased latency whenever block needs fet ched DRAM. However, often price paid suc h attempts maximize concur- rency, ince typically require speculative actions may ultimately prove unnecessary. preceding multiprocessor example, newer, modified copy found another processor's cache, block must supplied cache. case, concurrent DRAM fetch proves unnecessary con- sumes excess memory bandwidth wastes energy. However, despite cost, various forms speculation commonly employed attempt reduce observed latency request.-As another example, modern processors incorporate prefetch engines look patterns reference stream issue specula- tive memory fetches bring blocks caches anticipation demand references blocks. many cases, additional speculative requests prefetches prove unnecessary, end consuming additional bandwidth. However, useful, subsequent demand reference occurs speculatively prefetched block, latency reference corresponds hitting cache much lower prefetch occurred. Hence, average latency memory ferences lowered expense consuming additional bandwidth issue number useless prefetches.M|P|L[E|110 MODERN PROCESSOR DESIGN summary, bandwidth la tency two fundamental attributes co puter system components, peripheral devices, interconnection networks.- Bandwidth usually improved adding cost system, well- engineered system maximizes concurrency, latency usually much difficult improve without changing th e implementation technology usingT various forms speculation. Speculation used improve observed* latency request, usuall happens expense additional band- width consumption. Hence, well-designed computer system, latency and' bandwidth need carefully balanced cost, since three factors are' interrelated. 3.4 Memory Hierarchy One fundamental needs computer system must meet need for1, storage data program code, computer running, support1 storage temporary results, well computer powered off, enable sults computation well programs used perform that> computation survive across power-down cycles. Fundamentally, storage nothing sea bits addressable processor. perfect storage  technology retaining thi sea bits computer system would satisfy the- following memory idealisms:  Infinite capacity. storing large data sets large programs.  Infinite bandwidth. rapidly treaming large data sets programs processor.  Instantaneous zero latency. prevent processor stal ling waiting data program code.  Persistence nonvolatility. allow data programs survive even power supply cut off.  Zero low implementation cost. Naturally, system processor designers must strive approximate these' idealisms closely possible satisfy pe rformance correctness^ expectations user. Obviously, final factor—cost—plays large role easy reach goals, well-designed memory system fact maintain illusion idealisms quite successfully. true despite the' fact pe rceived requirements first three—capacity, bandwidth, and* latency—have increasing rapidly past decades. Capacity requirements grow programs operating systems users demand increasing size complexity, data sets operate over. Bandwidth requirements increasing reason. Meanwhile, latency requirement becoming increasingly important processors continue to' become faster faster mor e easily starved data program code die perceived memory latency long.MEMORY I/O SYSTEMS 111 3.4.1 Components Modern Memory Hi erarchy modern memory system, often referred memory hierarchy, incorporates various storage technologies create whole approximates five memory idealisms. Figure 3.2 illustrates five typical c omponents modern memory hierarchy plots approximate axes indicate relative latency capacity (increasing axis) bandwidth cost per bit (increasing x axis). important attributes eac h components summarized Table 3.1. Magnetic Disks. Magnetic disks provide cos t-efficient storage largest capacities memory technology today, costing less one-ten- millionth cent per bit (i.e., roughly $1 per gigabyte storage), providing hundreds gigabytes storage 3.5-inch (in.) standard form factor. However, tremendous c apacity low cost comes expense limited effective band- width (in tens megabytes per second single disk) extremely long latency (roughly 10 ms per random access). hand, magnetic storage tech- nologies nonvolatile maintain state even power turned off. DRAM L2$ — :L1 $ 1Reg. file Bandwidth cost per bit Figure 3.2 Memory Hierarchy Components. Table 3.1 Attributes memory hierarchy components Component Technology Bandwidth LatencyCost per Bit(S)Cost per Gigabyte ($) Disk drive Magnetic field 10+Mbytes/s 10 ms < 1 x 10~9< 1 Main memory DRAM 2+ Gbytes/s 50+ns <2x10~7<200 On-chip l _ 2 cache SRAM 10+Gbytes/s 2+ ns < 1 x10"" < 100K On-chip LI cache SRAM 50+ Gbytes/s 300+ps > 1 x10~" > 100K Register file Multiported SRAM 200+ Gbytes/s 300+ps >1 x10"2(?) > 10 Mbytes (?)112 MODERN PROCESSOR DESIGN Main Memory. Main memory based standard DRAM technology much expensive approximately two hundred-thousandths cent per bit (i.e, roughly $200 per gigabyte storage) provides much higher bandwidth (several gigabytes per second even low-cost commodity personal computer) much lower latency (av eraging less 100 ns modern design). study various aspects memory design length Section 3.4.4. Cache Memory. On-chip off-chip cache memories, secondary (L2) primary (LI), utilize static random-access memory (SRAM) tec hnology pays much higher area cost per torage cel l DRAM technology, sulting much lower storage de nsity per unit chip area driving cost much higher. course, latency SRAM -based storage much lower—as low hundred picoseconds small LI caches several nanoseconds larger L2 caches. bandwidth provided caches tremendous, cases exceeding 100 Gbytes/s. cost much harder estimate, ince high-speed custom cache SRAM available commodit prices ntegrated high-performance processors. However, ignoring nonrecurring expenses con- sidering $50 estimated manufacturing cost modern x86 pr ocessor chip like Pentium 4 incorporates 512K bytes cache ign oring cost processor core itself, arrive estimated cost per bit one hundredth cent per bit (i.e., rou ghly $10 0,000 per gigabyte). rVFTPi n E[X|A|MP It*Register File. Finally, fastest, smallest, expensive element modern memory hierarchy register file. register file responsible supplying operands execution units processor low latency—usually hun dred picoseconds, corresponding single processor cycle—and high bandwidth, satisfy multiple execution units parallel. Register file bandwidth approach 200 Gbytes/s modern eight-issue processor like IBM PowerPC 970, operates 2 GHz needs read two write one 8-byte operand eight issue slots cycle. Estimating cost per bit registe r file virtually impossible without detailed knowledge particular design yield characteristics; suffice say likely several orders magnitude higher estimate $10 0,000 per gigabyte on- chip cache memory. memory hi erarchy components attached processor hierar- chical fas hion provide overall storage system approximates five idealisms—infinite capacity, infinite bandwidth, zero latency, persistence, zero cost—as closely possible. Proper design effective memory hie rarchy requires careful analysis characteristics processor, programs operating system running processor, thorough understanding capabilities costs component memory hierarchy . Table 3.1 sum- marizes key attrib utes memory hierarchy components illustrates ban dwidth vary four orders magnitude, latency vary eight orders magnitude, cost per bit vary seven orders mag- nitude. drastic variations, continue change nonuniform rates asMEMORY I/O SYSTEMS 11 technology evolves , lend vast incredibly dynamic design space system architect. 3.4.2 Temporal Spatial Locality possible design memory hierarchy reason ably approximates infinite capacity bandwidth, low latency, persistence, low cost specified five memory idealisms? one assume truly random pattern accesses vast storage space, task would appear hopeless: excessive cost fast storage technologies prohibits larg e memory cap acity, long latency low bandwidth affordable technologies violates performance requirements system. Fortunately, empiricall observed attribute program execution called locality reference provides opportunity design- ing memory hierarchy manner satisfies seemingly contradictory requirements. Locality reference describes propensity computer programs access nearby memory locations frequently repeatedly. Specifically, break locality reference int two dimensions: temporal locality spatial locality. types locality common instruction data reference streams empirically observed use r-level application programs, shared library code, well operating system kernel code. Temporal locality refers accesses sam e memory location occur close together time; many real application programs exhibit tendency p rogram text instruction references, well data references. Figure 3.3(a) annotates example sequence memory ferences arrows representing temporal locality; arrow connects earlier later memory reference address. Temporal locality instruction reference stream easily explained, since caused loops pro gram execution. iteration loop executed, instructions forming body loop fetched ADEFCBACDGHEF 1 1 ' \ 11 (a) E A+l A+2 D+2 D+4 D+6 A+3 E+4 D+8 A+4 E+8 1t f f f j \ 1 1 1 1 1 (b) Figure 3.3 Illustration (a) emporal (b) Spatial Locality.114 MODERN PROCESSOR DESIGN again. Similarly, nested outer loops cause repetition occur coarser scale. Furthermore, even programs contain discernible loop struc- tures still share key subroutines called various locations; time subroutine called, temporally loca l instruction references occur. Within data reference stream, accesses widely used program variables lead temporal locality, acc esses current stack frame call-int ensive programs. call-stack frames deallocated procedure returns reallo- cated subsequent call, memory locations corresponding top stack accessed repeatedly pass parameters, spill registers, return func- tion results. activity leads abundant temporal locality data access stream. Spatial locality refers accesses nearby memory locations occur close together time. Figure 3.3(b) annotates example sequence memory refer- ences arrows representing temporal locality; earlier reference address (for example, A) followed references adjacent nearby addresses (A+l, A+2, A+3, on). Again, real application programs exhibit tendency instruction data referen ces. instruction stream, instructions make sequential execution path program laid sequentially program memory. Hence, absence branches jumps, instruction fetches sequence program memory linear fashion, subsequent accesses time also adjacent address space, leading abundant spatial locality. Even branches jumps cause discontinuities fetching, targets branches jumps often ne arby, maintaining spatial locality, though slightly coarser level. Spatial locality within data reference stream often occurs algorithmic reasons. example, numerical applications traverse large matrices data often access matrix elements serial fashion. long matrix elements laid memory order traversed, abundant spatial locality occurs. Applications stream large data files, like audio MP3 decoder encoders, also access data sequential, linear fashion, leading many spatially local references . Furthermore, accesses automatic variables call-intensive environments also exhibit spatial locality, since th e automatic variables given function laid adjacent stack frame corresp onding current function. course, possible w rite programs exhibit little temporal spatial locality. pro grams exist, difficult design cost- efficient memory hierarchy behaves well programs. programs classes applications deemed important enough, special-purpose hi gh-cost systems built execute them. past, many sup ercomputer designs optimized applications wit h limited locality reference avoide using many techniques introduced chapter (cache memories, virtual memory, DRAM main memory), ince tech niques require locality reference order effective. Fortunately, important applications exhibit locality benefit techniques. Hence, vast jority computer sys- tems designed today incorporate techniques.MEMORY I/O SYSTEMS 115 3.4.3 Caching Cache Memories principle caching instructions data paramount exploiting temporal spatial locality create illus ion fast yet capacious memory. Caches first proposed Wilke [1965] first implemented IBM System 360/85 1968 [Liptay, 1968]. C aching accomplished placing small, fast, expensive memory processor slow, large, inexpensive main memory, placing instructions data exhibit tempo- ral spatial reference locali ty cache mem ory. R eferences memory loca- tions cached satisfied quickly, reducing average memory reference latency, low latency small cache also naturally provides high bandwidth. Hence, cache e ffectively approximate second third memory idealisms—infinite bandwidth zero latency—for references satisfied cache. Since temporal spatial locality prevalent programs , even small first-level caches satisfy excess 90% references cases; referen ces said hit cache. references cannot satisfied cache called misses must satisfied slower, larger, memory behind cache. 3.4.3.1 Average Reference Latency. Caching extended multiple levels adding caches increasing capacity latency hierarchical fashion, using technologies enumerated Table 3.1. long level cache able capture reasonable f raction references sent it, r eference latency perceived pro cessor substantially lower references sent directly lowest level hierarchy. Th e average memory refer- ence latency computed using E quation (3.1), computes weighted average based distribution references satisfied level cache. latency satisfy reference level cache hierarchy defined /,, fr action references satisfied level ht. n Latency = £fc, x /, (3.1) i=0 equation makes clear long hit rates h, upper levels cache (those low latency /,) relatively high, average latency observed processor low. example, tw o-level cach e hierarchy ^ = 0.95, 1, = 1 ns, h2 = 0.04, l2 = 10 ns, h3 = 0.01, /3 = 100 ns deliver average latency 0.95 x 1 ns + 0.04 x 10 ns + 0.01 x 100 ns - 2.35 ns, nearly two orders magnitude faster simply sending reference directly lowest level. 3.4.3.2 Miss Rates Cycles per Instruction Estimates. Equation (3.1) assumes ht hit rates specified global hit rates, specify fraction memory references hit level memory hierarchy. often useful to-also understand local hit rates caches, specify fraction memory ferences serviced particular cache hit cache. first- level cache, global local hit rates same, since first-level cache116 MODERN PROCESSOR DESIGN services refe rences program. second-level cache, however, sei* vices thos e references result miss first-level cache. Similarly, third-level cache services references miss second-level cache, and, on. Hence, local hit rate lht cache level defined E quation (3.2). Ih, = (3.2) Returning earlier example, see local hit rate second-leve* cache Ih, = 0.04/(1 - 0.95) = 0.8. tells us 0.8 80% references ser-1 viced second-level cache also satisfied cache, 1 - 0.8 = 0.2* 20% sent next level. latter rate often called local miss rate, indicates fraction reference serviced part icular level cache- missed level. Note first-level cache, l ocal global hit rates equivalent, since first-level cache services references. true local global miss rates first-level cache. Finally, often useful report cache miss rates per-instruction miss rates. metric reports misses rmalized numbe r instructions executed, rather; number memory references performed provides intuitive basis, reasoning estimating p erformance effects v arious cache organi- zations. Given th e per-instruction miss rate m, specific execut ion-time pen alty p, miss cache system, one quickly estimate performance^ effect cache hierarchy using memory-time-per-instruction (MTPI) metric^ defined Equation (3.3). MTPI = £ rriiXpi (3.3) equation p, term equivalent latency term Z, used Equation (3.1). Instead, must reflect penalty associated miss level hierarchy, assuming reference satisfied next level. miss; penalties computed ifference latenc ies adjacent levels hierarchy, sh Equation (3.4). Pi = IM - h (3.4) Returning earlier example, h, = 0.95, Z, = 1 ns, h2 = 0.04, Z2 = 10 ns, Zi3 = 0.01;. Z3 = 100 ns, p, = (Z2 - Z,) = (10 ns - 1 ns) = 9 ns, difference Z, Z2 latencies reflects additional penalty missing thA first level fetch second level. Similarly, p2 = (Z3 - = (100 ns - 10 ns) = 90 ns, difference Z2 Z3 latencies. m, miss rates also expressed per-instruction miss rates need converted global miss rates used earlier. perform conversion, wc need know number references performed per instruction. assume thaiMEMORY I/O SYSTEMS 117 instruction fetched individually 40% instructions either loads stores, total n = (1 + 0.4) =1.4 references per instruction. Hence, compute per-instruction miss rates using Equation (3.5).  £ h Jmisses i=i ' n ref ref inst(3.5) Returning example, would find m1 = (l- 0.95) x 1.4 = 0.07 misses per instruction, m2 = [1 - (0.95 + 0.04)] X 1.4 = 0.014 misses per instruction. Finally, substituting E quation (3.3), compute memory-time-per- instruction metric MTPI = (0.07 x 9 ns) + (0.014 x 90 ns) = 0.63 + 1.26 = 1.89 ns per instruction. also conveniently expressed terms cycles per instruction normalizing cycle time proce ssor. example, assum- ing cycle time 1 ns, memory-cycles-per-instruction (MCPI) would 1.89 cycle per instruction. Note definition MTPI Equation (3.3) account latency spent servicing hits first level cache, time spent misses. definition useful performance modeling, since cleanly separates time spent processor core time spent ou tside core servicing misses. example, deal scalar processor pipeline would execute instructions rate one per cycle, resulting core cycles pe r instruction ( CPI) equal one. CPI assumes memory references hit cache; core CPI also often called perfect cache CPI, ince cache perfectly able satisfy references fixed hit l atency. shown, Equation (3.6), core CPI added MCPI computed previously reach actual CPI processor: CPI = 1.0 + 1.89 = 2.89 cycles per instruction rec urring example. CPI = CoreCPI + MCPI (3.6) However, one careful using equations reason absolute performance effects, since account overlap concurrency cache misses. Chapter 5, investigate numerous techniques exist express purpose maximizing overlap concurrency, see performance approximations like Equation (3.3) less effective pre dict- ing performance cache hierarchies incorporate techniques. 3.4,33 Effective Band width. Cache hierarchies ar e also useful sa tisfying second memory idealism infinite bandwidth. higher level cache hierarchy also inherently able provide higher bandwidth lower levels, due lower access latency, hierarchy whole manages maintain illusion infinite bandwidth. recurring example, latency first- level cache 1 ns, single-ported nonpipelined implementation provide bandwidth 1 billion references per second. contrast, second level, also pipelined, satisfy one reference e 10 ns, resulting bandwidth of118 MODERN PROCESSOR DESIGN 100 million references per second. course, possible increase concurrency lower levels provid e greater effective bandwidth either multiporting banking (see Section 3.4.4.2 explanation banking int erleaving) cache memory, pipelining initiate new requests rate greater inverse access l atency. Goodman [1983] conducted classic study bandwidth benefits cac hes. 3.4.3.4 Cache Organization Design. level cache hierarchy must designed way matches requirements bandwidth latency level. Since upper levels hierarchy must operate speeds comparableto proce ssor core, must impl emented using fast hardware techniques, necessarily limiting complexity. Lower cache hierarchy, latency critical, sophisticated schemes attractive, even software tech- niques widely deployed. However, levels, must efficient policies mechanisms place locating particular piece block data, evicting existing blocks make room newer ones, reliably handling upd ates block processor writes. section presents brief overview common approaches; additional implementation details provided Section 3.6. Locating Block. level must implement mechanism enables low- latency lookups check whether particular block cache-resident. two attributes determin e process locating block; first size block, second organization blocks within cache. Block size (sometimes referred line size) describes granularity cache operates. block contiguous serie bytes memory begins naturally aligned boundary. example, cache 16-byte blocks, block would contain 16 bytes, first byte block would aligned 16-byte b oundaries address space, implying low-order 4 bits address first byte would always zero (i.e., Ob — 0000). smallest usable block size n atural word size processor (i.e., 4 bytes 32-bit chine, 8 tes 64-bit machine), since access require cache supply least many bytes, splitting single access multi- ple blocks would introduce unacceptable overhead access path. practice, applications abundant spatial locality benefit larger blocks, ref- erence word within block p lace entire block cache. Hence, spatially local references fall within bo undaries block satisfied hits block installed cache response first reference block. Whenever block size greater 1 byte, low-order bits address must used find byte word accessed within block. stated above, low-order bits first byte block must always zero, corresponding naturally aligned block memory. However, byte first byte needs accessed, low-order bits must used block offset index block find right byte. number bits needed block offset log2 block size, enough bits available toMEMORY / SY STEMS 119 (a)£2 Tag Data1 \ (b) (c)Tag Data 1 Figure 3.4 Block Placement Schemes: ( ) Direct-Mapped, (b) Fully Associative, (c) Set-Associative. span bytes block. e xample, block size 64 bytes, log2(64) = 6 low-order bits used block offset. rema ining higher-order bits used locate appropriate block cache memory. second attribute deter mines blocks located, cache organiza- tion, determines blocks arranged cache contains multiple blocks. Figure 3.4 illustrates three fundamental approaches organ izing cache directly affect complexity th e lookup process: direct-mapped, fully associa- tive, set-associative. simplest approach, direct-mapped, forces many-to-one mapping addresses available storage locations cache. words, p articu- lar address reside single location cache; location usually determined extracting n bits address using n bits direct index one 2" possible locations ca che. course, since many-to-one mapping, location must also store tag contains remaining address bits corresponding block data stored location. lookup, hardware must read tag com- pare address bits reference performed determin e whether hit miss ha occurred. describe process greater detail Section 3.6. degenerate case direct-mapped memory contains enough storage locations every addres block (i.e., n index bits include bits address), tag needed, mapping addresses storage locations one-to-one instead many-to-one. register file inside processor example memory; need tagged since address bits (all bits register identifier) used ndex register file. second approach, fully associative, allows any-to-any mapping addresses available storage locations cache. organi- zation, memory address reside cache, locations must searched find right one ; hence, index bits extracted address determ ine storage location. Again, entry must tagged with120 MODERN PROCESSOR DESIGN address cu rrently holding, tags compared address current reference. Whichever entry matches used supply data; entry matches, iss occurred. final approach, set-associative, compromis e two. many-to-few mapping exists addresses storage locations. lookup, subset address bits used generate index, direct-mapped case. However, index corresponds set entries, usually two eight, searched parallel matching tag. practice, approach much efficient hardware implementation perspective, since requires fewer ad dress comparators fully associative cache, due- flexible mapping policy behaves similarly fully associative cache. Hill Smith [1989] present classi c evaluation associativity caches. Evicting Blocks. Since level cache finite capacity, must policy mechanism moving evicting current occupants make room blocks corresponding recent references. replacement policy cache determines algorithm used to. identify cand idate eviction. direct-mapped cache, trivial problem, since single potential candidate, single entry cache used store new block, current occupant entry must evicted free entry. fully associative set-associative caches, however, choice made, since new block placed one several entries, current occupants entries candidates eviction. three common policies implemented modern cache designs: first in, first (FIFO), least recently used (LRU), random. FIFO policy simply keeps track insertion order candidates evicts entry resided cache longest amount time. mechanism implements policy straightforward, since candidate eviction set (all blocks fully associative cache, blocks single set set-associative cache) managed c ircular queue. circular queue single pointer oldest entry used identify eviction candidate, pointer incre mented whenever new entry placed queue. results single update every miss cache. However, FIFO policy always match temporal locality char- acteristics inherent program's reference stream, since memory locations accessed continually throughout execution (e .g., comm referenced glo- bal variables). references would experience frequent misses FIFO policy, since blocks used satisfy would evicted regula r intervals, soon every block candidate eviction se evicted. LRU policy attempts mitigate problem keeping ordered list tracks recent references blocks form eviction set. Every time block refer enced hit miss, placed head ordered list, blocks set pushed list. Whenever block needs ev icted, one tail list chosen, since referenced least recently (hence name least recently u sed). Empirically, policyMEMORY I/O SYSTEMS 121 found work quite well, challenging implement, requires storing ordered list hardware updating list, every cache miss, every hit well. Quite often, practical hardware mechanism implement approximate LRU policy, rather exact LRU policy, due implementation challenges. instance approximate algorithm not-most-recently-used (NMRU) policy, history mechanism must remember block ferenced recently victimize one blocks, choos ing randomly one block choose from. case two-way associative cache, LRU NMRU eq uivalent, higher degrees assoc iativity, NMRU less exact simpler implement, since history list needs single element (the recently referenced block). final policy consider random replacement. name implies, thi policy block candidate eviction set chosen random. may sound risky, empirical studies shown random replace- ment slightly worse true LRU still significantly better FIFO. Clearly, implementing true random policy would difficult, practical mechanisms usually employ reasona ble pseudo-random approximation choosing block eviction candidate set. Handling Updates Block. presence cache memory sub- system implies existence one copy block memory system. Even single level cache, block currently cached also copy still stored main memory. long blocks read, never written, prob lem, since copies block exactly contents. However, processor writes block, mechanism must exist updating copies block, order guarantee effects write persist beyond time block r esides cache. two approaches handling problem: write-through caches writeback caches. write-through cache, name implies, simply propaga tes w rite cache next level. approach tractive due simplicity, since correctness easily main tained never ambiguity copy particular block current one. However, main draw- back amount bandwidt h required support it. Typical programs contain 15% writes, meaning one six instructions updates block memory. Provi ding adequate bandwidth th e lowest level memory hierarchy write rate practically impossible, given current contin- ually increasing disparity frequency processors memories. Hence, write-through pol icies rarely ever used throughout levels cache hierarchy. write-through cache must also decide whether fetch allocate space block experienced miss due write. write-allocate policy implies fetching block installing cache, write-no-allocate policy would avoid fetch would fetch install blocks read misses. main advantage write-no-allocate policy occurs streaming writes overwrite entire block unwritten part the122 MODERN PROCESSOR DESIGN block read. scenario, useless fetch data next level avoided (the fetched data useless since overwritten read). writeback cache, contrast, delays updating copies block order maintain correctness. writeback cache hierarchy, implicit priority order used find up-to-date copy block, copy updated. priority order corresponds levels cache hierarchy order searched processor ttempt- ing satisfy reference. ther words, block found highest level cache, copy updated, copies lower levels allowed become stale, since update propagated them. block found lower level, promoted top level cache updated there, leaving behind stale copies lower levels th e hierarchy. updated copy writeback cache also marked dirty bit flag indicate updated stale copies ex ist lower levels hierarchy. Ultimately, dirty block evicted make ro om blocks, written back next level hierarchy, make sure update block persists. copy next level becomes up-to-date copy must also dirty bit set, order ensure block get written back next level gets evicted. Writeback caches almost universally deployed, since require much less write bandwidth. Care must taken design caches c orrectly, updates ever dropped due losing track dirty cache line. revisit writeback hierarchies greater depth Chapter 11 context systems multiple processors multiple cache hierarchies. However, despite apparent drawbacks write-through caches, several modern processors, including IBM Power4 [Tendler et al., 2001] Sun UltraSPARC III [Lauterbach Horel, 1999], use write-through policy first level cache. schemes, hierarchy propagates writes second-level cache, also processor chip. Since next level cache chip, relatively easy provide adequate bandw idth write-through traffic, design first-level cache simplified, since longer needs serve sole repository up-to-date copy cache block never needs initiate writebacks dirty blocks evicted it. However, avoid excessive off-chip bandwidth consumption due write-throughs, second-level cache maintains dirty bits implement writeback policy. Figure 3.5 summarizes main parameters—block size, block organization, replacement policy, write policy, write-allocation policy—that used describe typica l cache design. 3.4.3.5 Cache Miss Classi fication. discussed Section 3.4.3.1, average reference latency delivered multilevel cache hierarchy computed average latencies level hierarchy, weighted global hit rate level. l atencies level determined technology used aggressiveness physical design, miss rates function theMEMORY / SYSTEMS 123 Cache design Block size Block organization Direct-mapped Fully associative ' Set-associative Block replacement policy FIFO LRU approximations LRU NMRU 1 Random ' Write policy Writeback ' Write-through Write-allocate ' Write-no-allocate Figure 3.5 Cache Design Parameters. organization cache access characteristics program run- ning processor. Attaining deeper understanding causes cache misses particular cache hi erarchy enables des igner realize shortcom- ings design di scover creative cost-effective solutions improving hierarchy. 3 C's model proposed Mark Hill [Hill, 1987] powerful intuitive tool classifying cache misses based underlying root cause. model troduces following mutually excl usive categories cache misses:  Cold compulsory misses. due program's first ference block memory. misses considered fundamental since cannot prevented caching technique.  Capacity misses. due insufficient capacity particular cache. Increasing capacity cache eliminate capacity misses occur cache. Hence, misses funda- mental, rather by-product finite cache organization.  Conflict misses. due imperfect allocation entries particular cache. Changing associativity indexing function used cache increase decrease number conflict isses. Hence, again, misses fundamental, rather by-product imperfect cache organiza- tion. fully associative cache organization eliminate conflict misses, since removes effects limited associativity indexing functions. Cold, capacity, conflict misses measured simulated cache hier- archy simulating three different cache organizations cache interest.124 MODERN PROCESSOR DESIGN first organization actual cache studied; notational convenience let's assume experiences cache isses. second organization fully associative cache capacity block size actual cache; experiences mf cache misses. third final organization fully associative cache block size inf inite capacity; experiences mc misses. number cold, capac ity, conflict misses computed  Cold misses = mc, number misses fu lly associative, nfinite cache.  Capacity misses -mf- mc, number additional misses finite fully associative cache infinite cache.  Conflict misses = ma-mf< number additional misses actual cache number misses fully associative, finite cache. Cold misses fundamental determined workin g set pro- gram question, rather cache organization. However, varying block size directly affects number cold misses exp erienced cache. Intuitively, becomes obvious considering two extreme block sizes: cache block size one word experi ence cold miss every unique word referenced program (this forms upper bound number cold misses cache organization), cache infinite block size expe rience single cold miss. latter true first reference install addressable memory c ache, resulting addi- tional misses type. course, practical cache organizations finite block size somewhere two endpoints, usually range 16 512 bytes. Capacity misses fundamental determined block size capacity cache. Clearly, capacity increases, the-number capacity misses reduced, since larger cache able captur e larger share program 's working set. c ontrast, block size increases, number unique blocks reside simultaneously cache fixed capacity decreases. Larger blocks tend utilized poorly, since probability program access words particular block decreases block gets bigger, leading lower effective capacity. result, fewer unique blocks decreased probability words block useful, larger block size usua lly results increased number capa city misses. ever, programs efficiently utilize contents large blocks would experience increase. Conflict misses also fundamental determined block size, capacity, associativity cache. Increased ca pacity invariably r educes number co nflict misses, ince probability conflict two accessed blocks reduced total number blocks reside cache simultaneously increases. capacity misses, larger number smaller blocks reduces probability conflict improves e ffective capacity, resulting likely fewer conflict misses. Similariy, increased associativi ty almost invariably reduce number conflict misses. (Problem 25 homework ask construct counterexample case.)MEMORY I/O SYSTEMS 125 Table 3.2 Interaction cache organization cache misses Cache Parameter Reduced capacity Increased capacity Reduced block size Increased block size Reduced associativity Increased associativity Writeback vs. write- Write-no-allocateCold Misses Capacity MISMSConflict Misses Overall Misses effect IncreaseLikely Increase Likely increase effect DecreaseLikely decrease Likely decrease IncreaseLikely decreaseLikely decrease Varies Decrease Likely increase Likely increase Varies effectNo effectLikely increase Likely increase effect effectLikely decrease Likely decrease effect effectNo effect effect Possible decrease Possible decreasePossible decrease Possible decrease Table 3.2 summarizes effects cache organizational parameters category cache misses, well overall misses. Note parameters unexpected effects, empirical evidence tells us programs, effects summarized Table 3.2. possible decrease noted write- no-allocate caches due blocks written never read; blocks never fetched cache, hence never incur type misses. directly reduces number cold misses directly reduce capacity misses, conflict misses, overall misses. 3.4.3.6 Exampl e Cache Hierarchy. Figure 3.6 illustrates typical two-level cache hierarchy, w CPU processor contains register file directly connected small, fast level-one instruction cache (LI I-$) small, fast level-one data cache (LI D-$). Since first-level primary caches rela- tively small, typically ranging 8 64K bytes, accessed quickly, usually single processor cycle, provide enough bandwidth keep processor core busy enough instructions data. course, rare cases provide enough capacity contain work- ing set program Inevitably, program issue reference found first-level ca che. reference results cache miss, refer- ence needs forwarded next level memory hierarchy. case example Figure 3.6, level-tw cache, contains program text data subs tantially larger. Modem second-level caches range 256K bytes 16 Mbytes, access latencies nanoseconds 10 15 ns large, off-chip level-two caches. Modern processors usually incorporate second l evel cache chip, recent processor designs like Intel Xeon Itanium 2 actually add third level cache onboard processor chip. High-end system designs like IBM xSeries 445126 MODERN PROCESSOR DESIGN CPU Reg File256 te-1 K-by te capacity «l-ns latency (typical) CU I-$ELI D-$3 8K-128K-byte capacity 3 < 1 -ns latency (typical) 1 7$128K-byte-16-Mbyte capacity -10-ns latency (typical) Memory K|A P E ^ Figure 3.6 Atypical Memory Hierarchy\28-Mbyte-100+ Gbyte capacity -lOO-ns latenc (typical) multiprocessors employ Itanium 2 processors intended extremely memory-intensive server applications even include fourth level cache memory system board. Finally, physical memory hierarchy backed DRAM ranges size 128 Mbytes entry-level desktop PCs 100 Gbytes high- end server systems. latency reference mu st satisfied DRAM typically least 100 ns, though somewhat less single-processor system. Systems multiple processors share memory typically pay overhead maintaining cache coherence increases latency main memory accesses, cases 1000 ns. Ch apter 11 discusses many issues related efficient support coherent shared memory. light example shown Figure 3.6, let's revisit five memory idealisms introduced earlier chapter:  Infinite capacity. storing large data sets large programs.  Infinite bandwidth. rapidly streaming large data sets programs processor.  Instantaneous zero latency. prevent processor stalling waiting data program code.  Persistence nonvolatility. allow data programs survive even power supply cut off.  Zero low implementation cost. MEMORY 1 / 0 SYSTEMS 127 see highest levels memory hierarchy—register files primary caches—are able supply near-infinite band width low average latency processor core, satisfying second third idealisms. first idealism— infinite capacity—is satisfied lowest level memory hierarchy, since capacities DRAM-based memories large enough contain working sets modern applications; applications case. Section 3.5 describes technique called virtual memory extends memory hierarchy beyon random-access memory devices magnetic disks, provide capacities exceed demands demanding applications. fourth idealism—persistence nonvolatility—can also supplied magnetic disks, designed retain state even powered down. final idealism—low implementation cost—is also satisfied, since high per-bit cost upper levels cache hierarchy multiplied rela- tively small number bits, lower levels hierarchy pro vide tre- mendous capacity low cost per bit. Hence, average cost per bit kept near low cost commodity DRAM magnetic disks, rather high cost custom SRAM cache memories. 3.4.4 Main Memory typical modern computer system, main memory buil standardized commodity DRAM chips organized flexible, expandable manner pr ovide substantial capacity expandability, high bandwidth, asonably low access latency slightly higher access latency DRAM chips themselves. Since current-generation DRAM chips ha capacity 256 megabits, computer system 1 Gbyte memory would require approximately 32 DRAM chips storage; including overhead parity error-correction codes detect tolerate soft errors would typically increase count 36 chips. Next- generation DRAM chips, around corner, provide 1 gigabit capacity each, reducing chip count factor 4. However, demand increased memory capacity future systems likely keep total number DRAM chips required sat isfy capacity relatively constant. Clearly, many possible ways configure large number DRAM chips optimize cost, latency, bandwidth, expandability. Figure 3.7 illus- trates one possible approach arranging interconnecting memory chips. configuration, multiple DRAM chips mounted dual inline memory module (DIMM); multiple DIMMs connected shared port bank, one banks co nnected memory controller. turn, memory controller connects system's processor bus responds processor's memory requests issuing appropriate command one memory banks. Sec- tion 3.4.4.1 introduces basic principles DRAM chip organization, Section 3.4.4.2 discusses several key issues memory controller design. 3.4.4.1 DRAM Chip Organization. DRAM chips commodity product manufactured several competing ven dors worldwide. DRAM manufact urers collaborate standardizing specification capacities interfaces of128 MODERN PROCESSOR DESIGN ReadQ WriteQ RespQ USMemory controller Scheduler -*\ Buffer BankOCommands,, Data JBankl 1 XCommands ,. Data H. DDDDD H. DDDDD DlMM(s) DIMM(s) Figure 3.7 Typical Main Memory Organization. generation DRAM chips order guarantee compatibility consistent performance. Conceptually, function organization DRAM chips quite straightforward, since designed store many bits possible as' compact area possible, minimizing area product cost maximiz- ing bandwidth. factors considered DRAM design, historically primary design constraints capacity cost. DRAM manufacturing extremely competitive business, even minor increases product cost, potentially caused complex designs reduce process yields, drive vendor business. Hence, DRAM vendors typically "very conservative adopting dramatically new ifferent approaches building DRAM c hips. semiconductor process geometries shrunk, DRAM capacity per chip increased rate less directly proportional Moore's law, predicts doubling devices per chip every two year so. has,resulted exponential growth capacity DRAM chips fixed die ize, turn tended hold product cost roughly constant. Despite fact device switching times improve reduced process geometries, DRAM chip latency improved dramatically. due fact DRAM access latency dominated wire delay, device switching times. Since wir e delay improved nearly dramatically device switching delay, overall dimension memory array r emained largely fixed (to accommodate increased capac- ity), end-to-end latency retrieve word DRAM chip improved compound rate 10% per year. provides stark contrastMEMORY I/O SYSTEMS 1 21 60% compound rate frequency growth observed gene ral-purpose microprocessors. divergenc e device frequency led many computer sys- tem designers researchers search new techniques surmount known memory wall [Wulf McKee, 1995]. main contributor DRAM product cost—packaging, driven per-chip pin count—has also remai ned relatively stable th e years, resulting dearth dramatic improvements bandwidth per chip. improvements made bandwidth largely realm enhanced signaling technology, synchronous interfaces [synchronous DRA (SDRAM)], higher inter- face frequencies (e.g., PC 100 runs 100 MHz, PC133 runs 133 MHz), aggressive use rising falling clock edges transmit twice amount data per clock period [known double-data rate (DDR)]. Figure 3.8 shows internal organization typical DRAM chip. heart, array binary storage elements organized rows col umns. storage elements tiny capacitors, store charge represent 1, store charge represent 0. capacitor-based cell connecte tran- sistor vertical bit line stretches top array bottom. transistor controlled horizontal word line selects particular row array either reading writing. bit line used read state cell: charged capacitor drive bit line higher voltage; higher volt- age sensed high-gain amplifier one end bit line converts signal standard logic level 1, discharged capacitor drain Bit lines Data bus Figure 3.8 DRAM Chip Organization.130 MODERN PROCESSOR DESIGN charge bitline, reducing voltage level tha amplified logic level 0. high-gain analog amplifier called sense amp relies bit-line precharging, process presets bi t-line voltage prior read interme- diate value swing high low quickly depending state accessed cell's capacitor. bit line also used store value cell driving bit line high store charge capacitor, driving low drain charge ca pacitor. Since charge stored capacitor decays time, cells DRAM chip must freshed periodically maintain state. dynamic behavior lends naming DRAM chips; acronym stands dynamic random-access memory. contrast, SRAM static random- access memory, employed higher levels cache hierarchy, need refreshed since th e storage cells static complementa ry metal-on-semiconductor (CMOS) circuits (a pair cross-cou pled inverters) hold tate indefi- nitely, long power supplied them. DRAM Addressing. word lines Figure 3.8 used select row within array either read row (i.e., let capacitors row drive bit lines) write (i.e., let bit lines driv e drain capacitors row). row address n bits must supplied DRAM chip, decoder circuit activates one 2" word lines corresponds supplied row address. storage cell, word line controls pass transistor either isolates capacitor bit line connects bit line, enable selective reading writing single row. read, bits selected row sensed sense amps stored row bu ffer. sub sequent command cycle, column address bits must also supplied; used select one 2m words within row. DRAM Access Lat ency. latency access random storage locatio n within DRAM chip determined inherent latency storage array aug- mented overhead required com municate chip. Since DRAM chips cost-sensitive, increased pin count drives cost higher, DRAM interfaces typically share physical pin several p urposes. example, DRAM chip provide enough address pins directly address word within array; instead, address pins time-multiplexed first provide row address ro w address strobe (RAS) control line asserted, followed column address strobe (CAS) column address provided. long equal number rows columns, half number address pins needed, reducing cost significantly. hand, two transactions required across interface provide complete address, increasing latency acc ess. decoupling row column addresses creates opportunity optimizing access latency memory references exhibit spatial locality. Since DRAM chip reads entire row row buffer, selects word row based column address, possible provide multiple column addresses back-to-back cycles read bursts words row high rate, usually limited interface frequency data rate.MEMORY I/O SYSTEMS 13' Historically, back-to-back acce sses row called page-mode accesses. same-row accesses complete much faster (up 3 times faster current-generation DRAM) since incur additional latency pro- viding row address, decoding row address, precharging bit lines, reading row memory cell array row bu ffer. creates performance-enhancing scheduling opportunity memory con troller, revisit Section 3.4.4.2. DRAM chips also share pins data bus reads writes. easy pipeline stream reads stream writes across interface, alternating reads writes requires bus turnaround. Since DRAM chip driving data bus ads, memory controller driving writes, care must taken ensure never time memory controller DRAM chip attempting drive bus, could result short-circuit c ondition respective drivers. Hence, interface must carefully designed allow enough timing margin current bus master (e.g., DRAM chip read) stop driving bus new bus master (e.g., memory controller write) starts drive bus. results additional delay reduces efficiency DRAM interface. Rambus DRAM. late 1990s, alternativ e standard called Rambus DRAM (RDRAM) emerged research high-speed signaling Stanford University. RDRAM employed many techniq ues recent sta ndards synchronous DRAM (e.g., DDR2) since adopted, including advanced sig- naling, higher interface frequencies, multiple data words per clock period. RDRAM chips also provide row buffer cache contains se veral recently accessed DRAM rows; increases probability random access satisfied much lower latency one row buffer entries, avoid- ing row access latency. improve interface bandwidth, RDRAM carefully specifies physical design board-level traces used connect RDRAM chips controller, uses source-synchronous clocking (i.e., clock signal trav- els data) drive clock frequencies several times higher con- ventional SDRAM approaches. result hese optimizations, RDRAM able provide substantially higher bandwidth per pin, albeit higher product cost increased design time due stringent physical design requirements. High bandwidth per pin useful systems require lots band- width relatively little capacity . Examples systems use RDRAM Sony Playstation 2 Microsoft X-Box game controllers, provide 32 Mbytes memory base configuration requiring lots memory bandwidth support intensive three-dimensional gaming. modest capacity requirement 32 Mbytes could satisfied single current- generation 256-Mbit DRAM chip. However, since DRAM chip small number data pins (between 2 16, typically 4 8), pin must provide high bandwidth satisfy system's overall bandwidth demand. contrast, general- purpose computer systems personal computers typically contain leas order magnitude mory, ranging atE _132 MODERN PROCESSOR DESIGN least 256 Mbit several gigabytes. system, per-pin bandwidth lessl important, since many DRAM chips required provide requisite capacity anyway, chips arranged parallel provide wide interface-; supplies required bandwidth. c ourse, increases package cost of* memory controller chip, since ha enough pins acces multiple DRAM chips parallel. hand, product cost lowered usingless aggressive less expensive circuit board technology, since pin signals lower rate. Detailed performance evaluation comparison various modem DRAM technologies found two recent studies [Cuppu et al., 1999; Cuppu and*; Jacob, 2001]. 3.4.4.2 Memory Controller Organization. Memory controllers serve interface system's processor bus, communicates reads writes issued processor, standard DRA command interface, which, expects tighdy specified sequence commands supplying row addresses, column addresses, read w rite commands DRAM chip. many alternatives arrangin g mapping physical addresses pro- vided pr ocessor DRAM addr esses needed access actual storage- locations. Furthermore, various optimizations applied within the.; memory controller improve read performance increase spatial local ity DRAM reference tream. net effect design memory controller substantially impact sustainable memory bandwidth observed latency j memory references. following sections discuss issues detail. Memory Module Organization. desired memory capacity system < determines total number DRAM chips needed provide capacity. prac- tice, systems des igned support range capacities enable use sev- eral generations memory chips allow future expansion initial* configuration. However, simplicity wil l assume fixed capacity only, result- ing fixed number DRAM chips memory subsystem. Figure 3.9 illus- trates four possible organizations four DRAM chips system, etermined J two fundamental attributes: serial vs. parallel interleaved vs. non-interleaved. top left case, DRAM chips share addr ess, command, data lines, single chip active time selected via chip select'i (CS) control line. case, number pins required controller mini- mized, since pins except CS control lines shared. However, data band- width limited data bus width DRAM chip. DRAM chips 2,4, 8, 16 data lines—typically 4 8 current-generation chips—with price pre- mium charged wider interfaces. organization, transaction b andwidth is' restricted one conc urrent command, since address command l ines shared across chips. top right case Figure 3.9 shows parallel organization, chips active c ommands (hence, chip selects), n-bit data bus- ses chip concatenated form 4n-bit interface. configuration provides much better data bandwidth, higher memory controller cost due toMEMORY I/O SYSTEMS 1; private data bus pins DRAM chip. provides increase transac- tion bandwidth, since address command lines still shared. bottom left shows interleaved (or banked) organization half DRAM chips connected one set chip select, address, command, data busses, half connected second set. organization pro- vides twice data bandwidth, also twice transaction bandwidth, since interleaved bank operate independently, cost twice many address command pins. final configuration, bottom right, combines parallel interleaved organizations, providing twice transaction band- width interleaving, four times data band width two 2/i -bit wide data busses. course, final configuration requires highest cost largest number pins the'memory controller. possible combinations techniques (for example, semi-parallel scheme half—instead all—DRAM chips parallel, chip selects still used select half drives 2/i-bit wide data bus). combinations , however, th e linear physical address presented processor needs translated n-tuple describes actual physical location memory locatio n addressed. Table 3.3 sum marizes con- tents DRAM address n-tuple se veral approaches rganizing memory modules. Linear physical addresses mapped n-tuples selecting appropriately sized su bsets bits physical address form element34 MODERN PROCESSOR DESIGN able 3.3 ranslating li near physical address DRAM address* Organization DRAM Address Example Physical Corresponding (Figure 3.9) Breakdown Address DRAM Address Serial non-interleaved, 16 1211 21 0 0x4321 RAS: 0x4, CAS: 0xC8, 8-bit data bus 1 RAS | CAS | CS | CS:0xl Parallel non-interleaved, 16 1211 21 0 0x4320 RAS: 0x4, CAS: 0xO3 32-bit data bus 1 RAS 1 CAS | n/a | Serial interleaved, 16 1211 2 1 0 0x8251 RAS: 0x4, CAS: 0x94, 8-bit data bus R [ C |Ba|CS| Bank: 0x0, CS:0x0 Parallel interleaved. 16 1211 2 1 0 0x8254 RAS: 0x4, CAS: 0x95, 16-bit data bus | R | C |Ba|n/a| Bank: 0x0 Examples assume 4 x 256-kbit DRAM w h 8-bit data path 8-kbit row, total 128 kB addressable memory. 1 n-tuple. general, sel ection problem Q possible solutions, x number physical address bits total number bits needed specify n-tuple elements. Table 3.3 shows one many possible ways choosing bits element DRAM address n-tuple. Regardless organization chose n, RAS bits selected maximize number row hits; careful study access patterns impor- tant applications reveal address bits best candidates RAS. Fur- thermore, interleaved design, bits used bank selection need selected carefully ensure even distribution ferences across memory banks, since poor choice bank bits direct references single bank, negating bandwidth benefit expected presence multiple banks. Components Memory Controller. shown Figure 3.7, memory con- troller contains interface processor bus interface DRAM chips. also contains logic buffering read write comm ands processor bus (the ReadQ WriteQ), response queue (RespQ) buffering res ponses heading back processor, scheduling logic issuing DRAM commands satisfy processor's read write requests, buffer space assemble wide data responses multiple narrow DRAM reads. reassembly needed whenever processor bus issues read commands wider DRAM data interface; cases multiple DRAM reads performed assemble block matches width processor's read request (which usually width cache block processor's cache). similar fashion, wide writes processor bus need dec omposed multiple narrower writes DRAM chips. Although Figure 3.7 shows memory controller physically separate entity, recent designs, exemp lified AMD Opteron [Keltcher et al„ 2003], integrate memory controller directly chip minimize memory latency, simplify chipset design, reduc e overall system cost. One drawbacks on-chip memoryMEMORY I/O SYSTEMS 1, controller processor designs must synchronized evolving memory standards. example, Opteron processors must redesigned take advan- tage new DDR2 DRAM standard, since onboard controller work older DDR standard. contrast, off-chip memory controller {North Bridge Intel/PC terminology) qui ckly redesigned replaced match new memory standards. ReadQ used buffer multiple outstanding reads; decouple comple- tion read accepting next one. Quite often processor issue reads bursts, cache misses tend occur clusters accepting multiple reads ReadQ prevents bus stalling. Queueing multiple req uests may also expose locality memory controller exploit schedules DRAM commands. Similarly, WriteQ prevents bus processor stall- ing allowing multiple writes outstanding time. Furthermore, WriteQ enables latency-enhancing optimization reads: since writes usually latency-critical, WriteQ delay favor outstanding reads, allow- ing reads satisfied first DRAM. delayed writes ca n retired whenever pending reads, utilizing idle memory channel cycles. Memory Reference Scheduling. course, reference reordering memory controller subject correctness requirements pipelined processor maintaining read-after-write (RAW), write-after-read (WAR), write-after- write (WAW) dependences. effect, means reads cannot reordered past pending writes address (RAW) , writes cannot reordered past pending reads address (WAR), writes cannot bypass pending writes address (WAW). reordering reads respect outstanding w rites, RAW condition needs checked. RAW condi- tion exists pending write newer read, read must either stall wait write performed DRAM, read satisfied directly write queue. Either solutio n maintain correctness, latter improve performance, since latency read on-chip WriteQ lower read external DRAM chip. However, Section 3.4.4.1 showed DRAM chips exploit spatial locality fetching multiple words row issuing different column addresses DRAM back-to-back cycles. references satisfied much quickly references different rows, incur latency row address transfer row read internal DRAM array. current-generation DRAMs, rows large 8 kilobits; eight-wide parallel organization (extrapolating two-wide parallel scheme shown Figure 3.9) would result 8-kilobits row physical memory. Accesses row satisfied much quickly ferences ther rows. Hence , scheduling logic advanced memory controllers attempt find references row ReadQ WriteQ attempt schedule together increase number row hits. type scheduling optimization substantially reduce average DRAM read latency improve sustained memory bandwidth, dramati- cally complicate scheduling logic well ReadQ WriteQ bypass logic.136 MODERN PROCESSOR DESIGN Current-generation DRAM chips also inter nally interleaved banked. typical chip contain four independent banks replicate structures shown Figure 3.8, shar ing external address, data, control lines. Internal DRAM banking allows memory controller overlap di fferent types commands bank; example, bank 0 begin bit-line precharge cycle bank 1 pe rforming row access bank 2 performing column access. Furthermore, bank separate row buffer, allows memory con- troller leave multiple rows open concurrently, increasing probabilit future request hit open row, reducing access latency request Finally, banking interleaving DRAM terface increases transaction bandwidth memory controller, since multiple banks operate indepen- dently concurrently, long ReadQ WriteQ contain references different banks. High-en memory c ontrollers multiprocessor server systems many independent memory banks; commodity PC systems typically one two. final note, parallel interleaved organizations described DRAM systems also applied SRAM caches higher levels memory hierarchy. particular, multibanked caches commonly used increase transac- tion bandwidth c ache. example, Intel Pentium processor incorporates eight-way interleaved primary data cache support co ncurrent memory accesses dual pipelines [Intel Corp., 1993]. Similarly, IBM Power four-chip multiprocessor described Chapter 6 three-way interleaved on-chip level-2 cache support concurrent requests two processor cores chip [Tendler et al., 2001]. 3.5 Virtual Memory Systems far, considered levels memory hierarchy tha employ random- access storage technology. However, modern high-performance computer systems, lowest level memory hierarchy actually imple mented using magnetic disks paging device backing store physical memory, comprising virtual memory system. backing store cont ains blocks memory displaced main memory due capacity reasons, blocks displaced caches placed either next level cache hierarchy main memory. Historically, virtual memory predates caches first troduced 40 years ago time-shared mainframe computers enable sharing precious commodity— main memory—among multiple active programs [Kilbum et al., 1962]. Virtual memory, name implies, virtualizes main memory separating program- mer's view memory actual physical placement blocks memory. adding layer cooperating hardware software manages mappings program's virtual address physical address actually stores data program text referenced. process address translation illustrated Figure 3.10. layer cooperating hardware soft- ware enab les address translation called virtual memory system isMEMORY I/O SYSTEMS 1 (^jrrualaddressjMain memory Address translation PhvsTal xMrm' Figure 3.10 virtual Physical Address Translation. responsible fo r maintaining illusion virtually addressable memory resident physical memory transparently accessed program, also efficiently sharing limited physical resources among competing demands multiple active programs. contrast, time-sharing systems pr edated failed pr ovide virtual memory handi capped users programmers requiring explicitly man- age physical memory shared resource . Portions physical memory statically allocated concurrent programs; portions manually replaced evicted allocate new space; cumbersome techni ques data program overlays employed reduce minimize amount space consumed program. example, program would explicitly load unload overlays corresponded explicit phases program execution, since loading entire program data set could either overwhelm physical memory starve c oncurrent programs. Instead, virtual memory system allows co ncurrent program allocate occupy much memory system's backing store virtual address space allows: 4 Gbytes machine 32-bit virtual addresses, assuming adequate backing store available. Meanwhile, separate demand paging mecha- nism manages placement memory either limited physical memory system's capacious backin g store, based policies virtual memory system. system respo nsible providing illusion virtually addressable memory resident physical memory transparently accessed program. illusion practically finite capacity requ irement transparent access sound quite similar th e principles caching described Section 3.4.3; fact, underlying principles temporal spatial locality, well poli- cies locating, evicting, handling u pdates blocks, conceptually similar virtual memory subsystems cache memories. However, since relative latencies accessing backing store much higher the' latencies satisfying cache miss next level physical memory hierarchy, policies particularly mechanisms differ substa ntially. refer- ence block resides backing store inflicts 10 ms of138 MODERN PROCESSOR DESIGN latency read block disk. pure hardware replacement scheme stalls processor waiting amount time would result poor utilization, since 10 ms corresponds approximately 10 million instruction execution opportunities processor executes one instruction per nanosec- ond. Hence, virtual memory subsystems implemented hybrid hardware software, references blocks reside physica l memory satis- fied quickly efficiently hardware, references miss invoke operating system page fault exception, initiates disk transfer also able schedule other, ready task execute window time initiat ing completing disk request. Furthermore, operat- ing system becomes r esponsible fo r implementing policy evicting blocks allocate space new block fetched disk. study issues detail Section 3.5.1. However, additional complicat ion arises fact multi- ple programs sharing physical memory: somehow pro- tected accessing others' memory, either accidentally due malicious program attempting spy subvert another conc urrent program. typ ical modem system, program runs virtual address space, disjoint address space concurrent pro gram long over- lap address spaces, operating system need ensure two con current address mappings different programs ever poin physical location, protection ensured. However, limit functionality, since two programs cannot communicate via shared memory location, also reduce p erform mance, since duplicates bjects may need exist memory satisfy needs multiple programs. two reasons, virtual memory systems typ- ically provide mechanisms protecting regions memory map program's address space; protection mechanisms allow e fficient sharing communication occur. describe Section 3.5.2. Finally, virtual memory system must provid e architected means trans- lating virtual address physical ddress structure storing mappings. outline several schemes Section 3.5.3. 3.5.1 Demand Paging Figure 3.11 shows example single process consumes virtual address space three reg ions: program text (to load program binaries shared libraries); process stack (for activation records automati c storage); process heap (for dyn amically allocated memory). se three regions noncontiguous, leaving unused holes virtual address space, regions accessed relatively spar sely. Practically speaking, regions currently accessed need reside physical memory (shown shaded figure), unaccessed rarely accessed regions stored paging device backing store, enabling use systemwith limited amount physical memory programs consume large frac- tions address space, or, alternatively, freeing main memory applications time-shared system.MEMORY I/O SYSTEMS 13' Virtual address space Physical memory Paging deviceProcess heap Process stack Program text Figure 3.11 Virtual Memory System. virtual memory demand paging system must track regions memory reasona ble granularity. caches track memory blocks, demand paging system must choose page size mi nimum granularity locating evicting blocks main memory. Typical page sizes current-generation systems 4K 8K bytes. modem systems also supp ort variable-sized pages multi- ple page sizes efficiently manage larger regions memory. However, restrict discussion fixed-size pages. Providing virtual memory subsystem relieves programmer manually explicitly manage program's use physical memory. Fur- thermore, enables efficient execution classes algorithms use virtual address space greedily sparsely, since avoids allocating physical memory untouched regions virtual memory. Virtual memory relies lazy allocation achieve purpose." instead eagerly allocating space program's needs, defers allocation program actually references memory. requires means program communicate virtual memory subsystem needs reference memory previously accessed. demand-paged system, communication occurs page- fault exception. Initially, new program starts up, none virtual address space may allocated physical memory. However, soon program attempts fetch instruction perform load store virtual memory location currendy virtual mory, page fault occurs. hardware registers page fault whenever cannot find valid translation current vir- tual address. conceptually similar cache memor experiencing miss whenever cannot find matching tag performs cache lookup.140 MODERN PROCESSOR DESIGN However, page fault handled implicitly hardware mechanism; rather, transfers control operating system, allocates page virtual address, creates mapping tween virtual physical addresses, installs contents page physical memory (usually accessing backing store magnetic disk), returns control faulting pro gram. program able continue ex ecution, since hardware satisfy vir tual address reference corresponding physical memory location. Detecting Page Fault. detect page fault, hardware must fail find valid apping current virtual address. requires architected structure hardware searches valid mappings raises page fault exception operating system. operating system's exception handler code invoked handle th e exception create valid map ping. Section 3.5.3 discusses several schemes storing mappings. Page Allocation. llocating space new virtual memory page similar allocating space new block ca che, depends page organiza- tion. Current virtual memory systems use fully-associative policy placing virtual pages physical memory, since leads efficient use main memory, overhead performing associative search significant compared overall latency handling page fault. However, must policy evicting active page whenever memory comple tely full. Since least- recently-used (LRU) policy would expensive implement thousands pages reasonably sized physical memory, current operating systems use approximation LRU called clock algorithm. scheme, page physical memory maintains reference bit set hardware whenever reference occurs page. operating system intermittently clears reference bits. Subsequent references set page reference bits, effec- tively marking pages referenced recendy. virtual memory system needs find page evict randomly chooses page set pages wit h cleared reference bits. scheme avoids evicting pages referenced since last time reference bits cleared, providing coarse approximation LRU policy. Alternatively, operating system easily implement FIFO policy evict- ing pages maintaining ordered list pages fetched main memory backing store. optimal, scheme perform reason- ably well easy implement since avoids overhead clock algorithm. page chosen eviction, operating system must place backing store, usually pe rforming write contents page mag- netic disk. write avoided hardware maintains change bit dirty bit page, dirty bit set. similar principle dirty bits writeback cache, w blocks heir dirty bit set need written back next level cache hierarchy evicted. Accessing Backing Store. backing store needs accessed supply paged contents virtual page installed physical memory.MEMORY I/O SYSTEMS 1 User process 1 (-RunningTranslation found; process sleeps Process O/S supervisor User process 2 Backing storePage table Evict victim; Schedule search initiate I/O read process 2 1 Running Fetch missing pageSchedule process 1 Figure 3.12 Handling Page Fault. Typically, involves issuing read agnetic disk, latency exceeding 10 ms. Multitasking operating systems put page-faulting task sleep dur ation disk read schedule active task run processo r instead. Figure 3.12 il lustrates steps occur satisfy page fault: first cur- rent process 1 fails find valid translation memory location attempting access; perating system supervisor invoked search page table valid translation via p age fault handler routine; failing find translation, supervisor evicts physical page make room faulting page initiates I/O read backing store fetch page; supervisor scheduler runs find ready task occupy CPU process 1 waits page fault satisfied; process 2 runs backing store completes read; supervisor notified read completes, runs scheduler find waiting proc ess 1; finally, process 1 resumes execution CPU. 3.5.2 Memory Protection system time-shares physical memory system use virtual memory allows physical memory concurrently contain pages multiple processes. scenarios, desirable allow multiple processes access physical page, order enable communication processes avoid keeping duplicate copies identical program binaries shared libraries memory. Furthermore , operating system kernel, also resident physical pages, ust able protect internal data structures user-level programs. virtual memory subsystem must provide means protecting shared pages defective malicious programs might corrupt state. Furthermore, even haring occurring, protecting various address ranges certain types accesses useful ensuring correct execution debugging new programs, since erroneous references flagged protec- tion mechanism.inJlf142 MODERN PROCESSOR DESIGN Typical virtual memory systems allow page g ranted separat e read, write, execute permissions. hardware responsible checking instruction fetches occur pages grant execute permi ssion, loads occur pages grant read permission, writes occur pages grant write permission. permissions mai ntained parallel virtual physical translations manipulated su pervisor-state code running operating system kernel. references violate permissions sp ecified page blocked, operating syste exception handler invoked deal problem, usually resulting termination offending process. Permission bits enable effi cient sharing read-only objects like program bina- ries share libraries. multiple concurr ent pro cesses executing program binary, single copy program needs reside physi- cal memory, since kernel map physical copy address space process. result multiple virtual-physical address map- pings physical ddress same. referred virtual address aliasing. Similarly, rea d-only objects ca n shared. Furthermore, p rograms need communicate request shared space operat- ing system communicate directly writing reading shared physical address. Again, sharing achieved via multiple v irtual mappings (one per process) physical ddress, appropriate read and/or write permissions set process sharing mem ory. 3.5.3 Page Table Architectures virtual address physical address mappings stored tran slation memory. operating system responsible updating mappings whenever need change, proce ssor must access translation memory determine physical address virtual address reference performs. translation entry contains fields shown Figure 3.13: virtual address, corresponding physical address, permission bits reading (Rp), writing (Wp), executing (Ep), well reference ( Ref) change (Ch) bits, possibly caching-inhibited bit (Ca). reference bit used demand paging systems eviction algorithm find pages replace, change bit plays part Virtual address Real address Rp Wp Ep Ref Ch c Caching-inhibited bit Change bitReference bit Execute permission Write permission Read permission Figure 3.13 Typical Page Table Entry.MEMORY I/O SYSTEMS 14 dirty bit, indicating eviction candidate needs written back backing store. caching-inhibited bit used flag pages memory not, either r^rformance correctness r easons, stored processor's cache hierar- chy. nstead, references addresses mu st communicated directly processor bus. learn Section 3.7.3 caching-inhibited bit vitally important communicating I/O devices memory-mapped control registers. translation memories usually called page tables organized either forward page tables inverted page tables (the latter often called hashed page tables well). simplest, forward page table contains page table entry every possible page-sized block virtual address space process using page table. However, would result large structure many unused entries, since processes consume virtual address space. Hence, forward page tables usually tructured multiple lev- els, shown Figure 3 .14. approach, th e virtual address ecomposed multiple section s. highest-order bits address added page table base register (PTBR), points base first level page table. first lookup provides pointer next table; next set bits virtual address added pointer find pointer next level. Finally, pointer added next set virtual add ress bits find final leaf-level page table entry, provides actual physical address permission bits corresponding virtual address. course, multilevel page table extended three levels shown Figure 3.14. multilevel forward page table efficiently store translations sparsely populated virtual address space, si nce leaf nodes needed portions Offset PTBR Figure 3.14 Multilevel Forward Page Table.144 MODERN PROCESSOR DESIGN address space actually use; unused tables middle leaf levels lazily allocated wh en operating system actually allocates storage portions address space. Furthermore, pa ge table entries stored virtual memory, allowing paged backing store. lead nested page faults, initial page fault experi ences second page fault trying find translation informatio n virtual address. paging page table allowed, root level page table needs remain resident physical memory avoid unserviceable page fault. alternative page ta ble organization derives observation litde motivation provide translation entries pages actually fit physical memory. inverted page table, enough entries map physical memory, rather enough entries map virtual memory. Since inverted page able far fewer entries fits comfortably main memory, need make pageable. Rather, operating sys- tem access directly physical addressing. Figure 3.15 illustrates translation entries found inverted hashed page table. virtual address hashed, usually applying exclusive-OR func- tion nonoverlapping portions virtual address, added page table base register. resulting address used directly physical address find set page tabl e entries (PTE PTE3 Figure 3.15). page table entries checked sequentially find matching entry. Multiple entries need searched provided, since possible multiple virtual addresses hash location. fact, possible number virtual page numbers map page table entry group exceed capacity group; results overflow condition induces additional page faults. e ffect, space physical memory al located set-associative manner, rather Virtual page numberOffset PTBRHash PTEO PTE1 PTE2 PTE3 Figure 3.15 Hashed Page TableMEMORY / SYSTEMS 14 fully-associati manner. Fortunately, types conflicts relatively rare. PowerPC v irtual memory architecture, uses hashed page table, mitigated providing secondary hashing scheme differs sub stantially primary hash. Whenever primary page able entry group fails provide valid translation, secondary hash used find second group also searched. probability fai ling find valid translation either two groups minimized, though still completely avoided. One drawback inverted page table contains mappings resident physical pages. Hence, pages evicted physical memory backing store need mappings stored elsewhere. handled operating system, whic h maintains separate oftware page table tracking pages reside backing store. course, software page table main- tains mapping virtual addresses corresponding disk blocks, rather physical memory addresses. final alternative, page tables need architected reside physical memory partic ular organization. Inst ead, structure called translation lookaside buffer (TLB, described Section 3.6 illustrated Figures 3.21 3.22) defined part supervisor state processor. TLB contains small number (ty pically 64) entries look like entry illus- trated Figure 3.13, arranged fully-a ssociative fashion. processor must provide fast associativ e lookup hardware searching structure trans- late references every instruction fetch, load, store. Misses architected TLB result page faults, invoke ope rating system. ope rating sys- tem uses page table mapping structure find valid translation create new one updates TLB using supervisor-mode instructions directly replace update entries TLB. scheme, operat- ing system structure page table whatever way deems best, since page table searched page fault handler software, modi- fied adapt variety page table structures. approach handling trans- lation misses called software TLB miss handler specified MIPS, Alpha, SPARC instruction set architectures. contrast, processor implements architecture specifies page table architecture provides hardware state machine accessing memory search page tabl e provide translations memory ferences. uch archi- tecture, page ta ble structure fixed, since ope rating system page fault handler access it, hardware state machine must also able search it. system provides hardware TLB miss handler. PowerPC Intel IA-32 instruction set architectures specify hardware TLB miss handlers. 3.6 Memory Hierarchy Implementation conclude disc ussion memory hierarchies, address several interesting issues arise realized hardware int erfaced high- performance process or. Four topics covered section: memory accessing mechanisms, cache memory implementations, TLB implementa tions, interac- tion cache memory TLB.146 MODERN PROCESSOR DESIGN discussed Section 3.4.3.4, three fu ndamental ways access multienrry memory: indexing via address, associative search via tag, com- bination two. indexed memory uses address index memory select particular entry; see Figure 3.4(a). decoder used decode «-bit address order enable one 2" entries reading writing. rigid direct mapping address data requires data stored fixed entry memory. Indexed direct-mapped memory rigid mapping less complex implement. contrast, associative memory uses key search memory select particular entry; see Figure 3.4(b). entry memory tag field comparator com- pares content tag field key. matc h occurs, tha entry selected. Using form associative search allows data flexibly stored location memory. flexibility comes cost implementa- tion complexity. compromise indexed memory associative memory set-associative memory uses indexing associative search; see Figure 3.4(c). address used index one sets, multiple entries within set searched key identify one partic ular entry. compromise provides f lexibility placement data without incurring complexity fully associative memory. Main memory normally implemented large indexed memory. However, cache memory implemented using one three memory accessing schemes shown Figure 3.4. cache memory implemented indexed memory, referred direct-mapped cache (illustrated Figure 3.16). Since direct-mapped cache smaller fewer entries main memory, requires fewer address bits smaller decoder decode subset main memory add ress bits. Consequently, many main memory addresses mapped entry direct-mapped cache. ensure selected entry contains correct data, remaining, i.e., decoded. | Tag | Index | | Tag [ Index |BO| (a) fb) Figure 3.16 Direct-Mapped Caches: (a) Single Word Per Block; (b) Multiword Per Block.MEMORY I/O SYSTEMS 14 address bits must used identify selected entry. Hence addition data field, entry additional tag field storing undecoded bits. entry cache selected, tag field accessed compared undecoded bits original address ensure entry contains th e data jeing addressed. Figure 3.16(a) illustrates direct-mapped cache entry, block, containing one word. order take advantage spatial locality, block size cache usually contains multiple words shown Figure 3.16(b). multi- word block, bits original ad dress used select particu- lar word referenced. Hence, original address partitioned three portions: index bits used select block; block offset bits used select word within selected block, tag bits used tag match tag stored tag field selected entry. Cache memory also implemented fully associative set-associative memory, shown Figures 3.17 3.18, respectively. Fully associative caches greatest flexibility terms placement data entries cache. block offset bits, address bits used key associatively searching entries cache. full sociativity facilitates efficient use entries cache, incurs greatest implemen- tation c omplexity. Set-associative caches permit flexible placement data among entries set. index bits select particular set, tag bits select entry within set, block offset bits select word within selected entry. discussed Section 3.5, virtual memory requires mapping virtual address space physical address space. requires translation virtual address physical address. Instead directly accessing main memory th e address generated processor, virtual address generated processor must first translated physical address. physical ddress used access physical main memory, shown Figure 3.10. | Tag |Block offset! Figure 3.17 Fully Associative Cache.148 MODERN PROCESSOR DESIGN | Tag | Index | B | Figure 3.18 Set-Associative CacheAssociative search Translation memory physical address Figure 3.19 Translation Virtual Word Address Physical Word Address Using Translation Memory. discussed Section 3.5.3, address translation done using transla- tion memory tha stores virtual-to-real mappings; structure usually called page table. virtual address used index search transla- tion memory. data retrieved selected entry translation memory used physical address index main memory. H ence, physical addresses correspond virtual addresses stored corresponding entries translation memory. Figure 3.19 illustrates use translation memory translate wor addresses; i.e., maps virtual address word virtual add ress space physical ad dress word physical main memory. two weaknesses naive trans lation scheme shown Figure 3.19. First, translation word addr esses require translation memory theMEMORY I/O SYSTEMS 14 Virtual address Page offset Virtual page number Figure 3.20 Translation Virtual Page Address Physical Page Address Using Translation Memory. number entries main memory. result doubling size physical main memory. Translation usually done coarser granularity. Multiple (usually powers 2) words main memory grouped together page, addresses page need translated. Within page, words selected using lower-order bits virtual address, form page offset. illustrated Figure 3.20. Within virtual memory paging system, translation memory called page table. second weakness translation memory scheme fact two memory accesses required every main memory reference instruction. First page table mu st accessed obtain physical page number, physical main memory accessed using translated physical page num- ber along page offset. actual implementations page table typically stored main memory (usually portion main memory allocated operating system); hence, every reference memory instruction requires two sequential accesses physical main memory. become serious botfleneck pe rformance. Th e solution cache portions page table small, fast memory called translatio n lookaside buffer (TLB). TLB essentially cache memory page table . like cache memory, TLB implemented using one three memory accessing schemes Figure 3.4. direct-mapped TLB simply smaller (and faster) version page table. virtual page number partitioned an150 MODERN PROCESSOR DESIGN index TLB tag; see Figure 3.21. v irtual page number translated physical pa ge number, concatenated page offset form physical address. ensure flexible eff icient use TLB en tries, associativity usually added TLB implementation. Figure 3.22 illustrates set-associative fully associative TLBs. Fo r set-associative TLB, virtual address bits partitioned three fields: index, tag, page offset. size page offset field dictated page size specified architecture operating system. remaining fields, i.e., index tag, constitute virtual Virtual address Virtual page no. Tag Figure 3.21 Direct-Mapped TLB.Page offset Index Physical page no.Page offset (^^^^PlvysicaTaddress^^^^ Virtual page no. | Tag | Index "T~PO T| Tag \"r6'"\ PPN Physical page no.TagVirtual page no. Page offsetPPN Tag Q^^^^^PJrysicaladdress^^^^^^ (a) Figure 3.22 Associative TLBs: (a) Set-Associative TLB; (b) Fully Associative TLB.Physical page no.Page offset ( Physical address (b)MEMORY I/O SYSTEMS 151 page number. fully associative TLB, index field missing, tag field contains virtual page number. Caching portion page table TLB allows fast address translation; however, TLB misses occur. virtual page physical page mappings page table simultaneously present TLB. accessing TLB, cache miss occur, case TLB must filled page table sitting main memory. incur number stall cycles pipeline. also possible TLB miss lead page fault. page fault occurs virtual page physical page mapping even exist page table. means particular page referenced resident main memory must fetched secondary storage. service page fault requires invoking operating system access disk storage require potentially tens thousands machine cycles. Hence, page fault triggered program, program suspended execution page fault serviced operating system. process illus trated Figure 3.12. data cache used cache portion main memory; TLB used cache portion page table. interaction TLB datacache illustrated Figure 3.23. n-bit virtual address shown Figure 3.23 effective address generated first pipe stage. virtual address consists virtual page number (v bits) page offset (g bits). TLB set- associative cache, v bits virtual page number split It-bit index (v-£)-bit tag. second pipe stage load/store unit corresponds accessing TLB using virtual page number. Assuming TLB miss, TLB output physical page number (p bits), concatenated g-bit page offset produce m-bit physical address m=p + g necessarily equal n. third pipe stage m-bit physical address used access data cache. exact interpretation Virtual address (n = v + g bits)Virtual page no. (VPN) Tag Index cPage fset (PO) TLB Physical address (m—p+g bits)Physical page no. (PPN) PO Tag Index BO  b D-cacheBO: block offset Data Figure 3.23 Interaction B etween TLB Data Cache152 MODERN PROCESSOR DESIGN m-bit physical address depends design data cache. data cache block contains multiple words, lower-order b bits used block off- set select referenced word selected block. selected block determined remaining (m - b) bits. data cache set-associative cache, remaining (m - b) bits are-split /-bit tag i-bit index. value determined total size cache set associativity; i.e., sets set-associative data cache. cache miss, at- end third pipe stage (assuming data cache accessed single cycle) data available data cache (assuming load ins truction ex ecuted). organization shown Figure 3.23 disadvantage TLB must accessed data cache accessed. Serialization TLB data cache ac cesses introduces ov erall latency sum two latencies. Hence, one might assume address translation memory access are' done two separate pipe stages. solution problem use virtually indexed data cache tha allows accessing TLB data cache performed parallel. Figure 3.24 illustrates scheme. straightforward way implement virtually indexed data cache use page offset bits access data cache. Since page offset bits require translation, used without translation. g bits page offset used block offset {b bits) index (i bits) fields accessing data cache. simplicity, l et's assume tha data cache direct-mapped cache 2' entries entry, block, containing 2b words. Instead stor- ing remaining bits virtual address, i.e., virtual page number, tag field, data cache store translated physical page number tag field. done time data cache line filled. time page offset bits used access data cache, remaining bits ' virtual address, i.e., virtual page number, used access TLB. ssum- ing TLB data cache access laten cies comparable, time Virtual page no. (VPN) Tag 1 Index  v - * TLB P PPNPage offset (PO)Virtual page no. (VPN) Figure 3.24 Virtually Indexed Data Cache.Tag Index Page offset Index BO :«  D-cache PPN Data <=) Hit/missMEMORY I/O SYSTEMS 153 physical page number TLB becomes available, tag field (also containing physical page number) data cache also available. two p-bit physical page numbers compared determine whether hit (matching physical page numbers) data cache not. virtually indexed data cache, address translation data cache access overlapped reduce overall latency. classic paper Wang, Baer, Levy discusses many tradeoffs involved designing multilevel virtually addressed cache hierarchy [Wang et al., 1989]. 3.7 Input/Output Systems Obviously, processor isolation largely usele ss end user serves practical purpose. course, virtually everyone interacted computers various types, either directly, keyboard display device, indirectly, phone system int erface embedded computing sys- tem. purpose interaction either log information computer system possibly request perform certain computations (input) either observe result allow computer directly interact external devices (output). Thus, computer system whole thought black box device set inputs, provided various interfaces, set utputs, also provided set interfaces. interfaces interact directly human (by capturing keystrokes keyboard, movements mouse, even spoken commands, disp laying text graphics play- ing audio comprehensible humans) instead interact digital devices various sp eeds. section discusses devices attributes. Table 3.4 summarizes tributes common nput/output devices. device type, table specifies device co nnected system; whether used input, output, input output, storage; whether communicates human machine; approximate data rates devices. table makes clear I/O devices quite diverse characteristics, data rates varying seven orders magnitude. Table 3.4 Types inpu t/output devices How/Where input/Output/ Data Rate Device Connected Storage Partner Mouse, keyboard Serial port Input Human 0.01 Graphical display I/O bus Output Human 100,000 memory bus Modem Serial port Input output Machine 2-8 LAN I/O bus Input output Machine sbo-i 20,000 Disk Storage bus Storage Machine 10,000+154 MODERN PROCESSOR DESIGN If3.7.1 Types I/O Devices section briefly discusses I/O devices enumerated Table 3.4 (mouse, keyboard, graphical displays, modems, LANs, disk drives), also provides overview high-performance fault-tolerant disk arrays. Mouse Keyboard. mouse keyboard used provide direct user input system. keyboard mouse devices usually connected system via low-speed serial port. universal serial bus (USB) example standardized serial port available many system today. data rates keyboards mice low, limited speed humans type keyboard operate mouse. Since data rates low, keyboard mouse input typically communicated CPU via external interrupts. Every ke press movement mouse ultimately invokes oper- ating system's interrupt handler, samples current state mouse keyboard determ ine key pressed directio n mouse moved respond appropriately. Though may appear create exces- sive rate interrupts might seriously perturb processor, th e low data rates devices generally avoid problem single-user system. However, large-scale time-shared system services keyboard input hundreds thousands users, th e interrupt rate quickly become piohibiti ve. envi- ronments, unusual provide terminal I/O controllers handle key- board interrupts users communicate main processor cluster keyboard activity aggregated controller. mode m-day equivalent type aggregation interactive I/O activity occurs users enter data form Web browser: data entry captured user's Web browser client, Web server doe get involved user clicks submit button transmits th e Web form data single transac- tion server. fashion, load server well communication links client server minimized, ince aggregated information communicated, rather every keystroke. Graphical Display. graphical display c onveys video image data, illustra- tions, formatted documents user, also presents user interface simplifies user's interaction system. Graphical displays must render million pixels screen using 24-bit color representation per pixel usually update screen rate 60 frames per second co ntents screen rendered frame buffer contains pixel-by-pixel representa- tion contents screen. random access memory digital-to-analog con- verter (JRAMDAC) uses high-speed interface fram e buffer's memory converts digitally represented image analog image displayed CRT (cathode-ray tube) LCD (liquid-crystal display) monitor. frame buffer contents updated graphics processor typically supports various schemes accelerating tw o-dimensional three-dimensional graphics transform ations. example, dedicated hardware graphics processor pipeline perform visi- bility checks see certain objects hidden behind oth ers, correct per- spective three-dimensional environment, ca n perform li ghting, shading, andMEMORY I/O YSTEMS 1 texture transforms add alism generated image. transforms require extremely high bandwidth frame buffer memory, well main memory access image database, objects represented collections polygons three-dimensional space. Hence, g raphical display adapters connected main I/O bus system interact main CPU, also often utilize special-purpose memory port [the accelerated graphics port (AGP) example port] enable high memory bandwidth performing transforms. Modem. Modems used interconnect digital systems analog com- munication line, usually standard telephone connection. nature standard phone lines, able provide limite bandwidth, maxi- mum 56 kbits/ latest standard. Hence, low overall data rates, modems usually connected system via low-speed serial port, like USB even older RS-232 serial port. LAN. Local area network adapter used connect computer systems other. LAN adapter must provide physical layer interfac e converts computer's internal signal l evel digital data signaling tec hnology employed LAN interface. Fast Ethernet, mnning 100 Mbits/s, dominates industry today, Gbit Ethernet rapidl adopted. LAN adapters, due reasonably high data rates, usually connected directly I/O backplane bus system provide hig h bandwidth access system's main memory processor. Originally, Et hernet conceived shared bus-based inter- connect scheme, time e volved switched, point-to-point orga- nization computer system dedicated link centralized switch responsible routing data packets ports based destination addresses packets. Ethernet switches connected hier- archically allow large r number systems communicate. Disk Drives. Magnetic disk drives store information platter changing orientation magnetic field individually addressable location platter. shown Figure 3.25, disk drive may contain multiple platters per spindle. Figure 3.25 Disk Drive Structure.156 MODERN PROCESSOR DESIGN fTT^n EXAMPLEUsually, data stored side platter, separate read/write head side platter. platter multiple concentric tracks divided sectors. Read/write heads rest cushion air top spin- ning platter seek desired track via mechanical actuator. desired sector found waiting disk rotate desired position. Typical disks today rotate 3000 15,000 revolutions per minute (rpm), contain any- 500 2500 tracks 32 sectors per track, platters diameters ranging size 1 3.5 in. Recent drives moved placing variable number sectors per track, outer tracks gre ater circumference sectors, inner tracks lesser circumference contain fewer tracks. approach maintains constant areal bit density pla tter substrate, complicates read/write head control logic, since linear velocity disk head varies track (with higher v elocity outer tracks). Hence, rate bits pass underneath head also varies track, higher bit rate outer tracks. contrast, older disks constant number sectors variable bit ensity, bit rate head observed remained c onstant, independent track accessed. older disk drives, notably floppy drives original Apple Macintosh computers, held bit rate linear velocity constant varying rotational speed disk based position head, leading audi- ble variation sound drive generates. approach substa ntially compli- cates motor control electronics, making infeasible high-speed hard drives spinning thousands rpm, abandoned recent disk designs. Latency = rotational + seek + transfer + queueing (3.7) E X PmAs shown Equation (3.7), access latency disk drive consists sum four terms: rotational latency, seek latency, transfer latency, E[^ queueing delays. Rotational latency determined speed disk rotates. example, 5400-rpm drive completes single revolution (60 sV (5400 rpm) =11.1 ms. average, random access wait half revolu- tion, leading average rotational latency 11.1 ms/2 = 5.5 ms 5400-rpm drive. seek latency determined number tracks, size plat- ter, design seek actuator, varies depending distance head's current position target track. Typical average seek latencies range 5 15 ms. transfer latency determined read/write head's data transfer rate divided block size. Data transfer rates vary 1 4 Mbytes/s more, typical blocks 512 bytes; assuming 4-Mbyte transfer rate 512-b block, drive would incur 0.12 ms transfer late ncy. Finally, queueing delays controller due multiple outstanding r equests consume 1 ms latency. final average latency example drive would add 5.5 ms (rotational latency) + 5 ms (seek latency) + 0.1 ms (transfer latency) + 1 ms (queueing latency) = 11.6 ms. Modern disk drives also provide cache buffers ranging size 2 8 Mbytes used capture temporal spatial locality disk refer- ence stream. hese operate similarly processor caches often able toMEMORY I/O SY5TEMS 15 satisfy substantial fraction disk requests low latency, hence reducing average disk latency considerable amount. course, worst-case access patterns exhibit little spatial temporal locality still incur access latencies determined physical design disk, since cannot satis- fied disk buffer. Subsequent references nearby tracks sectors satisfied much quickly average case, since rota tional seek latencies minimized cases. Hence, modern operating systems attempt reorder references order create schedule maximizes type spatial locality, hence minimizing average reference latency. long operating system aware physical disk layout blocks referencing, scheduling possible desirable. Disk drive performance modeling issues discussed length classic paper Ruemmler Wilkes [1994]. Disk Arrays. High-performance computer syste ms typically contain one disk provide increased capacity well highe r bandwidth file system demand-paged backing store. Quite often, disks arranged arrays c onfigured provide high performance well degree fault tolerance. arrays, data striped across multiple disks varying levels granularity enable either higher data band- width higher transaction bandwidth accessing multiple isks parallel. Figure 3.26 illustrates several approaches striping interleaving data across AO B 0 CO Al Bl CI Dl A2 B2 C2 D2 IndependentAO AO A.0 AO Al Al Al Al A2 A2 A2 A2 B0 B0 B0 B0 Bl Bl Bl Bl B2 B2 B2 B2 CO CO CO CO CI Cl Cl Cl C2 C2 C2 C2 Fine-grainedAO l A2 A3 B0 Bl B2 B3 CO C l C2 C3 Coarse-grained disk represented column, block represented name (AO, Al, A2, etc.), blocks file named letter (e.g., AO, Al, A2 file). ndependent disk arrays place r elated blocks drive. Fine-grained interleaving subdivides block stripes across multiple drives. Coarse-grained interleaving stripes related blocks across multiple drives. Figure 3.26 Striping Data Disk Arrays.158 MODERN PROCESSOR DESIGN multiple disks varying levels granularity. Without interleaving, shown left figure Independent, blocks single file (e.g. AO, Al, A2) placed consecutively single disk (represented column Figure 3.26. fine-grained interleaving, shown middle figure, blocks sub- divided striped across multiple di sks. Finally, coarse-grained interleaving, shown right figure, block placed single disk, related blocks stripe across multiple disks. redundant arrays inexpensive disks (RAID) nomenclature, introduced, Patterson, Gibson, Katz [1988] summarized Ta ble 3.5, provides useful framework describing different approaches combining disks inta arrays. RAID level 0 provides degree fault tolerance, provide high performance striping data across multiple disks. striping, greate-j aggregate read wri te bandwidth available objects stored disks. Table 3.5 Redundant rrays inex pensive disks (RAID) le vels RAID Level Explanation> -   **- .-**«  - ^' Fault Overhead leranceUsage Comments 0 Data striped across disks None None Widely used; fragile 1 Data mirrored disk duplicated 1 2 Widely used: high overhead 2 Hamming code ECC protection; data + ECC bits striped across many disksVery high forfew disks; reasonable large disk arraySingle disk failureNot practical; requires many disks amortize cost ECC bits 3 Data striped; single parity disk per wordParity disk per striped blockSingle disk failureAvailable; high data 1 bandwidth, poor transaction bandwidth " 4 Data striped (interleaved block granularity); single parity diskParity disk per block setSingle disk failureAvailable; poor write performance due parity disk bottleneck 5 Data striped (interleaved block granularity); parity blocks interleaved disks1 ofn disk blocks used parity (e.g., 5 disks provide data capacity 4)Single disk failureWidespread; writes require updates two disks—one data, one parity 6 Data striped (interleaved block granularity); two- dimensional parity blocks interleaved disks2 n disk blocks used parity (e.g., 6 disks provide data capacity 4)Multiple disk failureAvailable; writes updates three disks—one data, one row parity, one column parity Source: Patterson et al, 1988.MEMORY / SYSTEMS 159 Furthermore, since disk operate independent ly, transaction bandwidth also increased dramatically. However, single disk failure cause entire array fail. RAID level 1, also known disk mirroring, addresses provid- ing fault tolerance mirroring data. approach simple imple- ment, hig h overhead provides improvement w rite bandwidth (since copies must updated), doubling read read transac- tion bandwidth. Higher levels RAID protection use parity error-correction codes (ECCs)1 reduce overhead fault tolerance much less th 100% overhead required RAID level 1. RAID level 2, word-level ECCs based Hamming codes used identify correct single errors. Conceptually, ECC con tains parity bit (used check bit error coded word), well off- set points data bit error. parity bit offset encoded using Hamming co de minimize storage overhead used together correct bit error flipping bit specified offset whenever parity bit indicates error. Unfortunately, inherent overhead word-l evel ECCs high enough RAID level 2 impractical largest disk arrays, w large words spread across dozens drives reduce ECC overhead. example, ECC SECDED2 overhead fo r 64-bit word size minimum 7 bits, requir- ing disk array 71 drives (64 data drives 7 ECC drives) achieve rea- sonable 11% overhead. Since ECC SECDED overhead much higher smaller word sizes, RAID level 2 rarely employed arrays drives. RAID level 3 replaces ECCs j ust parity, since fa iling drives typically detected disk array controller without explicit error-correction-coded offset identifies failing bit (modem disk include diagnostic firmware able report disk failure even predict imminent failure th e disk control- ler). Using parity reduces overhead simplifies RAID implementation. However, since data striped fine grain RAID level 3 (at bit level), transaction requires coordinated participation th e drives parity set; hence, transaction bandwidth scale well. Instead, RAID level 4 maintains parity coarser block level, ducing transaction overhead sup- plying much better transaction bandwidth scaling. illustrated Figure 3.27, RAID level 4 places parity blocks drive. leads parity drive bottleneck, since writes must access single drive update block set's parity. RAID level 5 solves th e parity block botdeneck rotating parity blocks across drives, shown righ Figure 3.27. RAID level 5 widely used provide high performance fault toler- ance protect single disk failure. RAID level 5, data blocks indepen- dently stored across disks array, parity blocks covering group "For background information error-correcting codes, covered detail book, interested reader referred Blahut [1983] Rao Fujiwara [19S9J. "Single-error correct dual-error detect fSECDED) codes powerful enough corr ect single bit error detect occurrence of—but correct—a dual-bit erroi within protected word.AMP1E160 MODERN PROCESSOR DESIGN 12 1613 1710 14 18 RAID level 4 15 1914 17 7 11 15 16 18 19 RAID level 5 Figure 3.27 Placement Parity Blocks RAID Level 4 (Left) vs. RAID Level 5 (Right). ±1 EXAMPIEdata blocks interleaved drives well. terms capacity overhead, means 1 n disks consumed storing parity, exchange tolerance single drive failures. Read performance still good, since logical read requires single physical read actual data block. Write performance suffers slightly comp ared RAID level 0, since parity block must updated addition writing die new data block. However, RAID level 5 provides powerful combination fault tolerance, reasonable overhead, high performance, widely deployed real systems. Finally, RAID level 6 extends l evel 5 maintaining two dimensions parity block, requiring double storage write overhead l evel 5 providing tolerance multiple disk failures. RAID level 6 typically employed environments data tegrity extremely important. interested reader referred th e original work Patterson et al. [1988] in-depth treat- ment advantages disadvantages various RAID l evels. RAID controllers implemented either completely hardware, limited operating system involvement, operating system's device driver (also known software RAID). exam ple, open-source Linux kernel supports software RAID levels 0, 1, 5 arrays inexpensive, commodity integrated drive electronics (IDE) drives, even across drives separate LAN- connected achines configured network block devices (nbd). makes possible implement fault-tole rant RAID arrays using inexpensive, commodity PC hardware. Hardware RAID controllers, ty pically used higher-end server systems, implement RAID functionality controller's firmware. High-end RAID controllers often also su pport hot-swappable drives, failed failing drive replaced fly, RAID array remains on-line. Alternatively, RAID array configured contain ho spare drives, controller automatically switch hot spare drive drive fail already failed (this called automated failover). period time failed disk st ill part array, accesses blocks stored failed diskMEMORY I/O SYSTEMS 161 satisfied reading parity block ot blocks th e parity set reconstructing con tents missing block using parit function. example, array employing even parity across four disks, failing disk third disk, controller might read parity bit <1> <0,1,?,1> remai ning good drives. Sinc e even parity implies even number " 1 " bits across parity set, missing "?" bit inferred " 1 . " Since access failed drive r equires coordinated reads remaining drives parity set, on-line forward e rror correction pr ocess result poor disk performance failed disk rep laced. similar fashion, RAID array hot spares ca n automatically recon- struct contents failed drive write spare disk, alerting system operator replace th e failed drive. RAID array support hot spares, thi reconstruction pr ocess conducted either line, array powered failed disk replaced, line, soon operator hot-swapped failed drive functioning one. 3.7.2 Computer System Busses typical computer system provides various busses fo r interconnecting co mpo- nents discussed preceding sections. ideal world, single com- ' munication technology would sa tisfy needs requirements system components I/O devices. H owever, numerous practical reasons—including cost, backward compatibility, suitability application—numerous interconnection schemes employed single system. Figure 3.28 shows three types busses: processor bus, I/O bus, storage bus. Processor busses used connect CPU main memory well I/O bridge. Since CPU performance depends heav ily high-bandwidth, Memory Figure 3.28 Different Types Computer System Busses.162 MODERN PROCESSOR DESIGN low-latency connecti main memory, processor busses employ leading-edge sig- naling technology running high frequencies. Processor busses also fre- quently dated improved, usually every proce ssor generation. Hence,^ devices connect processor bus (typically CPU, memor controller, I/O bridge; often referred chip set) need updated regular intervals. de facto update requirement, little or- pressure processor busses maintain backward compatibility beyond more, one processor generation. Hence, signaling technology evolve quickly, also protocols used communicate across bus adapt quickly take advantage new opportunities improving performance. Sec- tion 11.3.7 provides additional disc ussion design processor busses1 coherent memory int erface modern CPU needs provide com- municate proc essor bus. Processor busses also designed electrical characteristics match short physical distances, since components attached bus usually close proximity inside physical com* puter package. enables high speed signaling technologies would impossible expensi longer physical distances. contrast processor bus, typical I/O bus evolves much slowly, since backward compat ibility legacy I/O adapters primary design constraint. fact, systems wil l frequently support multiple generations, I/O busse enable use legacy adapters well modern ones. example,, many PC systems support peripheral component interface (PCI) I/O bus industry standard architecture (ISA) bus, ISA bus standard stretches back 15 year past. Also, cost physical design reasons, I/O busses usually employ less aggre ssive signaling technology, run much lower clock frequencies, employ less comple x communication protocols processor busses. example, modern PC system might hav e 533-MHz pro- cessor bus 8-byte dat apath, PCI I/O bus would run 33 MHz 4-byt e datapath. Since peripheral I/O devices cannot support higher data rates anyway, cost reasons I/O busses less aggre ssive design. onl standard peripheral requires much higher bandwidth modern graphics proce ssing unit (or display adapter); modem PCs provide dedicated accelerated graphics port (AGP) supply b andwidth main memory, control communication display adapter still occurs PCI I/O bus. I/O busses typi cally need span physical dis tances limited computer system enclosure; distances substantially longer processor bu needs span, still limited less 12 in. (30 cm) cases. Finally, storage busses, used primarily connect magnetic disk drives system, suffer even legacy issues backward compatibility. result, often h obbled ability dopt new signaling technology clean, straightforward fashion imply less-than-elegan solutions. example, storage busses limited use newer technology signaling oldest peer sharing partic ular bus. presence one oldMEMORY / SYSTEMS 16 device hence limit newer devices lowest common denominator performance. Storage busses must also able span much greater physica l distances, since storage device connecting may reside external case adjacent rack. Hence, signaling technology communication protocol must tolerate long transmission latencies. case Fiber Channel, optical fiber links used span several hundred meters, enabling storage devices reside separate buildings. Simple busses support single concurrent transaction. Following arbi- tration cycle, device wins arbitration allowed place command bus. requester proceeds hol occupy bus c ommand completes, usually involves waiting response entity connected bus. course, providing response entails long-latency event like per forming read disk drive, bus occupied long time transaction. bus relatively easy design, suffers poor utilization due thes e long wait times, bus effectively idle. fact, virtually modem bus designs upport split trans- actions, enable multiple concurrent requests single bus . split transaction bus, requester first arbitrates bus, occupies bus long enough ssue request, surrenders bu next user with- waiting response. period time later, responder arbi- trates bus transmits response separate bus e vent. fashion, transactions bus split two—and times two—separate events. interleaving multiple concurrent requests leads much better efficiency, since bus utilized transfer independent requests responses long-latency request pending. Naturally, design complexity bus much higher, since devices connected bus must able track outstanding requests identify bus transac- tions correspond requests. However, far higher e ffective bandwidth results justifies additional complexity. Figure 3.29 summarizes key design parameters describe computer system busses. First all, bus topology must set either point-to-point, enables much higher frequency, multidrop, limits frequency due added capacitance electrical connection shared bus, pro- vides flexible connectivity. Second, particular signaling technology must chosen determine voltage levels, frequency, receiver/transmitter design, use differential signals, etc. Then, several pa rameters related actual data transfer must set: width data bus; whether data bus lines shared multiplexed control lines; either single-word data transfer gr anularity, support multiword transfers, possibly including sup- port burst-mode op eration saturate data bus back-to-back transfers. lso, bidirectional bus supports multiple signal drivers per data wire must provide mechanism turning bus around switch one driver another; usually leads dead cycles bus reduces sus- tainable bandwidth (a uni directional bus avoids problem). Next, clocking164 MODERN PROCESSOR DESIGN Bus design  Topology Point point Multidrop (broadcast)  Signaling echnology (voltage levels, frequency, etc.) - Data transfer - Data bus width - Data bus wires  Shared/multiplexed address lines - Dedicated data lines - Transfer granularity  Directionality- Single word  Multiple words (burst mode)  Clocking strategy- Unidirectional (single driver per data wire)  Bidirectional (multiple drivers, bus turnarounds required) - Asynchronous (handshaking required)  Synchronous (single shared clock) - Source synchronous (clock travels address and/or data) ' Bus arbitration - Single bus master (no arbitration necessary) - Multiple bus masters - Arbitration mechanism Daisy chain Centralized Distributed - Switching strategy Blocking (circuit-switched pended) Nonblocking (packet-switched split transaction) Figure 3.29 Bus Design Parameters. strategy must also set: simplest option avoid bus clocks, instead employing handshaking sequences using request valid lines signal presence valid com mands data bus. alternative, single shared clock used synchronous bus avoid handshaking improve bus utilization. Finally, aggressive source-synchronous clocking approach used, clock travels data commands, ena bling th e highest operating frequency wave pipelin ing multiple packets flight time. Finally, bus designs allow multiple bus masters control theMEMORY I/O SYSTEMS 16i bus, arbitration switching policy must specified. Possible arbitration mechanisms include daisy-chaine arbiters, centralized arbiters, distri buted arbiters; switching poli cies either circuit-switched (also known blocking), single transaction holds bus u ntil completes, packet- switched (also known nonblocking, pipelined, split transaction buses), bus transactions split two packets packet occu- pies separate slot bus, allowing interleaving packets multiple distinct requests. Modern high-p erformance bus designs trending toward following characteristics maximize signaling frequency, bandwidth, util ization: point- to-point connections relatively data lines minimize crosstalk, source- synchronous clocking support burst mode transfers, dist ributed arbitration schemes, support split transactions. One interesting alternative bus design emerged recently simultaneous bidirectional bus: scheme, bus wires multiple pipelined source-synchronous transfers flight time, additional twist signaling simult aneously directions across set wires. advanced bus designs conceptually treat digital signal analog waveform traveling well-behaved waveguid e (i.e., copper wire), require careful driver rece iver design borrows con- cepts techniques signal processing advanced communications transceiver design communities. 3.7.3 Communication I/O Devices Clearly, processor needs co mmunicate I/O devices system using mechanism. practice, two types communication need occur control flow, communicates commands responses I/O device; data flow, actually transfers data I/O device. Contr ol flow broken commands flow processor I/O device (outbound control flow), responses signaling completion commands status information back processor (inbound control flow). Figure 3.30 summarizes main attributes I/O device communication discussed section. Outbound Control Flow. two basic approaches communicating commands (outbound control flow) p rocessor I/O device. first programmed I/O: certain instruction set architectures provide specific instructions communicating I/O devices; example, Intel IA-32 instruction set provides primitives. hese programmed I/O instructions directly connected contro l registers I/O devices, I/O devices react accordingly changes written control registers. main sh ortcom- ing approach ISA provides finite set I/O port interfaces processor, presence multiple I/O devices, need shared virtualized manner complicates op erating system device driver software. Furthermore, special-purpose instructions map cleanly to166 MODERN PROCESSOR DESIGN I/O device communication Control flow granularity - Fine-grained (shallow adapters) - Coarse-grained (deep adapters, e.g., channels)  Mechanics control flow - Outbound control flow Programmed I/O Memory-mapped control registers  Inbound control flow  Polling  Imerrupt-driven  Mechanics data flow311  Programmed I/O  Direct memory access (DMA) - Software cache coherence - Hardware cache coherence <3- Figure 3.30 Communication I/O Devices. pipelined out-of-order designs escribed book, require complex specialized handling within processor core. general approach outbound control flow use memory- mapped I/O. approach, device-sp ecific control registers mapped memory address space system. Hence, accessed conventional load store instructions, special support ISA, However, care must taken cache-based systems ensure effects loads stores memory-mapped I/O registers actually' visible I/O device, rather masked cache references hit cache hierarchy. Hence, virtual memory pages corresponding memory-mapped control registers us ually marked caching nhibited uncacheable virtual memory page table (refer Section 3.5.3 for. information page table design). R eferences uncacheable pages must routed processor chip I/O bridge interface, satisfies- control registers I/ device mapped address question. Inbound Control Flow. inbound control flow, i.e., responses status infor- mation returned I/O devices back processor, two funda- mental approaches: polling interrupts. polling system, th e operating system- intermittently check either programmed I/O memory-mapped control! register determine status I/O device. straightforward implement, hardware software, polling systems suffer inefficiency, since theMEMORY / processor spend significant amount time polling completion long- latency event. Furthermore, since polling activity requires communication across processor I/O busses system, busses become over- whelmed begin suffer excessive queueing delays. Hence, much cleaner scalable approach involves utilizing processor's support external interrupts. Here, processor responsible polling I/O device completion. Rather, th e I/O device instead responsible asserting external interrupt line processor completes activity , hen initiates operating system's interrupt handler conveys processor I/O complete. interrupt signal routed I/O device, I/O bridge, processor's external interrupt controller. Control Flo w Granularity. Command response control flow also vary granularity. typical PC-bas ed systems, I/O devices expose simple interface control registers. perform fairly simple activities support straightforward requ ests like reading writing simple block memory peripheral device. devices fine-grained control flow, since processor (or operating system device driver rurining processor) control devices fine-grained manner, issuing many simple commands complete complex transaction th e peripheral I/O device. contrast, mainframe minicomputer world, I/O evices often expose much powerful complex interface processor, allowing processor control devices coarse-grained fashion. example, I/O channel controllers IBM S/390-compatibl e mainframe systems actually execute separate programs contain internal control flow structures like loops conditional branches. Thi richer functionality used off-load fine-grained control main CPU, freeing focus tasks. Modem three-dimensional graphics adapters today's desktop PC systems another example I/O devices coarse-grained control flow. command set avail- able operating system device drivers adapters semantically rich powerful, graphics-related pr ocessing effectively off- loaded main processor graphics adapter. Data Flow. Data flow proc essor I/O devices occur two fundamental ways. first relies instruction set support pro- grammed I/O. Again, ISA must specify primitives commu nicating I/O devices, primitives used initiate requests poll com- pletion, also data transfer. Hence, processor actually needs individu- ally read write word transferred I/O device internal processo r register, ove operating system's in- memory b uffer. course, extremely inefficient, occupy pro- cessor thousand cycles whenever large blocks data transferred. data transfers also unnecessarily pollute cause co ntention processor's cache hierarchy.168 MODERN PROCESSOR DESIGN Instead, modern systems enable direct memory access (DMA) peripheral I/O devices. effect, devices ssue reads writes directly main memory controller, proc essor can. fashion, I/O device canr update op erating syst em's in-memory receive buffer irectly, interven- tion processor, signal processor interrupt transfer complete. Conversely, ransmit buffers read directly main memory, transmission completion terrupt sent processor transmission completes. course, memory-mapped control registers, DMA cause1 problems cache-based systems. operating system must guarantee cache blocks I/O device wants read currently proces- sor's caches , otherwise I/O device may read stale copy die cache block main memory. Similarly, I/O device writing memory location, processor must ensure satisfy next read location cached copy become stale, since I/O device updated copy memory. effect, caches must kept coherent latest updates th eir corresponding memory blocks. oe done manually, operatin g system software, using primitive ISA enable flushing blocks cache. approach called software cache coherence. Alternatively, system provide hardware maintains coherence; scheme called hardware cache coherence. maintain hardware cache coherence, I/O devices' DMA accesses must made visible processor's cache hier- archy. words, DM writes must either directly update matching copies processor's cache, matching copies must marked invalid prevent processor fro reading future. Similarly, DMA reads must satis- fied processor's caches whenever matching up-to-date copy al block found there, rather satisfied stale copy main mem- ory. Hardware cache coherence often achieved requiring processor's caches snoop read write transactions occur across processor-memory bus respond appropriately snooped commands match cached l ines either invalidating (when bus write snooped) supplying up-to- date data (when bus read dirty line snooped). Chapter 11 provides details hardware mechanisms enforcing cache coherence. 3.7.4 Interaction /O Devices Memory Hierarchy discussed Section 3.7.3, direct memory access (DMA) I/O de vices causes cache coherence problem cache-base systems. Cache coherence general pervasive problem systems contain multiple processors, since proc essor als update copies blocks memory locally cache, whereas effects updates made visible proces- sors system well (i.e., caches remain coherent). revisit problem length Section 11.3 describe cache coherence protocols used elegantl efficiently solve problem systems even hundreds processors.MEMORY / SYSTEMS 169 Single user: [ CPU1 Disk access , CPU1 Think time Time-shared:CPU1 CPU1 Disk access Think time CPU2 CPU2Increase number active threads reduces effectiveness spatial locality increasing working set. Disk access Think time CPU3 CPU3 Disk access Think time Time dilation thread reduces effectiveness temporal locality. example, three users time-share CPU, overlapping CPU usage disk latency think time interactive users. inc reases overall throughput, since CPU always busy, increase latenc observed user. Latency increases due context switch overhead queuing delay ( waiting CPU another user occupying it). Temporal spatial locality adversely affec ted time-sharing. Figure 3.31 Time-Sharing CPU.rTnTi n EXAMPLE However, another interesting interaction occurs memory hierarchy due long-latency I/O events. discussion demand-paged virtual memory ubsystems Section 3.5.1, noted operating system put faulting process sleep fetches mi ssing page backing store sche dule alternative process to" run processor. Th process called time-sharing CPU illustrated Figure 3.31. top half figure shows single process first consuming CPU, performing long- latency disk access, consuming CPU time again, finally shows think time waiting user r espond program output. bottom half figure, processes similar behavior interleaved onto processor first process waiting disk "access" user response. Clearly, much better CPU utilization results, since CPU longer idle long p eriods time. However, increased utilization comes price: since process's exe- cution dilated time due intervening execution processes, temporal locality process suffers, resulting high cache miss rates. Furthermore, fact processor's memory hierarchy must contain working sets active processes, rather single active process, places great strain th e caches reduces beneficial effects spatial locality. result, substantial increases cache miss rates substantially worse averag e memory refer ence latency heavily time-share systems.170 MODERN.PROCESSOR DESIGN processors continu e increase speed, latency I/O devices improves glacial rate, ratio active processes need scheduled cover latency single process's I/O event increasing rapidly. result, effects pointed Figure 3.31 pronounced, effective- ness cache-based memory hierarchies deteriorating. r evisit problem Chapter 11 discuss systems execute multiple threads simultaneously. 3.8 Summary chapter introduces basic concept memory hierarchy, discusses vari- ous technologies used build memory hierarchy, covers many e ffects memory hi erarchy processor performance. addition, studied key input output devices exist systems, tech nology used implement them, means connected interact processor rest computer system. also discussed following memory idealisms showed well- designed memory hierarchy provide illusion satisfied, least extent:  Infinite capacity. storing large data sets large programs.  Infinite bandwidth. rapidly streaming large data sets programs processor.  Instantaneous zero latency. prevent proc essor talling waiting data program code.  Persistence nonvolatility. allow data programs survive ever* power supply cut off.  Zero low implementation cost. learned highest levels memory hierarchy—register files primary caches—are able supply near-infinite bandwidth low average latency processor core, atisfying second third idealisms. first idealism—infinite capacity—is satisfied lowest level memory hierarchy, since capacities DRAM-based memories large enough contain w orking sets ost modern applications; pplica- tions w case, learned te chnique called virtual mem- ory extends memory hierarchy beyond random-access memory devices- magnetic disks, provide capacities exceed demands demanding applications. fourth idealism—persistence nonvolatility— also supplied magnetic disks, designed retain state even powered down: final ideali sm—low implementation cost—is also satisfied, since high per-bit cost upper levels cache hierarchy multiplied relatively small number bits, lower levels hi erarchy provide tremendous capacity low cost per bit Hence, average cost per bit kept near low cost c ommodity DRAMMEMORY I/O SYSTEMS 17 magnetic disks, rather high cost custom SRAM cache memories register files. REFERENCES Blahut, R. E.: Theory Practice Error Control Codes. Reading, MA: Addison-Wesley Publishing Company, 1983. Cuppu, V., B. L. Jacob: "Concurrency, latency, system overhead: largest impact uniprocesor DRAM-system performance?," Proc. 28th Int. Symposium Computer Architecture, 2001, pp. 62-71. Cuppu, V., B. L. J acob, B. Davis, T. N. Mudge: "A performance comparison con- temporary DRAM architectures," Proc. 26th Int. Symposium Computer Architecture, 1999, pp. 2 22-233. Goodman, J.: "Using cache memory reduce processor-memory traffic," Proc. 10th Int. Symposium Computer Architecture, 1983, pp. 124-131. Hill, M., A. Smith: "Evaluating associativity CPU caches," IEEE Trans, Computers, 38,12,1989, pp. 1612-1630. Hill, M. D.: Aspects Cache Memory Instruction Buffer Performance. PhD thesis, University California Berkeley, Computer Science Division, 1987. Intel Corp.: Pentium Processor User's Manual, Vol. 3: Architecture Programming Manual. Santa Clara, CA: Intel Corp., 1993. Keltcher, C, K. McGrath, A. Ahmed, P. Conway: "The AMD Opteron processor multiprocessor servers," IEEE Micro, 23,2,2003, pp. 66-76. Kilburn, T., D. Edwards, M. Lanigan, F. Sumner: "One-level storage systems," IRE Transactions, E C U , 2, 1962, pp. 223-235. Lauterbach, G., T. Horel: "UltraSPARC-Ill: Designing third generation 64- bit perfor- mance," IEEE Micro, 19, 3, 1999, pp. 56-66. Liptay, J.: "Structural aspects system/360 model 85, part ii," IBM Systems Journal. 7 1,1968, pp. 15-21. Patterson, D., G. Gibson, R. Katz: "A case redundant arrays inexpensive disks (RAID)," Proc. ACM SIGMOD Conference, 1988, pp. 109-116. Rao, T. R. N., E. Fujiwara: Error-Control Coding Computer Systems. Englewood Cliffs, NJ: Prentice Hall, 1989. Ruemmler, C, J. Wi lkes: "An int roduction disk drive modeling," IEEE Computer 27, 3, 1994, pp. 5-15. Tendler, J. M., S. Dodson, S. Fields, B. Sinharoy: "IBM eS erver POWER4 system microarchitecture," IBM Whitepaper, 2001. Wang, W.-H., J.-L. Baer, H. Levy: "Organization performance two-level virtual-real cache hierarchy," Proc. 16th Annual hit Symposium Computer Architecture. 1989, pp. 140-148. Wilkes, M.: "Slave memories dynamic st orage allocation," IEEE Trans, Electronic Computers. EC-14, 2,1965, pp. 270-271. Wulf, W. A., S. McKee: "Hitting memory wall: Implications obvious," Computer Architecture News, 23, 1,1995, pp- 20-24.172 MODERN PROCESSOR DESIGN HOMEWORK PROBLEMS P3.1 Given following benchmark code assuming virtually-addressed fully-associative cache infinite capacity 64-byte blocks, com- pute overall miss rate (num ber misses divided number refer- ences). Assume variables except array locations reside registers arrays A, B, C placed consecutively memory, double A[1024], B[1024], C[1024j; for(int i=0;i<1000;i +=2) { A[i] = 35.0 * B[i] + C[i+1]; } P3.2 Given example code Problem 3.1 assuming vi rtually- addressed di rect-mapped cache capacity 8K-byte 64-byte blocks, compute overall miss rate (number misses divided number references). Assume variables except array locations reside registers arrays A, B, C placed consecutively memory. P 3 J Given example code Problem 3.1 assuming virtually- addressed two-way set-associative cache capacity 8K-byte 64- byte blocks, compute overall miss rate (number misses divided number references). Assume variables except array locations reside registers arrays A, B, C placed consecutively memory. P3.4 Consider cache 256 bytes. word size 4 bytes, block size 16 bytes. Show values cache tag bits following memory access operations following two cache organizations: direct mapped two-way associative. Also indicate whether access hit miss. Justify. addresses hexadecimal repr esentation. Use LRU (least recently used) replacement algorithm wherever needed. 1. Read 0010 2. Read 001C 3. Read 00184. Write 0010 5. Read 0484 6. Read 051C 7. Read 001C 8. Read 0210 9. Read 051C P3.5 Describe program high temporal locality. Write pseudocode program, show ha high cache hit rate.MEMORY I/O SYSTEMS 13 P3.6 Describe program low temporal locality. Write pseudocode program, show high cache miss rate. P3.7 Write programs Problems 3.5 3.6 compile plat- form supports performance counters (fo r example, Microsoft Win- dows Intel VTune performance counter soft ware). Collect report perfo rmance counter data verifies program high temporal localit experiences fewer cache misses. P3.8 Write programs Problems 3.5 3.6 C, compile using Simplescalar compilation tools available http://www .simplescalar.com . Download compile Simplescalar 3.0 simula- tion suite use sim-cache tool run programs.Verify program high temporal locality experiences fewer cache misses reportin g cache miss rates programs. P3.9 Describe program high spatial locality. Write ps eudocode program, show high cache hit rate. P3.10 Describe program low spatial locality. Write pseudocode program, show high cache miss rate. P3.ll Write programs Problems 3.9 3.10 compile plat- form supports performance counters (for example, Linux Intel VTune p erformance counter oftware). Collect report perfor- mance counter dat verifies program high temporal locality experience fewer cache misses. P3.12 Write programs Problems 3.9 3.10 C, compile using Simplescalar compilation tools available http:// www.simplescalar.coni . Download compi le Simple-scalar 3.0 simulation suite use sim-cache tool run programs.Verify program high temporal locality ex peri- ences fewer cache misses reporting cache miss rates programs. P3.13 Consider processor 32-bit virtual addresses, 4K-byte pages, 36-bit physical addresses. Assume memory byte-addressable (i.e., 32-bit virtual address specifies byte memory).  LI instruction cache: 64K bytes, 128-byte blocks, four-way set- associative, indexed tagged virtual address.  LI data cache: 32K bytes, 64-byte blocks, two-wa set-associative, indexed tagged physical address, writeback.  Four-way set-associative TLB 128 entries all. Assume TLB keeps dirty bit, reference bit, three permission bits (read, write, execute) e ntry.74 MODERN PROCESSOR DESIGN Specify number offset, index, tag bits structures following table. Also, compute total size num- ber bit cells tag data arrays. Offset indue Tag Size Tag Size Data Structura Bits Bits Bits Array Array > l-cache D-cache TLB P3.14 Given cache organization Problem 3.13, explain accesses data cache would take longer accesses instruction cache. Suggest lower-latency data cache design capacity describe organization cache would change achieve lower latency. P3.15 Given cache organization Problem 3.13, assume architecture requires writes modify instruction text (i.e„ self-modifying code) reflected immediately modified instructions fetched executed. Explain may difficult support requirement instruction cache organization. P3.16 Assume two-level cache hie rarchy p rivate level-1 instruction cache (L1I), private level-1 data cache (LID), shared level-two data cache (L2). Given local miss rates 4% LI I, 7.5% LID, 35% L2, compute global miss rate fo r L2 cache. P3.17 Assuming 1 L1I access per instruction 0.4 data accesses per instruction, compute misses per instruction L1I, LID, L2 caches Problem 3.16. P3.18 Given miss rates Problem 3 .16 assuming accesses L1I LID caches take 1 cycle, accesses L2 take 12 cycles, accesses main memory take 75 cycles, clock rate 1 GHz, compute average memory reference latency cache hierarchy. P3.19 Assuming perfect cache CPI (cycles per instr uction) pipelined pro- cessor equal 1.15 CPI, compute MCPI overall CPI pipe- lined processor memory hierarchy described Problem 3.18 miss rates access rates specified Problems 3.16 3.17. P3.20 Repeat Problem 3.16 assuming L1I local miss rate 7%, LID local miss rate 3.5%, L2 local miss rate 75%. P3.21 Repeat Problem 3.17 given miss rates Problem 3.20. P3.22 Repeat Problem 3.18 given miss rates Problem 3.20. P3.23 Repeat Problem 3.19 given miss rates Problem 3.20.MEMORY I/O SYSTEMS 17! P3.24 CPI equations used model performance in-order super- scalar processors multilevel cache hierarchies. Compute CPI processor, given following parameters:  Infinite cache CPI 1.15  LI cache miss penalty 12 cycles  L2 cache miss penalty 50 cycles  LI instructio n cache per-instruction miss rate 3% (0.03 misses/ instruction)  data cache per-instruction miss rate 2% (0.02 misses/ instruction).  L2 local cache miss rate 25% (0.25 misses/L2 reference). P3.25 usually case set-associative f ully associative cache higher hit rate direct-mapped cache. However, always true. illustrate this, show memory reference trace program higher hit rate two-block direct-mapped cache fully associative cache two blocks. P3.26 Download install Simplescalar 3.0 simul ation suite instruc- tional ben chmarks www.simplescalar.com . Using sim-cache cache simulator, plot cache miss rates benchmark fol- lowing cache hierarchy: 16K-byte two-way set-associative instruc- tion cache 64-byte lines; 32K-byte fo ur-way set-associative data cache 32-byte lines; 12K-byte eight-way set-associative L2 cache 64-byte lines. P3.27 Using benchmarks tools Problem26, plot several miss- rate sensitivity curves three caches (L1I, LID, L2) varying following parameters: cache size 0.5X, lx, 2x, 4x; associativity 0.5x, lx, 2x, 4x; block size 0.25x, 0.5x, lx, 2x, 4x. Hold parameters fixed values Problem 3.26 varying three parameters sensitivity curve. Based sensitivity curves, identify appropriate value parameter near knee curve (if any) benchmark. P3.28 Assume synchronous front-side processor-memory bus operates 100 MHz 8-byte data bus. Arbitration bus takes one bus cycle (10 ns), issuing cache line read command 64 b ytes data takes one cycle, memory controller latency (including DRAM access) 60 ns, data doublewords returned back-to- back cycles. assume bus blocking circuit-switched. Compute latency fill single 64-byte cache line. compute peak read bandwidth processor-memory bus, assuming processor arbitrates bus new read bus cycle following completion last read.176 MODERN P ROCESSOR DESIGN P3.29 Given assumptions Problem 3.28, assume nonblocking ( split- transaction) bus overlaps arbitration commands data transfers, multiplexes data address lines. Assume read command requires single bus cycle, assume memory controller infinite DRAM bandwidth. Compute peak data band- width front side bus. P330 Building assumptions Problem 3.29, assume bus dedicated data lines separate ar bitration mechanism addresses/ commands data. C ompute peak data bandwidth front side bus. P3.31 Consider finite DRAM bandwidth memory con troller, follows. Assume double-data-rate DRAM operating 100 MHz parallel non-interleaved organization, 8- byte interface DRAM chips. assume cache line read results DRAM row miss, requiring precharge RAS'cycle, followed row-hit CAS cycles doublewords cache line. Assuming mem- ory controller overhead one cycle (10 ns) initiate read operation, one cycle latency transfer data DRAM data bus processor-memory bus, compute latency reading one 64-byte cache block. w compute peak data bandwidth memory interface, ignoring DRA refresh cycles. P332 Two page-table architectures common use today: multilevel forward page tables hashed page tables. Write pseudocode function matching th e following function declaration searches three-level forward page table returns 1 hit 0 miss,and assigns * real address n hit. int FPTSEARCH(VOID *PAGETABLEBASE, VOID* VIRTUALADDRESS, VOID** REAL ADDRESS); P3.33 Problem 3.32, write pseudocode function matching fol- lowing function declaration searches hashed page table returns 1 hit 0 miss, assigns *realaddress n hit: INT HPTSEARCH (VOID *PAGETABLEBASE, VOID* VIRTUALADDRESS, VOID** REALADDRESS); P3.34 Assume single-platter disk drive average seek time 4.5 ms, rotation speed 7200 rpm, data transfer rate 10 Mbytes/s per head, controller overhead queueing 1 ms. average access latency 4096-byte read? P3.35 Recompute average access latency Problem34 assuming rota- tion speed 15K rpm, two platters, average seek time 4.0 ms. J CHAPTER Superscalar Organ ization CHAPTER OUTLINE 4.1 Umrtations Scalar Pipelines 42 FromScalarto Superscalar Pipelines 4.5 Superscalar Pipeline Overview 4.4 Summary References Homework Problems pipelining proved extremely effective inicroarchitecture tech- nique, type scalar pipelines presented C hapter 2 number short- comings limitations. Given never-ending push higher performance, limitations must overcome order continue p rovide fu rther speedup existing programs. solution superscalar pipelines able achieve per- formance levels beyond possible scalar pipelines. Superscalar machines go beyond single-instruction pipeline able simultaneously advance multiple instructions pipeline stages. incorporate multiple functional units achieve greater concurrent p rocessing multiple instructions higher instruction execution throughput. Another foundational attr ibute superscalar processors ability execute instructions order different specified original program. sequential ordering instructions standard programs implies un necessary pr ece- dences ins tructions. capability executing instructions program order relieves sequential imposition allows par allel processing instructions without requiring modification original program. following chapters attempt codify body knowledge superscalar processor design systematic fashion. chapter focuses issues related pipeline organization superscalar machines. techniques address dynamic 177178 MODERN PROCESSOR DESIGN interaction superscalar machine instructions processed presented Chapter 5. Ca se studies two commercial superscalar processors presented Chapters 6 7, Chapter 8 provides broad survey his- torical current designs. 4.1 Limitations Scalar Pipelines Scalar pipelines c haracterized single-ins truction pipeline k stages. instructions, regardless type, traverse set pipeline stages. most, one instruction resident pipeline stage one time, instructions advance pipeline stages lockstep fashion. Except pipeline stages stalled, instruction stays pipeline stage exacdy one cycle advances next stage next cycle. rigid scalar pipelines three fundamental limitations: 1. maximum throughput scalar pipeline bounded one instruction per cycle. 2. unification instruction types one pipeline yield ineffi- cient design. 3. stalling lockstep rigid scalar pipeline induces unnecessary pipe- line bubbles. elaborate limitations Sections 4.1.1 4.1.3. 4.1.1 Upper Bound Sc alar Pipeline Throughput stated Chapter 1 shown Equation (4.1), processor performance increased either increasing instructions per cycle (IPC) and/or frequenc decreasing total instruction count. Performance = ! xinstructions x = "^frequency instruction count cycle cycle time instruction count (4.1) Frequency increased employing deeper pipeline. deeper pipeline fewer logic ga te levels pipeline stage, leads shorter cycle time higher frequency. However, point diminishing return due hardware overhead pipelining. Furthermore, deeper pipeline potentially incur higher penalties, terms number penalty cycles, dealing inter-instruction ependences. additional average cycles per instruction (CPI) overhead due higher penalty possibly eradicate benefit due reduction cycle time. Regardless pipeline depth, scalar pipeline initiate process- ing one instruction every machine cycle. Essentially, average IPC scalar pipeline fundamentall bounded one. get instruction throughput, especially deeper pipelining longer cost-e ffectiveSUPERSCALAR RGANIZATION 13 way get performance, ability initiate one instruction every machine c ycle necessary. achieve IPC greater one, pipelined processor must able initiate proc essing one instruction every machine cycle. require increasing width pipeline facilitate one instruction resident pipeline stage one time. identify pipelines parallel pipelines. 4.1.2 Inefficient Unification ingle Pipeline Recall second idealized assumption pipelining r epeated computations processed pipeline identical. instruction pipelines, clearly case. different instruction types require different sets subcomputations. unifying different requirements one pipeline, difficulties and/or inefficiencies result. Looking unification different instruction types TYP pipeline Chapter 2, observe earlier pipeline stages (such IF, ID, RD stages) significant uniformity. However, execution stages (such ALU MEM stages) sub- stantial diversity. fact, TYP exam ple, ignored floating-point instruc- tions purpose due difficulty unifying instruction types. reason one point time "RISC revolution," floating- point instructions categorized inherently CISC considered violating RISC principles. Certain instruction ypes make unification single pipeline quite difficult. include floating-point instructions certain fixed-point instruc- tions (such multiply divide instructions) require multiple ex ecution cycles. Instruction require long possibly variable latencies difficult unify simple instructions require single cycle latency. disparity CPU memory speeds continues widen, latency (in terms number machine cycles) memory instructions continue increase. latency differences, hardware resources required support execution different instruction types also quite different. co ntinued push faster hardware, specialized execution units cus tomized specific instruction types required. also contribute ne ed greater diversity execution stages instruction pipeline. Consequently,, forced unification instruction ty pes single pipe- line becomes either impossible extremely ineffic ient future high-performance processors. parallel pipelines strong motivation unify execution hardware one pipeline, instead implement multiple different execution units subpipelines execution portion parallel pipelines. call parallel pipelines diversified pipelines. 4.1.3 Performance Lost due Rigid Pipeline Scalar pipelines ar e rigid sense instructions advance pipeline stages lockstep fashion. Instruction enter scalar pipeline according program order, i.e., order. stalls pipeline, instructions pipeline stages advance synchronously program order instructions is180 MODERN PROCESSOR DESIGN maintained. instruction stalled pipeline stage due dependence leading instruction, instruction held stalled pipeline stage leading instructions allowed proceed pipeline stages. rigid nature scalar pipeline, dependent instruction stalled pipeline stage j, earlier stages, i.e., stages 1,2,.... - 1, containin g trailing instruc- tions also stalled. stages pipeline stalled instruction stage forwarded dependent operand. inter-instruction dependence satisfied, stalled instructions advance synchronously pipeline. rigid scalar pipeline, stalled stage middle pipeline affects earlier stages p ipeline; essentially stalling stag e < propa- gated backward preceding stages pipeline. backward propagation talling stalled stage scalar pipeline induces unnecessar pipeline bubbles idling pipeline stages. instruc- tion stalled stage due dependence leading instruction, may another instruction trailing stalled instruction depen- dence leading instruction would require stalling. example, independent trailing instruction could stage — 1 would unnecessarily stalled due stalling instruction stage i. According program semantics, necessary instruction wait stage — 1. instruction allo wed bypass stalled instruction continue p ipe- line stages, idling cycle pipeline eliminated, effectively reduces penalty due stalled instr uction one cycle; see Figure 4 .1. multiple instructions able allowed bypass stalled instruction, multiple penalty cycles eliminated "covered" sense idling pipeline stages given useful instructions process. Potentially penalty cycles due stalled instruction covered. Allowing bypassing stalled leading instruction trailing instructions referred out-of-order execution instructions. rigid scalar pipeline allow out-of-order exe- cution hence incu r unnecessary penalty cycles enforcing int er-instruction dependences. Parallel pipelines support out-of-order execution called dynamic pipelines. Bypassing stalled instruction allowedStalled instructionBackward propagation stalling Figure 4.1 Unnecessary Stall Cycles Induced Backward Propagation Stalling Rigid Pipeline.SUPERSCALAR ORGANIZATION 18 4.2 Scalar Superscalar Pipelines Superscalar pipelines viewed natural descendants scalar pipelines involve extensions alleviate three limitations (see Section 4.1) scalar pipelines. Supersca lar pipelines parallel pipelines, instead scalar pipelines, able initiate processing multiple instructions every machine cycle. ddition, superscalar pipelines diversified pipelines employing multiple heterogeneous functional units e xecution stage(s). Finally, superscalar pipelines implemented dynamic pipelines rder achieve best possible performance without requiring reordering instructions compiler. three characterizing attributes superscalar pipelines elaborated section. 4.2.1 Paralle l Pipelines degree parallelism machine measured maximum number instructions concurrently progress one time. fc-stage scalar pipe- line k instructions concurrently resident machine potentially achieve factor-of-A- speedup nonpipelined machine. Alternatively, speedup achieved employing copies nonpipelined machine pro- cess k instructions parallel. two forms machine parallelism illustrated Figure 4.2(b) (c), de noted temporal machine parallelism spa- tial machine parallelism, respectively. Temporal spatial parallelism degree yield factor potential spee dup. Clearly, temporal parallel- ism via pipelining requires less hardware spatial parallelism, requires repli- cation entire processing unit. Parallel pipelines viewed em ploying temporal spatial machine parallelism, illustrated Figure 4.2(d), achieve higher instruction pr ocessing throughput effi cient manner. speedup scalar pipeline measured respect nonpipelined design primarily determined th e depth scalar pipeline. parallel pipelines, superscalar pipelines, speedup usually measured re- spect scalar pipeline primarily etermined width parallel pipeline. parallel pipeline width concurrently process 5 instruc- tions pipeline stages, lead potential speedup scalar pipeline. Figure 4.3 illustrates parallel pipeline width = 3. Significant additional hardware resources required implementing parallel pipelines. pipeline stage potentially process advan ce instruc- tions every mach ine cycle. Hence, logic complexity pipeline stage increase factor s. worst case, circuitry interstage intercon- nection increase factor s2 sxs crossbar used connect instruction buffers one stage instruction buffers next stage. order support concurrent register file accesses instructions, th e number read write ports register file must increased factor s. Similarly, additional I-cache D-cache access ports must provided. shown Chapter 2, Intel i486 five-stage scala r pipeline [Crawford, 1990]. Th e sequel i486 Pentium micro processor from182 MODERN PROCESSOR DESIGN (a) parallelism (b) Temporal parallelism (c) Spatial parallelism11"1 (d) Parallel pipeline Figure 4.2 Machine Parallelism: (a) Parallelism (Nonpipelined); (b) Temporal Parallelism (Pipelined); (c) Spatial Parallelism (Multiple Units); (d) Combined Temporal Spatial Parallelism Intel [Intel Corp., 1993]. Pentium microprocessor superscalar machine implementing parallel pipeline width = 2. essentially implements two i486: pipelines; see Figure 4.4. Multiple instructions fetched decoded first two stages parallel pipeline every machine cycle. cycle, potentially two instructions issued two execution pipelines, i.e., U pipe V pipe. goal maximize number dual-issue cycles. superscalar Pen tium microprocessor ach ieve peak execution rate two instructions per machine cycle. compared scalar pipeline i486, Pentium parallel pipeline requires significant additional hardware resources. First, five pipeline stagesSUPERSCALAR ORG ANIZATION 183 ID RD ALU MEM WB1T " ! 1 ; 1i—*— *— rr j 1~ii 1 _| Figure 4.3 Parallel Pipeline Width s= 3. DI D2 EX (a)IF D2 EXDl Dl D2 EX Upipe Vpipe (b) Figure 4.4 (a) Five-Stage i486 Scalar Pipeline; (b) Five-Stage Pentium Parallel Pipeline ofWidthi=2.184 MODERN PROCESSOR DESIGN doubled width. two execution pipes accommodate two instructions last three stages pipeline. execute stage perform ALU operation access D- cache. Hence, additional ports register file must provided support con current execution two ALU operations every cycle. .two instructions th e execute stage load/store instructions, D-cache must provide dual access. true dual- ported D-cache expensive implement. Instead, Pentium D-cache imple- mented single-ported D-cache eight-way interlea ving. Simultaneous accesses two different banks two load/store instructions U V pipes supported. bank conflict, i.e., load/st ore instructions must access bank, two D-cache accesses serialized. 4.2.2 Diversified Pipelines hardware resources required support execution different instruction types vary significantly. scalar pipeline, diverse requirements execution instruction types must unified single pipeline. resultant pipeline highly inefficient. instruction type requires subset execution stages, ust traverse execution stages. Every instruction idling traverses unnecessary stages incurs signifi cant dynamic external fragmentation. executio n latency instru ction types equal total number execution stages. result unnecessary stalling trailing instructions and/or require additional forwarding paths. inefficiency due unification one single pipeline na turally addressed parallel pipelines employing multiple ifferent functional units execution stage(s). Instead implementing J identical pipes s-wide parallel pipeline, execution portion parallel pipeline, diversified execution pipes implemented; see Figure 4.5. example, four'exe- cution pipes, functional units, differing pipe depths implemented. RD stage dispatches instructions four execution pipes based instruction types. number advantages implementing diversified execution pipes. pipe customized particular instructio n type, resulting efficient hardware design. instruction type incurs necessary latency makes use stages execution pipe. certainly e ffi- cient implementing identical copies universal execution pipe execute instruction types. al l inter-instruction de pendences different instruction types resolved prior dispatching, instructions issued individual execution pipes, stall ing occur due instructions pipes. allows distributed indepen- dent control execution pipe. design diversified parallel pipeline require special considerations. One important consideration th e number mix functional units. Ideally number functional units match available instruction-level parallelism program, mix functional units match dynamic mix instruction types program. first-generation superscala r processorsSUPERSCALAR ORGANIZATION 185 Figure 4.5 Diversified Parallel Pipeline Four Execution Pipes. simply ntegrated second execution pipe processing floating-point nstruc- tions existing scalar pipe proce ssing non-floating-point instructions. superscalar designs evolved two-issue machines four-issue machines, typi- cally four functional units implemented executing integer , floating-point, load/store, branch instructions. recent designs corporate multiple integer units, dedicated long-latency integer operations multiply divide, others dedicated processing special operations image, graphics, signal processing applications. Similar pipelining, employment multiplicity diversified functional units design high-p erformance CPU recent invention. CDC 6600 incorporates pipelining th e use multiple functional units [Thornton, 1964]. CPU CD C 6600 employs 10 diversified functional units, shown Figure 4.6. 10 functional units operate data stored 24 operating registers, consist 8 address registers (18 bits), 8 ndex reg isters (18 bits), 8 floating-point registers (60 bits). 10 functional units operate independently consist fixed-point adder (18 bits), floating-point adder (60 bits), two multiply units (60 bits), divide unit (60 bits), shift unit (60 bits), boolean unit (60 bits), two increment units, branch unit. CDC 6600 CPU pipelined processor two decoding stages preceding execution portion;186 MODERN PROCESSOR DESIGN Long add Multiply Multiply Divide Shift Boolean Add Increment Increment Branch Figure 4.6 CDC 6600 10 Diversified F unctional Units CPU. however, 10 functional units pipelined variable exe cution latencies. example, fixed-point add requires 3 cycles, floating-point multiply (divide) requires 10 ( 29) cycles. goal CDC 6600 CPU sustain issue rate one instruction per machine cycle. Another su perscalar microprocessor employed similar mix functional units CDC 6600. prior formation PowerPC alliance IBM Apple, Motorola developed clean design wide superscalar microprocessor called 88110 [Diefendorf Allen, 1992]. Inte restingly, 88110 also employs 10 functional units; see Figure 4.7. 10 functional units consist two integer units, bit field unit, floating-point add unit, multiply unit, divide unit, two graphic units, l oad/store unit, instruction sequencing/ branch unit. units single-cycle latency. exception divide unit, units multicycle latencies pipelined. terms total number functional units, the.88110 represents one wider super- scalar designs. 4.2.3 Dynamic Pipelines pipelined design, buffers required pipeline stages. scalar rigid pipeline, single-entry buffer placed bet ween two consecutive pipeline stagesSUPERSCALAR ORGANIZATION 187 Target instruction cacheInstruction cacheBus interface History bufferGeneral register fileFloating-point register fileInstruction sequencer branch unitData cache Integer unitInteger unitSource busses Bit Held unitMultiplier unitFloating-point add unitDivider unitGraphics add unitGraphics pack unitLoad/ store unit Writeback busses Figure 4.7 Motorola 88110 Superscalar Microprocessor. Source: Diefendorf Allen, 1992.1X AMPL E (stages + 1), shown Figure 4.8(a). buffer holds essential control data bits instruction traversed stage pipeline ready traverse stage + 1 next machine cycle. Single-entry buffers quite easy control. every machine cycle, buffer's current content used input stage f + 1; end cycle, buffer latches result produced stage i. Essentially buffer clocked every machine cycle. e xception occurs ir tstruction buffer must held back prevented tra- versing stage + 1. In-that case, clocking buffer disabled, instruc- tion stalled buffer. Clearly buffer stalled scalar rigid pipeline, stages preceding tage must also stalled. Hence, scalar rigid pipeline, stalling, every instruction r emains buffer exactly one machine cycle adva nces next buffer. instructions enter leave buffer exactly order specified original sequential code. parallel pipeline, multientry buffers needed two consecutive pipeline stages hown Figure 4.8( b). Multientry buffers viewed sim- ple extensions single-entry-buffers. Multiple instructions latched multientry buffer every machine cycle. next cycle, instructions traverse next pipeline stage. instructions multientry buffer required adva nce simultaneously lockstep fashion, control the188 MODERN PROCESSOR DESIGN Stage Buffer ( 1 ) Stage + 11 | \ Stage Buffer (n) Stage + 1 (in order) n (in order) (a)T (b) Stage Buffer (* «) Stage1 (in oi (out order) 1 (c) Figure 4.8 Interpipeline-Stage Buffers: (a) Single-Entry B uffer; (b) Multientry Buffer; (c) ultientry Buffer Reordering. multientry buffer similar singlfeentry buffer. Th e entire multientry buffer either clocked stalled machine cycle. However, operation parallel pipeline may induce unnecessary stalling instructions multientry buffer. efficient operation parallel pipeline, much more- sophisticated multientry buffers needed entry simple mu ltientry buffer Figure 4.8(b) hardwired one- write port one read port, interaction multiple entries. One enhancement simple buffer add connectivity entries facilitate movement data entries. example, entries con- nected linear chain like shift register function FIFO queue. Another enhancement provide mechanism inde pendent accessing entry buffer. require ability explicitly address individual entry buffer independently control reading writing entry. input/output port buffer given ability access entry buffer, multientry buffer effectively resemble small multiported RAM. buffer instruction remain entry buffer many machine cycles updated modified resident buffer. enhancement incorporate associative accessing entries buffer. Instead using conventional ad dressing index entry buffer, content anSUPERSCALAR ORGANIZATION 1£ entry used associative tag index entry. accessing mechanism, multientry buffer becomes small associative cache memory. Superscalar pipelines differ (rigid) scalar pipelines one key aspect, use complex mul tientry buffers buffering instructions flight. order minimize unnecessary stalling instructions parallel pipeline, trailing instructions must allowed bypass stalled leading instruction. bypassing change order execution instructions original sequential order static code. out-of-order execution instructions, potential approaching data flow limit instruction exe cution; i.e., instructions executed soon operands available. parallel pipeline supports out-of-order execution instructions called dynamic pipeline. dynamic pipeline achieves out-of-order execution via use complex multientry buffers allow instructions enter leave b uffers different orders. reordering multientry buffer shown Figure 4.8(c). Figure 4.9 illustrates parallel diversified pipeline width * = 3 dynamic pipeline. execution portion th e pipeline, co nsisting four pipelined (in order) Figure 4.9 Dynamic Pipeline Width j = 3 190 MODERN PROCESSOR DESIGN functional units, bracketed tw reordering multientry buffers. Th e first buffer, called dispatch buffer, loaded decoded instructions according p rogram order dispatches instructions functional units potentially order different program order. Hence instructions leav e dispatch buffer different order order enter dispatch buffer. pipeline also implements set diverse functional units di fferent latencies. potential out-of-order issuing functional units and/or variable latencies functional units, instructions finish exe cution order. ensure exceptions handled according original program order, instructions must com pleted (i.e., machine state must updated), program order. instructions finish execution order, another reordering mul tientry buffer needed back end execution portion pipeline ensure in- order completion. b uffer, called completion buffer, buffers instructions may finished execution order retires instructions order outputting instructions final writeback stage program order. dynamic pipeline facilitates out-of-order execution instructions order achieve shortest possible execution time, yet able provide precise exception retir- ing instructions upd ating machine state according th e program order. 4.3 Superscalar Pipeline Overview section presents overview critical issues involved design superscalar pipelines. focus organization, struc tural design, super- scalar pipelines. Issues techniques related dynamic interaction machine organization instruction semantics optimization resultant machine performance covered Chapte r 5. Essentially chapter focuses design machine ganization, Chapter 5 takes acco unt interac- tion machine program. Similar use six-stage TY P pipeline Chapter 2 vehicle presenting scalar pipeline design, use six-stage TEM superscalar pipeline shown Figure 4.10 "template" disc ussion organization super- scalar pipelines. Compared scalar pipelines, far v ariety greater diversity th e implementation superscalar pipelines. TEM superscalar pipeline viewed actual implementation typical represen- tative superscalar pipeline. six stages TEM superscalar pipeline viewed logical pipeline stages may may correspond six physical pipeline stages. six stages TEM superscalar pipeline provide nice framework outline discussing six major portions of, six major tasks performed by, superscalar pipeline organizations. six stages TEM superscalar pipeline fetch, decode, dispatch, execute, complete, retire. execute sta ge include multiple (pipelined) functional units different types different execution latencies.This necessitates dispatch sta ge distribut e instructions different ypes corresponding functional units. out-of-order execution instructions execute stage, complete stage needed reorder instructions ensure in-orderSUPERSCALAR ORGANIZATION 19 Fetch Decode Dispatch Execute nI Complete ~T RetireInstruction buffer Dispatch b uffer Issuing buffer Completion buffer Store buffer Figure 4.10 Six-Stage Template (TEM) Superscalar Pipeline. updating machine state. Note also mult ientry buffers separating six stages. complexity buffers vary depending func- tionality location superscalar pipeline. six stages design issues related addressed turn. 4.3.1 Instruction Fe tching Unlike scalar pipeline, superscalar pipeline, parallel pipeline, capable fetching one instruction I-cache every machine cycle. Given superscalar pipeline width s, fetch stage able fetch * instructions I-cache every machine cycle. implies physical organization I-cache must wide enough row I-cache array store192 MODERN PROCESSOR DESIGN Address Address Tag Tag  * Tagi Tag  Cache line Tag'1 1 1 hi 1 1 1 Tag' 1 1 1 1 ~ 1 1 1 Tag () (b) Figure 4.11 Organization Wide l-Cache: (a) One Cache Line Equal One Physical Row; (b) One Cache Line Equal Two Physical Rows. instructions entire row accessed one time. current discus- sion, assume acces latency I-cache one cycle fetch width equal row width. Typically wide cache organization, cache line corresponds physical row cache array; also possible cache line span several physical rows cache array, illustrated Figure 4.11. primary objective fetch stage maximize instruction-fetching bandwidth. sustained throughput achieved fetch stage impact overall throughput superscalar pipeline, throughput subse- quent stages depends cannot possibly exceed throughput fetch stage. Two primary impediments achieving maximum throughput instructions fetched per cy cle (1) misalignment instructions fetched, called fetch group, respect row organization I-cache array; (2) presence control-flow changing instructions fetch group. every machine cycle, fetch stage uses program counter (PC) index I-cach e fetch instruction pointed PC along next - 1 instructions, i.e., instructions fetch g roup. entire fetch group stored row cache array, instructions fetched. hand, fetch group crosses row boundary, instructions fetched cycle (assuming one row I-cache accessed cycle). Hence, instructions first row fetched; remaining instructions require another cycle fetching. fetch bandwidth effectively reduced e-half, requires two cycles fetch 5 instructions. due misalignment fetch group respect row b oundaries I-cache array, illustrated Figure 4.12. misalignments reduce effective fetch bandwidth. case cache line corresponds physical row, sho wn Figure 4.11(a), crossing row boundary also corresponds crossing cache line boundary, incur additional problems. fetch group spans two cache lines, induce I-cache miss involving second line even though theSUPERSCALAR ORGANIZATION 193 JL PC = XX00001 "^.000 001 11100t 0110 -Fetch group - -Row width - rigure4.12 Misalignment Fetch Group Relative Row Boundaries l-Cache Array. first line resident. Even lines resident I-cache, physical accessing multiple cache lines one cycle problematic. two possible solutions misalignment problem. first solution static technique employed compile time. compiler given informa- tion organization I-cache, e.g., indexing scheme row size. Based infor mation, instructions appropriately placed memory loca- tions ensure aligning fetch groups physical rows. example, every instruction target branch placed memory location mapped first instruction row. increase probability fetching * instructions beginning row. techniques implemented reaso nably effective. problem solution object code tuned partic ular I-cache organization may properly aligned I-cache organizations. Another problem static code occupy larger address range, potentially lead higher I-cache miss rate. second solution misalignment problem involves using hardware run time. Alignment hardware incorporated ensure instructions fetched every cycle even fetch group crosses row boundary (but cache line boundary). alignment hardware inco rporated IBM RS/6000 design; briefly describe design [Grohoski, 1990; Oehler Groves, 1990]. RS/6000 employs two-way set-associative I-cache line size 16 instructions (64 bytes). row I- cache array stores four associative sets (two per set) instructions. Hence, line I-cache spans four physical rows, shown Figure 4.13. physical I-cache array actually composed four independent subarrays (denoted 0,1,2, 3), accessed parallel. One instruction fetched subarray every I-cache access. Which194 MODERN PROCESSOR DESIGN FFAR Odd directory sets & B Even directory sets A&BT logic TLB hit buffer control logic Interlock, dispatch, branch, execution logic0AO BO 1A4 B4 2A8 B8 3A12 B12 . 255T logicT logic 255Al Bl A5 B5 A9 B9 A13 Bl3  255A2 B2 A6 B6 A10 BIO A14 B14 Instruction buffer network Figure 4.13 Organization RS/6000 Two-Way Set-Associative l-Cache Auto-Realignment.A3 B3 A7 B7 BllA15 B15  two instruction (either B) associative set accessed depends two tag match address. instruction addresses allocated interleaved fashion across four subarrays. PC happens point first subarray. i.e., subarray 0, n four con- secutive instructions simultaneously fetched four subarrays. four instructions reside physical row I-cache, four subarrays accessed using row address. hand, PC indexes middle row, e.g., first instruction fetch group resides subarray 2, four consecut ive instructions fetch group span across two rows. RS/6000 deals problem detecting starting address points subarray subarray 0 aut omatically incre- menting row address nonconsecutive subarrays. done "T-logic" hardware ssociated subarray. example, PC indexes subarray 2, th en subarrays 2 3 accessed row address presented them. However T-logic subarrays 0 1 detect condi- tion automatically increment row address presented subarrays 0 t.SUPERSCALAR ORGANIZATION 19 Consequently two instructions fetched sub arrays 0 1 actually next physical row I-cache. Therefore, regardless starting address address points I-cache row, four consecutive instructions always fetched every cycle long fetch group cross cache line boundary. fetch group crosses cache line boundary, instructions first cache line fetched cycle. Given fact cache line RS/6000 consists 16 instructions, 16 possible starting addresses word cache line, average fetch bandwidth I-cache organization (13/16) x 4 + (1/16) x 3 + (1/16) x 2 + (1/16) x 1 = 3.625 instructions per cycle. Although fetch group begin one four ubarrays, subarrays 0,1, 2 require T-logic hardware. row address subarray 3 never needs incremente regardless starting subarray fetch group. instruction buffer network RS/6000 contains rotating network rotate four fetched instructions present four instructions, output, original program order. design I-cache quite sophisticated ensure high fetch bandwidth even fetch group isaligned respect row organization I-cache. However, quite hardware intensive made feasible RS/6000 implemented multiple chips. misalignment problem, second impediment sustaining maximum fetch bandwidth instructions per cycle presence control- flow changing instructions within fetch group. one instructions middle fetch group conditional branch, subsequent instructions fetch group discarded branch ta ken. Consequently, happens, fetch bandwidth effectively redu ced. problem fundamen- tally due presence control dependences instructions related handling conditional branches. topic, viewed related dynamic interaction machine program, addressed greater detail Chapte r 5, covers techniques dealing control dependences branch instructions. 4.3.2 Instruction Decoding Instruction decoding involves ide ntification indi vidual instructions, determination instruction types, de tection inter-instruction depen- dences among group instructions fetched yet dis- patched. complexit instruction decoding task strongly influenced two factors, namely, ISA width parallel pipeline. typ ical RISC instruction set wit h fixed-length instructions simple instruction formats, decoding task quite straightforward. expl icit effort needed determine beginning ending instruction. relatively different instruc- tion formats addressing modes make distinguishing instruction types reasonably easy. simply decoding small portion, e.g., one op code byte, instruction, instruction type format used determined the196 MODERN PROCESSOR DESIGN remaining fields instruction interpreta tion quickly deter- mined. RISC instruction set si mplifies instruction decoding task. RISC scalar pipeline, instruction decoding quite trivial. Frequently decode stage used accessing register operands merged register read stage. However, RISC parallel pipeline multiple instruc- tions simultaneously decoded, decode stage must identify dependences instructions determine independent instructions dispatched parallel. Furthermore, support efficient nstruction fetching, decode st age must quickly identify control-flow changing branch instructions among instructions decoded order provide quick feedback fetch stage. two tasks conjunc tion accessing many register operands make logic decode stage RISC parallel pipeline somewhat complex. large number comparators needed determining register dependences nstructions. register files must multiported able support man simultaneous accesses. Multiple busses also needed route accessed operands appropriate destination buffers. possible decode stage become critical stage overall superscalar pipeline. CISC parallel pipeline, instruction decoding task become even complex usually require multiple pipeline stages. parallel pipeline, identification individual instructions types longer trivial. Intel Pentium AMD K5 employ two pipeline stages decoding IA32 ins tructions. mor e deeply pipe lined Intel Pentium Pro, total five machine cycles required access I-cache decode IA32 instructions. use variable instruction lengths imposes undesirable sequentiality instructio n decoding task; leading instruction must decoded length determined beginning next instruction identified. Conseque ntly, simultaneous parallel decoding multiple instructions become quite challenging. w orst case, must assumed new ins truction begin anywhere within fetch group, large number decoders used simultaneously "speculatively" decode ins truc- tions, starting every byte boundary. extremely complex quite inefficient. additional burden instruction decoder CISC parallel pipeline. decoder must translate architected instructions internal low- level operations directly executed hardware. translation process first described Part, Hwu, Shebanow seminal paper high-performance substrate (HPS), de composed complex VAX CISC instructions RISC-like primitives [Part et al., 1985]. Th ese internal operations resemble RISC instructions viewed vertical micro-instructions. AMD K5 operations called RISC operations ROPs (pronounced "ar-ops"). Intel P6 hese internal operations identified micro-operations [lops (pronounced "you-ops"). IA32 instruction translated one ROPs (tops. According Intel, average, one IA32 instruction trans- lated 1.5 2.0 Hops. hese CISC parallel pipelines, instruction decoding instruction completion stages, instructions flight within theSUPERSCALAR ORGANIZATION 1 pJlOMMacro-instruction bytes IFU L. Instruction buffer 4 fiOps /MOP 77m n e x 16^twl address calculation Decoder Decoder Decoder 2 0Decoder Decoder 2 1 fiop Branch address calculation fiap queue (6) Figure 4.14 Fetch/Decode Unit Intel P6 Superscalar Pipeline.: X A|M pi E machine internal operations. book, convenience adopt Intel terminology refer internal operations (tops. instruction decoder Intel Pentium Pro presented illustrative example instruction decoding CISC parallel pipeline. diagram fetch/decode unit P6 shown Figure 4.14. machine cycle, I-cache del iver 16 aligned bytes instruction queue. Three parallel decod- ers simultaneously decode instruction bytes instruction queue. first decoder front queue capable decoding IA32 instructions, two decoders limited capability decode simple IA32 instructions su ch register-to-register instructions. decoders translate IA32 instructions internal three-address uops. ^ops employ load/store model. IA32 instruction complex addressing modes translated multiple uops. first (generalized) decoder generate four Uops per cycle response decoding IA32 instruction. two (restricted) decoders generate one uop per cycle response decoding simple IA32 instruction. machine cycle least one IA32 instructio n decoded generalized decoder, leading generation one uops. goal go beyond two restricted decoders also decode two simple IA32 instructions trail leading IA32 instruction th e achine cycle. ideal case three parallel decoders generate total six (lops one machine cycle. complex IA32 instructions require four uops translate, reach front instruction queue, th e generalized decoder invoke uops sequencer emit microcode, simply pre- programmed sequence normal Uops. uops require two machine cycles generate. Uops generated three parallel decoders loaded reorder buffer (ROB), 40 entries hold 40 uops, await dispatching functional units.198 MODERN PROCESSOR DESIGN many su perscalar processors, es pecially implement wide and/or CISC parallel pipelines, instructio n decoding hardware extremely com- plex require partitioning multiple pipeline stages. number decoding stages increased, branch penalty, terms number machine cycles, also increased. Hence, desirable keep increasing depth decoding portion parallel pipeline. help alleviate complexity, tech nique called predecoding propose implemented. Predecoding moves part decoding task side, i.e., input side, I-cache. I-cache miss occurs new cache line brought memory, instructions cache line partially decoded decoding hardware placed memory I-cache. instructions som e additional decoded inform ation stored I-cache. decoded information, form predecode bits, simplifies instruction decoding task instructions fetched I- cache. Hence, part decoding performed instructions loaded I-cache, instead every time instructions fetched I-cache. decoding hardware moved input side I-cache, instruction decoding complexity parallel pipeline simplified. AMD K5 example CISC superscalar pipeline employs aggressive predecoding IA32 instructions fetched memory prior loaded I-cache. single bus transaction total eight instruction tes fetched memory. bytes predecoded, five additional bits generated pr edecoder instruction bytes. five predecode bits contain info rmation location start end IA32 instruction, number (lops (or ROPs) needed translate IA32 instruction, location op codes prefixes. additional predecode bits stored I-cache along original instruction's bytes. Consequently, original I-cache line size 128 bits (16 bytes) increased additional 80 bits; see Figure 4.15. 1-cache access, 16 instruction bytes fetched along 80 predecode bits. predecode bits significantly simplify instruction decoding allow th e simultaneous decoding multiple IA32 instructions fo ur identical decoders/translators generate four flops cycle. two forms verhead associated predecoding. I-cache miss penalty increased due necessity predecoding instruction bytes fetched memory. serious problem I-cache miss rate low. overhead involves storing predecode bits I-c ache consequent increase I-cache size. K5 size I-cache increased 50%. clearly tradeoff bet ween aggressiveness predecoding I-cache size increase. Predecoding j ust limited alleviating sequential bott leneck parallel decoding multiple CISC instructions CISC parallel pipeline. also used support RISC paralle l pipelines. RISC instructions predecoded loaded I-cache. predecode bits used identify control-flow changing branch instructions within fetch group exp licitlySUPERSCALAR ORGANIZATION memory 8 instruction bytes ^64 - - Bytel Byte2 Byte8 j Predecode logic 8 instruction bytes + predecode bits5 bits 5 bits 5 bits 64 + 40 Bytel Byte2...Byte8 I-Cache 16 instruction bytes + predecode bits' 128 + 80 Decode, translate, dispatch 4 ROPs ROP1 ROP2 ROP3 ROP4 Figure 4.15 Predecoding Mechanism K5. identify subgroups independent instructions within fetch group. example, PowerPC 620 employs 7 predecode bits instruction word I-cache. UltraSPARC, MIPS RIOOOO, HP PA-8000 also employ either 4 5 prede- code bits fo r instruction. superscalar pipelines become wider number instructions must simultaneously decoded increases, instruction decoding task become bottleneck aggr essive use predecoding expected. predecoder partially ecodes instructions, effectively transforms original undecoded instructions format makes final decoding task easier. One view predecoder translating instructions fetched memory different instructions loaded I-cache. expand view, possibility enhancing predecoder run-time object code translation ISAs could interesting. 4.3.3 Instruction Dispatching Instruction dispatching necessary superscalar pipelines. scalar pipeline, instructions regardless type flow th rough single pipeline. Super- scalar pipelines diversified pipelines employ multiplicity heteroge- neous functional units execution portion. Different types instructions executed different functional units. type instruction identified decode stage, must routed appropriate functional unit execution; task instruction dispatching. Although superscalar pipelines parallel pipelines, instruction fetching instruction decoding tasks usually carried centralized fashion; i.e., instructions managed controller. Although multiple instructions fetched cycle, al l instructions must fetched I-cache. Hence instructions fetch group accessed T_rHI200 MODERN PROCESSOR DESIGN I-cache time, deposited buffer. Instruction decoding done centralized fashion case CISC instructions, bytes fetch group must decoded collectively centralized decoder order identify ind ividual instructions. Even RISC instruc- tions, decoder must identify inter-instruction dependences, also requires centralized instruction decoding. hand, diversified pipeline functional units operate independently distributed fashion executing types instructions inter-instruction dependences resolved. Consequently, going instruction decoding instruction execution, change centralized processing instructions distrib uted processing instructions. change carried by, reason for, instruction dispatching stage superscalar pipeline. llustrated Figure 4.16. Another mechanism necessary instruction decoding ins truc- tion execution temporary buffering instructions. Prior execution, instruction must operands. decoding, register operands fetched register files. superscalar pipeline possible operands yet ready earlier instructions update regis- ters finished execution . situation occurs, obvious solution stall decoding stage un til register operands ready. solution seri- ously restricts decoding throughput desirable. better lution fetch register operands ready go ahead advance 1 Instruction dispatching Figure 4.16 Necessity Instruction Dispatching Superscalar Pipeline.SUPERSCALAR ORGA NIZATION 201 ICentralized reservation station (dispatch buffer) P Figure 4.17 Centralized Reservation Station instructions separat e buffer await registe r operands ready. register operands ready, instructions exit buffer issued functional units execution. Borrowing term used Tomasulo's algorithm employed IBM 360/91 [Tomasulo, 1967], denote temporary instruction buffer reservation station. use reservation station deco uples instruction decoding instruction e xecution provides buffer take slack decoding execution stages due temporal variation throughput rates two st ages. eliminates unnecessary stalling decoding stage prevents unnecessary starvation execution stage. Based placement reservation station relative instruction dis- patching, two types reservation station implementations possible. single buffer used source side dispatching, identify centralized reservation station. hand , multiple buffers placed destina- tion side dispatching, identi fied distributed reservation stations. Figures 4.17 4.1 8 illustrate two ways implementing reservation stations. Intel Pen tium Pro implements centralized reservation station. implementation, one reservation station wit h many entries feeds functional units. Instructions dispatched centralized reservation station directly functional units begin execution. hand, PowerPC 620 employs distributed reservation stat ions. implementation, functional unit reservation station input side unit. Instructions dispatched individual reservation stations based instruction type. instructions remain reservation stations ready issued functiona l units execution. course, two implementations ofDispatch (issue) Execute « TJ Completion buffer!02 MODERN PROCESSOR DESIGN Dispatch j Dispatch buffer Issue]L~tD_ Distributed 1 reservation J stations J Execute « Finish Complete Figure 4.18 Distributed Reservation Stations.tJ Completion buffer reservation stations represent two e xtreme alternatives. Hybrids two approaches also possible. example, MIPS R10000 emp loys one hybrid implementation. iden tified hybrid implementations clus- tered reservation stations. clustered reser vation stations, instructions dis- patched multiple reservation stations, reservation station feed shared one functional unit. Typically reservation stations functional units clust ered based instruction data types. Reservation station design nvolves certain tradeoffs. centralized reserva- tion station allows instruction types share reservation station likely achieve best overall utilization reservation station entries. However, centralized implementation incur g reatest complexity hard- ware design. requires central ized control buffer highly multiported allow multiple concurrent accesses. Distri buted reservation stations singie- ported buffers, small number ent ries. However, reservation station's idling en tries cannot used instructions destined execution functional units. overall utilization reservation station entries lower. also likely one reservation station saturate entries occupied hence induce stalls instruction dispatching. different al ternatives implementing reservation stations, need clarify use certain terms. book term dispatching implies asso- ciating instruction types functional unit types instructions decoded. hand, term issuing always means initiation execution functional units. distributed reservation station design, two events occurSUPERSCALAR ORGANIZATION 203 separately. Instructions dispatched centralized decode/dispatch buffer indiv idual reservation stations first, operands av ailable, issued individual functional units execution. cen- tralized reservation station, disp atching instructions centralized reservation station occur operands rea dy. instructions, regardless type, held centralized reservation station ready execute, time instructions dispatched directly individual functional units begin execution. Hence, machine centralized reserva- tion station, associating instructions indi vidual functional units occurs time execution initiated. Therefore, centr alized reservation station, instruction dispatching instruction issuing occur time, two terms become interchangeable. illustrated Figure 4.17. 4.3.4 Instruction Execution instruction execution stage heart superscalar machine. Th e current trend supe rscalar pipeline design toward parallel di versified pipelines. transl ates functional units func- tional units specialized. specializing executing specific instruction types, functional units performance efficient. Early scalar pipelined processors essentially one functional unit. instruction types (excluding floating-point instructions executed separate floating- point copr ocessor chip) executed functional unit. TYP pipe- line example, functional unit two-stage pipelined unit consisting ALU MEM stages TYP pipeline. first-generation superscalar pro- cessors parallel pipelines two diversified functional units, one executing integer instructions th e executing floating-point instructions. early superscalar processors simply integrat ed floating-point execution instruction pipeline ins tead employing separate coprocessor unit. Current superscalar processors employ multiple integer units, multiple floatin g-point units. two fundamental functional unit types. units becoming quite sophisticated capable executing one operation involving tw source operands cycle. Figure 4.19(a) illustrates integer execution unit Tl SuperSPARC contains cascaded ALU configuration [Blanck Krueger, 1992]. Three ALUs included two-stage pipelined unit, two integer operations issued unit one cycle. inde- pendent, bot h operations executed first stage using ALUO ALU2. second operation depends first, first one executed ALU2 first stage th e second one executed ALUC sec- ond stage. Implementing functional unit allows cycles two instructions simultaneously sued. floating-point unit IBM RS/6000 implemented two-stage pipe- lined multiply-add-fused fMAF) unit takes three inputs (A, B, Q performs (AxB) + C. illustrated Figure 4.19(b). MAF unit mo tivated common use floating-point multiplication carry dot-product operation = {A xB) + C. compiler able merge many multiply-add204 MODERN PROCESSOR DESIGN ALU 2 ALUCAX B (AXB) + C | Round/Normalize (a) (b) Figure 4.19 (a) Integer Functional Unit theTI SuperSPARC; (b) Floating-Point Unit IBM RS/6000. EXAMIP E Ipairs instructions single MAF instructions, MAF unit sustain issuing one MAF instruction every cycle, effective thr oughput two floating-point instructions per cycle achieved using e MAF unit. normal floating-point multiply instruction actually executed MAF unit (A x B) + 0, floating-point add instruction performed MAF unit (A x 1) + C. Since MAF unit pipelined, even without executing MAF instruc- tions, still sustain executio n rate one flo ating-point instruction per cycle. addition executing integer ALU instructions, integer unit used generating memory addresses executing branch load/store instruc- tions. However, recent designs separate branch load/store units incorporated. branch unit responsible updating PC, load/store unit directly connected th e D-cache. specialized functional units emerged supporting graphics imag e processing applications. example, Motorola 88110 dedicated functional unit bit manipulation two functional units supporting pixel processing. many signal processing multimedia app lications, th e common data type byte. Frequently 4 bytes packed 32-bit word simultaneous pr ocessing specialized 32-bit functional units increased throughput. TriMedia VLIW processor intended applications, f unctional units employed [Slavenburg et al., 1996] . example, TriMedia-1 processor execute quadavg instruction one cycle. Th e quadavg instruction sums four rounded averages quite useful MPEG decoding decompressing compressed video images; carries following computation. quadavg = (fl + ;;+1> + (*+j+1Mc + f + (±t|±l> (4 .2)SUPERSCALAR ORGANIZATION 20 eight variables denote 8-byte operands a, b, c, stored one 32-bit quantity e,f, g, h stored another 32-bit qu antity. functional unit takes input two 32-bit operands produces quadavg result one cycle. single-cycle operation replaces numerous add divide instructions would required eight single-byte operands manipu- lated individually. widespread deployment multimedia applications, specialized functional units operate special data types emerged. the- best mix functional units superscalar pipeline inter- esting question. answer dependent application domain. use statistics Chap ter 2 typical programs 40% ALU instructions, 20% branches, 40% load/store instructions, 4-2-4 rule thumb. every four ALU units, two branch units four load/store units. Many current leading superscalar processors four ALU- type functional units (including integer floating-point units). one branch unit able speculate beyond one conditional branch instruction. However, processors one load/store unit; able process tw load/store instructions every cycle constraints. C learly seems imbalance load/store units. reason implementing multiple lo ad/store u nits operate par- allel accessing D-cache difficult task . requires D-cache multiported. Multiported memory modules involve complex circuit design significantly slow memory speed. many designs multiple memory banks used simulate truly multiported memory. memory partitioned multiple banks. bank perform read/write operation machine cycle. effective addresses two load/ store instructions happen reside different banks, instructions carried two diff erent banks th e time. However, bank conflict, two instructions must serialized. Multibanked D-caches used simulate multiported D-caches. example, Intel Pentium pro- cessor uses eight-banked D-cache simulate two-ported D-cache [Intel Corp., 1993]. truly multiported memory guarantee conflic t-free simulta- neous accesses. Typically, read ports write ports needed. Multiple read ports implemented multiple copies memory. memory writes broadcast copies, th e copies identical content. copy provide small number read ports total number read ports sum read ports copies. example, memory four read ports two write ports implemented two copies simpler memory modules, two write ports two read ports. Implementing multiple, especially two, load /store units operate parallel challenge designing wide superscalar pipelines. amount resource parallelism instruction execution portion determined combination spatial temporal parallelism. multi- ple functional units form spatial parallelism. Alternatively, parallelism obtained via pipelining functional units, form temporal,7' J206 MODERN PROCESSOR DESIGN parallelism. example, instead implementing dual-ported D-cache, current designs D-cache access pipelined two pipeline stages two load/store instructions concurrently serviced D-cache. C urrendy, general trend toward implementing deeper pipelines order reduce cycle time increase clock speed. Spatial parallelism also tends require greater hardware complexity silicon real estate. Temporal parallelism makes efficient use hardware increase overall instruction pro- cessing latency potentially pipeline stall penalties due inter-instruction dependences. real superscalar pipeline designs, often see total number functional units exceeds actual width parallel pipeline. ypically width superscalar pipeline determined number instructions fetched, decoded, comple ted every machine cycle. H owever, dynamic variation instruction mix resultant nonuniform distribu- tion instruction mix program execution cycle-by-cycle basis, potential dynamic mismatch instruction mix functional unit mix. former varies time latter stays fixed. Bec ause specialization heterogeneity functional units total number functional units must exceed width superscala r pipeline avoid inst ructio n execur tion portion become bottleneck due e xcessive structural dependences related unavailability certain functional unit types. aggressi com- piler back ends actually try smooth dynamic vari ation instruction mi* ensure bette r sustained match functional unit mix. course, differ- ent application programs exhibit different inherent overall mix instruction types. compiler make localized adjustments achieve perfor- mance gain. Studies done assessing best number mix func- tional units based SPEC benchmarks [Jourdan et al., 1995]. large number functional units, additional hardware complexity functional units themselves. Results outputs functional units need forwarded inputs functional units. multiplicity busses required, potentially logic bus control arb itration needed. Usually full cr ossbar interconnection network costly absolutely necessary. mechanism routing operands functional units introduces another form structural dependence. interconnect mechanism also contributes latency execution stage(s) pipeline. orde r support data forwarding reserva- tion station(s) must monitor busses g ^ matches, indicating availability needed operands, latch operands they^are broadcasted busses. Potentially complexity instruction execution stage grow rate n2, n total number functional units. 4.3.5 Instruction Completion Retiring instruction considered completed finishes execution updates machine state. instruction fini shes execu tion ex functional unit enters completion buffer. ubsequently exits completion buffer becomes completed. instruction finishes ex ecution, result may onlym reside nonarchitected buffers. However, completed, result written architecture register. instructions actually update memory loca- tions, time period architecturally completed memory locations updated. example, store instruction architecturally completed exits completion buffer enters store buffer wait availability bus cycle order write D-cache. store instruction considered retired exits store buffer updates D-cache. Hence, book instruction completion involves updating machine state , whe reas instruction retiring involves updating memory state. instructions update memory, retiring occurs time completion. So, distributed reservation station machine, instruction go following phases: fetch, decode, dispatch, issue, execute, finish, complete, retire. Issuing finishing simply refer starting execution ending execution, respectively. superscalar processor vendors use terms slightly different ways. Frequen tly, dispatching issuing used almost interchangeably, similar completion retiring. Sometimes completion used mean finishing exe cution, retiring used mean upd ating machine's architectural state. yet standardization use erms. execution program, interrupts exceptions occur disrupt execution flow prog ram. Superscalar processors emp loying dynamic pipelines faci litate out-of-order e xecution must able deal disruptions program execution. Interrupts usually induced external environ ment I/O devices operating system. occur asynchronous fashion respect program execution. interrupt occurs, program execution must suspended allow operating system service interrupt. One way stop fetching new instructions allow instructions already pipeline finish execution, time state machine saved. interrupt serviced operating system, saved machine tate r estored original program resume execution. Exceptions induced execution th e instructions pro gram. instruction induce exception due arithm etic operations, divid- ing zero floating-point overflow underflow. exceptions occur, results computation may longer valid operating sys- tem may need intervene log exceptions. Exceptions also occur due occurrence page faults paging-based virtual memory system. exceptions occur instructions reference memory. excep- tions occur, new page must brought secondary storage, require order thousands machine cycles. Consequently, execution program indu ced page fault usually suspended, execution new program initiated multiprogramming environment. page fault serviced, original program resume execution. important architectural state machine present time excepting instruction executed saved program resume execu- tion exception serviced. Machines c apable supporting thisSUPERSCALAR ORGANIZATION 207:08 MODERN PROCESSOR DESIGN suspension resumption execution program granularity individual instruction said precise exception. Precise exception involves able checkpoint tate machine prior execu- tion excepting instruction resume execution storing check-: pointed state restarting execution excepting instruction. order to- support precise exception, superscalar processor must maintain architec- tural state evolve machine state way instructions the- program executed one time according original program order. reason exception occurs, state machine time must reflect condition instructions preceding excepting instruction completed instructions following e xcepting instruction completed. dynamic pipeline precise exception, sequential evolving arch itectural state must maintained even though instructions actually executed program order. dynamic pipeline, instructions fetched decoded program order executed program order. Essentially, instructions enter res- ervation station(s) order exit reservation station(s) order. also finish execution order. support precise exception, instruction com- pletion must occur program order update architectural state machine program order. order accommodate out-of-order finishing exe-  cution in-order completion instructions, reorder buffer needed instruction completion stage parallel pipeline. instructions finish execu- tion, enter reorder buffer order, exit reorder buffer program order. exit reorder buffer, considered architec- turally completed. illus trated Figure 4.20 reservation station reorder buffer bounding out-of-order region pipeline essen- tially instruction execution portion pipeline. terms adopted book, referring various phases instruction processing, illustrated Figure 4.20. Precise exception handled instruction completion stage using reorder buffer. exception occurs, excepting instruction tagged reorder buffer. completion stage checks instruction instruction completed. tagged instruction detected, allowed completed. instructions prior tagged instructions allowed completed. machine state checkpointed saved. machine tate includes architected gisters program counter. remaining instructions pipeline, may already finished execution, discarded. exception serviced, checkpointed machine state restored execution resumes fetching instruction triggered original exception. Early work support providing precise exce ptions processor supports out-of-order execution conducted Acosta et al. [1986], Sohi Vajapeyam [1987], Smith Pleszkun [1988]. early proposal describing Metaflow processor, never completed, also provides interesting insights curious reader [Popescu et al., 1991].SUPERSCALAR ORGANIZATION 209 Fetch Decode Issue g Execute - FinishDispatch Complete RetireInstruction/decode buffer ] |J Dispatch buffer LmixncmiXD iI Reservation stations J Reorder/completion buffer Store buffer Figure 4.20 Dynamic Pipeline Reservation Station Reorder Buffer. 4.4 Summary Figure 4.20 represents archetype contemporary out-of-order superscalar pipeline. in-order front end, out-of-order execution core, in-order back end. fron t-end bac k-end pipeline stages advance multiple instructions per machine c ycle. Instructions remain reservation stations one cycles waiting th eir source operands. source operands available, instruction issued th e reservation st ation execution unit execution, instruction enters reorder buffer (or comple- tion buffer). Instructions reorder buffer completed according program210 MODERN PROCESSOR DESIGN order. fact reorder buffer managed circular queue instructions arranged according program order. chapter focuses superscalar pipeline organization highlights issues associated various pipeline stages. far, addr essed mostly static structures superscalar pipelines. Chapter 5 get dynamic behavior superscalar processors. chosen presen superscalar pipeline organization fairly high level, avoiding implementation details. main purpose chapter provide bridge scalar superscalar pipelines convey high-level framework superscalar pipelines useful navigational aid get plethora supersc alar processor techniques Chapter 5. REFERENCES Acosta, R., J. Kilestrup, H. Torng: "An instruction issuing approach enhancing per- formance multiple functional uni processors," IEEE Trans, Computers, C35,9,1986, pp. 815-828. Blanck, G., S. Krueger: "Th e SuperSPARC microprocessor," Proc. IEEE COMPCON, 1992, pp. 136-141. Crawford, J.: "The execution pipeline Intel i486 CPU," Proc. COMPCON Spring'90, 1990, pp. 254-258. Diefendorf, K., M. Allen: "Organization Motorola 88110 superscalar RISC microprocessor." IEEE MICRO. 12, 2,1992, pp. 40-63. Grohoski, G.: "Machine organization th e IBM RISC System/6000 processor," IBM Journal Research Development. 34,1,1990, pp. 37-58. Intel Corp.: Pentium Processor User's Manual, Vol. 3: Architecture Programming Manual. Santa Clara. CA: Intel Corp., 1993. Jourdan. S., P. Sainrat, D. Litaize: "Exploring configurations functional units out-of-order superscalar processor," Proc. 22nd Annual Int. Symposium Computer Architecture, 1995, pp. 117-125. Oehler, R. R., R. D. Groves: "IBM RISC System/6000 processor architecture," IBM Journal Research Development, 34,1,1990, pp. 23-36. Patt, Y., W. Hwu, M. Shebanow: "HPS, new microarchitecture: Introduction rationale," Proc. I8lh Annual Workshop Microprogramming (MICRO-18), 1985, pp. 103-108. Popescu, V., M. Schulz, J. Spracklen, G. Gibson, B. Lightner, D. Isaman: "The Meta- flow architecture," IEEE Micro., June 1991, pp. 10-13,63-73. Slavenburg, G., S. Rathnam, H. Dijkstra: "The TriMedia TM-1 PCI VLIW media pro- cessor," Proc. Hot Chips 8, 1996, pp. 171-178. Smith, J., A. Pleszkun: "Implementing precise int errupts pipelined processors," IEEE Trans, Computers, 37, 5, 1988, pp. 562-573. Sohi. G. S. Vajapeyam: "Instruction issue logic high-performance, interruptible pipelined processors," Proc. 14th Annual Int. Symposium Computer Architecture, 1987, pp. 27-34.SUPERSCALAR ORGANIZATION 211 Thornton, J. E.: "Parallel operation Control Data 6600," AFIPS Proc. FJCC part 2 vol. 26,1964, pp. 33-40. Tomasulo, R.: "An efficient algorithm exploiting multiple arithmetic units," IBM Journal Research Development, 11,1967, pp. 25-33. HOMEWORK PROBLEMS P4.1 reasonable build scalar pipeline supports out-of-order exe- cution? so, describe code execution scenario pipeline would perform better conventional in-order scalar pipeline. P4.2 Superscalar pipelines require replication pipeline resources across parallel pipeline, naively including replication cache ports. practice, however, two-wide superscalar pipeline may two data cache ports single instruction cache port. Explain po ssible, also discuss single instruction cache port perform worse tw (replicated) instructio n cache ports. P4.3 Section 4.3.1 suggests compiler generate object code branch targets aligned beginning physical cache lines increase likelihood fetching multiple instructions branch target single cycle. However, given fixed number instructions take n branches, approach may simply shift unused fetch slots branch target branch terminates sequential fetch target. example, moving c e label 0 align physical cache line improve fetch efficiency, since wasted fetch slot shifts beginning physical line end. Original code: cond. labelO labelO: add r l , r2. r3cmp cone), r l , r5 cond, label 1 p z e code:Physical cache line * '  add cmp ^ 1 *- . Wasted slot Physical cache line be... add cmp 1 Wasted slot Discuss relationship fetch block size dynamic distance taken branches. Describe one affects other, describe important branch target alignment small vs. large fetch blocks short vs. long dynamic distance, describe well static compil er-based target alignment might work cases.PROCESSOR DESIGN P4.4 auto-realigning instruction fetch hardware shown Figure 4.13 still fails achieve full-width fetch bandwidth (i.e., four instructions per cycle). Describe aggressive organization always able fetch four instructions per cycle. Comment additional hard- ware organization implies. P4.5 One idea eliminate branch mis prediction penalty build machine executes paths branch. two three pa ragraph essay, explain may may good idea. P4.6 Section 4.3.2 discusses adding predecode bits instruction cache simplify task decoding instructions fetched. logical extension predecode bits simply store instructions decoded form decoded instruction cache; par- ticularly attractive processors li ke Pentium Pro dynamically translate fetched instructions sequence simpler RISC-like instructions core execute. Identify describe least one factor complicates building decoded instruction caches processors translate complex ins truction set simpler RISC-like nstruction set. P4.7 important advantage centralized reservation station distributed reservation stations? P4.8 in-order pipelined processor, pipeline latches used hold result operands time execution unit computes written back register file write back stage. out-of-orde r processor, rename registers used pur- pose. Given four-wide out-of-order proce ssor TYP pipeline, compute minimum number rename registers needed prevent rename register starvation limiting concurrency. happens number frequency emands force designer add five extra pipeline stages dispatch execute, five stages execute retir e/writeback? P4.9 banked interieaved cache effective approach allowing multiple l oads stores p erformed one cycle. Sketch data flow two-way interleaved data cache attached two load/ store units. sketch data flow eight-way interleaved data cache attached four load /store units. Comment well inter- leaving scales doe scale. P4.10 Pentium 4 processor operates integer arithmetic units double nominal clock frequency rest processor. accomplished pipelinin g integer adder two stages, comput- ing low-order 16 bits first cycle high-order 16 bits second cycle. Naively, appears increase ALU latency one cycle two cycles. However, assuming two dependentSUPERSCALAR ORGANIZATION 213 instructions arithmetic instructions, possible issue second instruction cycle immediately following issue first instruction, ince low-order bits second instruction dependent low-order bits first instruction. Sketch pipeline diagram ALU along additional bypass paths needed handle optimized case. P4.ll Given ALU configuration described Problem 4.10, specify many cycles trailing dependent instruction following types ust delay, following issue leading arithmetic instruction: arithmetic, log ical (and/or/xor), shift left, shift right. P4.12 Explain kind bit-slice pipelining described Problem 4.10 cannot usefully employed pipeline dep endent flo ating-point arith- metic instructions. P4.13 Assume four-wide superscalar processor attempts retire four instructions per cycle reorder buffer. Explain data dependences need checked time, sketch th e dependence- checking hardware. P4.14 Four-wide superscalar processors rarely sustain throughput much greater one instruction per cycle (IPC). Despite fact, e xplain four-wide retirement still useful processor. P4.15 general-purpose instruction sets recently added multimedia extensions support vector-like operations arrays small data types. example, Intel IA32 added MMX SSE instruction set extensions purpose. single multimedia instruction load, say, eight 8-bit operands 64-bit register parallel, arit hmetic instructions perform operation eight operands single-instruction, multiple data (SIMD) fashion. Describe changes would make fetch, decode, dispatch, issue, execute, retire logic typical superscalar processor accommodate instructions. P4.16 PowerPC instruction set provides support fused floating- point multiply-add op eration multiplies two input r egisters adds product third input register. Explain addition instruction complicates decode, dispatch, issue, exe- cute stages typical superscalar processor. effect think changes processor's cycle time? P4.17 semantics fused multiply-add instruction described Problem 4.16 mimicked issuing separate floating-point add floating-point multiply whenever instruction decoded. fact, MIPS R10000 that; rather supporting instruction (which also exists MIPS instruction set) directly, the214 MODERN PROCESSOR DESIGN decoder simply inserts add/multiply instruction pair execution window. Identify discuss least two r easons approach could reduce performance measured instructions per cycle. P4.18 ALU mix Motorola 8811 0 processor sh Figure 4.7 agree IBM instruction mix provided Section 2.2.4.3? not, would yo u change ALU mix? Terms Buzzwords problems similar "Jeopardy Game" TV. "answers" given provide best co rrect "que stions." "answer" may one appropriate "question"; ne ed provide th e best one. P4.19 A: mechanism tracks out-of-order execution maintains sp ecu- lative machine state. Q: is. P4.20 A: significan tly reduce machine cycle time, increase branch penalty. Q: is. P4.21 A: Add itional I-cache bits generated cache refill time ease decoding/dispatching task. Q: ? P4.22 A: program attribute causes inefficiencies su perscalar fetch unit. Q: ? P4.23 A: internal RISC-like instruction executed Pentium Pro (P6) microarchitecture. Q: ? P4.24 A: l ogical pipeline stage assigns instruction appropri- ate execution unit. Q: ? P4.25 A: early processor design incorporated 10 diverse functional units. Q: ? P4.26 A: new instruction allows scalar pipeline achieve one floating-point operation per cycle. Q: ? SUPERSCALAR ORGANIZATION 215 P4.27 A: effective technique allowing one memory operation performed per cycle. Q: ? P4.28 A: useful architectural property simplifies task writing low-level operating system code. Q: ? P4.29 A: first research paper describe run-time, hardware translation one instruction set another, simpler one. Q: ? P4.30 A: first real processor implement run-time, hardware translation one instruction set another, simpler one. Q: ? P4.31 A: attribute RISC instruction sets substan tially simplifies task decoding multiple instructions parallel. Q: ? CHAPTER Superscalar Techniques CHAPTER OUTLINE 5.1 5.25.35.4Instruction Flow Techniques Register Data Flow Techniques Memory Data Flow Techniques Summary References Hornework Problems* Chapter 4 focused structural, organizational, design super- scalar pipeline dealt issues somewhat independent specific types instructions processed. thi chapter focus dynamic behavior superscalar processor consider techniques deal spe cific ypes instructions. ultimate performance goal superscalar pipeline achieve maximum throughput instruction processing. convenient view instruction processing involving three component flows instructions and/or data, namely, instruction flow, register dataflow, memory dataflow. partitioning three flow paths similar used Mike Johnson's 1991 textbook entitled Superscalar Microprocessor Design [Johnson, 1991]. overall performance objective maximize volumes three flow paths. course, makes task interesting three flow paths independent interactions quite complex. chapter classifies presents superscalar microarchitecture echniques based association three flow paths. three flow paths correspond roughly proc essing three major types instructions, namely, branch, ALU, load /store instructions. Conse- quently, maximizing throughput th e three flow paths corresponds minimizing branch, ALU, load pen alties. 217218 MODERN PROCESSOR DESIGN 1. Instruction flow. Branch instruction processing. 2. Register dataflow. ALU instruction processing. 3. Memory dataflow. Load/store instruction processing. chapter uses three flow paths convenient framework fo r presenting plethora microarchitecture techniques optimizing performance modern superscalar processors. 5.1 Instruction Flow Techniques present instruction flow techniques first deal early stages, e.g., fetch decode stages, superscalar pipeline . throughput early pipeline stages impose upper bound throughput subsequent stages. contemporary pipelined processors, traditional partition- ing processor control path data path longer clear effective. Nevertheless, early pipeline stages along branch execution unit viewed corresponding traditional control path wh ose primary function enforce control flow semantics program. primary goal instruc- tion flow techniques maximize supply instructions superscalar pipeline subject requirements control flow semantics program. 5.1.1 Program Control Flow Control Dependences control flow semantics program specified form control flow graph (CFG), nodes represent basic blocks edges repre- sent transfe r control flow basic blocks. Figure 5.1(a) illustrates CFG four basic blocks (dashed-line rectangles), containing number instructions (ovals). directed edges represent control flows basic blocks. edges induced conditional branch instructions (diamonds). run-time execution program entails dynamic traversal nodes edges CFG. actual path traversal dictated branch instruc- tions branch condition dependent ru n-time data. basic blocks, constituent instructions, CFG must stored sequential locations program memory. Hence partial ordered basic blocks CFG must arranged total order program memory. map- ping CFG linear consecutive memory locations, additiona l unconditional branch instructions must added, illus trated Figure 5.1(b). mapping CFG linear program memory fac ilitates implied sequentia l flow control along sequential memory locations program execution. However, encounter conditional unconditional branches ru n time induces deviations implied sequential control flow consequent disrup- tions sequential fetching instructions. Suc h disruptions cause stalls instruction fetch stage pipeline reduc e overall instruction fetching bandwidth. ubroutine jump return instructions also induce similar disrup- tions sequentia l fetching instructions.SUPERSCALAR TECHNIQUES 21 (a) (b) Figure 5.1 Program Control Flow: (a) Control R w Graph (CFG); (b) Mapping CFG equential Memory Locations. 5.1.2 Performance Degradation Due Branches pipelined machine achieves maximum throughput streaming mode. fetch stage, streaming mode implies continuous fetching instructions sequential locations program memory. Whenever control flow program deviates sequential path, potential disruption streaming mode occur. unconditional bran ches, subse quent instructions cannot fetched.until th e target address branch de termined. condi- tional branches, machine must wait resolution branch condition, branch taken, must wait target address avail- able. Figure 5.2 illustrates disruption streaming mode branch instruc- tions. Branch instructions executed branch functional unit. conditional branch, exits branch unit branch condition branch target address known fetch stage correctly fetch next instruction. Figure 5.2 illustrates, delay pr ocessing conditional branches incurs penalty three cycles fetching next instruction, corresponding tra- versal decode, dispatch, execute stages conditional branch. actual lost-opportunity cost three stalled cycles three empty instruction220 MODERN PROCESSOR DESIGN Issue - - Branch Execute FinishFetch Decode Dispatch CompleteDecode buffer Dispatch buffer iLXDLXaLXDCxa| Reservation stations J Completion buffer Store buffer Retire Figure 5.2 Disruption Seq uential Control Flo w Branch Instructions. slots scalar pipeline, number empty instruction slots must multiplied width machine. example, four-wide machine total penalty 12 instruction "bubbles" supersc alar pipeline. Also recall Chapter 1, tha pipeline stall cycles effectively correspond sequential bottleneck Amdahl's law rapidly significantly reduce actual performance potential peak performance. conditional branches, actual number talled penalty cycles dictated eithe r target address generation condition resolution. Figure 5.3 illus- trates potential cycles incurred target address generation. TheSUPERSCALAR TECHNIQUES 22 Register indirect offsetRegister IndirectFetch PC- relative Issue Branch Execute FinishDecode X Dispatch Figure 5.3 Branch Target Address Generation Penalties.Decode buffer Dispatch bu ffer Reservation stations J Completion buffer Complete T7 Store buffer Retire | actual number penalty cycles etermined addr essing modes branch instructions. PC-relative ddressing mode, th e branch target address generated fetch stage, resulting penalty one cycle. register indirect addressing mode used, branch instruction must traverse decode stage access register. case two-cycle penalty incurred. register indirect offset addressing mode, offset must added register access total three-cycle penalty result unconditional branches, penalty due target address generation concern. c onditional branches, branch condition resolution latency must also considered.222 MODERN PROCESSOR DESIGN GP register value comparisonCC register Issue Branch Execute FinishFetch Decode DispatchDecode buffer Dispatch buffer IReservation stations "J Completion buffer Complete Store buffer Figure 5.4 Branch Condition Resolution Penalties.Retire Different methods performing condition resolution also lead different penalties. Figure 5.4 illustrates two possible penalties . condition code registers used, assuming relevant condition code register accessed dispatch stage, penalty two cycles result. ISA permits comparison two g eneral-purpose regi sters generate branch condition, one cycle needed perform ALU operation contents two registers. result penalty three cycles. conditional branch, depending addr essing mode condition resolution method used, either one penalties may critical one. example, even PC-relativeSUPERSCALAR ECHNIQUES 223 addressing mode used, conditional branch must access condition code register still incur two-cycle penalty instead one-cycle penalty target address generation. Maximizing volume instruction flow path equival ent maximiz- ing sustained instruction fetch bandwidth. this, number stall cycles fetch stage must minimized. Recall total lost-opportunity cost equal product number penalty cycles width machine. n-w ide machine stalled cycle equal fetching n no-op instructions primary aim instruction flow techniques minimize number fetch stall cycles and/or make use cycles poten- tially useful work. current ominant approach accomplishing via branch prediction subject Section 5.1.3. 5.1.3 Br anch Prediction Techniques Experimental studies shown behavior branc h instructions highly predictable. key approach minimizing branch penalt maximizing instruction flow throughput speculate branch target addresses branch conditi ons branch instructions. static branch instruction repeatedly executed run time, dynamic behavior tracked. Based past behavior, future behavior effectively predicted. Two fundamental components branch predictio n branch target speculation branch condition speculation. speculative technique, must mechanisms validate predic- tion safely recover mispredictions. Branch misprediction recovery covered Section 5.1.4. Branch target speculation involves use branch target buffer (BTB) store previous branch target addresses. BTB small cache memory accessed instruction fetch stage using instruction fetch address (PC). entry BTB contains two fields: branch instruction address (BIA) branch target address (BTA). static branch instruction executed first time, entry BTB allocated it. inst ruction address stored BIA field, target address stored BTA field. Assuming BTB fully associative cache, BIA field used associative access BTB. BTB accessed concurrently accessing I-cache. current PC matches BIA entry BTB, hit BTB results. implies current instruction fetched I-cache executed branch instruction. hit BTB occurs, BTA field hit entry accessed used next instruction fetch address partic ular branch instruction predicted taken; see Figure 5.5. accessing BTB using branch instruction address retrieving branch target address BTB fetch stage, speculative branch target address ready used next machine cycle new instruction fetch address branch instruction predicted taken. branch instruc tion predicted taken prediction turns cor- rect, branch instruction effectively executed fetch stage, incurring branch penalty. nonspeculative execution branch instruction still224 MODERN PROCESSOR DESIGN Access 1-cache Access BTB PC (instruction fetch address) +Branch target buffer (BTB) Branch target address (BTA) fieldBranch ins truction address (BIA) field BIA BTA Speculative . target address (Used new PC branch predicted taken) Figure 5.5 Branch Target Speculation Using Branch Target Buffer. 1LYperformed purpose validating speculativ e execution. branch instruction still fetched I-cache executed. resultant target address branch condition compared th e speculative version. agree, correc prediction made; otherwise, misprediction occurred recovery must initiated. result nspeculative execution also used update c ontent, i.e., BTA field, BTB. number ways branch condition speculation. simplest form design fetch hardware biased taken, i.e., always pre- dict taken. branch instruction encou ntered, prior resolution, fetch stage continues fetching fall-through path without stalling. form minimal branch prediction easy implement eff ective. example, many branches used loop closing instructions, mostly taken execution except exiting loops. Another form predic- tion employs software support require ISA changes. example, extra bit allocated branch instruction format set compiler. bit used hint hardware perform either predict taken pre- dict taken depending value bit. compiler use branch instruc- tion type profiling information det ermine appropriate value bit. allows static branch instruction specified prediction. However, prediction static sense prediction used dynamic executions branch. static software prediction technique used Motorola 88110 [Diefendorf Allen, 1992]. aggressive dynamic form prediction makes prediction based branch target address offset. form prediction first determines relative offset address branch instruction address target instruction. posi- tive offset trigger hardware predict taken, whereas negative offset, likely indicating loop closing branch, trigger hardware predict taken. branch offset-based technique used original IBM RS/6000 design adopted machines well [Grohoski, 1990; Oehler Groves, 1990]. common branch condition speculation techniqueSUPERSCALAR TECHNIQUES 22 Actual direction resolved branch Predicted direction fetched branch Figure 5.6 FSM Model History-Baser} Branch Direction Predictors. employed contemporary superscalar machines based history previ- ous branch executions. History-based branch prediction makes prediction branch direction, whether taken (T) taken (N), based previously observed branch direc- tions. approach first proposed Jim Smith, patented technique behalf employer, Control Data, later published important early study [Smith, 1981]. assumption historica l information direc tion 'that static branch takes previous executions give hel pful hints direction likely take future executions. Design decisions type branch prediction include much history tracked observed history pattern prediction made. specific algo- rithm history-based branch direction prediction characterized finite state machine (FSM); see Figure 5.6. n state variables encode directions taken last n executions branch. Henc e state represents partic- ular history pattern terms sequence takens takens. output logic generates prediction based current state FSM. Essentially, prediction made based outcome previous n executions branch. predicted branch finally executed, actual outcome used input FSM trigger state transition. next state logic trivial; simply involves chaining state variables shift register, records branch directions previous n executions branch instruction. Figure 5.7(a) illustrates FSM diagram typical 2-bit branch p redictor employs two hi story bits track outcome two previous ex ecutions branch. two history bits constitute stat e variables FSM. pre- dictor one four states: NN, NT, TT, TN, representing directions taken previous two executions branch. NN state esig- nated initial state. output value either N associ ated four states representing prediction would made predictor state. Whe n branch executed, actual direction taken used input FSM, state transition occurs update branch history used next pred iction. particular algorithm implemented predictor Figure 5.7(a) biased toward predicting branches taken; note three th e four states226 MODERN PROCESSOR DESIGN Branch history Predicted " \ direction Actual direction (a)I-cache BTBBranch instruction address field PCBranch target address field Predict taken _ taken (b)Branch history BIA BTA Speci lative Figure 5.7 History-Based Branch P rediction: (a) 2-Bit Branch Predictor Algorithm; (b) Branch Target Buffer Additional Field Storing Branch History Bits. predict branch taken. anticipates either long runs N's (in NN state) long runs T's (in state). long least one two previ- ous executions taken branch, predict next execut ion taken. prediction switched aken encountered two consecutive N's row. represents one particular branch direction prediction algorithm; clearl many possible designs history-based predictors, many designs en evaluated res earchers. support history-based branch direction predictors, BTB aug- mented includ e history field entries. width, number bits, field determined number h istory bits tracked. PC address hits BTB, addition speculativ e target address, history bits retrieved. hist ory bits fed logic implements next- state output functions branch predictor FSM. retrieved history bits used state variables FSM. Based history bits, output logic produces 1-bit output indicates predicted direction. predic- tion taken branch, output used steer speculative target address PC used new instruction fetch address next machine cycle. prediction turns correct, effectively branch instruction executed fetch stage without incurring penalty stalled cycle. classic experimental study branch prediction done Lee Smith [1984]. study, 26 programs six ifferent types workloads three different machines (IBM 370, DEC PDP-11, CDC 6400) used. Averaged across benchmarks, 67.6% branches taken 32.4% taken. Branches tend taken taken ratio 2 1. Withstatic branch prediction based op-code type, prediction accuracy ranged 55% 80% six workloads. Using 1 bit hist ory, history-based dynamic branch prediction achieved prediction accuracies ranging 79.7% toSUPERSCALAR TECHNIQUES 227 96.5%. 2 history bits, accuracies six workloads ranged 83.4% 97.5%. Continued increase number history bits brought addi- tional incremental accuracy. However, beyond four history bits minimal increase prediction accuracy. implemented four-way set associative BTB 128 sets. averaged BTB hit rate 86.5%. Com- bining prediction accuracy BTB hit rate, th e resultant average prediction effectiveness approximately 80%. Another experimental study done 1992 IBM Ravi Nair using RS/6000 architecture Systems Performance E valuation Cooperative (SPEC) benchmarks [Nair, 1992]. co mprehensive study possible branch prediction algorithms. goal branch prediction overlap execution branch instructions instructions achieve zero-cycle branches accomplish branch folding; i.e., branches folded critical latency path instruction execution. study performed exhaustiv e search optimal 2-bit predictors. 220 possible FSMs 2-bit predictors. Nair determined many hese machines ar e uninteresting pruned entire design space 5248 machines. Extensive simulations performed determine th e optimal (achieves b est prediction accuracy) 2-bit predictor benchmarks. list SPEC benchmarks, best prediction accu- racies, associated optimal predictors shown Figure 5.8. Figure 5.8, states denoted bold circles represent states branch predicted taken; nonbold circles represent states predict taken. Similarly bold edges represent state transitions branch actu- ally taken; nonbold edges represent transitions corresponding branch Benchmark Optimal "Counter" spice2g6 97.2 97.0 '^-^^^^^^^^^O*15 doduc 94.3 94.3 <~^(*X~^X2)* *Q« * gcc 89 .i 89 .i <=^yr^x-xyr*xy^ espresso 89.1 89.1 C=Kl)<~"">"0« * Q < - O^""* Initial state ( ^ ) Predict NT Predict Figure 5.8 Optimal 2-Bit Branch Predictors Six SPEC Benchmarks Nair Studyli 87.1 86.8 eqntott 87.9 87.2(8 MODERN PROCESSOR DESIGN actually taken. state denoted asterisk ind icates initial state. prediction accuracies optimal predictors six benchmarks range 87.1 % 97.2%. Notice optimal predictors doduc, gcc,-and espresso identical (disregarding different initial state gcc predictor) exhibit behavior 2-bit up/down saturating counter. label four states left right 0, 1, 2, 3, representing four count values 2-bit counter. Whenever branch resolved taken, count incremented; decre- mented otherwise. two lower-count states predict branch aken, two higher-count states predict branch taken. Figure 5.8 also pro- vides prediction accuracies six benchmarks 2-bit saturating counter predictor used six benchmarks. prediction accuracie spice2g6, li, eqntott decreas e rnirdmally optimal values, indicating 2-bit saturating counter good candidate general use benchmarks. fact, 2-bit saturating counter, originally invented Jim Smith, become popular prediction algorithm real experimental designs. study Nair also investigated effectiveness counter- based predictors. 1-bit counter pre dictor, i.e., remembering direction taken last time predicting direction next time, prediction accuracies ranged 82.5% 96.2%. seen Figure 5.8, 2-bit counter yields accuracy range 86.8% 97.0%. 3-bit counter used, increase accuracy minimal; accuracies range 88.3% 97.0%. Based study, 2-bit saturating counter appears good choice history-based predictor. Direct-mapped branch history tables assumed study. programs, gcc, 7000 conditional branches, programs, branch penalty due aliasing finite-sized branch history tables levels 1024 entries table size. 5.1.4 Branch Misprediction covery Branch prediction speculative technique. spec ulative technique requires mechanisms validating speculation . Dynamic branch prediction viewed consisting two interacting engines. leading engine pe rforms speculation front-end stages pipeline, trailing engine performs validation later stages pipeline. case mispre diction trailing engine also performs reco very. two aspects branch prediction illus- trated Figure 5.9. Branch speculation involves predicting irection branch pro- ceeding fetch along predicted path control flow. fetching predicted path, additional branch instructions may encountered. Prediction additional branches similarly performed, potentially resulting spec- ulating past multiple conditional branches first speculated branch resolved. Figure 5.9(a) illustrates speculating past three branches first third branches predicted taken second one predicted taken. occurs, instructions three speculative basic blocks resident machine must appro priately identified. Instructions speculative basic block given identifying tag. th e example ofSUPERSCALAR TECHNIQUES 229 Figure 5.9 Two Aspects Br anch Prediction: (a) Branch Speculation; (b) Branch Validation/Recovery. Figure 5.9(a), three distinct tags used identify instructions three speculative basic blocks. tagged instruction indicates speculative instruction, value tag identifies basic block belongs to. speculative instruction advances pipeline stages, tag also carried along. speculating, instruction addresses speculated branch instructions (or next sequential instructions) must buffered event recovery required. Branch validation occurs branch executed actual di rection branch resolved. correctness earlier prediction deter- mined. prediction turns correct, speculation tag deallocated instructions associated tag become nonspeculative allowed complete. misprediction detected, two actions required; namely, incorrect path must terminated, fetching new correct path must initiated. initiate new path, PC must updated new instruction fetch address. incorrect prediction not-taken prediction, PC updated computed branch target address. incorrect prediction taken prediction, PC updated sequential (fall- through) instruction address, obtained previously buffered instruction ddress branch predicted taken. PC updated, fetching instructions resumes along new path, branch predic- tion begins anew. terminate incorrect path, speculation tags ar e used. tags associated mispredicted branch used identify the30 MODERN PROCESSOR DESIGN instructions must eliminated. instructions still decode dispatch buffers well reservation station entries invalidated. Reorder buffer entries occupied thes e instructions deallocated. Figure 5.9(b) illustrates validation/recovery task second three predictions incorrect. first branch correctly predicted, therefore instructions Tag 1 become nonspeculative allowe complete. second prediction is.incorrect, instructions w ith Tag 2 Tag 3 must invalidated reorder buffer entries must deallocated. fetching correct path, branch prediction begin again, Tag 1 used denote instructions firs speculative basi c block. branch validation, associated BTB entry also updated. use PowerPC 604 superscalar microprocessor illustrate implementation dynamic branch prediction real sup erscalar processor. PowerPC 604 four-wide superscalar capable fetching, decoding, dispatch- ing four instructions every mac hine cycle [IB Corp., 1994]. Instead single unified BTB, PowerPC 604 employs two separate buffers support branch prediction, namely, branch target add ress cache (BTAC) branch history table (BHT); see Figure 5.10. BTAC 64-entry fully associative cache stores branch target addresses, BHT, 512-entry direct- mapped table, stores history bits branches. reason separation become clear shortly. BTAC BHT accesse fetch stage using current instruction fetch address PC. BTAC responds one cycle; however, BHT requires two cycles complete access. hit occurs BTAC, indicating presence branch instruction current fetch group, predict taken occurs branch target address retrieved BTAC used next fetch cycle. Since PowerPC 604 fetches four instructions fetch cycle, multiple branches fetch group. Hence, BTAC entry indexed fetch address contains branch target address first branch instruction fetch group predicted taken. second cycle, decode stage, history bit retrieved BHT used generate history-based prediction branch. prediction agrees taken prediction made BTAC, earlier prediction allowed stand. hand , BHT pr ediction disagrees th e BTAC prediction, BTAC prediction annulled fetching fall-through path, corre- sponding predict taken, initiated. essence, BHT prediction over- rule BTAC prediction. expected, cases two predictions agree. cases, BHT corrects wrong prediction made BTAC. possi- ble, however, BHT erroneously change correct prediction BTAC; occurs infrequently. branch resolved, BHT updated; based updated content BHT turn updates BTAC either leaving entry BTAC predicted taken next time, deleting entry BTAC branch predicted taken next time. PowerPC 604 four entries reservation station feeds branch execution unit. Hence, speculate past four branches; i.e., thereSUPERSCALAR TECHNIQUES 23 5 9 2soFA I-cache FA +4Branch history Holt (BHT) BHT prediction BTAC predictionFA Branch target add rem cache (BTAC) BHT updateBTAC update 1T Decode buffer Decode 1 I1 Dispatch buffer Issue ExecuteBRN SFXDispatch Reservation stations SFX cp cp cd rt] CFX FPU LS Finish-_1ICompietioi buffer Figure 5.10 Branch Prediction PowerPC 604 Superscalar Microprocessor. maximu four speculative branches present machine. denote four speculative basic blocks involved, 2-bit tag used identify specu anve instructions. branch resolves, branch validation takes pTaceId Z IT\ TmCtT dther ™ speculative invalidated vtthe "e deaUocatS ^ "ff * ******** icnonsare deallocated. Again, performed using 2-bit tag. 5.1.5 Advanced Branch Prediction Tec hniques dynamic branch p rediction schemes dis cussed thus far number limi- UUions Prediction branch made based limited historToTonry Z particular static branch instruction. actual prediction algorithm no, S!32 MODERN P ROCESSOR DESIGN account dynamic context within branch executed. example, make use information particular control flow path taken arriving branch. Furthermore fixed algorithm used make prediction regardless dynamic context. observed experimentally behavior certain branches strongly correlated behavior branches precede execution. Consequendy accurate branch prediction achieved algorithms take account branch histor correlated branches adapt pre- diction algorithm dynamic branching context. 1991, Yeh Part proposed two- level adaptive branch prediction technique potentially achieve better 95% prediction accuracy highly flexible prediction algorithm adapt changing dynamic contexts [Yeh Part, 1991]. previous schemes, single branch history table used indexed branch address. branch address one relevant entry branch history table. two-leve l adaptive scheme, set history tables used. identified pattern history table (PHT); see Figure 5.11. branch address indexes set relevant entries; e entries selected based dynamic branching context. context determined specific partem recendy executed branches stored branch history shift register (BHSR); see Figure 5.11. content BHSR used index PHT select one relevant entries. content entry used state predic- tion algorithm FSM produce prediction. branch resolved, branch result used update BHSR selected entry PHT. two-level adaptive branch prediction technique actually specifies frame- work within many possible designs implemented. two options Pattern histor table (PHT) Branch instruction address 00.. .00 Branch history shift register (BHSR) 00...01 (Shift left whe n update) 00...10 1 01 1 1 1 Figure 5.11 Two-Level Adaptive Branch Prediction Yeh Pan. Source: Yeh Part, 1991.SUPERSCALAR TECHNIQUES 233 implementing BHSR: global (G) individual (P). global implemen- tation employs single BHSR k bits tracks branch directions last k dynamic branch instructions pro gram execution. involve number (1 k) stati c branch instructions. individu al (called per-branch Yeh Part) implementation employs set it-bit BHSRs illustrated Figure 5.11, one selected based branch address. Essentially global BHSR shared static branches, whereas individual BHSRs BHSR dedicated static branch subset static branches address aliasing indexing set BHSRs using branch address. three options implementing PHT: global (g), individual (p), shared (s). global PHT uses single table support prediction static branc hes. Alternatively, indivi dual PHTs used PHT dedicated static branch (p) small subset static branches (s) address aliasing indexing set PH Ts using branch address. third dimension design space involves implementation actual predic- tion algorithm. history-based FSM used implement prediction algorithm, Yeh Part identified chemes adaptive (A). possible implementations two-level adaptive branch prediction classified based three dimensions design parameters. given im ple- mentation denoted using three-letter notation; e.g., GAs represents design employs single global BHSR, adaptive prediction algorithm, set PHTs shared number static branches. Yeh Part presented three specific implementations able achieve prediction accuracy 97% heir given set benchmarks:  GAg: (1) BHSR size 18 bits; (1) PHT size 218 x 2 bits.  PAg: (51 2x4) BHSRs size 12 bits; (1) PHT size 212 x 2 bits.  PAs: (512x4) B HSRs size 6 bits; (512) PHTs size 26 x 2 bits. three implementations use adaptive (A) predictor 2-bit FSM. first implementation employs global BHSR (G) 18 bits global PHT (g) 2,s entries indexed BHSR bits. second implementation employs 512 sets (four-way set-associative) 12-bit BHSRs (P) global PHT (g) 21 entries. third implementation also employs 512 sets four-way set- associative BHSRs (P), 6 bits wide. also uses 512 PHTs (s), 26 entries indexed BHSR bits. 512 sets BHSRs 512 PHTs indexed using 9 bits branch address. Additional branch address bits used set-associative access BHSRs. 512 PHTs direct- mapped, aliasing, i.e., multiple branch addresses sharing PHT. experimental data, aliasing rriinimal impact degrading prediction accuracy. Achieving greater 95% prediction accuracy two- level adaptive branc h prediction schemes quite impressive; best traditional prediction techniques onl achieve 90% prediction accuracy. two- level adaptive branch prediction approach adopted number real designs, including Intel Pentium Pro AMD/NexGen Nx686.234 MODERN PROCESSOR DESIGN Traditional PHT 2* x 2'x2 Figure 5.12 Correlated Branch Predictor Global BHSR Shared PHTs (GAs). Following original Yeh Part proposal, studies McFarling [1993J? Young Smith [1994], Gloy et al. [1995] gained insights two- level adaptive, recently called correlated, branch predictors. Figure 5.12 illustrates correlated branch predictor global BHSR (G) shared PHT (s). 2-bit saturating counter used predictor FSM. global BHSR tracks directions last Jt dynamic branches captures dynamic control flow con- text. PHT viewed single table containing two-dimensional array, V columns 2* rows, 2-bit predictors. branch address n bits, subset j bits used index PHT select one 2; columns. Since j less n, aliasing occur two different branch addresses index column PHT. Hence designation shared PHT. k bits BHSR used select one 2* entries selected column. 2 history bits sel ected entry ar e used make history-based prediction. traditional branch history table equivalent one row PHT indexed j bits branch address, illustrated Figure 5.12 dashed rectangular block 2-bit predictors first row PHT. Figure 5.13 illustrates correlated branch predictor individual, per^ branch, BHSRs (P) shared PHT (s). Similar GAs scheme, PAs scheme also uses j bits branch address select one 2/ columns f " PHT. However, bits branch address, overlap j bits used access PHT, used index set BHSRs. Depe nding branch address, one 2' BHSRs selected. nce, BHSR associated one particular branch add ress, set branch addresses aliasing. Essentially, instead usin g single BHSR provide dynamic control flow context static branches, multiple BHSRs used provide distinct dynamic control flow contexts different subsets static bra nches. addsSUPERSCALAR TECHNIQUES 23: j^ranch^ddressj /bits | ~ | j bits Individual branch history shift registers ( 2'xt E—i PHTof2*x2Jx2 Figure 5.13 Correlated Branch Predictor Individual BHSRs Shared PHTs (PAs). Branch address Global branch h istory shift register (BHSR) rTTTMl i— (kbits PHTof2 ""u'<*~'l X 2 Figure 5.14 gshare Correlated Branch Predictor McFarling. Source McFarling, 1993. flexibility tracking exploitin g correlations ifferent branch instruc- tions. BHSR tracks directions last Jt dynamic branches belonging subset static bran ches. GAs PAs schemes require PHT size 2* x 2J x 2 bits. GAs scheme one Jt-bit BHSR whereas PAs scheme requires 2' Jt-bit BHSRs. fairly efficient correlated branch pr edictor called gshare proposed Scott McFarling [1993]. sche me,; bits branch address "hashed" (via bitwise XOR function) Jt bits global BHSR; see Figure 5.14. resultant max{k,j) bits used index PHT size 2max|^/1 x 2 bits to16 MODERN PROCESSOR DESIGN select one 2max'*-jl 2-bit branch predictors. gshare scheme requires one Jt-bit BHSR much sm aller PHT, yet achieves compar able predic- tion accuracy correlated branch predictors. scheme used DEC Alpha 21264 4-way superscalar microprocessor [Keller, 1996]. 5.1.6 Instruction Flow Techniques primary objective instruction flow techniques supply many useful instructions possible execution core processor every machine cycle. two major challenges deal conditional branches taken branches. wide superscalar processor, provide adequate conditional branch through- put, processor must accurately predict outcomes targets multi- ple conditional branches every machine cycle. example, fetch group four instructions, possible four instructions conditional branches. Ideally one would like use addresses four instructions index four-ported BTB retrieve history bits target addresses four branches. complex predictor make overall prediction based history bits. Speculative fetching proceed based prediction. Techniques predicting multiple branches every cycle proposed Conte et al. [1995] well Rotenberg et al. [1996]. also important ensure high accuracy predictions. Global branch history used conjunc- tion per-branch history achieve accurate predictions. branches sequences branches exhibit strongly biased branching behavior fore predictable, dynamic eager execution (DEE) proposed Gus Uht [Uht Sindagi, 1995]. DEE employs multiple PCs simultaneously fetch multiple addresses. Essentially fetch stage pursues multiple control flow paths branches resolved, time wrong path dynamically pruned invalidating instructions paths. Taken branches second major obstacle supplying enough useful instructions execution core. wide machine fetch unit must able correcdy process one taken branch per cycle, nvolves predicting branch's direction target, fetching, aligning, mergin g instruc- tions multiple branch targets. effective approach alleviating problem involves use trace cache initially proposed Eric Rotenberg [Rotenberg et al., 1996]. Since then, form trace caching implemented Intel's recent Pen tium 4 superscalar microprocessor. Trace cache history-based fetch mechanism store dynamic instruction traces cache indexed fetch address branch outcomes. traces assembled dynamically based dynamic branching behavior contain multiple nonconsecutive basic blocks. Whenever fetch address hits trace cache, instructions ar e fetched trace cache rather instruction cache. Since dynamic sequence instructions trace cache contain multiple taken branches stored sequentially, need fetch multiple targets need multiported ins truction cache complex merging aligning logic fetch stage. trace cache viewed dynamicSUPERSCALAR TECHNIQUES 237 basic block reordering according dominant execution paths taken program. merging aligning done completion time, wh en nonconsecutive basic blocks dominant path first executed, assembl e trace, stored one line trace cache. goal race cache warmed up, fetching come trace cache instead instruction cache. Since reordered basic blocks trace cache better match dynamic execution order, fewer fetches nonconsecutive locations trace cache, effective increase overall throughput ta ken branches. 5.2 Register Data Flow Techniques Register data flow tec hniques concern effective execution ALU (or register- register) type instructions execution core processor. ALU instructions viewed performing "real" work specified program, control flow load/store instructions playing supportive roles providing neces- sary instructions required data, respectively. ideal machine, branch load/store instructions, "overhead" instructions, take time execute computation latency stricdy determined processing AL U instructions. e ffective proce ssing instructions foundational achieving high pe rformance. Assuming load/ store architecture, ALU instructions specify operations performed source operands stored registers. Typically ALU instruction specifies binary operation, two source registers operands re- trieved, destination register result placed, it, <— F„(RJi^ specifies typical ALU instruction, execution requires availability (1) F„, functional unit; (2) Rj Rt, two source operand registers; (3) Rt, destination register. functional unit F„ available , struc- tural dependence exists ca n result struc tural hazard. one source operands «, Rk available, hazard due true data depen- dence occur. th e destination register Rt available, hen hazard due anti- output dependences occur. 5.2.1 Register Reuse False Data Depe ndences occurrence anti- output dependences, false dat dependences, due reuse registers. registers never reused store operands, false data dependences occur. r euse registers commonly r eferred register recycling. Register recycling occurs two different forms, one static dynamic. static form due optimization p erformed compiler presented first. typical compiler, toward back end compilation process two tasks ar e performed: code generation register alloca- tion. code generation task responsible actual emitting machine instructions. Typically code generator assumes availability unlimited number symbolic registers stores te mporary data. sym- bolic register used store one value written once, producing is38 MODERN PROCESSOR DESIGN commonly referred single-assignment code. However, ISA limited number architected registers, hence register allocation tool used map unlimited number symbolic gisters limited fixed number architected registers. register allocator attempts keep many tempo- rary values registers possible avoid move data memory locations reloading later on. accomplishes reusing registers. register written new value old value stored longer needed; effectively register recycled hold multiple values. Writing register referred definition register reading register use register. definition, one uses efinition. duration definition last use value referred live range value. last use live range, register assigned store another value begin another live range. Register allocation procedures attempt map nonoverlapping live ranges architected register maximize register reuse. single-assignment code one-to-one correspondence symbolic registers values. register allocation, architected register receive multiple assignments, register becomes variable take multiple valu es. Consequently one-to-one correspondence registers values lost. instructions executed se quentially redefinition never allowed precede previous definition last use previous definition, live ranges share register never overlap execution recycling registers doe induce problem. Effectively, e-to-one cor- respondence values registers mai ntained implicitly instructions processed original program order. However, superscalar machine, especially out-of-order processing instructions, register reading writing operations occur order different pro gram order. Con- sequently one-to-one correspondence values registers poten- tially perturbed; order ensure semantic correctness anti- output dependences must detected enforced. Out-of-order reading (writing) regis- ters permitted long anti- (output) depende nces enfor ced. dynamic form register recycling occurs loop instructions repeatedly executed. aggr essive superscalar machine capable support- ing many instructions flight relatively small loop body executed, multiple iterations th e loop simultaneously flight machine. Hence, multiple copies register defi ning instruction multiple erations simultaneously present machine, inducing dynamic form register recycling. Consequently anti- output dependences induced among dynamic instructions multiple iterations loop must detected enforced ensure semantic correctness program execution. One way enforce anti- output depend ences simply stall depen- dent instruction leading instruction finished accessing dependent register. anti - [write-after-read (WAR)] dependence exists pair instructions, trailing instruction (register updating struction) must stalled leading instruction read dependent register. output [write- after-write (WAW)] dependence exists pair instructions, trailingSUPERSCALAR TECHNIQUES 2 instruction (register updating instruction) must stalled leading instruc- tion first updated register. stalling anti- output dependent instructions lead significant performance loss necessary. Recall false data dependences induced recycling architected registers intrinsic program semantics. 5.2.2 Register Ren aming Techniques aggressive way deal false data de pendences dynamically assign different names multipl e definitions architected register and, result, eliminate presence false dependences. called register renaming requires use hardware mechanisms run time undo effects register recycling reproducing one-to-one correspondence registers values instructions might simultaneously flight. performing register renaming, single assignment effectively recov- ered instructions flight, anti- output dependences exist among instructions. llow instructions originally false dependences executed parallel. common way implement register renam ing use eparate rename register file (RRF) addition architected register file (AR F). straightfor- ward way implement RRF simply duplicate ARF use RRF shadow version ARF. allow architected register renamed once. However, efficient way use registers RRF. Many existing designs implement RRF fewer entries ARF allow registers RRF flexibly used rename one architected registers. facilitates efficient use rename registers, require mapping table store pointers entries RRF. use separat e RRF conjunction mapping table perform nam- ing ARF illustrated Figure 5.15. separate RRF used register renaming, implementation choices terms place RRF. One option implement separate stand-alone structure similar ARF perhaps adjacent ARF. shown Figure 5.15(a). alternative incorporate RRF part reorder buffer, hown Figure 5.15(b). options busy field added ARF along mapping table. busy bit selected entry ARF set, indicating architected register renamed, corresponding entry map table accessed obtain tag poi nter RRF entry. former option, tag specifies rename register used index RRF; wh ereas latter option, tag specifies reo rder buffer entry used index reorder buffer. Based diagrams Figure 5.15, ifference tw options may seem artificial; however, important subtle differences. RRF incorporated part reorder buffe r, every entry reorder buffer con- tains additional field functions rename register, hence rename register allocated every instruction flight. design based worst-case scenario may wasteful since every instruction defines regis- ter. example, bi men instructions update architected register. the240 MODERN PROCESSOR DESIGN AR1 Map table Register _ specifierData Busy Tag 1 DataData > -Si Reorder buffer (b) Figure 5.15 Rename Register File (RRF) Implementations: (a) Stand-Alone; (b) Attached Reorder Buffer. hand, reorder buffer already contains ports receive data func- tional units update ARF instruction completion time. separate stand-alone RRF used, introduces additional structure requires ports receiving data functional units updating ARF. choice two options implement involves design tradeoffs, options employed real designs. focus stand-alone option get better feel register renaming actually works. Register renaming invo lves three tasks: (1) source read, (2) destination allocate, (3) register update. first task source read typically occurs decode (or possibly dispatch) tage purpose fetching register operands. instruction decoded, source register specifiers used index multiported ARF order fetch register operands. Three possi- bilities occur register operand fetch. First, busy bit set, indicating pending write specified register archi- tected register contains specifi ed operand, operand fetched ARF. busy bit set, indicating pending write register content th e architected register st ale, corresponding entry map table accessed retrieve rename tag. rename tag specifies SUPERSCALAR TECHNIQUES Update instruction completion Register specifierARF Map table Dats Busy TagUpdate instruction finish RRF / Data Valid Busy \ 5^ Operat Operand read Figure 5.16 Register Renaming Tasks: Source Read, Destination Allocate, Register Update.From functional units rename register used index RRF . Two possibilities occur indexing RRF. valid bit ind exed entry set, indicates register-updating instruction already finished execu tion, although still waiting completed. c ase, sourc e operand available rename register retrieved indexed RRF entry. valid bit set, indicates register-updating instruction still executed rename register pending update. case tag, rename register pecifier, map table forwarded reservation sta- tion instead th e source operand. tag used later reserva tion station obtain operand becomes available. Thes e three possibilities source read shown Figure 5.16. task destination allocate also occurs decode (or poss ibly dis- patch) stage three subtasks, namely, set busy bit, assign tag, update map table. instruction decoded, destination register specifier used index ARF. se lected architected register pending write, busy bit must set. specified destination register must mapped rename register. particular unused (indi cated busy bit) rename register must selected. busy bit selected RRF entry must set, index selected RRF entry used tag. tag must written corresponding entry map table, used subsequent dependent instruc- tions fetching source operands. task register update takes place back end mac hine part actual renaming ac tivity decode/dispatch stage, direct impact operation RRF. Register update occur two separate steps; see Figure 5.16. register-updating instruction finishes execution, result written entry RRF indicated tag. Later this242 MODERN PROCESSOR DESIGN instruction completed, result copied RRF ARF. Hence, register update involves updating first entry RRF entry ARF. two steps occur back-to-back cycles register-updating instruction head reorder buffer, separated many cycles unfinishe instructions reorder bu ffer ahead instruction. rename register copied corresponding architected register, busy bit r eset used rename another architected register. far assumed register renaming implementation requires use two separate physical register files, namely ARF RRF. However, assumption necessary. architected registers rename registers pooled together implemented single physical register file number entries equal sum ARF RRF entry counts. pooled register file rigidly designate registers architected registers others rename registers. physical register flexibly assigned architected register rename register. Unlike separate ARF RRF implementation must physically copy result RRF ARF instruction completion, pooled register file needs change designation register rename register architected register. save data transfer interconnect RRF ARF. key disadvantage pooled register file hardware complexity. secondary disadvantage context swap time, machine state must saved, subset regis- ters constituting architected state machine must explicidy identified state saving begin. pooled register file approach used floating-point unit original IBM RS/6000 design illustrated Figure 5.17 [Gr ohoski, 1990; Oehler Groves, 1990]. design, 40 physical registers implemented supporting ISA specifies 32 architected registers. mapping table implemented, based whose content subset 32 40 physical registers de signated architected registers. mapping table contains 32 entries indexed 5-bit architected register specifier. entry indexed returns 6-bit specifier indi- cating physica l register architected register mapped. OP si S2 3 FADOP Si S2 S3 2 1 FAD 3 Map table 32X6Head Free list Tail 32 33 343536 37 38 39 | P<nding tilrget retur n queue 1Head release tail Figure 5.17 Floating-Point Unit (FPU) Register Renaming IBM RS/6000.SUPERSCALAR TECHNIQUES 243 floating-point unit (FPU) RS/6000 pipelined fun ctional unit rename pipe stage preceding decode pipe tage. rename pipe stage con- tains map table, two circular queues, associated control logic. first queue called free list (FL) contains physical registers available new renaming. second queue called pending target return queue (PTRQ) contains physical registers used rename architected regis- ters subsequently re-renamed map table. Physical registers PTRQ returned FL last use register ha occurred. Two instructions traverse rename tage every machine cycle. pos- sibility fused multiply-add (FMA) instructions three sources one des- tination, two instructions contain four register specifiers. Hence, map table must eight-ported support imultaneous tr anslation eight architected register specifiers. p table initialized identity mapping; i.e., architected register mapped physical register = 0,1 31. ini- tialization, physical registers 32 39 placed FL PTRQ empty. instruction traverses rename stage, architected register specifi- ers used index map table obtain translated physical register specifiers. eight-ported map table 32 entries, indexed 5-bit archi- tected register specifier, entry containing 6 bits indicating physical reg- ister architected register mapped content map table represents latest mapping architected registers physical registers speci- fies subset physical registers currendy represents architected r egisters. FPU RS/6000, design load instructions trigger new renaming. register renaming prevents FPU stalling waiting loads execute order enforce anti- output dependences. load instruction traverses rename stage, destination register specifier used index map table. current con tent entry map table pushed PTRQ, next physical gister FL loaded map table. effectively renames redefinition destination register different physical r egister. subsequent instructions specify architected register sourc e operand receive new physical register specifier source register. Beyond rename stage, i.e., decode execute stages, FPU uses physical register specifiers, true register dependences enforced using physical register specifiers. map table approach represents th e aggressive versatile imple men- tation register renaming. Every ph ysical register used represent redefinition architected register. significant hardware complexity required implement multiported map table logic control two circular queues. return register PTRQ FL especially trouble- due ifficulty ident ifying last-use nstruction register. How- ever, unlike approaches based use separate rename r egisters, instruction completion time copying content rename registers architected registers necessary. ther hand, interrupts occur part con- text swap, th e subset physical registers constitute current architected machine state must explicitly determined based map able contents.244 MODERN PROCESSOR DESIGN contemporary sup erscalar microprocessors implement form register renaming avoid stall anti- output register data depen- dences induced reuse registers. Typically register renaming occurs instruction decoding time, implementation become quite complex, especially wide superscalar machines many register specifiers multiple instructions mu st simultaneously renamed. It' possible multiple redefinitions register occur within fetch group. Implementing register renaming mechanism wide superscalars without seriously impacting machine cycle time real challenge. achieve high p erformance serialization con- straints imposed false register data depe ndences must eliminated ; hence, dynamic register renaming absolutely essential. 5.2.3 True Data Dependences Data Flow Limit RAW dependence two instructions called true data dependence due producer-consumer relationship two ins tructions. trailing consumer instruction cannot obtain source operand leading producer instruction produces result. true data dependence imposes serialization con- straint two dependent instructions; leadi ng instruction must finish execution trailing instruction begin execution. true data dependences result semantics pr ogram usually represented data flow graph data dependence graph (DDG). Figure 5.18 illus trates code fragment fast Fourier transform (FFT) imple- mentation. Two source-level statements compiled 16 assembly instruc- tions, including load store instructio ns. floating-point array variables w[i+k] .ip = z[i].rp + z[m+i].rp; w[i+j].rp = e[k+l].rp » (z[i].rp - z[m+i].rp) - e[k+l].ip * (z[i].ip - z [m+i] . ip) ,- (a) X AIM P L 11: f2 <_ load, 4 (r2) 12 : fO «- load, 4 (r5) i3: fO «- fadd, f2,f0 i4: 4(rt) «- store, fO 15: fl4 «- laod,8 (r7) i6: f6 «- load, 0(r2) 17: fS «- load,0(r3) 18: f 5 «- fsub, f6,f5 i9 : f4 «- fmul, f 14, f 5 110: fl5 «- load, 12 (r7) ill: f7 «- load, 4 (r-2) 112 : f8 «- load, 4 (r3) il3: f8 *- fsub,f7,f8 il4: fB «- fmul, f 15, f8 EL il5 : fB «- fsub, f4 , f8 il6: 0(r8) «- store, f8 (b) Figure 5.18 FFT Code Fragment: (a) Original Source Statements; (b) Compiled Assembly Instructions.SUPERSCALAR TE CHNIQUES 245 Figure 5.19 Data Flow Graph Code Fragment Figure 5.18(b). stored memory must first loaded operations per- formed. computation, results stored back memory. Integer registers (ri) ar e used hold.addresses arrays. Floating-point registers (ff) used hold temporary data. DFG induced writing reading floating-point registers 16 instructions Figure 5.18(b) shown Figure 5.19. node Figure 5.19 represents instruction Figure 5.18(b). directed edge exists two instructions exists true data dependenc e two instructions. dependent register identified depen- dence edges DFG. latency also associated dependence edge. Figure 5.19. edge labeled execution latency pr oducer instruction. example, load, store, add ition, subtra ction instructions assumed wo-cycle execution latency, multiplication instructions require fo ur cycles. latencies ssociated dependence edges cumulative. longest dependence chain, measured terms total cumulative latency, iden tified critica l path DFG. Even assuming unlimited machine resources, code fragment cannot executed faster length critical path. commonly referred data flow limit program execution represents best performance po ssibly achieved. code fragment Figure 5.19 data flow limit 12 cycles. data flow limit dictated true data dependences program. Traditionally, data flow execution model stipulates every instruction program begin execution immedi- ately cycle following operands become available. effect, existing register data flow techniques attempts approach data flow limit.246 MODERN PROCESSOR DESIGN ±-pi, IXAMPLE _5.2.4 Classic Tomasulo Algorithm design IBM 360/91's floating-point unit, incorporating come known Tomasulo's algorithm, laid groundwork modern supersca- lar processor designs [Tomasulo, 1967]. Key attributes contemporary reg- ister data flow echniques found classic Tomasulo algorithm, deserves in-depth examination. first introduce original design floating-point unit IBM 360, describe detail th e modi fied design FPU IBM 360/91 incorporated Tomasulo's al gorithm, finally illustrate operation effec tiveness processing example code sequence. original design IBM 360 floating-point unit shown Figure 5.20. FPU contains two functional units: one floating-point add unit one floating-point multiply/divide unit. three register files FPU: fl oating-point regist ers (FLRs), floa ting-point buffers (FLBs), store data b uffers (SD Bs). four FLR registers; archi- tected floating-poin registers. Floating-point instructions wit h storage-register storage-storage addressing modes preprocessed. Address generation memory Storage bus 6 Control5 ControlFloating-point 4 Control buffers (F LBs) 3 Control 2Control 1Control 1 '  1 1 Floating-point buffer (FLB) bus Sink Source Ctrl. Adder Result Instruction unit _L Floating- poim operand stack (FLOS) DecoderControl8 ControlFloating-point 4 Controlregisters (FLRs) 2 Control 0 i—::—i—::—i Floating-point register (FLR) busControlStore 3 Control data 2 Control buffers (SDBs) 1 storage Result bus Figure 5.20 Original Design IBM 360 Floating-Point UnitSUPERSCALAR TE CHNIQUES 247 accessing p erformed outside FPU. data retrieved memory, loaded one six FLB registers. Similarly destina- tion instruction memory location, result stored placed one three SDB registers separate unit accesses SDBs complete storing result memory location. Using two additional register files, FLBs, SDBs, support storage-register storage-storage instructions, FPU effectivel functions register-registe r machine. IBM 360/91, instruction unit (IU) decodes instructions passes floating-point instructions (in order) floa ting-point operation stack (FLOS). FPU, floating-point instructions decoded issued order FLOS two functional units. two functional units pipelined incu r multiple-cycle latencie s. adder incurs 2 cycles add instructions, multiply/divide unit incurs 3 cycles 12 cycles performing mu ltiply divide instructions , respectively. mid-1960s, IBM began developing eventually became Model 91 Systems 360 family. One goals achieve concur rent execution multiple floating-point instructions sustain throughput one instruc- tion per cycle instruction pipeline. quite aggressive considering complex add ressing modes 360 ISA multicycle latencies exe- cution units. end result modified FPU 360/91 incorporated Tomasulo's algorithm; see Figure 5.21. Tomasulo's algorithm consists adding three new mechanisms original FPU design, namely, reservation stations, common data bus, register tags. original design, functional unit single buffer input side hold instruction currently executed. functional unit busy, issuing instructions FLOS stall whenever next instruction issued requires functional unit. alleviate thi structural bottleneck, multiple buffers, called reservation stations, attached input side func- tional unit. adder unit thre e reservation stations, multiply/divide unit two. hese reservation stations viewed virtual functional units; long free reservation station, FLOS issue instruction functional unit even currently busy exec uting another instruction. Since FLOS issues instructions order, prevent unnecessary stalling due unfortunate ordering different floating-point instruction types. availability r eservation stations, instructions also issued functional units FLOS even though operands yet avail- able. instructions wait reservation st ation operands begin exe cution become available. common data bus (CDB) connects outputs two functional units reservation stations well FLRs SDB registers . Results produced functional units broadcast CDB. instructions reservation stations needing results heir operands latch data CDB. registers FLR SDB dest inations results also latch data CDB. CDB facilitates forwarding results directly pro- ducer instructions consumer instructions waiting reservation stations248 MODERN P ROCESSO R DESIGN Storage bus Instruction unit 6 5 Floating-point 4 Control buffers (FLBs) 3 Control 2 1Floating- point operand stack (FLOS) DecoderBusy bitsTag!8 Floating-point 4 registers (FLRs )2 FLB bus FLRbus CDB Tag Sink Tag Source Ctrl. Tag Sink Tag Source Ctrl. Tag Sink Tag Source Ctrl Adder f Result Store Control data j buffers (SDBs)l Tag Sink Tag Source Cut Tag Sink Tag Source Ctrl. Common data bus (CDB)nk Tag Source nk Tag Source Multiply/divide Result | j Figure 5.21 Modified Design IBM 360/91 Floating-Point Uni Tomasulo's Algorithm. EXAMP E without go registers. Destination registers upda ted, simultaneously forwarding results dependent ins tructions. operand coming memory location, loaded FLB register memory accessing performed. Hence, FLB also output onto CDB, allowing waiting ins truction reservation station latch operand. Consequently, two functional units FLBs drive data onto CDB, reservation station's FLRs SDBs latch data CDB. FLOS dispatching instruction functional unit, allocates reservation station checks see needed operands available. operand available FLRs, content register FLRs copied reservation station; otherwise tag copied reservation sta- tion instead. tag indicates p ending operand going come from.SUPERSCALAR TECHNIQUES 249 pending operand come producer instruction currently resident one five reservation stations, come one six FLB regis- ters. uniquely id entify one 11 possible sources pending operand, 4-bit tag required. one two operand fields r eservation station con- tains tag instead actual operand, indicates instruction waiting pending operand. pending operand becomes available, pro- ducer operand drives tag along actual operand onto CDB. waiting instr uction reservation station uses tag monitor CDB. detects tag match CDB, latches associated operand. Essentially producer operand broadcasts tag operand CDB; consumers operand monitor CDB tag, broadcasted tag matches tag, latch asso ciated operand CDB. Hence, possible destinations pending operands must carry tag field must monitor CDB tag match. reservation station contains two operand fields, must carry tag field since two operands pending. four FLRs three registers SDB must also carry tag fields. total 17 tag fields representing 17 places monitor receive operands; see Figure 5.22. tag field potential con- sumer site used associative fashion monitor possible matching content tag value broadcasted CDB. tag match occurs, consum er latches broadcasted operand. IBM 360 floating-point instructions use two-address instruction format. Two source operands specified. first operand specifier called sink also doubles destination specifier. second op erand speci fier called source. reservation station two operand fields, one sink source. operand field accompanied tag field. operand field contains real data, tag field set zero. Otherwise, tag field identifies source pending operand corning from, used monitor CDB availability pending operand. Whenever Reservation station FLRTag BusyFirst operand Second operand "Sink" Tag (a) Tag Data (b) SDB registerTag Data (c) Figure 5.22 Use Tag Fields (a) Reservation Station, (b) FLR, (c) SDB Register.250 MODERN PROCESSOR DESIGN instruction dispatched FLOS reservation station, data FLR corresponding sink operand retrieved copied reservation station. time, "busy" bit associated FLR set, indicating pending update register, tag value identifies particular reservation station instruction dispatched written tag field th e FLR. Thi clearly identifies reservation stations eventually produce updated data FLR. Subsequentiy trailing instruction specifies register one source op erands, dispatched reservation station, tag field (called pseudo-operand) copied corresponding tag field reservation station actual data. busy bit set , indicates data FLR stale tag represents th e source real data come. reservation stations FLRs, SDB registers also destinations pe nding operands hence tag field required three SDB registers. use example sequence instructions illustrate operation Tomasulo's algorithm. deviate actual IBM 360/91 design several ways help clarify example. First, stead two-address format IBM 360 instructions, use thr ee-address instructions avoid potential confusion. example sequence contains register-register instructions. reduce number machine cycles trace, allow FLOS dispatch (in program order) two instructions every cycle. also assume instruction begin execution cycle dispatched reservation station. keep latencies two three cycles add multiply instructions, respectively. However, allow ins truction forward result dependent instructions last execut ion cycle, dependent instruction begin execution next cycle. tag values 1, 2, 3 used identify three reservation stations adder functional unit, 4 5 used identify two reservation stations mu ltiply/divide func- tional unit. tag values called IDs reservation stations. example sequence consists following four register-register instructions. w: R4 <- R0 + R8 x: R2 <- R0 * R4 y: R4 <- R4 + R8z: R8 <- R4 * R2 Figure 5.23 illustrates first three cycles execution. cycle 1, instructions w x dispatched (in order) reservation stations 1 4. destination registers instructions w x R4 R2 (i.e., FLRs 4 2), respectively. busy bits two registers set. Since instruction w dispatched reservation station 1, tag value 1 entered tag field R4, indicating instruction res ervation station 1 produce result updating R4. Similarly tag value 4 entered tag field R2. source operands instruction w available, begins execution immediately. Instruction x SUPERSCALAR TECHNIQUES 251 CYCLE 1 Dispatched instruction(s): w, x (in order) RS RSTag Sink Tag Source w 1 0 6.0 0 7.8 2 3 w Adder | Tag Sink Tag Source x 4 5FLRBusy Tag Data 0 6.0 1 1 0 6.0 1 2 Yes 43 . 5 Mult/Div |~ 4 Yes 1 10.0Mult/Div |~ 4 7.8 CYCLE 2 Dispatched ins truction(s): y, z (in order) RSTag Sink Tag Source w 1 0 6.0 0 7.8 2 >v0 7.8 3 ~- Adder -JRSTag Sink Tag Source 0 6.0 f'.... 2 4-- Mult/Div P FLRBusy Tag Data 0 6.0 2 Yes 4 3.5 4Yes 2 10.0 8Y e 5 7.8 CYCLE 3 Dispatched instruction(s): RSTag Sink Tag Source 1 2 0 13.8 0 7.8 3 yAdder | RS_Tag Sink Tag SourceFLRBusy Tag Data 0 6.0 0 13.8 0 2 | — 4—2 Mult/Div j 4 86.0 Yes 4 3.5 Yes 2 10.0 Yes 5 7.8 Figure 5.23 Illustration Tomasulo's Algorithm Example instruction Sequence (Part 1). requires result (R4) instruction w second (source) operand. Hence instruction x dispatched reservation station 4, tag field second operand written ilic tag V2!ue 1, indicating th e instruction reservation station 1 produce needed operand. cycle 2, instructions z dispatched (in order) reservation stations 2 5, respectively. needs result nstruction w first operand, instruction y, dispatched reservation station 2, receives tag value 1 tag field first operand. Similarly instruction z, dis- patChcd reservation station 5, receives tag values 2 4 two tag fields, indicating reservation stations 2 4 eventually produce two operands needs. Since R4 destination instruction y, tag field R4 updated ne w tag value 2, indicating reservation station 2 (i.e., instruc- tion y) responsible pendin g update R4. busy bit R4 remains set. busy bit R8 set instruction z dispatched reservation station 5, tag field R8 set 5. end cycle 2, instruction w finishes execution broadcasts ID (reservation station 1) result onto CDB. tag fields c ontaining tag value 1 trigger tag match latch broadcasted result. first tag field reservation station 2 (h olding instruction y) second tag field reservation station 4 (holding instruction x)152 MODERN PROCESSOR DESIGN CYCLE 4 Dis patched instruction(s): RSTag Sink Tag Source 1 2 0 13.8 0 7.8 3RSTag Sink Tag Source 0 6.0 0 13.8 | 2--4|FLRBusy Tag Data 0 6.0 2Yes 4 3.5 4Yesr2 10.0 5 7.8 CYCLE 5 Dispatched instrucUon(s): RS.Tag Sink Tag SourceRSTag Sink Tag SourceFLRBusy Tag Data Adder | x 4 0 6.0 0 13.8 | 0 6.0 z 5 0 21.6 4 2Yes 4 3.5 XMult/Div V 4 21.6 Yes 5 7.8 CYCLE 6 Dispatched instruction(s): RSTag Sink Tag SourceRSTag Sink Tag Source 1 4 2 z 5 0 21.6 o82.8 | 3 zMult/Div P Adder | FLRBusy Tag Data 0 6.0 2 82.8 4 21.6 8Yes 5 7.8 Figure 5.24 Illustration Tomasulo's Algorithm Example Instruction Sequence (Part 2). Ar-. E AMP E tag matches. Hence result ins truction w forwarded dependent instructions x y. cycle 3, instructio n begins execution adder unit, instruction x begins execution multiply /divide unit. Instruction fin ishes execution cycle 4 (see Figure 5.24) broadcasts result CDB along tag value 2 (its reservation station LD). first tag field reservation station 5 (holding instruction z) tag field R4 tag matches pull result instructi y. Instruction x finishes execution cycle 5 broadcasts result CDB along tag value 4. second tag field reservation station 5 (holding instruction z) tag field R2 tag matches pull result instruction x. cycle 6, instruction z begins execution finishes cycle 8. Figure 5.25(a) illustrates data flow graph example sequence four instructions. four solid arcs represent four true data dependences, three arcs represent anti- output dependences. Instructions dispatched program order. Anti-dependences resolved copying operand dispatch time reservation station. Hence, possible trailing instruction overwrite register e arlier instruction chance read gister. operand still pending, dispatched nstruction receiveSUPERSCALAR TECHNIQUES tag operand. operand becomes available, instruction receive operand via tag match reservation station. instruction dispatched, tag field destination register written reservation station LD instruction. subseq uent instruction destination register dispatched, tag field updated reservation station LD new instruction. tag field register always con- tains reservation station ID latest updating instruction. multiple instructions flight destination register, latest instruction able update register. Output dependences implicitly resolved making impossible earlier instruction update register later instruc- tion updated register. introduce pr oblem able support precise exception ince register file necessarily evolve sequential states; i.e., register potentially miss intermediate update. example, Figure 5.23, end cycle 2, instruction w updated destination register R4 . However, instruction destination register, dispatched earlier cycle, th e tag field R4 changed 1 2 anticipating update R4 instruction y. end cycle 2 inst ruc- tion w broadcasts tag value 1, tag field R4 fails trigger tag match pull result instruction w. Eventually R4 updated instruc- tion y. However, exception triggered instruction x, precise exception impossible since register file evolve sequential states. Tomasulo's algorithm res olves anti- output dependences via form regis- ter renaming. definition FLR triggers renaming register register tag. tag taken fro LD reservation station conta ining instruction redefines regist er. effectively removes false dependences causing pipeline stalls. Hence, data flow limit strictly determ ined true data dependenc es. Figure 5.25(b) depicts data flow graph invo lving onlyI MODERN PROCESSOR DESIGN true data de pendences. shown Figure 5.25(a) four instructions required execute sequ entially enforce data dependences, including anti- output dependences, total latency required executing sequence instructions would 10 cycles, given latencies 2 3 cycles addition multiplication instructions, respectively. true dependences con- sidered, Figure 5.25(b) reveals critical path 8 cycles, i.e., path involving instructions w, x, z. Hence, data flow limit sequ ence four instructions 8 cycles. limit achieved Tomasulo's algorithm demonstrated Figures 5.23 5.24. 5.2.5 Dynamic Execution Core current state-of-the-art su perscalar microprocessors consist out-of-order execution core sandwiched in-order front end, fetches dis- patches instructions program order, in-order back end, completes retires instructions also progr order. out-of-order execution core (also referred dynamic execution core), resembling refinement Tomasulo's algorithm, viewed embedded data flow, micro-dataflow, engine attempts approach data flow limit instruction execution. operation dynamic execution core des cribed according three phases pipeline, namely, instruction dispatching, instruction execution, instruction completion; see Figure 5.26. Dispatch buffer £ 1 Disr. atch j-Register writeback Forwarding results toreservation stations renameregisters Completion buffer (reorder buffer)Managed queue; maintains sequential order instructions flight ("takeoff" = dispatching; "landing" = completion) Figure 5.26 Micro-Dataflow Engine Dynamic Execution.SUPERSCALAR TECHNIC instruction dispatching phase consists renaming destination registers, allocating reservation stati reorder buffer entries, advancing instruc- tions dispatch buffer reservation stations. ease presentation, assume register naming performed dispatch stage. redefinition architected registers renamed rename registers. Trailing uses redefinitions assigned corresponding rename register specifiers. ensures producer-consumer relationships properly identified false register dependences ar e removed. Instructions dispatch buffer dispatched appropriate reser- vation stations based instruction type. assume use dis tributed reservation stations, use reservation station refer (multientry) instruction buffer attached functional unit reservation station entry refer one entries buffer. Simultaneous allocation reser- vation station entries dispatched instructions allocation entries reorder buffer inst ructions. Reorder buffer entries allocated according program order. Typically, instruction dispatched must availability rename register, reservation station entry, reorder buffer entry. one three available, instruction dispatching stalled. actual dispatching instructions fro dispatch buffer entries reservation station entries via complex rout ing network. connectivity routing network less full cr ossbar (this frequently case real designs), stalling also occur due resource c ontention routing network. instruction execution phase consists issuing ready instructions, execut- ing issued instructions, forwarding results. reservation station responsible identifying instructions ready execute scheduling execution. instruction first dispatched reservation station, may source operands therefore must wait reservation sta- tion. Waiting instructions continually monitor busses tag matches. tag match triggered , indicating availability pending operand, result broadcasted latched res ervation st ation entry. instruction reservation station entry operands, becomes ready execution issued functional unit. given machine cycle multiple instructions reservation station ady, scheduling algorithm used (typically oldest first) pick one issuing functional unit begin execution. one functional unit connected reservation station (as case distributed rese rvation stations), tha reservation station issue one instruction pe r cycle. issued functional unit, instruction executed. Functional units vary terms latency. si ngle-cycle latencies, others fixed multiple-cycle latencies. Certain functional units require variable number cycles, depending th e values operands operation performed. Typically, even function units requir e multiple-cycle latencies, instruction begins execution pipelined functional unit, stalling instruction middle execution pipeline since data dependences resolved prior issuing resource contention.256 MODERN PROCESSOR DESIGN instruction finishes execution, asserts destination tag (i.e., specifier rename register assigned fof destination) th e actual result onto forwarding bu s. dependent instructions waiting reservation stations trigger tag match latch broadcasted result. instruction forwards result dependent instructions without requiring intermediate steps updating th en reading dependent register. Concurrent result forwarding, RRF uses broadcasted tag index loads broadcasted result selected entry RRF. Typically reservation station entry deallocated instruction issued order allow another trailing instruction dispatched it. Reser- vation station saturation cause instruction dispatch stall. Cert instructions whose execution induce exceptional condition may require rescheduling execution future cycle. Frequently, instructions, r eservation station entries deallocated finish e xecution without triggering exceptions. example, load instruction potentially trigger D-cache miss may require many cycles service. Instead stalling functional unit, excepting load reissued reservation station miss serviced. dynamic execution core described previously, producer-consumer relationship satisfied wi thout wait writing reading dependent register. Th e dependent operand directly forwarded producer instruction consumer instruction minimize latency incurred due true data dependence. Assuming instruction issued cycle receives last pending operand via forwarding bus, instruction contending functional u nit, instruction- able begin execution cycle immediately foll owing availabil- ity operands. Hence, adequate resources st alling due structural dependences occurs, th€ dynamic execution core able approach data flow l imit. 5.2.6 Reservation Stations Reorder Buffer functional units, critical components dynamic execution core reservation stations reorder buffer. operations components dictate function dynamic execution core. present issues associated implementation reservation station reorder bu ffer. present organization behavior special focus loading unloading entry reservation station reorder buffer. three tasks associated operatio n reservation station: dispatching, waiting, issuing. typical reservation st ation shown Figure 5.27(b), various fields entry reservatio n station illus- trated Figure 5.27(a). entry busy bit, indicating entry allocated, ready bit , indicating instruction entry source operands. Dispatching involves loading instruction dispatch buffer entry reservation station. ypically dispatching instruction requires foll owing thre e steps: select free, i.e., busy, reservationSUPERSCALAR TECHNIQUES BusyDispatch Forwarding slots bussesJ"' Li  \ IDispatch Forwarding slots . busses Operand 1 Valid^ * ' ' "  * Operand 2 Valid Ready TTTTT Tag bussesTag match Tag bussesTag match (a) Entry allocated BusyDispatching (b)Entry issued Ready Figure 5.27 Reservation Station Mechanisms: (a) Reservation Station Entry; (b) Dispatching Issuing Reservation Station. station en try; load operands and/or tags selected entry; set busy bit entry. selection free entry based busy bits per- formed allocate unit. allocate unit examines busy bits selects one nonbusy entries allocated entry. implemented using priority encoder. entry allocated, operands and/or tags instruction loaded entry. entry two operand fields, ssociated valid bit. operand field contains actual operand, valid bit set. field contains tag, indicating pend ing operand, valid bit reset must wait th e operand forwarded. entry allocated, busy bit must set. instruction pending operand must wait reservation station. reservation stat ion entry waiting pending operand, must continu- ously monitor tag busses. tag match occurs, operand field latches forwarded result sets valid bit. perand fields valid, ready bit set, indicating instruction source operands ready issued. usually referred instruction wake up. 258 MODERN PROCESSOR DESIGN issuing step responsible selecting ready instruction reserva- tion station issues functional unit. usually referred instruction select. ready instructions identified ready bits set. selecting ready instruction performed issuing unit based scheduling heuristic; see Figure 5.27(b). heuristic based program order long ready ins truction waiting reservation station. Typically instruction issued functional unit, reserva- tion station entry deallocated resetting busy bit. large reservation station quite complex implement. Cm input side, must support many possible sources, including dispatch slots forwarding busses; see Figure 5.27(a). data routing network input side quite complex. waiting step, operand fields reservation station pending operands must continuously compare tags poten- tially multiple tag busses. comparable associative search across reservation station entries involving multiple keys (tag busses). num- ber entries small, quite feasible. However, number entries increases, increase complexity quite significant. portion hard- ware commonly referred wake-up logic. entry co unt increases, also complicates issuing unit scheduling heuristic selecting best ready instruction issue. portion hardware commonly referred select logic. given machine cycle, multiple ready instructions. select logic must determine best one issue. superscalar machine, reservation station potentially support multiple instru ction issues per cycle, case select logic must pick best subset instructions issue among ready instructions. reorder buffer contains instructions flight, i.e., instructions dispatched yet completed architecturally. include instructions waiting reservation stations execut- ing functional units finished execution waiting completed program order. status eac h instruction reorder buffer tracked using several bits entry reorder buffer. instruction one several states, i.e., waiting execu tion, execu tion, finished execution. status bits updated instruction traverses one state next. additional bit also used indicate whether instruction speculative (in predicted path) not. speculation cross multiple branches, additional bits employed identify speculative basic block instruction belongs to. branch resolved, speculative instruction become nonspeculative (if prediction correct) invalid (if prediction incorrect). finished nonspeculative instructions completed. instruction marked invalid architecturally completed exiting reorder buffer. Figure 5.28(a) ill ustrates fields typically found reorder buffer entry; figure rename register field also included. reorder buffer managed circular queue using head pointer tail pointer; see Figure 5.28(b). tail pointer advanced reorder buffer entries alloca ted instruction dispatch. number entries beSUPERSCALAR TECHNIQUES 1 Busy Issued FinishedInstruction addressRename registerSpeculative Valid (a) Next entry Next instruction allocated complete (tail pointer) (head pointer) 1 B0000 01 11 1 111 F IA RR V Reorder buffer (b) Figure 5.28 (a) Reorder Buffer Entry; (b) Reorder Buffer Organization. allocated per c ycle limited dispatch bandwidth. Instructions completed head queue. head queue many instructions finished execution completed completion bandwidth allows. completion bandwidth determined capacity another routing network ports available register writeback. One critical issues num- ber write ports architected register file ar e needed support trans- ferring data rename registers (or reorder buffer entries used rename registers) architected registers. instruction com- pleted, rename register reo rder buffer entry deallocated. head pointer reorder buffer also appropriately updated. way reorder buffer viewed heart central control dynamic execution core status in-flight instructions tracked reorder buffer. possible combine reservation stations reorder buffer one single structure, called instruction window, manages instruc- tions flight. Since dispatch entry reservation station entry reorder buffer must allocated instruction, combined one entry instruction window. Hence, instructions dispatched instruction window, entries instruction window monitor tag busses pending operands, results forwarded instruction window, instructions issued instruction window ready, instructions completed instruction window. size instruction window determines maximum number instructions simultaneously flight within machine consequently degree instruction-level parallelism achieved machine.260 MODERN PROCESSOR DESIGN 5.2.7 Dynamic instruction Scheduler dynamic instruction scheduler heart dynamic execution core. use term dynamic instruction scheduler include inst ruction window associated instruction wake-up select logic. Currently two styles design dynamic nstruction scheduler, n amely, data capture without data capture. Figure 5.29(a) illustrates scheduler data capt ure. style scheduler design, ispatching instruction, operands ar e ready copied register file (either architected physical) instruction win- dow; hence, term data captured. operands ready, tags copied instruction window used latch operands forwarded th e functional units. Results forwarded th eir waiting instructions instruction window. effect, result forwarding instruction wake combined, la Tomasulo's algorithm. separate forwarding path needed also update register file subsequent dependent instructions grab source operands register file dispatched. recent microprocessors adopted different style employ data capture scheduler design; see Figur e 5.29(b). style, reg- ister read performed scheduler, instructions issued functional units. instruction dispatch copying operands instruction window; tags (or pointers) operands loaded win- dow. scheduler still performs tag match wake ready instructions. How- ever, sults functional units forwarded register file. ready instructions issued obtain ir operands directly register file prior execution. effect, result forwarding instruction wake decou- pled. instruction wake tag needs forwarded scheduler. non-data-captured style scheduler, size (width) instruction window significantly reduced, much wider result-forwarding path scheduler needed. Register file Operand copying Data-captured scheduling window (reservation station) J Functional units (a)11 rNon-data-captured scheduling window Register file _tt:__. Functional units (b) Figure 5.29 Dynamic Instruction Scheduler Design: (a) Data Capture (b) Without Dat Capture.SUPERSCALAR TECHNIQUES 261 close relationship register renaming instruction schedul- ing. stated earlier, one purpose dynami c register renaming elimi- nate false dependences induced register cycling. Another purpose establish producer-consumer relationship two dependent instructions. true data dependence determined common rename register spe cifier producer consumer instructions. rename register specifier func- tion tag result forwarding. non-data-captured scheduler Figure 5.29(b) register specifiers used access register file retrieving source ope rands; (destination) register specifiers used tags waking dependent instructions scheduler. data-c aptured type scheduler Figure 5.29(a), tags used result forwarding instruction wake actual register specifiers. tags mainly used identify producer- consumer relationships dependent instructions assigned arbi- trarily. example, Tomasulo's algorithm uses reservation station IDs tags forwarding results dependent instructions well upd ating architected registers. explicit register renaming involving physical rename registers. 5.2.8 Register Data Flow Techniques many years data flow limit en assumed absolute theoretical limit ultimate performance goal. Extensive research efforts data flow archi- tectures data flow machines going three decades. data flow limit assumes true data dependences absolute cannot possibly overcome. Intere stingly, late 1960s early 1970s similar assumption made concerning control dependences. generally thought control dependences absolute wh en encountering conditional branch instruction choice wait conditional branch executed pro- ceeding next instruction due uncertainty actual control flow. Since then, witnessed tremendous strides made area branch prediction techniques. Conditional branches associated control dependences longer absolute barriers frequently overcome speculating direction target address branch. made speculation possible fre- quently outcome branc h instruction quite predictable. 1995 researchers began also question absoluteness true dat dependences. 1996 several researc h papers appeared proposed concept value prediction. first paper Lipasti, Wilkerson, Shen focused predicting load values based observation frequently values loaded particular static load instruction quite predictable [Lipasti et al., 1996]. sec- ond paper generalized th e basic idea predicting result ALU instruc- tions [Lipasti hen, 1996]. Experimental data based real input data sets indicate results produced many instructions actually quite predictable. notion value locality indicates certain instructions tend repeatedly pro- duce small set (sometimes one) result values. trac king results produced instructions, future values become pred ictable based historical val ues. Since thes e seminal papers, numerous papers published recent years proposing various designs valu e predictors [Mendelson Gabbay, 1997; Saz eides Smith, 1997; Calder et al., 1997; Gabbay Mendelson, 1997;262 MODERN PROCESSOR DESIGN 1998a; 1998b; Calder et al., 1999]. recent study, shown hybrid value predictor achieve prediction rates 80% realistic design incorporating value prediction achieve IPC improvements th e range 8.6% 23% SPEC benchmarks [Wang Franklin, 1997]. result instructio n correctly predicted via value prediction, typically perform ed fetch stage, subsequent dependent instruction can. begin execution using speculative result without wait actual decoding execution leading instruction. effectively removes serialization constraint imposed rue data dependence two instructions. way particular dependence edge data flow graph effectively removed correct value prediction performed. Hence, value pre- diction provides potential exceed classical data flow limit. course, vali- dation still required ensure prediction correct becomes new limit instruction execution throughput. Value prediction becomes effective increasing machine perform ance misprediction rarely occurs mispredic- tion penalty small (e.g., zero one cycle) validation latency lesg average instructio n execution latency. Clearly, efficient implementation value prediction c rucial ensuring efficacy improving performance. Another recently proposed idea called dynamic instruction reuse [Sodani Sohi, 1997]. Similar concept value locality, observed experiments real programs frequently sam e sequence instructions repeatedly executed using set input data. results redundant com- putation performed machine. Dynamic instruction reuse tech niques attempt track redundant computations, detected, pre- vious results used without perfor ming redundant comp utations. tech- niques nonspeculative; hence, validation required. value prediction viewed elimination certain dependence edges data flow graph, dynamic instruction reuse techniques attempt remove nodes edges subgraph data flow graph. much earlier research effort shown elimination redundant computations yield significant per- formance gains programs written functional languages [Harbison, 1980; 1982]. recent study also yields similar data presence redundant computations real programs [Richardson, 1992]. area currently actively researched, new insightful results expected. revisit hese advanced register data flow techniques Chapter 10 greater detail. 5.3 Memory Data Flow Techniques Memory instructions responsible moving data main memory register file, essential supporting execution ALU instructions. Register operands needed ALU instructions must first loaded memory. limited number registers, execu tion pro- gram operands kept register file. compiler generates spill code temporarily place certain operands main memory toSUPERSCALAR TECHNIQUES 263 reload needed. spill code implemented using store load instructions. Typically, compiler allocates scalar variables registers. Complex data structures, arrays linked lists, far exceed size register file usually kept main memory. perform operations data structures, load store instructions required. effective pro- cessing load/ store instructions minimize overhead moving data main memory register file. processing load/store instructions res ultant memory data flow become bottl eneck overall machine performance due potential long latency exe cuting memory instruc tions. long latency load/store instruc- tions results need compute memory address need access memory location. support virtual emory, computed memory address (called virtual address) also needs translated physical address physical memory accessed. Cache memories effective reducing effective latency accessing main memory. Furthermore, various techniques hav e developed reduce overall latency increase overall throughput processing load/store instructions. 5.3.1 Memory Accessing Instructions execution memory data flow instructions occurs three steps: memory address generation, memory address translation, data memory accessing. first state basis three steps describe processing load/ store instructions superscalar pipeline. register file main memory defined instruction set architecture data torage. main memory defined instruction set architecture collection 2" memory locations random access capability; i.e., every memory location identified «-bit address directly accessed latency. like architected register file, main memory architected entity visible software instructions. How- ever, unlike register file, addr ess identifies pa rticular memory loca- tion usually exp licitly stored part instruction format. Instead, memory address usually generated based register offset specified instruction. Hence, address generation required involves accessing specified register adding offset value. addition address gen eration, address translation required virtual memory implemented system. architected main memory constitutes virtual address space program viewed program private address space. physical memory implemented machine constitutes physical address space, may smaller virtual address space may even shared multiple programs. Virtual memory mechanism maps virtual address space program physical address space machine. addres mapping, virtual memory able support execution program virtual address space larger physical address space, multiprogramming paradigm mapping multiple virtual address spaces physical address space. mapping mechanism involves translation the264 MODERN PROCESSOR DESIGN computed effective address, i.e., virtual address, physical address used access physical memory. mechanis usually implemented using mapping table, address translation performed via table lookup. third step proce ssing load/ store instruction memory accessing. load instructions data rea memory location stored register, store instructions register value stored memory location. first two steps address generation addres translation performed identical fashion bot h loads stores, third step p erformed ifferendy loads stores superscalar pipeline. Figure 5.30, illustrate thes e three steps occurring three pipeUne stages. first pi pe stage performs effective address generation. assume typical ad dressing mode register indirect offset load store instructions. load instruction, soon address register operand avail- able, issued pipelined functional unit effective address gener- ated first pipe stage. store instruction must wait availability address register data register operands issued. Dispatch buffer [""1Register writeback ^ispatcii ^J"Architected RF BranchJ Integer | Integer | Reorder buffer 1Floating- pointRename RF Load/ Store Store buffer Complete I Retire(Reservation stations Address generation Address translation ^Memory access DatamemoryJ Figure 5.30 Processing Load/Store Instructions,SUPERSCALAR ECHNIQUES 265 first pipe stage generates effective address, second pipe stage translates virtual address physical address. Typically, done accessing translation lookaside buffer (TLB), hardware-controlled table containing mapping virtual physical addresses. TLB essentially cache page table stored main memory. (Section 3.6 provides background material page table TLB.) possible vir- tual address translated belongs page whose mapping currently resi- dent TLB. called TLB miss. particular mapping present page table, retrieved accessing page table main memory. missing mapping retrieved loaded TLB. translation completed. also possible th e mapping resident even page table, meaning particular page referenc ed mapped resident main memory. induce page fault require accessing disk storage retrieve missing page. constitutes program exception necessitate suspension ex ecution current program. successful address translation second pi pe stage, load instruction accesses data memory third pipe stage. end machine cycle, data retrieved data memory written either rename register reorder buffer. point lo ad instruction finishes execution. updating th e architected register performed load instruction completed reorder buffer. assume data memor access done one machine cy cle third pipe stage. possible data cache employed. (Section 3.6 provides background material caches.) data cache, possible data loaded resident data cache. result data cache miss require fi lling data cache main memory. Suc h cache misses necessitate stalling load/store pipeline. Store instructions processed somewhat differently load instructions. Unlike load instruction, stor e instruction considered finished exe- cution end second pipe stage successful translation address. register data stored memory kept reorder buffer. time store completed, data written memory. reason delayed access memory prevent premature potentially erroneous update memory case store instruction may flushed due occurrence exception branch mispredic- tion. Since load instructions read memory, flushing result unwanted side effects memory st ate. store instruction, instead updating memory completion, pos- sible move data store buffer co mpletion. store buffer FIFO buffer buffers architect urally completed store instructions. store instructions retired, i.e., updates th e memory, memory bus avail- able.The purpose store buffer allow stores retired mem- ory bus busy, thus giving priority loads need access memory bus. use term completion refer updating CPU state term retiring refer upd ating memory state. stor e buffer, store instruction arc hitecturally complete yet retired memory.56 MODERN PROCESSOR DESIGN program exception occurs, instructions follow excepting instruction may finished order, must flushed reor- der buffer; however, store buffer must drained, i.e., th e store instructions store buffer must retired, excepting program suspended assumed address translation memory accessing done one machine cycle. course, th e case TLB first level memory hierarchy return hits. in-depth treatment memory hierarchies maximize occurrence cache hits exploiting temporal spatial locality using various forms caching provided Chapter 3. Section 5.3.2, focus specifically additional complica- tions result out-of-order execution memory refer ences mechanisms used modern problems address complications. 5.3.2 Ordering Memory Accesses memory dat dependence exists two load/store instructions reference memory location, i.e., exists aliasing, collision, two memory addresses. load instruction performs read memory location, store instruction performs write memory location. Similar register data dependences, r ead-after-write (RAW), write-after-read (WAR), write-after- write (WAW) dependences exist load store instructions. store (load) instruction followed load (store) instruction involving memory location induce RAW (WAR) memory data dependence. Two stores memory location induce WAW dependence. memory data depen- dences must enforced order preserve correct semantics program. One way enforce memory data dependences execute load/store instructions program order. Suc h total ordering memory instructions sufficient enforcing memory data dependences necessary. conservative impose unn ecessary limitation performance program. use example Figure 5.31 illustrate point. DAXPY name piece Y(i) = A* X(i) + Y(i) FO «- LD, R4 i- ADDI,Rx.#512 Loop: F2 «- LD, 0 (Rx) F2 «- MULTD, FO, F2 F4 «- LD, 0 (Ry) F4 «- ADDD, F2, F4 0 (Ry) «- SD,F4 Rx «- ADDI, Rx, #8 Ry «- ADDI, Ry, #8 R20 <- SUB,R4.Rx BNZ,R20,Loop Figure 5.31 DAXPY Example.,-last address ,-load X(i) ,A*X(i) .-load Y(i) ,-A*X(i) +Y(i) ,-store Y(i) tine, index X ;inc- index ,-compute bound ,-check doneSUPERSCALAR TECHNIQUES 26^ code multiplies array coefficient adds resultant array another array. DAXPY (derived "d ouble precision times X plus Y") kernel LINPAC routines commonly found many numerical pro- grams. Notice iterations loop data-independent executed parallel. However, impose constraint load/store instructions executed total order, first load ins truction second iteration cannot begin store ins truction first iteration performed. constraint effectively erialize execution iterations loop. allowing load/store instructions execute order, without violating memory data dependences, performance gain achieved. Take example DAXPY loop. graph Figure 5.31 presents true data dependences involving core instructions loop body. dependences exist among instructions iteration loop. data dependences multiple iterations loop. loo p closing branch instruction highly predictable; hence, fetching subsequent iterations done quickly. architected registers specified instructions subsequent iterations dynamically renamed register renaming mechanisms; hence, reg- ister dependences b etween iterations due dynamic reuse archi- tected registers. Consequently load/store instructions allowed execute order, load instructions trailing iter ation begin execution store instr uction earlier iteration. overlapping execution multi- ple iterations loop, performance gain achieved execution loop. Memory models impose certain limitations out-of-order execution load/ store instructions processor. First, facilitate recovery exceptions, sequential state memory must preserved. words, memory state must evolve according sequential execution load/stor e instructions. Second, many shared-memory multiprocessor systems assume sequential consistency memory model, requires accessing shared memory processor done according pro gram order [Lamport, 1979, Adve Gharachorloo, 1996].1 thes e reasons effectively require store instruc- tions executed program order, least memory must updated stores performe program order. stores required execut e program order, WAW WAR memory data dependences implicitly enforced issue. Hence, RAW memory data dependences must enforced. 5.3.3 Load Bypassing Load Forwarding Out-of-order execution load instructions primary source potential performance gain. seen DAXPY example, load instructions frequently beginning dependenc e chains, early execution facilitate early execution depen dent instructions. Whi le relative memory data dependences, load instructions viewed performing read opera- tions memory l ocations, actually performing write operations destination registers. loads register-defining (DEF) instructions, Consistency models discussed greater detail Chapter 11.!68 MODERN PROCESSOR DESIGN Dynamic instruction sequence: Store X Store X Store Store Load Z Load XDynamic instruction sequence: J directly load (a) (b) Figure 5.32 Early Execution Load Instructions: (a) Load Bypassing; (b) Load Forwarding. typically followed immediately dependent register use (USE) instruc- tions. goal allow load instructions begin execution early possible,, possibly jumping ahead preceding store instructions, long RAW memory data dependences violated memory dated according sequential memory consistency model. Two specific techni ques earl out-of-order exe cution loads loa^ bypassing load forwarding. shown Figure 5.32(a), load bypassing allows trailing load executed earlier preceding stores load address alias preceding stores; i.e., memory data dependence stores load. hand, trailing load aliases preceding store, i.e., RAW dependence store th e load, load forwarding allows load receive data directiy store without access data memory; see Figure 5.32(b). cases, earlier execu-- tion load instruction achieved. discuss issues must addressed order imp lement load bypassing load forwarding, first present organization por- tion execution core responsible p rocessing load/st ore instructions. organization, shown Figure 5.33, used v ehicle discussion load bypassing load forwarding. one store unit (two pipe stages) one load unit (three pipe stages); bot h fed common reservation station. assume load store instructions issued shared reserva- tion station program rder. store unit supporte store buffer. load unit store buffer access data cache. Given organization Figure 5.33, store instruction one sev- eral states flight. store instruction dispatched reserva- tion station, entry reorder buffer allocated it. remains reservation station source operands become available issued pipe lined execution unit. memory address gener ated success fully translated, considered finished execution placed finished portion stor e buffer (the reo rder buffer also updated). store buffer oper- ates queue two portions, finished completed. finished portion contains stores finished execution yet architecturallySUPERSCALAR TECHNIQUES 26' Reservation station Address generation Address translation (Finished) Store buffer (Completed) Store buffer£ Store unitLoad unitAddress g eneration Address translation Memory access Data Address Data cache Memory update Figure 5.33 Mechanisms Load/Store Processing: Separate Load Store Units In-Order Issuing Common Reservation Station. completed. completed portion store buffer contains stores completed architecturally waiting update memory. identification two portions store buffer done via pointer store buffer status bit store buffer entries. store finished portion store buffer potentially spe culative, misspeculation detected, need flushed fro store buffer. finished store completed reorder buffer, changes finished state completed state. done updating store buffer pointer flipping status bit. completed store finally exits store buffer updates memory, considered retired. Viewed perspective memory state, store really finish execution retired. exception occurs, stores completed portion store buffer must drained order appropriately update memory. dispatched retired, store instruction one three states: issued (in execution unit), finished (in finished portion store buffer), completed (in completed portion store buffer). One key issue implementing load bypassing need check possible aliasing preceding stores', i.e., stores bypassed. load considered bypass preceding store load reads memory store writes memory. Hence, load allowed execute read memory, ust determined alias preceding stores still flight, i.e., issued retired. Assuming in-order issuing lo ad/store instructions load/store reservation station, stores sitting store buffer, including finished com- pleted portions. alias checking possible dependence load preceding store done using store buffer. tag field containing memory address store incorporated entry th e store buffer. Once'0 MODERN PROCESSOR DESIGN Reservation station (Finished) Store buffer (Completed) Store bufferStore unitI data addr 1 1Tag matchLoad unitAddress Data match: update destination register Data cache Match/no match Figure 5.34 Illustration Load Bypassing. memory address load available, address used perform associative search tag field store buffer entries. match occurs, aliasing exists load allowed execute order. Otherwise, load independent preceding stores store buffer executed ahead them. associative search performed third pipe stage load unit conc urrent accessing data cache; see Figure 5.34. aliasing detected, l oad allowed finish corresponding renamed destination register updated data returned data cache. alias- ing detected, data returned data cache discarded load held back reservation station future reissue. complexity implementing load bypassing lies store buffer associated associative search mechanism. reduce complexity, tag field used associative search reduced contain subset address bits. Using subset address bits reduce width comparators needed associative search. However, result pe ssimistic. Potentially, alias indicated narrower comparator really exist full-length address bits used. load bypassing opportunities lost due compromise implementation. general, degradation performance minimal enough address bits used. load forwarding technique enhances complements load bypassing technique. load allowed jum p ahead preceding stores, determined load alias preceding store, potential satisfy load forwarding data' directly aliased store. Essen- tially memory RAW dependence exists leading store tr ailing load. associative search store buffer needed. alia sing detected, instead holding load back future reissue, data aliased entry store buffer forwarded ren amed destination registerSUPERSCALAR TECHNIQUES 27 j Reservation station Store unit (Finished) Store buffer (Completed) Store buffer1 Tag matchLoad unitAddress data add; —1— \.'jnffilfl —i—Data match: forward destination registerData cache Match/no match Figure 5.35 Illustration Load Forwarding. load instruction. technique allows load executed early, also eliminates need load access data cache. reduce bandwidth pressure bus data cache. support load forwarding, added complexity store buffer required; see Figure 5.35. First, full-length ad dress bits must used p erforming associative search. subset ad dress bits used supporting load bypassing, negative consequence lost opportunity. load forwarding alias det ection must exact forwarding data performed; other- wise, lead semantic incorrectness. Second, multiple preceding stores store buffer alias load. multiple matches occur associative search, must logic added determine aliased stores recent. require additional prio rity encoding logic id entify latest store l oad dependent forwarding performed. Third, additional read port may required store buffer. Prior incorporation load forwarding , store buffer one write port interfaces wit h store unit one read port interfaces data cache. new read port required interfaces load unit; other- wise, port contention occur load forwarding data cache update. Significant performance improvement obtained load bypassing load forwarding. According Mike Johnson, typically load bypassing yield 11% 19% performance gain, load forwarding yield ano ther 1% 4% additional performance improvement [Johnson, 1991]. far assumed loads stores share common reservation sta- tion instructions issued reservation station store load units program rder. in-order issuing assumption ensures preceding stores load store buffer load executed. simplifies memory dependence checking; associative search the»2 MODERN PROCESSOR DESIGN store buffer necessary. ever, in-order issuing assumption introduces unnecessary limitation out-of-order execution loads. l oad instruction ready issued; however, preceding store hold issuing load even though two memory instructions alias. Hence, allowing out-of-order ssuing loads stores load/store reservation station permit grea ter degree out-of-order early execution loads. espe- cially beneficial loads beginnings critical dependence chains early execution remove critical performance bottienecks. out-of-order issu ing load/store reservation station allowed, new problem must solved. load allowed issued order, possible stores precede still reservation station execution pipe stages, yet store buffer. Hence, simply per- forming associative search entries store buffer adequate checking potential aliasing load preceding stores. W orse yet, memory addresses se preceding stores still reserva- tion station execution pipe stages may available yet One approach allow load proceed, assuming aliasing preceding stores yet store buffer, validate assumption later. approach, load allowed issue order executed speculatively. alias stores store buffer, load allowed finish execution. However, load must put new buffer called finished load buffer; see Figure 5.36. finished load buffer man- aged similar fashion finished store b uffer. load resident finished load buffer finishes execution completed. Reservation station (Finished) Store buffer (Completed) Store bufferStore unitTag match data addr  1 1 iTag match store completion Finished load bufferLoad unitAddress Data finish: update renamed register addr data r 1 Match/no matchData cache completion: update architected register Match/no match match: flush aliased load trailing instructions Figure 5.36 Fully Out-of-Order Issuing Execution Load Store Instructions.SUPERSCALAR TECHNIQUES 27! Whenever store instruction completed, must perform alias checking loads finished load buffer. aliasing detected, store allowed complete. aliasing detected, means trailing load dependent store, load alread finished execution. implies speculative execution load must invalidated corrected reissuing, even refetching, load subsequent instruc- tions. require significant hardware complexity performance pe nalty. Aggressive early issuing load instructions lead significant perfor- mance benefits. ability speculatively issue loads ahead stores lead early execution many dependent instructions, loads. important espe cially cache misses. Early issuing loads lead early triggering cache misses turn mask cache miss penalty cycles. downside speculative issuing loads potential overhead recover misspeculation. One way reduce overhead alias dependence prediction. typical programs, dependence relationship load preceding stores quite pre- dictable. memory dependence predictor implemented predict whether load likely alias preceding stores. predictor used determine whether speculatively ssue load . obtain actual perfor- mance gain, aggressive speculative issuing loads must done judiciou sly. Moshovos [1998] proposed memory dependence predictor used di- rectly bypass store data dependent loads via memory cloaking. subsequent work, Chrysos Emer [1998] escribed similar predictor called store-set predictor. 5.3.4 Memory Data Flow Techniques load bypassing load forwarding, memory data flow techniques. techniques object ives increasing memory bandwidth and/or reducing memory latency. superscalar processors get wider, greater memory bandwidth capable supporting multiple load/store instructions per cycle needed. th e disparity processor speed memory speed continues increase, latenc accessing memory, especially cache misses occur, become serious bottlenec k machine performance. Sohi Fr anklin [1991] studied high-bandwidth data memory systems. One way increase memory band width employ multiple load/store units execution core, supported multiported data cache. Section 5.3.3 assumed presence one store unit one load unit supported single-ported data cache. load unit priority accessing data cache. Store instructions ar e queued store buffer retired store buffer data cache whenever memory bus busy store buffer gain access data c ache. overall data memory bandw idth limited one load/store instruction per cycle. serious limitation, es pecially bursts load instructions. One way alleviat e bottleneck provide two load units, shown Figure 5.37, dual-ported data cache. dual-ported data cache able support two simultaneous cache accesses in274 MODERN PROCESSOR DESIGN Reservation station £ Store (Finished) Store buffer (Completed) Store bufferunitLoad unitAddress Cache DataLoad unitAddress Cache Data Data cache T"1_T Missed load queue —"f— II Main memory Figure 5.37 Dual-Ported Nonlocking Data Cache. every cycle. double potential memory bandwidth. However, comes cost hardwar e complexity; dual-ported cache require doubling cache hardware. One way alleviate hardware cost implement nterleaved data cache banks. data cache implemented multiple banks memory, two simultaneous accesses different banks supported e cycle. two accesses need access bank, bank conflict occurs two ac cesses must serialized. practical experi- ence, cache eight banks keep frequency bank conflicts acceptable levels. common way reduce memory latency use cache. Caches widely employed. gap processor speed memory speed w idens, multiple levels caches required. high-pe rformance superscalar processors incorporate least two levels caches. first l evel (Ll) cache usually keep processor speed access latency one cycles. Typically separate caches toring instruc- tions data. second level (L2) cache typically supports storing instructions data, either on-chip off-chip, accessed series (in case miss Ll) parallel cache. emerging designs third level (L3) cache used. likely future high- end designs large on-chip L3 cache become commonplace. use cache hierarchy caches, two ot techniques reducing effective memory latency, namely, nonblocking cache prefetch- ing cache. nonblock ing cache, first proposed Kroft [1981], reduce effective memory latency reducing performance penalty due cache misses. Tradi- tionally, load instruction encounters cache miss, stall load unit pipeline issuing load instructions cache miss serviced.SUPERSCALAR TECHNIQUES X stalling overly con servative prevents subsequent inde pendent loads may hit data cache fro issued. nonblocking data cache alleviates unnecessary penalty putting side load missed cache missed load queue allowing subsequent load instructions issue; see Figure 5.37. missed load sits missed load queue cache miss serviced. issing block fetched main memory, missed load exits missed load queue finishes execution. Essentially cache miss penalty cycles overlapped with, masked by, processing subsequent independent instructions. course, subsequent instruction depends missed load, issuing instruction talled. number penalty cycles tha masked depends number inde- pendent instructions following missed load. missed load queue contain multiple entries, allowing multiple missed loads serviced concurrently. Potentially cache penalty cycles multiple missed loads verlapped result fewer total penalty cycles. number issues must considered implementing nonblocking caches. Load misses occur bursts. ability upport multiple misses overlap servicing important. interface main memory, l ower- level cache, must able support overlapping p ipelining multiple accesses. filling cache triggered missed load may need contend store buffer write port cache. one complication emerge non blocking cac hes. missed load speculative path, i.e., predicted path, possibility spe culation, i.e., branch prediction, turn incorrect. missed load mispredicted path, question whether cache miss serviced . machine aggressive branch prediction, number loads mispredicted path significant; servicing misses speculatively require significant memory bandwidth. tudies shown tha nonblocking cache reduce amount load miss penalty 15%. Another way reduce mask cache miss penalty use prefetching ca che. prefetching cache anticipates future misses trig gers misses early overlap miss penalty processing instructions preceding missing load. Figure 5.38 illustrates prefetching data cache. Two structures needed implement prefetching cache, namely, memory reference prediction table prefetch queue. memory reference prediction table stores information previously executed loads three different fields. first field contains instruction address load used tag field selecting entry table. second fi eld contains previous data address load, third field contains stride value indicates difference previous two data addresses used load. memory reference prediction table accessed via associative search using fetch address produced branch predictor th e first field table. tag match, indicating hit memory refe rence prediction table, previous address added stride value produce predi cted memory address. predicted address loaded prefetch queue. Entries prefetch queue are276 MODERN PROCESSOR DESIGN Branch predictorInstruction cache } ]'^\"\ ^ Decode buffer Decode \ [ f 1 Dispatch buffer | Dispatch | Memory reference prediction 1Reservation stations r E rfn gjD Uf2 Q^D U~p l Branch Integer Integer Floating Store Load Completion buffer EDpoint 111111 Complete | Prefetch queue Data cache Main memory Figure 5.38 Prefetching Data Cache. retrieved speculatively access data cache, cache miss triggered, main memory next-level cache acce ssed. acces data cache reality cache touc h operation; i.e., access cache attempted order trigger potential cache miss necessarily actually retrieve data cache. Early work prefetching done Baer Chen [1991] Jouppi [1990]. goal prefetching cache try anticipate forthcoming cache misses trigger misses early hide cache miss penalty overlapping cache refill time processing instructions preceding missing load. anticipated missing load executed, data resi- dent cache; hence, cache miss triggered, cache iss penalty incurred. actual effectiveness prefetc hing epends number factors. prefetching distance, i.e., far advance prefetching trig- gered, must larg e enough fully mask miss penalty. reason thatSUPERSCALAR TECHNIQUES 277 predicted instruction fetch address used access memory reference pre- diction table, th e hope data prefetch occur far enough advance load. However, makes prefetching effectiveness subject effective- ness branch prediction. Furthermore, potential polluting data cache prefetches mispredicted path. Status confidence bits added entry memory reference prediction table modulate aggressiveness prefetching. Another problem occur prefetch- ing performed early evict useful block cache induce unnecessary miss. One factor actual memory reference prediction algorithm used. Load addr ess prediction based stride quite effective loads stepping array. loads traversing linked list data structures, stride prediction work well. Prefetching memory references require much sophisticated prediction algorithms. enhance memory data flow, load instructions must executed early fast. Store instructions less important experimental data indicate occur less frequently load instructions usually performance critical path. speed execution loads must reduce latency processing load instructions. overall latency processing load instruction includes four components: (1) pipeline front-end latency fetching, decoding, dispatching load instruction; (2) reservation tation latency waiting register data dependence resolve; (3) execution pipeline latency address generation translation; (4) cache/memory access latency retrieving data memory. nonblocking pref etching caches address fourth component, crucial component due slow memory speed. achieve higher clocking rates , superscalar pipelines ar e quickly becoming deeper dee per. Consequently latencies, terms number machine cycles, first three compo nents also becoming quite significant number speculative techniques proposed address reduction latencies; include load address prediction, load value prediction, memory dependence prediction. Recently load address prediction techniques proposed address latencies associated first three compo nents [Austin Sohi, 1995]. deal th e latency asso ciated first component, load prediction table, similar th e memory reference prediction table, proposed. table indexed pr edicted instruction fetch address, hit able indi- cates presence load instruction upcoming fetch group. Hence, prediction presence load nstruction upcoming fetch group per- formed fetch stage without requiring decode dispatch stages. entry table contains predicted effective address whic h retrieved fetch stage, effect eliminating need waiting reservation station availability base register value address generation stage execution pipeline. Consequently, data cache access begin next cycle, potentially data retrieved fro cache end decode stage. form load address prediction e ffectively collapse latencies th e first three c omponents two cycles, i.e.,278 MODERN PROCESSOR DESIGN fetch decode stages, address prediction correct hit data cache. .i, hardware structures needed support load address prediction quite similar needed memory pre fetching, two mechanisms significant differences. Load address prediction actually executing, though spec- ulatively, load instruction early, whereas memory prefetching mainly trying prefetch needed data cache without actually executing load" instruction. load address prediction, instructions depend load also execute early dependent data available early. Since load address prediction speculative technique, must validated, mispre- diction detected, recovery must performed. validation performed allowing actual load inst ruction fetched nstruction cache executed normal fashion. result speculative version compared normal version. results concur, speculative result1 becomes nonspeculative dependent instructions executed speculatively also declared nons peculative. results agree, nonspeculati result used dependent instructions must reexe- cuted. load address prediction mechanism quite accurate, mispredictions occur nfrequently, misprediction penalty minimal, overall net per- formance gain achieved. Even aggressive load address prediction technique load value prediction [Lipasti et al., 1996]. Unlike load address prediction attempts predict effective ad dress load instruction, load value prediction actually attempts predict value data retrieved memory. accomplished extending load prediction table contain tho predicted address, also predicted value destination register. Experi- mental studies shown many load instructions' destination values quite predictable. Fo r example, many loads actually load value last time. Hence, storing last value loaded static load nstruction load prediction table, value used predicted value static load encountered again. result, load prediction table accessed fetch stage, end cycle, actual destination value predicted load instruction available used next cycle depen- dent instruction. sign ificantly reduce latency required processing load instruction load value prediction correct. Again, validation required, times misprediction penalty must paid. load address prediction load value prediction, third spec- ulative technique proposed called memory dependence prediction [Moshovos, 1998]. Recall Section 5.3.3 perform load bypassing load forwarding, memory dependence checking required. load bypassing, must de termined load alias stores bypassed. load forwarding, recent aliased store must identified. Memory dependence checking become quite complex larger number load/store instructions involved potentially require entire pipe stage. would nice eliminate latency. Experimental data shownSUPERSCALAR TECHNIQUES 279 memory dependence relationship quite predictable. possible track memory dependence relationship load/store instructions executed use information make memory dependence prediction load/store instructions en countered again. memory dependence predic- tion facilitate earlier exe cution load bypassing load forwarding. speculative techniques, validation needed recovery mechanism misprediction must provided. 5.4 Summary chapter attempt cover fundamental microarchitecture techniques used modem superscalar microprocessor design systematic easy-to- digest way. intentionally avoid inundating readers lots quantitative data bar charts. also focus generic techni ques instead features specific commercial products. Chapters 6 7 present detailed case studies two actual products. Chapter 6 introduces IBM/Motorola PowerPC 620 great detail along quantitative performance data. successful com- mercial product, PowerPC 620 represents one earlier mo st aggressive out-of-order designs. Chapter 7 presents Intel P6 microarchitecture, first out-of-order implementation IA 32 architecture. Intel P6 likely commercially successful icroarchitecture. fact P6 microarchi- tecture core provided foundation multiple generations products, including Pentium Pro, Pentium II, Pentium in, Pentium M, clear testi- mony effectiveness elegance original design. Superscalar microprocessor design rapidly evolving art simultaneously ha rnessing creative ideas researchers th e insights skills architects designers. Chapter 8 historical chronicle practice art form. Interesting valuable lessons gleaned historical chronicle. body knowledge superscalar microarchitecture techniques constantly expa nding. New innovative ideas research community well ideas traditional "macroarchitecture" domain likely find way future superscalar micropr ocessor designs. Chapters 9, 10, 11 document ideas. REFERENCES Adve, S. V., K. Gharachorloo: " Shared memory consistency models: tutorial," IEEE Computer, 29, 12, 1996, pp. 66-76. Austin, T. M., G. S. Sohi: "Zero-cycle loads: Microarchitecture support reducing load latency," Proc. 28th Annual ACM/IEEE Int. Symposium Microarchitecture, 1995, pp. 82-92. Baer, J., T. Chen: "An effective on-chip preloading scheme reduce data access penalty," Proc. Supercomputing '91, 1991, pp. 176-186. Calder, B., P. Feller, A. Eustace: "Value profiling," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1997, pp. 259-269.280 MODERN PROCESSOR DESIGN Calder, B., G. Reinman, D. Tullsen: "Selective value prediction," Proc. 26th Annual Int. Symposium Computer Architecture (ISCA'99), vol. 27, 2 Computer Architecture News, New York, N.Y.: ACM Press, 1999, pp. 64-75. Chrysos, G., J. Emer: "Memory dependence prediction using store sets," Proc. 25th Int. Symposium Computer Architecture, 1998, pp. 142-153. Conte, X, K. Menezes, P. Mills, B. Patel: "Optimization instruction fetch mecha- nisms high issue rat es," Proc. 22nd Annual Int. Symposium Computer Architecture, 1995, pp. 333-344. Diefendorf, K., M. Allen: "Organization Motorola 88110 superscalar RISC microprocessor," IEEE MICRO, 12, 2.1992, pp. 40-63. Gabbay, R, A. Mendelson: "Using value prediction increase power speculative execution hardware," ACM Transactions Computer Systems, 16,3,1988b, pp. 234-270. Gabbay, F., A. Mendelson: "Can program profiling support value prediction," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1997, pp. 270-280. Gabbay, F., A. Mendelson: "The effect instruction fetch bandwidth value prediction," Proc. 25th Annual Int. Symposium Computer Architecture, Barcelona, Spain, 1998a, pp. 272-281. Gloy, N. C, M. D. Smith, C. Young: "Performance issues correlated branch prediction schemes," Proc. 27th Int. Symposium Microarchitecture, 1995, pp. 3-14. Grohoski, G.: " Machine organization IBM RISC System/6000 processor," IBM Journal Research Development, 34, 1,1990, pp. 37-58. Harbison, S. P.: Computer Architecture Dynamic Optimization High-Level Language Programs. PhD thesis, Carnegie Mellon University, 1980. Harbison, S. P.: "An architectural alternative optimizing compilers," Proc. Int Conference Architectural Support Programming Languages Operating Systems (ASPLOS), 1982, pp. 57-65. IBM Corp.: PowerPC 604 RISC Microprocessor User's Manual. Essex Junction, VT: IBM Microelectronics Division, 1994. Johnson, M.: Superscalar Microprocessor Design. Englewood Cliffs, NJ: Prentice Hall, 1991. Jouppi, N. P.: "I mproving direct-mapped cache performance addition small fully-associative cache prefetch buffers," Proc. 17th Annual Int. Symposium Computer Architecture. 1990, pp. 364-373. Kessler, R.: "The Alpha 21264 microprocessor," IEEE MICRO, 19,2, 1999, pp. 24-36. Kroft, D.: "Lockup-free instruction fetch/prefetch cache organization," Proc. 8th Annual Symposium Computer Architecture, 1981, pp. 81-88. Lamport, L.: "How make multiprocessor computer correctly executes multiprocess programs," IEEE Trans, Computers. C-28,9, 1979, pp. 690-691. Lee, J., A. Smith: "Branch prediction strategies branch target buffer design," IEEE Computer, 21,7,1984, pp. 6-22. Lipasti, M. H., J. P. Shen: "Exceeding dataflow limit via value prediction," Proc. 29th Annual ACM/IEEE Int. Symposium Microarchitecture, 1996, pp. 226-237. Lipasti, M. H., C. B. Wilkerson, J. P. Shen: "Value locality load value prediction," Proc. Seventh Int. Conference Architectural Support Programming Languages Operating Systems (ASPLOS-VII), 1996, pp. 138-147.SUPERSCALAR TECHNIQUES 281 McFarling, "Combining branch predictors," Technical Report TN-36, Digital Equipment Corp. ( http://research.compaq.com/wrl/techreports/abstracts/TN-36.html ), 1993. Mendelson, A., F. Gabbay: "Speculative execution based value prediction," Technical report, Technion ( http://vfw-ee.tectaiion.ac.il/%7efredg ), 1997. Moshovos, A.: "Memory Dependence Prediction," PhD thesis, University Wisconsin, 1998. Nair, R.: "Branch behavior IBM RS/6000," Technical report, IBM Computer Science, 1992. Oehler, R. R.. R. D. Groves: " IBM RISC System/6000 processor architecture," IBM Journal Research Development, 34,1,1990, pp. 23-36. Richardson, S. E.: "Caching function results: Faster arithmetic avoi ding unnecessary computation," Technical report. Sun Microsystems Laboratories, 1992. Rotenberg, E., S. Benne tt, J. Smith: 'Trace cache: low latency approach high band- width instruction fetching," Proc. 29th Annual ACM/IEEE Int. Symposium Microarchi- tecture, 1996, pp. 24-35. Sazeides, Y., J. E. Smith: "The predictability data values," Proc. 30th Annual ACM/ IEEE Int. Symposium Microarchitecture, 1997, pp. 248-258. Smith, J. E.: "A study branch prediction techniques," Proc. 8th Annual Symposium Computer Architecture, 1981, pp. 135-147. Sodani, A., G. S. Sohi: "Dynamic instruction reuse," Proc. 24th Annual Int. Sympo- sium Computer Architecture, 1997, pp 194-205. Sohi, G., M. Franklin: "High-bandwidth data memory systems superscalar proces- sors," Proc. 4th Int. Conference Architectural Support Programming Languages Operating Systems, 1991, pp. 53-62. Tomasulo, R "An efficient algorithm exploiting multiple arithmetic units," IBM Journal Research Development, 11,1967, pp. 25-33. Uht, A. K., V. Sindagi: "Disjoint eager execution: optimal form speculative execution," Proc. 28th Annual ACM/IEEE Int. Symposium Microarchitecture, 1995, pp. 313-325. Wang, K., Franklin: "Highly accurate data value prediction using hybrid predictors," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1997, pp. 281-290. Yeh, T. Y., Y. N. Patfc 'Tw o-level adaptive raining branch prediction," Proc. 24th Annual Int. Symposium Microarchitecture, 1991, pp. 51-61. Young, C, M. D. Smith: "Improving accuracy tatic branch prediction using branch correlation," Proc. 6th Int. Conference Architectural Support Programming Languages Operating Systems (ASPLOS-VI), 1994, pp. 232-241. HOMEWORK PROBLEMS Problems 5.1 5.6 displayed code follows steps elements two arrays (A[] B[]) concurrendy, e lement, puts larger two values corresponding element third array (C[]). three arrays length N.282 MODERN PROCESSOR DESIGN instruction set used Problems 5. 1 5.6 follows: add rd, rs, r addi rd, rs, lw rd, offset(base) sw rs, offset(base) bge rs, rt, address bit rs, rt, address b addressrd < — r + r rd < — r + imm rd 4- MEM[offset+base] (offset = , base = reg) MEM[offset+base] «- rs (offset = , base = reg) (rs >= rt) PC <— address (rs < rt) PC <- addressPC <- address Note: rO hardwired 0. benchmark code f ollows: Static Inst# Label Assembly .Instruction main: 1 addi r2. rO, 2 addi r3. rO, B 3 addi r4. rO, C 4 addi r5. rO, N 5 add rlO, rO, rO 6 loop:bge rlO, r5. end 7 lw r20, 0(r2) 8 lw r21. 0(r3) 9 bge r20. r21. Tl 10 sw r21. 0(r4) 11 Tl:b T2 12 T2:sw r20. 0(r4) 13 addi no. rlO, 1 14 addi r2. r2. 4 15 addi r3. r3. 4 16 addi r4, r4. 4 17 bit no. r5. loop end: PS.l Identify basic blocks benchmark code listing static instructions belonging basic block following able. Number basic blocks based lexical ordering code. Note: may boxes basic blocks.  SUPERSCALAR TECH NIQUES 283 Bask Block No. 4 5 6 Instr. nos. P5.2 Draw control flow graph benchmark. P5.3 generate instruction execution trace (i.e., sequence basic blocks executed). Use following arrays input pro- gram, trace co de execution recording number basic block executed. N = 5 ; A[] = { 8 , 3 , 2 , 5 , 9 ) ; B [ ] = { 4 , 9 , 8 , 5 , 1 ) ; P5.4 Fill Tables 5.1 5.2 based data generated Problem 5.3. Table 5.1 Instruction mix Static Dynamic Instr. Class Number % Number % ALU Load/store Branch Table 5.2 Basic block/branch data* Static Dynamic Average basic block size (no. instr.) Number taken branches Number not-taken branches "Count unconditional branches taken branches. P5.5 Given branch profile information collected Problem 5.4, rearrange basic blocks reverse sense branches program snippe minimize number taken branches pack284 MODERN PROCESSOR DESIGN code frequently executed paths placed together. Show new program. P5.6 G iven new program wrote Problem 5.5, recompute branch statistics last two rows Table 5.2. Problems 5.7 5.13 Consider following code segment within loop body Problems 5.7 5.13: (X EVEN) «- (branch bl) INCREMENT «—(bl taken) (X MULTIPLE 10) <~(branch b2) INCREMENT b «-(b2 taken) Assume following list n ine values x processed nine itera- tions loop. 8,9,10,11,12,20,29, 30,31 Note: Assume predictor entries updated dynamic branch next dynamic branch accesses predictor (i.e., update delay). Branch history table bl b2= NT = PS.7 Assume 1-bit (history bit) state machine ( see above) used prediction algorithm predicting execution two branches loop. Indicate predicted actual branch direc- tions bl b2 branch instructions iteration loop. Assume initial state 0, i.e., NT, predictor. 8 9 10 11 12 20 29 30 31 bl predicted bl actualb2 predicted b2 actual P5.8 prediction accuracies bl b2?SUPERSCA LAR TECHNIQUES 285 PS.9 overall prediction accuracy? P5.10 Assume two-level branch prediction scheme used. addition 1-bit predictor, 1-bit global register (g) used. Register g stores direction la st branch executed (which may branch branch currentiy predicted) used ndex two separate 1-bit branch history tables (BHTs) shown following figure. Depending value g, one two BHTs selected used normal 1-bit prediction. Again, fill p redicted actual branch directions bl b2 nine iterations loop. Assume initial value g = 0, i.e., NT. prediction, depending current value g, one two BHTs accessed updated. Hence, entries fo llowing table empty. Note: Assume predictor entries updated dynamic branch next dynamic branch accesses predictor (i.e., update delay). 8 9 10 11 12 20 29 30 31 Forg = 0: bl predicted bl actual b2 predicted b2 actual g = 1: bl predicted bl actual b2 predicted b2 actual PS.ll prediction accuracies bl b2? PS.12 overall prediction accuracy? P5.13 prediction accuracy b2 g = 0? Explain why.286 MODERN PROCESSOR DESIGN P5.14 figure shows control flow graph simple program. CFG annotated three different execution trace paths. execution trace, circle branch predictor (bimodal, local, gse- lect) best predict branching behavior given trace. b§5Co?) Circle one: p ^ Bimodal Local GselectSUPERSCALAR TECHNIQUES 287 one predictor may perform equally well particular trace. However, use three predictors exactly choosing best predictors three traces. Circle choice thre e traces add. (A ssume trace executed many times every node CFG conditional branch. branch history register local, global, gsel ect predictor limited 4 bits.) Problems 5.15 5.16: Combining Branch Prediction Given combining branch predictor two -entry direct-mapped bimodal branch direction predictor, gshare predictor 1-bit BHR two PHT entries, wo-entry selector table, simulate sequence taken not-taken branches hown rows.of table Problem 5.15, record prediction made predictor branch resolved well change predictor entries branch resolves. Branch address BimodalGshare Selector7 Use following assumptions:  Instructions fixed 4 b ytes long; hence, two low-order bits branch address shifted indexing predictor. Use next lowest-order bit index predictor.  predictor selector entry saturating up-down 2-bit Smith counter initial states shown.  taken branch (T) increments predictor entry; not-taken branch (N) decrements predictor entry.  predictor entry less 2 (0 1) results not- taken (N) prediction.  predictor entry greater equal 2 (2 3) results taken (T) prediction.288 MODERN PROCESSOR DESIGN  selector value 0 1 selects bimodal predictor, selector value 2 3 selects gshare predictor.  None predictors tagged branch address.  Avoid destructive interference updating "wrong" predictor when- ever predictor right. P5.15 Fill following table prediction outcomes, predictor state following resolution branch. Branch | tad com* (tut) Bimodal tractor PradtcMf GfhartPracfBunch Addrasi (TNT) Bimodaltad com* (tut) Bimodal tractor PradtcMf Bunch Addrasi (TNT) Bimodal Gshara Combined PHT0 PHTt PMT0 PHT1 BHR PHTO PHT1 Initial N/A N/A N/A N/A 2 0 0 2 0 2 0x654 N 0x780 0X78C 0x990 OxA04 N 0X78C N P5.16 Compute overal l branch prediction rates (number correctly pre- dicted branches / total number p redicted branches) bimodal, gshare, final (combined) predictors. P5.17 Branch predictions resolved branch instruction executes. Early out-of-order processors like PowerPC 604 si mplified branch resolution forcing branches execute strictly program order. Discuss simplifies branch red irect logic, expl de tail microarchitecture must change accommodate out-of-order branch resolution. P5.18 Describe scenario out-of-order branch resolution would important performance. State hypothesis describe set experiments validate hypothesis. Optionally, modify timing simulator conduct experiments. Problems 5.19 5.20: Register Ren aming Given DAXPY kernel shown Figure 5.31 IBM RS/6000 (RIOS-I) floating-point load renaming scheme also discussed class (both shown theSUPERSCALAR TECHNIQUES 21 following figure), simulate execution two iterations DAXPY loop show state th e floating-point map table, p ending target return queue, free list.  Assume initial state shown table Problem 5.19.  Note table contains column registers th referenced DAXPY loop.  RS/6000 implementation discussed, assume single load instruction renamed per cycle single floating-point instruction complete per cycle.  floating-point load, mul tiply, add instructions shown table, since relevant renaming scheme.  Remember load destination registers renamed.  first load loo p prologue filled you. OP Si S2 S3 FAD 3 2 1 OP SI S2 S3 FAD 3 2 1 Head Free list Tail Map table 32x« Y(i)-A*X(i) + Y(i) F0 <— LD, R4 «- ADDI,Rx,#512 Loop: F2 «- LD, 0 (Rx) F2 <- MULTD, F0 , F2 F4 «- LD,0(Ry) F4 «- ADDD,F2,F4 0(Ry) <- SD, F4 Rx «- ADDI,Rx,#8 Ry «- ADDI,Ry,#8 R20 «— SUB,R4,Rx BNZ,R2 0,Loop32 33 34 353637 38 39 | Pending target return queue ast address load X(i) A*X(i) load Y(i) A*X(i) + Y(i) store Y(i) inc. index X inc. index compute bound check doneHead release tail LD dULTDj LD290 MODERN PROCESSOR DESIGN P5.19 Fill remaining rows following table map table staie pending target return queue state instruction renamed, free list state instruction completes. Floating-Point Instruction Initial state FO <= LD, F2<=LD, 0(Rx) F2«=MULTD,F0,F2 F4<=LD.0(Ry) F4<=ADDD,F2,F4F2<=LD,0(Rx)F2«=MULTD,F0, F2 F4<=LD, 0(Ry) F4<=ADDD,F2,F4Pending Target Return Queue Instruction RenamedFree List Instruction Completes 32,33,34,35,36,37,38,39 33,34,35,36,37,38,39 P5.20 Given RS/6000 rename floating-point load parallel floating-point ari thmetic instruction (mult/add), assuming map table write-before-read structure, internal bypassing needed within map table? Explain not. P5.21 Simulate execution following code snippet using Tomasulo's algorithm. Show content reservation station entries , register file busy, tag (the tag RS ID number), data fields cycle (make copy table shown next page cycle simulate). Indic ate instruction executing functional unit cycle. Also indicate result forwarding across common data bus circling producer consumer connecting wit h arrow. i: R4<-R0 + R8 j: R2<-R0*R4 k: R4<-R4 + R8 1 : R8<-R4*R2 Assume dual dispatch dual common data bus (CDB). Add latency two cycles, multiply latency three cycles. instruc- tion begin execution cycle dispatched, assum- ing dependences satisfied.SUPERSCALAR TECHNIQUES 291 P5.22 Determine whether code executes data-flow limit Problem 5.21. Explain not. Show work. CYCLE #: ID AdderID 4 5Tag Sink Tag Source Busy Tag Data 0 6.0 2 3.5 MuUTDiv4 10.0 8 7.8 8 7.8 DISPATCHED INSTRUCTION(S): P5.23 presented chapter, load bypassing technique enhancing memory data flow. load bypassing, load instructions allowed jump ahead earlier store instructions. address generation done, store instruction completed architecturally enter store buffer await available bus cycle writing memory. Trailing loads allowed bypass stores store buffer address aliasing. problem simulate load bypassing (there load forwarding). give n sequence load/store instructions addresses (symbolic). number left instruction indicates cycle whic h instruction dispatched reservation station; begin execution cycle. store instruction additional number right, indicating cycle ready retire, i.e., exit store buffer write memory. Use following assumptions:  operands needed address calculation available dispatch.  One load one store addresses calculated per cycle.  One load e store executed, i.e., allowed access cache, per cycle.  reservation statio n entry deallocated cycle address calculation issue.  store buffer entry deallocated cache accessed.  store instruction access cache cycle ready retire.  Instructions issued order reservation stations.  Assume 100% cache hits.S92 MODERN PROCESSOR DESIGN Load/store unit Example: Dispatch cycle Instruction 1 Load 1 LoadB 1 Store C Retire cycleLoad RSStore RS b Address 1 unit Store buffer Cache address Cache write data Load Store Reservation Reservation Cycle Station Station 1 Ld l_d B C 2 Ld B 3 4 5 6Cache Cache Write Store Buffer Address Data StC L StC Ld B StC StC St C data Load/store unit Code: Dispatch cycle 1 2 3 4 44 5Instruction Store LoadB Load Store Load E Load LoadDRetire cycle 6 10Load RS Address unitStore RS Address unit TStore bu ffer 3—R :F Cache address Cache write dataSUPERSCALAR TECHNIQUES 29: Load Store Cache Reservation Rese rvation Cache Write Cycle Station Station Store Buffer Address Data 2 3 4 5 6 7 8 9 10 1112131415 P5.24 one two sentences compare contrast load forwarding load bypassing. P5.2S Would load forwarding improve performance code sequence Problem 5.23? not? Problems 5.26 5.28 goal lockup-free cache designs increase amount concurrency parallelism pro cessor overlapping cache miss handling pro- cessing cache mi sses. problem, assume following simple workload running processor primary cache 64-byte lines 16-byte memory bus. Assume fmiss 5 cycles, rtransfer 16-byte sub- block 1 cycle. Hence, simple blocking cache, amiss take fmiss + (64/16) x 'transfer = 5 + 4 x 1 = 9 cycles. workload: for(i=l;i<10000;++i) + = A[i] + B[i]; RISC assembly language, assuming r3 points A[0] r4 points B[0]: li r2,9999 # load iteration count r2 loop: lfdu r5,8(r3) # load A[i] , incr. pointer r3 lfdu r6,8(r4) # load B[i], incr. pointer r4294 MODERN P ROCESSOR DESIGN add r7,r7,r5 # add A[i] add r7,r7,r6 # add B[i] bdnz r2,loop # decr ement r2, branch zero timing diagram loop body ass uming neither array hits cache cache simple blocking design (m = miss latency, = transfer latency, = load A, B = load B, = add, b = branch): Cycl« 0123456789 0123458789 0 1 2 3 4 6 7 8 9 Array mmmmmt ArrayB mmmmmt B aabABaab ~T~ Problems 5.26 5.28 assume instruction takes single cycle execute subsequent instructions ar e stalled cache miss requested data returned cache (as shown timing dia- gram). Furt hermore, assume portion either array (A B) cache initially, must fetched demand misses. Als o, assume enough loop iterations fil l table entries provided. P5.26 Assume blocking cache design critical word forwarding (i.e., requested word forwarded soon transferred), support single outstanding miss. Fill timing diagram explain work. Cycta 0123456789 0 1 2 Array ArrayB Execution * 2 * 3456769 0 1 234567 « # 0123456789 01234 5 6 7 9 0123456789 Array Array B ExecutionSUPERSCALAR TECHNIQUES 29! P5.27 fill timing diagram lockup-free cache supports multiple outstanding misses, explain work. 01234507 8 9 0 1 2 3456789 0 1 2 345078* Array ArrayB Execution -!"S~^* 0 1 2 3 4 5 6 > 8 9 0 1 23456 7 8 90123456781 Array Array B Execution P5.28 Instead 64-byte cache line, assume 32-byte line size cache, fill timing diag ram Problem 5.27. Explain work 0123456789 0123456789 01 23456781 Array Array B Execution 01234567S9 0123456789 012345678' Array ArrayB Execution Problems 5.29 5.30 goal prefetching nonblocking cache designs increase amount concurrency parallelism processor overlapping cache miss handling processing cache misses. prob lem, assume simple workload Problem 5.26 running processor primary cache 16-byte lines 4-byte memory bus. Assume 2 cycles and296 MODERN PROCESSOR DESIGN Array Array B Prefetch 1 Prefetch 2 Execution'transfer f°r eacn 4-byt e subblock 1 cycle . Hence, miss take fmiss + (16/4) x 'tranSfer = 2 + 4 x l =6cyCleS. Problems 5.29 5.30, assume instruction takes single cycle execute subsequent instructions stalled cache miss requested data returned cache (as shown table Problem 5.29). Furthermore, assume portion either array (A B) cache ini- tially, must fetched demand mi sses. Also, assume ther e enough loop iterations Fill table entries provided. assume stride-based hardware prefetch mechanism track two independent strided address streams, issues stride prefetch cycle observed stride twice, observing three strided misses (e.g., misses A, + 32, + 64 triggers prefetch + 96). prefetcher issue next strided prefetch cycle foll owing demand reference previous prefetch, sooner (to avoid overwhelming memory subsystem). Assume demand reference always get priority prefetch shared resource. P5.29 Fill following table indicate miss (m) transfer (t) cycles demand misses prefetch requests assumed work- load. Annotate prefetch symbolic addr ess (e.g., + 64). first 30 cycles filled you. 0123456719 0123456 89 012345679.9 Array Array B Prefetch 1 Prefetch 2 Execution 0123456789 0123456789 0123456789 Array Array B Prefetch 1 Prefetch 2 ExecutionSUPERSCALAR TECHNIQUES 29". P5.30 Report overall miss rate portion execution shown Problem 5.29, well cover age prefetches generated (coverage defined as: (number misses eliminated prefetching)/ (number misses without prefetching). P5.31 victim cache used augment direct-mapped cache reduce conflict misses. additional background problem, read Jouppi's paper victim caches [Jouppi, 1990]. Please fill follow- ing table reflect state cache line four-entry direct- mapped cache two-entry fully associative victim cache following memory reference shown. Also, record whether th e reference cache hit cache miss. refe rence addresses shown hexa- decimal format. Assume direct-mapped cache indexed low-order bits 16-byte line offset (e.g., address 40 maps set 0, address 50 maps set 1). Use dash (—) indicate invalid line address th e line indicate valid line. Assume LR U policy victim cache mark LRU lin e table. Reference Address [mit state] 80 AO 200 80 BOEO 200 BO 200HK/Miss LhwOIXract afiparf Cacha Unci Unc-2 noUna 3 FF0VktbnCacha LhwO Unc-1 1F0 210/LRU P5.32 Given results Problem 5.31, excluding data provided processor supply requested instruction wor ds, compute total number bytes transferred direct- mapped cache array. Assume read-only instruction cache; hence, writebacks dirty lines. Show work. P5.33 Fill details th e block diagram read-only victim I-cache design shown following figure (based victim cache out- lined Problem 5.31). additional background problem, read Jouppi's paper victim caches [Jouppi, 1990]. Show data and. address paths, identify address bits used indexing tag comparison, show logic gener ating hit/miss signal well control logic multiplexers included design. forget data p aths "swapping" cache lines the298 MODERN PROCESSOR DESIGN victim cache DM cache. Assume arrays read first half eac h clock cycle written second half clock cycle. include LRU update control data paths handling misses. Tag Index Offset Fetch address: l^^^^^^^j ^ (<= *n n w many bits each) LRU Info: | Write Port Write Port Write Port DM Tag Array Victim TagO Victim Tagl Row ID Read Port Read Port Read Port 00 Write Port Write Port Write Port DM Data Array Row IDVictim DataO Victim Datal Read Port Read Port Read Port Data out: £~ Hit/Miss: [> | Problems 5.34 5.36: Simplescalar Simulation Problems 5.34 5.36 require use modify Simplescalar 3.0 simu- lator suite, available http://www.sunplescalar.com . Use Simplescalar simulators execute four benchmarks instructional benchmark suite http://www.simplescalar.com . P5.34 First use th e sim-outorder simulator default machine parame- ters simulate out-of-order pr ocessor performs right-path wrong-path speculative references instruction cache, data cache, unified level 2 cache. Report struction cache, data cache, 12 cache miss rates (misses pe r reference) well total number references total number misses caches. P5.35 simulate exact memory hierarchy Problem 5.34 using sim-cache simulator. Note determine theSUPERSCALAR TECHNIQUES 299 parameters use invoke sim-cache. Report statis- tics Problem 5.34, compute increase (or decre ase) statistic. P5.36 modify sim-outorder.c inhi bit cache misses caused wrong- path references. easy instruction fetches sim- outorder, ince global variable "spec_mode" set whenever processor begins execute instructions inco rrect branch path. use global flag inhibit instruction cache misses wrong path instruct ion fetches. data cache misses, check "spec_mode" flag within RUU entry inhibit data cache misses instruction . "Inhibiting misses" means even bother check cache hierarchy; simply tr eat refe rences hit cache. find places cache accessed searching calls function "cache_access" within sim-outorder.c. recollect statistics Problem 5.34 modified simula- tor compare results Problem 5.35. results still differ Problem 5.35, expbjn might case.JCHAPTER Trung A. Diep PowerPC 620 CHAPTER OUTLINE <p btffcoW** - 6.1 INTRCKJUCTIORI 6.2 Experimental Framework 63 Instruction Fetching 6.4 Instruction Dispatching 65 Instruction Execution 6.6 Instruction Compl etion 6.7 Conclusions Observations 6.8 Bricigingtothel^ 6.9 Summary  j References Homework Problems PowerPC family microprocessors includes 64-bit PowerPC 620 micro- processor. 620 firs 64-bit superscalar processor employ true out-of- order execution, aggressive branch prediction, distributed multientry reservation stations, dynamic renaming registe r files, six pipelined execution units, completion buffer ensure precise exceptions. features previously implemented single-chip microprocessor. actual effectiveness great interest academic researchers well industry designers. chapter pr esents instruction-level, machine -cycle lev el, performance evalua- tion 620 microarchitecture using VMW-generated performance si mulator 620 (VMW Visualization-based Microarchitecture Workbench Carnegie Mellon University) [Levitan et al., 1995; Diep et al., 1995]. also describe IBM POWER3 POWER4 designs, highlight differ predecessor PowerPC 620 . fu ndamentally 301302 MODERN PROCESSOR DESIGN similar aggressively extract instruction-le vel parallelism sequen- tial code, di fferences 620, POWER3, POWER4 designs help highlight recent trends processor implementation: increa sed memory bandwidth aggre ssive cach e hierarchies, better branch prediction, execution resources, deeper pipelining. 6.1 Introduction PowerPC Architecture result PowerPC alliance among IBM, Motorola, Apple [May et al., 1994]. based Performance Optimized Enhanced RISC (POWER) Architecture, designed f acilitate parallel instruc- tion execution scale well advancing technology. PowerPC alliance released announced number chips. first, provided transition POWER Architecture PowerPC Architecture, PowerPC 601 microp rocessor [IBM Corp., 1993]. secon d, low-power chip, PowerPC 603 micro processor [Motorola, Inc., 2002]. Subsequendy, advanced chip desktop systems, PowerPC 604 microprocessor, shipped [IBM Corp., 1994]. fourth chip 64-bit 620 [Levitan et al., 1995; Diepetal., 1995]. recendy, Motorola IBM pursued independent development general-purpose PowerPC-compatible parts. Motorola focused 32-bit desk- top chips Apple, IBM concentrate server parts Unix (AIX) business (OS/400) systems. Recent 32-bit Motorola designs, detailed here, PowerPC G3 G4 designs [Moto rola, Inc., 2001; 2003]. 32-bit parts derived PowerPC 603, short pipelines, limited execution resources, low cost. IBM's server parts included in-order multi- threaded Star series (Northstar, Pulsar, S-Star [Storino et al., 1998]), well out-of-order POWER3 [O'Connell White, 2000] POWER4 [Tendler et al., 2001]. addition, Motorola D3M hav e developed various PowerPC cores embedded marketplace. focus chapter PowerPC 620 heirs high-performance end marketplace , POWER3 andthePOWER4. PowerPC Architecture 32 general-purpos e registers (GPRs) 32 floating-point registers (FPRs). also condition register addressed one 32-bit register (CR), register file 8 four-bit fields (CRFs), 32 single-bit fields. architecture count register (CTR) link register (LR), primarily used branch nstructions, integer exception register (XER) floatin g-point status control register (FPSCR), used record exception status ppropriate instruction types. Th e PowerPC instructions typ ical RISC instructions, ihe addition floating-point fused multiply-add (FMA) instructions, load/s tore instructions addr essing modes update effective address, instructions set, anipulate, branch condition register bits. 620 four-wide superscalar machine. uses aggre ssive branc h prediction fetch instructions early possible dispatch polic distribute thoseTHE POWERPC 620 30: Completion unit Completion buffer (16 entries)Fetch unit BTAC (256 entries)Instruction buffer (8 entries) Dispatch unit (2,048 entries) Reservation station (2 entries) + xsuo (1 stage)Reservation station (2 entries) ^3i XSUl (1 stage)Reservation ' station (2 entries) ~T-Reservation station (3 entries)Reservation station (2 entries) MC-FXU ( > 1 stage)LSU (2 stages)FPU (3 stages) GPR file (32 registers)Rename buffers (8 entries)FPRfUe (32 registers)Rename buffers (8 entries)CRFfile (8 fields)Rename buffers (16 entries)Reservation station (4 entries) —T~* BRU CTR LR Figure 6.1 Block Diagram PowerPC 620 Microprocessor. instructions execution units. 620 uses six parallel execution units: two simple (sing le-cycle) integer units, one complex (multicycle) integer unit, one floating-point unit (three stages), one load /store unit (two stages), branch unit. 620 uses distributed reservation stations register renaming imple- ment out-of-order execution. Th e block diagram 620 shown Figure 6.1. 620 processes instructions five major stages, namely fetch, dispatch, execute, complete, writeback stages. Som e stage separated buffers take slack dynamic va riation available parallelism. buffers instruction buffer, reservation stations, th e completion buffer. pipeline stages buffers shown Figure 6.2. units execute stage actually multistage pipelines. Fetch Stage. fetch unit accesses instruction cache fetch four instructions per c ycle instruction buffer. end jache line taken branch prevent fetch unit fetching four useful instructions cycle. mispredicted branch waste cycles fetching wrong path. Dur- ing fetch stage, preliminary branch prediction made using branch target address cache (BTAC) obtain target address fetching next cycle. Instruction Buffer. instruction buffer holds instructions fetch dispatch stages. dispatch unit cannot keep fetch unit, instruc- tions buffered dispatch unit pr ocess them. maximum eight304 MODERN PROCESSOR DESIGN Fetch stage Instruction buffer (8) | Dispatch tage' 1 1 1 xsuo XSU1 MC-FXULSU 3 Ba 3   Reservation stations (6) Execute stage(s) Completion buffer (16) Complete stage Writeback tage Figure 6.2 Instruction Pipeline PowerPC 620 Microprocessor.FPU r BRU instructions buffered time. Instructions buffered shifted groups two simplify logic. Dispatch Stage. dispatch unit decodes instructions instruction buffer checks whether dispatched reservation stations. dispatch conditions fulfilled instruction, dispatch stage allocate reserva- tion station ent ry, completion buffer entry, entry rename buffer destination, needed. six execution units accept one instruc- tion per cycle. Certain frequent serialization c onstraints also stall instruction dispatch. four instructions dispatched program rder per cycle. eight integer register rename buffers, eight floating-point register rename buffers, 16 condition register field rename buffers. count register link register one shadow register each, used renaming. dispatch, appropriate buffers alloc ated. source operands renamed previous instructions marked tags associ- ated rename buffers. source operand available instruction dispatched, approbate result busses forwarding results watched obtain operand data. Source operands renamed previous instructions read architected register files. branch dispatched, resolution branch tempted immedi- ately. resolution still pending, is, branch depends operand yet available, predicted using branch h istory table (BHT). pre- diction made BHT disagrees prediction made earlier BTAC fetch stage, BTAC-based prediction discarded fetching proceeds along direction predicted BHT. Reservation Stations. execution unit ex ecute stage associ- ated reservation tation. execution unit's reservation station holds thoseTHE POWERPC 620 301 instructions waiting execute there. reservation station hold two four instruction entries, depending execution unit. dispatched instruction waits reservation station source operands read forwarded execution unit available. Instructions leave reservatio n stations issued execution units order [except FPU branch unit (BRU)]. Execute Stage. major stage ca n require multiple cycles produce results, depending type instruction executed. lo ad/store unit two-stage pipeline, floating-point unit hree-stage pipeline. end execution, instruction results sent destination rename buffers forwarded waiting instructions. Completion Buffer. 16-entry completion buffer records state in- flight instructions architec turally complete. entry allocated instruction th e dispatch stage. execute stage marks instruction finished unit done executing instruction. instruction finished, eligible completion. Complete Stage. completion stage, finished instructions removed completion buffer order, four time, passed write- back stage. Fewer instructions complete cycle insufficient number write ports architected register files. holding instructions completion buffer writeback, 620 guarantees architected registers hold correct state recendy completed instruction. Hence, precise exception maintained e ven aggressive out-of-order execution. Writeback Stage. stage, writeback logic retires instruc- tions completed previous cycle commi tting results rename buffers architected register files. 6.2 Experimental Framework performance simulator 620 implemented using VMW frame- work developed Carnegie Mellon University. five machine specification files 620 gene rated based design documents provide periodi- cally updated 620 design team. Correc interpretation design docu- ments checked member design team series refinement cycles 620 design finalized. Instruction data traces generated existing P owerPC 601 micro- processor via software instrumentation. Traces several SPEC 92 benchmarks, four integer three floating-point, generated. benchmarks dynamic instruction mixes shown Table 6.1. integer nchmarks similar ins truction mixes; li contains multicycle instructions rest. instructions move values special-purpose registers. greater diversity among f loating-point benchmarks. Hydro2d uses nonpipelined floating-point instructions. instructions floating- point divides, require 18 cycles 620.306 MODERN PROCESSOR DESIGN Table 6.1 Integer Benchmarks <SPECInt«|HMeaH(SPECfp92) Instruction Mix1Instruction Mix compress eqntott espresso tf aMm hydro2d tomcat* Integer Arithmetic 42.73 48.79 48.30 2954 37.50 2625 19.93 (single cycle) Arithmetic 0.89 1.26 1.25 5.14 029 1.19 0.05 (multicycle) Load 25.39 2321 24.34 28.48 0.25 0.46 0.31 Store 16.49 6.26 829 18.60 0.20 0.19 0.29 Floating-point Arithmetic 0.00 0.00 0.00 0.00 12.27 26.99 37.82 (pipelined) Arithmetic 0.00 0.00 0.00 0,00 0.08 1.87 0.70 (nonpipelined) Load 0.00 0.00 0.00 0.01 26.85 22.53 27.84 Store 0.00 0.00 0.00 0.01 12.02 7.74 9.09 Branch Unconditional 1.90 1.87 132 3.26 0.15 0.10 0.01 Conditional 12.15 17.43 15.26 12.01 10.37 12.50 3.92 Conditional 0.00 0.44 0.10 0.39 0.00 0.16 0.05 count register Conditional 4.44 0.74 0.94 2.55 0.03 0.01 0.00 link register "Values given ar e percentages. Trace-driven performance simulation used. trace-driven simulation, instructions variable latency integer mul tiply/divide floating- point divide cannot simulated accurately. instructions, assume minimum latency. frequency operations amount var iance latencies quite low. Furthermore, traces contain instructions actually executed. speculative instructions later discarded due misprediction included simulation runs. I-cache D-cache activities included simulation. caches 32K bytes 8-way set-associative. D-cache two-way interleaved. Cache miss latency eight cycles perfect u nified L2 cache also assumed.THE POWERPC 620 3 Table 6.2 Summary benchmark performance Benchmarks Dynamic Instructions Execution Cycles IPC compress 6,884,247 6,062,494 1.14 eqntott 3,147,233 2,188,331 VA4 espresso 4,615,085 3,412,653 ~ ~135~ li 3,376,415 3,399,293 0799 alvinn 4,861,138 2,744,098 -1.77 hydro2d 4,114,602 4,293,230 0.96 tomcatv 6,858,619 6,494,912 1.06 Table 6.2 presents total number instructions simulated bench- mark total number 620 machine cycles required. sustained average number instructions per cycle (IPC ) achieved 620 benchmark also shown. IPC rating reflects overall degree instr uction-level parallelism achieved 620 microarchitecture, detailed analysis presented Sections 6.3 6.6. 6.3 Instruction Fetching Provided instru ction buffer saturated, 620's fetch unit capable fetching four instructions every cycle. fetch unit wait branch resolution continuing fetch non speculatively, bias naively branch-not-taken, machine execution would drastically slowed bottleneck fetching taken branches. Hence, accurate branch prediction crucial keeping wide superscalar processor busy. 6.3.1 Branch Prediction Branch prediction 620 takes place two phases. first prediction, done fetch stage, uses BTAC provide preliminary guess target address branch encountered instruction fetch. second, accurate, prediction done dispatch stage using BUT, contains branch history makes predictions based two history bits. dispatch stage , 620 attempts resolve immediately branch based available information. branch unconditional, condition register appropriate bits ready, branch prediction necessary. branch executed immediately. hand, source condition register bits unavailable instruction generating finished, branch prediction made using BHT. BHT contains two history bits per entry accessed dispatch stage predict whether branch taken taken. Upon resolution th e predicted branch, actual direction branch updated BHT. 2048-entry BHT direct-mapped table, unlike BTAC, associative cache. concept 308 MODERN PROCESSOR DESIGN hit miss. two branches update BHT exact multiple 2048 instructions apart, i.e., aliased, affect ot her's predictions. 620 resolve predict branch dispatch stage, even incur one cycle delay new target branch fetched. reason, 620 makes preliminary prediction fetch stage, base solely address instruction currently fetching. one addresses hits BTAC, th e target address stored BTAC used fetch address next cycle. BTAC, smaller BHT, 256 entries two-way et-associative. holds targets branches predicted taken. Branches predicted taken (fall through) stored BTAC. unconditional PC-relative condi- tional branches use BTAC. Branches count register link register unpredictable target addresses never stored BTAC. Effectively, branches always predi cted taken BTAC fetch stage. link register stack, stores addresses subroutine returns, used predicting condit ional return instructions. link register stack modeled simulator. four possible cases BTAC prediction: BTAC miss branch taken (correct prediction), BTAC miss branch taken (incorrect p rediction), BTAC hit taken branch (correct prediction), BTAC hit not-taken branch (incorrect prediction). BTAC never hit taken branch get wr ong target address; PC-relative branches hit BTAC therefore must always use target address. Two predictions made branch, BTAC fetch stage, another BHT dispatch stage. BHT prediction dis- agrees BTAC prediction, BHT prediction used, BTAC prediction discarded. predictions agree c orrect, al l instructions speculatively fetched used penalty incurred. combining possible pr edictions resolutions BHT BTAC, six possible utcomes. general, predictions made BTAC BHT strongly correlated. small fraction th e time wrong prediction made BTAC corrected righ prediction BHT. unusual possibility correct prediction made BTAC undone incorrect prediction BHT. However, cases quite rare; see Table 6.3. BTAC makes early prediction without using branch history. hit BTAC effectively implies branch predicted taken. miss BTAC implicitly means not-taken prediction. BHT prediction based branch history accurate potentially incur one-cycle pen- alty prediction differs made BTAC. BHT tracks branch history updates entries BTAC. reason strong correlation two predictions. Table 6.3 summarizes branch prediction statistics th e benchmarks. BTAC prediction accuracy integer nchmarks ranges 75% 84%. floating-point benchmarks ranges 88% 94%. correct predictions BTAC, branch pena lty incurred lik ewise predictedTHE POWERPC 620 30 Table 6.3 Branch prediction data* BranchHI BVSSBB9BBSB89 naaasjaaaHi Processing compress etfrifoft *sprt$so « aMnnIn ii Tit' Cwncotv Branch resolution taken 40.35 31.84 40.05 33.09 6.38 17.51 6.12 Taken 59.65 68.16 59.95 66.91 93.62 82.49 93.88 BTAC prediction Correct 84.10 82.64 81.99 74.70 94.49 88.31 93.31 Incorrect 15.90 17.36 18.01 25.30 5.51 11.69 6.69 BHT prediction Resolved 19.71 18.30 17.09 28.83 17.49 26.18 45.39 Correct 68.86 72.16 7227 62.45 81.58 68.00 52.56 Incorrect 11.43 9.54 10.64 8.72 0.92 5,82 2.05 BTAC incorrect 0.01 0.79 1.13 "7.78 0.07 0.19 ~ 0.00 BHT correct BTAC correct 0.00 0.12 0.37 0.26 0.00 0.08 0.00 BHT incorrect Overall branch 88.57 90.46 89.36 91.28 99.07 94T8~ 97.95 prediction accuracy "Values given percentages. correctly BHT. ov erall branch prediction accuracy etermined BHT. integer b enchmarks, 17% 29% branches resolved time reach dispatch stage. floating-point benchmarks, range 17% 45%. overall isprediction rate integer benchmarks ranges 8.7% 11.4%; whereas floating-point benchmarks ranges 0.9% 5.8%. e xisting branch prediction mechanisms work quite well floating-point benchmarks. still room improvement integer benchmarks. 6.3.2 Fetching Speculation main purpose branch prediction sustain high instruction fetch band- width, turn keeps rest superscalar machine busy. Misprediction translates wasted fetch cycles reduces effective instruction fetch band- width. Another source fetch bandwidth loss due I-cache misses. effects two impediments fetch bandwidth benchmarks shown Table 6.4. Again, integer benchmarks, significant percentages (6.7% 11.8%) fetch cycles lost due misprediction. benchmarks, I-cache misses resulted loss less 1% fetch cycles.310 MODERN PROCESSOR DESIGN Table 6.4 Zero bandwidth fetch cycles* Benchmarks Misprediction l-Cache Miss compress 6g5 001 eqntott 11.78 0.08 espresso 10.84 0.52 li 8.92 0.09 atvinn 0.39 0.02 hydro2d 5.24 0.12 tomcatv 0.68 0.01 "Values given percentages. Table 6.5 Distribution average number branches bypassed* Number Bypassed Bran Benchmarks 9 1 1 3 - 4 Average compress 66.42 27.38 5.40 0.78 0.02 0.41 eqntott 48.96 2827 " 20.93 1.82 0.02 0.76 espresso 53.39 29.98 11.97 4.63 0.03 0.68 li 63.48 25.67 7.75 2.66 0.45 0.51 alvinn 83.92 15.95" 0.13 0.00 0.00 0.16 hydro2d 68.79 16.90 ~1032 3.32 0.676.50 tomcatv 92.07 2.30 3.68 ~ 155 " 0.00 0.16 "Columns 0-4 show percentage cycles. Branch prediction form speculation. speculation done effec-J tively, increase performance machine alleviating con- straints imposed control dep endences. 620 speculate past four predicted branches stalling fifth branch dispatch stage. Specula- tive instructions llowed move pipeline stages branches resolved, time speculation proves incorrect, speculated instructions canceled. Speculative instructions potentially finish execution reach completion stage prior branch resolution. H owever, allowed complete resolution branch. Table 6.5 displays fr equency bypassing specific numbers branches, reflects degree speculation sustained average number branches bypassed determined obtaining number correctiy predicted branches that'THE POWERPC 620 311 bypassed cycle. branch det ermined mispredicted, spec- ulation instructions beyond branch simulated. integer bench- marks, 34% 51% cycles, 620 speculatively executing beyond one branches. floating-point benchmarks, degree peculation lower. frequency misprediction, shown Table 6.4, rel ated com- bination average number branches bypassed, provided Table 6.5, prediction accuracy, provided Table 6.3. 6.4 Instruction Dispatching primary objective dispatch stage advance instructions ins- truction buffer reservation stations. 620 uses in-order dispatch policy. 6.4.1 Instruction Buffer eight-entry instruction buffer sits fetch stage dispatch stage. fetch stage responsible filling instruction buffer. dispatch stage examines first four entries instruction buffer attempts dispatch reservation stations. instructions dispatched, r emaining instructions instruction buffe r shifted groups two fill vacated entries. Figure 6.3(a) shows utilization instruction buffer profiling fre- quencies havin g specific numbers instructions instruction buffer. instruction buffer decouples fetch tage dispatch stage moderates temporal variations differences fetching dispatching par- allelisms. frequency zero instructions instruction buffer sig- nificantly lower floating-point bench marks integer benchmarks. frequency directly related misprediction frequency shown Table 6.4. end spectrum, instruction buffer saturation cause fetch stalls. 6 2 Dispatch Stalls 620 dispatche instructions checking parallel conditions cause dispatch stall. list cond itions described th e following greater detail. simulation, conditions list checked one time order listed. nce condition causes dispatch instruction stal l identified, checking rest condit ions abor ted, condition identi fied source stall. Serialization Constraints. Certain instructions cause single-instruction serializa- tion. previously dispatched instructions must complete ser ializing instruction begin ex ecution, subsequent instructions must wait serializing instruction finished dispatch. condition, though extremely disruptive performance, quite rare. Branch Wait mtspr. forms branch instructions access count register dispatch stage. move special-purpose register (mtspr) instruction writes count register cause su bsequent dependent branch instructions delay dispatching finished. condition also rare.312 MODERN PROCESSOR DESIGN (a) Instruction (b) Completion !2 2 0 % 10% 8 o% e 20% g. 10% o% S. 20% I. 10% 0% 20% te 10% 0% c 20% | 10% 0% 3 20% 5 10% 0% £ 20% 10% o%JUIIIll1.20% Ill-10% 0% 20%.....IIll.llll 0 4 118   10% 0% 20%0 4 8 12 16 ll.llll llr10% 0%lllhllll lllh.l 0 4 8 12 16 1  20% 1 1 ll10% l.lllllll0%...iillllllln.. 0 4 8 12 16 120% 1 .1 III10% l.lllllll0%- 111111 11  0 4 8 12 16 20% 10% 0% 11,20% l.-.ll110%l.-.llIII0%0 4 8 12 16 ujjjillllllll 0 4 8 12 16 1.1 20% ..llII 10% ^ 0%I llllllll 0 4 8 12 16 Figure 63 Profiles (a) Instruction Buffer (b) Completion Buffer Utilizations. Register Read Port Saturation. seven read ports general- purpose register file four read ports float ing-point register file. Occa- sionally, saturation read ports occurs read port needed none available. enough condition register field read ports (three) satura- tion cannot occur.THE POWERPC 620 3 Reservation Station Saturation. instruction dispatched, instruction placed reservation station instruction's ssociated e xecution unit. instruction remains reservation station un til issued. one reservation station per execution unit, reservation station multiple entries, depending execution unit. Reservation st ation saturation occurs instruction dispatched reservation statio n reservation station empty entries. Rename Buffer Saturation. nstruction dispatched, destination register renamed nto appropriate rename buffer files. three rename buffer files, general-pur pose registers, floating-point registers, condition register fields. general-purpose register file floating- point register file eight rename buffers. con dition register field file 16 rename buffers. Completion Buffer Saturation. Completion buffer entries also allocated dispatch stage. kept u ntil instruction completed. 620 16 completion buffer entries; 16 instructions flight time. Attempted dispatch beyond 16 in-flight instructions cause stall. Figure 6.3(b) illustrates utilization profi les completion buffer benchmarks. Another Dispatched U nit. Although reservation station multi- ple entries, reservation station receive one instruction per cycle even multiple available entries reservation station. Essentially, constraint due fact res ervation stations one write port. 6.4.3 Dispatch Effectiveness average utilization buffers provided Table 6.6. Utilization load/store unit's three reservation station entries averages 1.36 1.73 entries integer benchmarks 0.98 2.26 entries floating-point benchmarks. Unlike execution units, loadVstore unit deallocate reservation sta- tion entry soon instruction issued. reservation tation entry held instruction finished, usually two cycles instruction issued. due potential miss D-cache TLB. res ervation station entries floating-point unit utilized integer units. in-order ssue constraint floating-point unit nonpipeltning floating-point instructions prevent ready instructions issuing. average utilization completion buffer ranges 9 14 bench- marks corresponds average number instructions flight. Sources dispatch stalls summarized Table 6.7 benchmarks. data table percentages cycles executed benchmarks. example, 24.35% compress execution cycles, dispatch stalls occurred; i.e., instructions dispatch buffer (first four entries instruction buffer) dispatched. common significant source bottleneck314 MODERN PROCESSOR DESIGN Table 6.6 Summary average n umber buffers used Buffer Usage compress eqntott instruction buffers (8) 5.41 4.43 Dispatch buffers (4) 3.21 2.75 ~"XSUORS entries (2) 037 0.66 XSU1 RS entries (2) 0.42 0.51 MC-FXU RS entries (2) 0.04 0.07 FPU RS entries (2) 0.00 0.00 " "LSU R entries (3) 1.69 1.36 BRU RS entries (4) 0.45 0.84 GPR rename buffers (8) 2.73 3.70 FPR rename buffers (8) 0.00 0.00 CR rename buffers (16) 1.25 1.32 Completion buffers (16) 10.75 8.83espresso ahflnn hydro2d tome* 4.72 4.65 5.13 5.44 6.42 2.89 2.85 3.40 3.10 3.53 0.68 036 0.48 0.23 0.11 0.65 0.32 0.24 0.17 0.10 0.09 028 0 . 0 1 0.10 0.00 0.00 " doo 0 . 7 0 ' 1.04 0.89 1.60 1.73 2.26 0.98 1.23 0.75 0.59 " 0.19 0.54 0.17 3.25 2.77 3.79 1.83 1.97 0.00 0.00 5.03 2.85 3.23 1.19 0.98 1.27 1.20 0.42 8.75 9.87 13.91 10.10 11.16 Table 6.7 Frequency dispatch stall cycles* { Source* Dispatch StaHs compress eqntott espresso II atvlnn hydro 2d tomcat* . Serialization 0.00 0.00 0.00 0.00 0.00 0.00 0.00 Move special 0.00 4.49 0.94 3.44 0.00 0.95 0.08 register constraint Read port saturation 0.26 0.00 0.02 0.00 0.32 2.23 6.73 Reservation station 36.07 22.36 31.50 34.40 22.81 42.70 36.51 saturation Rename buffer 24.06 7.60 13.93 17.26 1.36 16.98 34.13 saturation Completion buffer 5.54 3.64 2.02 4.27 21.12 7.80 9.03 saturation Another 9.72 20.51 18.31 10.57 24.30 12.01 7.17 unit ~ dispatch stalls 24.35 41.40 33.28 30.06 30.09 17.33 6.35 Values given percentages. benchmarks saturation reservation stations, especially load/store unit. sources dispatch stalls, th e degrees various botdenecks vary among different benchmarks. Satu ration rename buffers significant compress tomcatv, even though average renameTHE POWERPC 620 31 buffers less one-half utilized. Completion buffer saturation highest alvinn, highest frequency 16 entries utilized; see Figure 6.3(b). Contention single write port reservation station also serious bottleneck many benchmarks. Figure 6.4(a) displays distribution dispatching parallelism (the number instruct ions dispatched per cycle). number instructions dispatched cycle range 0 4. distribution indicates fr equency (averaged across entire trace) dispatching n instructions cycle, n = 0,1,2,3,4. benchmarks, least one instruction dispatched per cycle one-half execution cycles. (a) Dispatching 3 30% c.20% 1 10% B 0% 0 1 2 3 4 (b) Issuing (c) Finishing (d) Completion I. j 30%(-«l \ 30%L|| - 30%r|  20% _ 20% II. 20%  . _LllJ sHIli- [III, ] .q%H 1 1. 0 1 2 3 4 5 6 0 1 2 3 4 5 6 30% 20% 10% 0% 0 1 2 3 4 | 30% 6 20% 10% 0% S3 30% 20% S- 10% * 0% 30% = 20% 10% 0% g 30% f 20% "3 10% 0% 20% 1 10% 0% 30% 20% § 10% ~ 0%hi 0 1 2 3 4 III 0 1 2 3 4 0 1 2 3 4 111 0 1 2 3 4 0 1 2 3 4 111 0 1 2 3 4 30% 20% 10% 0% 30% 20% 10% 0%Uli0 1 2 3 4 5 6 30% 20% 10% 0%lill0 1 2 3 4 5 6 30%20% 10% 0%II... 0 1 2 3 4 30% - - 30%  |  LLi 1 ItilL 3 SI Jin 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 |30%f| \ 30%r| \ 30%[ . 20% li. 20% | . 20%  | | 10% | | | _ 10% | | | 10% II- . ~T—T 1 0% Li-Li-" 1 0% LBJLI_« 1 ntJ  0 1 2 3 4 5 6 0 1 2 3 4 5 6 30% 20% 10% 0% 30%20% 10% 0%I j 30% [ JL 3 ?§IjllL0 1 2 3 4 5 6 0 1 2 3 4 5 6 30% 20% 10% 0%0 1 2 3 4 l I, 0 1 2 3 4 |30%r|| \ 30%rl \ 30%11  20% 20% 20%  . II l0% . 107 - 10%  ' 1 1 IE111" 1 0% 1 B 1  1 0%' ' ' * 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 30% 20% 10% 0% 30% - j 30%[l LLL 1 'SHIl-. 1 1H 11.  0 1 2 3 4 5 6 0 1 2 3 4 5 6 2 3 4 Figure 6.4 Distribution Instruction (a) Dispatching, (b) Issuing, (c) Finishing, (d) Completion Parallelisms.116 MODERN PROC ESSOR DESIGN 6.5 Instruction Execution 620 widens execute stage. fetch, dispatch, complete, writeback stages four wide, i.e., advance four instructions per cycle, execute stage contains six execution units issue finish six instructions per cycle. Furthermore, unlike stages, th e execute tage processes instructions order achiev e maximum throughput. 6.5.1 Issue Stalls instructions dispatche reservation stations, must wait source operands become available, begin execution. constraints, however. full list issuing hazards described here. Order Disallowed. Although out-of-order execution usually allowed reservation stations, sometimes case certain instructions may proceed past prior instruction reservation station. Th case branch unit floating-point unit, instructions must issued order. Serialization Constra ints. Instructions read write non-renamed regis- ters (such XER), read write renamed registers non-renamed fash- ion (such load/store multiple instructions), change synchronize machine state (such eieio instruction, enforces in-order execution I/O) must wait prior instructions complete executing. instructions stall reservation stations erialization constraints satisfied. Waiting Source Operand. primary purpose reservation stations hold instructions source operands ready. instruction requires source available, must stall operand forwarded it. Waiting Execution Unit. Occas ionally, two instructions ready begin execution cycle. case, first issued, second must wait. Th condition also applies instruction executing MC-FXU (a nonpipelined unit) floating-point divide instruction puts FPU nonpipelined mode. frequency occurrence four issue stall types summa- rized Table 6.8. data tabulated execution units except branch unit. Thus, in-order issuing restriction concerns floating-point unit. number issue serialization stalls roughly proportional number multicycle integer instructions benchmark's instruction mix. multicycle instructions access special-purpose registers entire condition register non-renamed unit, requi res serialization. issue stalls due waiting execution unit occur load/store unit. load/store instructions ready execute load/stor e execution unit accommodate. Across benchmarks, significant percentages cycles, issuing stalls encountered.THE POWERPC 620 31 7 Table 6 .8 Frequency issue stall cycles* Sources ts.ua St.Rs compress eenfoff espresso f ctvmn Hyaroll temcetr order disallowed 0.00 0.00 0.00 0.00 0.72 11.03 1.53 Serialization 1.69 1.81 3.21 10.81 0.03 4.47 0.01 Waiting fo r source 21.97 2930 37.79 32.03 17.74 22.71 3.52 Waiting execution unit 13.67 3.28 7.06 11.01 150 1.30 issue stalls ~62,67~ 65.61 51.94 46.15 78.70 60.29 93.64 "Values given percentages. 6.5.2 Exec ution Parallelism examine propagation instructions issuing, execution, finish. Figure 6.4(b) shows distribution issuing parallelism (the number instructions issued per cycle). maximum number instructions issued cycle six, number execution units. Although issuing parallelism dispatch parallelism istributions must average value, issuing less centralized fewer constraints therefore achieve consistent rate issuing. cycles, number issued instructions close th e overall sustained IPC, dispatch parallelism extremes distribution. expected distribution finishing parallelism, hown Figure 6.4(c), look like distribution issuing parallelism instruction issued must finish certain number cycles later. Yet always case seen issuing finishing parallelism dis tributions eqntott, alvinn, hydro2d benchmarks. difference comes high frequency load/store floating-point instructions. Since instructions take amount time finish issuing integer nstructions, tend shift issuing parallelism distribution. integer benchmarks, consistent instruction execution la tencies, generally similar- ity ssuing finishing parallelism distributions. 6.5.3 Execution Latency interest examine average latency encountered instruction dispatch finish. issuing constraints satisfied execution unit available, inst ruction dispatched dispatch buffer res- ervation station, issued execution unit next cycle. best case. Frequently, instructions must wait res ervation stations. Hence, overall execution latency includes waiting time reservation station actual latency execution units. Table 6.9 shows average overall execution latency encountered benchmarks six execution units.318 MODERN PROCESSOR DESIGN Table 6.9 Average execution lat ency (in cycles) six execution units benchmarks Execution Unto compress egntott espresso tl aMnn hydro2d tomcat* ^Bwa ^aaawkWBa^aaaa^aa ^a^a^a^aal XSUO (1 stage) 1.53 1.62 1.89 2.28 1.05 1.48 1.01 XSU1 (1 stage) 172 1.73 2.23 2.39 1.13 1.78 1.03 MC-FXU (>1 stage) 4.35 4.82 6.18 5.64 3.48 9.61 1.64 FPU (3 stages)—* —* *5.29 6.74 4.45 LSU (2 stages) 3.56 2.35 2.87 3.22 2.39 2.92 2.75 BRU (>1 stage) 2.71 2.86 3.11 3.28 1.04 4.42 4.14 "Very instructions executed unit benchmarks. 6.6 Instruction Completion instructions finish exe cution, enter completion buffer in-order completion writeback. completion buffer functions reorder buffer reorder out-of-order execution execute stage back sequential order in-order retiring instructions. 6.6.1 Completion Parallelism distributions completion parallelism benchmarks shown Figure 6.4(d). Again, similar dispatching, four instructions com- pleted per cycle. average value computed parallelism distributions. fact, benchmark, average completion parallelism exa ctly equal average dispatching, issuing, finishing parallel- isms, equal sustained IPC benchmark. case instruction completion, instructions allowed finish order, complete order. means occasionally completion buffer wait fo r one slow instruction finish, able retire maximum four instructions once. ccasions, completion buffer saturate cause stalling dispatch stage; see Figure 6.3(b). integer benchmarks co nsistent exec ution latencies usually one instruction completed per cycle. Hydro2d completes zero instructions large percentage cycles must wait floating-point divide instruc- tions finish. Usually, instructions cannot complete they, instructions preceding them, finished yet. However, occasionally reasons. 620 four integer two floating-poin writeback ports. rare run integer register file write ports. However, floating-point write port saturation occurs occasionally. 6.6.2 Cache Effects D-cache behavior direct impact CPU performance. Cache misses cause additional stall cycles th e execute complete stages. D-cache 620 interleaved two banks, address port. load storeTHE POWERPC 620 31 9 instruction use either port. cache service one load one store time. load instruction store instruction access cache cycle acce sses made di fferent banks. cache nonblocking, po rt load access. load cache miss encountered cache line filled, subsequent load instruction proceed access c ache. access results cache hit, instruc- tion proceed without blocked earlier miss. Otherwise, instruction returned reserv ation station. multiple entries load/ store reservation station out-of-order issuing instructions allow servicing load cache hit past three outstanding load cache misses. sequential consistency model memory imposes constraint memory instructions must appear execute order. However, memory instructions ex ecute sequential order, significant amount perfor- mance lost. 620 exec utes store instructi ons, access cache complete stage (using physical address), order; however, allows load instructions, access cache execute stage (using virtual address), bypass store ins tructions. relaxation possible due weak memory consistency model specified PowerPC ISA [May etal., 1994]. store completed, aliasing address loads bypassed finished checked. aliasing detected, machine flushed next load instruction examined completion, refetching load instruction carried out. forwarding data made pend- ing store instruction dependent load instruction. weak ordering mem- ory accesses eliminate unnecessary stall cycles. load instructions beginning dependence chains, earliest possible ex ecution make available othe r instructions earlier execution. Table 6.10 summarizes nonblocking cache effect weak ordering load/store instructions. first line table gives D-cache hit rate benchmarks. hi rate ranges 94.2% 99.9%. non- blocking feature ca che, load bypass another load trailing load cache hit time leading load serviced cache miss. percentage (as p ercentage load instructions) su ch trailing loads actually bypass missed load given second line Table 6.10. store completed, enters complete store queue, waits store writes cache, exits queue. time pending store queue, load potentially access cache bypass store. third line Table 6.10 gives percentage loads that, time load cache access, bypass least One p ending store. loads addresses tha alias addr esses pending stores. percentage loads bypass pending store alias pending store addresses given fourth line table. benchmarks insignifi- cant number aliasing occurrences. fifth line able gives average number pending sto res, number stores store complete queue, in. cycle.320 MODERN PROCESSOR DESIGN Table 6.10 Cache effect data* Cache E ffects compp&s eqntott espresso 0 ahrinn hydro2d tomcatv Loads/stores 94.17 99.57 99.92 99.74 99.99 94.58 96.24 cache hit Loads bypass 8.45 053 0.11 0.14 0.01 4.82 5.45 missed load Loads bypass 58.85 21.05 27.17 48.49 98.33 58.26 43.23 pending store Load aliased 0.00 0.31 0.77 2.59 0.27 0.21 0.29 pending store Average number 1.96 0.83 0.97 2.11 1.30 1.01 1.38 pending stores per cycle Values given percentages, except average number pending stores per cycle. 6.7 Conclusions Observations interesting parts 620 microarchitecture branch prediction mechanisms, out-of-order execution engine, weak ordering memory accesses. 620 reasona bly well branch prediction. floating-^xrint benchmarks, 94% 99% branches resolved correctly predicted, incurring little penalty cycles. Integer ben chmarks yield another story. range drops 89% 91%. sophisticated prediction algorithms, example, usin g history formation, increase prediction accuracy. also clear floating-point integer bench marks exhibit si gnificantly different branching behaviors. Perhaps separate different branch prediction schemes employed dealing two types benchmarks. Even support precise exce ptions, out-of-order execution engine 620 still able achieve reasonable degree instruction-level parallelism, sustained IPC ranging 0.99 1.44 integer bench marks 0.96 1.7 7 floating-point benchmarks. One hot spot lo ad/store unit. number load/store reservation station entries and/or number load/store units needs increased. Although difficulties designing system multiple loa d/store units myriad, load/store bottleneck 620 evident. one floating-point unit three integer units also source bottleneck. integer benchmarks rarely stall integer units, floating-point benchmarks stall waiting floating-point resources. single dispatch reservation station cycle also source dis- patch stalls, reduce number instructions available out-of-order executioa One interesting tradeoff involves choice implementing distributed reservation stations, 620, versus one centralized reservatio n station, Intel P6. former approach permits simpler hardware since onlyTHE POWERPC 620 32 single-ported reservation stations. However, latter share multiple ports reservation station entries among different instruction types. Allowing weak-ordering memory accesses essential achieving high per- formance modern wide superscalar processors. 620 allows loads bypass stores loads; however, p rovide forwarding pending stores dependent loads. 620 allows loads bypass stores check ali asing completing store. store-centric approach makes forwarding difficult quires machine flushed point dependent load aliasing occurs. 620 implement D-cache two interleaved banks, permits concurrent p rocessing one load one store cycle bank conflict. Using standard rule thumb dynamic instruction mix, clear imbalance proc essing load/ store instructions current superscalar proc essors. Increasing throughput load/stor e instructions currently critical cha llenge. future superscalar processors get wider clock speeds increase, memory bottle neck prob- lem exacerbated. Furthermore, commercial applications transaction pro cessing (not characterized SPEC benchmarks) put even greater pressure memory chip I/O bottleneck. interesting examine superscalar introductions contemporaneous 620 different companies, different microprocessor fam ilies evolved; see Figure 6.5. PowerPC microprocessors Alpha AXP microprocessors represent two different approaches achieving high performance superscalar machines. two approaches respectively dubbed "brainiacs vs. speed demons." PowerPC microprocessors attempt achieve 2.0 n Clock f requency ( MHz) Figure 6.5 Evolution Superscalar Families.322 MODERN PROCESSOR DESIGN highest l evel IPC possible without overly compromising clock speed. hand, Alpha AXP microprocessors go hig hest possible clock speed achieving reasonable level IPC. course, future versions PowerPC chip get faster future Alpha AXP chips achieve higher IPC. key issue is, take precedence, IPC MHz? approach yield easier path get next level performance? Although versions PowerPC 620 microprocessor Alpha AXP 21164 microprocessor seem indicate speed demons ar e winning, strong consensus answer future. interesting way, two rivaling approaches resemble, perh aps reincarnation of, CISC vs. RISC debate decade earlier [Colwell et al., 1985]. announcement P6 Intel pr esents another interesting case. P6 comparable 620 terms microarchitectural aggressiveness achieving high IPC. hand P6 somewhat similar 21164 "superpipelined" 620. P6 represents yet third, perhaps hybrid, approach achieving high performance. Figure 6.5 reflects landscape existed circa mid-199 0s. Since landscape hifted sig- nificantly right. Today longer using SPECInt92 benchmarks SPECInt2000, dealing frequencies multiple-gigahertz range. 6.8 Bridging IBM POWER3 POWER4 PowerPC 620 intended initial high-end 64-bit implementation PowerPC architecture would satisfy needs server high- performance workstation market. However, numerous difficulties finishing design timely fashion, part wa delayed several years ended used server systems developed Groupe Bull. meantime, IBM able satisfy need server product line Star series POWER2 microprocessors, developed independent design teams differed subst antially PowerPC 620. However, IBM POWER3 processor, released 1998, heavily influ- enced PowerPC 620 design reused overall pipeline structure many functional blocks [O'Connell White, 2000]. Table 6.11 summa- rizes key differences 620 POWER3 processors. Design optimization combined several years advances semiconductor technology resulte nearly tripling processor frequency, even similar pipeline structure, r esulting iceable performance improvement. POWER3 addressed shortcomings PowerPC 620 design substantially improving instruction execution bandwidth well memory bandwidth. Although front back ends pipeline remained width, POWER3 creased peak ssue rate eight instructions per cycle providing two load/stor e units two fully pipelined floating-point units. effective window size also doubled increasing completion buffer 32 entries doublin g number int eger rename reg isters tripling number floating- point rename registers. Memory bandwidth enhanced novel 128-way set-associative cache design embedsTHE POWERPC 620 Table 6.11 PowerPC 620 versus IBM POWER3 POWER4 Attribute 620 POWER3 POWER4 Frequency 172 MHz 450 MHz 1.3 GHz Pipeline length 5+ S+ 15+ Branch prediction Bimodal BHT/BTAC 620 3x16Kcombming Fetch/issue/completion width 4/6/4 4/8/4 4/8/5 Rename/physical registers 8lnt,8FP 16lnt,24FP 80lnt,72 FP In-flight instructions 16 32 200 Floating-point units 1 2 2 Load/store units 1 2 2 Instruction cache 32K 8-way SA 32K128-waySA 64KDM Data cache 32K 8-way SA 64K128-waySA 32K2-waySA L2/L3 size 4 Mbytes 16 Mbytes -1.5 Mbytes/32 Mbytes L2 bandwidth 1 Gbytes/s 6.4 Gbytes/s 100+Gbytes/s Store queue entries 6 x 8 bytes 16x8 bytes 12 x 64 bytes MSHRs 1:1/0:1 l:2/D:4 l:2/D:8 Hardware prefetch None 4 streams 8 streams SA—set-associative DM—direct mapped tag match hardware directly tag arrays caches, significantly reducing miss rates, doubling overall size dat cache. L2 cache size also increased substantially, available bandwidth off-chip L2 cache. Memory latency also effectively decreased incorp orating aggressive hardware prefetch engine detect four independent refer- ence streams prefetch memory. prefetching scheme works extremely well floating-point workloads regular, predictable access patterns. Finally, support fo r multiple standing cache misses added providing two miss-status handling registers (MSHRs) instruction cache four MSHRs data cache. next new high-performance processor P owerPC family POWER4 processor, introduced 2001 [Tendler et al., 2001]. Key tributes entirely new core design summarized Table 6 .11. IBM achieved yet another tripling processor frequency, thi time employing substantially deeper pipeline conjunction major advances process technology (i.e., reduced feature sizes, copper interconnects, silicon-on-insulator technology). POWER4 pipeline illustrated Figure 6.6 extends 15 stages best case single-cycle integer ALU instructions. keep pipeline fed useful instr uctions, POWER4 employs advanced combining branch predictor uses 16K entry selector table choose 16K entry bimodal predictor 16K entry gshare predictor. entry three tables 1 bit,324 MODERN PROCESSOR DESIGN Branch redirects Instruction fetchOut-of-order processing "I Figure 6.6 POWER4 Pipeline Structure. Source Tendler et alv 2001. rather 2-bit up-down counter, since studies showed 16K 1-bit entries per- formed better 8K 2-bit entries. indicates server workloads POWER4 optimized for, branch predictor capacity misses important hysteresis provided 2-bit counters. POWER4 matches POWER3 execution bandwidth, provides substantially mor e rename registers (now form single physical register file) supports 200 in-flight instructions pipeline. POWER3, memory bandwidth latency important c onsiderations, multiple load/ store units, support eight outstanding cache misses , very-high-bandwidth interface on-chip L2, support ssive off-chip L3 cache, play integral role improving overall performance. POWER4 also packs two com- plete processor cores sharing L2 cache single chip chip multiprocessor configuration. details arrangement ar e discussed Chapter 11. 6.9 Summary PowerPC 620 interesting first-generation out-of-order superscalar pro- cessor design exemplifies short pipelines, aggr essive support extracting instruction-level parallelism, support weak ordering memory ferences typical processors similar vintage (for example, HP PA- 8000 [Gwennap, 1994] MIPS R10000 [Yeager, 1996]). evolution IBM POWER3 part illustrates natural extension execution resources extract even greater parallelism also tackling memory bandwidth latency bottlenecks. Finally, recent POWER4 design highlights seemingly heroic efforts microprocessors today tolerate memory bandwidth latency aggressive on- off-chip cache hierarchies, stream-based hardwareTHE POWERPC 620 prefetching, large instruction windows. time, POWER4 illustrates trend toward higher higher clock frequency extremely deep pipelining, sustained result increasingly accurate branch predictors keep pipelines filled useful instructions. REFERENCES Colwell, R, C. Hitchcock, E. Jensen, H. B. Sprunt, C. Kollar: "Instructions sets beyond: Comp uters, complexity, controversy," IEEE Computer, 18,9,1985, pp. 8-19. Diep, T. A., C. Nelson, J. P. Shen: " Performance evaluation PowerPC 620 microarchitecture," Proc. 22nd bit. Symposium Computer Architecture, Santa Margherita Ligure, Italy, 1995. Gwennap, L.: "PA-8000 combines com plexity speed," Microprocessor Report, 8, 15, 1994. pp. 6-9. IBM Corp.: PowerPC 601 RISC Microprocessor User's Manual. IBM Microelectronics Division, 1993. IBM Corp.: PowerPC 604 RISC Microprocessor User's Manual. IBM Microelectronics Division, 1994. Levitan, D., T. Thomas, P. Tu: "The PowerPC 620 microprocessor: high perfor- mance superscalar RISC processor," Proc. ofCOMPCON 95, 1995, pp. 285-291. May, C, E. Silha, R Simpson, H. Warren: PowerPC Architecture: Specification New Family RISC Processors. 2nd ed. San Francisco, CA, Morgan Kauffman, 1994. Motorola, Inc.: MPC750 RISC Microprocessor Family User's Manual. Motorola. Inc., 2001. Motorola, Inc.: MPC603e RISC Microprocessor User's Manual. Motorola, Inc., 2002. Motorola, Inc.: MPC7450RISC Microprocessor Family User's Manual. Motorola, Inc., 2003. O'Connell, F., S. White: "POWER3: next generation PowerPC processors," IBM Journal Research Development, 44,6,2000, pp. 873-884. Storino, S., A. Aipperspach, J. Borkenhagen, R Eickemeyer, S. Kunkel, S. Levenstein, G. Uhlmann: "A commercial multi-threaded RISC processor," Int. Solid-State Circuits Con- ference, 1998. Tendler, J. M., S. Dodson, S. Fields, B. Sinharoy: "IBM eserver POWER4 system microarchitecture," IBM Whitepaper, 2001. Yeager, K.: "The MIPS R10000 superscalar microprocessor," IEEE Micro, 16, 2, 1996, pp. 28^10. HOMEWORK PROBLEMS P6.1 Assume IBM instruction mix Chapter 2, consider whether PowerPC 620 completion buffer versus integer rename buffer design asonably balanced. Assume load ALU instructions need integer rename buffer, instructions not. 620 design balanced, many rename bu ffers be?!6 MODERN PROCESSOR DESIGN P6.2 Assuming instruction mixes Table 6.2, nchmarks likely rename buffer constra ined? is, ones run rename buffers run completion buffers vice versa? P6.3 Given dispatc h retire ment bandwidth specified, many integer archi tected register file (ARF) read write ports needed sustain peak throughput? Give n instruction mixes Table 6.2, alsq compute average ports needed fo r benchmark. Explain would build average case. Given actual number read write ports specified, likely dispatch port-limited? likely tirement wil l port-limited? P6.4 Given dispatch retirement bandwidth specified, many integer rename buffer read w rite ports needed? Given instruction mixes Table 6.2, also compute verage ports needed bench- mark. Explain would build average case. P6.5 Compare PowerPC 620 BTAC design ne xt-line/next-set predic- tor Alpha 21264 described Alpha 21264 Microproces- sor Hardware Reference Manual (available www.compaq.com ). key differences similarities two techniques? P6.6 would expect results Table 6.5 change recent design deeper pipeline (e.g., 20 stages, like Pentium 4)? P6.7 Judging Table 6.7, PowerPC 620 appears reservation station- starved. double number reservation stations, much performance improvement would expect benchmarks? Justify answer. P6.8 One obvious bottlenecks 620 design single load/store unit. IBM POWER3, subsequent design based heavily 620 microarchitecture, added second load/store unit along second floating-point multiply/add unit. Compare SPECInt2000 SPECFP2000 score IBM POWER3 (as reported www.spec.org ) another modern processor, Alpha AXP 21264. Normalized frequency, p rocessor scores higher? fair normali ze frequency? P6.9 data Table 6.10 seem support 620 designers' decision implement load/s tore forwarding 620 processor. Discuss tradeoff changes pipeline depth increases relative memory latency (as measured processo r cycles) increases. P6.10 Given data Table 6.9, present hypothesis XSU1 appears consistently longer execution latency XSU0. Describe experiment might conduct verify hypothesis.THE POWERPC 620 32 P6.ll IBM POWER3 detect four regular access streams issue prefetches future references. Construct address reference trace utilize four treams. P6.12 IBM POWER4 detect eight regular access streams issue prefetches future references. Construct address reference trace utilize eight streams. P6.13 stream prefetching POWER3 POWER4 processors done outside processor core, using physical addresses cache lines miss cache. Explain large virtual memory page sizes improve efficiency prefetch cheme. P6.14 Assume program streaming sequentially 1-Gbyte array reading aligne 8-byte floating-point double 1-Gbyte array. assume prefetch engine st art prefetching seen three consecutive cache line references miss cache (i.e., fourth cache line sequential stream prefetched). Assuming 4K page sizes given cache line sizes thePOWER3 POWER4, compute overall miss rate program following three assumptions: pr efetching, physical- address prefetching, virtual-addres prefetching. Report miss rate per D-cache reference, assuming single reference every 8-byte word. P6.15 Download install sim-outorder simulator Simplescalar simulator suite (available wwwjsimplescalar .com ). Configure simulator match (as clos ely possible) microarchitecture PowerPC 620. collect branch prediction cache hit data using instructional b enchmarks available Simplescalar website. Compare results Tables 6.3 6.10 provide reasons differences might observe.Robert P. Colwell THAPTFR Dave B. Papworth Glenn J. Hinton Mike A. Fetterman Andy F. Glew Intel's P6 Microarchitecture CHAPTER OUTLINE 7.1 Introduction 7.2 Pipelining 7.3 In-Order Front End 7.4 Out-of-Order Core 7.5 Retirement 7.6 Memory Subsystem 7.7 Summary7.8 Acknowledgments References Homework Problems 1990, Intel began development new 32-bit Intel Architecture (IA32) microarchitecture core known P6. Intr oduced product 1995 [Colwell Steck, 1995], named Pentium Pro processor became popu- lar workstation server systems. desktop proliferation P6 core, Pentium II processor, launched May 1997, added MMX instruc- tions basic P6 engine. P6-based Pentium UJ proc essor followed 1998, included MMX SSE instructions. chapter refers core P6 products respective product names. P6 microarchitecture 32-bit Intel Architecture-compatible, high- performance, superpipelined dynamic execution engine. order-3 superscalar uses out-of-order speculative execution techniques around micro-dataflow execution core. P6 includes nonblocking caches transactions-based snooping bus. chapter describes various c omponents design combine deliver exbaordinary performance economical die size. 329330 MODERN PROCESSOR DESIGN 7.1 Introduction basic block diagram P6 rnicroarchitecture shown Figure 7.1. three basic sections microarchitecture: th e in-order front end, out-of-order middle, in-order bac k-end "retirement" process. Intel rclutoture- compatible, machine must obey certain conventions execution program code. achieve high performance, must relax conventions, exe- cution program's operators trictly order implied program itself; True data dependences must observed, beyond that, certain memory ordering constraints precise faulting semantics IA32 architecture must guaranteed. j maintain precise faulting semantics, processor must ensure asynchro- nous events interrupts synchronous awkw ard events faults traps handled exactly way would i486 system. im plies in-order ret irement process reimposes original pro- gram ordering com mitment instruction results p ermanent architectural External bus Chip boundary L2 Bus interface logic IFU (includes I-cache ITLB) BTB/BAC MISRAT ILiAllocator Figure 7.1 P6 Microarchitecture Block Diagram.DCU j- MOB itAGU MMX IEU/JEU FEU MTU RS | RS | ROB 1 RRF 1 ROB 1 RRF 1 Key: L2: LeveI-2 cache DCU: Data cache unit (level 1) B : e r ordering buffer AGU: Address generation unit MMX: MMX instruction execution unit 1EU: Integer execution unit JEU: Jump execution unit FEU: Floating-poin execution unit MIU: Memory interface unit RS: Reservation station ROB: Reorder buffer RRF: Retirement register file RAT: Register alias table ID: Instruction decoder MIS: Microinstruction sequencer IFU: Instruction fetch unit BTB: Branch target buffer BAC: Branch address calculator 'Branches faults use mechanism recover state. However, performance reasons, branches clear restart front end early possible. Page faults handled speculatively, floating-point faults handled achine sure th e faulted instructions path certain execution.INTEL'S P6 MICROARCHITECTURE 3: machine state. in-order mechanisms ends execution pipe- line, actual execution instructions proceed unco nstrained arti- facts ther true data dependences machine resources. explore details three sections th e machine r emainder chapter. many novel aspects microarchitecture. instance, almost universal processors central controller unit omewhere monitors controls overall pipeline. controller "understands" state instructions flowing pipeline, governs coordinates changes state constitute computation process. P6 microarchit ecture purposely avoids centralized r esource. simplify hardware rest machine, microarchitecture translat es Intel Architecture instructions simple, stylized atomic units computation called micro-operations (micro-ops flops). rnicroarchitecture k nows state pr ogram's e xecution, way change machine state, manipulation flops. P6 microarchit ecture deeply pipelined, relative competitive designs era. deep pipeline implemented several short pipe seg- ments connected queues. approach af fords much higher clock rate, negative effects deep p ipelining ameliorated advanced branch pre- dictor, fast high-bandwidth access L2 cache, much higher clock rate (for given semiconductor process technology). microarchitecture speculat ive, out-of-order engine. engine speculates als misspeculate must provide means detect recover condition. ensure fundamental microarchitecture feature would implemented error-free humanly possible, designed recov- ery mechanism extremely simple. Taking advantage simplicity, fact mechanism would hea vily validated, mapped machine's event-handling events ( faults, traps, int errupts, breakpoints) onto set protocols mechanisms. front-side bus change ntel's Pentium processor family. transaction-oriented desig ned high-performance multiprocessing sy stems. Figure 7.2 shows four Pentium Pro processors connected single shared bus. chipset provides main memory, data Figure 7.2 P6 Pentium Pro System Block Diagram.332 MODERN PROCESSOR DESIGN 1 Figure 7.3 P6 Product Packaging. path (DP) data controller (DC) parts, memory interface controller (MIC) chip. I/O provided via industry standard PCI bus, bridge lder Extende Indu stry Standard Architecture (EISA) standard bus. Various proliferations P6 chipsets platform designs optimized multipl e market segments including (1) high-volume, (2) worksta- tion, (3) server, (4) mobile market segments. differed mainly amount type memory could accommodated, number CPUs could supported single platform, L2 cache design placement. Pentium U processor use two-die-in-a-package approach original Pentium Pro; approach yielded fast system product expensive manufac- ture due specia l ceramic dual-cavity package unusual manufacturing steps required. P6-based CPUs packaged several formats (see Figure 7.3):  Slot 1 brought P6 microarchitecture volume price points, combining one P6 CPU two commodity cache RAMs tag chip one FR4 fiberglass substrate cartridge. substrate electrical contacts contained one edge connector, heat sink attached one side packaged substrate. Slot l's L2 cache runs one-half clock frequency CPU.  Slot 2 physically larger Slot 1, allow four custom RAMs form larg e caches required high-performance workstation server markets. Slot 2 cartridges carefully esigned that, despite higher number loads L2 bus, access large L2 cache full clock frequency CPU.  improved silicon process technology, 1998 Intel returned pin- grid-array packaging Celeron processor, L2 caches con- tained CPU die itself. obviated need Pentium Pro's two-die-in-a-package Slot 1/Slot 2 cartridges. 7.1.1 Basics P6 Microarchitecture subsequent sections, operation various compone nts microarchi- tecture examined. first, may helpful consider overall machine organization higher level.INTEL'S P6 ICROARCHITECTURE 3. useful way view P6 microarchitecture dataflow engine, fed aggressive front end, constrained implementation code compatibility. difficult design microar chitectures capable e xpressing instruction- level parallelism; adding multiple execution units trivial. Keeping execu- tion units gainfully employed hard. P6 solution problem  Extract useful work via deep speculation front end (instruction cache, decoder, register renaming).  Provide enough temporary storage lot work "kept air."  Allow instructions ready execute pass others (in out-of-order middle section).  Include enough memory bandw idth keep work progress. speculation proceeding right path, pops generated front end flow smoothly reservation station (RS), execute data operands become available (often order implied source program), take place reti rement line reorder buffer (ROB), retire heir turn. Micro-ops c arry along information required sched- uling, dispatch, execution, retirement. Micro-ops two source references, one destination, operation-type field. logical refere nces renamed register alias table ( RAT) physical registers residing ROB. inevitable misprediction occurs,2 simple protocol exercised within th e out-of-order core. protocol ensures out-of-order core flushes speculative state known bogus, keeping work yet known good bogus. protocol directs front end drop start mispredicted target's corr ect address. Memory operations special category pop. IA323 instruc- tion set architecture registers, IA32 programs must access memory fre- quently. means dependency chains characterize program generally start memory load, turn means important loads speculative. (If not, rest speculative engine would starved waiting loads go order.) loads speculative; consider load memory -mapped I/O system load nonrecoverable side effect. Section 7.6 cover special cases. Stores never speculative, way "put back old data" misspeculated store later found error. However, performance reasons, store data forwarde store buffer (SB), data actually 2For instance, first encounter branch, branch predictor "know" branch all. much less way branch might go. "*This chapter uses 1A32 refer standard 32-bit Intel Architecture, embodied processors Intel 486, Pentium, Pen tium 11. Pentium 111 , Pentium 4 processors.334 MODERN PROCESSOR DESIGN appeared caches memory, llow dependent loads (and heir progeny) proceed. closely analogous writeback cache, data loaded cache yet written data main memory. IA32 coding semantics, important care fully control trans- fer information th e out-of-order, speculative engine permanent machine state saved r estored program context switches. call "retirement" Essentially, machine's activity point undone. Retirement act irrevocably committing changes program's state. P6 microarchitecture retire three pops per clock cycle, therefore retire many three IA32 instructions' worth changes per- manent state. (If three ops needed express given IA32 instruction, retirement process makes sure necessary all-or-none atom icity obeyed.) 7.2 Pipelining examine individual elements P6 micro-architecture chap- ter, look pieces, may help see fit together. pipeline diagram Figure 7.4 may help put pieces perspective. first thing note figure appears show many separate pipe- lines, rather one single pipeline. intentional; reflects philos- ophy desig n microarchitecture. pipeline segments three cluste rs. First in-order front end, sec- ond out-of-order core, third retirement. reasons become clear, essential pipeline segments separable operation- ally. example, recovering mis predicted branch, front end machine immediately flush b ogus information processing mispredicted target address refetch corrected stream corrected branch target, out-of-order core continues working previously fe tched instructions (up mispredicted branch). also important separate overall pipeline independent segments various queues happen fill up, thus require th eir suppliers stall tables drained, little overall machine neces- sary stalls, entir e machine. 7.Z1 In-Order Fr ont-End Pipeline first stage (pipe stage l l4) in-orde r front-end pipeline used branch target buffer (BTB) gen erate pointer instruction cache (I-cache) use, accessing hope right set instruction bytes. Remember mac hine always speculating, guess wrong; is, error recognized one several places machine misprediction recovery sequence initiated time. second pipe stage in-order pipe (stage 12) initiates I-cache fetch address BTB generated pipe stage 11. third pipe stage (stage 13) *Why first stage numbered 11 instead t? remember. think arbitrary.INTEL'S P6 MICROARCHITECTURE In-order pipeline9 1 J - 1-121314,5,6r 2021 22 Single-cycle pop pipeline Pipelined multicycle pxyp 20: Exit ID queue 21: RAT, RS Psrc write 81: MenVFP writeback 82: Intege r writeback 83: Writeback data Nonblocking memory pipeline Blocking memory access pipeline sback "3 |sched dispa| jjCO aX W 3132 33 82 83 32 33... ...... 3- 818283 & & % 8 H 313233 42 43 81 82 83 AGU Mobblk Mobwr 323342 4313 * '-3 1 40 41 42 43 818283  91 92 Figure 7.4 P6 Pipelining. continues I-cache access. fourth pipe stage (stage 14) completes I-cache fetch transfers newly fetched cache line instruction decoder (ID) commence decoding. Pipe stages 15 16 used ID align instruction tes, identify ends three IA32 instructions, break instructions sequences constituent pops. Pipe stage 17 stage part ID detect branches instructions decoded. certain conditi ons (e.g., unpredicted unconditional branch), ID notice branch went unpredicted BTB (probably BTB never seen particular branch before) flush in-order pipe refetch branch target, without wait branch actually tries retire many cycles future.16 MODERN PROCESSOR DESIGN Pipe stage 17 synonymous pipe stage 21,5 rename stage. register alias table (RAT) renames uop destination/source linkages large set physical register reorder buffer. pipe stage 22, RAT transfers Hops (three time, since P6 microarchitecture order-3 superscalar) out-of-order core. Pipe stage 22 marks transition in-order section out-of-order section machine. uops making transition written reservation station (where wait execute appropriate execution unit) reorder buffer (where "take p lace line," eventually commit changes permanent machine state order mplied original user program). 7.2.2 Out-of-Order Core Pipeline in-order front end written new set (up to) three uops reser- vation station, uops become possible candidates execution. RS takes several factors account deciding uops ready exec ution: uop must operands ava ilable; execution unit (EU) needed uop must available; writeback bus must ready cycle EU complete uop's execution; RS must Hops thinks (for whatever reason) important overall performance uop discussion. Remember out-of-order core machine. RS know, care about, original program order. observes data dependences tries maximize overall performance so. One implication given uop wait zero dozens even hundreds clock cycles w ritten RS. point RS scheduling delay label Figure 7.4. scheduling delay low zero, machine recovering mis predicted branch, first "known good" uops new instruction stream. (There would point writing uops RS, RS "disco ver" data-ready two cycles later. first uop issue in-order section guaranteed dependent speculative state, b ecause speculative state point!) Normally, ever, Uops get written RS, stay RS notices data-ready (and constraints previously listed satisfied). takes RS two cycles notice, dispatch, Uops execution units. pipe stages 31 32. Simple, single-cycle-execution uops logical op erators simple arith- metic operations execute pipe stage 33. complex operations, integer multiply, floating-point operations, take many cycles needed. One-cycle operators provide heir results writeback bus end pipe stage 33. writeback busses shared resource, managed RS, RS must ensure writeback bus available future cycle given Uop time Uop dispatched Writeback bus scheduling occurs pipe stage 82, writeback execution cycle, 83 (which synonymous pipe stage 33). 5Why pipe stages one name? pipe segments independent. Sometimes part one pipe segment lines one stage different pipe segment, sometimes another.INTEL'S P6 MICROARCHITECTURE 337 Memory operations bit complicated. memory operations must first generate effective address, per usual IA32 methods combining segment base, offset, base, index. uop generates memory address exe cutes address generation unit (AGU) pipe stage 33. data cache (D Cache) accessed next two cycles, pipe stages 42 43. access cache hit, accessed data return RS become available source uops. DCache reference miss, machine tries L2 cache. misses, load Uop suspended (no sense trying time soon; must refill miss main memory, slow operation). memory ordering buffer (MOB) maintains list active memory operations keep load uop suspended cache line refill arrived. conserves cache access bandwidth nop sequences may independent suspended nop; uops go around suspended load continue making forward progress. Pipe stage 40 used MOB identify "wake up" suspended load. Pipe stage 41 re-dispatches load DCache, (as earlier) pipe stages 42 43 used DCache accessing line. MOB scheduling delay labeled Figure 7.4. 7.2.3 Retirement Pipeline Retirement act transferring speculative state permanent, irrevoca- ble architectural machine state. instance, sp eculative out-of-order core may uop wrote OxFA instruction result appropriate field ROB entry 14. Eventually, mispredicted branch found interim, become uop's turn retire next, become oldest uop machine. point, uop's original intention (to write OxFA into, e.g., EAX register) realized transferring OxFA data ROB Slot 14 retirement register file's (RRF) EAX register. several complicating factors simple idea . First, ROB actually retiring viewed sequence uops, rather series IA32 instructions. Since architecturally illegal retire part IA32 instruction, either uops comprising IA32 instruction retire, none do. atomicity require ment generally demands partially modified architectural state never visible world outside processor. part ROB must detect beginning end given IA32 instruc- tion make sure atomicity rule strictly obeyed. ROB observing marks left uops instruction decoder (ID): Hops marked first uop IA32 instruction, others marked last. (Obviously, others marked all, implying somewhere mid- dle IA32 uop sequence.) retiring sequence uops comprise IA32 instruction, external events handled. simply wait, previous gener- ations Intel Architecture (i486 Pentium processors, instance). two IA32 instructions, chine must capable taking interrupts, breakpoints, trap s, handling faults, on. reorder buffer makes sure events possible right times multiple pending events serviced priority order implied Intel instruction set architecture.338 MODERN PROCESSOR DESIGN processor must capability stop microcode flow partway through, switch microcode assist routine, pe rform number Hops, resume flow point interruption, however. sense, instruction may considered partially executed time trap taken. first part instruction cannot discarded restarted, would prevent forward progress. kind behavior occurs TLB updates, kinds floating-point assist s, more. reorder buffer implemented circular list pops, one reti rement pointer one new-entr pointer. reorder buffer writes results just- executed pop array pipe stage 22. pop results ROB read pipe stage 82 committed permanent machine state RRF pipe stage 93. 7.3 In-Order Front End primary responsibility front end keep ex ecution engine full useful work do. every clock cycle, front end makes new guess best I-cache address fetch new line, sends cache line guessed last clock decoders get started. guess can, course, discovered incorrect, wh ereupon front end later redirected fetch really from. substantial perfor- mance penalty occurs mispredicted branch discovered, key chal- lenge microarchitecture one ensure branches predicted correctiy often possible, minimize recovery time found mispredicted. discussed greater detail shortly. decoders convert three IA instructions corresponding pops (or pop flows, IA instructions complex enough) push hese queue. register renamer assigns new physical register designators source destination ferences pops, pops issue out- of-order core machine. implied pipelining diagram Figure 7.4, in-order front end P6 microarchitecture runs independently rest achine. mispredicted branch detected out-of-order core [in jump ex ecutio n unit (JEU)], out-of-order core continues retire pops older mis predicted branch pop flushes everything younger. Refer Figure 7.5. Meanwhile, die front end immediately flushed begins fetch decode instructions starting correct branch target (sup plied JEU). simplify handoff in-order front end out-of-order core , new pops corrected branch target strictly quarantined whatever pops remain out-of-order core, out-of-order section drained. Statistically, out-of-order core usu- ally drained time new FE2 pops get th e in-order front end. 73.1 Instruction Cache ITLB on-chip instruction cache (I-cache) performs usual function serving repository recentiy used instructions. Figure 7.6 shows four pipe stages instruction fetch unit (IFU). first pipe stage (pipe stage 11), IFUINTEL'S P6 MICROARCHITECTURE 33S Normal operation, front end speculatively fetching decoding IA-32 instrs, naming, streaming jtops out-of-order (OOO) core. 3 FE2oooi oooi]0 0 0 core detects mispredicted branch, instructs front end flush begin refetching. OOO core continues executing retiring juops ahead mispredicted branch, core drains. Front end flushed, refetched corrected branch target. New /io p stream propagated rename, ready enter OOO core. core finished Mops present bad branch detected. Stall front end, continue draining OOO core. 4 FE2OOO core drained; retire bad branch, flush rest OOO core. FE2FE2 000 2 Figure 7.5 Branch Misspeculation Recovery.Normal operation, front end speculatively fetching decoding IA-32 instrs, renaming, streaming p,ops int out-of-order core. nEXAMPn _LT_J selects address next cache access. address selected num- ber competing fetch requests arrive IFU (among others) BTB branch address calculator (BAC). IFU picks request highest priority schedules service second pipe stage (pipe stage 12). second pipe stage, IFU accesses many caches b uffers using fetch ad dress selected previous stage. Among caches buffers accessed instruction cache instruction st reaming buffer. hit caches buffers, instructions read forwarded third pipe stage. miss buffers, external fetch initiated sending request external bus logic (EBL). Two caches also accessed pipe stage 12 using fetch address: ITLB IFU branch targe buffer (BTB). ITLB access obtains physical address memory type fetch, BTB access obtains branch prediction. BTB takes two cycles complete one access. third pipe stage (13), th e IFU marks instructions received previous tage (12). Marking process determining instruction boundaries. Additi onal marks predicted branches delivered BTB end pipe stage 13. Finally, fourth pipe stage (14), instructions marks written instruction buffer optionally teered ID, instruction buffer empty.340 MODERN PROCESSOR DESIGN fetch requests Next IP muxData Fetch L2 address cache EBL J_LData select mux Instruction streaming buffer Instruction cache Instruction victim cacheInstruction buffer Instruction length decoderLength marks Physical address Instruction TLBInstruction rotator Bytes consumed ID Branch target buffer Figure 7.6 Front-End Pipe Staging. fetch address selected pipe stage 11 service pipe stage 12 linear address, virtual physical address. fact, IFU oblivious virtual addresses indeed segmentation. allows IFU ignore segment boundaries delaying checking segmentation-related violations units downstream IFU P6 pipeline. IFU does, however, deal paging. paging turned off, linear fetch address selected pipe stage 11 identical th e physical address directiy used search caches buffers pipe stage 12. However, paging turned on, linear address must translated ITLB physical address. virtual linear physical sequence shown Figure 7.7. IFU caches bu ffers quire physical ddress (actually, untrans- lated bits, match physical address) access instruction cache, instruction streaming buffer, instruction victim cache. branch targetINTEL'S P6 MICROARCHITECTURE 34' Virtual address .0Linear address (Linear Segment basepage \,20 bitsTLB Physical addressPhysical - page no. 20 bits Page  offset 12 bits Figure 7.7 Virtual Linear Physical Addresses. buffer accessed using linear fetch address. block diagram front end, BTB's place it, shown Figure 7.6. 7.3.2 Branch Prediction branch target buffer two major functions: predict branch direction predict branch targets. BTB must operate early nstruction pipeline prevent machine executing wrong program stream. (In specula- tive engine P6, executing wrong stream is, course, per- formance issue, cor rectness issue. machine always execute correctly, question quickly.) branch decision (taken taken) known jump execution unit (JEU) resolves branch (pipe stage 33). Cycles would wasted machine wait branch resolved start fetching instructions branch. avoid delay, th e BTB predicts de cision branch IFU f etches (pipe stage 12). prediction wrong: machine able detect case recover. predictions made BTB verified downstream either branch address calculator (pipe stage 17) JEU (pipe stage 33). BTB takes starting linear address instructions fetched produces prediction target address branch instructions fetched. information (prediction target address) sent IFU, next cache line fetch redirected branch pre dicted taken. branch's entry BTB updated allocated BTB cache JEU resolves it. branch update sometimes late help next instance branch instruction stream. overcome delayed update problem, branches also speculatively updated (in sepa rately maintained BTB state) BTB makes prediction (pipe stage 13). 7.3.2.1 Branch Prediction Algorithm. Dynamic branch prediction P6 BTB related two-level adaptive tr aining algorithm proposed Yeh Patt [1991]. algorithm uses two levels branch history information make predictions. first level history bran ches. second level branch behavior specific pattern branch history. branch, BTB keeps N bits "real" branch history (i.e., branch decision last N dynamic occurrences). history called branch h istory register (BHR).342 MODERN PROCESSOR DESIGN pattern BHR indexes 2 entry table states, pattern table (FT). state given pattern used predict branch act next time seen. states pattern table updated using Lee Smith's [1984] saturating up-down counter. BTB uses 4-bit semilocal pattern table per set. means 4 bits history kept entry, entries set use pattern table (the four branches set share pattern table). equivalent per- formance 10-bit global table, le ss hardware complexity smaller die area. speculative copy BHR updated pipe stage 13, real one updated upon branch resolution pipe stage 83. pattern table updated conditional branches, computed jump execution unit. obtain prediction branch, decision branch (taken taken) shifted old history pattern branch, field used index pattern table. significant bit state pattern table indicates prediction used next time seen. old state indexed old history pattern updated using Lee Smith state machine. example algorithm works shown Figure 7.8. history entry updated 0010, branch decision taken. new oooo poolbo lo 0011 0100 0101 0110 10001001 10101011110011011110 1111\\o B < ) Two processes occur parallel: 1. new history used access partem table get new prediction bit. Thi prediction bit written BTB next phase 2. old history used access pattern table get state updated. updated state written back pattern table. Figure 7.8 Yeh's Algorithm.INTEL'S P6 MICROARCHITECTURE 343 2:1 Spec. muxPartem tables Control signals 4:1 Target muxReturn register 2:1 Return mux Figure 7.9 Simplified BTB Block Diagram. history 0101 used index th e pattern table new prediction 1 branch (the significant bit state) obtained. old history 0070 used index pattern table get old state 70. old state 70 sent state machine along branch decision, new state 77 written back pattern table. BTB also maintains 16-deep return st ack buffer help pre dict returns. circuit speed reasons, BTB accesse require two clocks. causes predicted- taken branches insert one-clock fetch "bubble" front end. double- buffered fetch lines instruction decoder ID's output queue help eliminate bubbles normal execution. block diagram BTB shown Figure 7.9. 7.3.3 Instruction Decoder first stage ID known instruction steering block (ISB) responsible latching instruction bytes IFU, picking individual instructions order, steering three decoders. ISB344 MODERN PROCESSOR DESIGN Macro-instruction bytes IFU Front-end control Instruction bufferingand steering logic UROM entry point vector MS fiops MS Decode block Output queue Figure 7.10 ID Block Diagram.Instruction buffer Queue control16 bytes Decoder Decoder Decoder 01 2 4/iops 1 fiop 1 pop Six-entry /top queueBranch logic Branch info BAC Virtual IPs BAC 3 ftops issued RAT/ALL quickly detects many instructions decoded clock make fast deter- mination whether instruction buffer empty. empty, enables latch receive ins truction b ytes IFU (refer Figure 7.10). miscellaneous functions performed thi logic "front end" ID. detects generates correct sequencing predicted branches. addition, LD front end generates valid bits pops produced decode PLAs detects stall con ditions ID. Next, instruction buffer loads 16 bytes time IFU. data aligned first byte buffer guaranteed first byte com- plete instruction. average instruction length 27 3.1 bytes. means average five six complete instructions loaded buffer. Loading new batch instruction bytes enabled following conditions:  processor front-end reset occurs due branc h misprediction.  complete instructions currently buffer successfully decoded.  BTB predicted-taken branch su ccessfully decoded three decoders.INTEL'S P6 MICROARCHITECTURE 345 Steering three properly aligned macro-instructions three decoders one clock complicated due variable length IA32 irisrructions. Eve n determining length one instruction straightforward, first b ytes instruction must decoded order interpret bytes follow. Since process steering three variable-length instructions inherently serial, help- ful know forehand location macro-instruction's boundaries. instruction length decoder (TJLD), resides IFU, performs pre-decode function. scans bytes macro-instruction stream locating instruction boundaries marking first opcode e nd-bytes e ach. addition, IFU marks bytes indicate BTB branch predictions code breakpoints. may 1 16 instructions loaded instruction buffer load. first three instructions steered one three decoders. instruction buffer contain three complete instructions, many possible steered decoders. steering logic uses first opcode mark- ers align st eer instructions parallel. Since may 16 instructions instruction buffer, may take several clocks decode them. starting byte location three struc- tions steered given clock may lie anywhere buffer. Hardware aligns three instructions steers three decoders. Even though three instructions may steered decoders one cycle, three may get uccessfully decoded. instruction uccessfully decoded, specific decoder flushed pops resulting decode attempt invalidated. take multiple cycles consume (decode) instructions buffer. following situations result invalidation pops resteering corresponding macro-i nstructions another decoder subsequent cycle:  comple x macro-instruction detected decoder 0, requi ring assis- tance microcode sequencer (MS) microcode read-only memory (UROM), pops su bsequent decoders invalidated. MS completed sequencing rest flow,6 subsequent macro-instructions decoded.  macro-instruction steered Umited-functionality decoder (which able decode it), macro-instructions subsequent macro- instructions resteered decoders next cycle. pops pro- duced subsequent decoders invalidated.  branch encountered, pops produced subsequent decoders invalidated. one branch decoded per cycle. Note number macro-instructions decoded simultaneously directly relate number pops ID issue decoder queue store pops ssue later. ^Flow refers sequence jiopa emitted microcode ROM. sequences commonly used microcode express IA32 instructions, microarchitectural housekeeping.346 MODERN PROCESSOR DESIGN 7.3.3.1 Complex instructions. Complex instructions requiring MS sequence uops UROM. decoder 0 handle instruc- tions. two ways MS microcode invoked:  Long flows w decoder 0 generates first four uops flow MS sequences remaining uops.  Low-performance instructions decoder 0 issues uops transfers control MS sequence UROM. Decoders 1 2 cannot decode complex instructions, design tradeoff reflects silicon expense implementation well statistics dynamic IA32 code execution. Complex instructions resteered next- lower available decoder subsequent clocks reach decoder 0. MS receives UROM entry point vector decoder 0 begins sequencing Uops end micro code flow encountered. 7.3.3.2 ecoder Branch Prediction. macro-instruction buffer loaded IFU, ID looks prediction byte marks see predicted-taken branches (p redicted dynamically BTB) set complete instructions buffer. proper prediction found byte correspond- ing last byte branch instruction. predicted-taken branch found any- buffer, ID indicates IFU ID "grabbed" predicted branch. IFU let 16-byte block, fetched target address th e branch, enter buffer input rotator. rotator aligns instruction branch target next instruction loaded ID's instruction buffer. ID may decode pred icted branch immedi- ately, may take several cycles (due decoding al l instructions ahead it). branch finally decoded, ID latch instructions branch target next clock cycle. Static branch prediction (prediction made without reference run-time history) made branch address calculator (BAC). BAC decides take branch, gives IFU target IP IFU tart fetching instruc- tions. ID must not, course, issue Uops instructions branch, decode branch target instruction. BAC make static branch prediction two conditions: sees absolute branch BTB make prediction on, sees conditional branch target address whose direction "backward" (which suggests return edge l oop). 7.3.4 Register Alias Tabie register alias able (RAT) provides register renaming integer fl oating- point registers flags make available larger register set expli citly provided Intel Architecture. uops pr esented RAT, logical sources destination mapped corresponding physical ROB addresses data found. mapping arrays updated new physical destination addresses granted allocator new uop.INTEL'S P6 MICROARCHITECTURE 34 EAX 25 EBX 1 ECX 26 EDX 32 FP registers, jicode tmps RAT Figure 7.11 Basic RAT Register Renaming.39 Reorder buffer fiops decoderFPTOSInteger array Floating-point array(§5 Physical ROB pointers allocator Figure 7.12 RAT Block Diagram.fiops OOOcore Refer Figures 7.11 7.12. clock cycle, RAT must look physical ROB locations corresponding logi cal source references uop. physical designators become part uop's overall state travel uop point on. machine tate modified uop (its "destination" reference) also renamed, via information provided allocator. physical desti nation reference becomes part uop's overall state written RAT use subsequent uops whose sources refer logical destination. physical destination value unique flop, used identifier uop throughout out-of-order section. checks references Uop performed using physical destina- tion (PDst) name.348 MODERN PROCESSOR DESIGN Since P6 superscalar design, multiple pops must renamed given clock cycle. true dependency c hain three pops, say, popO: EAX, EBX; r c 1 = EBX, r c 2 = EAX, = EAX popl: EAX, ECX; pop2: E X , EDX; RAT must supply renamed source locations "on fly," via logic, rather looking destination, dep endences tracked across cloc k cycles. Bypass logic directly supply popl's source register, src 2, EAX, avoid wait uopO's EAX destination written RAT read popl's src. state RAT speculative, bec ause RAT constantly updating array entries per pop destinat ions flowing by. W hen inevitable branch misprediction occurs, RAT must flush bogus state collected revert logical-to-physical mappings work next set p ops. P6's branch mispr ediction recovery scheme guarantees RAT new renamings out-of-order core flushed bogus misspec- ulated state. useful, means register ferences reside retirement register file new speculative p.ops begin appear. Therefore, recover branch misprediction, RAT needs revert integer pointers point directly th eir counterparts RRF. 7.3A1 RAT Implem entation Details. IA32 architecture allows partial- width reads writes ge neral-purpose integer registers (i.e., EAX, AX, AH, AL). presents problem register renaming. problem occurs partial-widih write followed larger-width read. case, data required larger-width read must assimilation multipl e previous writes different pieces th e register. P6 solution problem requires RAT remember width integer array entry. done maintaining 2-bit size field entry integer low high banks. 2-bit encoding distinguish betwe en three register write sizes 32,16, 8 bits. RAT uses register size infor- mation determine larger register value needed previously written. c ase, RAT must generate partial-write stall. Another case, common 16-bit code, independent use 8-bit regis- ters. one alia maintained three sizes integer register access, independent use 8-bit subsets registers would cause tremendous number false dependences. Take, example, following series pops: popO: MOV AL,#DATA1 popl: MOV AH,#DATA2 pop2: AL,#DATA3± r1 EX*MP E L pop3: AH,#DATA4INTEL'S P6 MICROARCHITECTURE 3' Micro-ops 0 1 move indepe ndent data AL AH. Micro-ops 3 4 source AL AH addition. one alias available "A" register, popl's pointer AH would overwrite popO's pointer AL. pop2 tried read AL, RAT would know correct pointer would stall popl retired. pop3's AH source would lost due pop2's write AL. CPU would essentially serialized, performance would diminished. prevent this, two integer register banks maintained RAT. 32-bit 16-bit RAT accesses, data read low bank, data written banks imultaneously. 8-bit RAT accesses, however, appro- priate high low bank read written, according whether high byte low byte access. Thus, high low byte registers use different rename entries, renamed independently. Note high bank four array entries f integer registers (namely, EBP, ESP, EDI, ESI) cannot 8-bit accesses, per Intel Architecture specification. RAT physical source (PSrc) designators point locations ROB array data may c urrendy found Data actually appear ROB pop generating data executed written back one write- back busses. execution writeback PSrc, ROB entry contains junk. RAT entry RRF bit select one two address spaces, RRF ROB. RRF bit set, data found real register file; physical address bits set appropriate entry RRF. RRF bit clear, data found ROB, physical address points correct position ROB. 6-bit physical address field access ROB entries. RRF bit set, entry points r eal register file; physical address field co ntains pointer appropriate RRF register. busses arranged RRF source data way ROB can. 7.3.4.2 Basic RAT Operation. rename logical ources (LSrc's), six sources three ID-issued pops used indices RAT's inte- ger array. entry array six read ports allow six LSrc's read logical entry array. read phase completed, array must updated new physical destinations (PDst's) allocator associated destinations current pops processed. possible intracycle destination dependences, priority write scheme employed guarantee correct PDst written array dest ination. priority write mechanism gi ves priority following manner: Highest: Current pop2's physical destination Current popl's physical destination Current popO's physical destination Lowest: retiring pops physical desti nations Retirement act removing completed u.op ROB committing state appropriate permanent architectural state achine. ROB informs RAT retiring pop's destination longer found the350 MODERN PROCESSOR DESIGN reorder buffer must (from on) taken real register file (RRF). retiring PDst found array , matching entry (or entries) reset point RRF. retirement mechanism requires RAT three associative matches array PSrc three retirement pointers valid current cycle. matches found, corresponding array entries reset point RRF. Retirement lowest priority priority writeback mechanism; log- ically, tirement happen new uops write back. Therefore, uops want write back concurrently retirement reset, PDst write- back would happen last. Resetting floating-point register rename ap paratus complicated, due Intel Architecture FP register stack organization. Special hardware provided remove top-of-stack (TOS) offset FP register references. addition, retirement FP RAT (RfRAT) table maintained, conta ins non- speculative alias information floating-point stack registers. updated upon uop retirement. RfRA entry 4 bits wide: 1-bit retired stack valid 3-bit RRF pointer. addition, Rf RAT maintains nonspeculative TOS pointer. reason RfRAT's existence able recover mispredicted branches ot events presence FXCH instruction. FXCH macro-op swaps floating-point TOS register entry stack entry (including itself, oddly enough). FXCH could bee n implemented three MOV uops, using temporary register. Pentium processor-optimized floating-point code uses FXCH exte nsively arrange data dual execution units. Using three uops FXCH would heavy performance hi P6 processors Pentiu processor-optimized FP code, hence motivatio n imple- ment FXCH single uop. P6 processors handle FXCH operation FP part RAT (fRAT) merely swap arra pointers two source registers. requires extra write ports fRAT bviates swa p 80+ bits data two stack registers RRF. addition, since pointer swap operation would require th e resources execution unit, FXCH marked "completed" ROB soon ROB receives RAT. FXCH effectively takes RS resources executes zero cycles. number previous FXCH operations, fRA may specula- tively swap number entries ispredicte branch occurs. point, instructions issued branch ar e stopped. Sometime later, sig- nal asserted ROB indicating uops including branching nop retired. means arrays CPU reset, macroarchitectural state must res tored machine state existing time mispre dicted branch. trick able correctly undo effects speculative FXCHs. fRAT entries cannot simply reset con- stant RRF values, integer rename references are, numbe r retired FXCHs may occurred, fRAT must forevermore reme mber retired FXCH mappings. purpose retire ment fRAT: "know" reset FP entries front end must flushed.INTEL'S P6 MICROARCHITECTURE 35 73A3 Integer Retirement Overrides, retiring uop's PDst still ferenced RAT, retire ment RAT entry reverts pointing retirement register file. implies retirement uops must take precedence table r ead. operation performed bypass table read hardware. way, data read table ov erridden current uop retirement information. integer tirement override mechanism requires associative match integer arrays' PSrc entries retirement pointers valid current cycle. matches found, corresponding array entries reset point RRF. Retirement overrides must occur, retiring PSrc's read RAT longer point correct data. ROB array entries retiring dur- ing current cycle cannot referenced current uop (because data found RRF). 73AA New PDst Overrides. Micro-op logical source r eferences used indices RAT's multiported integer array, physical sources output array. sources ar e subject retirement overrides. time, RAT also receives newly alloca ted physical destinations (PDst's) allocator. Priority comparisons logical sources de stinations ID used gate either PSrc's integer array PDst's allocator actual renamed uop physical source s. Notice source 0 never overridden previous uop cycle dependent. block dia- gram RAT's override hardware shown Figure 7.13. Array PSrcsRenamed uop PSrcs LDstsAllocatorPDstsNote: one source renaming shown here. actually two source ports (Src 1 Src 2). Figure 7.13 RAT New PDst Overrides.352 MODERN PROCESSOR DESIGN Suppose following pops processed: popO: rl + r3 -> r3popl: r3+r2->r3pop2: r3+r4—>r5 Notice popl source relies th e destination reference popO. means data required popl found register pointed RAT, rather found new location provided allocator. PSrc information RAT made stale allocator PDst popO must beoverridden renamed pop physical urces output RS ROB. Also notice pop2 source uses register written popO popl. new PDst override control ust indicate PDst popl (not popO) appropriate pointe r use override pop2's source. Note pop groups mixture integer floating-point operations. Although two separate control blocks perform integer FP overrides, compariso n logical register names sufficiently isolates two classes pops. naturally case like types sources destina- tions override other. (For example, FP destination cannot override integer source.) Therefore, differences floating-point overrides han- dled independently integer mechanism. need floating-point overrides integer overrides. Retirement c oncurrent issue pops prevent array updated newest information conc urrent pops read array. There- fore, PSrc information read RAT arrays must overridden retirement overrides new PDst overrides. Floating-point retirement ove rrides identical integer retirement over- rides except value PSrc ove rridden detennined logical register source name integer case. Rather, retiring logical reg- ister destination reads RfRAT rese value. Depending retire- ment pop content addressable memory (CAM) matched array read, retirement override control must choose one three RfRAT reset values. hese reset values must modified concurrent retiring FXCHs well. 7.3.4.5 RAT Stalls. RAT stall two ways, internally externally. RAT generates internal stall unable comple tely process cur- rent set pops, due partial register write, flag mismatch, microar- chitectural conditio ns. allocator may also unable proc ess pops due RS ROB table overflow; external stall RAT. Partial Write Stalls. partial-width write (e.g., AX, AL, AH) followed larger-width read (e.g., EAX), RAT must stall u ntil last partial-width write desired register retired. point, portions register reassembled RRF, single PSrc specified required data.INTEL'S P6 MICROARCHITECTURE 31 RAT performs function maintaining size information (8,16, 32 bits) register alias. handle independent use 8-bit registers, two entries aliases (H L) intained integer array registers EAX, EBX, ECX, EDX. (The othe r macroregisters cannot par- tially written, pe r Intel Architecture specification.) 16- 32-bit writes occur, entries updated. 8-bit writes oc cur, corre- sponding entry (H L, both) updated. Thus entry targeted logical source, size information read array com pared requested size information specified pop. size needed greater size available (read array), RAT stalls instruction decoder allocator. addition, RAT clears "valid bits" pop causing stall (and pops younger is) partial write retires; in-order pipe, subsequent pops cannot allowed pass stalling pop here. Mismatch Stalls. Since read ing writing flags common occurrences therefore performance-critical, renamed reg isters are. two alias entries flags, e arithmetic flags one floating- point c ondition code flags, maintained much f ashion integer array entries. pop known write flags, PDst granted pop written corresponding flag entry (as well des- tination register entry). subsequent pops use flags source, appro- priate flag entry read find PDst flags live. addition general renaming scheme, pop emitted ID associated flag information, form masks, tell RAT flags pop touches flags pop needs input. event previous yet retired pop touch flags current pop needs input, RAT stalls in-order machine. informs ID allocator new pops driven RAT one current pops cannot issued previous flag write retires. 7.3.5 Allocator clock cycle, allocator assumes allocate three reor- der buffer, reservation station, load buffer entries two store buffer entries. allocator generates pointers thes e entries decodes pops coming ID unit determine many entries eac h resource really needed RS dispatch port dispatched on. Based pop decoding valid bits, allocator determine whether resource needs met. not, stall asse rted pop ssue fro- zen sufficient resources become available retirement previous pops. first step allocation decoding pops delivered ID. pops need LB SB entry; pops need ROB en try. 7.3.5.1 ROB Allocation. ROB entry addr esses physical destinations PDst's assigned allocator. PDst's used directly354 MODERN PROCESSOR DESIGN address ROB. means ROB full, allocator mus assert stall signal early enough prevent overwriting valid ROB data. ROB buffer treated circular buffer allocator. words, entry addresses assigned sequentially 0 highest address, wraparound back 0. three-or-none allocation policy used: every cycle, least three ROB entries mus available allocator stall. means ROB allocation independent type uop even depen uop's validity. three-or-none policy simplifies allocation. At end cycle, address last ROB entry allocated pre- served becomes new allocation starting point. Note depend real number valid uops. ROB also uses number valid uops determine stop retiring. 7.3.5.2 MOB Allocation. uops load buffer ID store buffer ID (together known MOB ID, MB ID) stored them. Load uops newly allocated LB address last SB address llocated. Nonload uops (store uop) MBID LBLD = 0 SBID (or store color) last store allocated. LB SB treated circular buffers, ROB. However, allocation policy slightly different. Since every uop need LB SB entry, would big performance hit use three-or-none policy (or two-or- none SB) stall whenever LB SB less three fre e entries. Instead use all-or-none policy. means stalling occur valid MOB Uops allocated. Another important part MOB allocation handling entries containing senior stores. stores committed retired CPU still actually awaiting completion execution memory. store buffer entries cannot deallocated u ntil store actually performed memory. 7.3.5.3 RS Allocation. allocator also generates write enable bits used RS directly entry enables. RS full, stall indication must given early order pr event overwrite valid RS data. fact RS full, enable bits cleared thus entry enabled writing. RS full stall occurs due som e resource conflict, RS invalidates data written RS entry cycle (i.e., data get written marked invalid). RS allocation works differently ROB MOB circular buffer model. Since RS dispatches uops order (as become data-ready), free entries typically interspersed used alloca ted entries, circu- lar buffer model work. Inst ead, bitmap scheme used RS entry maps bit RS allocation pool. way, entries may rawn replaced pool order. Th e RS searches free entries scanning location 0 first three free entries found. Uops dispatch one port, act committing given uop given port called binding. binding uops RS functional unit interfaces done allocation. allocator load-balancing al gorithmINTEL'S P6 MICROARCHITECTURE 3 knows many uops RS waiting execute given ter- face. algorithm used uops execute one EU. referred static binding load balancing ready uops exe- cution interface. 7.4 Out-of-Order Core 7.4.1 Reservation Station reservation station (RS) basically place uops wait oper- ands become ready th e appropriate execution unit become avail- able. c ycle, RS determines execution unit availability source data validity, perfo rms out-of-order scheduling, dispatches uops execution units, controls data bypassing RS array execution units. entries RS identical hold kind uops. RS 20 entries. control portion entry (uop, entry valid, etc.) written one three ports (there three ports P6 microarchitecture superscalar order 3.). information comes allocator RAT. data portion entry written one six ports (three ROB three execution uni writebacks). CAMs control sn arfing valid writeback data uop Src fields data bypassing execution unit (EU) interfaces. CAMs, EU arbitration, control information used determine data validity EU availability entry (ready bit generation). scheduler logic uses ready information schedule five Hops. entries scheduled dispatch read array driven execution unit. pipe stage 31, RS etermines entries are, be, ready dispatch stage 32. this, necessary know availability data execution resources (EU/AGU units). ready information sent scheduler. 7.4.1.1 Scheduling. basic function scheduler enable dis- patching five Hops per clock RS. RS five schedulers, one execution unit interface. Figure 7.14 shows mapping functional units RS ports. RS uses priority poin ter specify scheduler begin scan 20 entries. priority pointer change according pseudo- FIFO algorithm. used reduce stale entry effects increase perfor- mance RS. 7.4.1.2 Dispatch. RS dispatch five Hops per clock. two EU two AGU interfaces one store data (STD) interface. Figure 7.14 shows connect ions execution units RS pons. instruction dispatch time, RS etermines whether resources needed par- ticular Hop execute available, ready entries scheduled. RS dispatches necessary Hop information scheduled functional unit. H°P en dispatched functional unit cancellation has356 MODERN PROCESSOR DESIGN RATRS PortO Portl Port 2 Port 3 Port 4 Bypass networkNote: one source shown per RS port. source identical wha shown. Writeback bus 0 JEUOf £j Fadd |J L| Fmul |1 LffanJ^jTL] Div | Writeback bus 1 jlEUl \ Lfjju~) Ld data memory -»|AGUO|LDAddr -»-)AGUlfSTAddrLDA DCU MOBSTA DCU MOB STDDCU MOB DCU Store data Ld data memory Writeback bus 0 Writeback bus 1 New jiopsROB RRF Figure 7.14 Execution Unit Data Paths. occurred due cache miss, entry deallocated use new pop. Every cycle, deallocation pointers used signal ocator avail- ability 20 entries RS. 7A1.3 Data Writeback. possible source data valid time RS entry initially written. (top must remain RS sources va lid. content addressable memories (CAMs) used compare writeback physical destination (PDst) stored physical sources (PSrc). match occurs, corresponding write enables asserted snarf needed writeback data appropriate source array. 7.4.1.4 C ancellation. Cancellation inhibiting pop sched- uled, dispatched, executed due cache miss possible future r esource con flict.INTEL'S P6 MICROARCHITECTURE canceled pops rescheduled later time unless out-of-order machine reset. times writeback data invalid, e.g., memory unit detects cache miss. case, dispatching pops dependent writeback data need canceled rescheduled later time. hap- pen RS pipeline assumes cache accesses hits, schedules dependent pops based assumption. 7.5 Retirement 7.5.1 Reorder Buffer reorder buffer ( ROB) part icipates three fundamental aspects P6 microarchitecture: speculative e xecution, register renaming, out-of-order exe- cution. ways, ROB similar register file in-order machine, additional functionality support retirement speculative operations register renam ing. ROB supports speculative execution buffering sults exe- cution units (EUs) commi tting architectu rally visible state. allows microengine fetch execute instructions maximum rate assuming branches properly predicted exc eptions occur. branch mispredicted exception occurs executing instruc- tion, microengine recover simply discarding speculative r esults stored ROB. mic roengine also restart proper instruction examining committed architectural state ROB. key function ROB control retirement completion pops. buffer storage EU results also used support register naming. EUs write result data renamed register ROB. retire- ment logic ROB updates architectural registers based upon contents renamed instance architectural registers. Micro-ops source architectural register obtain either contents actual architectural register contents renamed register. Since P6 microarchitecture supersca- lar, different pops clock use architectural register may fact access different physical registers. ROB supports out-of-order execution allowing EUs complete pops write back results without regard pops executing simultaneously. Therefore, far execution units con cerned, pops com- plete order. ROB tirement logic reorders completed pops original sequence issued instruction decoder updates architectural state retirement. ROB active three separate parts proc essor pipeline (refer Figure 7.4): rename register read stages, execute/writeback stage, retirement stages. placement ROB relative ot units P6 shown block diagram Figure 7.1. ROB closely tied allocator (ALL) register358 MODERN PROCESSOR DESIGN alias table (RAT) units. allocator manages ROB physical registers support speculative operations register renaming. actual renaming architectural registers ROB managed RAT. allocator RAT func- tion within in-order part P6 pipeline. Thus, rename register read (or ROB read) functions performed sequence program flow. ROB interface reservation station (RS) EUs out-of- order part machine loosely coupled nature. data read ROB register read pipe stag e consist operand sources Uop. operands stored RS uop dispatched execution unit. EUs write back uop results ROB five writ eback ports (three full writeback ports, two partial writ ebacks STD TA). result writeback order respect uops issued instruction decoder. results EUs speculative, exceptions detected EUs may may "real." exceptions w ritten special field Uop. turns Uop misspeculated, exception "real" flushed along rest Uop. Otherwise, ROB notice exceptional condi tion retirement flop cause appropriate exception-handling action invoked then, making deci- sion commit uop's result architectural state. ROB retirement logic ha important interfaces micro-instruction sequencer (MS) memory ordering buffer (MOB). ROB/MS interface allows ROB signal exception MS, forcing micro-instruction sequencer jump particular exception handler microcod e routine. Again, ROB must force control flow change EUs report events ou order respect program flow. ROB/MOB interface allows MOB commit memory state stores store uop committed machine state. 7.5.1.1 ROB Stages Pipeline. ROB active in-order out-of-order sections P6 pipeline. ROB used in-order pipe pipe stages 21 22 . Entries reorder buffer hold results speculative uops allocated pi pe stage 21. reorder buffer managed allocator retirement logic circular buffer. unused entries reorder buffer, allo cator use Uops issued clock. entries used signaled RAT, allowing update renaming alias tables. addresses entries used (PDst's) also written RS Uop. PDst key token use out-of-order section machine iden tify uops execution; actual slot number ROB. entries ROB allocated, certain fields written data fields uops. information written either allocation time results written back EUs. reduce width RS entries well reduce amount information must circulated EUs memory su bsystem, uo p information required retire Uop deter- mined strictly decode time written ROB allocation time. pipe stage 22, immediately fol lowing entry allocation, sources uops read ROB. physical source addresses, PSrc's, deliveredINTEL'S P6 MICROARCHITECTURE 3! RAT based upon alias table update performed pipe tage 21. source may reside one three places: committed architectural state (retirement register file), reorder b uffer, writeback bus. (The RRF contains architec tural state micro code visible stat e. Subsequent ferences RRF state call macrocode microcode visible state). Source operands read RRF ar e always valid, ready execution unit use. Sources read ROB may may val id, depending timing source read respect writebacks previous Uops updated entrie read. source operand delivered ROB invalid, RS wait u ntil EU writes back PDst matches physica l source address source operand order capture (or bypass EU) valid source operand given uop. EU writes back destination data entry allocated (lop, along event information, pipe stage 83. (Event refers exceptions, inter- rupts, microcode assists, on.) writeback pipe stage decoupled rename register read pi pe stages uops issued order RS. Arbitration use write back busses detenrtined EUs along RS. ROB simply terminus writeback busses stores whatever data busses writeback PDst's signaled EUs. ROB retirement logic commits macrocode microcode visible state pipe stages 92 93. Th e retirement pipe stages decoupled writeback pipe stage writebacks order respect program microcode order. Retirement e ffectively reorders out-of-order completion Uops EUs in-order completion uops machine whole. Retirement two-clock operation, retiremen stages pipe- lined. allocated entries reorder b uffer, retirement logic attempt deallocate retire them. Retirement treats reorder buffer FIFO deallocating entries, since Uops originally allocated sequential FIFO order earlier pipeline. ensures retir ement follows original program source order, terms allowing architectural state modified. ROB contains P6 macrocode microcode state may modified without serialization mac hine. (Seri alization limits one number uops may flow out-of-order section machine, effectively making execute order.) Much state updated directly speculative state reorder buffer. extended instruction pointer (EIP) one archit ectural register exception norm. EIP requires signi ficant amount hardware ROB update. reason number uops may retire clock varies zero three. ROB implemented multiported register file separate ports allocation time writes uop fields needed retir ement, EU writebacks, ROB reads sources RS, retirement logic reads speculative result data. ROB 40 entries. entry 157 bits wide. ocator retire- ment logic manage register file FIFO. source read destination writeback functions treat reorder buffer register file.360 MODERN PROCESSOR DESIGN Table 7.1 Registers RRF Qty. Register Name(s) Size (bits) Description 8 i486 general registers 32 EAX, ECX, EDX, EBX, EBP, ESP, ESI, EDI 8 i486 FP stack registers 86 FST(0-7) 12 General microcode temp, registers86 storing integer FP values 4 Integer microcode temp, registers32 storing integer values 1 EFLAGS 32 i486 system flags register 1 ArithFlags 8 i486 flags ar e renamed 2 FCC 4 FP condition codes 1 EIP 32 architectural instruction pointer 1 FIP 32 architectural FP instruction pointer 1 EventUIP 12 micro-instructio n reporting event 2 FSW 16 FP status word RRF contains macrocode microcode visible st ate. processor state located RRF, state may renamed there. Table 7.1 gives listing registers RRF. Retirement logic generates addresses retirem ent reads performed clock. retirement logic also computes tirement valid signals indi- cating entries valid writeback data may retired. IP calculation block produces architectural instruction pointer well several macro- micro -instruction pointers. macro-instruction pointer generated based l engths al l macro-instructions may retire, well branch target addresses may delivered jump execution unit. ROB det ermined processor started ex ecute operations wrong path branch, operations path must allowed retire. ROB accomplishes asserting "clear" signal point first operations would retired. speculative operations flushed machine. ROB retires operation faults, clears in-order out-of-order sections machine pipe stages 93 94. 7.5.1.2 Event Detection. Events include faults, traps, assists, interrupts. Every entry reorder buffer event information field. execution JINTEL'S P6 MICROARCHITECTURE units write back field. retirement ret irement logic looks field three ent ries candidates retirement. event information field tells r etirement logic whether exception whether fault trap assist. Interrupts signaled directly int errupt unit. jump unit marks event information field case taken mispred icted branches. event detected, ROB clears machine pops forces MS jump microcode event handler. Event records saved allow microcode handler properly repair th e result invoke correct macrocode handler. Macro- micro-instr uction pointers also saved allow program resumption upon termination event handler. 7.6 Memory Subsystem memory ordering buffer (MOB) part memory subsystem P6. MOB interfaces p rocessor's out-of-order engine memory sub- system. MOB contains two main buffers, lo ad buffer (LB) store address buffer (SAB). buffers circ ular queues entry within th e buffer representing either load store micro-operation, respec- tively. SAB works unison memory interface unit's (MIU) store data buffer (SDB) DCache's physical address buffer (PAB) effectively manage processor store operation. SAB, SDB, PAB viewed one buffer, store buffer (SB). LB contains 16 buffer entries, holding 16 loads. LB queues load operations unable complete origi nally dispatched reservation station ( RS). queued operations redispatched conflict removed. LB maintains processor ordering loads snooping external writes completed loads. second processor's w rite specula- tively read memory location forces out-of-order engine clear restart load operation (as well younger pops). SB contains 12 entries, holding 12 store operations. SB used queue store operations dispatch memory. stores dispatched original program order, OOO engine signals heir state longer speculative. SAB also checks l oads store address con- flicts. checking keeps loads con sistent previously executed stores still SB. MOB resources allocated allocator load store opera- tion issued reservation tation. load operation decodes one pop store operation decoded two pops: store data (STD) store address (STA). allocation ime, operation tagged eventual location LB SB, collectively referred MOB ID (MBID). Splitting stores two distinct pops allows possible concurrency generation address data stored expressed. MOB receives speculative LD STA operations reserv ation station. RS provides opcode, address generation unit (AGU)362 MODERN PROCESSOR DESIGN calculates provides linear address th e access. DCache either executes operations imme diately, dispatched later MOB. either case written one MOB arrays. memory operations, data translation lookaside buffer (DTLB) converts linear address physical address signals page miss page miss handler (PMH). MOB also p erform numerous checks linear address data size determine operation continue must block. case load, data cache unit expected return data core. parallel, MO B writes addr ess status bits LB, signal operation's completion. case STA, MOB completes operation writing valid bit (Ad dressDone) SAB array reorder buffer. indicates address portion store completed. data portion store executed SDB. SDB signal ROB SAB data received written b uffer. MOB retain store information ROB indicates store operation retired committed processor state. dispatch MOB data cache unit commit store system state. completed, MOB sig- nals deallocation SAB resources reuse allocator. Stores executed memory subsystem program order. 7.6.1 Memory Access Ordering Micro-op register operand dependences trac ked explicitly, based regis- ter references original program instructions. Unfortunately, memory opera- tions implicit dependences, load operations dependency previous store address overlap load. operations often speculative inside MOB, stores loads, system memory access may return stale data produce incorrect results. maintain self-consistency loads stores, P6 employs con- cept termed store coloring. load operation tagged store buffer ID (SBID) store previous it. ID represents relative location load compared stores execution sequence. load executes memory subsystem, MOB use SBI beginning point ana- lyzing lo ad olde r stores b uffer, also allowing MOB ignore younger stores. Store coloring used maintain ordering consistency loads stores processor. similar problem occurs processors multiprocessing system. loads execute order, effectively make another processor's store operations appear order. results younger load passing older load performed yet. younger load reads old data, older load, performed, chance read- ing new data writte n another proce ssor. allowed commit state, loads would violate processor ordering. prevent violation, LB watches (snoops) data writes bus. another proc essor writes location speculatively read, speculatively completed load subsequent operations cleared re-executed get correct data.INTEL'S P6 MICROARCHITECTURE 3t 7.6.2 Load Memory Operations Load operations issue RS allocator register allocation table (RAT). allocator assigns new load buffer ID (LBID) load issues RS. allocator also assigns store color load, SBID last store previously alloc ated. load waits RS data operands become available. avail able, RS dispatches l oad port 2 AGU LB. Assuming dispatches waiting fo r port, LB bypasses operation immediate execution memory subsystem. AGU generates linear address used DTLB, MOB, DCU. DTLB translation physical address, DCU initial data lookup using lower-order 12 bits. Likewise, SAB uses lower-order 12 bits along store color SBID check potential conflicting addresses previ- ous stores (previous p rogram order, time order). Assuming DTLB page hit SAB conflicts, DCU uses physical address final tag match return correct data (assuming miss block). completes load operation, RS, ROB, MOB write ir completion status. SAB noticed address match, SAB would cause SDB forward SDB data, ignoring DCU data. SAB conflict existed addresses match (a f alse conflict detection), load would blocked written LB. load wait conflicting store left store buffer. 7.6.3 Basic Store Memory Operations Store operations split two micro-ops, store data (STD) followed store address (STA). Since store represented combination operations, allocator allocates store buffer entry STD issued RS. allocation store buffer entry reserves location SAB, SDB, PAB. store's source data become available, RS dispatches STD port 4 MOB writing SDB. STA address source data become available, RS dispatches STA port 3 AGU SAB. AGU generates linear ddress translation DTLB writing SAB. Assuming DTLB page hit, physical address written PAB. Th completes STA operation, MOB ROB update th eir completion status. Assuming faults mi spredicted branches, ROB r etires STD STA. Monitoring retirement, SA B marks store (STD/STA pair) committed, senior, processor state. senior, MOB dispatches operations sending opcode, SBID, lower 12 address bits DCU. DCU MIU use SBID access physical address PAB store data SDB, respectively, complete final store operation. 7.6.4 Deferring Memory Operations general, memory operations expected complete three cycles dispatch RS (which two clocks longer ALU operation). However, memory operations totally predictable translation availability LI cache. cases these, operations require other164 MODERN PROCESSOR DESIGN resources, e.g., DCU fill buffers pending cache miss, may avail- able. Thus, operations must deferred resource becomes available. MOB load buffer employs general mechanism blocking load memory operation -a later wakeup received. blocking information associated entry load bu ffer contains two fields : blocking code type blocking identifier. block code identifies source block (e.g., address block, PMH resource block). block identifier refers specific ID resource associated block code. wakeup signal received, deferred memory ope rations match blocking code identifier marked "ready dispatch." load bu ffer schedules dis patches one ready operations manner similar RS dispatching. MOB store buffer uses restricted mechanism blocking STA memory operations. operations remain blocked ROB retirement pointers indi- cate STA pop oldest nonretired operation machine. operation dispatch retirement write DCU occurring simulta- neously dispatch STA. simplified mechanism tores used ST rarely blocked. 7.6.5 Page Faults DTLB translates linear addresses physical addresses memory load store address pops. DTLB address translation performing lookup cache array physical address page accessed. DTLB also caches page attributes physical address. DTLB uses information check page protection faults paging-related exceptions. DTLB stores physical addresses subset possible memory pages. address lookup fails, DTLB signals miss PMH. PMH executes page walk fetch physical address page tables located physical memory. PMH looks effective memory type physical address on-chip memory type range registers supplies physical address ef fective memory type DTLB store cache array. (These memory type range registers usually configured processor boot time.) Finally, DTLB performs fault detection writeback various types f aults including page faults, assists, machine check architecture errors DCU. true data ins truction pages. DTLB also checks I/O data breakpoint traps, either writes back (for store address pops) passes (for loads I/O pops) results DCU responsible supplying data ROB writeback. 7.7 Summary design described thi chapter began brainchild authors chapter, also reflects myriad c ontributions hundreds designers, micro- coders, vali dators, performance analysts. Subject economics rule Intel's approach business, tried times obey prime directive: Make choices maximize delivered performance, quantify choicesINTEL'S P6 MICROARCHITECTURE 36 wherever poss ible. out-of-order, speculative execution, superpipelined, super- scalar, micro-dataflow, register-renaming, glueless multiprocessing design described result. Intel shipped approximately one billion P6-based microprocessors 2002, many fundamental ideas described chapter reused Pentium 4 processor generation. details P6 microarchitecture found Colwell Steck [1995] Papworth [1996]. 7.8 Acknowledgments design P6 microarchitecture collaborative effort among large group ar chitects, designers, validators, others. microarchitecture described benefited enormously contributions extraordinarily talented people. also contributed text des criptions found chapter. Thank you, one all. would also like thank Darrell B oggs careful proofreading draft chapter. REFERENCES Colwell, Robert P., Randy Steck: "A 0.6 |im BiCMOS microprocessor dynamic execution," Proc. Int. Solid State Circuits Conference, San Francisco, CA 1995, pp. 1 76-177. Lee, J., A. J. Smith: "Branch predictions br anch target buffer desi gn," IEEE Computer, January 1984,21,7, pp . 6-22. Papworth, David B.: "Tuning Pentium Pro microarchitecture," IEEE Micro August 1996, pp. 8-15. Yen, T.-Y., Y. N. Patt: "Two-level adaptive branch prediction," 24th ACM/IEEE Int. Symposium Workshop Microarchitecture, November 1991, pp. 51-61. HOMEWORK PROBLEMS P7.1 PowerPC 620 implement load/store forwarding, Pentium Pro does. Explain design teams likely made right design tradeoff. P7.2 P6 recovers. branch mis predictions somewhat coarse- grained manner, illustrated Figure 7.5. Explain implifies misprediction recovery logic manages reorder buffer (ROB) well register alias table (RAT). P7.3 AMD's Athlon (K7) processor takes somewhat different approach dynamic translation IA32 macro-instructions machine instructions actually executes. example, ALU ins truction one memory operand (e.g., add eax.feax]) would tr anslate two uops Pentium Pro: load writes temporary register fol- lowed register-to-register add instruction. contrast, Athlon1 6 6 MODERN PROCESSOR DESIGN would simply dispatch original instruction issue queue macro-op, would issue queue twice: load ALU operation. Identify discuss least tw microar- chitectural benefits accrue "macro-op" approach instruction-set translation. P7.4 P6 two -level branch predictor speculative nonspecula- tive branch history register stored entry. Describe branch history register updated, provide reason- ing justifies design decision. P7.5 P6 dynamic branch predictor backed static predictor able predict branch instructions reason pre- dicted dynamic predictor. static prediction occurs pipe stage 17 (refer Figure 7.4). One scenario th e stati c predic- tion used occurs th e BTB reports tag mismatch, reflecting fact branch history information pa rticular branch. Assume static branch prediction tu rns correct. One possi- ble optimization would avoid installing branch (one statically predictable) BTB, since might displace another branch needs dynamic prediction. Discuss least one reason might bad idea. P7.6 Early P6 development, design two different ROBs, one integers anothe r floating-point. save die ize, combined one Pentium Pro development. Explain advantages disadvantages separate integer ROB floating-point ROB versus unified ROB. P7.7 timing diagrams see P6 ret irement process takes three clock cycles. Suppose knew way implement ROB retirement took two clock cycles. Would expect substantial performance boost? Expl ain. P7.8 Section 7.3.4.5 describes mismatch stall occurs condition flags partially written in-flight pop. Sug gest solution would prevent mismatch stall occurring renaming process. P7.9 Section 7.3.5.3 escribes RS allocation policy Pentium Pro. Based description, would call P6 centralized RS design distr ibuted RS design? Justify answer. P7.10 P6 microarchitecture support instniction set in- cluded predication, effect would register renaming process? P7.ll described text, P6 microarchitecture splits store opera- tions STA STD pa ir handling address generation data movement. Explain makes sense microarchitec- tural implementation perspective.INTEL'S P6 MICROARCHITECTURE P7.12 Following Problem 7.11, would performance benefit (measured instructions per cycle) stores split? Explain not? P7.13 changes would one make P6 microarchitecture accommodate stores split separate STA STD oper- ations? woul likely effect cycle time? P7.14 AMD recently announced x86-64 extensions Intel IA32 architecture add support 64-bit registers addressing. Investigate extensions (more information available www.amd.com ) outline changes would need make P6 architecture accommodate thes e additional instructions.Mark Smotherman Survey Superscalar Processors CHAPTER OUTLINE ftl * Development Superscalar Proc essors 82 Classification Recent Designs 8.3 PrccessctfCJescrjptiortt 8.4 Verification Superscalar Proc essors 8.5 Ad(novv4edgrnents References Homework Problems 1990s decade superscalar processor design blossomed. However, idea decoding issuing multiple instructions per cycle single instruction stream dates back 25 years that. chapter review history superscalar design examine number selected designs. 8.1 Development Superscalar Processors section reviews history superscalar design, beginning IBM Stretch direc superscalar descendant, Advanced Computer System (ACS), follows developments current processors. 8.1.1 Early Advances Uniprocessor Parallelism: IBM Stretch first e fforts call super scalar instruction issue started IBM machine directly descended IBM Stretch. use ag- gressive implementation tec hniques (such pre-decoding, out-of-order execution, speculative exe cution, branch misprediction recovery, precise exceptions) precursor IBM ACS 1960s (and RS/6000 POWER 369370 MODERN PROCESSOR DESIGN architecture 1990s), appropriate review Stretch, also known IBM 7030 [Buchholz, 1962]. Stretch design started 1955 IBM lost bid high-performance decimal computer system University California Radiation Laboratory (Livermore Lab). Univac, IBM's competitor minant computer man ufac- turer time, contract build Livermore Automatic Research Computer (LARC) promising deli requested machine 29 months [Bashe et al., 1986]. IBM aggressive, bid based renegotiation clause machine four five times faster requested cost $3.5 million rather requested $2.5 million. following year, IBM bid binary computer "speed least 100 times greater existing machines" Los Alamos Scientific Laboratory contract would become Stretch. Delivery slated 1960. Stephen Dunwell chosen head project, among recruited design effort Gerrit Blaauw, Fred Brooks, John Cocke, Harwood Kolsky. Blaauw Brooks investigated instruction set design ideas, would later serve worked IBM S/360, Cocke Kolsky constructed crucial simulator would help team explore organization options. Erich Bloch, later become chief scientist IBM, named engineer- ing manager 1958 led implementation efforts prototype units year engineering model 1959. Five test programs selected simulation help determine machine parameters: hydrodynamics mesh problem, Monte Carlo ne utron-diffusion code, inner loop second neutron diffusion code, polynomial evaluation routine, inner loop matrix inversion routine. Severa l Stretch instructions intended scientific computation kind, branch-on-coun multiply-and- add (called cumulative multiply Stretch later known fused multiply add), would become important RS/6000 performance 30 years later. Instnictions Stretc h flowed two processing elements: indexing instruction unit fetched, pre-de coded, partially executed instruc- tion stream, arithmetic uni executed remainder instructi ons. Stretch also partitioned registers according organization; set sixteen 64-bit index registers associated wit h indexing ins truction unit, set 64-bit accumulators registers associated arithmetic unit. Partitioned register sets also appear ACS RS/6000. indexing instruction unit (see Figure 8.1) Stretch fetched 64-bit memory words two-word ins truction buffer. Instructions could either 32 64 bits length, four instructions could buffered. indexing instruction unit directiy executed indexing instructions prepared arithmetic instructions calculating effective addresses (i.e., adding index register contents address fields) starting memor operand fetches. unit pipe- lined decoded instructions parallel execution. One interesting feature instruction fetch logic addition pre-decoding bits instructions; done one word time, two half-word instructions could pre- decoded parallel.SURVEY SUPERSCALAR PROCESSORS 3i Memory (two-way interleaved)Memory (four-way interleaved) Instructions E3Addresses Indexing instruction unit (index registers)Partially fully executed instructions, old index valuesInstructions typically allocated two-way interleaved memory, data typically allocated four-way interleaved memory Data Index value recoveryLookahead Ready instructions data Parallel serial arithmetic units (arithmetic registers) Figure 8.1 IBM Stretch Block Diagram. Unconditional branches conditiona l branches depended state index registers, branch-on-count instruction, could fully executed indexing instruction unit (compare branch unit RS/6000). Conditional branches depended state arithmetic registers pre- dicted untaken, untaken path speculatively executed instructions, either fully executed prepared, placed novel form buffering called lookahead unit, time also called virtual memory would view today combination completion buffer history buffer. fully executed indexin g instruction would placed one fo ur levels lookahead along instruction address previous value index register modified. history old values provided way lookahead levels rolled back thus restore contents index registers mispr edicted branch, interrupt, exception. prepared ar ithmetic instruction would also placed lookahead l evel along instruction address, would wait completion memory operand fetch. feature foreshadows many current processors complex Stretch instructions broken separate parts stored multiple lookahead levels. arithmetic instruction would executed arithmetic unit whenever lookahead level became oldest memory operand available. Arithmetic exceptions made precise causing rollback lookahead levels, would done case mispredicted branch. store instruc- tion also executed lookahead level became oldest. store lookahead , store forwarding implemented checking memory address subsequent load placed lookahead. address read matched address stored, load canceled, store value was372 MODERN PROCESSOR DESIGN directly copied buffer reserved th e load value (called short-circuiting). one outstanding store time allowed lookahead. Also, potential instruction modification, store address compared instruction addresses lookahead levels. Stretch implemented 169,100 transistors 96K 64-bit words core memory. clock cycle time 300 ns (up initial estimates 100 ns) indexing unit lookahead unit, clock cycle time variable-length field unit parallel arithmeti c unit 600 ns. Twenty-three levels logic allowed path, connection approximately 15 feet (ft) counted one-half level. parallel rithmetic unit performed one floating- point add 1.5 us one floating-point multiply every 2.4 us. pro cessing units dissipated 21 kilowatts (kW). CPU alone (without memory banks) measured 30 ft 6 ft 5 ft clock c ycle change indicates, Stretch live ini tial performance promises, ranged 60 100 times performance 704. 1960, product planners set price $13.5 million commercial form Stretch, 7030. estimated perform ance would eight times performance 7090, eight times performance 704. estimation heavily based arithmetic opera tion timings. Stretch became operational 1961, nchmarks indicated four times faster 7090. di fference large part due store latency branch misprediction recovery time, ince cases stalled arithmetic unit. Even though Stretch f astest computer world (and remained introduction CDC 6600 1964), performance shortfall caused considerable embarrassment IBM. May 1961, Tom Watson announced price cut 7030s negotiation $7.7 8 million immedi- ately withdrew product sales. tretch turned slower expected delivered year later planned, provided IBM enormous advances transisto r design computer organization principles. Work Stretch circuits allowed IBM deliver first popular 7090 series 13 months initial contract 1958; and, multiprogramming, memory pr otection, generalized interrupts, 8-bit byte, ideas originated Stretch subsequently used successful S/360. Stretch also pioneered techniques uniprocessor parallelism, including decoupled access-execute executi on, speculative execu tion, branch misprediction recovery, precise exceptions. also first machine use memory inter- leaving first buffer store values provide forwarding subs equent loads. tretch provided wonderful training ground John Cocke others would later pr opose investigate idea parallel decoding multiple instruc- tions foll ow-on designs. 8.1.2 First Superscalar Design: IBM Advanced Computer System 1961, IBM started planning fo r two high-performance projects exceed capabilities Stretch. Project X goal 10 30 times performance Stretch, led announcement IBM S/360 Model 91 1964 andSURVEY SUPERSCALAR PROCESSORS 3' delivery 1967. Model 91's floating-point unit famous executing instructions out-of-order, according al gorithm devised Robert Tomasulo. initial cycle time goal Project X 50 ns, Model 91 shipped 60-ns cycle time. Mike Flynn project manager IBM S/360 Model 91 le ft IBM 1966. second project, named Project Y, goal building machine 100 times faster Stretch. Project started 1961 IBM Watson Research Center. However, Watson's overly critical assessment tretch, Project languished 1963 announcement CDC 6600 (which com- bined scalar instruction issue out-of-order instruction execution among 10 execution units ran 100-ns cycle time; see Figure 4.6). Project assigned Jack Bertram's experimental computers programming group; John Cocke, Brian Randell, Herb Schorr began playing major roles defining circuit technology, instruction set, compiler te chnology. late 1964, sales CDC 6600 announcement 25-ns cycle time 6800 (later redesigned renamed 7600) added urgency Project effort. Watson decided "go fo r broke advanced machine" (memo ated May 17, 1965 [Pugh, 1991]), May 1965, su percomputer laboratory established Menlo Park, California, direction Max Paley Ja ck Bertram. architecture team led Herb Schorr, circuits team Bob Domenico, compiler team Fran Al len, engineering team Russ Robelen. John Cocke arrived California work compilers 1966. design became known Advanced Computer System 1 (ACS-1 ) [Sussenguth, 1990]. initial clock cycle time goal ACS-1 10 ns, aggressive goal embraced 1000 times performance 7090. reach cycle time goal, ACS -1 pipeline designed target five gate levels logic per stage. overall plan ambitious included optimizing com- piler well new operating system, streamlined I/O channels, multi- headed disks integral parts system. Delivery first anticipated 1968 expected customers Livermore Los Alamos. However, late 1965, target introduction date moved back 1970 time frame. Like CDC 6600 modem RISC architectures, CS-1 instructions defined three register specifiers. thirty-one 24-bit ndex registers thirty-one 48-bit arith metic r egisters. targeted number- crunching national labs, single-precision float ing-point data used 48-bit format double-precision data used 96 bits. ACS-1 also used 31 backup regis- ters, one paired corresponding arithmetic register. provided form register renaming, load writeback could occur backup regis- ter whenever dependency previous register value still outstanding. Parallel decoding multiple instructions dispatch two reservation stations, one whic h provided out-of-order issue, proposed proces- sor (see Figure 8.2). Schorr wrote hi 1971 paper ACS-1 "multiple decoding new function examined project." Cocke 1994 inter- view stated arrived idea multiple instruction decoding ACS-1 response IBM internal report written Gene Amdahl early 1960s374 MODERN PROCESSOR DESIGN Fetch branch unitIndex instruction decode bufferSix index functional units Arithmetic instruction decode bufferSeven arithmetic functional unitsMemory system 2 Id/st 1 fetch ll/O per cycle Out-of-order issueLoaded data placed "backup" registers Figure 8.2 IBM ACS Block Diagram. Amdahl postulated one instruction decode per cycle one funda- mental limits obtainable performance. Cocke wanted test supposed fun- damental limitation decided tha multiple decoding feasible. (See also Flynn [1966] discussion limit difficulty multiple decoding.) Although Cocke made early proposals methods multiple instruction issue, late 1965 Lynn Conway made contributio n general- ized scheme dynamic instruction scheduling used design. described contender stack scheduled instructions terms source des- tination scheduling matrices busy vector. Instruction decoding filling matrices would stop appearance conditional branch resume branch resolved. matrices also scanned reverse order give priority issue conditional branch. resulting ACS-1 processor design six function units index opera- tions: compare, shift, add, branch address calculation, two effective address adders. seven function units arit hmetic operations: compare, shift, logic, add, divide/integer multi ply, floating-point add, floating-point multiply. seven instructions could issued per cycle: three index operations (two could load/stores), three arithmetic operations, one branch. e ight-entry load/store/index instruction buffer could issue three instructions order. eight-entry arithmetic instruction buffer would search three ready instruc- tions could issue instructions order. (See U.S. Patent 3,718,912.) Loads sent instruction b uffers maintain instruction ordering. Recognizing could lose half design's performance branching, designers adopted several aggr essive techniques reduce number branches speed proc essing branches transfers control remained:  Ed Sussenguth Herb Schorr divided actions conditional branch three separate categories: branch target address calculation, taken/ untaken determination, PC update. Th e ACS-1 combined first twoSURVEY SUPERSCALAR PRO CESSORS 3 actions prepare-to-branch instruction used exit instruction perform last action. allowed variable number branch delay slots filled (called anticipating branch); but, importantly, provided mult iway branch specification. is, multiple p repare-to- branch instructions could executed thereby set internal table multiple branch con ditions ass ociated target addresses, one (the first one evaluated true) would used exit instruction. Thus one redirecti instruction fetch stream would quired. (See U.S. Patent 3,577,189.)  set 24 condition code regi sters allowed precalculation branch con- ditions also allowed single prepare-to-branch instruction specify logical expr ession involving two condition codes. similar concept eight independent condition codes RS/6000.  handle case forward conditiona l branch small di splace- ment, conditional bit added eac h instruction forma (i.e., form predication). special form prepare-to-branch instruction used conditional skip. point condition resolution, condition skip instruction true, instructions marked conditional removed instruction queues. condi tion resolved false, marked instructions unmarked allowed execute. (See U.S . Patent 3,577,190.)  Dynamic branch prediction 1-bit histories provided instruction prefetch decoder , speculative execution ruled previous performance problems Stretch. 12-entry target instruction cache eight instructions per entry also proposed Ed Sussenguth provide initial target instructions thus eliminate four-cycle penalty taken br anches. (See U. S. Patent 3,559,183.)  50 instruction could stage execu tion given time, interrupts exceptions could costly. external interrupts converted hardware specially marked branches appropriate interrupt handler routines inserted th e instruction stream allow previously issued instructions complete. (These called soft interrupts.) Arithmetic excep tions h andled two modes: one multiple issue imprecise interrupts one serialized issue. approach used S/360 Model 91 RS/6000. Main memory 16-way int erleaved, store buffer provided load bypassing, done Stretch. Cache memory int roduced within IBM 1965, leading announcement S/360 odel 85 1968. ACS-1 adopted cache memory approach proposed 64K-word unified instruc- tion data cache. ACS-1 cache two-way set-associative line size 32 words last-recently-used (LRU) replacement; block eight 24-bit instructions could fetc hed cycle. cache hit would requirePROCESSOR DESIGN five cycles, cache miss would require 40 cycles. I/O performed cache. ACS-1 processor design called 240,000 circuits: 50% floating-point, 24% indexing, remaining 26% instruction sequencing. 40. circuits included integrated- circuit die. approximately 30 mW per circuit, total power dissipation processor greater 7 kW. optimizing compiler instruction scheduling, register allocation, global code motion developed parallel machine design Fran Allen John Cocke [Allen, 1981]. Simulation demonstrated compiler could produce better code c areful hand optimization several instances. article, Fran Allen credits ACS-1 work providing foundations program analysis machine independen t/dependent optimization. Special emphasis given si x benchmark kern els instruction set design compile r groups. One double-precision floa ting-point inner product. Cocke estimated machine could reach five six instruc- tions per c ycle linear al gebra codes type [1998]. Schorr [1971] Sussenguth [1990] contai n performance comparisons IBM 7090, CDC 6600, S/360 Model 91', ACS-1 simple loop [Lagrangian h ydrody- namics calculation (LHC)] complex loop [Newtonian diffusion (ND)], comparisons given Table 8.1. analysis several chines also performed normalized relative performance respect number circuits th e circuit speed machine. result called relative architectural factor, although average memory access time (affected presence absence cache memory) affected results. Based analysis, using 7090 normalization, Stretch factor 1.2; CDC 6600 Model 91 factors 1.1; Model 195 (with cache) factor 1.7. CS-1 factor 5.2. ACS-1 competition projects within IBM, late 1960s, design incompatible S/360 architecture losing support within company. Gene Amdahl, become IBM Fellow Table 8.1 ACS -1 performance comparison 7090 Relative performance LHC Relative performance ND Sustained IPC ND Performance limiter ND1 0.26 Sequential, nature machineCDC 6600 50 21 0.27 Inst, fetchS/360 M91 ACS-1 72 0.4 Branches2S00 1608 1.8 ArithmeticSURVEY SUPERSCALAR PROCESSORS 37 1965 come California consultant Paley, began working John Earle proposal redesign ACS provide S/360 compatibil- ity. early 1968, persuaded increased sales forecasts, IBM management accepted th e Amdahl-Earle plan. However, overall project thrown state disarray decision, app roximately one-half design team left. th e constraint architectural compatibility, ACS-360 discard innov ative branching predication schemes, also pro- vide strongly ordered memory model well precise interrupts. Compatibility also meant extra gate level logic required execution stage, consequent loss clock frequency. One ACS-360 struction set innovation later made S/370 start I/O fast release (SIOF), pro- cessor would unduly slowed initiation I/O channels. Unfortunately, design longer targeted number-crunching, ACS-360 compete IBM S/360 projects basis bench- marks included commercial data processing. result IPC ACS-360 less one. 1968, second instruction counter second set registers added simulator make ACS-360 first simulta- neous multithreaded design. Instructions tagged wit h additional red/blue bit designate instruction stream register set; , project members expected, utilization function units increased. However, late. 1969, emitter coupled logic (ECL) circuit design problems, coupled performance achievements th e cache-based S/360 Model 85, slow national economy, East Coast/West Coast ten- sions within company, led cellation ACS-360 [Pugh et al., 1991]. Amdahl left shortly thereafter start company. work done IBM superscalar S/370s th rough 1990s. Howe ver, IBM never produced superscalar mainframe, notable exception processor announced 25 years later, ES/9000 Model 520 [Liptay. 1992]. 8.1.3 Instr uction-Level Parallelism Studies early 1970s two important studies multiple instruction decoding issue published: one Gary Tjaden Mike Flynn [1970] one Ed Riseman Caxton Foster [1972]. Flynn remembers skeptical idea multiple decoding, later, student Tjaden. examined inherent problems interlocking cont rol context multiple- issue 7094. Flynn also published appears first open-literature reference multiple decoding part classic SISD/SIMD/MIMD paper [Flynn, 1966]. Tjaden Flynn concentrated decoding logic multiple- issue IBM 7094, Riseman Foster examined effect branches CDC 3600 programs. groups reported small amounts available parallelism benchmarks studied (1.86 1.72 ins tructions per cycle, respectively); however, Riseman Foster found increasing levels parallelism number branches eliminated knowing p aths executed.378 MODERN P ROCESSOR DESIGN results papers aken quite negative dampened general enthusiasm fine-grain, single-program parallelism (s ee Section 1 .4.2). would early 1980s Josh Fisher Bob Rau's VLIW efforts [F isher, 1983; Rau et al., 1982] Tilak Agerwala John C ocke's superscalar efforts (see following) would convince designers feasibility multiple instruction issue thus inspire numerous design e fforts. 8.1.4 By-Products DAE: F irst Multiple-Decoding Im plementations early 1980s, work Jim Smith appeared decoupled access-execute (DAE) architectures [Smith, 1982; 1984; Smith Kaminski, 1982; Smith et al., 1986]. Smith veteran Control Data Corporation (CDC) design efforts teaching th e University Wisconsin. 1982 Interna tional Symposium Computer Architecture (ISCA) paper gives credit IBM Stretch first machine decouple access execution, thereby allowing memory loads start early possible. Smith's design efforts included archi- tecturally visible queues loads stores operated. Computational instructions referenced either registers loaded-data queues. ideas led design development dual-issue Astronautics ZS-1 id-1980s [Smith et al., 1987]. shown Figure 8.3, ZS-1 fetched 64-bit words memory instruction splitter. Instructions could either 32 64 bits length, splitter could fetch two instructions per cycle. Br anches 64 bits fully executed sp litter removed instruction stream; Memory systemJitore ^ddM ^j-* Load^ddrji^-" load store q V* Instruction splitterA inst q X inst q Xloadc> Figure 8.3 Astronautics ZS-1 B lock Diagram.Access processor * | Copy | iJJL Execute processorSURVEY SUPERSCALAR PR OCESSORS 37 unresolved co nditional branches stalled splitter. Access (A ) instructions placed four-entry instruction queue, execute (X) instructions placed 24-entry X instruction queue. In-order issue occurs instruction queues; issu e requires dependences conflicts, operands fetched tim e registers and/or lo ad queues, specified instruction. access processor included three ex ecution units: integer ALU, shift, integer multiply/divide; ex ecute processor included four execution units: logical, floating-point adder, floating-point multiplier, reciprocal approximation unit. manner, two instructions could issued per cycle. 1982 ISCA paper. Smith also cites CSPI MAP 200 array processor example de coupling access execution [Cohler Storer, 1981]. MAP 200 separat e access execute processors coupled FIFO buffers, processor program memory. programmer ensure correct coordination processors. 1986 Glen Culler announced dual -issue DAE machine, Culle r-7, multiprocessor M68010-based kernel processor four user pro- cessors [Lichtenstein, 1986]. user processor combination machine, used control pr ogram sequencing data memory addressing access, microcoded X machine, used floatin g-point computations could run parallel machine. X machines coupled four-entry input FIFO buffer single-entry output buffer. program memory contained sequences X instructions, sometimes paired trailed number instructions. X instructions lookups control store microcode routines; routines sequences horizontal micro-instructions specified operations floating-point adder multiplier, two 4K-entry cratch pad memories, various registers busses. Single-precision floating-point operations single-cycle, double-precision operations took two cycles. User-microcoded routines could also placed control store. X instruction pairs fetched, decoded, executed together available. common sequence single X instruction , would start microcoded routine, followed series instructions provide neces- sary memory accesses. first pair would fetched executed together, remaining instructions would f etched executed overlapping manner multicycle X instruction. input output buffers b etween X machines interlocked, programmer/compiler respon- sible deadlock avoidance (e.g., omission required instruction next X struction). ZS-1 Culler-7, developed without knowledge other, represent first comm ercially sold processors multiple instructions single instruction stream fetched, ecoded, issued parallel . dual issue access execute instructions appear several times later designs (albeit without FIFO buffers) integer unit responsibility integer instructions memory loads stores issu e parallel floating-point computation instructions floating-point unit.380 MODERN PROCESSOR DESIGN 8.1.5 IBM Cheetah, Panther, America Tilak Agerwala IBM Research started dual- issue project, code-named Cheetah, early 1980s urging support John Cocke. design incorporated ACS ideas, backup registers, well ideas IBM 801 RISC ex peri- mental achine, another John Cocke project (circa 1974 1978). three logical unit types seen RS/6 000, i.e., branch, fixed-point (intege r), floating-point, first proposed Cheetah. member Cheetah group, Pradip Bose, published compiler research paper 1986 Fall Joint Computer Conference describing dual- issue mach ines Astronautics ZS-1 IBM design. invited talks several universities 1983 1984, Agerwala first pub- licly used term coined ACS Cheetah-like machines: superscalar. name helped describe th e potential p erformance multiple-decoding machines, especially compared vector processors. Thes e talks, avail- able videotape, related IBM technical report influential rekind ling interest multiple-decoding designs. tim e Jouppi Wall presented paper available inst ruction-level parallelism ASPLOS-III [1989] Smith, Johnson, Horowitz presented p aper limits multiple instruction issue [1989], also ASPLOS-ITI, superscalar VLIW processors hot topics. deve lopment Cheetah/Panther design occurred 1985 1986 led four-way issue design called America [Special issue, IBM Journal Research Development, 1990]. design team led Greg Grohoski included Marc Auslander, Al Chang, Marty Hopkins, Peter Markstein, Vicky Markstein, Mark Mergen, Bob Montoye, Dan Prener. design, generalized register renam- ing facility floating-point loads replaced use backup registers, aggressive branch-folding approach replaced Cheetah's delayed branching scheme. 1986 IBM Austin devel opment lab ado pted America design began refining RS/6000 architecture (also known RIOS POWER). 8.1.6 Decoupled Microarchitectures middle 1980s, Yale Part hi students University California, Berkeley, including Wen-Mei Hwu, Steve Melvin, Mike Shebanow, proposed generalization Tomasulo floating-point unit IBM S/360 Model 91, called restricted data flow. key idea sequential instruction stream could dynamically converted partial data flow graph executed data flow manner. results decoding instruction stream would stored decoded instruction cache (DIC), buffer area decouples instruction decoding engine execution engine. 8.1.6.1 Instruction Fission. work high-performance substrate (HPS). Patt students determined regardless complexity tar- get instruction set, nodes partial da taflow graph stored DIC could RISC-like micro-instructions. applied idea VAX ISA found average four HPS micro-instructions needed per VAX instruction restricted dataflow implementation could reduce CPI VAX instruction stream then-current 6 2 [Patt et al., 1986; Wilson et al., 1987].SURVEY SU PERSCALAR PROCESSORS translation CISC instruction streams dynamically scheduled, RISC-like micro-instruction streams basis number IA32 proces- sors, including NexGen Nx586, AMD K5, Intel Pentium Pro. recent Pentium 4 caches translated micro-instruction stream trace cache, similar decoded instruction cache HPS. f ission-li ke approach also used nominally r educed instruction set computer processors. One example recent POWER4, cracks complex PowerPC instructions multiple internal operations. 8.1.6.2 Instruction Fusion. Another approach decouple microarchitecture fetch instructions llow decoding logic fuse compatible instructions together, rather break apart smaller micro-operations. resulting instruction group traverses th e execution engine unit, almost manner VLIW instruction. One early effort along line undertaken AT&T Bell Labs middle 1980s design decoupled scalar pipeline part C Machine Project. result C RISP microprocessor, described 1987 [Ditzel McLellan, 1987; Ditzel et al., 1 987]. CRISP translated variable-len gth instructions fixed-length formats, including next-address fields, traversal three-stage decode pipeline. resulting dec oded instructions placed 32-entry DIC, three- stage e xecution pipeline fetched executed decoded entries. collapsing computation instructions branches manner, CRISP could run simple inst ruction sequences rate greater one instruction per cycle. Motorola 68060 draws heavily fr om design. Another effort fusing instructions Nat ional Semiconductor Sword- fish. design, led Alpert, began Israel late 1980s featured dual integer pipelines (A B) multiple-unit floating-point coprocessor. decoded ins truction cache organized instruction pair entries. instruc- tion cache miss started fetch pre- decode process, called instruction loading. process examined instructions, precalcul ated branch target addresses, checked opcodes register dep endences dua l issue. dual issue possi- ble, special bit cache entry set. gardless dual issue, first instruction cache entry always sent pipeline A, second instruc- tion supplied pipeline B floating-point pipeline. Program- sequencing instructions could executed pipeline B. Loads could performed either pipeline, thus could issue parallel branches floating-point operations B. Pipeline B ope rated lockstep floating-point pipeline; cases floating-point operation could trap, pipeline B cycled twice memory stage floating-point pipeline would enter writeback stages simu ltaneously. provided in-order completion thus made floating-point excep tions precise. designs using instructio n fusion include Tra nsputer T9000, intro- duced 1991 TI SuperSPARC, introduced 1992. Within T9000, four instructions could fetched per cycle, instruction grouper could build groups eight instructions would flow five-stage382 MODERN PROCESSOR DESIGN pipeline together [May et al., 1991]. SuperSPARC similar grouping stage combined three instructions. recent processors use idea grouping instruction larger units way gain efficiency reservation station slot allocation, reo rder buffer alloca- tion, retirement actions, e.g., Alpha 21264, Athlon, Intel Pentium 4, IBM POWER4. However, cases instructions icro-operations truly fused together independendy executed within execution engine. 8.1.7 Efforts 1980s several efforts multipl e-instruction ssue undertaken 1980s. H. C. Tomg Cornell University examined multiple-instruction issue Cra y-like machines developed out-of-order multiple-issue mechanism called dispatch stack [Acosta et al., 1986]. Intro duced 1986 Stellar GS-1000 graphics supercomputer workstation [Sporer et al., 1988]. GS-1000 used four- way multithreaded, 12-stage pipelined processor two adjacent instructions instruction stream could packetized executed single cycle. Apollo DN10000 Intel i860 dual- issue processors introduced late 1980s, case compile-time marking dual issue makes machines better understood long-instruction-word architectures rather superscalars. particular, Apollo design used bit integer instruction format indicate whether companion floating-point instruction (immediately fol- lowing int eger instruction, pair double-word aligned) dual-issued Th e i860 used bit floating-point instruction format indicate dual operation mode aligned pairs integer floating-point instructions would fetched executed together. pipelining, effect bit i860 governed dual issue next pair instructions. 8.1.8 Wide Acceptance Superscalar 1989 Intel announced first single-chip superscalar, i960CA, triple-issue implementation i960 embedded processor architecture [Hinton, 1989]. Also 1989 saw announcement IBM RS/ 6000 first superscalar workstation; special session three RS/6000 papers presented Inter- national Conference Computer Design tha October. Ap pearing 1990 aggressively microcoded Tandom C yclone, executed special dual-instruction- execution micro programs whenever possible [Horst et al., 1990], Motorola introduced dual-issue 8 8110 1991. Mainframe manufacturers also exper- imenting superscalar designs; Univac announced A19 1991, following year Lipta [1992] described IBM ES/9000 Model 520. flurry announcements occurred early 1990s, including dual- issue Intel Pentium triple-issue PowerPC 601 personal computers. 1995 saw intro- duction five major processor cores that, wit h various tweaks, powered computer systems past several years: HP 8000, Intel P6 (basis Pentium Pro/n/m) MIPS R10000, HaL SPARC64, UltraSPARC-I. Intel recendy intro- duced Pentium 4 redesigned core, Sun introduced redesigned UltraSPARC-Ill. AMD actively involved multiple superscalar designsSURVEY UPERSCALAR PROCESSORS 3 since K5 1995 current Athlon (K7) Opteron (K8). IBM Motorola h ave also introduced multiple designs POWE R PowerPC fami- lies. However, several system manufacturers, Compaq MIPS (SGI), trimmed canceled superscala r processor design plans anticipation adopting processors Intel Itanium processor family, new explicitly paral- lel instruction compu ting (EPIC) architecture. e xample, Alpha line pro- cessors began introduction dual-issue 21064 (EV5) 1992 continued cancellation eight-issue 21464 (EV9) design 2001. Figure 8.4 presents time line designs, papers, com mercially available processors important development superscalar 1961 1964IBM Stretch ("lookahead") 1995 19961997 19981999 2000 2001 200220031967 IBM S/360 M91 (Tomasulo) IBM ACS-1 (first superscalar) 1970 \ 1971 Schorr (ACS paper) IBM 801 (RISC)CDC 6600 (single-issue, out-of-order) 1972 1975 1982 Cheetah project Smith (DAE paper) 1983 Agerwala presentations 1985 | | 1986 America Astronautics ZS-1 1989 R S/6000 1990 K 1991 \ 88110 1992 RSC jf 1993 | -PPC 601 1994 POWER2 / j } / 603/604 P2SC 620 } \ POWER3 { RS64 7400I POWER4 7450 1 J Tjaden Flynn (ILP seen limited) Riseman Foster (effect branches) (VLIWpapers start appearing) Culler 7 i960CA 21064 21164 \ 21264 21364Metaflow papers PA 7100 SuperSPARC | HyperSPARC PA 7200 PA 8000 R 10000 UltraSPARC-I PA 8X00 R 1X000 970UltraSPARC-II UltraSPARC-IllPentium P6Patt (HPS pope NexGen FS6 pap Nx586 K5 | K6 Pentium 4 Athlon (K7) Opteron (K8) Figure 8.4 Time Line Superscalar Development.384 MODERN PROCESSOR DESIGN techniques. (There many superscalar processors available today fit figure, favorite one may listed.) 8.2 Classification Recent Designs section pr esents cla ssification superscalar designs. distinguish among various techniques levels sophistication used provide multiple issue pipelined ISAs, compare superscalar processors devel- oped fastest clock cycle times (speed demons) developed high issue rates (brainiacs). course, designers pick choose among dif- ferent design tec hniques given pr ocessor may exhibit characteristics multiple categories. 8.2.1 RISC CISC Retrofits Many manufacturers chose compete level performance introduced IBM RS/6000 1989 retrofitting superscalar techniques onto heir 1980s-era RISC architectur es, typically optimized single inte- ger pipeline, onto legacy complex instruction set computer (CISC) architec- tures. Six subcategories, levels sophistication, retrofit eviden (these adapted Shen Wolfe [1993]). levels design points rather st rictly chronological developments. example, 1996, QED chose use first design point 200-MHz MIPS R5000 obtained impressive SPEC95 numbers: 70% SPECint95 performance 85% SPECfp95 performance contemporary 200-MHz Pentium Pro (a level-6 design style). 1. Floating-point coprocessor style -  processors cannot issue multiple integer instructions, even integer instruction branch cycle; instead, issue logic allows dual issue integer instruction floating-point instruction. easiest extension pipelined RISC. P erformance gained floating-point codes allowing integer unit execute necessary loads stores floating-point values, well index register updates branching.  Examples: Hewl ett-Packard PA-R ISC 7100 MIPS R5000. 2. Integer branch  type processor allows combined issue integer instructions branches. Thus performance integer codes improved.  Examples: Intel 1960CA HyperSPARC. 3. Multiple integer issue  processors clude multiple integer units allow dual issue multiple integer and/or memory instructions.  Examples: Hewlett-Packard PA-RISC 7100LC, Intel i960MM, Intel Pentium.— — EA MPE _ XAMr LE — XAMPE T_TSURVEY SUPERSCALAR PROCESSORS 385 4. Dependent integer issue  type processor uses cas caded three-inpu ALUs allow multiple issue dependent integer instructions. related technique double-pump ALU clock cycle.  Examples: SuperSPARC Motorola 68060. 5. Multiple function units precise ex ceptions  type processor emphasizes precise exception model sophis- ticated recovery mechanisms cludes large number function units few, any, ssue restrictions. Restricted forms out-of-order execu- tion using distributed reservation stations possible (i.e., interunit slip).  Example: Motorola 88110. 6. Extensive out-of-order issue  type.of processor provides complete out-of-order issue instructions. addition normal pipeline stages, identifi- able dispatch stage, instructions placed centralized reservation station set distributed reservation stations, identifiable retirement stage, point instructions allowed change architectural register state stores allowed change state data cache.  Examples: Pentium Pro HaL SPARC64. Table 8.2 illustrates variety buffering choices found level-5 -6 designs.E X Pi L E uLTy EX AMP L E T__r Table 8.2 Out-of-order organization Reservation Processor Alpha 21264 HP PA 8000 AMD K 5 AMD K7 Pentium Pro Pentium 4 MIPS Rl 0000 PPC604 PPC 750 PPC620 POWER3 POWER4SPARC64Station Structure (Number Entries) Queues (15,20) Queues (28,28) Decentralized (1,2,2,2,2,2) Schedulers (6x3,12x3) Centralized (20) Queues schedulers Queues (16,16,16) Decentralized (2,2,2,2,2,2) Decentralized (1,1,1,1,2.2) Decentralized (2,2,2,2,3.4) Queues (3,4,6,6,8) Queues (10,10.10,12.18.18) Decentralized (8,8,8,12)Entries Operand Reorder Result Copies Buffer Copies 20 x 4 insts. Combined w/RS Yes 16 Yes Yes, 24 x 3 macroOps Yes, Yes 40 Yes 128 32 (active list) Yes 16 Yes 6 Yes 16 32 20x5IOPseach Yes 64 (A-ring) No386 MODERN PROCESSOR DESIGN 8.2.2 Speed Demons: Emphasis Clock Cycle Time High clock rate primary goal speed demon design. designs characterized deep pipelines, designers typically trade lower issue rates longer load-use branch misprediction penalties clock rate. Sec- tion 2.3 discusses tradeoffs detail. initial DEC Alpha implementation, 21064, illustrates speed demon approach. 21064 combined superpipelining tw o-way superscalar issue used seven stages integer pipeline, whereas contemporary designs 1992 used five six stages. However, tradeoff 21064 would cla ssified level 1 retrofit categories given earlier. one integer instruction could issue per c ycle could paired integer branch. alternative view speed demon processor consider without superpipelining exposed, is, look accomplished every two clock cycles. illustrated 21064 since clock rate typi- cally two times clock rate contemporary chips. view, 21064 four-way ssue design depen dent instructions allowed mild ordering constraint (i.e., dependent instructions cannot doubleword); thus level 4 retrofit categories. high clock rate often dictates full custom logic design. Bailey gives brief overview clocking, latching, choices static dynamic logic used first three Alpha designs [1998]; claims full custom design neither difficult time-consuming generally thought. Grundmann et al. [1997] also discusses full-custom philosophy used Alpha designs. 8.2.3 Brainiacs: Emphasis IPC separate design philosophy, brainiac approach, based getting work done per clock cycle. involve instruction set design decisions well implementation decisions. Designers school thought trade large reservation stations, complex dynamic scheduling logic, lower clock cycle times higher IPC. characteristics approach include emphasis low load-use penalties special support dep endent instruction execution. brainiac approach architecture im plementation illustrated IBM POWER (performance ptimized enhanced RISC). Enhanced instruc- tions, fused multiply-add, lo ad-multiple/store-multiple, string operations, automatic ndex register updates load/stores, included order reduce number instructions needed fetched executed. instruction cache specially de signed avoid alignment constraints full- width fetches, instructio n distribution crossbar front ends execu- tion pipelines designed accept many instructions possible branches could fetched handled quickly possible. IBM also empha- sized time market and, many components, used standard-cell design approach left circuit design relatively unoptimized. especially true forSURVEY SUPERSCALAR PROCESSORS POWER2. Thus, example, 1996, fastest clock rates POWER andPOWER2 implementations 62.5 71.5 MHz, respectively, Alpha 21064A 21264A ran 300 500 MHz, respectively. Smith Weiss [ 1994] offer interesting comparison DEC IBM design philoso- phies. (See also Section 6.7 Figure 6.5.) brainiac approach implementation seen levels 4 6 retrofit categories. 8.3 Processor Des criptions section pres ents brief descriptions several superscalar processors. descriptions ordered alphabetically according manufacturer and/or architec- ture family (e.g., AMD Cyrix described Intel IA32 processors). descriptions intended complete rather give brief overviews highlight interesting unusual design choices. information design obtained references cited. Microprocessor Reports also excellent source descriptive articles microarchitectural features processors; descriptions often derived manufacturer presentations annual icroprocessor Forum. annual IEEE International Solid-State Circuits Conferenc e typically holds one sessions short papers circuit design techniq ues used newest processors. 8.3.1 Compaq/DEC Alpha DEC Alpha designed 64-bit replacement 32-bit VAX archi- tecture. Alpha arc hitects Richard Sites Rich Witek paid special attention multiprocessor support, op erating system independence, multiple issue [Si tes, 1993]. explic itly rejected saw scalar RISC implementation arti- facts found contemporary instruction sets, delayed branches single- copy resources like multiplier-quotient string registers. also spurned mode bits, condition codes, strict memory ordering. contrast recent superscalar designs, th e Alpha architects chose allow imprecise arithmetic exceptions and, furtherm ore, provide mode bit change precise-exception mode. nstead, defined trap barrier instruction (TRAPB, almost ide ntical EXCB) serialize implementation pending exc eptions forced occur. Pr ecise floating-point exceptions provided naive way inserting TRAPB floating-point operation. efficient approach ensure compiler's register allocation allow instructions over- write source registers within basic block smaller region (e.g., code block corresponding single high-level language statement): constraint allows precise exceptions provided one RAPB per basic block (or smaller region) since exception handler th en completely determine correct values destination registers. Alpha arc hitects also rejected byte 16-bit word load/store pera- tions, since require shift mask network read-modify-write188 MODERN PROCESSOR DESIGN sequencing mechanism memory processor. Instead, short ins- truction sequences developed perform byte word operations software. However, turned design mis take, particularly painful emulating IA32 programs Alpha; and, 1995, byte short loads stores introduced Alpha architecture supported 21164A. 8.3.1.1 Alpha 21064 (EV4) / 1992. 21064 first implementation Alpha architecture, design team led Alpha architect Rich Witek. ins truction fetch/issue unit could fetch two instructions per cycle aligned double word boundary. two instructions could issued together according complex rules, direct consequences alloca- tion register file ports instruction issue paths within design. decoder unaggressive; is, first instruction pa ir could issued, instructions fetched examined second instruction pah- also issued removed decoder. However, pipe stage dedicated swapping instruction pair appropriate issue slots eliminate ordering constraints issue rules. simple approach instruction issue one many tradeoffs made design support highest clock rate possible. pipeline illustrated Figure 8.5. 8K-byte struction cache contained 1-bit dynamic branch predictor instruction (2 bits 21064A); however, appropriately setting con- trol register, static prediction based sign displacement could instead selected. four-entry subroutine address prediction stack also included 21064, hint bits e xplicitly set within jump instructions push, pop, ignore stack. nteger Fetch Swap Decode Registers Execute Execute Writeback | Floating-point pipeline loating-point Fetch Swap Decode Issue Registers Add Multiply MultiplyAdd + roundWriteback igure 8.S Ipha 21064 Pipeline Stages5URVEY SUPERSCALAR PROCESSORS 38 21064 three function units: teger, load/store, floating-point. integer unit pipelined two stages longer-executing instructions shifts; howe ver, adds subtracts finished first stage. load/store unit interfaced 8K-byte data cache four-entry write buffer. entry cache-line sized even though cache writethrough; thi sizing provided write merging. Load bypass provided, three outstanding data cache misses supported. informat ion, see special issue Digital Technical Journal [1992], Montanaro [1992], Sites [1993], andMcLellan [1993]. 8.3.1.2 Alpha 21164 (EV5) /1995. Alpha 21164 aggress ive second implementation Alpha architecture. John Edmondson lead architect design, Jim Keller lead architect advanced development. Pete Bannon contributor also le design follow-on chip, 21164PC. 21164 integr ated four function units, three separate caches, L3 cache controller chip. function units integer unit 0, also per- formed integer shift load store; integer unit 1, also pe rformed integer multiply, integer bran ch, load (but store); floating-point unit 0, per- formed floating-point add, subtract, compare, branch controlled floating-point divider; floating-point unit 1, performed floating-point multiply. designers cranked clock speed 21164 also reduced shift, integer ultiply, floating-point multiply divide cycle count laten- cies, compared 21064. Howe ver, simple approach fast unag- gressive decode r retained, multiple issue occur order quadword-aligned instruction quartet; decoder advanced everything current quartet issued. correct instruction mix fo ur-issue cycle two independent inte- ger instructions, floating-point multiply instruction, independent non- multiply floating-point instruction. However, four instructions ordering constraints within quartet, since slotting stage included pipeline route instructions two-quartet instruction-fetch buffer decoder. two integer instruction could loads, load could either integer floating-point values. allow compiler greater flexibility branch target alignment g enerating correct instruction mix quartet, three flavors nops provided : integer unit, flo ating-point unit, vanishing nops. Special provision made dual ssue compare logic instruction dependent con ditional move branch. Branches nto middle quartet supported valid bi instruction decoder. Exceptions 21164 handled manner 21064: issue de coder stalled whenever trap exception barrier instruction encountered. second Alpha design, branch prediction bits removed instruction cache instead packaged 2048- entry BHT. return address stack also increased 12 entries. correctly predicted ta ken branch390 MODERN PROCESSOR DESIGN could result one-cycle bubble, thi bubble often squashed stalls previous instructions within issue stage. novel, commo n, approach pipeline stall control adopted 21164. control logic checked stall conditions early pipeline stages, late-developing hazards cache miss w rite buffer overflow caught point execution th e offending instruction succes- sors replayed. approach eliminated severa l critical paths design, handling load miss specially designed addi- tional performance lost due replay. on-chip cache memory consisted instruction cache (8K bytes), dual-ported data cache (8K bytes), unified L2 cache (96K bytes). split caches provided necessary bandwidth pipelines, size data cache limited du al-port design. data cache provided two-cycle latency loads could accept two loads one store per cycle; L2 access time eight cy cles. ix-entry miss address file (MAF) sat L2 caches provide nonblocking access cache. MAF merged nonsequential loads L2 cache line, much way large store buffers merge stores cache line; four destination registers could remembered per missed address. also two-entry bus address file (B AF) sat L2 cache ff-chip memory provide nonblocking access line-length refills L2 cache. See Edmondson [1994], Edmondson et al. [1995a, b], Bannon Keller [1995] details 21164 design. Circuit design discussed Benschneider et al. [1995] Bowhill et al. [1995], Alpha 21164A described Gronowski et al. [1996]. 8.3.1.3 Al pha21264(EV6)/1997 .The 21264 first out-of-order imple- mentation Alpha architecture. Ho wever, in-order parts pipeline retain efficiency dealing aligned instruction quartets, instructions preslotted one two sets executio n pipelines. Thus, could said design approach marries efficiency VLIW-like constraints instruction alignment slotting flexibility out-of-order superscalar. Jim Keller lead architect 21264. hybrid (or tournament) branch predictor used tw o-level adaptive local predictor paired two-level adaptive global predictor. local predic- tor contains 1024 ten-bit local history entries index 10 24-entry pattern his- tory table, global predictor uses 12-bit global history register indexes separate 40 96-entry pat tern history table. 4096-entry choice predictor driven global history register chooses l ocal global pre- dictors. instruction fetch designed speculate 20 branches. Four instructions renamed per cycle, se dispatched either 20-entry integer instruction queue 15-entry floating-point instruction queue. 80 physical integer registers (32 architectural, 8 priv ileged archi- tecture library (PAL) shadow regis ters, 40 renaming registers) 72 physical floating-point registers (32 architectural 40 renaming registers). Th e instructionSURVEY SUPERSCALAR PROCESSORS 3! quartets retaine 20-entry reorder buffer/active list, 80 instruc- tions along renaming status tracked. mispredicted branch requires one cycle recover appropriate instruction qu artet. Instructions retire rate two quartets per cycle, 21264 unusual retire instructions whenever previous instructions past point possible e xception and/or misprediction. allow retirement instructions even execution results calc ulated. integer instruction queue ssue four instructions per cycle, one four integer function units. integer unit execute add, subtract, logic instructions. Additionally,-one unit execute branch, shift, multi- media instructions; one unit execute branch, shift, multiply instructions; remaining two execute loads tores. integer register file implemented two identical copies enough register ports pro- vided. Coherency two copies maintained one-cycle latency write one file corresponding update other. Gieseke et al. [1997] estimate performance penalty split integer clusters 1%, whereas unified integer cluster would required 22% increase area, 47% increase data path width, 75% increase operand bus length. unified register file approach would also limited clock cycle time. floating-point instruction queue issue two instructions per cycle, one two floating-point function units. One floating-point unit exe- cute add, subtract, divide, take square root, floating-point unit dedicated multiply. Floating-point add, subtract, multiply pipe- lined four-cycle latency. 21264 instruction data caches 64K bytes size organized two-way pseudo-set-associative. data cache cycled twice fast processor clock, two loads, store victim extract, executed processor clock cycle. 32-entry load reorder buffer 32-entry store reorder buffer provided. information 21264 microarchitecture, see Leibholz Razdan [1997], Kessler et al. [1998], Ke ssler [1999]. sp ecifics logic design, see Gowan et al. [1998] Matson et al. [1998]. 8.3.1.4 Alpha21364(EV7)/2001. Alpha 21364 uses a21264 (EV68) core adds on-chip L2 cache, two memory controllers, network interface. L2 cache seven- way set-associative contains 1.75 Mbytes. Th e cache hierarchy also contains 16 victim b uffers cast-outs, 16 victim buffers L2 cast-outs, 16 miss buffers. memory controllers support directory-based cache coherency provide RAM bus interfaces. network interface supports out-of-order transactions adaptive routing four links per processor, provide bandwidth 6. 4 Gbytes/s per link. 8.3.1.5 Alpha 21464 (EV8) / Canceled. Alpha 21464 ag gressive eight-wide superscalar design included four-way simultaneous multithread- ing. design oriented toward high single-thread throughput, yet chip392 MODERN PROCESSOR DESIGN area cost adding simultaneous multithreading (SMT) control replicated resources minimal (reported 6%). two branch predictions performed cycle, instruction fetch designed return two blocks, possibly noncontiguous, eight instructions each. fetch, 16 instructions would collapsed group eight instruc- tions, based branch predictions. group renamed dis- patched single 128-entry instruction queue. queue implemented dispatched instructions assigned age vectors, opposed collapsing FIFO design instruction queues 21264. cycle eight instructions would issued set 16 function units: eight integer ALUs, four flo ating-point ALUs, two load pipelines, two store pipelines. register file designed 256 architected regis- ters (64 four threads) additional 25 6 registers available renaming. Eight-way issue required equivalent 24 ports, structure would difficult implement. Instead, two banks 512 registers used, register eight-ported. structure required significantly die area 64K-byte LI data cache. Moreover , integer execution pipeline, planned requiring equivalent 18 stages, devoted three clock cycles registe r file r ead. Several eight-entry register caches wer e included within function units provide forwarding (compare UltraSPARC-Ill working register file). chip design also included system interconnect router building directory-based cache coherent NUMA system 512 processors. Alpha processor development, including 21464, canceled June 2001 Compaq favor switching Intel Itanium p rocessors. Joe l Emer gave overview 21464 design keynote talk PACT 2001, slides available Internet. See also Preston et al. [2002]. Seznec et al. [2002] describe branch predictor. 8.3.2 Hewlett-Packard PA-RISC Version 1.0 Hewlett-Packard's Precision Architecture (PA) one first RISC architec- tures; designed 1981 1983 Bill Worley Michael Mahon, prior introduction MIPS SPARC. load/store architecture many RISC-like qua lities, also slight VLIW flavor instru ction set architecture (ISA). several cases, multiple operations specified one instruction. Thus, superscalar processors 32-bit PA line (PA-RISC version 1.0) relatively unaggressive superscalar instruction issue width, capable executing multiple operations per cycle. Indeed, first dual-issue PA processor , 7100, could issue four operations given cycle: intege r ALU operation, condition test ALU result determine next instruction would nullified (i.e., p redicated execution), floating-point add, independent floating-point multiply. Moreover, four operations could issued previous floati ng-point divide still execution cache miss outstanding.SURVEY SUPERSCALAR PROCESSORS 32-bit PA processors prior 7300LC characterized relatively low numbers transistors per chip instead emphasized use large off-chip caches. simple five-stage pipeline starting point design, bu care- ful attention given tailoring pipelines run fast external cache SRAMs would allow. small, specialized on-chip caches introduced 7100LC 7200, 7300LC featured large on-chip caches. 64-bit, out-of-order 8000 reverted back reliance large, off-chip caches. Later versions 8x00 series added large, on-chip caches tra nsistor budgets allowed hese design choices resulted close attention HP system designers paid commercial workloads (e.g., transaction processing), exhibit large working sets thus poor l ocality small on-chip caches. implementations listed next follow HP tradition team designs. is, one two lead architects identified. Perhaps compa- nies, HP attempted include compile r writers teams equal level hardware designers. 8.3.2.1 PA 7100/1992. 7100 first superscalar implementation Precision Architecture series. dual- issue design one nteger unit three independent floating-point units. One integer unit instruction could issued along one floating-point unit instruction cycle. integer unit handled integer floating-point load/stores, integer multiply performed fl oating-point multiplier. Sp ecial p re-decode bits instruc- tion cache assigned refill instruction issue implified. ordering alignment requirements dual issue. Branches 7100 statically predi cted based sign dis- placement. Precise exceptions provided dual-issue instruction pair bydelaying writeback integer unit flo ating-point units suc- cessfully written back. load could issued cycle, returned data cache hit two cycles. Special pair ing allowed dependent floating-point store issued cycle result-producing floating-point operation. Loading R0 pro- vided software-controlled data prefetching. See Asprey et al. [1993] DeLano et al. [1992] information PA 7100. 7150 125-MHz implementation 7100. 8.3.2.2 PA7100LCand7300LC/1994and1996 .The PA 7100LC low- cost, low-power extension 7100 oriented toward graphics mul- timedia workstation use. available uniprocessor only, provided second integer unit, IK-byte on-chip instruction cache, integrated memory controller, new instructions multimedia support. Figure 8.6 illustrates PA 7100 pipeline. integer units 7100LC asymmetric, one shift bit-field circuitry. Given could one shift instruction per cycle, either two integer instructions, integer instruction load/store, integer instruction floating-point instruction, load/store floatin g-point394 MODERN PROCESSOR DESIGN EX DC WB Branch *j fetch J*"]]"*' bufferInteger unit—ALU load/store Floating-point units—pipelined arithmetic unit, pipelined multiplier, nonpipelined divide/sort Integer fetch dec Floating-point fetch dec fpexecl lpexec2 Figure 8.6 HP PA 7100 Pipeline Stages. instruction could issued cycle. also provision two loads two stores two words 64-bit aligned doubleword memory could also issued cycle. valuable technique speeding subroutine entry exit. Branches, integer instructions nullify next sequential instruction, could dual issued predecessor instruction successor (e.g., delayed branch cannot issued branch delay slot). Instruction pairs crossed cache line boun daries could issued, except pair integer struction load/store. register scoreboard 7100LC also allowed write-after-writ e dependences issue cycle. However, reduce control logic, th e whole pipeline would stall operation longer durati two cycles; included integer multiply, double-precision floating-point operations, floating-point divide. See Knebel et al. [1993], Undy et al. [1994], April 1995 special issue Hewlett-Packard Journal information PA 7100LC. 7300LC derivative 71O0LC dual 64K-byte on-chip caches [Hollenbeck et al., 1996; Blanchard Tobin, 1997; Johnson Undy, 1997]. 8.3.2.3 PA 7200 /1994. PA 7200 added second integer unit 2K-byte on-chip assist cache data. instruction issue logic similar 7100LC, pipeline stall multiple-cycle operations. 7200SURVEY SUPERSCALAR PROCESSORS provided multiple sequential prefetches instruction cache also aggressively prefetched data. data prefetches internally generated according direction stride address-register-update forms load/store instructions. decoding scheme 7200 similar National Semi- conductor Swordfish. Th e instruction cache expanded doubleword six pre-decode bits, indicated data dependences two instructions used steer instructions correct function units. hese pre-decode bits set upon cache refill. interesting design twist 7200 use on-chip, fully associative assist cache 64 entries, 32-byte data cache line. data cache misses prefetches directed assist cache, FIFO replacement external data cache. l oad/store hint set instruction indicate spatial locality (e.g., block copy), replacement marked lines assist cache would bypass external data cache. hus cache pollution unnecessary conflict misses direct-mapped external data cache reduced See Kurpanek et al. [1994] Chan et al. [1996] details PA 7200. 8.3.3 Hewlett-Packard PA -RISC Version 2.0 Michael Mahon Jerry Hauck led Hewlett-Packard efforts extend Preci- sion Architecture 64 bits. PA-RISC 2.0 also includes multimedia extensions, called MAX [Lee Huck, 1996]. major change superscalar implementa- tions definition eight floating-point condition bits rather original one. PA-RISC 2.0 adds speculative cache line prefetch instruction avoids invoking miss actions TLB miss, weakly ordered memory model mode bit processor status word (PSW), hints procedure calls returns maintaining turn-address prediction stack, fused multiply-add instruction. PA-RISC 2.0 defines cache hint bits loads stores (e.g., marking accesses spatial locality, done HP PA 7200). PA-RISC 2.0 also uses unique static branch prediction method: register numbers ascending order compare branch instruction, branch pre- dicted one way; descending ord er, branch predicted opposite manner. esigners chose covert manner passing static pre- diction information since spare opcode bits available. 8.3.3.1 HP PA 80 00/1996. PA 8000 first implementation 64-bit PA-RISC 2.0 architecture. core still used today various 8x00 chips. shown Figure 8.7, PA 8000 two 28-entry combined reserva- tion station/reorder buffers 10 function units: two integer ALUs, two shift/merge unit, two divide/square root unit, two multiply/accumulate units, two load/store units. ALU instructions dispatched ALU buffer, memory instruc- tions dispatched memory buffer well matching 28-entry address buffer. instructions, load-and-modify branch, dispatched buffers.Integer fetch dec queue arb exec wb Floating-point fetch dec queue arb mul add rnd wb Figure 8.7 HP PA 8000 Pipeline Stages. combined reservation tation/reorder buffers operate interesting divide-and-conquer manner [Gaddis Lotz, 1996]. Instructions even- numbered slots buffer issued one integer ALU one load/store unit, instructions odd-numbered lots issued integer ALU load/store unit. Additionally, arbitration issue done subdividing buffer's even half odd half four banks (with sizes 4,4,4, 2). Thus four groups four banks each. Within group, first ready instruction bank.that contains oldest instruction wins ssue arbitra- tion. Thus, one instruction issued per group per cycle, leading maxi- mum issue rate two ALU instructions two memory instructions per cycle. large number comparators used check register dependences instructions dispatched buffers, equally large numbeT com- parators used match register updates waiting instructions. Special propa- gate logic within reservation station/reorder buffers handles carry-borrow dependences. ff-chip instruction c ache, taken branch 8000 two-cycle penalty. However, 32-entry, fully assoc iative BTAC used 8000 along 256-entry BHT. BHT maintained three-bitSURVEY SUPERSCALAR PROCESSORS 3' branch history register entry, prediction made majority vote history bits. hit BTAC leads correctly predicted taken branch penalty. Alternatively, prediction performed statically using register number ordering scheme within instruction format (see intro- ductory PA-RISC 2.0 paragraphs earlier). Static dynamic prediction select- able page basis. One sugge stion made HP profile dynamic library code, set static prediction bits accordingly, select static prediction library pages. preserves program's dynamic history BHT across library calls. See Hunt [1995] Gaddis Lotz [1996] description PA 8000. 8.3.3.2 PA 8200 /1997. PA 8200 follow-on chip includes improvements quadrupling number entries BHT 1024 allowing multiple BHT entries updated single cycle. TLB ent ries also increased 96 120. See special issue Hewlett-Packard Jour- nal [1997] information PA 8000 PA 8200. 8.3.3.3 PA 8500 /1998. PA 8500 shrink PA 8000 core inte- grates 0.5 Mbyte instruction cache 1 Mbyte data cache onto chip. 8500 changes branch prediction method use 2-bit saturating agree counter BHT entry; counter decremented branch follows static prediction incremented branch mispredicts. The-number BHT entries also incr eased 2048. See Lesartre Hunt [1997] brief descrip- tion PA 8500. 83.3.4 Additional PA 8x00 Processors. PA 8600 introduced 2000 provides lockstep operation two chips fault tolerant designs. PA 8700, introduced 2001, features increased cache sizes: 1.5-Mbyte data cache 0.75-Mbyte instruction cache. See Hewlett-Packard technical report [2000] details PA 8700. Hewlett-Packard plan introduce PA 880 0 PA 8900 designs switching product line processors Intel Itanium processor fam- ily. PA 8800 slated dual PA 8700 cores, 0.75-Mbyte data cache 0.75-Mbyte instruction cache, on-chip L2 tags, 32-Mbyte off-chip DRAM L2 cache. 8.3.4 IBM POWER design POWER architecture led Greg Grohoski Rich Oehler based ideas Cocke Agerwala. Following Chee- tah America designs, three separate function units defined, register set. approach reduced design complexity initial implementations since instructions different units need complex dependency checking identify shared registers.398 MODERN PROCESSOR DESIGN architecture oriented toward high-performance double-precision floating point. floating-point register 64 bits wide, floating-point operations done double precision. Indeed, single-precision loads stores require extra time early implementations converting internal double-precision format. major factor performance fused multiply-add instruction, four-operand instruction multiplies two operands, adds product th ird, stores overall result fourth. exactly operation needed inner product function found frequently inner loops nu merical codes. Brilliant logic design ac- complished operation two-pipe-stage design initial implemen- tation. However, side effect addition must done greater double precision, visible results slightly different obtained normal floating-point rounding performed operation. Support innermost loops floating-point codes seen use hranch-and-count instruction, fully executed branch unit, renaming floating- point registers load instructions first imple- mentation. Renaming destination gisters floating-point loads sufficient allow multiple iterat ions innermost loop floating-point codes over- lap execution, since load subs equent iteration delayed reuse architectural register. Later implementations extend register renaming instructions. Provision precise arithmetic exceptions obtained POWER use mode bit. One setting serializes flo ating-point execut ion, setting provides faster alternative imprecise exceptions. Two major changes/extensions made POWER architecture. Apple, IBM, Motorola joined forces early 1990s define PowerPC instruction set, includes subset 32-bit instructions well 64-bit instructions. Also early 1980s, IBM Rochester de fined PowerPC-AS extensions 64-bit PowerPC architecture PowerPC processor could used AS/400 compute r systems. POWER family divided four major groups, ll-known members shown followingi table. (Note many additional family members within 32-bit PowerPC group explicitly named, e.g., 8xx embedded processor erifes.) 32-Bit POWER 32-Bit PowerPC 64-Bit Power*: RIOS RSC POWER2 P2SC601 603 604 740/750 (G3) 7400 (G4)7450 (G4+)620 POWER3 ' POWER4 970 (G5)64-Bit Pow erPC-AS A30(Muskie) A10 (Cobra) A35 (Apache) /RS64 A50(Northstar )/RS64ir Pulsar/RS64 S-Star/RS64IVSURVEY SUPERSCALAR P ROCESSORS 399 8.3.4.1 RIO Pipelines /1989. Figure 8.8 depicts pipelines initial implementation POWER. essentially ones America processor designed Greg Grohoski. instruction cache branch unit could fetch four instructions per cycle, even across cache line boundaries. sequential execution four instructions placed eight-entry sequential instruction buffer. Al though predict-untaken policy implemented conditional ssue/dispatch sequential instructions, branch logic inspected first five entries buffer, branch found spec- ulative fetch four instructions branch target address started. special buffer held hese target ins tructions. branch taken, target buffer flushed; however, branch taken, sequential buffer wa flushed, target instructions moved sequential buffer, conditionally dis- patched sequential instructions flushed, branch unit registers restored history registers necessary. Se quential exe cution would begin branch-taken path. Floating-point I-fetch Dispatch Partial decode Rename fp-decode fp-exec-1 fp-exec-2 fp-wb | Load destinatio n register add Bypass Bypass fp load data fp store dataStore data queue Integer ('fixed-point') I-fetch Dispatch Decode Execute Cache Writeback | Figure 8.8 IBM POWER (RIOS) Pipeline Stages.400 MODERN PROCESSOR DESIGN instruction cache branch unit dispatched four instructions cycle. Two instructions could issued branch unit itself, two floating- point integer (called fixed-point POWER) instructions could dis- patched buffers. latter two instructions could floating-point, integer, one each. fused multiply-add c ounted one floating-point instruction. Floating-point loads stores went pipelines, floating-point instruction discarded integer unit integer instructions discarded floating-point unit. instruction buffers empty, first instruction appropriate type al lowed issue unit. dual dispatch buffers obviated instruction pairing/ordering rules. Pre-decode tags added instructions instruction cache refill identify required unit thus speed instruction dispatch. instruction cache branch unit executed branches condition code logic perations. three architected gisters: link register, holds return addresses; count register, holds loop counts; condition regis- ter, ha eight separate condition code fields (CRi). CRO default con- dition code integer unit, CRI de fault condition code floating-poin unit. Explicit integer floating-point compare instructions specify eight, condition code updating optional side effect execution occurs default condition code. Mu ltiple condition codes provide reuse also allow compiler su bstitute condition code logic operations place conditional jumps evaluation compound con- ditions. However, Hall O'Brien [1991] indicated XL compilers point appear benefit multiple condition codes. integer unit four stages: decode, execute, cache access, writeback. Rather traditional cache bypass ALU results, th e POWER integer unit passed ALU r esults directly writeback stage, could write two integer results per cycle (this approach came 801 pipeline). Floating-point stores performed effective address generation set side store address queue floating-point data arrived later. four entries queue. Floating-point load data sent floating-point writeback stage well first floating-point execution stage. latter bypass allowed floating-point load dependent floating-point instruction issued cycle; loaded value arrived execution without load penalty. floating-point unit accepted two instructions per cycle pre- decode buffer. Integer instructions discarded pre-decode stage, floating-point loads stores identified. second stage unit renamed floating-point register references rate two instructions per cycle; new physical registers assigned targets floating-point load instructions. (Thirty- eight physical registers provided map 32 architected registers.) point loads, stores, ALU operations separated; instructions latter two types sent respective buffers. store buffer thus allowed loads ALU operations bypass. third pipe stage floating- point unit decoded one instruction per cycle would read necessary operands register file. final three pipe stages multiply, add, writeback.SURVEY SUPERSCALAR PROCESSORS information POWER architecture implementations, see Januar 1990 special issue IBM Journal Research Development, IBM RISC System/6000 Technology book, Hester [1990], Oehler Blasgen [1991], Weiss Smith [1994]. 8.3.4.2 RSC /1992. 1992 RSC wa announced single-chip im plemen- tation POWER architecture. restriction one million transistors meant level parallelism RIOS chip set could supported. RSC therefore designed three function units (branch, teger, floating- point) ability issue/dispatch two instructions per cycle. 8K-byte uni- fied cache wa included chip; two-way set-ass ociative, write-through, line size 64 bytes split four sectors. four instructions (one sector) could fetched cache cycle; instructions placed seven-entry instruction queue. first three entries instruction queue decoded cycle, either one first two instruction entries could issued integer unit dispatched floating-point unit. integer unit buffered; three-stage pipe- line consisting decode, execute, writeback. Cache access could occur either execute writeback stage help tolerate co ntention access single cache. floating-point unit two-entry buffer instructions dispatched; allowed dispatch logic reach subsequent integer branch instructions quickly. Th e floating-point unit rename registers. instruction prefetch direction branch instru ction enc ountered predicted according sign displacement; however, opcode bit could r everse direction prediction. branch unit quite restricted could independently handle branche dependent counter register target address page branch instruction. branches issued integer unit. speculative execution beyond unresolved branch. Charles Moore lead designer RSC; see paper [Moore et al., 1989] description RSC. 8.3.4.3 POWER2 /1994. Greg Grohoski led effort extend four-way POWER increasing instruction cache 32K b ytes adding second integer unit second floating-point unit; result allowed six-way instruction issue called POWER2. Additional g oals pro cess two branches per cycle allow dependent integer instruction issue. POWER ISA extended POWER2 introduction load/store quadword (128 bits), floating-point integer conversions, floating-point square root. page ta ble entry search caching rule also changed reduce expected number cach e misses TLB miss handling. Instruction fetch POWER2 increased eight instructions per cycle, cache line crossing permitted; and, number entries fo r sequential tar- get instruction buffers increased 16 8, respectively. sequential dispatch mode, instruction cache branch unit attempted dispatch six instructions402 MODERN PROCESSOR DESIGN per cycle, branch unit inspected additional two instructions look ahead hranches. target dispatch mode, th e instruction cache branch unit prepared dispatch four integer floating-point instructions placing bus integer float ing-point units. latter mode con- ditionally dispatch reduce branch penalty taken branches two cycles. also two independent branch stations could evaluate branch conditions generate th e necessary target addresses next target fetch. major benefit using two branch units calculate prefetch target address second branch follows resolved-untaken first branch; untaken-path instructions beyond one unresolved branch (either first second) could conditionally dispatched; second unresolved branch stopped dispatch. two integer units. copy integer register file, hardware maintained consistency. unit could execute simple integer operations, including loads stores. Cache control privileged instructions executed first unit, second unit executed multi- plies divides. second unit provided integer multiplies two cycles could also execute two dependent add instructions one cycle. integer units also handle load/stores. data ca che, including direc- tory, fully dual-ported. fact, cache ran three times faster normal clock cycle time; integer unit got turn, sometimes reverse order allow read go first, cache refill got turn. also two floating-point units, could execute fused multiply-add two cycles. buffer front floating-point ALU allowed one long-running instruction one dependent nstruction assigned one ALUs, independent instructions subsequent dependent pair could issued order second ALU. ALU multiple bypasses other; however, n ormalized floating-point numbers could routed along bypasses. Numbers denormalized special-valued (e.g., number (NaN), infinity) handled via register file. Arithmetic exceptio ns imprecise POWER2, precise- interrupt-generating instructions load/stores integer traps. floating-point unit integer unit synchronized whenever interrupt-generating instruction wa issued. See Barreh et al. [1994], Shippy [1994], Weiss Smith [1994], White [1994] information POWER2. single-chip POWER2 implemen- tation called P2SC. 8.3.5 Intel i960 i960 architecture announced Intel 1988 RISC design embedded systems market. basic architecture integer-only 32 registers. registers divided 16 global reg isters 16 local registers, latter windowed nonoverlapping manner procedure call/return. numerics extension th e architecture provides single-precision, double- precision, extended-precision floating point; case, four 80-bit registers added programming model. i960 chief architect Glen Myers.SURVEY SUPERSCALAR PROCESSORS 4( comparison operations i960 carefully designed p ipelined implementations:  conditional compare instruction available use standard compare. conditional compare execute first co mpare true. allows range checking implemente one conditional branch.  compare nstruction increment/decrement pro vided fast loop closing.  combined compare branch instruction provided cases independent inst ruction cannot scheduled compiler delay slot normal compare instruction sub sequent conditional branch instruction. opcode name space also carefully allocated first 3 bits instruction easily distinguish control instructions (C-type), register-to-register integer instructions (R-type), load/store instructions (M-type). Thus dispatching different function units occur quickly, p re-decode bit unnecessary. 8.3.5.1 i960 CA /1 989. i960 CA ntroduced 1989 first superscalar microprocessor. unique among early superscalar designs still available product today. Chief designers Glenn Hinton Frank Smith. CA model includes chip: interrupt controller, DMA controller, bus controller, 1.5K bytes memory, partially allo- cated register window stack remaining part used low-latency memory. three units: instruction-fetch/branch (instruction sequencer), integer (register side), address generation (memory side). integer unit includes single-cycle integer ALU pipel ined multiplier/ divider. address generation unit controls access memory also handles accesses on-chip memory. i960 CA pipeline shown Figure 8.9. Integer fetch dec exec wb Instructio n sequencer dec reg branch* «) alu [—»-j wb | J-*-  -*\ wb | Register-side Integer alu Multiply/divide Memory-side data    -*jjvbj Figure 8.9 Intel i960 CA Pipeline Stages.404 MODERN PROCESSOR DESIGN decoder fetches four instructions quickly e two cycles attempts issue according following rules: 1. first instruction four slots issued possible. 2. first ins truction register-side instruction, is, neither memory-side instruction control instruction, second in- struction examined. memory-side instruction, issued possible. 3. either one two instructions issued neither one control instruction, remaining instructions examined. first control instruction found issued. Thus, new instruction quadword fetched, nine possi- bilities issue. 3-tssue 2-lssue, R 2-lssue, 2-lssue, C RMCx MCxx RCxx RMxx RMxC MxCx RxCx MxxC RxxC (Here x represents ins truction issued.) Notice lower rows table control instruction executed early, out-of-order manner. However, instruction sequencer retains current instruction quadword instructions issued. Thus, peak issue rate three instructions given cycle, maximum sustained issue rate two per cycle. instruction ordering constraint issue rules criticized irregular avoided designers. How- ever, M-type load-effective-address (Ida) ins truction general enough many cases pair integer instructions migrated instruction scheduler peephole optimizer equivalent pair one integer instruction one Ida instruction. (See also th e MM model, des cribed Section 8. 3.5.2, provides hardware-based instruction migration.) i960 CA includes lK-byte, two-way set-associative instruction cache. set-associative design allows either one banks loaded (via spe- cial instruction) time-critical software routines locked pr event instruc- tion cache misses. Moreover, sp eculative memory fetch tarted branch target address anticipation instruction cache miss; reduces instruction cache miss penalty. Recovery e xceptions handled either software use except ion barrie r instruction. Please see Hinton [1989] McGeady [1990a, b] details i960 CA. U.S. Statutory Invention Registration HI291 also describes i960 CA.SURVEY SUPERSCALAR PROCESSORS 8.3.5.2 Models 960. i960 MM introduced military applications 1991 included 2K-byte instruction cache 2K-byte data cache chip [McGeady et al., 1991]. decod er automatically rewrote second integer instructions equivalen Ida instructions possible. MM model also included floating-point unit implement numerics exten- sion architecture. CF model announced 1992 4K-byte, two-way set-associative instruction cache lK-byte, direct-mapped data cache. i960 Hx series models provides 16K-byte four-w ay set-associative instruction cache, 8K-byte four-way set-associative data cache, 2K-bytes on-chip RAM. 8.3.6 Intel IA32—Native Approaches Intel IA32 probably widely used architecture developed 50+ years electronic computer history. Although roots trace back 8080 8-bit microproc essor designed Stanley Mazor, Federico Faggin, Masatoshi Shima 1973, Intel introduced 32-bit computing model 386 1990 (the design led John Crawford Patrick Gelsinger). follow-on 486 (designed John Crawford) integrated FPU chip also used extensive pipe lining achieve ingle-cycle execution many instructions. next design, Pentium, first superscalar mplementa- tion IA32 brought market: Overall, IA32 design efforts classified whole-instruction (called native) approaches, like Pentium, decoupled microarchitecture approaches, like P6 core Pentiu 4. Processors using native approach examined section. 8.3.6.1 Intel Pentium/1993. Rather called 586, Pentium name selected trademark purp oses. Actually series Pen- tium implementations, ifferent fe ature sizes, power management techniques, clock speeds. last implementation added MMX multimedia instruc- tion set extensio n [Peleg Weiser, 1996; Lempel et al., 1997]. Th e Pentium chief architect Alpert, assisted Jack Mills, Bob Dreyer, Ed Grochowski, Uri Weiser. Weiser led initial design study Israel framed P5 du al-issue processor, Mills instrumental gaining final management approval dual integer pipelines. Pentium designed around two integer pipelines, U V, oper- ated lockstep manner. (The exception lockstep peration paired instructio n could stall execute stage V without stalling instruction U.) stages pipelines similar stages 486 pipeline. first decode stage determined instruction lengths checked dual issue. second decode stage cal culated effective mem- ory address execute stage could access data cache; stage dif- fered 486 counterpart ability read index register 406 MODERN PROCESSOR DESIGN T_Tbase register cycle ability handle displacement immediate cycle. execute stage performed arithmetic logical operations one cycle operands registers, required multiple cycles complex instructions. example, common type complex instruction, add register memory, required three cycles exe- cute stage: one read memory operand cache, one execute add, one store result back cache. However, two instructions form (read-modify-write) paired, two additional stall cycles. integer instructions, shift, rotate, add carry, could performed U pipeline. Integer multiply done floating-point pipeline, attached th e U pipeline, st alled pipelines 10 cycles. pipeline illustrated Figure 8.10. Single issue U pipelin e occurred (1 ) complex ins tructions, in- cluding floating-point; (2) first struction possible dual-issue pair branch; (3) second ins truction possible pair dependent first (although WAR dependences checked limit dual issue). Complex instructions generated sequ ence control words micro- code sequencer Dl st age control pipelines several cycles. special handling pairing flag- setting instruction dependent condi- tional branch, pairing two push two pop instructions sequence (helpful procedure entry/exit), pairing floating-point stack registe r exchange (FXCH) floating-p oint arithm etic instruction. data cache Pentium interleaved eight ways 4-byte bound- aries, true dual porting TLB tags. allowed U V pipe-lines access 32-bit doublewords data cache parallel l ong Integer Fetch Decode1 Decode2Cache/Writeback executeWriteback | fetch \* +\ dl | Floating-pointd2 Floating-point pipelined2 *n exec !* -H wb IV integer pipeline wb U integer pipeline ex fpwb Fetch Decode 1 Decode2Operand fetchfp exec 1 fp exec2 WritebackError report Figure 8.10 Intel P entium Pipeline Stages.SURVEY SUPERSCALAR PR OCESSORS * bank conflict. cache allocate lines write misses, thus dummy reads sometimes inserted set sequential writes com- piler hand-coded optimization. floating-point data paths 80 bits wide, directly supporting ex- tended precision operations, delayed exception model 486 changed predicting exceptions (called safe instruction recognition). single-issue floating-point instructions Pen tium restrictive constraint would RISC architecture, since floating-point instruc- tions IA32 allowed memory operand. cache design also supported doub le-precision floating-point loads stores using U V pipes parallel access upper 32 bits lower 32 bits 64-bit double-precision value. Branches predicted fetch tage use 256-entry BTB; entry held branch instruction addre ss, target address, two bits history. information original Pentium, see Alpertand Avnon [1993]. Pentium MMX design included 57 multimedia instructions, larger caches, better branch prediction scheme (derived P6). Instruction-length decod- ing also one separate pipeline stage [Eden Kagan, 1997]. 8.3.6.2 Cyrix6x86(Ml)/1994. Mark Bluhm Ty Garibay led design effort Cyrix improve Pentium-like dual-pipeline design. design, illustrated Figure 8.11, called 6x86, included register renaming (32 physical registers), forwarding paths across dual pipelines (called X 6x86), decoupled pipeline e xecution, ability swap instructions th e pipelines decode stages support dynamic load balancing stall avoidance, ability dual issue larger variety instruction pairs, use eight-entry address stack support branch prediction return addresses, Integer Fetch Decode 1 Decode2 addrcalcladdrcalc2 (cache read)ExecuteWriteback (cache write) Floating point pipelined Figure 8.11 Cyrix 6x86 Pipeline Stages408 MODERN PROCESSOR DESIGN speculative execution past combination four branches flo ating- point instructions. also made several design decisions better support older 16-bit IA32 programs well mixed 32-bit/16-bit programs. X pipelines 6x86 seven stages, compared five stages U V pipelines .the Pentium. Dl D2 stages Pen- tium split two stages 6x86: tw instruction decode stages two address calculation stages. Instructions obtained using two 16-byte aligned fetches; even worst case instruction placement, pr ovided least 9 bytes per c ycle decoding. first instruction decode stage id entified boundaries two instructions, second decode stage identified operands. second decode stage 6x86 optimized processing instruction prefix bytes. first ad dress calculation stage renamed registers flags per- formed address calculation. allow address calculation start early possible, overlapped register renaming; thus, ddress adder access values logical register file execute stage ALU access values ren amed physical register file. register scoreboard used track pending updates lo gical registers used address calcula- tion enfor ced address generation interlocks (AGIs). Coher ence logical physical copies register wa updated address calcula- tion, stack pointer, specially handled hardware. support, two pushes two pops could executed simult aneously. segment registers renamed, segment register scoreboard maintained stage stall ed dependent instructions segment register updates complete. second address calculation stage performed address transl ation cache access memory operands (the Pentium execute stage). Memory access e xceptions also handled st age, certain instruc- tions exceptions cannot easily predicted singly issued executed serially point pipelines. example return instruction targe address popped stack leads segmentation exception. instructions results going memory, cache access occurred writeback stage (the Pentium wrote memory results execute stage). 6x86 handled branches X pipeline and, Pentium, special instruction pairing provided compare dependent conditional j ump. Unlike Pentium, 6x86 ability dual issue predicted-untaken branch X pipeline along fall-through nstruction pipeline. 6x86 also used checkpoint-repair approach allow speculative execution past predicted branches float ing-point instructions. Four levels checkpoint storage provided. Memory-accessing instructions typically routed pipeline, X pipeline could continue used cache miss occur. 6x86 pro- vided special support repeat string move ins truction combination in'RVEY SUPERSCALAR PROCESSORS 4 resources pipelines used allow move instruction attain speed one cycle pe r iteration. forwarding paths X pipelines (which present U V pipelines Pentium) cache access occurred outside execute stage, depen- dent instruction pairs could dual-issued 6x86 wl  n form move memory-to-register in^  action paired arithmetic instruction using register, form ar ithmetic operation writing register paired move register-to-memory instruction using register. Floating-point instructions handled X pipeline placed four-entry flo ating-point instruction queue. point floating-point instruc- tion known cause memory-access fault, checkpoint made instruction issue continued. allowed floating-point instruction execute order. 6x86 could also dual issue floating-point instruction along integer instruction. However, contrast Pentium's support 80-bit extended precision floating point, th e data path 6x86 floating-point unit 64 bits wide pipelined. Also FXCH instructions could dual-issued floating-point instructions. Cyrix chose use 256-byte fully associative instruction cache 16K-byte unifi ed cache (four-way et-associative). unified cache 16-way interleaved (on 16-bit boundaries provide better support 16-bit code) provided dual-ported access similar Pentium's data cache. Load bypass load forwarding also supported. See Burkhardt [1994], Gw ennap [1993], Ryan [1994a] overviews 6x86, called Ml. McMahan et al. [1995] provide details 6 x86. follo w-on design, known M2 6x86MX, designed Doug Beard Dan Green. incorporated MMX instruction set extensions well increasing unified cache 64K bytes. Cyrix continued use dual-pipeline native approach several subsequent chip designs, small improvements made. example, Cayenne core allowed FXCH dual issue FP/MMX unit. However, decoupled design started Ty Garibay Mike banow mid-1990s came called Jalapeno core (also known Mojave). Greg Grohoski took chief architect core 1997. 1999, Via bought Cyrix Centaur (designers WinChip series), mid-2000 th e Cyrix design efforts canceled. 8.3.7 Intel IA32—Decoupled Approaches Decoupled efforts building IA32 processors began least 1989, NexGen publicly described efforts F86 (later called Nx586 Nx686, became AMD K6 product line). efforts influenced wo rk Yale Patt students high-performance substrate (HPS). section, two outwardly scalar, internally superscalar efforts discussed first, th en superscalar AMD Intel designs presented.410 MODERN PROCESSOR DESIGN 8.3.7.1 Ne xGen Nx586 (F86) /1994. Nx586 appeared outside scalar processor; however, nternally decoupled microarchitecture operated sup erscalar manner. Nx586 translated one IA3 2 instruction per cycle e RISC86 instructions dispatched RISC-like instructions three function units: intege r multiply/divide, integer, address ge neration. function unit Nx586 14-entry reservation station, RISC86 instructions spent least one cycle renaming. reservation station also operated FIFO manner, out-of-order issue RISC86 instructions could occur across function units. major drawback arrangement first instr uction reservation station must stall due data dependency, complete reservation station stalled. Nx586 required separate FPU chip floating-point, included two 16K-byte on-chip ca ches, dynamic branch prediction using adaptive branch predictor, speculative execution, register renaming using 22 physical registers. NexGen worked basic design several ye ars; three preliminary articles presented 1989 Spring CO MPCON called F86. L ater information found Ryan [1994b]. Mack McFarland first NexGen architect, Dave Stiles Greg Favor worked Nx586, later Korbin Van Dyke oversaw actual implementation. 8.3.7.2 W inChip Series /1997 Present. Glenn Henry working outwardly scalar, internally superscalar approach, similar NexGen effort, past ecade. Although one IA3 2 instruction ecoded per cycle, dual issue translated micro-operations possible. One item int erest WinChip approach load-ALU-store combination represented one micro-operation. See Diefendorff [1998] description 11-pipe-stage Win- Chip 4. Via currently shipping C3, 16 -pipe-stage core known Nehemiah. 83.7.3 AMD K5/1995. lead architect K5 Mike Johnson, whose 1989 Stanford Ph.D. dissertation published first superscalar micropro- cessor design textbook [Johnson, 1991]. K5 followed many design sug- gestions book, based superscalar AMD 29000 design effort. K5, IA 32 instructions fetched memory placed 16K- byte instruction cache additional p re-decode bits assist locating instruction fields boundaries (see Figure 4.15). cycle 16 bytes fetched instruction cache, based branch prediction scheme detailed Johnson's book, merged byte queue. According scheme, could one branch predicted taken per cache l ine, cache lines limited 16 tes avoid conflicts among taken branches. cache line initially marked fall-through, marking changed mispre- diction. effect using one history bit per cache line. part filling line instruction c ache, IA32 instruction tagged number micro-instr uctions (R -ops) would produced.SURVEY SUPERSCALAR PROCESSORS 4 tags acted repe tition numbers corresponding number decoders could assigned decode ins tructions. anner, IA32 instruction could routed one two-stage decoders without wait con- trol logic propa gate instruct ion alignment information across dec oders. tradeoff incre ased instruction cache refill time required pre-decoding. four decoders K5, could produce one R-op per cycle. interesting aspect process that, depending sequential assign- ment nstruction + tag packets decoders, R-ops one instruction might split different decoding cyc les. Complex instructions overrode normal decoding process caused stream four R-ops per cycle fetched control store. R-ops renamed dispatched six execution units: two integer units, two load/store units, floating-point unit, branch unit. execution unit two-entry reservation station, e xception tha floating-point reservation station one entry. reservation station could issue one R-op per cycle. two entries branch reservation station, K5 could speculatively e xecute past two unresolved br anches. R-ops completed wrote results 16-entry reorder buffer; four results could retired pe r cycle. reservation stations reorder buffer entries handled mixed operand sizes (8,16, 32 bits) treating IA32 reg- ister three separate items (low byte, high byte, extended bytes). item separate dependency-checking logic individual renaming tag. . K5 8K-byte dual-ported/four-bank data cache. proces- sors, stores written upon R- op retirement. Unlike processors, refill load miss started u ntil load R-op became oldest R-op reorder buffer. choice made avoid incorrect acce sses memory- mapped I/O device registers. Starting refills earlier would required spe- cial case logic handle device registers. K5 performanc e disappoin tment, allegedly design decisions made without proper workload information Windows 3.x applications. agreement Compaq 1995 supply K5 chips fell through, AMD bought NexGen (see Section 8.3.7.4). See Gwennap [1994], Halfhill [1994a], Christie [1996] information design. 8.3.7.4 K6 (NexGen Nx686) /1996. 1995, AMD acquired NexGen announced follo w-on design Nx586 would marketed AMD K6. design, called Nx686 done Greg Favor, extended Nx586 design integrating floa ting-point unit well multimedia operation unit onto chip. caches enlarged, decode rate doubled. K6 three types decoders operating mutually exclus ive manner. pair short decoders decoded one IA32 instruction each. could produce one two RISC86 operations each. alternate long decoder could handle single, complex IA32 instruction pro duce four RISC86 op erations. Finally, vector decoder provided initial RISC86 operation group began streaming groups RISC86412 MODERN PROCESSOR DESIGN operations control store. result maximum rate four RISC86 operations per cycle one three decode r types. Pre-decode bits assisted K6 decoders, similar approach K5. K6 dispatched RISC86 operations 24-entry centralized reservation station, six R1SC86 operations issued per cycle. eight IA32 registers used instructions renamed using 48 physical registers. Branch support included 8192-entry BHT implementing adaptive branch prediction according global/adaptive/set (GAs) scheme. 9-bit global branch history shift register 4 bits ins truction pointer used identify one 8192 saturating 2-bit co unters. also 16-entry target instruction cache (16 bytes pe r line) 16-entry return address stack. data cache K6 ran twice per cycle give appearance dual porting one load one store per cycle; chosen rather banking order avoid dealing bank conflicts. See Halfhill [1996a] description K6. Shriver Smith [1998] written book-length, in-depth case study K6-III. 8.3.7.5 Athlon (K7) /1999. Dirk Meyer Fred Weber chief architects K7, later branded Athlon. Athlon uses approaches K5 K6; however, striking differences deeper pipelining, use MacroOps, special handling floating-point/ multimedia instructions distinct integer instructions. Stephan Meier led floating- point part design. front-end, in-order pipeline Athlon consists six stages ( dispatch). Branch prediction front end handled 2048-entry BHT, 2048-entry BTAC, 12-entry return stack. scheme simpler two-level adaptive scheme used K6. Decoding performed three Direct- Path decoders produce one MacroOp each, or, comple x instructions, VectorPath decoder sequences three MacroOps per cycle control store. K5 K6, pre-decode bits assist decoding. MacroOp representation IA32 nstruction moderate com- plexity. MacroOp fixed length contain one two Ops. integer pipeline, Ops six types: load, store, combined load-store, address gener- ation, ALU, multiply. Thus, regi ster-to-memory well memory-to-register IA32 instructions represented single MacroOp. floating-point pipeline, Ops thre e types: multiply, add, mi scellaneous. advan- tage using MacroOps reduced number buffer entries needed. dispatch stage, MacroOps placed 72-entry reorder buffer called instruction control unit (ICU). buffer organized 24 lines three slots each, rest pipelines follow three-slot org anization. integer pipelines organized symmetri cally address generation unit integer function unit connected slot. Integer multiply asym- metric integer instruction; must placed first slot since integer multiply unit attached first integer function unit. Floating-point multimedia (MMX/3DNow! later SSE) instructions restrictive slotting constraints.SURVEY SUPERSCALAR PROCESSORS 4' ICU, MacroOps placed either integer scheduler (18 entries, organized six lines three slots ea ch) th e floating-rwint/multimedia scheduler (36 entries, organized 12 lines three slots each). schedulers schedule Ops individually ou order, MacroOp remains scheduler buffer Ops completed. integer side, load store Ops sent 44-entry load/store queue processing; combined load-store Op remains load/store queue load complete value store forwarded across result bus; point ready act store Op. integer side also uses 24-entry integer future file register file (IFFRF). Integer operands tags read unit dispatch, integer results w ritten unit ICU upon completion. ICU performs update architected integer registers MacroOp retires. IA32 floating-point stack model width XMM regis- ters, MacroOps sent floating-point/multimedia side handled special manner quire additional pipeline stages. Rather reading operands tags dispatch, floating-point/multimedia register ferences later renamed using 88 physical registers. occurs three steps: first, stage 7, stack regis- ter references renamed linear map; second, stage 8, mapped ref- erences renamed onto physical registers; then, stage 9, renamed MacroOps stored floating-point/multimed ia scheduler. operands read dispatch side, extra stage reading physi- cal registers needed. Thus, floa ting-point/m ultimedia execution start stage 12 Athlon pipeline. Athlon on-chip 64K-byte caches initially contained control- ler off-chip L2 8 Mbytes MOESI cache coherence. Later shrinks allowed on-chip L2 caches. data cache multibanked sup- ports two loads stores per cycle. AMD licensed Alpha 21264 bus, Athlon co ntains on-chip bus controller. See Diefendorff [1998] description Athlon. 8.3.7.6 Intel P6 Core (P entium Pro / Pentium II / Pentium III) /1996. Intel P6 discussed depth Chapter 7. P6 core design team included Bob Colwell chief architect, Glenn Hinton Dave Papworth senior architects, along Michael Fetterman Andy Glew. Figure 8.12 illustrates P6 pipeline. P6 Intel's first use decoupled microarchitecture decomposed IA32 instructions. Intel calls translated micro-instructions pops. eight-stage fetch tran slate pipeline allocates entries pops 40-entry reorder buffer 20-entry reservation station. Limitations IA32 floating-point stack model rem oved allowing FXCH (exchange) instructions inserted directly reorder buffer tagged complete processed renaming hardware. instructions never occupy reservation station slots. ransistor count limitations instruction cache prob- lem branching pre-decoder marked interior byte instruction, extra pre-decode bits rejected. Instead, fetch stages mark the414 MODERN PROCESSOR DESIGN Instruction queue (X86insts.) Next I-cache lit 1 h b , II IP (upto6/iOps) |[[ ol f >]o2 f +\ o3 | Decode (up 6 (tops) Rename DispatchReservation Reor der station buffer (uops) (uops) Issue Execute BTB Shortstop branch predictionv ... *U 10 units 4 issue portsRetire (X86insts.) Figure 8.12 Intel P6 Pipeline Stages. instruction boundaries fo r decoding. three IA32 instructions decoded parallel; obtain maximum decoding effectiveness, instructions must arranged first one generate multiple uops (up four) two instructions behind must generate one Uop only. Instructions operands memory require multiple Uops therefore limit decoding rate one IA32 instruction per cycle. Extremely complex IA32 instructions (e.g., PUSHA) require long sequence uops fetched control store dispatched processor several cycles. Prefix b ytes combination immediate operand displacement addressing mode instruction also qu ite disruptive dec oding. reservation station scanned FIFO-like manner cycle attempt issue four Uops five issue ports. Issue ports collection two read ports one write port reservation station. One issue pott supports wide data paths six execution units various types attached: teger, floating-point add, floating-point mu ltiply, integer divide, floating-point divide, integer shift. second issue port handles uops second integer unit branch unit. third port dedicated loads, fourth fifth ports dedicated stores. scanning reservation station, preference given back-to-back uops increase amount op erand forwarding among exe- cution units. Branch handling uses two-level adaptive branch predictor, assist branching branch found BTB lookup, decoder shortstop middle front-end pipeline predicts branc h based sign dis- placement. conditional move added IA32 ISA help avoid conditional branches, new instructions move floating-point condi- tion codes integer condition codes also added. Pentium Pro introduced 133 MHz, almost immediate avail- ability 200-MHz version caught many industry surprise, since performance SPEC integer benchmarks exceeded even contem- porary 300-MHz DEC Alpha 21164. personal computer marketplace, theSURVEY SUPERSCALAR P ROCESSORS 41 performance results less dramatic. designers traded 16-bit code performance (i.e., virtual 8086) provide best 32-bit performance possible; thus chose serialize machine instructions far calls segment register switches. small-model 16-bit code runs well, large-model code (e.g., DOS Windows programs) runs Pentium speed worse. See Gwennap [1995] Ha lfhill [1995] additional descriptions P6. Papworth [1996] discusses design tradeoffs made microarchitecture. 8,3.7.7 Intel Pentium4/2001. Pentium 4 design, led Glenn Hinton, takes decoupled approach P6 core, Pen tium 4 looks even like Yale Part's HPS proposal including decoded instruction cache. cache, called trace cache, organized hold 2048 lines six Uops trace order, is, branch target uops placed immediately next predict- taken branch uops. fetch bandwidth trace cache three Uops per cycle. Pentium 4 much deeply pipelined P6 core, resulting 30 pipeline stages. number actions back-end stages yet disclosed, branch misprediction pipeline described. 20 stages starting trace cache access mispredicted path restarting trace cache correct path. equivalent length P6 core 10 stages. two branch predictors Pentium 4, one front end pipeline smaller one trace cache . front-end BTB 4096 entries reportedly uses hybrid prediction scheme. branch misses structure , front-end pipe stages predict based sign displacement, similar P6 shortstop. trac e cache eliminates need re-decode recently executed IA32 instructions, P entium 4 uses single decoder front end. Thus, corn- pared P6 core, Pentium 4 might take cycles decoding first visit code segment efficient ubsequent visits. maximize hit rate trace cache, Pentium 4 optimization manual advises overuse FXCH instructions (whereas P6 optimization encour- ages use; see Section 8.3.7.6). Excessive loop unrolling avoided reason. Another di fference two Intel designs Pentium 4 store source result values reservation stations reorder buffer. Instead, uses 128 physical registers renaming th e architected integer registers second set 128 physical registers renaming floating-point stack XMM registers. front-end register alias table used along retire ment register alias table keep track lookahead retirement states. Uops dispatched two queues: one memory operations one operat ions. four issue port s, two handle load/stores two handle operations. latter two ports multiple schedulers examining uop queue arbitrating issue permission two ports. schedulers issue one uop per cycle, fast schedulers issue ALU Uops twice per cycle. double ssue integer ALUs pipelined operate three half-cycles, two half-cycle416 MODERN PROCESSOR DESIGN stages handling 16 bits operation third half-cycle stage setting flags. overall effect integer ALUs one-half cycle effective latencies. staggered structure, dependent pops issued back- to-back half cycles. Pentium 4 reorder buffer 128 entries, 126 pops flight. processor also 48-entry load queue 24-entry tore queue, 72 126 pops load/stores. data cache provides two-cycle latency integer values ix-cycle latency floating-point values. cache also supports one load one store per cycle. Th e schedulers speculatively issue pops dependent loads loaded values immediately for- warded dependent pops. However, load ha cache miss, dependent pops must replayed. See Hinton et al. [2000] details Pentium 4. 8.3.7.8 Intel Pentium Ml2003. Pentium design wa led Simcha Gochman low-power revision P6 core. Intel team Israel started revision adding streaming SIMD extensions (SSE2) Pen- tium 4 branch predictor basic P6 microarchitecture. also extended branch pr ediction two ways. first loop detector captures stores loop counts set hardware counters; leads perfect branch prediction for-loops. second extension adaptive indirect-branch prediction scheme designed data-dependent indirect branches, found byte- code interpreter. Mispredicted indirect branches allocated new table entries locations corresponding current global branch history hift register contents. Thus, global history used choose one predictor among many possible instances predictors fo r data-dependent indirect branch. Pentium team made two changes P6 core. first IA32 instruction decoders redesigned produce single, fused pops load-and-operate store instructions. P6 instruction types decoded complex decoder result two pops each. Pentium handled three decoders, type allocated single reservation statio n entry ROB entry. However, pop scheduling logic recognizes treats fused-pop entry two separate pops, execution pipelines remain virtually same. retirement logic also recog- nizes fused-pop entry requiring two completions r etirement (compare AMD Athlon MacroOps). major benefit approach 10% reduc- tion number pops handled front-end rear-end pipeline stages consequent power savings. H owever, team also reports 5% increase performance integer code 9% increase floating-point code. due increased decoding bandwidth less contention reservation station ROB entries. Another change made Pentium addition register tracking logic hardware stack pointer (E SP). stack pointer updates required push, pop, call, return done using dedicated logic dedi- cated adder front end, rather sending stack pointer adjustment popSURVEY SUPERSCALAR PROCESSORS 41 execution pipelines update. Address offsets stack pointer adjusted needed load store pops reference stack, history buffer records peculative stack pointer updates case branch mispredict exceptio n (compare Cyrix 6x86 stack pointer tracking). See Gochman et al. [2003] details. 8.3.8 X86 -64 AMD proposed 64-bit extension x86 (Intel IA32) architecture. Chief architects extension wor e Kevin McGrath Dave Christie. x86-64, compatibility IA32 paramount. existing eight IA3 2 registers extended 64 bits width, eight general registers added. Also SSE SSE2 register set doubled 8 16 size. See McGrath [2000] presentation x86-64 architecture. 8.3.8.1 Opteron (K8) / 2003. first proce ssor supporting extended architecture AMD Opteron. initial K8 project led Jim Keller canceled. Opteron processo r brought market adaptation Athlon design, wor k led Fred Weber. compared Athlon (see Section 8.3.7.5), Opteron retains three-slotted pipeline organization. three regular decoders, called FastPath, handle multimedia instructions without resort VectorPath decoder. Th e Opteron two front-end pipe stages Athlon (and fewer pre-decode bits instruction cache), integer instructions start execution stage 10 rather 8, floating-point/multimedia instructions start stage 14 rather 12. Branch prediction enhanced enlarging BHT 16K entries. integer scheduler 1FFRF sizes increased 24 40 entries, r espectively, number floating-point/multimedia physical registers increased 120. Opteron chip also contains three HyperTransport links multiprocessor interconnection on-chip controller integrates many normal North- bridge chip functions. See Keltcher et al. [2003] information Opteron. 8.3.9 MIPS MIPS architecture quintessential RISC. originated research work noninterlocked pipelines John Hennessy Stanford University, first design MIPS company included noninterlocked loa delay slot. MIPS-I -II architectures wer e defined 1986 1990, respectively, Craig Hansen. Earl Killian 64-bit MIPS-III architect 1991. Peter Hsu started MIPS-IV exten sions SGI prior SGI/MIPS merger; R8000 R10000/1200 0 implement MIPS-IV. MIPS-V finalized Earl Killian 1995, input fro Bill Huffman Peter Hsu, includes MIPS digital media extensions (MDMX). MIPS known clean, fast pipeline design, R4000 designers (Peter Davies, Earl Killian, Tom Riordan) chose introduce superpipelined (yet418 MODERN PROCESSOR DESIGN single-cycle AL U) processor 1992 companies choosing superscalar approach. simulation, superpipeline design performed better unrecompiled integer codes competing in-house uperscalar design. superscalar required multiple integer units issue two integer instructions per cycle, lacked ability issue depen dent integer instruction pairs cycle. contrast, superpipelined design ran clock twice fast and, use single fast-cycle ALU, could issue depen- dent integer instruction pair two fast cycles [Mirapuri et al., 1 992]. interesting compare approach issue depen dent instructions using cascaded ALUs SuperSPARC, also 1992 design. Also, fast ALU idea helpful Pentium 4 de sign. 8.3.9.1 MIPS R8000(TFP)/1994. MIPS R8000 superscalar superpipelined; might seem anomaly, indeed, R8000 actually final name tremendous floating-point (TFP) design st arted Silicon Graphics Peter Hsu. R8000 64-bit machine aimed floating- point computation seems ways reaction IBM POWER. How- ever, many main ideas R8000's design inspired Cydrome Cydra-5. Peter Hsu alumnus Cydrome, design team: Ross Towle, John Brennan, Jim Dehnert. Hsu also hired Paul Rodman John Ruttenberg, formerly Multiflow. R8000 unique separating floating-point data integer data addresses. latter could loaded on-chip l6K-byte c ache, floating- point data could not. decision made effort prevent poor temporal loc ality floating-point data many programs rendering on-chip cache ineffective. Instead R8000 provided floating-point memory bandwidth using large second-level cache two-way interleaved five-stage access pipeline (two stages included chip crossings). Bank conflict reduced help one-entry address bellow; pro- vided reordering cache accesses increase frequency pai ring odd even bank requests. coherency problem could exist external cache on-chip cache floating-point nteger data mixed structure assigned Field (i.e., union data structure). -chip cache pre- vented maintaining one valid bit per word (the MIPS architecture requires aligned accesses). Cache refill would set valid bits, integer floating- point stores would set reset appropriate bits, r espectively. R8000 issued four instructions per cycle eight execution units: four integer, two floating-point, two load/store. integer pipelines inserted empty stage decode ALU peration relative position cache access load/store pipelines. Thus ther e load/ use delays, ad dress arithmetic stalled one cycle depended loaded value. floating-point q ueue buffered floating-point instructions u ntil ready issue. allowed integer pipelines proceed even SURVEY SUPERSCALAR PROCESSOF floating-point load, five-cycle latency, dispatched along dependent floating-point instruction. Imprecise excepti ons thus rule floating-point arithmetic, f loating-point serialization mode bit help debugging, IBM POWER. combined branch prediction instruction alignment scheme similar one AMD K5 used. single branch prediction bit block four instructions cache. source bit mask prediction entry indicated many valid instructions existed branch block, another bit mask indicated branch target instruction started target block. Compiler support eliminate problem two likely-taken branches placed block helpful. Hsu [1993, 1994] presents R8000 greater detail. 8.3.9.2 MIPS R10000 (T5)/1996. Whereas R800 0 multichip imple- mentation, R10000 (previously code-named T5, designed Chris Rowen Ken Yeager) single-chip implementation peak issue rate five instructions per cycle. su stained rate limited four per cycle. Figure 8.13 illustrates MIPS R10000 pi peline. Instructions R10000 stored 32 K-byte instruction cache pre-decode bits fetched four per cycle anywhere cache line. Decoding run rate four per cycle, eight-entry instruction buffer bet ween instruction cache decoder allows fetch continue even decoding stalled. Decode/ rename| issue \*- -»j exec |» wb j - Queue (16) | issue \*- -»-| exec \*- -»-| wb j Figure 8.13 MIPS R10000 Pipeline Stages.Integer pipelines J wb | Load/store pipeline Floating-point pipelines120 MODERN PROCESSOR DESIGN decoder also renames registers. MIPS ISA defines 33 integer registers (31 plus two special registers multiply divide) 31 fl oating- point registers, R10000 64 physical registers integer 64 physical registers floating-point. current register mapping logical registers physical registers mai ntained two mapping tables, one integer one floating-point. Instructions dispatched decoder one three instruction queues: integer, lo ad/store, floating-point. hese queues ser role res- ervation stations, contain operand values, onl physical register numbers. Operands instead read physical register files instruc- tion issue queue. queue 16 entries support out-of-order issue. five instruc- tions issued per cycle: two teger instructions issued two integer units, one execute branches conta ins integer multiply/divide circuitry; one load/store instruction issued unit; one floating-point add instruction one floating-point multiply instruction issued parallel. implementation th e combined floating-poin multiply-add instruction unique instruction type must first traverse first two execu- tion stages multi ply pipeline routed add pipeline, finishes normalization writeback. Results floating-point opera- tions also forwarded two execution stages. processor keeps track physical register assignments 32-entry active list decoded instructions. list maintained program rder. entry list allocated instruction upon dispatch, done flag entry ini tialized zero. indices active list entries also used tag dispatched instructions placed instruction queues. comple- tion, instruction writes result assigned physical register sets done flag 1. manner active list serve order buffer supports in-order ret irement (called graduation). Entries active list contain logical register number named desti- nation instruction well physica l register previously assigned. arrangement provides type history buffer exception handling. Upon detect- ing exception, instruction dispa tching ceases; current instructions allowed complete; then, active list traversed reverse order, four instructions per cycle, unmapping physical registers restoring previous assignments mapping table. provide precise exceptions, process continues excepting instruction unmapped. point, exception handler called. make branch misprediction recovery faster, checkpoint-repair scheme used make copy register mapping tables preserve alternate path address branch. four checkpoints exist one time, R10000 speculatively execute past four branches. Recovery requires one cycle repair mapping tables point branch restart instruction fetch correct address. Speculative instructions ispredicted path flushedSURVEY SUPERSCALAR PROCESSORS 4 processor use 4-bit branch mask added dec oded instruc- tion. mask indicates instruction speculative four predicted branches depends (m ultiple bits set). branches resolved, correct prediction causes instruction th e processor reset correspond- ing branch mask bit . Conversely, misprediction causes instruction corresponding bit set flushed. Integer multiply divide instructions multiple destination registers (HI, LO) thus disrupt normal instr uction flow. decoder R10000 stalls encountering one ins tructions; also, decoder dis- patch multiply divide fourth instruction decode group. reason special handling required multiple destinations: two entries must allocated active list multiply divide. Conditional branches supported R10000 spec ial condition file, one bit per physical register set 1 whenever result equal zero written physical register file. conditional branch compares zero immediately determine taken taken checking appro- priate bit condition file, rather read value physical register file check zero. 512-entry BHT mai ntained branch prediction, caching branch target addresses. results one-cycle pen- alty correctly predicted taken branches. 'Another interesting branch support feature R10000 branch link quadword, holds four instructions past recent subroutine call. acts return target instruction cache supports fast returns leaf subroutines. initial design 1994, similar cache structure proposed holding four instructions fall-through path four recent predicted-taken branches. Upon detecting misprediction branch-resume cache would allow immediate restart, R10000 descriptions 1994 1995 describe unique feature. However, best mechanism saves single cycle simpler method fetching fall-through path instructions instruction cache, left actual R10000 chip. support strong memory consistency, load/store instruction queue main- tained program order. Two 16-by-16 matrices address matching used determine load forwarding also cache set conflicts detected avoided. See Halfhill [1994b] overview RIOOOO. Yeager [1996] presents in-depth description design, including details instruction queues active list. Vasseghi et al. [1996] presents circuit-design details RIOOOO. follow-on design, R12000, increases active list 48 entries BHT 2048 entries, adds 32-entry two-way set-associativ e branch target cache. recent R14000 R16000 similar R12000. 8.3.9.3 MIPS R5000 QED RM7000 /1996 1997. R5000 de- signed Q ED, company started Earl Killian Tom Riordan, also designed R4x000 family members. R5000 organization very422 MODERN PROCESSOR DESIGN simple provides one integer one floating-point instruction issued per cycle (a level-1 design); however, performance impressive com- peting designs extensive out-of-order capabilities. Riordan also extended approach RM7000, retains R5000's du al-issue structure inte- grates one chip 16K-byte four-way set-associative LI instruction cache, 16K-byte four-way set-associative LI data cache, 256K-byte fou r-way set- associative L2 unif ied cache. 8.3.10 Motorola Two Motorola designs superscalar, apart processors PowerPC family. 8.3.10.1 Motorola88110/1991. 88110 aggressive design time (1991) introduced shortly IBM RS/6000 started gaining popularity. 88110 dual-issue implementation Motorola 88K RISC architecture extended 88 K architecture introducing separate extended-precision (80-b it) floating-point register fil e adding graphics instructions nondelayed branches. 88110 notable 10 function units (see Figure 4.7) use history buffer provide precise excep- tions recovery branch mispredictions. Keith Diefendorff chief architect; Willie Anderson designed th e graphics floating-point extensions; Bill Moyer designed memory system. 10 function units instruction-fetch/branch unit, load/store unit, bit- field unit, floating-point add unit, multiply unit, divide unit, two integer units, two graphics units. Floating-point operation performed using 80-bit extended precision. integer floating-point register files ha two dedicated history buffer ports record old values two result registers per c ycle. history buffer provided 12 entries could restore two reg isters per cycle. cycle two instructions fetched, unless instruction pair crossed cache line. decoder aggressive tried dual issue cycle. one-entry reservation station branches three-entry reservation station stores; thus proce ssor performed in-order issue except branches stores. Instructions speculatively issued past predicted branch tagged con- ditional flushed branch mispredicted; registers already written mispredicted conditional instructions restored using history buffer. Con- ditional stores, however, allowed update data cache remained reservation station branch resolved Branches statically predicted. target instruction cache returned pair instructions branch's target address th e 32 recently taken branches. TIC virtually addressed, flushed context switch. register renaming, instruction pairs write-after-read dependences allowed dual issue, dependent stores allowed dual issue r esult-producing instruction. load/store unit four-entry load buffer allowed loads bypass stores.SURVEY SUPERSCALAR PROCESSORS two 80-bit writeback busses shared among 10 function units. different latencies among function units, instructions arbi- trated th e busses. arbitration priority unusual gave priority lower-cycle-count operations could thus delay long-latency opera- tions. apparently done response customer demand typeof priority. Apple, Next, Data General, Encore, Harris designed machines 88110 (with latter three delivering 88110-based system s). However, Motorola difficulty manufacturing fully functional chips canceled revisions follow-on designs favor supporting PowerPC. However, several cache, TLB, bus design te chniques 88110 used IBM/ Motorola PowerPC processors Motorola 68060. See Diefendorff Allen [1992a,b] Ullah Holle [1993] articles 88110. 83.10.2 68060/1993. 68060 first superscalar implementation 68000 CISC architecture family make market. Even though many earliest workstation used 680x0 processors Apple chose Macintosh, 680x0 family displaced numerous IA32 RISC designs. Indeed, Motorola chosen 1991 target PowerP C workstation market, thus 68060 designed low-cost, low-power entrant embedded systems market. architect Joe Circello. 68060 pipeline illustrated Figure 8.14. 68060 implementation decoupled microarchitecture translates variable-length 6 8000 instruction fixed-length format completely iden- tifies resources required. translated instructions ar e stored 16-entry FIFO buffer. entry room 16-bit opcode, 32-bit extension words, early decode information. complex instructions require one entry buffer. Moreover, ost complex 68040 nstruction types Integer FetchEarly decodeDecodeAddress generateD-cache readExecuteD-cache write acc -Writeback Secondary pipeline Figure 8.14 Motorola M68060 Pipeline Stages.424 MODERN PROCESSOR DESIGN handled 68060 hardware instead implemented traps emulation software. 68060 contains 256-entry branch cache 2-bit predictors. branch cache also uses branch folding, branch condition address recov- ery increment stored along target address branch cache entry. entry tagged wit h address instruction prior branch thus allows branch eliminated instruction stream sent FIFO buffer whenever condition code bits sa tisfy branch condition. issue logic attempts in-order issue two instructions per cycle FIFO buffer two four-st age operand-execution pipelines. primary operand- execution pipeline execute instructions, including init iation floating- point instructions separate execution unit. secondary operand-execution pipeline executes integer instructions. dual pipelines must operated lockstep manner, similar Pentium, design control logic much sophisticated. operand execution pipeline composed two pairs fetch execute stages; Motorola literature describes two RISC engines placed back back. required instructions memory operands: first pair fetches address com- ponents uses ALU calculate effective address (and starts cache read), second pair fetches register operands uses ALU calculate operation result. Taking further, generalizing effective address ALU, operations executed first two stages primary pipe- line results forwarded second pipeline cascaded manner. instructions always executed first two stages, oth- ers dynamically alloca ted according issue pair dependency; thus many times p airs dependent instructions issued cycle. Register renaming also used remove f alse dependences issue pairs. data cache four-way int erieaved allows one load one nonco nflict- ing store execute simultaneously. See Circello Goodrich [1993] , Circello [1994], Circello et al. [1995] detailed descriptions 68060. 8.3.11 PowerPC—32-bit Architecture PowerPC architecture result cooperation begun 1991 IBM. Motorola, Apple. IBM Motorola set joint Somerset Design Center Austin, Texas, POWER ISA 88110 bus interface adopted starting points joint effort. Single-precision floating-point, revised int eger multiply divide, load word reserve store word condi- tional, support bot h little-endian well big-endian added ISA, along definition weakly ordered memory model I/O barrier instruction (the humo rously named "eieio" instruction). Features moved include record locking, th e multiplier-quotient (MQ) register associated instructions, several bit-field string instructions. Cache control instructions also changed provide greater flexibility. lead architects Rich Oehler (IBM), Keith Diefendorff (Motorola), Ron Hochsprung (Apple), John Sell (Apple).SURVEY SUPERSCALAR PROCESSORS Diefendorff [1994], Diefendorff et al. [1994], Diefendorff Silha [1994] contain informa tion history PowerPC cooperation changes POWER. 8.3.11.1 PowerPC 601 /1993. 601 first implementation PowerPC architecture designed Charles Moore John Muhich. important design goal time market, Moore's previous RSC design used starting point. bus cache coherency schemes Motorola 88110 also used leverage Apple's previous 88110-based system designs. Compared RSC, 601 unified cache enlarged 32K bytes TLB structure followed 88110 approach map ping pages larger blocks. cycle, bottom fo ur entries eight-entry instruction q ueue decoded. Floating-point instructions branches could dispatched four entries, integer instructions issued bottom e ntry. unique taggin g scheme linked instructions issued/dispatched given cycle instruction packet. progress packet monitored integer instruction served th e anchor pa cket. integer instruction available issue given cycle, nop generated could serve anchor packet. instructions packet com- pleted time. Following RSC design, 601 two-entry floating-point instruction queue instructions dispatched, rename floating- point registers. RSC floating-point pipeline stage design multiply add reused. integer unit also handled l oads stores, sophisticated memory system RSC. processor cache, 601 added two-entry load queue three-entry store queue. cache memory five-entry memory queue added make cache nonblocking. Branch instructions predicted manner RSC, conditional dispatch execu tion could occur past unresolved branches. designers added many multiprocessor capabilities 601. example, data cache implemented MESI protocol, tags double-pumped cycle allow sn ooping. writ eback q ueue entries also snooped refills could priority without causing coherency problems. See Becker et al. [1993], Diefendorff [1993], Moore [1993], Potter et al. [1994], Weiss Smith [1994] information 601. 8.3.11.2 PowerPC603/1994. 603 low-power implementation PowerPC designed Brad Burgess, Russ Reininger, Jim Kahle small, single-processor systems, laptops. 603 separate 8K-byte instruction data caches five independent execution units: branch, integer, system, load/store, floating-point. system unit executes condition code logic operations instructions move data special system registers. Two instructions fetched cycle instruction cache sent branch unit six-entry instruction queue. branch unit delete426 MODERN PROCESSOR DESIGN branches instruction queue change branch unit registers; otherwise, branches pass thr ough system unit. decoder looks bottom two entries instruction queue issues/dispatches two instructions per cycle. Dispatch includes reading register operands assigning rename register destination registers. five integer rename registers four floating-point rename registers. reservation station execution unit dispatch occur even data dependences. Dispatch also requires entry issued/dispatched instruction allocated five-entry completion buffer. Instructions retired completion buffer rate two per cycle; retire- ment includes updating register files transferring contents assigned rename registers. instructions change registers retire completion buffer program order, exceptions precise. Default branch prediction based sign displacement, bit branch opcode used th e compilers reverse prediction. Specula- tive execution past one condition al branch provided, speculative path able follow unconditional branch branch-o n-count conditional branches wait resolved. Branch misprediction handled flushing pre- dicted instructions completion buffer contents subs equent branch. load/store unit performs multipl e accesses un aligned operands sequences multiple accesses load-multiple/store-multiple string instruc- tions. Loads pipelined two-cycle latency; stores pipelined. Denormal floating-point numbers supported special internal format, flush-to-zero mode ena bled four power man agement modes: nap, doze, sleep, dynamic. dynamic mode allows idle execution units reduce powe r consumption without impacting performance. See Burgess et al. [1994a, 1994b] special issue Communications ACM "The Making PowerPC" information. excellent article describing simulation studies design tradeoffs 603 found Poursepanj et al. [1994]. 603e, done Brad Burgess Robert Golla, later implementation doubles sizes on-chip caches provides system unit ability execute integer adds compares. (Thus 603e could described limited second integer unit.) 83.113 PowerPC604/1994. 604 looks much like standard processor design Mike Johnson's textbook superscalar design. shown Figure 8 .15, six function units, two-entry r eservatio n station, 16-entry reorder buffer (completion buffer). four instructions fetched per cycle four-entry decode buffer. instructions next placed four-entry dispatch buffer, read operands pe rforms register renaming. bu ffer, four instructions dispatched per cycle six func- tion units: branch unit, two integer units, integer multiply unit, load/store unit, floating-point unit. integer units issue instructionSURVEY SUPERSCALAR PROCESSORS 41 Integer Fetch Decode Dispatch Execute Complete writeback r—#He»* 1 Branch unit Floating-point unit —*|||*-[mult |*--»-) add [» *-| round [< Reservation stationsReorder buffer Figure 8 .15 IBM/Motorola PowerPC 604 Pipeline Stages. either reservation station entry (i.e., order), whereas reservation stations assigned units issue order given instruction type but, course, provide interunit slip. two levels speculative execution supported. reorder buffer retire four instructions per cycle. Renaming provided 12-entry rename buffer integer registers, eight-entry rename buffer fl oating-point registers, eight-entry rename buffer condition codes. Speculative exec ution allowed stores, 604 also disallows speculative execution logical operations condition regis- ters integer arithmetic operations use carry bit. Branch prediction 604 supported 512-entry BHT , entry 2-bit predictor, 64-entry fully associative BTAC. decode stage recognizes handles prediction unconditional branches branches hit BHT BTAC. also special branch prediction branches count register, typically implement innermost loops. dispatch logic stops collecting instructions multiple dispatch encoun- ters branch, one branch per cycle processed. Peter Song chief architect 604. See Denman [1994] Song et al. [1994] nformation 604. Denman et al. [1996] discuss follow-on chip, 604e, lower-power, pin-compatible version. 604e doubles cache sizes provides separate execution units condition register operations branches. hese two units two-entry reservation station, dispatch limited one per cycle.428 MODERN PROCESSOR DESIGN 8.3.11.4 PowerPC 750 (G3)/1997. chief architect 750 Brad Burgess. 750 designed low-power chip four pipeline stages, less buffering 604 . Burgess c haracterizes design "modest issu: width, short pipeline, large caches, ag gressive branch unit focused resolving branches rather predicting them." 750 six function units: two integer units, system register u nit, load/store uni t, floating-point unit, ar"* branch unit. Unlike 604, function units branch unit load/store unit one entry reservation stations. Instructions pre-decoded 36-bit format prior storing instruction cache. Four instructions fetched per cycle, two instructions dispatched per cycle two bottom entries six-entry instruction bu ffer. Branches processed soon recognized, predicted taken, deleted (folded out) instruction buffer overlaid instructions fro branch target path. Speculative exec ution provided past one unresolved branch, specu lative fetching continues past two unresolved branches. interesting approach save space six-entry com- pletion buffer "squashing" nops untaken branches instruc- tion buffer prior dispatch. two types instructions allocated completion buffer entries, unresolved, predicted-untaken branch held th e branch unit resolution misprediction recovery performed. 750 includes 64-entry four-way set-associative BTIC 512-entry BHT. chip also includes two-way set-associative level-two cache controller level-two tags; supports 256K bytes, 512K bytes, 1 Mbyte off-chip SRAM. (The 740 version chip contain L2 tags controller.) See Kennedy et al. [1997] details 750. 8.3.11.5 PowerPC 7400 (G4) 7450 (G4+) /1999 2001 . 7400, design led Mike Snyder, essentially th e 750 AltiVec added. major redesign 74xx series occurred 2001 seven-stage 7450. design, led Brad Burgess, issue retire rates increased three per cycle, 10 function units provided. Th e completion queue enlarged 16 entries, branch prediction also en improved quadru- pling BHT 2048 entries doubling BTIC 128 entries. 256K-byte L2 cache integrated onto 7450 chip. See Diefendorff [1999] description 7450. 8.3.11.6 PowerPCe500Core/2001. e500 core imp lements 32-bit embedded processor "Book E" instruction set. e500 also implements signal processing engine (SPE) extensions, provide two-element vector operands, integer select extension, provides fo r partial predication. e500 core two-wa issue, seven-stage-pipeline design similar ways 7450. Branch prediction e500 core provided single struc- ture, 512-entry BTB. Two instructions dispatched 12-entry instruction queue per cycle; and, instructions moved theSURVEY SUPERSCALAR PROCESSORS 4: four-entry general instruction queue, one moved moved two-entry branch instruction queue. single reservation station placed branch instruction queue branch unit, similar arrangement occurs functional units, fed instead general instruction queue. units include two simple integer units, multiple- cycle integer unit, load/store unit. SPE instructions execute sim- ple multiple-cycle integer units along rest instructions. completion queue 14 entries ca n retire two instructions per cycle. 8.3.12 PowerPC—6 4-bit Architecture PowerPC architecture de fined early 1990s, 64-bit mode operation also defined along 80-bit virtual address. See Peng et al. [1995] detailed descrip tion 64-bit PowerPC architecture. 8.3.12.1 PowerPC 620/1995. 620 first 64-bit implementation PowerPC architecture detailed Chapter 6. designer included Waldecker, Chin Ching Kau, Dave Levitan. four-wa issue organization similar 604 used mix function units. However, 620 aggressive 604 several ways. example, decode stage removed instruction pipeline instead replaced pre-decoding instruction cache refills (see Figure 6.2). load/s tore unit reservation station incr eased tw entries three entries out-of-order issue, branch unit reservation station increased two entries four entries. assist speculating four branc hes, 620 doubled number condition register fields rename buffers (to 16), quadrupled number entries BTAC BHT (to 256 2048, respectively). implemen- tation simplifications (1) nteger instructions require two source registers could dispatched bottom two slots eight-entry instruction queue, (2) teger rename registers cut 12 604 8 620, (3) reorder buffer entries allocated released pairs. 620 implementation reportedly bugs initially restricted multi- processor operation, systems shipped 620 chips. information design 620, see Thompson Ryan [1994] Levitan et al. [1995]. 8.3.12.2 POWER3 (630) /1998. Chapter 6 note single floating-point unit 620 inability issue one load plus one store per cycle major bottlenecks design. problems addressed follow-on esign, called first 630 later known POWER3. Starting 620 core, POWER3 doubled number f loating-point load/ store units two each. data cache supported two loads, one store, one refill cycle; also four miss handling registers rather single register found 620. (See Table 6.11.) eight function units POWER3 could issued instruction cycle (versus issue limit four 620). branch load/store430 MODERN PROCESSOR DESIGN instructions issued order respective reservation stations, five units could issued instructions ord er. completion buffer doubled 32 entries, number rename registers doubled tripled integer instruction floating-point instructions, respectively. four- stream hardware prefetch facility also added. decision made store operands reservation station entries; instead, operands read physical registers separate pipe stage prior execution. Also, timing issues led separate finish stage prior commit stage. Thus POWER3 pipeline two additional stages compared 620 able reach clock rate approximately three times faster 620. See Song [1997b] O'Connell White [2000] information POWER3. 8.3.12.3 POWER4/2002. IBM POWER4 high-performance multipro- cessing system design. Jim Kahle Chuck Moore chief archi tects. chip contains two processing cores, core eight function units (including two floating-point units two load/store units) LI caches sharing single unified L2 cache L3 cache controller directory. single multichip module package four chips, basic system building block eight-way SMP. eight-way issue core equally ambitious design surrounding caches memory access path logic. traditional IBM brainiac style explicitly discarded POWER4 favor deeply pipelined speed demon even cracks th e enhanced-RISC PowerPC instructions separate, sim- pler internal operations. 200 instructions in-flight. Instructions fetched based hybrid branch prediction scheme unusual use 1-bit predictors rather typical 2 -bit predictors. 16K-entry selector chooses 16K-entry local predictor gshare- like 16K-entry global predictor. Special handling branch-to-link branch-on- count instructions also provided. POWER4 allows hint bits branch instructions override dynamic branch prediction. scheme somewha reminiscent PowerPC 601, instruction groups formed track instruction completion; however, POWER4, group anchored branch instruction. Groups five formed sequentially, anchoring branch instruction fifth slot nops used pad unfilled slots. Condition register instructions must specially handled, assigned first second slot group. groups tracked use 20-entry global completion table. one group dispatched issue queues per cycle, one group complete per cycle. Instructions require serialization form single-issue groups, groups cannot execute u ntil uncompleted groups front them. Instructions cracked two inter- nal operations, like load-with-update, must internal operations group. complex instructions, like lo ad-multiple, cracked intoSURVEY SUPERSCALAR PROCESSORS 4 several internal operations (called millicoding), operations must placed groups separated instructions. Upon exception, instruc- tions within group exception occurred redispatched sep- arate, single-instruction groups. issue queues, instructions internal operations issue order. 11 issue queues total 78 entries among them. scheme somewhat reminiscent HP 8000, even-odd distribution group slots issue queues function units used. abundance physi- cal registers provided, including 80 physical registers 32 architected general registers, 72 physical registers 32 architected floating-point regis- ters, 16 physical registers architected link count registers, 32 phys- ical reg isters eight condition register fields. POWER4 pipeline nine stages prior instruction issue (see Figure 6.6). Two stages required instruction fetch, six used instruction cracking group formation, one stage provides res ource mapping dispatch. simple integer instruction requires five stages executing, clud- ing issue, reading operands, executing, transfer, writing result. Groups complete final complete stage, making 15-stage pipeline integer instruc- tions. Floating-point instructions require extra five stages. on-chip caches include two 64K-byte instruction ca ches, two 32K-byte data ca ches, unified L2 cache approximately 1.5 Mbytes . LI data cache provides two reads one store per cycle. eight prefetch streams off-chip L3 32 Mbytes supported. L2 cache uses seven- state, enhanced MESI coherency protocol, L3 uses five-state protocol. See Section 6.8 Tendler et al. [2002] information POWER4. 8.3.12.4 PowerPC 970 (G5)/2003. PowerPC 970 single-core version POWER4, includes AltiVec extensions. chief architect Peter Sandon. extra pipeline stage added front end timing pur- poses, 970 16-stage pipeline integer instructions. two SIMD units added make total 10 function units, instruction group size remains five issue limit remains eight instructions per cycle. See Halfhill [2002] details. 8.3.13 PowerPC-AS Following directive IBM President Jack Kuehler 1991, corporate-wide effort made investigate standardizing PowerPC. Engineers AS/400 division Rochester, Minnesota, working commercial RISC design (C-R ISC) next generation single-level store AS/400 machines, told instead adapt 64-bit PowerPC rchitecture. extension, called Amazon later PowerPC-AS, designed Andy Wottr eng Mike Corrigan IBM Rochester, leadership Frank Soltis. Since th e 64-bit PowerPC 620 ready, Rochester went develop multichip A30 (Muskie), Endicott developed single-chip A10 (Cobra). designs include 32-bit PowerPC instructions, the432 MODERN PROCESSOR DESIGN next Rochester design, A35 (Apache), did. Apache used RS/6000 series called RS64. See Soltis [2001] details Rochester efforts. Currently, PowerPC-AS processors, including POWER4, implement 228 64-bit PowerPC instruc tion set plus 150 AS-mode instructions. 8.3.13.1 PowerPC-ASA30(Muskie)/1995. A30 seven-chip, high- end, SMP-capabl e implementation. design based five-stage pipeline: fetch, dispatch, execute, commit, writeback. Five function units provided, f instructions could issued per cycle, order. Hazard detection done execute stage, rather dispatch stage, floating- point registers r enamed avoid hazards. commit stage held results could written back register fil es. Branches handled using predict-untaken, branch unit could look six instructions back 16-entry current inst ruction queue determine branch target addresses. eight-entry branch target queue used prefetch taken-path instructions. Borkenhagen et al. [1994] describes A30. 8.3.13.2 PowerPC-AS Al 0 (Cobra) A35 (Apache, RS64) /1995 1997. A10 single-chip, uniprocessor-onl implementation four pipeline stages in-order issue three instructions per cycle. renaming done. See Bishop et al. [1996] details. A35 (Apache ) follow- design Rochester added full PowerPC instruction set multipro- cessor support A10. f ive-chip implementation introduced 1997. 8.3.13.3 PowerPC-AS A50 (Star series) /1998-2001 . 1998, Rochester intro- duced first multithreaded Star series PowerPC-AS processors. A50, also called Northstar known RS 64-II used RS/6000 systems. Process changes [specifically, copper interconnect silicon insulator (SOI)] led th e A50 design renamed Pulsar / RS64-III i-Star. See Borkenhagen et al. [2000] description recent member Star series, s-Star RS64-IV. 8.3.14 SPARC Version 8 SPARC architecture RIS C design derived work David Patterson University California Berkeley. One distinguishing feature early work use register windows reducing memory traffic procedure calls, feature ado pted SPARC chief architect Robert Garner. first SPARC processors implemented called version 7 archi- tecture 1986. highly pipeline oriented defined set integer instruc- tions, could implemented one cycle execution (integer multiply divide missing ), delayed branches. architecture manual explicitl stated "an untaken branch takes much time taken branch." floating-point queue also e xplicitly defined architectureSURVEY SUPERSCALAR PROCESSORS manual; reorder buffer directly accessed exception handler software. Although version 7 architecture manual included su ggested subroutines integer multiply divide, version 8 architecture 1990 adopted integer multiply. SuperSPARC HyperSPARC processors implement version 8. 8.3.14.1 Texas Instruments SuperSPARC (Viking)/1992. SuperSPARC designed Greg Blanck Sun , implementation overseen Steve Krueger Tl. SuperSPARC issued three instructions per cycle pro- gram order built around control unit handled branching, floating- point unit, unique integer unit contained three cascaded ALUs. cascaded ALUs permitted simultaneous ssue dependent pair integer instructions. SuperSPARC fetched aligned group four instructions cycle. decoder required one one-half cycles attempted issue three instructions la st half-cycle, Texas Instruments called grouping stage. instructions single-issue (e.g., register window save restore, integer multiply), grouping logic could combine two integer instructions, one load/store, and/or one floating-point instruction per gr oup. actual issue rule quite complex involved resource constraints limit number integer register write ports. instruction group said finalized control transfer instruction. general, issued, group proceeded pipelines lockstep manner. However, floating- point instructions would placed four-entry inst ruction buffer await floating-point unit availability thereafter would execute independently. SPARC floating-point queue provided dealing wit h exceptions. noted before, dependent instruction (integer, store, branch) could included group operand-producing integer instruction due cascaded ALUs. true operand-producing load; possible cache misses , instruction dependent load placed next group. SuperSPARC contained two four-instruction fetch queues. One used fetching along sequential path, used prefetch instruc- tions branch targets whenever branch encountered sequential path. Since group finalized control transfer instruction, delay slot ins truction placed next group. group would speculatively issued. (Thus SuperSPARC actually predict-untaken design). branch taken, instructions speculative group, delay slot instruction, would squashed, prefetched target instructions would issued next group. Thus branch penalty aken branch; rather one-issue cycle branch group the, target group delay slot instruction executed tself. See Blanck Krueger [1992] overview SuperSPARC. chip somewhat performance disappointment, allegedly due problems cache design rather c ore.434 MODERN PROCESSOR DESIGN 8.3.14.2 Ross HyperSPARC (Pinnacle) /1993. HyperSPARC came mar- ket year two SuperSPARC less ag gressive desig n terms multiple issue. However, success competing performance SuperSPARC another example, like Alpha versus POWER, speed demon versus brainiac. Hy perSPARC specification done Raju Vegesna first simulator Jim Monaco. preliminary article Hy perSPARC published Vegesna [1992]. HyperSPARC four execution units: integer, floating-point, load/ store, branch. Tw instructions per c ycle could fetched 8K-byte on-chip instruction cache placed decoder. two-instruction-wide decoder unaggressive would accept instructions previ- ously fetched instructions iss ued. decoder also fetched register oper- values. Three special cases dependent issue supported: (1) sethi depen- dent, (2) sethi dependent load/store, (3) integer ALU ins truction sets condition code dependent branch. Two f loating-point instructions could also dispatched nto four-entry floating-point prequeue cycle, queue room. several stall conditions, involved register file port contention since two read ports integer register file. Moreover, 53 single-issue instructions, including call, save, restore, mu ltiply, divide, floating-point compare. integer unit total 136 registers, thus providing eight overlapping windows 24 registers eight global registers. integer pipeline, well load/store branch pipelines, consisted four stages beyond common fetch decode: execute, cache read, cache write, register update. integer unit use two cache-related stages, included non-floating-point pipelines would equal length. Integer multiply divide unusually long, 18 37 cycles, respectively; moreover, stalled instruction ssue completed. floating-point unit's four-entry pr equeue three-entry postqueue together implemented SPARC floating-point queue technique out-of-order completions floating-point unit. prequeue allowed de coder dis- patch floating-point instructions quickly possible. Instructions floating- point preq ueue decoded order issued postqueue; postqueue entry corresponded execution stage floa ting-point pipeline (execu te-1, execute-2, round). floating-point load dependent floating-point instruction could issued/dispatched cycle; however, dependent instruction would spend two cycles prequeue loaded data forwarded execute-1 stage. floating-point instruction dependent floating- point store paired decoder, store waited least two cycles decoder opera tion result entered round stage forwarded load/store unit subsequent cycle. 8.3.14.3 Metaflow Lightning Thunder /Canceled. Lightning Thunder out-of-order execution SPARC designs Bruce Lightner andSURVEY SUPERSCALAR PROCESSORS 43 Val Popescu. designs used centralize reservation station approach called deferred-scheduling register-renaming instruction shelf (DRIS). Thunder described 1994 Hot Chips improved three-chip version four-chip Lightning, designed 1 991. Thunder issued four instructions per cycle eight execution units: three integer units, two floating- point units, two load/store units, one branch unit. Branch prediction dynamic included return address prediction. See Lightner Hill [1991] Popescu et al. [1991] articles Lig htning, see Lightner [1994] pre- sentation Thunder. Neither design delivered, Hyundai assigned patents. 8.3.15 SPARC Version 9 64-bit SPARC instruction set known version 9. revisions decided large committee 100 meetings. Major contributors Dave Ditzel (c hairman), Joel Boney, Steve Chessin, Bill Joy, Steve Kleiman, Steve Kruger, Dave W eaver, Winfried Wilcke, Robert Yung. goals version 9 architecture also included avoiding serialization points. Thus, four separate floating-point condition codes well new type integer branch conditionally branches basis integer register contents, giving effect multiple integer condition cod es. Version 9 also added support nonfaulting speculative loads, branch prediction bits branch instruction for- mats, conditional moves, memory-barrier instruction weakly ordered memory model. 8.3.15.1 HaLSPARC64/1995. SPARC64 first several imple- mentations SPARC version 9 architecture planned HaL, including ultiprocessor version directory-based cache cohe rence. HaL designs use unique three-level memory man agement scheme (with regions, views, pages) reduce amount storage required mapping tables 64-bit address space. SPARC64 designers Hisashige Ando, Winfried Wilcke, Mike Shebanow. windowed register file contained 116 integer registers, 78 bound given time form four SPARC register windows. left 38 free integer registers used renaming. also 112 floating-point regis- ters, 32 bound given time singl e-precision another 32 bound double-precision. left 48 free floating-point registers used renaming. integer register file 10 read ports 4 write ports, floating-point register file 6 read ports 3 write ports. SPARC64 four 64K-byte, virtually addressed, four-way set-associative caches (two used instructions, two used data; allowed two nonconflicting load/stores per cycle). real address table p rovided inverse mapping data caches, nonblocking access data caches (with load merging) also provided using eight reload buffers. sp eeding instruction access, level-0 4K-byte direct-mapped instruction cache provided SPARC instructions stored partially decoded internal436 MODERN PROCESSOR DESIGN format; format included room partially calculated branch target addresses. 2-bit branch history also provided instruction level-0 instruction cache. four instructions dispatched per cycle, limits according instruction type, four reservation stations. 8-entry reservation station four integer units (two integer ALUs, integer multiply unit, integer divide unit); 8-entry reservation station two address generation units; 8-entry reservation station two floating-point units (a floating-point multiplier-adder unit floati ng-point divider); 12-entry reservation sta- tion two load/store units. Register renaming performed di spatch. load store instruction dispatched address generation unit reser- vation sta tion loa d/store unit reservation station. effective address sent address generation unit value cache associ ated load/ store reservation station. designs provide equal number instructions dis- patched, issued, completed, retired given cycle, SPARC64 wide variance. given cycle, four instructions could dispatch, seven instructions could issue, ten could execute, nine instructions could complete, eight instructions could c ommit, four instructions could retire. maximum 64 instructions could active point, hard- ware kept track ring via individually assigned 6-bit serial num- bers. ring operated checkpoint-repair manner provide branch misprediction recovery, ro om 16 checkpoints (at branches instructions modified unrenamed co ntrol registers). Four pointers used update ring: last issued serial number (ISN), last committed serial number (CSN), resource recovery pointer (RRP), noncommitted memory serial num- ber pointer (NCSNP), allowed ag gressive scheduling loads stores. pointer last checkpoint appended instruction allow one- cycle recovery checkpoint. trapping instructions aligned checkpoint, processor could undo four instructions per cycle. integer instructio n pipeline seven stages: fetch, disp atch, execute, write, complete, commit, retire. decode stage missing ince decod- ing primarily accomplished instructions loaded level-0 instruction cache. complete stage checked errors/exceptions; commit stage performed in-order update results architectural state; retire stage deallocated resources. Two extra execution stages required load/stores. Using trap definitions version 9, SPARC64 could rename trap levels, allowed processor specul atively enter traps detected dispatch. See Chen et al. [1995], Patkar et al. [1995], Simone et al. [1995], Wilcke [1995], Williams et al. [1995] details SPARC64. Th e Simone paper detail several interesting design tradeoffs, including special priority logic issuing condition-code-modifying instructions. HaL bought Fujitsu, produced various revisions basic design, called SPARC64-II, -III, GP, -IV (e.g., increa sed level-0 instructionSURVEY SUPERSCALAR PROCESSORS cache BHT sizes). two-level branch predictor additional pipeline stage dispatch introduced SPARC64-III [Song, 1997a]. ambi- tious new core, known SPA RC64 V, eight-way issue design using trace cache value prediction. Mike Shebanow, chief architect, described design 1999 Microprocessor Forum [Diefendorff, 1999b] semi- nar presentation Stanford University 1999 [Shebanow, 1999]. Fujitsu can- celed project inste ad introduced another revision original core name SPARC64-V 20 03 [Krewell, 2002]. 8.3.15.2 UttraSPARC-l/1995. UltraSPARC-I designed Les Kohn, Marc Tremblay, Guillermo Maturana, Robert Yung. provided four-way issue nine function units (two integer ALUs, load/s tore, branch, floating-point add, floating-point ultiply, floating-point divide/square root, graphics add, graphics multiply). set 30 graphics instructions introduced UltraSPARC called th e visual instruction set (VIS). Block load/store instructions addi- tional register windows also provided UltraSPARC-I. Figure 8.16 illustrates Ul traSPARC-I pipeline. UltraSPARC-I ambitious out-of-order design many contemporaries. design team extensively sim ulated many designs, including various forms out-of-order processing. reported out-of- order approach would cost 20% penalty clock cycle time would likely increased time market three six months. Instead, high performance sought including features speculative, n onfaulting loads, UltraSPAR C compilers use perform aggressive global code motion. Figure 8.16 Sun UltraSPARC-I Pipeline tages.438 MODERN PROCESSOR DESIGN Building th e concepts grouping fixed-length pipeline segments found SuperSPARC HyperSPARC, UltraSPARC-I performed in- order issue groups four instructions each. design provided precise exceptions discarding traditional SPARC floating- point queue favor padding function unit pipelines four stages each. Exceptions longer- running operations (e.g., divide, square root) predicted. Speculative ssue provided using branch prediction mechanism similar Johnson's proposal extended instruction cache. instruction cache line UltraSPA RC-I contained eight instructions. instruction pair 2-bit history, instruction quad 12-bit next-cache-line field. history next- line field used fill instruction buffer, allocation history bits claimed improve prediction accuracy removing interference mul- tiple branches map entry traditional BHT. Branches resolved first execute stage integer floating-point pipelines. UltraSPARC-I relatively aggressive memory interface. instruction cache used set prediction metho provi ded access speed direct-mapped cache retaining red uced conflict behavior two-way set-associative c ache. nine-entry load buffer eight-entry store buffer. Load bypass provided well write merging la st two store buffer e ntries. See Wayner [1994], Greenley et al. [1995], Lev et al. [1995], Tremblay O'Connor [1996] descriptions UltraSPARC-I processor. Tremblay et al. [1995] discusses tradeoff decisions made design proc essor memory system. Goldman Tirumalai [1996] discuss UltraSPARC-Il, adds memory system enhancements, prefetching, UltraSPARC-I core. 8.3.15.3 UltraSPARC-Ill/2000. Gary Lauterbach chief designer UltraSPARC-Ill, retains in-order issue approach predecessor. UltraSPARC-Ill pipeline, however, extended 14 stages careful attention memory oandwidth. two nteger units, memory/special-instruction unit, branch unit, two floating-point units. Instructions combined groups four instructions, group proceeds pipelines lockstep man ner. Grouping rules reminiscent SuperSPARC Ultra- SPARC-I. However, Alpha 21164, UltraSPARC-Ill rejects global stall signaling scheme instead adopts replay approach. Branches predicted using form gshare 16K predictors. Pipeline timing co nsiderations led design wit h pattern history table held eight banks. xor result 11 bits program counter 11 bits global branch history shift register used read one predictor per bank, additiona l three low-order bits program counter used next stage select among eight predictors. Simulations indicated approach simila r accuracy normal gshare scheme. four-entry miss queue holding fall-through instructions used along 16-entry instruction queue (although sometimes described havingSURVEY SUPERSCALAR PROCESSORS 20 entries) reduce branch misprediction penalty untaken branches. Conditional mo ves available instruction set partial predication, code optimization section manual advises code performance better conditional branches conditional moves branches fairly predictable. reduce number integer data forwarding paths, variant future file, called working register file, used. Results written thi structure out-of-order manner thus available dependent instructions early possible. Registers renamed tagged. Instead, age bits included decoded instruction fields along destination register IDs used elim- inate WAW haza rds. WAR hazards prevented reading operands issue ("dispatch") stage. Precise exceptions supporte updating architectural register file last stage, possible exceptions checked. recovery necessary, working register file loaded architec- tural register fil e single cycle. UltraSPARC-Ill 32K-byte LI ins truction cache 64K-byte LI data cache. data cache latency two cycles, derives sum- addressed memor technique. 2K-byte write cache allows data cache appear write-through defers actual L2 update line evicted write cache itself. Individual byte valid bits allow storing changed bytes write cache also support write-merging. 2K-byte triple-ported prefetch cache provided, clock cycle pro vide two independent 8-byte reads receive 16 bytes main memory. addition availabl e software prefetch inst ructions, hardware prefetch engine detect stride load instruction within loop automatically generates prefetch requests. Also included on-chip memory controller cache tags 8-Mbyte L2 cache. See Horel Lauterbach [1999] Lauterbach [1999] information UltraSPARC-TII. working register file described detail U.S. Patent 5,964,862. UltraSPAR C-IV plan ned chip multiprocessor two UltraSPARC-Ill cores. 8.4 Verification Superscalar Processors Charles Moore (RSC, PPC 601, POWER4) recently started series articles IEEE Micro challenges complexity faced processor design teams [2003]. suggested design team trouble less two rification people assigned clean architect. simple humorous rule thumb may hold every case, true superscalar processors complex types logical designs. unusual 100 in-flight instructions may interact various ways interact corner ca ses, exceptions faults. combinatorial explosion possible states overwhelming. Indeed, several architects chosen simpler in-order design trategies explici tly reduce com- plexity thereby improve time-to-market.40 MODERN PROCESSOR DESIGN study verification techniques beyond scope chapter. How- ever, since veri fication p lays important role design process, sam- pling references verification efforts commercial su perscalar processors follows. Articles design verification techniques used Alpha processors includes Kantrowitz ack [1996], Grundmann et al. [1997], Reilly [1997], Dohm et al. [ 1998], Taylor et al. [1998], Lee Tsien [2001]. article Monaco et al. [1996] study functional verification PPC 604, article Ludden et al. [2002] recent study topic foi POWER4. sample approaches taken industry design teams, Turumella et al. [1995] review design verifi cation HaL SPARC64, Mangelsdorf et al . [1997] discuss verification HP PA-8000, Bentley Gray [2001] present verification te chniques used Intel Pentium 4. 8.5 Acknowledgments Author (Mark Smotherman) Several people helpful providing information superscalar pro- cessors covered chapter: Tilak Agerwala, Fran Allen, Gene Amdahl, Erich Bloch, Pradip Bose, Fred Brooks, Brad Burgess, John Cocke, Lynn Conway, Marvin Denman.Mike Flynn, Greg Grohoski, Marty Hopkins , Peter Song, Ed Sussenguth, associated IBM efforts; Alpert, Gideon Intrater, Ran Talmudi, worked NS Swordfish ; Mitch Alsup, Joe Circello, Keith Diefendorff, associated Motorola efforts; Pete Bannon, John Edmondson, Norm Jouppi, worked DEC; Mark Bluhm, worke Cyrix; Joel Boney, worked SPARC64; Bob Colwell, Andy Glew, Mike Haertel, Uri Weiser, worked Intel designs; Josh Fisher Bill Worley HP; Robert Garner, Sharad Mehrotra, Kevin Normoyle, Marc Tremblay Sun; Earl Killian, Kevin Kissell, John Ruttenberg, Ross Towle, associated SGI/M IPS; Steve Krueger Tl; Woody Lichtenstein Da Probert, worked Culler 7; Tim Olson; Yale Patt University Texas inventor HPS; Jim Smith University Wisconsin, designer ZS-1, co-enthusiast processor pipelin e version Jane's Fighting Ships; John Yates, wh worked Apollo DN10000. Peter Capek IBM also instrumental helping obtain information ACS. also want thank numerous students Clemson, including Michael Laird , Stan Cox, T. J. Tumlin. REFERENCES Processor manuals available ndividual manufacturers included references. Acosta, R., J. Kjelstrup, H. Tomg: "An instruction issuing approach enhancing performance multiple functional unit processors," IEEE Trans, Computers, C-35, 9, September 1986, pp. 815-828. Allen, F.: "The history language processor echnology BM," IBM Journal Research Development. 25. 5. September 1981, pp. 535-548.SURVEY SUPERSCALAR PROCESSORS 44' Alpert, D., D. Avnon: "Architecture Pentium microprocessor." IEEE Micro, 11, 3, June 1993, pp. 11-21. Asprey, T., G. Averill, E. DeLano, R. Mason, B. Weiner, J. Yetten "Performance features PA7100 microprocessor," IEEE Micro, 13,3, June 1993, pp. 22-35. Bailey, D.: "High-performance Alpha microprocessor design," Proc. Int. Symp. VLSI Tech., Systems andAppls., Taipei, Taiwan, June 1999, pp. 96-99. Bannon, P., J. Keller: 'The nternal architecture Alpha 21164 icroprocessor," Proc. COMPCON, San Francisco, CA, March 1995, pp. 79-87. Barreh, J., S. Dhawan, T. Hicks, D. Shippy: " POWER 2 processor," Proc. COMPCON. San Francisco, CA, Feb.-March 1994, pp. 389-398. Bashe, C, L. Johnson, J. Palmer, E. Pugh: IBM's Early Computers. Cambridge, MA: M.I.T. Press, 1986. Becker, M., M. Allen, C. Moore, J. Muhich, D. Turtle: "The PowerPC 60 1 micropro- cessor," IEEE Micro, 13, 5, October 1993, pp. 54-67. Benschneider, B., et al.: "A 300-MHz 64-b quad issue CMOS RISC microprocessor," IEEE Journal Solid-State Circuits, 30,11, N ovember 1995, pp. 1203-1214. [21164] Bentley, B., R. Gray, "Validating Intel Pentium 4 Processor," Intel Technical Journal, quarter 1, 2001, pp. 1-8. Bishop, J., M. Campion, T. Jeremiah, S. Mercier, E. Mohring, K. Pfarr, B. Rudolph, G. Still, T. White: "PowerPC A10 64-btt RISC microprocessor," IBM Journal Research Development, 40,4, July 1996, pp. 495-505. Blanchard, T., P. Tobin: "The PA 7300LC microprocessor: highly integrated system chip," Hewlett-Packard Journal, 48,3, June 1997, pp. 43-47. Blanck, G, S. Krueger: h e Su perSPAR C microprocessor," Proc. COMPCON, San Francisco, CA. February 1992, pp. 136-141. Borkenhagen, J., R. Eickemeyer, R. Kalla, S. Kunkel: "A multithreaded PowerPC processor commercial servers," IBM Journal Research Development, 44, 6, November 2000, pp. 885-898. [SStar/RS64-IV] Borkenhagen, J., G. Handlogten, J. Irish, S. Levenstein: "AS/400 64-bit Po werPC- compatible processor implementation," Proc. ICCD, Cambridge, , October 1994, pp. 192-196. Bose, P.: "Optimal code generation expressions super scalar machines," Proc. AFIPS Fall Joint Computer Conf., Dallas, TX, November 1986, pp. 372-379. Bowhill, W., et al.: "Circuit implementation 300-MHz 64-bit sec ond-generation CMOS Alpha CPU," Digital Technical Journal, 7, 1, 1995, pp. 100-118. [21164] Buchholz, W., Planning Computer System. New York: McGraw-Hill, 1962. [IBM Stretch] Burgess, B., M. Alexander, Y.-W . Ho, S. Plummer Litch, S. Mallick, D. Ogden, S.-H. Park, J. Slaton: "The PowerPC 603 microprocessor: high performance, low power, superscalar RISC microprocessor," Proc. COMPCON, San Francisco, CA, Feb.-March 1994a, pp. 300-306. Burgess. B., N. Ullah, P. Van Overen, D. Ogden: "The PowerPC 603 microprocessor," Communications ACM, 37, 6, June 1994b, pp. 34-42. Burkhardt B.: "Delivering next-generation performance today's installed computer base," Proc. COMPCON, San Francisco, CA, Feb.-March 1994, pp. 1 1-16. [Cyrix 6x86]442 MODERN PROCESSOR DESIGN Chan, K., et al.: "Design HP PA 7200 CPU," Hewlett-Packard Journal, 47, 1, February 1996, pp. 25-33. Chen, C, Y. Lu, A. Wong: "Microarchitecture HaL's cache subsystem," Proc. COMPCON San Francisco, CA, March 1995, pp. 267-271. [ SPARC64] Christie, D.: " Developing AMD-K5 architecture," IEEE Micro, 16,2, April 1996, pp. 16-26. Circello, J., F. Goodrich: "Th e Motorola 68060 microprocessor," Proc. COMPCON. San Francisco, CA, February 1993, pp. 73-78. Circello, J., "The superscalar hardware architecture MC68060," Hot Chips VI, videotaped lecture, August 1994, http://murLmicrosoft.corn/LectureDetails.asp7490 . Circello, I., et al.: "The superscalar architecture MC68060," IEEE Micro. 15, 2, April 1995, pp. 10-21. Cocke, J.: "The search performance scientific processors," Communications ACM, 31, 3, March 1988, pp. 250-253. Cohler, E., J. Storer: "Functionally parallel architecture array processors," IEEE Computer, 14, 9, Sept. 1981, pp. 28-36. [MAP 200] DeLano, E., W. Walker, J. Yetter, M. Forsyth: "A high speed superscalar PA-RISC processor," Proc. COMPCON. San Francisco, CA, February 1992, pp. 116-121. [PA 7100] Denman, M., "PowerPC 604 RISC microprocessor," Hot Chips VI. videotaped lecture, August 1994, http://mur1.inicrosoft.com/LectureDetails.asp7492 . Denman, M., P. Anderson, M. Snyder: " Design PowerPC 604e microprocessor," Proc. COMPCON Santa Clara, CA, February 1996, pp. 126-131. Diefendorff, K., "PowerPC 601 microprocessor," Hot Chips V, videotaped lecture, August 1993, http://murl.microsoft.com/LectureDetails.asp7483 . Diefendorff, K.: "History PowerPC aichitecture," Communications ACM, 37, 6, June 1994, pp. 28-33. Diefendorff, K., "K7 challenges Intel," Microprocessor Report. 12,14, Oct ober 26,1998a, pp. 1,6-11. Diefendorff, K., "WinChip 4 thumbs nose ILP," Microprocessor Report, 12, 16, December 7, 1998b, p. 1. Diefendorff, K., "PowerPC G4 gains velocity," Microprocessor Report, 13, 14, October 25, 1999a, p. 1. Diefendorff, K., "Hal makes Spares fly," Microprocessor Report, 13, 15, November 15, 1999b, pp. I. 6-12. Diefendorff, K., M. Allen: "The Motorola 88110 superscalar RISC microprocessor," Proc. COMPCON. San Francisco, CA, February 1992a, pp. 157-162. Diefendorff. K., M. Allen: "Organization Motorola 88110 superscalar RISC microprocessor," IEEE Micro, 12, 2, April 1992b, pp. 40-63. Diefendorff, K., R. Oehler, R. Hochsprung: "Evolution PowerPC architecture," IEEE Micro, 14,2. April 1994, pp. 34-49. Diefendorff, K., E. Silha: 'The PowerPC user instruction se architecture," IEEE Micro, 14, 5, December 1994, pp. 30-41. Ditzel, D., H. McLellan: "Branch folding CRISP microprocessor: Reducing branch delay zero," Proc. ISCA. Philadelphia, PA, June 1987, pp. 2-9.SURVEY SUPERSCALAR PROCESSORS Ditzel, D, H. McLellan, A. Berenbaum: "The hardware architecture CRISP microprocessor," Proc. ISCA, Philadelphia, PA, June 1987, pp. 309-319. Dohm, N., C. Ramey. D. Brown, S. Hildebrandt, J. Huggins, M. Quinn, S. Taylor, "Zen art Alpha verification," Proc. ICCD. Austin, TX, October 1998, pp. 111 -117. Eden, M., M. K agan: "The Pentium processor MMX technology," Proc. COMPCON, San Jose, CA, February 1997, pp. 260-262. Edmondson, J., "An ov erview Alpha AXP 21164 micr oarchitecture," Hot Chips VI, videotaped lecture, August 1994, http://murl.microsort,com/LectureDetalls.asp?493 . Edmondson, J., et al.: "I nternal organization Alpha 21164, 300-MHz 64-bit quad- issue CMOS RISC microprocessor," Digital Technical Journal, 7,1,1995a, pp. 119-135. Edmondson, J., P. Rubinfeld, R. Preston, V. Rajagopalan: "Superscalar instr uction execution 21164 Alpha microprocessor," IEEE Micro, 15, 2, April 1995b, pp 33-43. Fisher, J.: "Very long instruction word architectures ELI-512," Proc. ISCA, Stockholm, Sweden, June 1983, pp. 14 0-150. Flynn, M.: "Very high-speed computing systems," Proc. IEEE, 54, 12, December 1966, pp. 1901-1909. Gaddis, N., J. Lotz: "A 64-b quad-issue CMOS RISC icroprocessor," IEEE Journal Solid-State Circuits, 31,11, November 1996, pp. 1697-1702. [PA 8000] Gieseke, B., et al.: "A 600MHz superscalar RIS C microprocessor out-of-order execu- tion," Proc. IEEE Int. Solid-Stale Circuits Conference, February 1997. pp. 176-177. [21264] Gochman, S., et al., "The Pentium processor: Microarchitecture performance," fnre! Tech. Journal, 7, 2, May 2003, pp. 21-36. Goldman, G„ K Tirumalai: "UltraSPARC-H: advancement ultracomputing," Proc. COMPCON, Santa Clara, CA, February 1996, pp. 417 -423. Gowan, M., L. Brio, D. Jackson, "Power considerations design Alpha 21264 microprocessor," Proc. Design Automation Conf, San Francisco, CA, June 1998, pp. 726-731. Greenley, D., et al.: "UltraSPARC: next generation superscalar 64-bit SPARC," Proc. COMPCON, San Francisco, CA, March 1995, pp. 442-451. Gronowski, P., et al.: "A 433-MHz 64-b quad-issue RISC microprocessor," IEEE Journal ofSolid-State Circuits. 31, 11, November 1996, pp. 1687- 1696. [21164A] Grundmann, W., D. Dobberpuhl, R. Almond, N. Rethman, "Designing high perfor- mance CMOS microprocessors using full custom techniques," Proc. Design Automation Conf, Anaheim, CA, June 1997, pp. 722-727. Gwennap, L: "Cyrix describes Pentiu competitor," Microprocessor Report, 7, 14, October 25, 1993, pp. 1-6. [Ml/6x86] Gwennap, L.: "AMD's K5 designed outrun Pentium," Microprocessor Report, 8, 14, October 14.1994, pp. 1-7. Gwennap, L.: "Intel's P6 uses decoupled superscalar design," Microprocessor Report, 9, 2, February 16,1995, pp. 9-15. Halfhill, T.: "AMD vs. Superman," Byte. 19,11, November 1994a, pp. 95-104. [AMD K5] Halfhill, T: "T 5: Brute force," Byte. 19, 11, November 1994b, pp. 123-128. [MIPS R10000] Halfhill, T: "Intel's P6," Byte, 20,4, April 1995, p. 435. [Pentium Pro]444 MODERN PROCESSOR DESIGN Halfhill, T.: "AMD K6 takes Intel P6," Byte, 21, 1, January 1996a, pp. 67-72. Halfhill, T.: "PowerPC speed demon," Byte, 21,12, December 1996b, pp. 88NA1-88NA8. Halfhill. T., "IBM trims P ower4, adds AltiVec," Microprocessor Report, October 28,2002. Hall, C, K. O'Brien: "Performance characteristics architectural features IBM RISC System/6000," Proc. ASPLOS-IV, Santa Clara, CA, April 1991, pp. 303-309. Hester, P., "Superscalar RISC concepts design IBM RISC System/6000," video- taped lecture, August 1990, http://murl.rnicrosoft.com/LechireDetails.asp7315 . Hewlett-Packard: "PA-RISC 8x00 family microprocessors focus PA-8700," Hewlett Packard Corporation, Technical White Paper, April 2000. Hinton, G.: "80960—Next generation," Proc. COMPCON, San Francisco, CA, March 1989, pp. 13-17. Hinton, G. D. Sager, M. Upton, D. Boggs, D. Carmean, A. Kyker, P. Roussel: "The microarchitecture th e Pentium 4 processor," Intel Technology Journal, Quarter 1, 2001, pp. 1-12. Hollenbeck, D., A. Undy, L. Johnson, D. Weiss. P. Tobin, R. Carlson: "PA7300LC integrates cache cost/perfoririance," Proc. COMPCON, Santa Clara, CA, February 1996, pp. 167-174. Horel, T„ G. Lauterbach: "UltraSPARC-ITI: Designing third generation 64-bit perfor- mance," IEEE Micro, 19,3, May-June, 1999, p. 85. Horst, R., R. Harris, R. Jardine: "Multiple instruction issue NonStop Cyclone processor," Proc. ISCA, Seattle, WA, May 1990, pp. 216-226. Hsu, P., "Silicon GraphicsTFP micro-supercomputer chipset," Hot Chips V, videotaped lecture, August 1993, http://murI.microsoft.com/LectureDetails.asp7484 . [R8000] Hsu, P.: "Designing TFP microprocessor," IEEE Micro, 14, 2, April 1994, pp. 23-33. [MIPS R8000] Hunt, J.: "Advanced performance features 64-bit PA- 8000," Proc. COMPCON, San Francisco, CA, March 1995, pp. 123-128. IBM: IBM RISC System/6000 Technology. Austin, TX: IBM Corporation, 1990, p. 421. Johnson, L., S. Undy: "Functional design PA 73O0LC," Hewlett-Packard Journal, 48, 3, June 1997, pp. 48-63. Jouppi, N., D. Wall: "Available instruction-level parallelism superscalar super- pipelined machines," Proc. ASPLOS-III, Boston, MA, April 1989, pp. 272-282. Kantrowitz, M., L. Noack, "I'm done simulating: w what? Verification coverage analysis correctness checking DECchip 21164 Alpha microprocessor," Proc. Design Automation Conf, Las Vegas, NV, June 1996, pp. 325-330. Keltcher, C, K. McGrath, A. Ahmed, P. Conway, "The AMD Opteron processor multiprocessor servers," IEEE Micro, 23, 2, March-April 2003, pp. 66-76. Kennedy, A., et al.: "A G3 PowerPC superscalar low-power microprocessor," Proc. COMPCON, San Jose, CA, February 1997, pp. 315-324. [PPC 740/750, one diagram lists chip 613.] Kessler, R.: "The Alpha 21264 microprocessor," IEEE Micro, 19, 2, March-April 1999, pp. 24-36. Kessler, R., E. McLellan, D. Webb: "The Alpha 21264 microprocessor architecture," Proc. 1CCD. Austin, TX, October 1998, pp. 90-95.SURVEY SUPERSCALAR PROCESSORS 44 Knebel, P., B. Arnold, M. Bass, W. Kever, J. Lamb, R. Lee, P. Perez, S. Undy, W. Walker: "HP's PA7100LC: lo w-cost superscalar PA-RISC processor," Proc COMPCON, San Francisco, CA, February 1993, pp. 441-447. Krewell, K., "Fujitsu's SPARC64 V real deal," Microprocessor Report, 16,10, October 21, 2002, pp. 1^1. Kurpanek, G., K. Chan, J. Zheng, E. DeLano, W. Bryg: "PA7200: PA-RISC proces- sor integrated high performance MP bu interface," Proc. COMPCON, San Francisco, CA, Feb.-March 1994, pp. 375-382. Lauterbach, G: "Vying lead high-performance processors," IEEE Computer, 32, 6, June 1999, pp. 38-41. [Ul traSPARC m] Lee. R., J. Huck, "64-bit multimedia extensions PA-RISC 2.0 architecture," Proc. COMPCON, Santa Clara, CA, February 1996, pp. 152-160. Lee, R„ B. Tsien, "Pre-silicon verification Alpha 21364 mi croprocessor error handling system," Proc. Design Automation Conf, Las Vegas, NV, 2001, pp. 822-827. Leibholz, D., R. Razdan: "The Alpha 21264: 500 MHz out-of-order-execution microprocessor," Proc. COMPCON, San Jose, CA, February 1997. pp. 28-36. Lempel, O., A. Peleg, U. Weiser: "Intel's MMX technology—A new instruction set extension," Proc. COMPCON. San Jose, CA, February 1997, pp. 255-259. Lesartre, G, D. Hunt: "P A-8500: continuing evolution PA-8000 family," Proc. COMPCON, San Jose, CA, February 1997. Lev, L., et al.: "A 64-b microprocessor multimedia support," IEEE Journal ofSolid- State Circuits. 30, 11, November 1995. pp. 122 7-1238. [UltraSPARC] Levitan, D., T. Thomas, P. Tu: "The PowerPC 620 microprocessor: high perfor- mance superscalar RISC microprocessor," Proc. COMPCON, San Francisco, CA, March 1995, pp. 285-291. Lichtenstein, W.: "The architecture Culler 7," Proc. COMPCON, San Francisco, CA, March 1986, pp. 46 7^170. Lightner, B., "T hunder SPARC processor," Hot Chips VI, videotaped lecture, August 1994, http://murl.microsott.com/LectureDetails.asp7494 . Lightner, B., G. Hill: "The Metaflow Lightning chipset," Proc. COMPCON, San Francisco, CA, February 1991, pp. 13-18. Liptay, J. S.: "Design IBM Enterprise System/9000 h igh-end processor," IBM Journal Research Development, 36,4, July 1992, pp. 713-731. Ludden. J., et al.: "Functional verification POWER4 microprocessor POWER4 multiprocessor systems," IBM Journal Research Development, 46,1,2002, pp. 53-76. Mangelsdorf, et al.: "Functional rification HP PA 8000 processor," HP Journal, 48,4, August 1997, pp. 22-31. Matson, M-, et al., "C ircuit implementation 600 MHz superscalar RISC microproces- sor," Proc. 1CCD, Austin, TX, October 1998, pp. 104-110. [Alpha 21264] May, D., R. Shepherd, P. Thompson, "The T9000 Transputer," Proc ICCD, Cambridge, MA, October 1992, pp. 209-212. McGeady, S.: "The 960CA superscalar implementation 80960 architecture," Proc. COMPCON, San Francisco, CA, February 1990a, pp. 232-240. McGeady, S.: "Inside Intel's i9 60CA superscalar processor," Microprocessors Micro- systems, 14,6, July/August 1990b, pp. 385-396.446 MODERN PROCESSOR DESIGN McGeady, S., R. Steck, G. Hinton, A. Bajwa: "Performance enhancements superscalar i960MM embedded microprocessor," Proc. COMPCON, San Francisco, CA, February 1991, pp. 4-7. McGrath, K., " x86-64: Extending x86 architecture 64 bits," videotaped lecture, September 2000. http://murl.microsoft.com/LectureDetails.asp7690 . McLellan, E.: "The Alpha AXP architecture 21064 processor," IEEE Micro, 11, 3, June 1993, pp. 36-47. McMahan, S., M. B luhm, R. Garibay, Jr.: "6x86: Cyrix solution executing x86 binaries high performance microprocessor," Proc. IEEE, 83, 12, December 1995, pp. 1664-1672. Mirapuri, S., M. Woodacre, N. Vasseghi: "The Mips R4000 processor," IEEE Micro, 12,2, April 1992, pp. 10-22. Monaco, J., D. Holloway, R. Raina: "Functional verification methodology PowerPC 604 microprocessor," Proc. Design Automation Conf.. Las Vegas, NV, June 1996, pp. 319-324. Montanaro, J.: "The design Alpha 21064 CPU chip," videotaped lecture, April 1992, http://murl.mlcrosoft.com/LectureDetails.asp7373 . Moore, C: "The PowerPC 60 1 microprocessor," Proc. COMPCON, San Francisco. CA, February 1993, pp. 109-116. Moore, C: "Managing transition complexity elegance: Knowing problem," IEEE Micro, 23,5, Sept.-Oct. 2003, pp. 86-88. Moore, C, D. Balser, J. Muhich, R. East: "IBM ingle chip RISC processor (RSC)." Proc. ICCD, Cambridge, , October 1989, pp. 200-204. O'Connell, F., S. White: "POWER3: next generation PowerPC processors," IBM Journal Research Development, 44,6, November 2000, pp. 873-884. Oehler, R., M. Blasgen: "IBM RISC System/6000: Architecture performance," IEEE Micro. 11, 3, June 1991, pp. 54-<52. Papworth, D.: 'T uning Pentium Pro microarchitecture," IEEE Micro. 16.2, April 1996, pp. 8-15. Patkar, N., A. Katsuno, S. Li, T. Maruyama, S. Savkar, M. Simone. G. Shen, R. Swami, D. Tovey: ' Microarchitecture HaL's CPU," Proc. COMPCON. San Francisco, CA, March 1995, pp. 259-266. [SPARC64] Patt, Y., S. Melvin, W-M. Hwu, M. Shebanow, C. Chen, J. Wei: "Run-time generation HPS microinstructions VAX instruction stream," Proc. MICRO-IS. New York, December 1986, pp. 75-81. Peleg, A., U. Weiser: "MMX technology extension Intel architecture," IEEE Micro, 16,4, August 1996, pp. 42-50. Peng, C. R., T. Petersen, R. Clark: "The PowerPC rchitecture: 64-bit power 32-bil compatibility," Proc- COMPCON, San Francisco, CA, March 1995, pp. 300-307. Popescu, V., M. Schultz, J. Spracklen, G. Gibson, B. Lighmen "The Metaflow archi- tecture," IEEE Micro, 11,3, June 1991, pp. 10-23. Potter. T., M. Vaden, J. Young, N. Ullah: "Resolution data control-flow depen- dencies PowerPC 601," IEEE Micro. 14, 5, October 1994. pp. 18-29.SURVEY SUPERSCALAR PROCESSORS 4 Poursepanj, A., D. Ogden, B. Burgess, S. Gary, C. Dietz, D. Lee, S. Surya, M. Peters: "The P owerPC 603 Microprocessor: Performance analysis design tradeoffs," Proc. COMPCON, San Francisco, CA, Feb.-March 1994, pp. 31 6-323. Preston. R., et al., "Desig n 8-wide superscalar RISC microprocessor simulta- neous multithreading," Proc. ISSCC, San Francisco, CA, February 2002, p. 334. [Alpha 21464] Pugh, E., L Johnson, J. Palmer: IBM's 360 Early 370 Systems. Cambridge, MA: MIT Press, 1991. Rau, B., C. Glaeser, R. Picard: "Efficient code generation horizontal architectures: Compiler echniques architectural support," Proc. ISCA, Austin, TX, April 1982, pp. 131-139. [ESL machine, later Cydrome Cydra-5] Reilly, M.: "Designing Alpha processor," IEEE Computer, 32, 7, July 1999, pp. 27-34. Riseman, E., C. Foster: "The inhibition potential parallelism conditional jumps," IEEE Trans, Computers, C-21, 12, December 1972, pp. 1405-1411. Ryan, B.: "Ml challenges Pentium," Byte, 19, 1, January 1994a, pp. 83-87. [Cyrix 6x86] Ryan, B. : "NexGen Nx586 straddles RISC/CISC divide," Byte, 19,6, June 1994b, p. 76. Schorr, H: "Design principles high-performance sys tem," Proc. Symposium Computers Automata, New York, April 1971, pp. 165-192. [IBM ACS] Seznec, A., S. Felix, V. Krishnan, Y. Sazeides: "Design tradeoffs Alpha EV8 conditional branch predictor," Proc. ISCA, Anchorage, AK, May 2002, pp. 295-306. Shebanow, M., "SPARC64 V: high performance high reliability 64-bit SPARC processor," videotaped lecture, December 1999, http://rnurl.microsoft.com/Lecture- DeUiils.asp?455. Shen, J. P., A. Wolfe: "Superscalar processor design," Tutorial. ISCA, San Diego, CA, May 1993. Shippy, D.: "POWER2+ processor," Hot Chips VI, videotaped lecture, August 1994, http:// murl.iuicrosoft.com/LectureDetails.asp7495 . Shriver, B., B. Smith: Anatomy High-Performance Microprocessor: Systems Perspective. Los Alamitos, CA: IEEE Computer Society Press, 1998. [AMD K6-III] Simone, M., A. Essen, A. Ike, A. Krishnamoorthy, T. Maruyama. N. Patkar, M. Ramaswami, M. Shebanow, V. Thirumalaiswamy, D. Tovey: "Implementation tradeoffs using restricted data flow architecture high performance RISC micro- processor," Proc. ISCA, Santa Margherita Ligure, Italy. May 1995, pp. 151-162. [HaL SPARC64] Sites, R.: "Alpha AXP architecture," Communications ACM. 36. 2, February 1993, pp. 33-44. Smith, J. E.: "Decoupled acc ess/execute computer architectures," Proc. ISCA, Austin, TX, April 1982, pp. 112-119. Smith, J. E. : "Decoupled access/execute computer architectures." ACM Trans, Computer Systems, 2,4, November 1984, pp. 289-308. Smith, J. E., G. Dermer. B. Vanderwarn, S. Klinger, C. Rozewski, D. Fowler, K. Scidmore, J. Laudon: "The ZS-1 central processor," Proc. ASPLOS-I1. Palo Alto, CA, October 1987, pp. 199-204.448 MODERN PROCESSOR DESIGN Smith, J. E., T. Kaminski: "Varieties decoupled access/execute computer architec- tures," Proc. 20th Annual Allerton Conf. Communication, Control Computing, Monticello, IL, October 1982, pp. 577-586. Smith, J. E., S. Weiss: "PowerPC 601 Alpha 21064: tale two RISCs," IEEE Computer, 27,6, June 1994, pp. 46-58. Smith, J. E., S. Weiss, N. Pang: "A simulation study decoupled architecture comput- ers," IEEE Trans, Computers, C-35, 8, August 1986, pp. 692-702. Smith, M., M. Johnson, M. Horowitz: "Limits multiple instruction sue," Proc. ASPLOS-III, Boston, MA, April 1989, pp. 290-302. Soltis, F. Fortress Rochester: Inside Story IBM iSeries. Loveland, CO: 29th Street Press, 2001. Song, P., "HAL packs SPARC64 onto ingle chip," Microprocessor Report, 11, 16, December 8, 1997a, p. 1. Song, P., "IBM's Power3 replace P2SC," Microprocessor Report 11,15, November 17 1997b, pp. 23-27. Song, S., M, Denman, J. Chang: "The PowerPC 604 RISC microprocessor," IEEE Micro, 14, 5, October 1994, pp. 8-17. Special issue: "The IBM RISC System/6000 processor," IBM Journal Research Development, 34, 1, January 1990. Special issue: "Alpha AXP architecture systems," Digital Technical Journal. 4,4, 1992. Special sue: "Digital's Alpha chip project," Communications ACM 36 2 February 1993. Special issue: "The making PowerPC." Communications ACM, 37,6, June 1994. Special issue: "POWER2 PowerPC architecture implementation," IBM Journal Research Development, 38, 5, September 1994. Special issue: Hewlett-Packard Journal, 46,2, April 1995. [HP PA 7100LC] Special issue: Hewlett-Packard Journal, 48,4, August 1997. [HP PA 8000 PA 8200] Special issue: IBM Journal Research Development, 46, 1, 2002. [POWER4] Sporer, M., F. Moss, C. Mathias: "An introduction architecture Stellar graph- ics supercomputer," Proc. COMPCON, San Francisco, CA, 1988, pp. 464-467. [GS-1000] Sussenguth, E. : "Advanced Computing Sys tems," video-taped talk, Symposium Honor John Cocke, IBM T. J. Watson Research Center, Yorktown Heights, NY, June 18, 1990. Taylor, S., et al.: "Functional verification multiple-issue, out-of-order, superscalar Alpha microprocessor," Proc. Design Automation Conf., San Francisco, CA 1998 pp. 638-643. Tendler, J.. J. Dodson, J. Fields, H. Le, B. Sinharoy, "POWER4 system microarchitec- ture," IBM Journal Research Development, 46,1,2002, pp. 5-26. Thompson, T., B. Ryan: "PowerPC 620 soars," Byte, 19, 11, November 1994 pp. 113-120. Tjaden, G., M. Flynn: "Detection parallel execution independent instructions," IEEE Trans, Computers, C-19, 10, October 1970, pp. 889-895. Tremblay, M., D. Greenly, K. Normoyle: "The design microarchitecture UltraSPARC I," Proc. IEEE, 83, 12, December 1995, pp. 1653-1663.SURVEY SUPERSCALAR PROCESSORS 44 Tremblay, M., J. M. O'Connor: "UltraSPARC I: four-issue processor supporting multimedia," IEEE Micro, 16,2, April 1996, pp. 42-50. Turumella, B., et al.: "Design verification super-scalar RISC processor," Proc. Fault Tolerant Computing Symposium, Pasadena, CA, June 1995, pp. 472^*77. [HaL SPARC64] Ullah, N., M. Holle: "The MC88110 implementation precise exceptions super- scalar architecture," ACM Computer Architecture News, 21,1, March 1993, pp. 15-25. Undy, S., M. Bass, D. Hollenbeck, W. Kever, L. Thayer: "A low-cost graphics multimedia workstation chip set," IEEE Micro, 14,2, April 1994, pp. 10-22. [HP 7100LC] Vasseghi, N., K. Yeager, E. Sarto, M. Seddighnezhad: "200-MHz superscalar RISC microprocessor," IEEE Journal Solid-State Circuits, 31,11, November 1996, pp. 1675-1686. [MIPS RIOOOO] Vegesna, R.: "Pinnacle-1: next generation SPARC processor," Proc. COMPCON. San Francisco, CA, February 1992, pp. 152-156. [HyperSPARC] Wayner, P.: "SPARC strikes back," B yfe, 19, 11, November 1994, pp. 105-112. [UltraSPARC] Weiss, S., J. E. Smith: POWER PowerPC. San Francisco, CA: Morgan Kaufmann, 1994. White, S.: "POWER2: Architecture performance," Proc. COMPCON, San Francisco, CA, Feb.-March 1994, pp. 384-388. Wilcke, W.: "Architectural overview HaL systems," Proc. COMPCON, San Francisco, CA, March 1995, pp. 251-258. [SPARC64] Williams, T., N. Patkar, G. Shen: "SPARC64: 64-b 64-active-instruction out-of- order-execution MCM processor," IEEE Journal Solid-State Circuits, 30,11, November 1995, pp. 1215-1226. Wilson, J., S. Melvin, M. Shebanow, W.-M. Hwu, Y. Patt: "On tuning th e microarchi- tecture HPS implementation VAX," Proc. Micro-20, Colorado Springs, CO, December 1987, pp. 162-167. [This proceeding hard btain, paper also appears reduced size SIGMICRO Newsletter. 19, 3, Septembe r 1988, pp. 56-58.] Yeager, K.: "Tiie MIPS RIOOOO superscalar microprocessor," IEEE Micro, 16, 2, April 1996, pp. 28-40. HOMEWORK PROBLEMS P8.1 Although logic design techniques microarchitectural tradeoffs treated independent design decisions, explain typical pairing synthesized logic brainiac design style versus full custom logic speed-dem design style. P8.2 late 1950s, Stretch design ers placed limit 23 gate levels logic path. recently 1995, UltraSP ARC-I de- signed 20 gate levels per pipe stage. Yet many designers tried drastically reduce number. example, ACS target five gate levels logic per stage, Ultra-SPARC-HI uses equivalent eight gate levels per stage. Explain rationale desiring low ga te-level counts . (You may also want examine thePROCESSOR DESIGN lower level-count trend recenl Intel processors, discussed Hinton et al. [2001].) P8.3 Prepare table comparing approaches floating-point rithmetic exception handling found IBM designs: Stretch, ACS, RIOS, PowerPC 601, PowerPC 620, POWER4. P8.4 Consider Table 8-2. identify trends? so, suggest ratio- nale trend identify. P8.5 Explain market forces led demise Compaq/DEC Alpha. known blemishes Alpha instruction set make high-performance implementations particularly difficult inef- ficient? Alpha tradition full custom logic design labor resource intensive? P8.6 Compare Compaq/DEC Alpha IBM RIOS eliminated type complex instruction pai ring rules found Intel i960 CA. Explain importance caches HP processor de signs. assist cache used HP 7200 surprise natural development HP design philosophy? Find description load/store locality hints Itanium Processor Family. Compare Itanium approach approaches used HP 7200 MIPS R8000. P8.9 Consider IBM RIOS. (a) integer unit pipeline design pipeline used IBM 801. Explain benefit routing cache bypass path directly ALU stage writeback stage opposed bypass contained within cache stage (and thus ALU results required flow ALU/cache cache/writeback latches done simple five- six-stage scalar pipeline designs Chapter 2). cost approach terms integer register file design? (b) New physical registers assigned floating-point loads. types code segments sufficient? (c) Draw pipeline timing diagram showing floating-point load dependent floating-point nstruction fetched, dispatched, issued together without stalls sulting. P8.10 IBM RIOS provide three separate logic units, separate register set? legacy carried PowerPC instruction set. legacy help, hindrance, inconsequential high-issue-rate PowerPC implementations?SURVEY SUPERSCALAR PROCESSORS 4* P8.ll Identify market and/or design factors led long life span Intel i960 CA. P8.12 Intel P6 speed demon, brainiac. bo th? Explain answer. P8.13 Consider completion/retirement logic PowerPC designs. 601, 620 , POWER4 related? P8.14 Draw pipeline timing diagram illustrating SuperSPARC pro- cessor deals delayed branch delay slot instruction. P8.15 UltraSPARC-I provides in-order completion cost empty stages additional forwarding paths. (a) Give list pipe stage destinations forwarding paths must accompany empty integer pipeline stage. (b) List possible sources inputs multiplexer fronts one leg ALU integer execution pipe stage. (Note: empty integer stages.) (c) Describe number forwarding paths reduced UltraSPARC-Ill, even pipeline stages.Gabriel H. Loh Advanced Instruction Flow Techniques CHAPTER OUTLINE 9.1 Introduction 9.2 Static Branch Prediction Techniques 9.3 Dynamic Branch Prediction Techniques 9.4 Hybrid Branch Predictors 9.5 Instruction Flow Issues Techniques 9.6 Summary References Homework Problems 9.1 Introduction Chapter 5, stated instruction flow, proce ssing branches, provides upper bound throughput su bsequent stages . particular, conditional branches programs serious bott leneck improving rate instruction flo w and, hence, performance processor. condi- tional branch resolved pipelined processor, unknown instructions follow branch. increase number instructions execute parallel, modern processors make branch prediction speculatively execute instructions predicted path program control f low. branch dis- covered later mis predicted, actions taken recover state processor point mispredicted branch, execution resumed along correct path. penalty associated mis predicted branches modern pipelined pro- cessors great impact performance. pe rformance penalty creased 4:454 MODERN PROCESSOR DESIGN pipelines deepen and" number outstanding instructions increases. example, AMD Athlon processor 10 stages integer pipeline [Meyer, 1998], Intel NetBurst microarchitecture used Pentium 4 processor "hyper-pipelined" 20-stage branch misprediction penalty [Hinton et al., 2001]. Several studies ha suggested processor pipeline depth may con- tinue grow 30 50 stages [Hartstein Puzak, 2002; Hrishikesh et al., 2002]. Wide-issue su perscalar processors exacerbate problem creat- ing greater demand instructions e xecute. Despite huge body existing research branch predictor design, microarchitecture trends toward deeper wider designs continue create demand accurate branch pre- diction algorithms. Processing condi tional branches two major components; predicting branch irection predicting branch target. Sections 9.2 9.4. bulk chapter, focus former problem predicting whether condi- tional branch taken taken. Section 9.5 discusses problem branch target prediction issues related effective inst ruction delivery. past two three decades, incredible body pub- lished research problem predicting conditional branches fetching instructions. goal chapter take papers distill infor- mation key ideas concepts. Absolute comparisons whether one branch prediction algorithm accurate another difficult make since comparisons depend large number assumptions instruction set architecture, die area clock frequency limitations, choice applications. text makes note techniques implemented com- mercial processors, nec essarily imply algorithms inherendy better alter natives covered chapter. chapter surveys wide breadth techniques aim making reader aware design issues known methods dealing instruction flow. predictors described chapter organized make predictions. Section 9.2 covers static branch predictors, is, predictors make use run-time information branch behavior. Section 9.3 explains wide v ariety dynamic branch prediction algorithms, is, predic- tors monitor branch behavior program running make future predictions based th ese observations. Section 9.4 describes hybrid branch predictors combine strengths multiple si mpler predictors form better overall predictor. 9.2 Static Branch Prediction Techniques Static branch prediction al gorithms tend simple definition incorporate fee dback run-time environment. characteristic strength weakness static prediction algorithms. paying atten- tion dynamic run-t ime behavior program, branch prediction inca- pable adapting changes branch outcome patterns. patterns may vary based input set program different phases program's execution.ADVANCED INSTRUCTION FLOW TECHNIQUES 4 advantage static branch prediction echniques simple implement, r equire little hardware resources. Static branch prediction algorithms less interest context future-generation, large transistor budget, large-scale integration (VLSI) processors additional area effective dynamic branch predictors afforded. Nevertheless, static branch predictors may still used components complex hybrid branch predictors simpler f allback predictor ther prediction information available. Profile-based static prediction achieve better performance simpler rule- based algorithms. key assumption underlying profile-based approaches actual run-time behavior program approximated different runs program different data sets. addition branch outcome statistics sample executions, profile-based al gorithms may also take advantage information available compile time su ch high-level structure prog ram. main disadvantage pro file-based techniques profiling must part compilation phase program, existing programs cannot take advantage benefits without recompiled. branch behavior statistics collected training runs representative branch behavior actual run, profile-based predictions may provide much benefit. section continues brief survey rule-based static branch prediction algorith ms presents overview profile-based static branch prediction. 9.2.1 Single-Direction Prediction simplest branch prediction strategy predict direction branches always go direction (always taken always taken). Older pipelined processors, Intel i486 [Intel Corporation, 1997], used always-not-taken prediction algorithm. trivial strategy simplifies task fetching instructions next instruction fetch branch always next sequential instruction static order program. Apart cache misses branch mispredictions, instructions fetched uninterrupted stream. Unfortunately, branches often taken taken. integer benchmarks, branches taken approximately 60% time [Uht, 1997]. opposite strategy always predict branch taken. Although usually chieves higher prediction accuracy rate always- not-taken strategy, hardware complex. problem branch target address generally unavailable time branch prediction made. One solution simply stall front end pipeline branch target computed. wastes processing slots pipeline (i.e., causes pipeline bubbles) leads reduced performance. branch instruction spec- ifies target PC-relative fashion, destination address may computed little extra cycle delay. case early MIPS R-series pipelines [Kane Heinrich, 1992]. attempt recover lost pro- cessing cycles due pipelin e bubbles, branch delay slot branch156 MODERN PROCESSOR DESIGN instruction architected ISA. is, instruction immediately fol- lowing branch instruction always executed regardless outcome branch. theory, branch delay slots filled useful instructions, although studies shown compilers cannot effectively make use available delay slots [McFarlin g Hennessy, 1986]. Faster cycle times may introduce pipeline stages branch target calculation completed, thus increasing number wasted cycles. 9.2.2 Backwards Taken/Forwards Not-Taken variation single-direction static prediction approach backwards taken/forwards not-taken (BTFNT) strategy. backwards branch branch instruction target lower address (i.e., one comes earlier program). ra tionale behind heuristic majority backwards branches loop branches, since loops usually iterate many times exit- ing, branches likely taken. approach require modifications ISA since sign target displacement already encoded branch instruction. Many processors used prediction strat- egy; example, Intel Pentium 4 processor uses BTFNT approach backup trategy dynamic predictor unable provide prediction [Intel Corporation, 2003]. 9.2.3 Ball/Larus Heuristics instruction set architectures provide compiler interface branch hints made. hints encoded branch instructions, implementation ISA may choose use hints not. compiler make use branch hints inserting bel ieves likely outcomes branches based high-level information struc ture program. kind static prediction called program-based prediction. branches programs almost always go direction, knowing direction may require high-level standing program- ming language application itself. example, consider following code: void * p = malloc (n umBytes); (p == NULL) errorHandlingFunctiori( ) ; Except exceptional conditions, call malloc w l l return valid pointer, following if-statement's condition false. Predicting conditional branch corresponds if-statement static prediction result perfect prediction rates (for practical purposes). Ball Larus [1993] introduced set heuristics based program struc- ture statically predict conditional branches. rules listed Table 9.1. heuristics make use branch opcodes, operands branch instructions, attributes instruction blocks succeed branch instructions attempt make predictions based knowledge common programming— XAMr E T_JADVANCED INSTRUCTION FLOW TECHNIQUES 4! Table 9.1 Ball Larus's static branch prediction rules Heuristic Name Descr iption Loop branch branch target back head loop, predict taken. Pointer branch compares pointer NULL, two pointers compared, predict direction corresponds pointer NULL two pointers equal. Opcode branch esting integer less zero, less equal zero, equal constant, predict direction corresponds test evaluating false. Guard operand branch instruction register gets used redefined successor block, predict branch goes successor block. Loop exit branch occurs inside loop, neither targets loop head, predict branch go successor loop exit. Loop header Predict successor block brancn loop header loop preheader taken. Call successor block contains subroutine call, predict branch goes successor block. Store successor block con tains store instruction, predict branch go su ccessor b lock. Return successor block contains return subroutine instruc- tion, predict branch go successor block. idioms. situations, one heuristic may applicable. situations, ordering heuristics, first rule applicable used. Ball Larus evaluated permutations rules decide best ordering. rules capture int uition test exceptional condi- tions rarely true (e.g., pointer opcode rules), rules based assumptions common control flow patterns (the loop rules call/return rules). 9.2.4 Profiling Profile-based static branch prediction involves e xecuting instrumented version program sample input data, collecting statistics, feeding back collected information compiler. compiler makes use p rofile infor- mation ake static branch predictions inserted final program binary branch hints. One simple approach run instrumented binary one sample data sets determine frequency taken branches static branch458 MODERN PROCESSOR DESIGN instruction program. one data set used, measured fre- quencies weighted number times static branch executed. compiler inserts branch hints corresponding frequently observed branch directions sample executions. profiling run, branch observed taken 50% time, compiler would set branch hint bit predict-taken. Fisher Freudenberger [1992], experi- ment performed, found benchmarks, diffe rent runs program successful predicting future runs different data sets. cases, success varied depending representative sample data sets were. advantage profile-based prediction techniques static branch prediction algorith ms simple implement hard- ware. One disadvantage pro file-based prediction predictions made, forever "set stone" program binary. input set causes branching behaviors different training sets, performance suf- fer. Additionally, instruction set architecture must provide interface programmer compiler insert branch hints. Except always- taken always-not-taken approaches, rule-based profile-based branch prediction shortcomin g branch instruction must fetched instruction cache able read th e prediction embed- ded branch hint. Modern processors use multicycle pipelined instruction caches, therefore prediction fo r next instruction must available sev- eral cycles current instruction fetched. following section, dynamic branch prediction algorithms make use address current branch information immediately available. 9.3 Dynamic Branch Prediction Techniques Although static branch prediction techniques achieve conditional branch pre- diction rates 70% 80% range [Calder et al., 1997], profiling infor- mation representative actual run-time behavior, prediction accuracy suffer greatly. Dynamic branch prediction al gorithms take advantage run-time nformation available processor, react changing branch patterns. Dynami c branch predictors typically achieve branch prediction rates range 80% 95% (for exam ple, see McFarling [1993] Yeh Patt [1992]). branches static prediction approaches cannot handle, branch behavior still fundamentally predictable. Consider branch always taken first half program, always taken second half program. Profiling reveal branch taken 50% time, static prediction result 50% prediction accuracy. hand, simply predict branch go direction last time encountered branch, ca n achieve nearly perfect prediction, single mis prediction halfway point program branch chan ges directions. Another situation predictable branche can- determined compile time branch's dire ction depends program's input. example, program performs matrix computationsADVANCED INSTRUCTION FLOW TECHNIQUES ' may different lgorithms optimized different sized matrices. Throughout program, may branches check size matrix branch appropriate optimized code. given execution program, matrix size constant, branches direction entire execution. observing run-time behavior, dynamic branch pre- dictor could easily pr edict branches. ther hand, compiler idea size matrices incapable making much blind guess. Dynamic branch predictors may r equire significant amount chip area implement, espec ially complex algorithms used. small proces- sors, older-generation CPUs processors targeted embedded systems, additional area prediction structure may simply ex pensive. larger, future-generation, wide-issue superscalar processors, accurate condi- tional branch prediction critical. Furthermore, processors much larger chip areas, considerable resources may dedi cated imple- mentation sophisticated dynamic branch predictors. additional benefit dynami c branch prediction performance enhancements realized without p rofiling applications one wishes run, recompilation needed existing binary executables benefit. section describes many dynamic branch prediction algorithms published. Many prediction lgorithms important own, even implemented commercial processors. Section 9.4, also explore ways composing one pre- dictors powerful hybrid branch predictors. section divided three parts based characteristics prediction al gorithms. Section 9.3.1 covers sev eral fundamental prediction schemes basis many sophisticated algorithms. Sectio n 9.3.2 describes predictors address branch aliasing problem. Section 9.3.3 covers pre- diction schemes make use wider variety information making predictions. 9.3.1 Basic Algorithms dynamic branch predictors roots one basic algo- rithms described here. 9.3.1.1 Smith's Algorithm main idea behind majority dynamic branch predictors tha time processor discovers th e true outcome branch (whether taken taken), makes note form context next time enco unters situtation, make predic- tion. analogy branch prediction pr oblem navigating car get one p lace another forks road. driver wants keep driving fast can, time encounters fork, guess direction keep going. time, "copilot" (who happens slow map reading) trying keep up. realizes made wrong turn, notifies driver backtrack resume along correct route.160 MODERN PROCESSOR DESIGN [BranchjjddressJ2™ i-bit counters Updated counter value Saturating counter increment/decrement Most-significant bit- Branch prediction Branch outcome Figure 9.1 Smith Predictor 2m-entry Table Saturating fc-bit Counters. two friends frequently drive area, driver better blindly guessing fork road. might notice always end making right turn th e intersection pizza shop always make left supermarket. landmarks form context driver's predictions. similar fashion, dynamic branch predictors make note context (in form branch history), make predictions based information. Smith's algorithm [1981] one earliest proposed dy namic branch direction prediction algorithms, one simplest. predictor consists table records branch whether previous insta nces taken taken. analogous driver keeping track head cross treets intersection remembering went left right. cross streets correspond branch addresses, left/right decisions corre- spond taken/not-taken branch outcomes. pred ictor tracks w hether branch mosdy ta ken mode mostly not-taken mode, name bimodal predictor also commonly used Smith predictor. Smith predictor consists table 2™ counters, counter tracks past branch directions. Since 2™ entries, branch address [program counter (PC)] hashed bits.' counter able width k bits. most-significant bit counter used branch direc- tion prediction. most-significant bit one, branch predicted taken; significant bit z ero, branch predicted not- taken. Figure 9. 1 illustrates hardware Smith's algorithm. notation SmithK means Smith's algorithm k = K. 'in 1981 paper. Smith proposed exclusive-OR hashing function although modern implementa- tions use simple (PC mod 2m) hashing function requires logic implement. Typically, least-significant bits ignored due fact ISA instruction word sizes pow- ers two, lower bits always zero.ADVANCED INSTRUCTION FLOW TECHNIQUES 461 branch resolved true direction known, counter updated depending branch outcome. branch taken, counter incre- mented current value less maximum p ossible. instance, fc-bit counter saturate 2* - 1. branch taken, counter decremented current value greater zero.2 simple finite state machine also called saturating k-bit counter, up-down counter. counter higher value corresponding branch often taken last several encounters branch. counter tend toward lower values recent branches mostly taken. case Smith's algorithm k = 1 simply keeps track last outcome branch mapped counter. branches predominantly biased toward one direction. branch end loop usually taken, except case loop exit. one exceptional case called anomalous decision. outcomes several recent branches map th e counter used k > 1. using histories several recent branches, counter thrown single anomalous decision. additional bits add hysteresis predictor's state. Smith als calls inertia. Returning analogy driver, may case almost always makes left turn parti cular intersection, recently ended make right turn instead friend go hospital due emergency. driver remembered recent trip, would predict make right turn next time intersection. hand, remembered last everal trips, would realize often ended making left turn. Using additional bits counter allows predictor effectively remember history. 2-bit saturating counter (2bC) used many branch prediction algorithms. four possible states: 00, 01, 10, 11. States 00 01, called st rongly not- taken (SN) weakly not-taken (WN), respectively, provide prediction not-taken. States 10 11, called weakly taken (WT) trongly taken (ST), respectively, pro- vide taken-branch pred iction. reason states 00 11 called "strong" outcome must occurred multiple times reach state. Figure 9.2 illustrates short seq uence branches predictions made Smith's algorithm k = 1 (Smith,) k = 2 (Smith2). Prior anomalous decision, versions Smith's algorithm predict branches accu rately. anomalous decision (branch C), predictors mispredict. following branch D, Smith, mispredicts gain remembers recent branch predicts direction. occurs despite fact vast majority prior branches taken. hand, Smith2 makes cor- rect decision prediction influenced several recent branches instead single recent branch. suc h anomalous decisions. Smith, makes two mispredictions Smith2 errs once. 2The original paper presented counter using values -2k"1 2*"' — 1 two's c omplement nota- tion. compl ement most-significant bit used branch direction prediction. formu- lation presented used recent literature.462 MODERN PROCESSOR DESIGN Branch B CBranch Direction oState Prediction~ Smith} state Prediction 1 l 1 (misprediction) 0 (misprediction) 1 111 11 II 10 11 II1 1 1 (misprediction) 1 1 1 Figure 9.2 Comparison Smith, Smith2 Predictor Sequence Branches wit h Single Anomalous Decision. Practically e dynamic branch prediction alg orithm published since Smith's seminal paper uses saturating counters. tracking branch directions, 2-bit counters provide better prediction accuracies 1-bit counters due additional hysteresis. Adding third bit improves performance small increment. many branch pre dictor designs, incremental improvement worth 50% increase area adding additional bit every 2-bit counter. 9.3.1.2 Two-Level Prediction Tables. Yeh Patt [1991; 1992; 1993] Pan et al. [1992] proposed variations branch prediction algor ithms called two-level adaptive branch prediction correlation branch prediction, r espec- tively. two-level predictor employs two separate levels branch history information make branch prediction. Using car na vigation analogy, Smith predictor parallel driving remember decision made intersection. car-navigation equivalent tw o-level predictor driver remember exact sequence turns de arriving current intersection. example, drive apartment bank, driver makes three turns: left turn, another left, r ight. drive mall bank, also makes three turns, right, left, another right. finds bank remembers recently wen right, left, right, could guess came mall heading home make next routing decision accordingly. global-history two-level predictor u ses history recent branch outcomes. outcomes stored branch history register (BHR). BHR shift register outcome branch shifted one end,ADVANCED INSTRUCTION FLOW TECHNIQUES 463 oldest outcome shifted end discarded. branch outcomes represented zeros ones, correspond not-taken taken, respectively. Therefore, Ai-bit branch history register records h recent branch outcomes. branch history first level glo bal-history two-level predictor. second level global-history two-level predictor table saturat- ing 2-bit counters (2bCs). able called pattern history table (PHT). PHT indexed concatenation hash branch address con- tents BHR. analogous driver using combination inter- section well mo st recent turn decisions making prediction. counter indexed PHT entry provides branch prediction fash- ion Smith predictor (prediction determined most-significant bit counter). U pdates counter also Smith predictor counters: saturating increment aken branch, saturating decrement not-taken branch. Figure 9.3 shows hardware organization sample global-history two- level predictor. predictor uses outcomes four recent branch instructions 2 bits th e branch address form index 64-entry PHT. h bits branch history bits branch address, PHT 2*+™ entries. using bits branch address (where less total width PC), branch address must hashed bits, similar Smith predictor. Note example Figure 9.3, means branch address ends 01 hare entries branch depicted figure. Using car navigation analogy again, similar driver remem- bering Elm Street simply "Elm," may cause co nfusion encounters Elm Road, Elm Lane, Elm Boulevard. Note problem unique two-level pr edictor, affect mith predictor well. Since PC = 01011010010101 010110  BHR 0110PHT 000000 000001 000010 000011 010100 010101 010110 010111 111110 1111111 0 1 Branch prediction Figure 9.3 Global-History Two-Level Predictor 4-bit B ranch History Register.164 MODERN PROCESSOR DESIGN Smith predictor use branch history, index bits branch address, reduces thi branch conflict problem. size global-history two-level predictor depends total avail- able hardware budget. 4K-byte budget, PHT would 16,384 entries (4,096 bytes times 4 two-bi counters per byte). general, X K-byte budget, PHT contain 4X counters. tradeoff number branch address bits used length BHR, since sum lengths must equal number index bits. Using branch address bits reduces branch conflict pr oblem, whereas using branch history allows predic- tor correlate complex branch history patterns . optimal balance depends many factors th e compiler arranges code , program run, instruction set architecture, input set program. intuition behind using global branch history behavior branch may linked correlated different earlier branch. example, branches may test conditions involve variable. Another common situation one branch may guard instruction modifies variable second branch tests. Figure 9.4 shows code segment if-statement (branch A) determines whether v ariable x gets assigned different value. Later program, another if-statement (branch C) tests value x. A's condition true, x gets assigned value 3, C evaluate x < 0 false. hand, A's condition false, x retains original value 0, C evaluate x 5 0 true. behavior (outcome) branch corresponding if-statement C strongly correlated outcome if-statement A. branch predictor tracks outcome if-statement could potentiall achieve perfect prediction branch if- statement C. Notice hat could intervening branch B affect outcome C (that is, C correlated B). irrelevant branches x = 0 ; (someCondition) { /* branch */ x = 3 ; ) (someOtherCondition) { /* branch B */ += 1 9 ; ) (x < = 0 ) doSomething(/* branch C */ } Figure 9.4 Sample Code Segment Correlated B ranches.ADVANCED INSTRUCTION FLOW TECHNIQUES 465 increase training time global history predictors predictor must learn ignore irrelevant history bits. Another variation two-level predictor local-history two-lev el pre- dictor. Whereas global history tracks outcomes last several branches encountered, local history tr acks outcomes last several encounters current branch. Using car navigation analogy again, driver might make right turn particular intersection week way work, weekends makes left turns exact intersection go downtown. turns made get intersection (i.e., global history) might regardless day week. hand, remem- bers last several days went R, R, R, L, today probably Sunday wou ld predict making left turn correct decision. remember driving decision history intersection require driver remember much inform ation simple global tory, patterns easier predict using local history. implement local-history branch pr edictor, single global BHR replaced one BHR per branch. collection BHRs form branch history table (BHT). global BHR really degenerate case BHT single entry. branch address used select one entries BHT, provides local history. contents selected BHR com bined PC fashion global-history two-level predictor index PHT. most-significant bit counter provides branch prediction, update counter als Smith predictor. update history, recent branch outcome shifted sele cted entry BHT. Figure 9.5 shows hardware organization example local-history two- level predictor. BHT eight entries indexed three least-significant bits branch address. PHT example 128 entries, wbichuses PC = 01011010010101 BHT 000001 010 100 - 101 110 111110eioi no-rm ooooooo ooooooi 0000010 0000011 0101100 0101101 - 0101110 0101111 0111110 01111110 1 - 0 Branch prediction Figure 9 .5 Local-History Two-Level Predictor Eight-Entry BHT 3-bit History Length.166 MODERN PROCESSOR DESIGN 7-bit index (log2I28 = 7). Since history length 3 bits long, 4 index bits Come branch ad dress. bits concaten ated together select counter PHT provides final prediction. tradeoffs sizing local-history two-level predictor complex case global-history predictor. addition balancing th e number history address bits PH index, also tradeoff number bits dedicated BHT number bits dedicated PHT. BHT, also balance number entries width entry (i.e., history length). local-history two-level predictor L- entry BHT /i-bit history uses bits branch address PHT index requires total size Lh + 2*+""+I bits. Lh bits BHT, PHT 2h*" entries, 2 bits wide (the +1 exponent). Figure 9.6(a) shows example predictor 8-entry BHT, 4-bit history length, 64-entry PHT (only first 16 entries shown). last four out- comes branch address 0xC084 T, N, T, N. select BHR, hash branch address three bits using 3 least-significant bits address (100 binary). Note th e selected branch history register's con- tents 1010, corresponds history TNTN. 64-entr PHT, size PHT index 6 bits, 4 branch history. PC = 0XC084 = 1100000010000100 (i n binary) BHT 000 001010 011  100 101 110 111PHT 001010 —, 1010000000 000001000010 000011 000100 O00101000110000111001O00001001001010001011 001100 0011010011100011110 1 1 0 BHT 000 001010 100 101 1101110101PHT (a) (b) Figure 9.6 Example Lookup Two-Level Branch Predictor: (a) Making Prediction, (b) Predictor UpdateADVANCED INSTR UCTION FLOW ECHNIQUES 467 leaves 2 bits branch address. concatenation branch address bits branch history selects one counters, whose signifi- cant bit dicates taken-branch prediction. actual branch outcome computed execute stage pipeline, BHT PHT ne ed updated. Assuming actual outcome taken branch, 1 shifted BHR shown Figure 9.6(b). PHT updated per Smith predictor algorithm, 2-bit counter gets incremented 10 11 (in binary). Note next time branch 0xC084 encountered, BHR contains pattern 0101 selects dif- ferent entry PHT. Intel P6 microarchitecture uses local-history two-level predictor 4-bit history length (see Chapter 7). tracking behavior branch individually, predictor detect patterns local particular branch, like alternating pattern hown Figure 9.6. second example, consider loop-closing branch short iter- ation count exhibits pattern 1110111011101 ..., w h e r e g n 1 denotes taken branch, 0 denotes not-taken b ranch. tracking last several outcomes particular branch, PHT quickly learn pattern. Figure 9.7 shows 16-entry PHT entries corresponding predicting pattern (no branch address bits used PHT index). last four outcomes branch 1101, loop yet terminated next time branch ta ken. Every time proces sor encounters pat- tern 1101, following branch always taken. results incrementing corresponding saturating counter index 1101 every time pattern occurs. pattern occurred times, PHT predict taken counter indexed 1101 remember (by state ST) Loop closing branch's history 11101110111011101110 PHT 0000 0001 0010 0011 010001010110 1 »-0111 100010011010 1 - 1011 1100 1 »-1101 1 »- 1110 1111 Figure 9.7 Local History Predictor Examplel 1 1 1 1 468 MODERN PROCESSOR DESIGN Figure 9.8 Alternative Views PHT Organization: (a) Collection PHTs, (b) Single Monolithic PHT. following instance thi branch taken. last four outcomes 0111, entry indexed 0111 not-taken prediction stored (SN) state. PHT basically learns mapping form "what see h istory pattern X, outcome next branch usually Y." texts research papers view two-level predictors multi- ple PHTs. branch address hash selects one PHTs, branch history acts subindex chosen PHT. view tw o-level predic- tor shown Figure 9.8(a). monolithic PHT shown Figure 9.8(b) actu- ally equivalent. chapter uses monolithic view PHT reduces PHT design tradeoffs conceptually simpler problem deciding allocate bits index (i.e., many index bits come PC long history length?). Yeh Patt [1993] also introduced third variation utilizes BHT uses arbitrary hashi ng function divide branches different sets. group shares single BHR . Instead using least-significant bits branch address select BHR BHT, example set-partitioning functions use higher-order bits PC, divide based opcode. type history called per-set branch history, table called per-set branch history table (SBHT). Yeh Patt use letters G (for global), P (for per-address) (for per-set) denote different variations two-level branch prediction algorithm. choice nonhistory bits used PHT index provide severa l addi- tional variations. first option simply ignore th e PC use BHR index PHT. branches thus share entries PHT, isADVANCED INSTRUCTION FLOW TECHNIQUES ' called global pattern history table (gPHT), used example Figure 9.7. second alternative, already illustrated Figure 9.6, use lower bits PC create per-address pattern history table (pPHT). last variation apply othe r hashing function (analogous hashing func- tion per-set BHT) provide nonhistory index bits per-set pattern history table (sPHT). Yeh Patt use letters g, p, indicate three indexing varia- tions. Combined th e three branch history options (G, P, S), total nine variations two-level predictors using taxonomy. notation pre- sented Yeh Patt form jrAy, x G, P, S, g, p, s. Therefore, nine two-level predictors GAg, GAp, GAs, PAg, PAp, PAs, SAg, SAp, SAs. general, two-level predictors identify patterns branch out- comes associate prediction pattern. captures correlations complex branch patterns simpler Smit h predictors cannot track. 9.3.1.3 Index-Sharing Predictors. two-leve l algorithm requires branch predictor designer make tradeoff widt h BHR (the number history bits use) number branch addres bits used index PHT. fixed PHT size, employing larger number history bits reveals opponunities correlate distant branch es, comes cost using fewer branch address bits. example, consider car navi- gation analogy. Assume driver limited memory remember seq uence six letters. could choose remember first five letters street name one recent turn decision. allows distinguish many street names, little decision history information correlate against. Alternatively, could choose remember first two letters street name, recording four recent turn decisions. Thi provides dec ision history, may get confused .Broad Street Bridge Street. Note histor length long , frequently occurring history patterns map PHT sparse distribution. example, consider local history pattern used Figure 9.7. Since history length 4 bits long, 16 entries PHT. particular pattern used example, 4 16 entries ever accessed. indicates ndex formation two-level pred ictor intro duces inefficiencies. McFarling [1993] proposed variation global-history two-level predictor called gshare. gshare algor ithm atte mpts make better use index bits hashing BHR PC together select entry PHT. hashing function used bit-wise exclusive-OR operation. combination BHR PC tends contain mor e information due nonuniform distri- bution PC values branch histories. called index sharing. Figure 9.9 illustrates set PC branch history pairs resulting PHT indices used GAp gshare al gorithms. GAp al gorithm forced trade numbe r bits used BHR width PC bits used, information one two sources must left out. the470 MODERN PROCESSOR DESIGN GAp BHR 1101- PC 0U0- BHR 1001- PC 1010  1 1001- J ~1 1001- J0000 0001 001000110100 010101100111 10001001 101010111100 11011110 1111gshare BHR PC1101 0110 XOR 1011 BHR PC1001 1010 XOR 0011 Figure 9.9 Indexing Example Global-History Two-Level Predictor gshare Predictor Branch address Global BHR (xor) Figure 9.10 gshare Predictor.PHT Final prediction example, GAp al gorithm uses 2 bits PC 2 bits global his- tory. Notice even though overall PC histo ry bits different, using 2 bits causes two map entry 1001. hand,the exclusive-OR 4 bits branch address full 4 bits global history yields different distinct PHT indices. hardware gshare predictor shown Figure 9.10. circuit similar global history two-level predictor, except concatenation opera- tor PHT index replaced XOR operator. numbeT glo- bal history bits used h less number branch address bits used m, global history XORed upper h bits branch address bits. rea- son upper bits PC tend sparser lower-order bits.ADVANCED INSTRUCTION FLOW TECHNIQUES Evers et al. [1996] proposed variation gshare predictor uses per- address branch history table store local branch history. pshare algorithm local-history analog gshare algorithm. low-o rder bits branch address used ndex first-level BHT fashion PAg/ PAs/PAp two-level predictors. th e contents indexed BHR XORed branch address form PHT index. Index sharin g predictors com monly used modem branch predictors. example, IBM Power4 microprocessor's global-history predictor uses 11-bit global history (BHR) 16,3 84-entry PHT [Tendler et al., 2002]. Alpha 21264 also makes use global history pred ictor 12-bit global history 4096-entry PHT [Kessler, 1999]. amount available storage, com monly measured bytes state, often deciding factor many PHT entries use. recently steady increase clock frequencies, latency PHT access also becoming limiting factor size PHT. Powerf's 1 6,384-entry PHT requires 2K b ytes storage since entry I-bit (1/8 byte) counter. number bits history use limited PHT size may also depend target set applications. fre- quently executed programs exhibit behavior requires longer branch history capture, likely longer BHR employed. exact behav- ior branches depend compiler, instruction set, input program, thus making difficult choose optimal history length per- forms well across wide range applications. 93.1.4 Reasons Mispredictions. Branch mispredictions occur variety reasons. branches simply hard predict. mispredic- tions due fact realistic branch predictor limited size complexity. several cases branch fundamentally unpredictable. first time predictor encounters branch, past information branch behaves, best predictor could make random choice expect 50% prediction rate. predictors use branch histories, similar situation occurs time predicto r encounters new branch hi story pattern. predictor needs see particular branch (or branch history) times learns proper prediction corresponds branch (o r branch history). training period, unlikely predictor perform well. branch history length n, 2" possible branch history patterns, training time predictor increases history length. program enters new phase execution (for example, compiler going parsing type-checking), branch behaviors may change predictor must relearn new patterns. Another case branches unpredictable data involved program intrinsically random. example, program processes com- pressed data may many hard-to-predict branches well-compressed input data appear random. application areas may hard-to- predict branches include cryptography randomized al gorithms.172 MODERN PROCESSOR DESIGN physical constraints size branch predictors int roduce additional sources branch mi spredictions. example, branch p redictor 128- entry table counters, 129 distinct branches program, least one entry two different branches mapped it. one branches always taken always taken, interfere ot cause branch mispredictions. interference called negative interference. branches always taken (or always taken), would still interfere, additional mis predictions would generated; called neutral interference. Interference also called aliasing bot h branches aliases predictor entry. Aliasing occur even predictor entries branches. 128-entry table, let us assume hashing function remainder branch address divided 128 (i.e., index := address mod 128). may two branches entire program, addresses 131 259, branches still map predictor entry 3. called conflict aliasing. similar case car driving analogy driver gets confused Broad St. Bridge St. remembering first two letters happen same. branches predictable, p articular predictor may still mispredict branch predictor ri ght information. example, consider branch strongly correlated ninth-most recent branch. predictor uses 8-bit branch history, predictor able accurately make prediction. Similarly, branch strongly correlated previous local tory bit, difficult global history p redictor make right prediction. sophisticated prediction algori thms deal classes types mispredictions. Fo r capacity problems, solution increase size predictor structures. always possible due die area, latency, and/or power constraints. conflict aliasing, wide variety algorithms developed address problem, many described Section 9.3.2. Furthermore, many algorithms developed make use different types information (such global vs. local branch histories short vs. long histories), covered Section 9.3.3. 9.3.2 Interference-Reducing Predictors PHT used two-level gshare predictors direct-mapped, tagless structure. liasing occurs diffe rent address-history pairs PHT. PHT viewed cache-like structure, three-C's model cache misses [Hill, 1987; Sugumar Abraham, 1993] gives rise analogous model PHT aliasing [Michaud et al., 1997]. particu lar address-history pair "miss" PHT follo wing reasons: 1. Compulsory aliasing occurs first time address-history pair ever used index PHT. recourse compulsory aliasing initialize PHT counters way majority lookupsADVANCED STRUCTION FLOW TECHNIQUES 473 still yield accurate predictions. Fortunately, Michaud et al. show com- pulsory aliasing accounts small fraction branch prediction lookups (much less 1% IBS benchm arks [ Richard Uhlig et al., 1995]). 2. Capacity aliasing occurs size current working set address-history p airs greater capacity PHT. aliasing mitigated increasing PHT size. 3. Conflict aliasing occurs two different address-history pairs map PHT entry. Increasing PHT size often little effect reducing conflict aliasing. caches, asso ciativity ca n increased better replacement policy used reduce effects co nflicts. caches, standard solution conflict aliasing increase asso- ciativity cache. Even direct-mapped cache, address tags neces- sary determine whether cached item belongs req uested address. Branch predictors diffe rent tags required proper opera- tion. many cases, ways use available transistor budget deal conflict aliasing use associativity. example, nstead adding 2-bit tag every saturating 2-bit counter, size pr edictor could instead doubled. Sec tions 9.3.2.1 9.3.2.6 describe variety ways deal problem interference branch predictors. Th ese predictors global-hist ory predictors global-history predictors usually accurate local- history predictors, ideas equally applicable local-history predictors well. Note many algorithms often referred two-level branch predictors, since use first level branch history second level counters state provides final prediction. 9.3.2.1 Bi-Mode Predictor. Bi-Mode predictor uses multiple PHTs reduce effects aliasing [Lee et al., 1997]. Bi-Mode predictor consists two PHTs (PHT0 PHT,), indexed gshare fashion. indices used PHTs identical. separate choice predictor indexed lower- order bits branch address only. choice pre dictor table 2-bit counters (ident ical Smith2 predictor), most-significan bit indicates two PHTs use. manner, branches h ave strong taken bias placed one PHT branches not-taken bias separated PHT, thus reducing amount de structive interference. two PHTs identical sizes, although choice predictor may dif- ferent number entries. Figure 9.11 illustrates hardware Bi-Mode predictor. branch address global branch history hashed together form index PHTs. index used PHTs, corr esponding predictions read. Simultaneously, low-order bits branch address used index choice predictor table . prediction choice predictor drives select line multiplexer choose one two PHT banks.474 MODERN PROCESSOR DESIGN (^ranchjiddressj Global BHRi ; PHTn PHT,Choice predictor Final prediction Figure 9.11 Bi-Mode Predictor. rationale behind Bi-Mode predictor branches biased toward one direction other. choice predictor effectively remembers bias branch is. Branches strongly biased toward one direc- tion us e PHT. result even two branches map entry PHT, likely go direction. result opportunity negative interference bee n converted neutra l interference. PHT bank selected choice predictor always updated final branch outcome determined. PHT bank updated. choice predictor always updated branch outcome, except case choice predictor's direction opposite branch outcome, overall prediction selected PHT bank correct. update rules implement partial update policy. 932.2 Thegskewed Predictor. gskewed algorithm divides PHT three (or more) banks. bank indexed different hash address- history pair. results three lookups combined majority vote determine overall prediction: intuition hashing functions different, even two address-history pairs destructively alias PHT entry one bank, unlikely conflict two banks. hashing functions f0, /„ f2 presented Michaud et al. [1997] property if/o(*i)=/o (*2). /,(*,) */,(x2) f2(xx) *f2(x2) x,*x2. is, two addresses conflict one PHT, guaranteed conflict inADVANCED INSTRUCTION FLOW TECHNIQUES 4 two PHTs. three banks 2"-entry PHTs, definitions three hashing functions /o(x,y) = H(y)©rr'(x)ex f,0c,y) = H(y)©rT'(x)©y f2(x, y) = H-'(y)©H(x)©x(9.1) (9.2) (9.3) H(fc„, b3, b2, fc,) = (b„ © bn, *>„_,, ...,b3, b2), H"1 inverse H, x n bits long. gskewed algorithm, th e arguments x hashing functions n low-order bits branch address, n recent global branch outcomes, respectively. amount conflict aliasing result hashing function used map PC-history pair PHT index. Although gshare exclusive-OR hash remove certain types interfere nce, also introduce interference well. Two different PC values two different histories still result index. example, PC-history pairs (PC = 0110) © (history = 1100) = 1010, 1101 ffi 0111 = 1010 map index. hardware gskewed predictor illustrated Figure 9.12. branch address global branch history hashed separately three hashing functions (9.1) (9.3). three resulting indices used address different PHT bank. direction bits 2-bit counters PHTs ar e combined majorit function make final prediction. PHT„ PHT, PHT2 Figure 9.12 gskewed PredictorFinal prediction476 MODERN PROCESSOR DESIGN Figure 9.13 shows gskewed predictor example two sets PC-history pairs corresponding two different branches. example, one PC-history pair corresponds strongly aken branch, whereas ot PC-history pair corre- sponds strongly not-taken branch. two branches map entry PHT, causes destructive int erference. hashing functions (9.1) (9.3) guarantee conflict one PHT means conflicts two branches two PHTs. result, majority function effectively mask one disagreeing vote still provide correct prediction. Two different update policies gskewed algorithm total update partial update. tota l update policy treats PHT banks identically updates banks branch outcome. partial update policy update bank part icular bank mispredicted, overall prediction correct. partial update policy improves overall prediction rate gskewed algo- rithm. one three banks mispredicts, updated, thus allowing contribute correct prediction address-history pair. choice branch history length involves tradeoff capacity aliasing conflicts. Shorter branch histories tend reduce amount possible aliasing fewer possible address-branch history pairs. hand, longer histories tend provide better branch prediction accuracy correlation information ava ilable. modification th e gskewed predictor enhanced gskewed predictor. variation, PHT banks 1 2 indexed usual fashion using branch address, global history, hashing functions /, and/2, PHT bank 0 indexed lower bits program counter. rationale behind approach follows. W hen history length becomes larger, number branches one inst ance branch address-branch history pair, another identical instance tends incr ease. increases theADVANCED INSTRUCTION FLOW TECHNIQUES probability aliasing occur meantime corrupt one banks. Since first ban k enhanced gskewed predi ctor addressed branch address only, distance successive accesses shorter, likelihood unrelated branch aliases entry PHT0 decreased. variant enhanced gskewed algorithm selected used Alpha EV8 microprocessor [Seznec et al., 2002], although EV8 project eventually cancelled late phase development. 9323 Agree Predictor. gskewed algorithm attempts reduce effects conflict aliasing storing branch prediction multiple locations. agree predictor reduces destructive aliasing interference reinterpreting PHT counters direction agreement bit [Spangle et al., 1997]. two address-history pairs map PHT entry, two types interference result. first destructive negative interference. Destructive interference occurs wh en counter updates one address-history pair corrupt stored state different address-history pair, thus causing mispre- dictions. address-history pairs result destructive interference try- ing update counter opposite directions; is, one address-history pair consistently incrementing counter, pair attempts decrement counter. type interference neutral interference PHT entry correctly predicts branch outcomes address-history pairs. Regardless actual direction history-address pairs, branches tend heavily biased one direction other. words, infinite- entry PHT interference, majority counters either strongly taken (ST) strongly not-taken (SN) states. agree predictor stores likely predicted direction separate biasing bit. biasing bit may stored branch target buffer (see Se ction 9.5.1.1) line corresponding branch, ther separate hardware structure. biasing bit may initialized outcome first instance branch, may branch hint inserted compiler. Instead predicting th e branch direction, PHT counter w predicts whether branch go direction corresponding biasing bit. Another interpretation PHT counter predicts whether branch outcome agree biasing bit. Figure 9.14 illustrates hardware gree predictor. Like gshare algorithm, branc h address global branch history combined index PHT. time, branch address als used look biasing bit most-significant bit indexed PHT counter one (predict agreement biasing bit), final branch prediction equal b iasing bit. significant bit zero (predict disagreement biasing bit), complement biasing bit used final pr ediction. number b iasing bits stored generally different number PHT entries. branch inst ruction resolved, corresponding PHT counter updated based whether actual branch outcome agreed biasing bit. fashion, two different address-history pairs may conflict map PHT entry, corres ponding biasing bits set accurately, predictions not478 MODERN PROCESSOR DESIGN Biasing bits Branch address 1 = agree bias bit 0 = disagree Figure 9.14 Agree Predictor. affected. agree prediction mechanism used HP PA-RISC 8700 proces- sor [Hewlett Packard Corporation, 2000]. biasing bits determined com- bination compiler analysis source code profile-based optimization. 9.3.2.4 YAGS Predictor. Bi-Mode predictor study demonstrated separation branches two separate mostly taken mostly not-taken sub- streams beneficial. yet another global scheme (YAGS) approach similar Bi-Mode pr edictor, except two PHTs record instances agree direction bias [Eden Mudge, 1998]. PHTs replaced -cache NT-cache. cache entry contains 2-bit counter small tag (6 8 bits) record branch instances agree over- bias. branch entry cache, selection counter used make prediction. hardware llustrated Figure 9.15. make branch prediction YAGS predictor, branch address indexes choice PHT (analogous choice predictor Bi-Mode predictor). 2-bit counter choice PHT indicates bias branch used select one two caches. choice PHT counter indicates taken, NT- Cache consulted. NT-Cache indexed hash branch address global history, stored tag compared least-significant bits theADVANCED INSTRUCTION FLOW TECHNIQUES Branch address | Global BHR <*OR^ T-cache Partial tag 2bCChoice PHT NT-cache Partial tag 2bC T/NT-cache hit?^T7 53—G - Final prediction Figure 9.15 YAGS Predictor. branch address. tag match occurs, prediction made counter NT-cache, otherwis e prediction made choice PHT (predict taken). actions taken choice PHT pr ediction not-taken analogous. conceptual level, idea behind YAGS predictor choice PHT provides prediction "rule," T/NT-caches record "exceptions rule," exist. components Figure 9.15 simply detecting exception (i.e., hit T/OT-caches) selecting appropriate prediction. branch outcome known, choice PHT updated partial update policy used Bi-Mode choice predictor. NT-cache updated used, choice predictor indicated branch taken, actual outcome not-taken. Symmetric rules apply T-cache. Bi-Mode scheme, second-level PHTs must store directions branches, even though branches agree choice predictor. Bi-Mode pr edictor reduces aliasing dividing branches two480 MODERN PROCESSOR DESIGN substreams. insight YAGS predictor PHT coun ter values second-level PHTs Bi-Mode predi ctor mostly redundant informa- tion conveyed choice predictor, allocates hardware resources make note cases prediction match bias. YAGS study, two-way- associativity also added T-cache NT-cache, required addition 1 bit maintain LRU state. tags already stored reused purposes associativity, extra comparator simple logic need added. replacement pol- icy LRU, exception counter entry T-cache indi- cates not-taken, evicted first information already captured choice PHT. reverse rule applies entries NT-cache. addition two-way ass ociativity slightly increases prediction accuracy, although adds additional hardware complexity well. 93.2.5 Branch Filtering. Branches tend highly biased toward one direc- tion other, Bi-Mode algorithm works well sorts branches based bias reduces negative interference. di fferent approach called branch filtering attempts remove highly biased branches PHT, thus reducing total number branches stored PHT helps alleviate ca pacity conflict aliasing [Change et al., 1996]. idea keep track many times branch gone direction. branch taken direction th certain number times, "filtered" longer make updates PHT. Figure 9.16 shows organization branch counting table PHT, along logic detecting whether branch filtered. Although figure shows branch filtering gshare predictor, branch filtering tech- nique could applie prediction lgorithms well. entry branch counting table tracks branch direction, many consecutive times branch taken direction. direction changes, th e new direc- tion stored c ount reset. counter incremented max- imum value, c orresponding branch deemed highly biased. point, branch longer update PHT, branch count table provides prediction. point direction changes branch, counter reset PHT takes making predictions. Branch filtering effectively removes branches corresponding error-checking code, almost-never-taken malloc checking branch example Section 9.2.3, ot dynamically constant branches. Although branch counting table b een described separate entity, counter direc- tion bit would actually part branch target bu ffer, described Section 9.5.1.1. 9.3.2.6 Selective Branch Inversion. previous several branch prediction schemes aim provide better branch prediction rates reducing amount interference PHT (interference avoidance). Another approach, selective branch inversion (SBI), attacks interference problem differently using interferenceADVANCED INSTRUCTION FLOW TECHNIQUES 4t ] Branch address Branch count ing table Direction counter \ ( ) gshare (xor)- | Global B HR~| V Figure 9.16 Branch Filtering Applied gshare Predictor. Branch pre diction Branch predictor Initial predictionConfidence estimator 1 Confidence prediction < Inversion threshold? 1 Invert prediction? Final pr ediction Figure 9.17 Selective Bra nch Inversion Applied Generic Branch Predictor. correction [Argon et al., 2001; Marine et al., 1999]. idea estimate confidence branch prediction; confidence lower threshold, direction branch prediction inverted. See Section 9.5.2 expla- nation predicting branch confidence. generic SBI predictor hown Figure 9.17. Note SBI technique applied existing branch predic- tion scheme. SBI gskewed SBI Bi-Mode predictor achieves better prediction rates performing interference avoidance interference correction.482 MODERN PROCESSOR DESIGN 93.3 Predicting Alternative Contexts Correlating branch predictors basically simple pattern-recognition mechanisms. predictors learn mappings context branch outcome. is, every time branch outcome becomes known, predictor makes note current context. fut ure, context arise, predictor make pre- diction corresponds previous time(s) encount ered context. far, predictors described chapter used combination branch address branch outcome history context making predictions. many design decisions go c hoosing context branch predictor. predictor use global history loc al history? many recent branch outcomes used? many bits branch address included? information combined form final context? general, context predictor uses, oppo rtunities detecting correlations. Using example given Section 9.3.1.4, branch cor- related branch outcome nine branches ago ac curately predicted predictor make use history eight branches deep. is, eight- deep branch history provide proper context making prediction. predictors described improve prediction accuracies making use better context. use greater amount context , use different contexts different branches, use additional types information beyond branch ddress branch history. 9.3.3.1 Alloyed History Predictors. GA* predictors able make pre- dictions based correlations global branch history. PA* predictors use correlations local, per-address, branch history. Programs may contain branches whose outcomes well predicted global-h istory predictors branches well predicted local-history predictors. hand, branches require global branch history per-address branch history correctly predicted. Mispr edictions due using wrong type history one type one needed called wrong-history mispredictions. alloyed branch predictor removes wrong-history mispredic- tions using global local branch history [Skadron et al., 2003]. per- address BHT maintained well global branch history register. Bits branch address, global branch history, local branch history con- catenated together form index PHT. combined global/local branch history called alloyed branch history. approach allows global local correlations distinguished structure. Alloyed branch history also enables branch predictor detect correlations simultaneously depend types history; class predictions one could suc cessfully predicted either global-history predictor local-history predictor alone. Alloyed predictors also c lassified MAg/MAs/MAp predictors (M "merged" history), second-level table indexed way two-level predictors. Therefore, three basic alloyed predictors MAg, MAp, andADVANCED INSTRUCTION FLOW TECHNIQUES Branch address Alloyed history ^ PHT Branch prediction Figure 9.18 Alloyed History Predictor MAs. Alloyed history version branch prediction algorithms also possi- ble, mshare (alloyed history gshare), mskewed (alloyed history gskewed). Figure 9.18 illustrates hardware organization alloyed predictor. Like PAg/PAs/PAp two-level predictors, low-order bits branch address used index local h istory BHT. corresponding l ocal history concatenated wit h contents global BHR bits branch address. index used perform lookup PHT, corre- sponding counter used make final branch pre diction. branch predictor designer must make tradeoff width th e global BHR width per-address BHT entries. 9.3.3.2 Path History Predictors. outcome history-based approaches branch prediction, may case two different paths program execution may h ave overlapping branch address branch history pairs. example, Figure 9.19, program may reach branch X block going blocks A, C, D, going B, C, D. attempting predict branch X block D, branch ad dress branch histories last two global branches identical either ACD BCD. Dep ending path program arrived block D, branch X primarily not-taken (for path ACD), primarily taken (for path BCD). using branch out- come history, different branch outcome patterns cause great deal interference corresponding PHT counter. Path-based branch correlation en proposed make better branch pre- dictions dealing situations like example Figure 9.19. Instead storing last n branch outcomes, k bits last n branch addressesPROCESSOR DESIGN ( — 0 ) goto C; History - C Path ACD: Branch address = X Branch history = TT Branch outcome = taken Figure 9.19 Path History Example.if (y = = S) goto C; (y < 12) goto D;History = History = TT (y % 2) goto E;Path BCD: Branch address = X Branch history = TT Branch outcome = taken Branch address Path history shift registersShift address updatePHT Branch prediction Figure 9.20 Path History Predictor. stored [Nair, 1995; Reches Weiss. 1997]. concatenation nk bits encodes branch path last n branches, also called path history, thus potentially allowing predictor differentiate two differ- ent branch behaviors example Figure 9.19. Combined subset branch address bits current branch, forms index PHT. pre- diction made way normal two-level predictor. Figure 9 20 illus trates hardware path history branch predictor. bits last n branches concatenated together form path history. path history concatenated low-order bits current branchADVANCED INSTRUCTION FLOW TECHNIQUES 4 address. index used perform lookup PHT, final prediction made. branch processed, bits current branch address added path history, oldest bits discarded. path history register implemented shift r egisters. number bits per branch address stored k, number branches path history n, number bits current branch address must carefully chosen. PHT 2"*+m entries, therefore area requirements become prohibitive even mod- erate values n, k, m. Instead con catenating n branch addresses, combi- nations shifting, rotating, hashing (typically using XORs) used compress nk + bits manageable size [Stark et al., 1998]. 9333 Variable Path Length Predictors. branches correlated branch outcomes branch add resses occurred recently. Incorporating longer history introduces additional bits provide additional information predictor. fact, useless context degr ade performance predic- tor predictor must figure parts context irrelevant, turn increases training time. hand, branches strongly correlated older branches, requires predictor make use longer history branches correctly predicted. One approach dealing varying history length requirements branches use different history lengths fo r branch [Stark et al., 1998]. following des cription uses path history, idea using different history lengths applied branch outcome histories well. Using n different hash- ing functions /1( /2, . . . , /„, bash function fL creates hash last branch addresses path history. hash function used may different different br anches, thus allowing variable-length path histories. selection hash function use determined statically compiler, chosen aid program profiling, dy namically selected additional hard- ware tracking wel l hash functions performing. elastic history buffer (EHB) uses v ariable outcome history length [Tarlescu et al., 1996]. profiling phase statically chooses branch history length static branch. compiler communicates chosen length using br anch hints. 933.4 Dynamic History Length Fitting Predictors. optimal history length use predictor varies applications. applications may pro- gram behaviors change f requently better predicted adaptive short-history predictors bec ause short-history predictors require less time train. programs may distantly correlated branches, require long histo- ries detect patterns. fixing branch history length constant, applications may better predicted cost reduced performance others. Furthermore, optimal history length program may change execution program itself. multiphased computation com- piler may exhibit different branch patterns different phases execu- tion. short history may optimal one phase, longer history may provide better prediction accuracy next phase.486 MODERN PROCESSOR DESIGN Dynamic histor length fitting (DHLF) addresses problem varying opti- mal history lengths. Instead fi xing history length constant, pre- dictor uses different history lengths attempts find length minimizes branch mispredictions [Juan et al., 1998]. applications require shorter his- tories, DHLF pr edictor tune consider fewer branch outcomes; benchmarks require longer histories, DHLF predictor adjust sit- uation well. DHLF technique applied kinds correlating pre- dictors (gshare, Bi-Mode, gskewed, etc.). 9.3.3.5 Loop Counting Predictors. general, termination for-loop difficult predict using algorithms already presented section. time for-loop encountered, number ite rations executed often previous time loop wa encountered. simple example inner loop matrix multiply algorith number iterations equal matrix block size. consistent number iterations, loop exit branch easy predict. Unfortunately, branch history register-based approach would require BHR sizes greater number iter- ations loop. Beyond small number iterations, th e storage requirements predictor become pr ohibitive, PHT size exponential history length. Pentium-M processor use loop predictor conjunction branch history-based predictor [Gochman et al., 2003]. loop pr edictor cons ists table entry contains fields record current iteration count, itera- tion limit, direction branch. illustrated Figure 9.21. loop branch one always goes direction (either taken not-taken) followed single instance branch direction opposite, pattern repeats. traditional loop-closing branch pattern 111... 1110111 ... 1110111 Pentium-M loop predictor also handle Figure 9.21 Single Entry Loop Predictor Table Used Pentium-M Processor.ADVANCED INSTRUCTION FLOW TECHNIQUES opposite pattern f 000... 0001000... 0001000 limit field stores count number iterations observed previous invocation loop. loop exit detected, counter value copied limit field counter reset next run loop. prediction field records predominant direction branch. long counter less limit, loop predictor use prediction field. counter reaches limit, indicates predictor reached end loop, predicts opposite direction stored prediction field. loop-counting predictors useful hybrid predictors (see Section 9.4), provide poor performance used cannot capture nonloop behaviors. 9.3.3.6 Perceptron Predictor. maintaining larger branc h history regis- ters, additional history stored provides opportunities correlating branch predictions. two major drawbacks approach. first size PHT exponential width BHR. second many history bits may actually relevant, thus act training "noise." Two-level predictors large BHR widths take longer train. One solution problem Perceptron predictor [Jimenez Lin, 2003]. branch address (not address-history pair) mapped single entry Perceptron table. entry table consists state single Per- ceptron. Per ceptron simplest form neural network [Rosenblatt, 1962]. Perceptron trained learn certain boolean functions. case Perceptron branch predictor, bit x, input x equal 1 branch taken ( BHR, = 1) xt equal -1 branch taken (BHR, = 0). one special bias input x0 always 1. Perceptron e weight w, input x„ including one weight w0 bias input. Perceptron's output computed n = W0+^(wrXi) i=l negative, branch predicted taken. Oth erwise branch predicted taken. branch outcome available, weighis Perceptron updated. Let = -1 branch taken, = 1 branch taken. addition, let 6 > 0 training threshold. variable yOUI computed 1 f y>G 0 if-G<y <e - 1 f y<-G y^ equal /, weights updated w, = w, + txit e [0, 1, 2,..., n\. Intuitively, -6 < < 0 indicates Perceptron trained state predictions made high confidence. setting yolI, to488 MODERN PROCESSOR DESIGN zero, condition yM*t always true, Perceptron's weights updated (training continues). correlation large, magnitude weight tend become large. One limitation using Percep tron learning al gorithm linearly separable functions learned. Linearly separable boolean functions instances outputs 1 separated hyperspace instances whose outputs 0 hyperplane. Jimenez Lin [2003], shown half SPEC2000 integer nchmarks, 50% branches linearly inseparable. Perceptron predictor generally performs better gshare nchmarks linearly separable branches, whereas gshare outperforms Perceptron predictor b enchmarks greater number linearly inseparable branches. Perceptron predictor adjust weights corresponding bit history, since algorithm effectively "tune out" history bits relevant (low correlation). ability selectively filter branches, Per ceptron often attains much faster tr aining times conventional PHT-based approaches. Figure 9.22 illustrates th e hardware organization P erceptron predictor. lower-order bits branch address used index table Table Perceptron weights Branch address "2Updated weight values Global BHR Branch outcome Sign bitBranch prediction Figure 9.22 Perceptron PredictorADVANCED INSTRUCTION FLOW TECHNIQUES Perceptrons per-address fashion. weights se lected Percept ron BHR forwarded block combinatorial logic computes y. pre- diction made based complement sign bit (most-significant bit) y. value also forwarded additional block logic combined actual branch outcome compute dated values weights Perceptron. design space Perceptron branch predictor appears much larger gshare Bi-Mode predictors, example. Perceptron predictor four parameters: number Perceptrons, number bits history use, width weights, learning threshold. empirically derived relation optimal threshold value function history length. threshold G e qual |_ l-93fi +14J, h history length. number history bits potentially used still much larger gshare predictors (and similar schemes). Similar alloyed history two-level branch predictors, alloyed history Per- ceptron predictors also proposed. rt bits global history bits local history, Perceptron uses n + m+l weights (+1 bias) make branch prediction. 9.3.3.7 Data Flow Predictor. Percept ron predictor makes use long- history register effectively finds highly correlated branches assigning higher weights. majority branches correlated two reasons. first branch may guard instructions affect test condition later branch, branch ex ample Figure 9.4 . branches called affector branches. second two branches operate similar data. Perceptron attempts find highly correlated branches fuzzy fashion assigning larger weights correlated branches. Another approach find highly correlated branches long-branch-history register data flow branch predictor explicitly tracks register depende nces [Thomas et al., 2003]. main idea behind data flow branch predictor explicitly track previous branches affector branches current branch. affector register file (ARF) stores one bitmask per architect ed register, entries bitmask correspond past branches. ah recent branch affector register R, ith recent bit entry R ARF set. register updating instructions form Ra = Rb op Rc, ARF entry Ra set equal bitwise-OR ARF entries Rb Rc, with'the least- significant bit (most recent branch) set 1. illustrated Figure 9.23. Set- ting least-significant bit one indicates recent branch (bO) guards struction modifies Ra. Note entries Rb Rc also corresponding affector bits set. ARF entries operands makes current register inherit affectors operands. fashion, ARF entry records bitmask specifies affector branches potentially affect register's va lue. branch instruction, ARF updated shifting entries lef one filling least-significant bit zero.490 MODERN PROCESSOR DESIGN 75 b3 Rc = . 7Rc 0 0  b2 Rb = ...zRb  bl 75bO Ra = Rb op RcRa 1 0Ra = Rb op Rc b3 b2 bl bO Figure 9.23 Affector Register File Update Register Writing Instruction. ARF specifies set potentially important br anches, branches affect values cond ition registers. conditional branch compares one register values evaluates condition val- ues (e.g., equal zero greater than). make prediction, data flow predic- tor uses ARF entries corresponding operand(s) current branch; two operands, final bitmask formed exclusive-OR respective ARF entries. affector bitmask ANDed global history register, isolates global history outcomes affector branches only. branch history register likely larger index PHT. masked version history register still needs hashed appropriate size. final hashed vers ion masked history register indexes PHT provide final prediction. overall organization th e data flow predictor illustrated Figure 9.24. original data flow branch predictor study, predictor presented corrector predictor, basically secondary predictor backs prediction mechanism. idea basically overriding predic- tor organization explained Section 9.5.4.2. primary predictor provides predictions, data flow predictor attempts leam provide corrections branches primary predictor properly handle. reason, PHT entries data flow predictor may augmented partial tags (see partial resolution Section 9.5.1.1) set associativity. allows data flow predictor carefully identify correct branches knows about. data flow predictor attempt correct select set branches, total size may smaller othe r conventional stand-alone branch predictors.ADVANCED INSTRUCTION FLOW TECHNIQUES  - Final prediction Figure 9.24 Data Flow Branch Predictor. 9.4 Hybrid Branch Predictors Different branches progra may strongly correlated different types history. this, branches may accurately predicted wit h global history-based predictors, others strongly correlated local h istory. Programs typically contain mix branch types, example, choosing implement global history-based predictor may yield poor prediction accuracies branches strongly correlated local history. certain degree, alloyed branch predictors address issue, tradeoff must made number global history bits used number local history bits used Furthermore, alloyed branch predictors cannot eff ectively take advantage predictors use forms information, loop predictor. section describes algorithms employ two single-scheme branch prediction algorithms combine multiple predictions together make one final prediction. 9.4.1 Tournament Predictor simplest earliest proposed multischeme branch predictor tourna- ment algorithm [McFarling, 1993]. predictor consists two component pre- dictors P0 P[ meta-predictor M. component predictors single-scheme predictors described Section 9.3, even one hybrid predictors described section. meta-predictor table 2-bit counters indexed low-order bits branch address. identical lookup phase Smith2, except (meta-)prediction zero indicates P0 used, (meta-)prediction one indicates P, used (the meta-prediction made most-significant bit counter). meta-predictor makes prediction predictor correct.92 MODERN PROCESSOR DESIGN branch outcome available, P0 P, upd ated according respective update rules. Although meta-predictor structurally identical Smith2, update rules (i.e., tate transitions) ifferent Recall 2-bit counters used predictors finite state machines (FSMs), inputs typically branch outcome previous state FSM. meta- predictor M, inputs c0, clt previous FSM state, ct one P, predicted correctly. Table 9.2 lists state transit ions. P,'s prediction correct P0 mispredicted, corresponding counter incremented, saturating maximum value 3. Conversely, P, mispredicts P0 pre- dicts correcdy, cou nter decreme nted, saturating z ero. P0 P, correct, mispredict, counter unm odified. Figure 9.25a illustrates hardware ournament selec tion mechanism two generic component predictors P0 P,. prediction lookups P0, P,, performed parallel. three predictions made, meta-prediction used drive select line mukiplexer choose predictions P0 P^ Figure 9.25b illustrates example tournament Table 9.2 Tournament meta-predictor update rules e,(P8 Correct?) c, (P, Correct?) Modification 0 0 nothing 0 1 Saturating increment J 0 Saturating decrement 1 1 nothing Branch prediction Branch prediction Figure 9.25 (a) Tournament Selection Mechanism, (b) Tournament Hybrid gshare PAp.ADVANCED INSTRUCTION FLOW TECHNIQUES 49 selection predictor gshare PAp component predictors. hybrid predictor similar one depicted Figure 9.25b implemented Compaq Alpha 21264 microprocessor [Kessler, 1999]. local history component used 1024- entry BHT 10-bit per-branch histories. 10-bit history used index single 1024-entry PHT. Th e global history component uses 12-bit history indexes 4096-entry PHT 2-bit counters. meta-predictor also uses 4096-entry table counters. Like two-level branch predictors, tournament's meta-predictor also make use branch history. shown global branch outcome history hashed PC (similar gshare) provide better overall prediction accuracies [Chang et al., 1995]. Either two components tournament hybrid predictor may hybrid predictors. recursively arranging multiple ournament meta-predictors tree, number predictors may combined [Evers, 2000]. 9.4.2 Static Predi ctor Selection profiling program-based analysis, reasonable branch prediction rates achieved many programs static branch prediction. downside static branch prediction way adapt u nexpected branch behavior, thus leaving possibility undesirable worst-case behaviors. Grunwald et al. [1998] proposed using p rofiling techniques, limited meta-predictor. entir e multischeme branch predictor supports two component predictors, may dynamic. selection compo- nent use det ermined statically encoded branch instruction branch hints. meta-predictor requires additional hardware except single mul- tiplexer select component predictors' predictions. proposed process determining static ta-predictions lot involved traditional profiling techniques. Training sets used execute programs profiled, programs executed native hardware. Instead, processor simulator used fully simulate th e branch prediction struc- tures addition functional behavior program. component predic- tor correct highest f requency selected static branch. may practical since simulator may slow full knowledge component branch predictors' implementations may available. several advantages static selection mechanism. first hardware cost negligible (a single additional n-to-1 multiplexer n compo- nent predictors). second advantage static branch assigned one one component branch predictor. means average number static branches per component reduced, alleviates problems conflict capacity aliasing. Although meta-predictions perf ormed statically, underlying branch predictions still incorporate dynamic informat ion, thus reduc- ing potential effects worst-case branc h patterns. disadvantages include overhead associated simulating branch prediction structures profiling phase, fact branch hints available instruc- tion fetch completed, fact number component predictors limited number hint bits available single branch instruction.194 MODERN PROCESSOR DESIGN 9.4.3 Branch Classification branch classification meta-prediction algorithm similar static selection algorithm may even viewed special case static selection [Chang et al., 1994]. profiling phase first performed, but, contrast static selection, branch taken rates collected (similar prof ile-base static branch prediction techniques described Section 9.2.4). static branch placed one'of six branch c lasses depending taken rate. heavily biased one direction, defined ta ken rate not-taken rate less 5%, statically predicted. remaining branches predicted using tour nament hybrid method. overall predictor structure static selection multischeme pre- dictor three components (P0, P , P2). P0 static not-taken branch pre- dictor. P, static taken branch predictor. P2 another multischeme branch predictor, consisting tour nament meta-predictor two component pre- dictors, P20 P2-I. two componen predictors P2 chosen dynamic static branch prediction algorithms, ypically global history predictor local history predictor. branch classification algorithm advantage easily predicted branches removed dynamic branch prediction structures, thus reducing number potential sour ces aliasing conflicts. similar benefits provided branch filtering. Figure 9.26 illustrates hardware branch classification meta-predictor static taken non-taken predictors, well two unspecified generic components P20 P2I, tournament sele ction meta-predicto r choose two dynamic components. Similar static hybrid selection Branch address branch prediction Figure 9.26 Branch Classification Mechanism.ADVANCED NSTRUCTION FLOW TECHNIQUES 4' mechanism, branch classification hint available predictor instruction fetch completed. 9.4.4 Multihybrid Predictor point, none mul tischeme meta-predictors presented capable dynamically selecting two component predictors (except recur- sively using tourn ament meta -predictor). definition, single tournament meta-predictor choose two components. static selection approach cannot dynamically choose components. branch classifi- cation algorithm statically choose one three components, dynamic selector used chooses two components. multihybrid branch predictor allow dynamic selection arbitrary number component predictors [Evers et al., 1996]. lower bits branch address used index table prediction selection counters. entry table consists n 2-bit saturating counters, c,, c2,; .. cm c, counter corresponding component predictor P,. c omponents predicting well higher counter values. meta-prediction made selecting th e component whose cou nter value 3 (the maximum) predetermined priority ordering used break ties . counters initialized 3, update rules guarantee least one counter value 3. update counters, least one component counter value 3 correct, counter values corresponding components ispredicted decremented (saturating .at zero). Otherwise, counters corresponding components predicted correctly incremented (saturating 3). Figure 9.27 illustrates hardware organization multihybrid meta- predictor n component predictors. branch address used look entry table prediction selection counters, n counters IjJrandiaddressJ Table counters 0 0-0 Figure 9.27 Multihybrid Predictor.496 MODERN PROCESSOR DESIGN checked value 3. priority encoder generates index component counter value 3 highest priority case tie. index sig- nal forwarded final multiplexer selects final prediction. Unlike static se lection even branch classification eta-prediction algorithms, multihybrid meta-predictor capable dynamically handling number component branch predictors. 9.4.5 Prediction Fusion hybrid predictors described far use selection mechanism choose one n predictions. singling single predictor, selection-based hybrids throw useful information conveyed predictors. Another approach, called prediction fusion, attempts combine predictions n individual predictors making final prediction [Loh Henry, 2002]. allows hybrid predictor leverage information available compo- nent predictors, potentiall making use global - local-history compo- nents, short- long-history components, combination these. Prediction fusion covers wide variety predictors. Selection-base hybrid predictors special cases fusion predictors fusion mechanism ignores n - 1 inputs. gskewed predictor thought prediction fusion predictor uses three gshare predictors different hashing func tions inputs, majority function fusion chanism. One fusion-based hybrid predictor fusion table. Like multihybrid predictor, fusion table take predictions arbitrary number subpredictors. n predictors, fusion table concatenates th e corresponding n predictions together index. index, combined bits PC possibly global branch history form final index table saturating counters. significant bit indexed saturating counter provides final prediction. illustrated Figure 9.28. Fusion table Branch address Global BHR  Final prediction Figure 9.28 Fusion Table Hybrid Predictor.ADVANCED INSTRUCTION FLOW TECHNIQUES fusion able provides way correlate branch outcomes multiple branch predictions. fusion table remember arbitrary mapping pre- dictions branc h outcome. example, case w branch always taken exactly one two predictors taken, entries fusion table correspond situation trained predict taken, entries correspond predictor predicting taken predicting not-taken train predict not-taken. fusion table hybrid predictor effective flexible. combination global- local-history components, sh ort- long- history components, fusion table accurately capture wide array branch behaviors. 9.5 struction Flow Issues Techniques Predicting direction conditio nal branches one several issues pro- viding high rate instruction fetch. section covers hese additional problems taken-branch target prediction, branch confidence prediction, predictor- cache organizatio ns interactions, fetching multiple instructions parallel, coping faster clock speeds. 9.5.1 Target Prediction conditional branches, predicting whether branch taken not-taken half problem. th e direction branch known, actual target address next instruction along predicted path must also determined. branch predicted not-taken, target address simply current branch's address plus size instruction word. branch predicted taken, targe depend type branch. Target prediction must also cover un conditional branches (branches always taken). two common types branch targets. Branch targets may PC- relative, means taken target always current branch's address plus constant (the constant may negative). branch target also indirect, means target computed run time. indirect branch target read register, sometimes constant offset added con- tents regis ter. Indirect branches frequently used object-oriented pro- grams (such C++ vtable det ermines co rrect method invoke classes using inheritance), dynamicall linked libraries, subroutine returns, sometimes multitarget control constructs (i.e., C switch statements). 9.5.1.1 Branch Target Buffers. target branch usually predicted branch target buffer (BTB), sometimes also called branch target address cache (BTAC) [Lee Smith, 1984]. BT B cache-like structure stores last seen target ad dress branch instruction. making branch prediction, traditional branch predictor provides predicted direction. parallel, processor uses current branch's PC ndex BTB. BTB typi- cally tagged structure, often implemented degree set associativity.198 MODERN PROCESSOR DESIGN Branch address Branch direction predictor Size instructionBranch target buffer Not-taken targetItarget tag target tag target tag Taken target * Branch targetOR BTB hit? Figure 9.29 Branch Target Buffer, Generic Branch Predictor, Target Selection Logic Figure 9.29 shows organization branch predictor BTB. branch predictor predicts not-taken, target simply next sequential instruction. branch predic tor predicts taken hit BTB, BTB's prediction used next instruction's address. also possi- ble taken-branch prediction, miss BTB. situation, processor may stall fetching target known. branch PC-relative target, fetch stalls cycles wait completion instruction fetch instruction cache, target offset extraction instruction word, addition offset current^ PC generate actual targ et. Another approach fall back not-taken target BTB miss. Different strate gies may used maintaining information stored BTB. simple approach store targets branches encountered. slightly better use BTB store targets aken branches. branch predicted taken, next address easily com- puted. filtering not-taken targets, prediction rate BTB may improved decrease interference.ADVANCED INSTRUCTION FLOW TECHNIQUES 499 Fetch IP Cycle n Predict +BTB Cycle n + Cycle n + 2 I-cache cycle 1 I-cache cycle 2 LD SUB (a)BRCycle n Figure 9.30 Timing Diagram Illustrating Branch Prediction Occurs Instruction Fetch Completes: (a) Branch Present Fetch Block, (b) Brancn Present Fetch Block— EXAMP E pipelined processor, instruction cache access may require multiple cycles fetch instruction. branch target predicted, processor immediately proceed fetch next instruction. potential problem instruction fetched decoded, pro- cessor know next instruction branch not? Figure 9.30 illustrates branch predictor two-cycle instruction cache. cycle n, current branch address (IP) fed branch predictor BTB predict target next fetch block. time, instruction cache access current block started. end cycle n, branch predictor BTB provided direction target prediction branch highlighted bold Figure 9. 30(a). Note cycle n tbe branch prediction made, known branch corresponding block. Figure 9.30(b) shows example branches present fetch block. Since branches, next block fetch simply next sequential block. cycle n branch prediction made, predictor know branches, may even provide target add ress corresponds taken branch! predicted taken branch corresponding branch instruction sometimes called phantom branch bogus branch. cycle n + 2, decode logic detect branches present and, taken-branch prediction, predictor instruction cache accesses redirected correct next-bl ock address. Phantom branches ncur slight performance penalty delay branch prediction phantom branch detection causes bubbles fetch pipeline. branches present fetch block, correct ne xt-fetch address next sequential instruction block. equivalent not-taken500 MODERN PROCESSOR DESIGN branch prediction, taker branch prediction without corres- ponding branch int roduces phantom bra nch. BTB ever u pdated targets taken branches, next block instructions contain branches, always BTB miss. proce ssor uses f allback not-taken strategy, result correct next-instruction address predic- tion branches present, thus removing phantom branches. Address tags typically fairly large, BTBs often use partial resolu- tion [Fagin Russell, 1995]. partial resolution, subset tags stored BTB entry. allows decrease storage require- ments, opens opportunity false hits. Two instructions different addresses may hit BTB entry subset bits used tag identical, differences address bits somewhere else. BTB typically fewer entries direction predictor must store entire target address per entry ( typically 30 bits pe r entry), whereas direction predictor stores small 2-bit counter per entry. slight increase mispredictions due f alse hits usually worth decrease structure size provided partial resolution. Note false hits enable phantom branches occur again, false hit rate low, serious problem. 9.5.1.2 Return Address Stack. Function calls frequently occur programs. jump function jump back (the return) usually unconditional branches. target jump function typically easy predict. branch instruction jumps printf w l l likely jump place every time encountered . hand, return printf function may difficult predict printf could called many different places program. instr uction set architectures support subroutine calls providing means storing subroutine return address. executing jump sub- routine, address instruction sequentially follows jump stored register. addr ess typically stored stack used jump address end function return called. return address stack ( RAS) special branch target predictor provides predictions subroutine returns [Kaeli Emma, 1991]. jump function happens, return address pushed onto RAS, shown Figure 9.31(a). initial jump, RAS provide prediction target must predicted regular BTB. later point pro- gram program returns subroutine, top entry RAS popped provides c orrect target prediction sho wn Figure 9.31(b). stack store multiple return addresses, returns nested functions also properly predicted. return address stack guarantee perfect prediction return target addresses. stack limited capacity, therefore functions deeply nested cause stack overflow. RAS often implemented cir- cular buffer, overflow cause recent return address over- write oldest return address. stack unwinds return wasADVANCED INSTRUCTION FLOW TECHNIQUES 501 Branch address iSize instructionBranch address 1 Return addressBTB Target p rediction return? Target prediction (a) (b) Figure 9.31 (a) Return Address Push Jump Subroutine, (b) Return Address Pop Subroutine Return. overwritten, target misprediction occur. Another source RAS misprediction irregular code matched subroutine calls returns. Usage C library functions e jmp long jmp could result th e RAS containing many incorrect targets. Usage RAS requires knowing whether branch function call return. formation typ ically available instruction fetched. subroutine call, target predicted BTB, introduce bubbles fetch pipeline. subroutine return, BTB may provide initial target prediction. Afte r instruction actually fetched , known return. point, instruction flow may corrected squashing instructions incorrectly fetched (or pro- cess fetched) resuming fetch return target provided RAS. Without RAS, target misprediction would detected return address loaded program stack register return instruction executed. Return address stacks implemented almost curren mircroprocessors. example Pentium 4, uses 16-entry return address stack [Hinton et al., 2001]. RAS also ometimes referred return stack buffer (RSB). 9.5.2 Branch Confidence Prediction branches easy predict, others cause g reat trouble branch predictor. Branc h confidence prediction make attempt predict outcome branch, instead makes prediction branch prediction.1 EXAMTLE TT502 MODERN PROCESSOR DESIGN purpose branch confidence prediction guess estimate certain processor par ticular branch prediction. example, selective branch inversion technique (see Section 9.3.2.6) switches direction initial branch prediction confidence predicted low. confidence predic- tion detects cases branch irection predictor consistently wrong thing, selective branch inversion (SBI) uses information rec- tify situation. many applications branch confidence informa- tion. section first discusses techniques predicting branch confidence, surveys applications branch confidence prediction. 9.5.2.1 Prediction Mechanisms. branch confidence prediction, infor- mation used whether branch predictions correct not, opposed whether prediction taken not-taken [Jacobson et al., 1996]. Figure 9.32 shows branch confidence predictor uses global branch outcome history context fashion similar gshare predictor, PHT replaced array correct/incorrect registers (CIRs). CIR shift register similar BHR conventional branch predictors, instead storing history branch directions, CIR stores history whether th e branch cor- rectly predicted. Assuming 0 indicates correct prediction, 1 indicates misprediction, four correct predictions followed two mispredictions followed three correct predictions would CIR pattern 000 011000. generate final confidence prediction hig h confidence low confi- dence, CIR must processed reduction function produce single bit ones-counting approach counts number CIR (that is, num- ber mispredictions). confidence predictor assumes large number recent mispredictions indicates future predictions also likely incorrect. Therefore, higher ones-count ndicates lower confidence. e fficient implementation replaces CIR shift register saturating counter. time Tabic CIRs | Branch address Global BHR Confidence prediction Figure 9.32 Branch Confidence PredictorADVANCED STRUCTION FLO W TECHNIQUES 5i correct prediction, counter incremented. counter decremented misprediction. counter large value, means branch predictions mostly correct, therefore large CIR counter value indi- cates high confidence. detect n consecutive correct predictions, shift register CIR needs rt bits wide. hand, counter-based CIR requires riog2n ] bits. alternative implementation uses r esetting counters misprediction causes counter reset zero instead decrementing counter. counter value equal number branches since last misprediction seen CI R. underlying branch prediction algo- rithms already accurat e, patterns observed shift-based CIRs dominated zeros (no recent mispredictions) single one (only one recent misprediction). Since r esetting counter tracks distance since last mispre- diction, approximately represents information. structure branch confidence predictor similar branch direction predictors. advanced techniques used branch direc- tion predictors could also applied confidence predictors. 9.5.2.2 Applications. Besides already discussed selective branch inversion technique, branch confidence prediction many ot potential applications. alternative approach predicting conditional branches speculatively going one path fetch execute taken not-taken paths time. technique, called eager execution, guaran- tees processor perform useful work, also guarantees instructions fetched executed discarded. Allowing pro- cessor "fork" every con ditional branch two paths rapidly becomes expensive since processor must able track di fferent paths flush different sets instructions different branches resolved. Further- more, performing eager execution highly predictable branch wastes resources. wrong path use fetch bandwidth, issue slots, functional units, cache bandwidth could otherwise used correct path instructions. Selective eager execution limits harmful effects un con- trolled eager execution limiting dual-path execution branches deemed difficult, i.e., low-confidence branches [Klauser et al., 1998]. variation eager execution called disjoint eager execution discussed Chapter 11 [Uht, 1997]. Branch mispredictions big reason performance de gradations, also represent large source wasted power energy. instructions mispredicted path eventually discarded, power spent fetching, scheduling, executing instructions energy spent nothing. Branch confidence used decrease power consumption processor. low-confidence branch encountered, instead making branch prediction poor chance correct, processor simply stall th e front-end wait actual branch outcome computed. reduces instruction- level parallelism covering parallelism blocked control dependency, greatly reduces power wasted branch mispredictions.504 MODERN PROCESSOR DESIGN Branch confidence also used modify fetch po licies simulta- neously multithreaded ( SMT) processors (see Chapter II). SMT processor fetches executes instructions multiple threads using hardware. fetch engine SMT processor fetch instructions one thread cycle, depen ding fetch policy, may choose fetch instructions another thread following cycle. current thread encounters low- confidence branch, fetch engine could stop fetching branches current thread start fetchin g instructions another thread tha likely useful. fashion, execution resources would wasted likely branch misprediction usefully employed another thread. fx 9.5.3 High-Bandwidth Fetch Mechanisms superscalar processor execute multiple instructions per cycle, fetch engine must also able fetch multiple instructions per cycle. Fetching instruc- tions instruction cache typicall limited accessing single cache line per cycle. Taken branches disrupt instruction delivery stream instruc- tion cache nex instruction fetch likely different cache line. example, co nsider cache line stores four instruction words one taken branch. taken branch first instruction cache line, instruction cache provide one useful instruction (i.e., branch). taken branch last position, instruction cache can. provide four useful instructions. maximum number instructions fetched per cycle bounded number words instruction cache line (assuming limit one instruction cache access per cycle). Unfortunately, increasing size cache line limited effectiveness increasing fetch bandwidth. typical intev ger applications, number instructions target branch next branch (i.e., basic block) five six instructions. Assuming six instructions per basic block, 60% branches taken, expected number instructions taken branches 15. Unfortunate ly, many situations rate taken branches close 100% (for example, loop). cases, instruction fetch engine able provide, one iteration loop per cycle. related problem th e situation block instruction split across cache lines. first f sequential iristructions fetched located end cache line, two cache acc esses necessary fetch four instructions^ section describes two mechanisms providing instruction fetch ba width handle multiple basic blocks per cycle. 9.5.3.1 Collapsing Buffer. collapsing buffer scheme uses combi! tion interleaved BTB provide multiple target predictions, ba instruction cache provide one line instructions para llel, masking alignment (collapsing) circuitry compact statically n onseq" tial instructions [Conte et al., 1995].ADVANCED INSTRUCTION FLOW TECHNIQUES 505 Figure 9.33 Collapsing Buffer Fetch Organization.To decode stage Figure 9.33 shows organization collapsing buffer. figure, cache lines four nstruction words wide, cach e broken two banks. instructions fetched A, B, C, E, G. conventional instruction cache, instructions A, B, C would fetched single cycle. branch target buffer provides predictions instruction C taken branch (with target address E) instruction E also taken branch target address G. One cache bank fetches cache line containing instructions B, C, D, cache bank provides instructions E, F, G, H. cache lines bank, n collapsing buffer fetch mechanism provide one cache line's worth instructions. two cache lines fetched, cache l ines go interchange switch swaps th e two lines second cache line con tains earlier instructions. interleaved BTB provides valid instruction bits specify entries cache lines part predicted path. collapsing circuit, implemented shifting logic, collapses disparate instructions one con- tiguous sequence forwarded decoder. instructions removed shaded Figure 9.33. Note example, branc h C crosses cache lines. branch E intrablock branch branch target resides the506 MODERN PROCESSOR DESIGN cache line branch. example, collapsing buffer provided five instructions whereas traditional instruction cache would fetch three. shift-logic based collapsing circuit uffers long latency. Another way implement collapsing buffer crossbar. cr ossbar allows arbitrary permutation inputs, even interchange network needed. this, overall latency interchange collapse may reduced, despite relatively complex nature crossbar networks. collapsing buffer adds complex circuitry fetch path. interchange switch collapsing circuit add considerable latency front-end pipeline. extra latency would take form additional pipeline stages, increases branch misprediction penalt y. organization collaps- ing buffer difficult scale support fetching two cache lines per cycle. 9.5.3.2 Trace Cache. collapsing buffer fetch mec hanism highlights fact many dynamically sequential instructions physically located contig- uous locations. Taken branches cache line alignment problems frequently dis- rupt fetch engine's attempt provide continuous high-bandwidth stream instructions. trace cache attempts alleviate probl em storing logi- cally sequential instructions consecutiv e physical locations [Friendly etal., 1997; Rotenberg et al., 1996; 1997]. trace dynamic sequence instructions. Figure 9.34(a) shows sequence instructions fetche locations conventional instruc- tion cache. Instruction B predicted aken branch C. Instructions C split across two separate cache lines. Ins truction another predicted taken branch E. Instructions E J split across two cache lines, expected since instructions group width cache line. conventional fetch architecture, take least five cycles fetch 10 instructions instructions scattered five different Instruction cache (a) (b) Figure 9.34 (a) Ten Instructions Instruction Cache, (b) 10 Instructions Trace CacheADVANCED INSTRUCTION FLOW TECHNIQUES 507 Instruction cache Trace construction buffer Trace cache (a)-To decode Store trace completeInstruction reorder buffer Trace const, buf.From retire Trace cache Store trace complete (b) Figure 9.35 (a) Fetch-Time Trace Construction, (b) Completion-Time Trace Construction cache lines. Even collapsing b uffer, would till take three cycles (maxi- mum fetch rate two lines per cycle). trace cache takes dif ferent app roach. Instead attempting fetch multiple locations stitch instructions back together like collapsing buffer, trace cache stores entire trace one physically contiguous location shown Figure 9.34(b). trace cache deliver entire 10-instruction trace sin- gle lookup without complicated reshuffling realignment instructions. Central trace cache fetch mechanism task trace construction. Trace construction primarily occurs one two locations. first possib ility perform trace construction fetch time, shown Figure 9.35(a). instruc- tions fetched conventional instruction cache, race construction buffer stores dynamic sequence instructions. trace complete, may det ermined various constraints width trace cache limit number branches per trace, newly constructed trace stored trace cache. future, path encountered, trace cache provide instructions single access. point trace construction back end processor instructions retire. Figure 9.35(b) shows proc essor back end retires instructions order, instructions placed trace construction buffer. trace complete, trace stored trace cache new trace started. One advantage back-end trace construction circuitry branch misprediction pipeline, trace constructor may take cycles construct traces. trace entry consists instructions trace, entry also con- tains tags corresponding starting points basic block included trace. perform lookup trace cache, fetch engine must provide trace cache addresses basic block starting addresses predicted path. addresses match, trace cache hit. prefix the508 MODERN PROCESSOR DESIGN addresses match (e.g., first two addresses match third not), possible provide subset trace corresponds predicted path. high rate fetch, trace cache requires front end pe rform multiple branch predictions per cycle. Adapting conventional branch predictors perform multiple predictions maintaining reasonable access latencies challenging design task. alternative making multiple branch predictions per cycle treat trace fundamental basic unit perform trace prediction. trace unique identifier defined starting PC outcomes conditional branches trace. trace predictor's output one trace identifiers. approach provides trace-level sequencing instructions. Even trace-level sequencing, level instruction-level sequencing (i.e., conventional fetch) must still provided. start program, program enters new regions code, trace cache constructed appropriate traces trace predictor learned trace-to-trace transitions. situation, conventional instruction cache branch pr edictor provide instructions slower rate unti l new traces constructed trace predictor ha p roperly trained. Intel Penti um 4 microarchitecture employs trace cache, first- level ins truction cache [Hinton et al., 2001]. Figure 9.36 shows block-level organization Pentium 4 fetch decode engine. trace cache use, trace cache BTB provides fetch addresses next-trace predictions. predicted trace trace ca che, instruction-level sequencing occurs, instructions fetched level-2 instruction/data cache. increases th e number cycles fetch instruction trace cache miss. instructions decoded, decoded instructions stored trace cache. Storing decoded instructions allows instructions fetched trace cache skip decode stage pipeline. Front-end BTBInstruction TLBLevel-2 unified data instruction cacheFront-end BTBand prefetcherLevel-2 unified data instruction cache Instruction decode Trace cache BTB Trace cache | Instruction fetch queue _p renamer, execute, etc. Figure 9.36 Intel Pentium 4 Trace Cache Organization.4«ADVANCED INSTRUCTION FLOW TECHNIQUES 509 9.5.4 High-Frequency Fetch Mechanisms Processor pipeline depths processor frequencies rapidly increasing. twofold impact design branch predictors fetch mechanisms. deeper pipelines, need accurate branch prediction increases due increased misprediction penalty. faster clock speeds, prediction cache structures must faster access times. achieve faster access time, predictor cache sizes must reduced, turn increases number mispredictions cach e misses. section describes technique provide faster single-cycle instruction cache lookups second technique combining multiple branch predictors different latencies. 9.5Al Line Way Prediction. provide one instruction cache access per cycle, instruction cache must lookup latency single cycle, processor mu st compute predict next cache access start next cycle. Typically, program counter provides dex instruction cache fetch, actually information strictly necessary. processor needs know sp ecific location th e instruction cache next instruction fe tched from. Instead predicting next instruction address, line prediction predicts cache line number next instruction located [Calder Grunwald, 1995]. line-predicted instruction cache, cache line stores next-line predic- tion addition th e instructions address tag. Figure 9.37 illustrates line- predicted instruction cache. first c ycle shown, instruction cache Cycle 1 ABCNexi predline icuon Target predictionCycle 2 Cycle 3 E F G H |tag| 6 Tag check cycle I's lookup Target prediction Figure 9.37 Direct-Mapped Cache Next-Line Prediction.510 MODERN PROCESSOR DESIGN accessed provides instructions stored next-line prediction. second cycle, next-line prediction (highlighted bold) used index instruction cache providing next line instructions. allows instruc- tion cache access start branch predictor completed target predic- tion. target prediction computed, already fetched cache line's tag compared target prediction. match, line prediction correct fetched cache line contained correct instruc tions. mismatch, line prediction wrong, new instruction cache access initiated th e predicted target address. line predictor correct, single-cycl e instruction fetch achieved. next-line misprediction causes injec- tion one pipeline bubble cycle spent fetching wrong cache line. Line prediction allows front end continually fetch instructions instruction cache, directly address latency cache access. Direct-mapped caches faster access times set-associative ca ches, suf- fer higher miss rates due conflicts. Set-associative caches lower miss rates direct-mapped caches, additional logic checking multiple tags performin g way-selection greatly increases lookup time. technique way prediction allows instruction cache accessed latencies direct-mapped caches still retaining miss rates set-associative caches. way prediction, cache lookup accesses single way cache struc- ture. Accessing single way appears much like access direct-mapped cache logic supporting set associativity removed. Simi- lar line prediction, v erification way prediction must performed, occurs critical path. way misprediction detected, another cache access needed provide correct instructions, results pipe- line bubble. combining line prediction way prediction, instruction cache fetch instructions every cycle aggr essive clock speed. Wa predic- tion also applied data cache decrease access times. 9.5.4.2 Overriding Predictors. Deeper processor pipelines enable greater increases processor clock frequency. Althou gh high clock speeds generally associ- ated high throughput, fast clock deep pipeline compounding effect branch predictors front end general. faster clock speed means less time perform branch prediction. achieve single- cycle branch prediction, sizes th e branch predictor tables, PHT, must reduced. Smaller branch prediction structures lead ca pacity conflict aliasing and, therefore, branch mispredictions. branch misprediction pe nalty also increased number pipe stages increased. Therefore, aggressive pipelining clock speed increased number branch mispredictions well performance penalty fo r mispre- diction. Trying increase branch prediction rate may require larger structures impact clock speed. tradeoff tween fetch efficiency clock speed pipeline depth. overriding predictor organization attempts rectify situation using two ifferent branch predictors [Jime nez, 2002; Jimenez et al., 2000]. TheADVANCED INSTRUCTION FLOW TECHNIQUES 51 Slow overriding predictorCycle 1 Cycle 2 Cycle 3 Stage 1 SmaU'fast »- predictorPredict -Predict PredictB Predict B Predict C Predict C SUgeZ — - Fetch Predict Fetch B Pr edictB Stage3 F«ch „ queue- Queue Predict slow predict agrees fast predict, nothit predictions match, flush A, B, C restart fetch new predicted target. Figure 9.38 Organization Fast Predictor Slower Overriding Predictor. first branch predictor small fast, single-c ycle predictor. predictor generally mediocre prediction rates due limited size still manage provide accurat e predictions rea sonable number branches. second predictor much larger requires multiple cycles access, much accurate. operation ove rriding predictor organization proceeds follows illustrated Figure 9.38. first predictor makes initial target predic- tion (A), instruction fetch uses prediction start fetch instructions. time, second predictor also starts prediction l ookup, prediction available several c ycles. cycle 2, waiting second predictor's prediction, first predictor provides another prediction instruction cache continue fetching instructions. lookup second predictor branch also started, therefore second predictor must pipelined. predictions fetches continue pipelined fashion second predictor finished prediction original instruction cycle 3. point, accurate prediction compared original "quick dirty" prediction. predictions match, first predictor correct (with respect second predictor) fetch continue. predic- tions match, second predictor overrides first prediction. fetches initiated meantime flushed pipe- line (i.e., A, B, C converted bubbles), first predictor instruc- tion cache reset target overridden branch. four possible outcomes two branch predictors. predictors made correct prediction, hen ar e bubbles injected.512 MODERN PROCESSOR DESIGN first predictor wrong, ov erriding predictor correct, bubbles injected (equal ifference access latencies two predic- tors, sometimes called override latency), still much smaller price pay full pipeline flush would occur overriding predic- tor. predictors mispredict,.then penalty regular pipeline flush would occurred even idealistic case better second pre- dictor sing le-cycle latency. first predictor actually correct, second predictor caused erroneous override, mispredict penalty equal pipeline flush plus override latency. overriding predictor organi- zation provides overall benefit frequency correct ov errides much greater erro neous overrides. Alpha 21264 uses technique similar overriding predictor configuration. f ast, accurate, first-level predictor combination instruction cache next-line next-way predictor. implicitly provides branch predictio n target ddress cache line predicted line way. accura te hybrid predictor (described Section 9.4.1) two-cycle latency provides second prediction following cycle. prediction results target different chosen next-line/next- way predictors, tha instruction flushed fetch restarts newly predicted target [Kessler, 1999]. 9.6 Summary chapter provided overview many ideas concepts proposed address problems associated providing effective instruction fetch bandwidth. problem predicting direction condi tional branches received large amount attention, much research effort produced myr- iad prediction algorithms. techniques target different challenges associated predicting conditional branches high accuracy. Research history-based correlating branch predictors influential, predictors used almost modern superscalar processors. ideas hybrid branch pre- dictors, various branch target predictor strategies, instruction delivery techniques also adopted commercial processors. Besides conditional branch prediction problem, chapter also sur- veyed design issues problems related processor's front- end microarchitecture. Predic ting branch targets, fetching instructions cache hierarchy, delivering instructions rest processor important, effective fetch engine cannot designed without paying close attention components. Although techniques influence others (as measured wheth er ever implemented real processor), single method absolute best way predict branches fetch instructions. real project, processor design involves many engineering tradeoffs best techniques one processor may com pletely inappropriate another. lot engineering certain degree art designing well-balancedADVANCED INSTRUCTION R GW TECHNIQUES 513 effective fetch engine. chapter written broaden knowl- edge understanding advanced instruction flow techniques, hopefully may inspire someday help advance state art well! REFERENCES Aragon, J. L., Jose Gonzalez, Jose M. Garcia, Antonio Gonzalez : "Confidence estima- tion branch prediction reversal," Lecture Notes Computer Science, 2228, 2001, pp. 214-223. Ball, Thomas, James R. Lams: "Branch prediction free," ACM S1GPLAN Sympo- sium Principles Practice Parallel Programming, May 1993, pp. 300-313. Calder, Brad, Dirk Grunwald: "Next cache line set prediction," Int. Symposium Computer Architecture, June 1995, pp. 287-296. Calder, Brad, Dirk Grunwald, Michael Jones, Donald Lindsay, James Martin, Michael Mozer, Benjamin Zom: "Evidence-based static branch prediction using machine learning," ACM Trans, Programming Languages Systems, 19,1, January 1997, pp. 188-222. Chang, Po-Yung, Mantis Evers, Yale N. Patt: "Improving branch prediction accuracy reducing pattern history table interference," Int. Conference Parallel Architectures Compilation Techniques, October 1996, pp. 48-57. Chang, Po-Yung, Eric Hao, Yale N. Patt: "Alternative implementations hybrid branch predictors," hit. Symposium Microarchitecture, November 1995, pp. 252-257. Chang, Po-Yung, Eric Hao, Tse-Yu Yeh, Yale N. Patt: "Branch classification: new mechanism improving branch predictor performance," Int. Symposium Microarchi- tecture, November 1994, pp. 22-31. Conte, Thomas M., Kishore N. Menezes, Patrick M. Mills, B urzin A. Patel: "Optimiza- tion instruction fetch mechanisms high issue rates," Int. Symposium Computer Architecture, June 1995, pp. 333-344. Eden, N. Avinoam, Trevor N. Mudge: "The YAGS branch prediction scheme," Int. Symposium Microarchitecture, December 1998, pp. 69-77. Evers, Marius: "Improving branch prediction understanding branch behavior," PhD Thesis, University Michigan, 2000. Evers, Marius, Po-Yung Chang, Yale N. Patt: "Using hybrid branch pred ictors improve branch prediction accuracy th e presence context switches," Int. Symposium Computer Architecture, May 1996, pp. 3—11. Evers, Marius, Sanjay J. Patel, Robert S. Chappell, Yale N. Patt: "An nalysis co rre- lation predictability: makes two-level branch predictors work," Int. Symposium Computer Architecture, June 1998, pp. 52-61. Fagin, B., K. Russell: "Pa rtial res olution branch target buffers," Int. Symposium Microarchitecture, December 1995, pp. 193-198. Fisher, Joseph A., Stephan M. Freudenberger "Predicting conditional branch direc- tions previous runs program," Symposium Architectural Support Program- ming Languages Operating Systems, October 1992, pp. 85-95. Friendly, Daniel H., Sanjay J. Patel, Yale N. Patt: Alternative fetch issue tech- niques trace cache mechanism," Int. Symposium Microarchitecture, December 1997, pp. 24-33.514 MODERN PROCESSOR DESIGN Gochman, Simcha, Ronny Ronen, Ittai Anati, Ariel Berkovitz, Tsvika Kurts, Alon Naveh, Ali Saeed, Zeev Sperber, Robert C. Valentine: "The Intel Pentium processor Microarchitecture performance," Intel Technology Journal, 7, 2, May 2003, pp. 21-36. Grunwald, Dirk, Donald Lindsay, Benjamin Zorn: "Static methods hybrid branch prediction," Int. Conference Parallel Architectures Compilation Techniques, October 1998, pp. 222-229. Hanstein, A., Thomas R. Puzak: "The optimum pipeline depth microprocessor," Int. Symposium Computer Architecture. May 2002, pp. 7-13. Hewlett Packard Corporation: PA-RISC 2.0 Architecture Instruction Set Manual, 1994. Hewlett Packard Corporation: "PA-RISC 8x00 Family Microprocessors Focus PA-8700," Technical White Paper, April 2000. Hill, Mark D.: "Aspects cache memory instruction buffer performance," PhD Thesis, University California, Berkeley, November 1987. Hinton, Glenn, Dave Sager, Mike Upton, Darrell Boggs, Doug Karmean, Alan Kyler, Patrice Roussel: "The microarchitecture Pentium 4 processor," Intel Technology Journal, Ql, 2001. Hrishikesh, M. S., Norman P. Jouppi , Keith I. Farkas, Doug Burger, Stephen W. Keckler, Primakishore Shivakumar: "The optimal useful logic depth per pipeline stage 6-8 F04," Int. Symposium Computer Architecture, May 2002, pp. 14-24. Intel Corporation: Embedded Intel 486 Processor Hardware Reference Manual. Order Number: 273025-001, July 1997. Intel Corporation: IA-32 Intel Architecture Optimization Reference Manual. Order Number 248966-009, 2003. Jacobson, Erik, Eric Rotenberg, James E. Smith: "Assigning confidence conditional branch predictions," Int. Symposium Microarchitecture, December 1996, pp. 142-152. Jimenez, Daniel A.: "Delay-sensitive branch predictors future technologies," PhD Thesis, University Texas Austin, January 2002. Jimenez, Daniel A., Stephen W. Keckler, Calvin Lin: "The impact delay cm design branch predictors," Int. Symposium Microarchitecture. December 2000, pp. 4-13. Jimenez, Daniel A., Calvin Lin: "Neural methods dynamic branch prediction," ACM Trans, Computer Systems, 20,4, February 2003, pp. 369-397. Juan, Toni, Sanji Sanjeevan, Juan J. Navarro: "Dynamic history-length fitting: third level adaptivity branch prediction," Int. Symposium Computer Architecture, June 1998, pp. 156-166. Kaeli, David R., P. G. Emma: "Branch history table prediction moving target branches due subroutine returns," fnr. Symposium Computer Architecture, May 1991, pp. 34—41. Kane. G, J. Heinrich: MIPS RISC Architecture. Englewood Cliffs, NJ: Prentice-Hall, 1992. Kessler. R. E.: "The Alpha 21264 Microprocessor," IEEE Micro Magazine, 19, 2, March- April 1999, pp. 24-26. Klauser, Anur, Abhijit Paithankar, Dirk Grunwald: "Selective eager execution poly path architecture," Int. Symposium Computer Architecture, June 1998, pp. 250-259.ADVANCED INSTRUCTION FLOW TECHNIQUES 5 Lee, Chih-Chieh, I-Cheng K. Chan, Trevor N. Mudge: "The Bi-Mode branch predic- tor," Int. Symposium Microarchitecture, December 1997, pp. 4-13. Lee, Johnny K F., Alan Jay Smith: "Branch prediction strategies branch target buffer design," IEEE Computer, 17, 1, January 1984, pp. 6-22. Loh, Gabriel H., Dana S. Henry: "Predicting conditional branches fusion-based hybrid predictors," Int. Conference Parallel Architectures Compilation Techniques, September 2002, pp. 165-176. Manne, Srilatha, Artur K lauser, Dirk Grunwald: "Branch prediction using selective branch inversion," Int. Conference Parallel Architectures Compilation Techniques, October 1999, pp. 48-56. McFarling, Scott: "Combining branch predictors," TN -36, Compaq Computer Corporation Western Research Laboratory, June 1993. McFarling, Scott, John L. Hennessy: "Reducing cost branches," Int. Symposium Computer Architecture, June 1986, pp. 396-404. Meyer, Dirk: "AMD-K7 technology presentation," Microprocessor Forum, October 1998. Michaud, Pierre, Andre Seznec, Richard Uhlig : "Trading conflict capacity aliasing conditional branch predictors," Int. Symposium Computer Architecture, June 1997, pp. 292-303. Nair, Ravi: "Dynamic path-based branch correlation," Int. Symposium Microarchitec- ture, December 1995, pp. 15-23. Pan, S. T., K So, J. T. Rahmeh: "Improving accuracy dynamic branch prediction using branch co rrelation," Symposium Architectural Support Programming Lan- guages Operating Systems, October 1992, pp. 12-15. Reches, S., S. Weiss: "Implementation analysis path histor dynamic branch prediction schemes," Int. Conference Supercomputing, July 1997. pp. 285-292. Rosenblatt F.: Principles Neurodynamics: Perceptrons Theory Brain Mecha- nisms. Spartan Books, 1962. Rotenberg, Eric, S. Bennett, James E. Smith: 'Trace cache: low latency approach high bandwidth instruction fetching," Int. Symposium Microarchitecture, December 1996, pp. 24-35. Rotenberg, Eric, Quinn Jacobson, Yiannakis Sazeides, Jim Smith: 'Trace processors," Int. Symposium Microarchitecture, December 1997, pp. 13 8-148. Seznec, Andre, Stephen F elix, Venkata Krishnan, Yiannakis Sazeides: "D esign tradeoffs Alpha EV8 conditional branch predictor," Int. Symposium Computer Architecture, May 2002, pp.. 25-29. Skadron, Kevin, Margaret Martonosi, Douglas W. Clark: "A Taxonomy Branch Mispredictions, Alloyed Pre diction Robust olution Wr ong-History Mispredic- tions," 7nr7 Conference Parallel Architectures Compilation Techniques, September 2001, pp. 199-206. Smith, Jim E.: "A study branch prediction strategies," Int. Symposium Computer Architecture, May 1981, pp. 135-148. Sprangle, Eric, Robert S. Chappell, Mitch Alsup, Yale N. Pan: "The agree predictor mechanism reducing negative branch history interference," Int. Symposium Com- puter Architecture, June 1997, pp. 284-291.516 MODERN PROCESSOR DESIGN Stark, Jared, Marius Evers, Yale N. Patt: "Variable path branch prediction," ACM SIG- PLAN Notices, 33, 11, 1998, pp. 170-179. Sugumar, Rabin A., Santosh G. Abraham: "Efficient sim ulation caches opti- mal replacement applications miss characterization," ACM Sigmetrics, May 1993, pp. 284-291. Tarlescu, Maria-Dana, Kevin B. Theobald, Guang R. Gao : "Elastic history buffer: low-cost method improve branch prediction accuracy," Int. Conference Computer Design, October 1996, pp. 82-87 Tendler, Joel M., J. Steve Dodson, J. S. Fields, Jr., Hung Le, Balaram Sinharoy: "POWER4 ystem microarchitecture," IBM Journal Research Development, 46, 1, January 2002, pp. 5-25. Thomas, Renju, Manoj Franklin, Chris Wilkerson, Jared Stark: "Improving branch pre- diction dynamic dataflow-based identification correlated branches large global history," Int. Symposium Computer Architecture, June 2003, pp. 3 14-323. Uhlig Richard, David Nagle, Trevor Mudge, Stuart Sechrest, Joel Emer: "Instruction fetching: coping code bloat" 77te 22nd Int. Symposium Computer Architecture, June 1995, pp. 345-356. Uht, Augustus K., Vijay Sindagi, Kelley Hall: "Disjoint eager execution: optimal form speculative execution," Int. Symposium Microarchitecture, November 1995, pp. 313-325. Uht, Augustus K.: " Branch effect reduction techniques," IEEE Computer, 30,5, May 1997, pp. 71-81. Yeh, Tse-Yu, Yale N. Patt: 'Two-level adaptive branch prediction," Int. Symposium Microarchitecture, November 1991, pp. 51-61. Yeh, Tse-Yu, Yale N Patt: "Alternative implementations two-level adaptive branch prediction," Int. Symposium Computer Architecture. May 1992, pp. 124-134. Yeh, Tse-Yu, Yale N. Patt: "A comparison dynamic branch predictors use two levels branch history," Int. Symposium Computer Architecture, 1993, pp. 257-266. HOMEWORK PROBLEMS P9.1 Profiling program indicated particular branch taken 53% time. effective following predicting branch why? (a) Always-taken static prediction, (b) Bimodal/Smith predictor, (c) Local-history predictor, (d) Eager execution. State assumptions. P9.2 Assume branch following se quence taken (T) not- taken (N) outcomes: T, T, T, N, N, T, T, T, N, N, T, T, T, N, N prediction accuracy 2-bit counter (Smith predictor) sequence assuming initial state strongly taken? P9.3 minimum local history length needed achieve perfect branch prediction branch outcome sequence used Problem 9.2?ADVANCED INSTRUCTION FLOW TECHNIQUES Draw corresponding PHT fill entry one (predict taken), N (predict not-taken), X (doesn't matter). P9.4 Suppose branches program need 6-bit glo- bal history predictor accurately predicted. advan- tages disadvantages using longer history length? P9.5 Conflict aliasing occurs conventional caches w hen two addresses map line ca che. Adding tags associativity one common ways reduce miss rate caches presence conflict aliasing. advan tages disadvantages adding set associativity branch prediction data structure (e.g., PHT)? P9.6 sense, way make "broken" branch predictor. example, predictor always predicted wrong branch direc- tion (0% accuracy) would still result correct program exec ution, correct branch direction computed later pipe- line misprediction wil l corrected. behavior makes branch predictors difficult debug. Suppose jus invented new branch prediction algorithm implemented processor simulator. particular program, algorithm achieve 93% prediction accuracy. Unbeknownst you, programming error part caused simulated pre- dictor report 95% accuracy. would go verifying correctness branch predi ctor implementatio n (beyond double- checking code)? P9.7 path history example Figure 9.19 showed situation global branch outcome history identical two different pro- gram paths. global path history provide superset information contained th e global branch outcome history? not, describe situation global path result two differ- ent global branch histories. P9.8 proposed hybrid predictors involve combination global- history predictor local-history predictor. Explain benefits, any, combining two global-history predictors (po ssibly differ- ent types like Bi-Mode gskewed, example) hybrid configu-ration. advantage global-global hybrid, explain why. P9.9 branches PC-relative target address, address next instruction taken branch always (not including self- modifying code). hand, indirect jumps may di fferent targets execution. BTB records recent branch target and, therefore, may ineffective predicting frequently changing targets indirect jump. could BTB modified improve prediction accuracy scenario?518 MODERN PROCESSOR DESIGN P9.10 Branch predictors usually assumed provide single branch prediction every cycle. alternative build predictor two-cycle latency attempts predict outcome current branch, next branch well (i.e., provides two predic- tions, every cycle). approach still provides average prediction rate one branch prediction per cycle. Explain benefits shortcomings approach compared conven- tional si ngle-cycle branch predictor. P9.ll trace cache's next-trace predictor relies p rogram repeatedly execute sequences code. Subr outine returns pre- dictable targets, targets frequently change one invocation subroutine next. frequently changing return addresses impact performance trace cache terms hit rates next-trace prediction? P9.12 Traces constructed either processor's front end fetch, back end instruction c ommit. Compare contrast front-end back-end trace construction respect amount time start trace construction wh en trace used, branch misprediction delays, branch/next-trace prediction, performance, interactions rest icroarchitecture. P9.13 Overriding predictors use two different predictors provide quick dirty prediction slower better prediction. scheme could generalized hierarchy predictors arbitrary depth. example, three-level ove rriding hierarchy would quick inaccurate first predictor, second predictor provides somewhat better prediction accuracy moderate delay, finally accurate much slower third predictor. difficulties involved implementing, example, 10-level hierar- chy overriding branch predictors? P9.14 Implement one dynamic branch predictors described chapter processor simulator. Compare branch prediction accu- racy default predictors. P9.15 Devise original branch prediction algori thm implement proce ssor simulator. Com pare branch prediction accuracy known techniques. Consider latency prediction lookup designing predictor. P9.16 processor's branch predictor provides diocre prediction accuracy. make sense implement large instruction window processor? State many reasons implementing larger instruction window situation.Advanced R egister Data Flow Techniques CHAPTER OUTLINE 10.1 Introduction 10.2 Value Locality Redundant Execution 10.3 Fjcplorting Value Locality without Speculation 10.4 Exploiting Value Locality Speculation 10.5 Summary References Homework Problems HBmP) 1 Itfifc li:**^ FR.-Y\^ ; 10.1 Introduction learned, modern processors fun damentally limited performance two program characteristics: control flow data flow. former exam- ined length study advanced instruction fetch te chniques branch prediction, trac e caches, high-bandwidth solutions Flynn's bottleneck [Tjaden Flynn, 1970]. Historically, techniques proved quite effective many widely adopted today's advanced pr ocessor designs. Nevertheless, resolving limitations control flow places processor performance continues extremely important area research advanced development. Chapter 11, revisit issue focus active area research attempts exploit multiple simultaneous flows control overcome bottlenecks caused inaccuracies branch prediction inefficiencies branch resolution. so, however, take closer look performance limits caused program's data flow.520 MODERN PROCESSOR DESIGN Earlier sections alre ady focused resolving performance limitations caused false name dependences program. reader may recall, false dependences caused reuse storage locations progra execution. reuse induced fact programmers compilers must specify temporary operands finite number unique registe r identifiers forced r euse register identifiers available identifiers allo- cated. Furthermore, even instruction set provided luxury unlimited number registers register identifiers, program loops induce reuse storage identifiers, since multiple instanc es single static loop body flight time. Hence, false name dependences unavoidable. learned Chapter 5, underlying techn ique employed resolve false dependences dynamically rename destination operand unique storage location, hence avoid unnecessary serialization multiple writes shared location. process register renaming, first troduced Tomasulo's algorithm IBM S/360-91 [Tomasulo, 1967] late 1960s, detailed Chapter 5, effec- tively removes false dependences allows instructions execute subject true dependences. case branch prediction, tech- nique proved effective, various forms register renaming implemented numerous high-pe rformance processor designs past four decades. chapter, turn attention techniques attempt elevate performance beyond achievable simply eliminating false data depen- dences. processor executes instructions rate limited true data dependences said operating data flow limit. Informally, proces- sor achieved data flow limit eac h instruction program's dynamic data flow graph executes soon source operands become avail- able. Hence, instruction's scheduled execution time determined solely position data flow graph, position defined longest path leads data flow graph. example, Figure 10.1, instruc- tion C executed cy cle 2 true data dependences position instructions B, exec ute cycle 1. Recall data flow graph Data flow limit = 1 .3 Enhanced ILP = 4 Instruction reuse( Predict ~\ financed ILP = 4 s| \f ^^erif^jredicuons^^ Value prediction Data flow execution Figure 10.1 Exceeding Instruction-Level Parallelism (ILP) Dictated Data Flow Limit.ADVANCED REGISTER DATA FLOW TECHNIQUES nodes represent instructions, edges represent data dependences instructions, edges weighted result latency producing instruction. Given data flow graph, compute lower bound program's exe- cution time computing height (i.e., length longest existing path) data flow graph. data flow limit represents lower bound and, turn, determines maximum ac hievable rale instruction execution (or HP), defined number instructions program divided height data flow graph. example, refer simple data flow graph shown left-hand side Figure 10.1, aximum achievable ILP determined data flow limit compute 4 instructions 3 cycles latency longest path graph = 1.3 chapter, focus two techniques—value prediction instruction reuse—that exploit program characteristic termed value locality accelerate processing instructions beyond c lassic data flow l imit. context, value locality describes likelihood program instruction's computed resuit—-or similar, predictable result—will recur later program's continued exe- cution. broadly, value locality programs c aptures empirical obser- vation limited set unique values constitute majority values produced consumed real programs. property analogous tem- poral spatial locality caches memory hierarchies rely on, except describes values themselves, rather heir storage locations. two techniques conside r exploit value loc ality either nonspecula- tively reusing results prior com putation (in instruction reuse) specula- tively predicting results future computation based results prior executions (in value prediction). approaches allow processor obtain results instruction earlier time position data flow graph might indicate, able reduce effective height graph, thereby increasing rate ins truction execut ion beyond data flow li mit. example, shown middle Figure 10.1, instr uction reuse scheme might recognize instructions A, B, C repeating earlier computation could reuse results earlier computation allow instruction exe- cute immediately, rather wait results A, B, C. would result effective throughput four instructions per cycle. Similarly, right side Figure 10.1 shows data valu e prediction scheme could used enhance available instruction-level parallelism meager 1.3 instructions per cycle ideal 4 instructions per cycle correctl predicting th e results instructions A, B, C. Since B predicted correctly, C need wait execute. Similarly, since C correctly predicted, need wait C execute. nce, four instructions execute parallel. Figure 10.1 also illustrates key distinction instruction reuse value prediction. middle case, invoking reuse comple tely avoids execution instructions A, B. C. co ntrast, right, value prediction avoids serializing effect instructions, able prevent heir execution.AMP522 MODERN PROCESSOR DESIGN distinction arises fundamental difference two techniques: instruction r euse guarantees value locality, value prediction predicts it. latter case, processor must still verify prediction executing predicted instructions com paring results predicted sults. similar branch prediction, outcome branch predicted, almost always correctly, branch must still executed verify correctness prediction. course, verification consumes execution bandwidth requires comparison mechanism validating results. Conversely, ins truction reuse provides priori guarantee correctness, verification code needed. However, find Section 10.3.2, guarantee correctness, seemingly attractive, carries baggage increas e imple- mentation cost reduce effectiveness instruction reuse. Neither value prediction instruction reuse, relatively recently troduced Uterature, yet imp lemented real design. However, demon- strate substantial potential improving performance real programs, particu- larly programs true data dependences—as opposed tructural control dependences—place limits achievable instruction-level parallelism. new idea, substantial challenges involved realizing performance po- tential reducing practice. explore challenges iden- tify known realizable solutions require investigation. First, examine instru ction reuse, since roots hist orical well-known pr ogram optimization called memoization. Memoization, performed manually programmer, automatically compiler, technique short-circuiting complex computations dynamically record- ing outcomes computations. Subsequent instances computa- tions perform table lookups reuse results prior computations whenever new instance matches th e preconditions earlier instance. may evident reader, memoization nonspeculative technique, since requires precisely correct preconditions satisfied computation reuse invoked. Similarly, instruction reuse also nonspeculative viewed hardware implementation memoization inst ruction level. Next, examine value prediction, fundamentally different due speculative nature. Rather reusing prior executions instructions, value prediction instead seeks predict outcome future instance instruction, based prior outcomes. respect similar widely used history- based dyn amic branch predictors (see Chapter 5), one significant difference. branch predictors collect outcome histories quite deep (up sev- eral dozen prior instances branches contribute outcome history prediction future instance), information content property predicting small, corresponding single state bit determines whether branch taken. contrast; value predictors attempt ecast full 32- 64-bit values computed gister-writing instructions. Naturally, challenges accurately generati ng predictions require much wider (full operand width) histories additional mechanisms avoiding mispredictions. Furthermore, gen- erating predictions small part implementation challenges required toADVANCED REGISTER DATA FLOW TECHNIQUES 51 realize value prediction's performance potential. branch prediction, mechanisms speculative execution base predicted values well predic- tion verification misprediction recovery, required correct operation. begin discu ssion value locality causes, consider many aspects nonspeculative technique (e.g., instruction reuse) spec- ulative techniques (e.g., value prediction) exploiting value locality. exam- ine aspects techniques detail; show techniques, though seemingly different, actually closely related; also describe two hybridized combining element instruction reuse aggressive implementation value prediction reduce cost prediction verification. 10.2 Value Locality Redundant Execution section, explore concept value locality, define likelihood previously seen value recurring repeatedly within storage loca- tion [Lipasti et al., 1996; Lipasti Shen, 1996]. Although concept general applied storage location within computer system, con- sider value locality general-purpose floating-point registers immediately following instructions write registers. plethora previous work dynamic branch prediction focused even restricted application value locality, namely, prediction single condition bit based past behavior. Many ideas chapter viewed logical continuation body work, extending th e prediction single bit prediction entire 32- 64-bit register. 10.2.1 Causes Value Locality Intuitively, seems would difficult task discover useful amount value loc ality register. all, 32-bit register contain one four billion values—how could one possibly predict even somewhat likely occur next* turns out, narrow scope prediction mechanism considering static instruction individually, task becomes much easier, able accurately predict significant fraction values written register file. makes thes e values predictable? examining number real- world programs, found value locality exists pr imarily real-world programs, run-time environments, operating systems general design. is, implemented handle contingencies, excepti onal conditions, erroneous inputs, occur relatively rarely real life, also often designed future expansion code reuse mind. Even code aggres- sively optimized modern, st ate-of-the-art compilers exhibits tendencies. following empirical observations result examin ation many real programs, help reader understand wh value locality exists:  Data redundancy. Frequently, input sets real-world prog rams contain data little variation. Examples sparse matrices con- tain many zeros, text files white space, empty cells spreadsheets.524 MODERN PROCESSOR DESIGN  Error checking. Checks infreque ntly occurring condi tions often com- pile loads effectively run-time constants.  Program constants. often e fficient generate code load program constants memory code co nstruct ir nmediate operands.  Computed branches. compute branch destination, say switch statement, compiler must g enerate code load register base address branch jump table, often run-time constant.  Virtual function calls. call virtual function, compiler must generate code load function pointer, often ru n-time constant.  Glue code. addressabil ity concerns linkage conventions, compiler must often g enerate glue code calling one compila- tion unit another. code frequently contains loads instruction data addresses remain constant throughout execution program.  Addressability. gain addressability nonautomatic storage, com- piler must load pointers table initialized program loaded, thereafter remains constant.  Call-subgraph identities. Functions procedures tend called fixed, often small, set fun ctions, likewise tend call fixed, often small, set functions. Hence, calls occur dynamically often form identities call graph program. result, loads restore link register well callee-saved registers h ave high value locality.  Memory alias resolution. compiler must conservative stores may alias loads, frequently generate appear redundant loads resolve aliases. loads likely exhibit high degrees value locality.  Register spill code. compiler runs registers, variables may remain constant spilled memory reloaded repeatedly.  Convergent algorithms. Often, value locality caused algorithms programmer chose implement. One common example convergent algorithms, iterate data set global convergence reached; quite often, local convergence occur global convergence, sult- ing redundant computation converged areas.  Polling algorithms. Another example algorithmic choices induce value locality us e polling algorithms instead effi- cient event-driven algorithms. polling algorithm, likely out- come event bein g polled yet occurred, resulting redundant computation repeatedly check ev ent. Naturally, many hese observations su bject particulars instruction set, co mpiler, run-ti environment employed, one could argue could eliminated changes ISA, compiler, run-timeADVANCED REGISTER DATA FLOW TECHNIQUES 5 environment, applying aggressive link-time run- time code optimizations. However, changes improvements slow appear; aggregate effect listed (and other) factors value locality measurable significant today two modern RISC instructio n sets examined, pro- vide state-of-the-art compilers run-tim e systems. worth pointing out, how- ever, value locality particular static loads program significantly affected compiler opti mizations loop unrolling, loop peeling, tail repli- cation, since types transformations tend create multiple instances load may exclusively target memory locations high low value locality. 10.2.2 Quantifying Value Locality Figure 10.2 shows value lo cality load instructions variety bench- mark programs. value locality benchmark measured counting number times static load instruction retrieves value memory Alpha AXP light bars show value locality history depth one, th e dark bars show history depth sixteen. Figure 10.2 Load Value Locality.526 MODERN PROCESSOR DESIGN Register value locality 100.0 r light bars show value locality history depth one, tRe dark bars show history depth four. Figure 10.3 Register Value Locality. matches previously seen value static load, dividing total number dynamic loads benchmark. Two sets numbers shown, one (light bars) history depth 1 (i.e., check matches gainst recently retrieved value), second set (dark bars) hist ory depth 16 (i.e., check last 16 unique values). see even history depth 1, integer programs exhibit load value locality 50% range, extending history depth 16 (along hypothetical perfect mechanism choosing right one 16 values) improve better 80%. means vast majority static loads exhibit little variation values load course program's exe- cution. Unfortunately, three benchmarks (cjpeg, swm256. tomcatv) demonstrate poor load value locality. Figure 10.3 shows average value locality instructions write integer floating-point register benchmarks. Th e value locality static instruct ion measured counting number times instruc- tion writes value matches previously seen value static instruction dividing total number ynamic occurrences instruction. average value loca lity benchmark dynamically weighted average value localities static instructions benchmark. Two sets num- bers shown, one (light bars) history depth one (i.e., check matches recently written value), second set (dark bars) hi story depth four (i.e., check last four unique val- ues). see even history depth one, programs exhibit value locality 40% 50% range (average 51%), extending history depth four (along perfect mech anism choosing r ight one four values) improve 60% 70% range (average 66%). means majority static instructions exhibit little variation theADVANCED REGISTER DATA FLOW TECHNIQUES 52i values write registers course program's execution. again, three ben chmarks— cjpeg, compress, gi«'dt—demonstrate poor register value locality. summary, programs studied here, many others studied exhaus- tively elsewhere, demonstrate significant amounts value locality, load instructions al l register-writing instructions [Lipasti et al., 1996; Lipasti Shen, 1996; 1997; Mendelson Gabbay, 1997; Gabbay Mendelson, 1997; 1998a; 1998b; Sazeides Smith, 1997; Calder et al., 1997; 1999; Wang Franklin, 1997; Bunscher Zorn, 1999; Sazeides, 1999]. Th property independently verified least half-dozen ifferent instruction sets compilers large number workloads including user-state kernel- state execution. 10.3 Exploiting Value Locality without Speculation widespread ccurrence value locality real programs creates opportunities increasing processor performance. already outlined, specula- tive nonspeculative techn iques possible. first describe nonspecu- lative techniques exploiting value locality, since related techniques known long time. recent pro posal reinvigorated interest tech- niques advocating instruction reuse [Sodani Sohi, 1997; 1998; Sodani, 2000], pure hardware techniqu e reusing result prior execution instruction. simplest form, instruction reuse mechanism avoids struc- tural data hazards caused execution instruction whenever discovers identical instructio n execution within history mech anism. cases, simply reuses h istorical outcome saved instruction reuse buffer dis- cards fetched instruction without executing it. Dependent instructions able issue execute immediately, since result available right away. value locality, reuse often possible since many static instructions repeat- edly compute result. 10.3.1 Memoization Instruction reuse roots historical well-known program optimization called memoization. Memoization, performed manually pro- grammer autom atically compiler, technique short-circuiting com- plex computations dynamically recording outcomes computations reusing outc omes whenever possible. example, <operand, result> pair resulting calls function fibonacci(x) shown Figure 10.4 recorded memoization table. Subsequent instances computa- tions perform table lookups reuse r esults prior computations when- ever new instance matches preconditions earlier instance. Continuing example, memoized version fibonacci(x) function checks see current call matches earlier call, returns value earlier call immediatel y, rather executing full routine recompute Fibonacci series sum.528 MODERN PROCESSOR DESIGN /* fibonacci series computation */ int fibonacci(x) { int result = 0; (x==0) result - 0; else (x<3) result = 1; else { result = fibonacci(x-2); result fibonacci(x-1); ) return result; } /* memoized version */ int memoized_fibonacci(x) { (seen_before(x)) return memoized_reBult(x); else { int result = fibonacci(x); memoize(x,result); return result; } } call fibonaccifx), shown left, easily memoized. shown memoized_fibonacci(x) function. call ordered_linked_list( record *x) would difficult memoize due reliance global variables side effect updates global variables. Figure 10.4 Memoization Example./* linked list example */ int ordered_linked_list_insert(record *x) { int position=0; record *c,*p;c-head; (c && (c->data < x->data)) { imposition; p = c; c = c->next,- } (P) { x->next = p->next; p->next = x; } else head - x; return position; } Besides overhead recording checking memoized results, main shortcoming memoization computation memoized must guaranteed free side effects. is, computation must modify global state, rely external modifications global state. Rather, inputs must clearly specified memoization table lookup verify match earlier nstance; outputs, effects rest program, must also clearly specified reuse mechanism perform correctly. Again, simple fibonaccifx) example, input operand x, output Fibonacci series sum corresponding x, mak- ing excellent candidate fo r memoization. oth er hand, procedure ordered_linked_list_insert(record *x), also shown Figure 10.4, would poor candidate memoization, since depends global state (a global head pointer linked list well nodes linked list) modifies global state updating next pointer linked list element. Correct memoization type function would require checking head pointer none elements list changed since previous invocation. Nevertheless, memoization powerful programming technique widely deployed effective. Clearly, memoization nonspecula- tive technique, since requires precisely correct preconditions satisfied reuse invoked.ADVANCED REGISTER DATA FLOW TECHNIQUES 5: 10.3.2 Instruction Reuse Conceptually, instruction reuse nothing hardware implementation memoization instruction l evel. exposes additional instruction-level par- allelism decoupling execution consumer instruction producers whenever finds producers need executed. possible when- ever reuse mechanism finds producer instruction matches earlier instance reuse hist ory able safely reuse results prior instance. Sodani Sohi's initial proposal instruction reuse advocated reuse individual machine instruction whenever operands instruction shown invariant respect prior instance instruction [Sodani Sohi, 1997]. advanced mechanism recording reusing sequences data-depe ndent instructions also described. mechanism stored data dependence relationships instructions reuse history table could automati cally reuse data flow region instructions (i.e., sub- graph dynamic data flow graph) whenever inputs region shown invariant. Subsequent proposals also considered expand- ing reuse scope include basic blocks well instruction traces fetched trace cache (refer Ch apter 5 details trace caches operate). proposals reuse share basic approach: execution individual instruction set instructions recorded history structure stores result computa tion later reuse. set instructions defined either control flow (as basic block reus e trace reuse) data flow (as data flow region reuse). history structure must mechanism guarantees contents remain coherent subsequent program execution. Finally, history structure lookup mec hanism allows subsequent instances checked stored instances. hit match lookup trig gers reuse mecha nism, allows processor skip execution reuse candidates. result, processor eliminates st ructural data dependences caused reu se candidates able fas t-forward subse- quent program instructions. process summarized Figure 10.5. 10.3.2.1 Reuse History Mechanism. implementation reuse must mechanism remembering, memoizing, prior exec utions instructions sequences instructions. history echanism must associate set precondi- tions previously computed result. preconditions must exactly specify computation performed well live inputs, operands affect outcome computation. instruction reuse, computation performed specified program counter (PC) tag uniquely identifies static instruction processor's address space, live inputs regis- ter memory operands static instruction. block reuse, computation specified address range instructions basic block, live inputs source register memory operands live entry basic block. trace reuse, computation corresponds trace cache entry, uniquely dentified fetch address set conditional branch530 MODERN PROCESSOR DESIGN Succeed; preconditions match prior instance I<eusej)riorj-esultJ| instruction fetched, history mechanism checked see whether instruction candidate reuse. so, instructions preconditions match historical instance, historical instance reused fetched instruction discarded. Otherwise, instruction executed always, outcome recorded history mechanism. Figure 10.5 Instruction Reuse. outcomes specify control flow path trace. extension, operands live entry trace must also specified. key attribute preconditions stored reuse buffer uniquely specify set events led compu tation memoized result. Hence, precise set events ever occurs again, computation need p erformed again. Instead, memoized result substituted result repeated computation. However, memoization example Figure 10.4, care must taken pr econditions fact full specify events might affect outcome computation. Otherwise, reuse mech- anism may introduce errors program execution. Indexing Updating Reuse Buffer. history mechanism, reuse buffer, illustrated Figure 10.6. usually indexed low-order bits PC, organized direct-mapped, set-associative, fully associative structure. Additional index information provided including input oper- value bits index and/or tag; approach enables multiple instances static instruction, varying input operands, coe xist reuse buffer. r euse buffer updated dynamically, instructions groups instructions retire execution window; may require multi- ported heavily banked structure accommodate high throughput. areADVANCED REGISTER DATA FLOW TECHNIQUES 53 PC reuse candidateResult V? PC tag SrcOpt SrcOpZ Address PC Source operandaCompare? Register file v7- Reused resultAll stores check formatching addressesand mark invalid. Remote stores multiprocessor system must also invalidate matching entries. instruction reuse buffer stores preconditions required guarantee correct reuse prior instances instructions. ALU branch instructions, includes PC tag source operand values. loads tores, memory address must also stored, intervening writes address invalidate matching reuse entries. Figure 10.6 Instruction Reuse Buffer. also usual design space issues regarding repla cement policy writeback pol- icy (for multilevel history structures), similar design issues caches cache hierarchies. Reuse Buffer Organization. reuse buffer organized store history individual instructions (i.e., entr corresponds single instruction), basic blocks, traces (effectively integrating reus e history race cache), data flow regions. scalability issues related tracking live inputs large num bers instructions per reuse entry. example, basic block history mechanism may store dozen live inputs half many results, given basic block size six instructions, two source operands e destination. Similar scalability problems exist proposed trace reuse mechanisms, attempt reuse entire traces 16 instructions. Imagine increasing width one-instruction-wide structure shown Figure 10.6 accommodate 16 instances columns. Clearly, building wide structures wide comparators checking reuse preconditions presents challenging task. Specifying Live Inputs. Live register inputs reuse entry specified either name value. Specifying name means recording either architected register number register operand address memory operand. Specifying value means recording actual value operand instead name. Either way, live inputs must sp ecified maintain cor- rectness, since failure specify live input lead incorrect reuse, computation reused even though subtle change unrecorded live input532 MODERN PROCESSOR DESIGN could cause di fferent result occur. Sodani Sohi investigated mechanisms specified register operands name value, considered specifying memory operands name. example reuse buffer Figure 10.6 specifies register source operands value memory locations name. Validating Live Inputs. validate live inputs reuse candidate, one must verify inputs stored reuse entry match current architected values operands; process called reuse test. Unless live inputs validated, reuse must occur, ince reused result may correct. named operands, property guaranteed coherence mechanism (explained next) checks program writes reuse buffer. operands specified value, reuse mechanism must compare current architected values reuse entry check match. register operands, involves reading current values th e architected register file comparing values stored reuse entry. Note creates considerable additional demand read ports physical registe r file, since operands reuse candidates must read simul taneously. memory operands sp ecified value, performing reuse test would involve fetching operand values memory order compare them. Clearly, little gained here, since fetching operands memory order compare less work performing memory operation itself. Hence, reuse proposals date specify memory operands name, rather va lue. Figure 10.6, reuse c andidate must fetch source operands register file compare values stored reuse buffer. Reuse Buffer Coherence Mechanism. guarantee co rrectness, r euse buffer must remain coherent program e xecution occurs insertion entry reuse buffer subsequent reuse entry. remain coherent, intervening writes either reg isters memory conflict named live inputs must properly refl ected reuse bu ffer. coherence mechanism responsible tracking writes performed program (or programs running processors multiprocessor system) mak- ing sure named live inputs correspon writes ar e marked invalid r euse structure. prevents invalid reuse occurring cases named live input ha changed. live inputs specified value, rather name, intervening writes need detected, since live input valida- tion compare resulting architected historic values trigger reuse values match. Note named inputs, coherence mechanism must perform associative lookup live inputs r euse buffer e program write. long names ( say, 32- 64-bit memory addresses), associative lookup prohibitively expensive even modest history table sizes. Figure 10.6. stores executed processor must check matching entries reuse buffer must invalidate entry address matches store. Similarly, multiprocessor system, remote writes must invalidate matching entries reuse buffer.ADVANCED REGISTER DATA FLOW TECHNIQUES 5 final note, systems allow self-modifying c ode, coherence mechanism must also track writes instruction addresses tha stored reuse buffer must invalidate matching reuse entries. Failure could result reuse entry longer corresponds current program image. Similarly, semantics instructions used invalidate instruc- tion cache entries (e.g., icbi PowerPC architecture) mus extended also invalidate reuse buffer entries wit h matching tags. 10.3.2.2 Reuse Mechanism. Finally, gain performance benefit reuse, processor must able eliminate reduce data str uctural dependences reused instructions omitting th e execution hese instructions skipping ahead subsequent w ork. seems straightforward, may require nontrivial modifications processor's data control paths. First, reuse candidates (whether individual instructions groups instructions) must inject heir results processor's architected state; since data paths real pro- cessors often allow functional units write results register file, probably involve adding write ports already heavily multiported physical register file. Second, instruction wakeup scheduling logic mod- ified accommodate reused instructions effectively zero cycles result latency. Third, reuse candidates must enter processor's reorder buffer order maintain support precise exceptio ns, must simultaneously bypass issue queue reservation stations; nonstandard behavior introduce additional control pat h complexity. Finally, reused memory instructions must still tracked processor's loa d/store queue (LSQ) maintain correct memory reference orde ring. Since LSQ entries typically updated ins truction issue based addresses generated execution, may also entail additional data paths LSQ write ports llow upd ates occur earlier (prior issue execute) pipeline stage. summary, implementing instruction reuse require substantial redesign modification existing control data p aths modern microprocessor design. requirement may reason r euse yet appeared real designs; changes substantial enough likely incorpo- rated brand-new, clean-slate design. 10.3.3 Basic Block Trace Reuse Subsequent proposals extended Sodani Sohi's original proposal instruction reuse encompass sets instructions defined control flow [Huang Lilja, 1999; Gonzalez et al., 19 99]. proposals, similar mechanisms storing looking reuse history employed, granularity basic blocks instruction traces. cases, control flow unit (either basic block trace) treated atomicalry reusable computation. words, partial reuse due partial matching input operands isallowed. Expanding scope instruction reuse basic blocks traces increases potential benefit per r euse instance, since substantial chunk instructions directly bypassed. How- ever, also decreases likelihood finding matching reuse entry, since the534 MODERN PROCESSOR DESIGN likelihood set half-dozen dozen live inputs identical previous computation much lower likelihood finding individual instructions within groups reused. lso, discu ssed earlier, scalabil- ity issues related c onducting reuse test large numbers live inputs basic blocks traces ha ve. time tell use coarser control-flow granularity prove effective struction-level reuse. 10.3.4 Data Flow Region Reuse contrast su bsequent approaches attempt reuse groups instructions based control flow, Sodani also proposed approach storing reusing data flow regions instructions (the S„+d Sv+d schemes). approach requires bookkeeping scheme embeds pointers reuse buffer connect" data-dependent instructions. pointers th en traversed r euse entire subgraphs data flow graph; possible since r euse propert transi- tive respect data flow graph. formally, instruction whose data flow tecedents reuse candidates (i.e., satisfy reuse test) also reuse candidate. applying principle inductively, r eusable data flow region constructed, resulting set connected instructions reusable. reusable region constructed dyn amically following th e data dependence pointers embedded reuse table. Dependent instructions con- nected edges, successful reuse test results ar e propagated along edges depend ent instructions. reuse tes dependent instructions simply involves checking live input operands originate instructions reused otherwise pass reuse test. condition satisfied, meaning al l operands found invariant originate reused antecedents, depe ndent instructions reused. reuse test performed either name (in S„+d scheme) value (in Sv+d scheme). Maintaining tegrity data dependence pointer presents difficult challenge dynamically managed structure: Whenever entry reuse buffer replaced, pointers entry become stale. stale pointers must found removed prevent sub sequent accesses reuse buffer resulting incorrect transitive propagation usability . Sodani proposed associative lookup mechanism automatically invalidates pointers every replacement. Clearly, expense complexity associative l ookup- coupled frequent replacement prevent scalable solution. Alternative schemes store dependence pointers separate, smaller stru cture feasibly support associative lookup als possible, though unex- plored current literature. Subsequent work Connors Hwu [1999] prop oses implementing region- level reuse strictly software modifying compiler generate code performs reuse test data flow regions constructed compiler. approach checks live input operands invokes region reuse omitting exe- cution region immediately wri ting results architected state whenever matching history entry found. fact, work takes us full circleADVANCED REGISTER DATA FLOW TECHNIQUES 5 back software-based memoization techn iques establishes automated, profile-driven techniques memoization indeed feasible desirable. 10.3.5 Concluding Remarks summary, various schemes reus e prior computation proposed. proposals conceptually similar well-understood technique memoization vary primarily granularity reuse details imple- mentation. rely program characteristic value locality, since with- it, likelihood identifying reuse candidates would low. Reuse techniques adopted real designs date; yet show signif- icant performance potential implementation challenges success- fully overcome. 10.4 Exploiting Value Locality Speculation considered nonspec ulative techniques ex ploiting value locality enhancing instruction-level parallelism, address speculative techniques same. delving details value prediction, step back consi der theoretical basis speculative execution—the weak depen- dence model [Lipasti Shen, 1997; Lipasti, 1997]. 10.4.1 Weak Dependence Model learned study techniques removing false dependences, implied inter-instruction preced ences sequential program over- specification need rigorously enforced meet requirements sequential execution model. actual pro gram semantics nter-instruction dependences specified co ntrol flow graph (CFG) data flow graph (DFG). long serialization constraints imposed CFG theDFG violated, execution instructions overlapped reordered achieve better performance avoiding enforcement implied unnecessary precedences. achieved Tomasulo's algo- rithm recent, modem order-buffer-based implementations. ever, true inter-instruction dependences must till enforced. date, machines enforce dependences rigorous fashion involves following two requirements:  Dependences deter mined absolute exact way; is, two instructions id entified either dependent dependent, doubt, dependences pessimistically assumed exist  Dependences enforced throughout instruction execution: is, de- pendences never allowed violated, enforced continuously instructions flight. traditional conservative approach program execution described ad hering strong dependence model. traditional strong536 MODERN PROCESSOR DESIGN dependence model overly rigorous unnecessarily restricts available parallel- ism. al ternative model enables aggre ssive techniques value prediction weak dependence model, specifies that:  Dependences need determined exactly assumed pessimistically, instead optimistically approximated even temporarily ignored.  Dependences temporarily violated instruction e xecution long recovery performed prior affecting perm anent machine state. advantage adopting wea k dependence model program seman- tics specified CFG DFG need com pletely determined machine process instructions. Furthermore, machine specu late aggressively temporarily violate depe ndences long corrective mea- sures place recover misspeculation. significant perce ntage speculations correct, machine effectively exceed performance limit imposed traditional strong dependence model. Conceptually speaking, machine tha exploits weak dependence model two interacting engines. Th e front-end engine assumes weak dependence model highly speculative. tries make predictions instructions order aggressively process instructions. predictions co rrect, speculative instructions e ffectively skipped folded certain pipeline stages. back-end engine still uses strong dependence model validate speculations, recover misspeculation, provide history guidance information speculativ e engine. combining two inter- acting engines, unprecedented leve l instruction-level parallelism har- vested without violating program semantics. edges DFG represent inter-inst ruction dependences enforced critical path misspeculations occur. Essentially, hese dependence edges become probabilistic erialization penalties incurred due enforcing depen- dences eliminated masked whenever correct speculations occur. Hence, traditional data flow limit based length critical path DFG longer hard limit cannot exceeded. 10.4.2 Value Prediction learned Section 10.2.2 register writes many programs demonstrate significant degree value locality. discovery opens exciting new possibil- ities microarchitect. Since results many instructions ac curately predicted issued executed, dependent instructions longer bound serialization constraints imposed operand data flow. Instructions scheduled speculatively additional degrees freedom better utilize existing functional units hardware buffers frequently able complete execution sooner since critical path dependence graphs collapsed. However, order exploit value locality reap benefits, variety hardware mechanisms must implemented: one accurately predicting values (the value prediction unit); microarchitectural support executingADVANCED REGISTER DATA FLOW TECHNIQUES 537 speculative values; mechanism verifying value predictions; finally recovery mechanism restoring correctness cases wher e incorrectly predicted values int roduced program's execution. 10 A3 Value Prediction Unit value prediction unit responsible ge nerating accurate predictions speculative consumption processor c ore. two competing factors determine efficacy value prediction unit accuracy coverage; third factor related cov erage predictor's scope. Accuracy measures pre- dictor's ability avoid mispredictions, coverage measures th e predictor's ability predict many instruction outcomes possible. predictor's scope describes set instructions predictor targets. Achieving high accuracy (e.g., mispredictions) generally implies trading coverage, since scheme tha eliminates mispredictions likely also eliminate correct pre- dictions. Conversely, ac hieving high coverage likely reduce accuracy reason: Aggr essively pursuing every prediction opportunity likely result larger number mispredictions. Grasping tradeoff accuracy coverage easy consider /, two extreme cases. e extreme, predictor achieve 100% coverage indiscriminately predicting instructions; result poor accuracy, since many instructions inherently unpredictable mispredicted. extreme, predictor achieve 100% accuracy predicting instructions eliminating mispredictions; course, result 0% coverage since none predictable instructions predicted either. designer's challenge find point two extremes provides high accuracy high coverage. Limiting scope value predictor focus par ticular class instructions (e.g., load instructions) dynamically statically deter- mined subset make easier improve accuracy and/or coverage sub- set, particularly fixed implementation cost budget. Building value prediction unit achieves right balance accuracy coverag e requires careful tradeoff analysis must consider performance effects variations coverage (i.e., proportional variation freedom sched- uling instructions execution changes height dynamic data flow graph) variations accuracy (i.e., fewer frequent mispredic- tions). analysis vary depending minute structural timing details microarchitectur e considered requires detailed register-transfer-level simulation correct tradeoff analysis. analysis complicated fact greater coverage always result better performance, since relatively small subset predictions actually critical per formance. Simi- larly, improved accuracy may improve performance either, since mispre- dictions eliminated may also critical p erformance. recent study Fields. Rubin, Bodik [2001] quantitatively demonstrates directly measuring critical path program's execution showing relatively correct value pr edictions actually remove edges along critical538 MODERN PROCESSOR DESIGN path. suggest limiting value predictor's scope thos e instructions critical (i.e., longest) path program's data flow graph. 10.4.3.1 Prediction Accuracy. naive value prediction scheme would simply endorse possible predictions generated prediction scheme supply speculative operands execution core. However, published reports shown, value predictors vary dramatically heir ccuracy, times provid- ing 18% correct predictions. Clearly, naive consumption incorrect predictions intellectuall unsatisfying; lead performance prob- lems due misprediction penalties. theoretically possible implement misprediction recovery schemes direct performance penalty, practical difficulties likely preclude schemes (we discuss one possible approach Section 10.4.4.5 heading Data Flow Eager Execution). Hence, beginning initial proposal value prediction, researchers described confi- dence estimation te chniques improving predictor accuracy. Confidence Estimation. Confidence estimation techniques associate confi- dence level value prediction, used filter incorrect predic- tions improve predictor accuracy. prediction ex ceeds confidence threshold, processor core actually consume predicted value. not, predicted value ignored executi proceeds nonspeculatively, forc- ing dependent operations wait producer finish computing result. Typically, confidence levels established history mechanism incre- ments counter every correct prediction decrements resets counter every incorrect prediction. Usually, counter associated every entry value prediction unit, although multiple counters per entry multi- ple entries per counter also studied. classification table shown Figure 10.7 simple example confidence estimation mechanism. design space confidence estimators explored quite extensively literature date quite similar design space dynamic branch pre- dictors (as discussed Chapter 5). Design p arameters include choice single multiple levels history; indexing prediction outcome history, PC value, hashed combination; number states transition functions predictor entry state machines; on. Even relatively simple confidenc e esti- mation scheme, one described Figure 10.7, provide prediction accuracy eliminates 90% mis predictions sacrificing less 10% coverage. 10.4.3.2 Prediction Coverage. second factor meas ures e fficacy value prediction unit prediction coverage. simple value predictors initially proposed simply remembered previous value produced part icular static instruction. example last value predictor shown Figure 10.7. Every time instruction ex ecutes, value prediction table (VPT) updated result. part date, confidence l evel classification table incremented prior value mat ched actual outcome, decremented otherwise. next time static instruction fetched, previous value isADVANCED REGISTER DATA FLOW TECHNIQUES 53 Classification table (CT) <v> <pred history>PC predicted instructionValue prediction table (VPT) <v> <value history> Prediction outcome-v7 Predicted valueUpdated value internal structure simple value prediction unit (VPU). VPU consists two tables: classification table (CT) value prediction table (VPT), direct-mapped indexed instruction address (PC) instruction predicted. Entries CT con tain two fields: valid field, consists either sin gle bit indicates valid entry partial complete tag field matched upper bits PC indicate valid field; prediction history, saturating counter 1 bits. prediction history incremented decremented whenever prediction correct incorrect, respectively, used classify instructions either predictable unpredictable. classification used decide whether result particular instruction hould predicted. Increasing number bits saturating counter adds hysteresis classification process help avoid erroneous classifications ignor ing anomalous values and/or destructive interference. Figure 10.7 Value Prediction Unit. retrieved along current confidence level. confidence level exceeds fixed threshold, predicted value used; otherwise, discarded. Simple last value predictors provide roughly 40% coverage set general- purpose programs. Better coverage obtained phisticate predic- tors either provide additional context allow predictor choose fro multi- ple prior values (history-based predictors) able detect predictable sequences compute future, previously u nseen, values (computational predictors). History-Based Predictors. simplest history-based predictors remeniber recent value written part icular static ins truction predict value computed next dynamic stance instruction. sophisticat ed predictors pro vide means storing multiple different val- ues static instruction, use scheme choose one values predicted one. example, last-n value predictor proposed Burtscher Zorn [1999] uses scheme prediction outcome histories choose one n values stored value prediction table. Altern atively, finite-context- method (FCM) predictor proposed Sazeides Smith [1997] also stores multi- ple values, chooses one based finite context recent values observed program execution, rather strictly PC value. value context analogous branch outcome contex captured branch history register is540 MODERN PROCESSOR DESIGN used su ccessfully implement two-level branch predictors. FCM scheme able capture periodic sequences values, set pointer addresses loaded traversal linked list. FCM predictor sho wn reach prediction coverage excess 90% certain workloads, albeit considerable implemen- tation cost storing multiple values contexts. Computational Predictors. Computational predictors attempt capture pre- dictable pattern sequence values generated static instruction compute next instance sequence. fundamentally ifferent history-based predictors since able generate predicted values occurred prior program execution. Gabbay Mendelson [1997] first pro- posed stride predictor detects fixed stride value sequence able compute next value adding observed stride prior value. stride predictor requires additional hardware: detect strides must use 32- 64-bit subtraction unit extract stride comparator check extracted stride previous stride instance; needs additional space value predic- tion table store stride value additional confidence estimation bits indicate valid stride; and, finally, needs adder add prior value stride compute new predic tion. Stride prediction quite effective certain workloads; however, clear additional storage, arithmetic hardware, complexity justified. advanced computational predictors en discussed, none formally proposed date. Clearly, continuum design space computational predictors tw extremes history-based prediction computational ability full-blown preexecution, archi- tected state made available context predict or, simply antici- pates semantics actual program preco mpute results. latter extreme obviously neither practical useful, since simply replicates functionality proc essor's execution core, interesting question remains whether useful middle ground least subset pro- gram computation abstracted point computational pre dictor reasonable cost able replicate high accuracy. Clearly, sophisticated branch predictors able abstract 95% many programs' control flow behavior; whether sophisticated computational value predictors ever reach goal program's data flow remains open question. Hybrid Predictors. Finally, analogous hybrid combining branch predic- tors described Chapter 9, various schemes combine multiple heterogeneous predictors single whole proposed. hybri prediction scheme might combine last value p redictor, stride p redictor, finite-context predictor attempt reap benefits each. Hybrid predictors enable bet- ter overall coverage, also llow efficient smaller implementations advanced prediction schemes, since ca n targeted subset static instructions require them. effective hybrid predictor proposed Wang Franklin [1997].ADVANCED REGISTER DATA FLOW TECHNIQUES 54 Implementation Issues. Several studies examined various implementation issues value prediction units. hese issues encompass size, organization, accessibility, sensitivity update la tency value prediction structures, difficult solve, particularly complex computational hybrid predictors. general, solutions clever hash f unctions indexing tables banking structure enable multiple simultaneous accesses shown work well. recent proposa l shifts complex value predictor access completion time, stores results access simple, dir ect- mapped table directly trace cache entry, able shift much access complexity away timing-critical front end processor pipeline [Lee Yew, 2 001], Another intriguing proposal refrains storing values separate history structure instead predicting needed value already register file, toring pointer appropriate register [Tullsen Seng, 1999]. Surprisingly, approach works reasonably well, especially compiler allocates register names knowledge values stored registers. 10.4.3.3 Prediction Scope. final factor determining ef ficacy value prediction unit intended prediction scope. initial proposal value predic- tion focused strictly load instructions, limiting scope subse instructions generally perceived critical performance. Reducing load lat ency predict- ing speculatively consuming values returned loads shown improve performance reduce effect str uctural hazards highly con- tended cache ports, increase memory-level parallelism allowing loads would normally blocked data flow-antecedent cache miss execute parallel miss. majority proposed prediction schemes target register-writing instruc- tions. However, interesting exceptions. Sodani Sohi [1998] point register contents directly used resolve conditional branches probably predicted, since value predictions usually less accurate tailored predictions made today's sophisticated branch predic- tors. issue sidestepped initial value prediction work, used PowerPC ins truction set arc hitecture, co nditional branches resolved using dedicated condi tion registers. Since general-purpose registers pre- dicted, th e detrimental effect value mispredictions misguidedly overriding cor- rect branch predictions kept minimum instruction sets similar MIPS PISA (used Sodani's work), condition registers, scheme value predicts general-purpose registers also predict branch source operands directly adversely affect branch resolution. Several researchers proposed focusing value predictions data dependences deemed critical performance [Calder et al., 1999]. several benefits: extra work useless predictions avoided; predictors better accuracy coverage lower imple mentation cost devised; mispredictions occur useless predictions r educed eliminated. Fields, Rubin, Bodik [2001] demonstrate many benefits in542 E R N PROCESSOR DESIGN recent proposal deriving data dependence c riticality novel approach monitoring out-of-order instruction execution. 10.4.4 Speculative Execution Using Predicted Values instruction reuse, value prediction requires microarchitectural support taking advantage eariy avaiiability instruction results. However, fundamental ifference required support due speculative nature value prediction. Since instruction reus e preceded reuse test guarantees correctness, microarchitectural changes outiined Section 10.3.2.2 consist primarily add itional" bandwidth bookkeeping structures within -of- order superscalar processor. contrast, value prediction—an inherently speculative technique—requires pervasive support microarchitecture handle detection recovery misspeculation. Hence, value prediction implies microarchitectural support value-speculative execution, verifying predictions, misprediction recovery. wiil first describe inimal approach sup- porting value-specu lative execution; discuss advanced verifica- tion recovery strategies. 10.4.4.1 Straightforward Value Spe culation. first glance, seems spec- ulative execution usin g predicted values maps quite naturally onto structures modern out-of-order superscalar proc essor aiready provides. First ali, support vaiue speculation, need echanism storing forwarding predictions from" value prediction unit dependent instruct ions: existing rename buffers rename registers erve purpose quite well. Second, need mechanism issue dependent instructions speculatively; standard out-of-order issue logic, minor modifications, work purpose well. Third, need mech- anism detecting mispredicted values. obvious solution augment res- ervation stations hold predicted output values instruction, provide additional data paths reservation station functionai unit output comparator checks values equality signals misprediction comparison fails. Finally, need way recover mispredictions. treat value mispredictions way treat branch mispredictions, sim- ply recycle branch misprediction recovery mechanism fl ushes specula- tive instructions refetches ali instructions foilowing misp redicted one. Surprisingly, min imal modifications sufficient correctness unipro- cessor system,1 even provide nontrivial speedup long predictor highly accurate misprediction relatively rare. However, sophisticated verification recovery techniques iead higher-perfo rmance designs, require additional complexity. discus techniques following. 'A recent publication discusses sufficient cache-coherent multiprocessor: essentially, value prediction removes natural reference ordering data-dependent loads allowing depen- dent load execute preceding load computes address: multiprocessor programs rely dependence ordering correctness fail naive value prediction scheme described here. interested reader referred artin et al. [200 11 details.ADVANCED R EGISTER DATA FLOW TECHNIQUES 543 10.4.4.2 Predictio n Verification. Prediction verification analogous reuse test guarantees correctness fo r instruction reuse. words, must guaran- tee predicted come value-predicted instruction matches actual outcome, det ermined architected state semantics instruction. straightforward approach verific ation execute predicted instruc- tion compare outcome execution value prediction. Naively, implies appending ALU-width comparator functional unit verify predictions. Since latency comparator e quivalent delay ALU, proposals assumed extra cycie latency deter- mine whether misprediction occ urred. Prediction veri fication serves two purposes. first trigger recovery action whenever misprediction occurs; possible recovery actions discussed Section 10.4.4.4. second purpose subtle occurs misprediction: fact c orrect prediction verified may need communicated dependent instructions executed speculatively using prediction. Depending recovery model, speculativ ely executed instruc- tions may continue occupy resources within processor window found nonspeculative. example, conventional out-of-order micropro- cessor, instructions enter issue queues reservation stations program order. issued executed, data control path enables piacing back issue queue reissue. microarchitecture, instruction consumed predicted source operand issued speculatively wouid need remain issue queue reservation station case needed reissue future corrected operand. Since issue queue slots important performance-critical hardware resource, timely notification fact instruction's input operands mispredicted important reducing structural hazards. mentioned, straightforward approach misprediction de tection wait predicted instruction's operands available ex ecuting instruction comparing result predicted result. problem approach instruction's operands may speculative (that is, producer instructions may value predicted, or, subtly, data flow antecedent producer instructions may value predicted). Since speculative input operands beget speculative outputs, single predicted value propagate transitively data flow graph distance limited size processor's instruction window, cr eating wave front speculative operand values (see Figure 10.8). speculative operand turns incorrect, verifying instruction's prediction incorrect operand may cause verification succeed fail succeed. Neither correctness issue; former case caught since incorrect input operand eventually detected misprediction caused verified, latter case cause unnecessary invocations recov- ery mechanism. However, fo r reason, latter cause performance problem, since correctly executed instructions ar e reexecuted unnecessarily.544 MODERN PROCESSOR DESIGN speculative operand wavefront traverses die dynamic data flow graph result predicted* outcome instruction P. consumers CI C2 propagate speculative property consumer* C3, C4, C5, on. Serial propagatio n prediction verification status propagates data flow graph similar manner. Parallel propagation, requires tag broadcast mechanism, allows speculatively executed dependent instructions notified verification status single cycle. Figure 10.8 Speculative Operand Wavefront. Speculative Veri fication. similar problem arises speculative operands used resolve branch instructions. scenario, correctly predicted branch resolved incorrectly due incorrect value prediction, r esulting branch mis prediction redirect. straightforward solution two prob- lems disallow prediction verification (whether value branch) specula- tive inputs. hortcoming solution performance opportunity lost whenever correct speculative input would appropriately resolved mispredicted branch corrected valu e misprediction. definitive answer th e importance performance effect; however, recent trend toward deep execution pipelines performance-sensitive branch mispredictions would lead one believe implementation decision delays resolution incorrectly predicted branches wrong one. Propagating rification Results. additional compli cation, order delay verifica tion input operands nonspeculative, must mechanism place informs instruction whether input operands verified. simp lest form, echanism simply reorder buffer (ROB); nstruction becomes oldest ROB, infer itsADVANCED REGISTER DATA FLOW TECHNIQUES 5* data flow antecedents verified, also verified. However, delay- ing verification instruction next commit negative performance implications, particularly mis predicted conditional branch instructions. Hence, mechanism propag ates verification status operands data flow graph desirable. Two fundamental design alt ernatives exist : verification sta- tus propagated serially, along data dependence edges, instructions verified; broadcast parallel. Serial propagation piggybacked existing broadcast result used wake dependent instructions -of- order execution. Parallel broadcast expensive, implies tagging oper- values speculative data flow antecedents, broadcasting tags predictions verified. Parallel broadcast significant la tency benefit, since entire dependence chains become nonspeculative cycle following verifi cation long-latency instruction (e.g., cache iss) head chain. discussed, instantaneous commit reduce structural hazards freeing issue queue reservation station slots right away, instead waiting fo r serial propagation data flow graph. 10.4.4.3 Data Flow Region Verification. One interes ting opportunity improv- ing effici ency value prediction verification arises concept data flow regions. Recall data flow gions subgraphs data flow graph defined set instructions reachable set live inputs. proposed Sodani Sohi [1997], data flow region reused en masse set live inputs region meets reuse test. pr operty also exploited verify correctness value predict ions occur data flow gion. mechanism similar e described Section 10.3.4 integrated value prediction unit construct data flow regions toring data dependence pointers value prediction table. Subsequent invocation value predictions self-consistent data flow region leads reduction verification scope. Namely, long data flow region mechanism guarantees predictions within region consistent other, initial predictions correspond live inputs data flow region need verified. initial pred ictions verified via conventional means, entire data flow region known verified, remaining instructions region need ever executed verified. approach strikingly similar data flow region reuse, requires quite similar mechanisms value prediction table construct data flow region information guarantee consistency (these issues discussed greater detail Section 10.3.4). However, one fundamenta l difference: data flow region reuse requires live inputs data flow region either unperturbed (if reuse test performed name) unchanged available register f ile (if reuse test performed value). Integrating data flow regions value prediction, however, avoids limitations deferring reuse test indefinitely, live inputs available within processor's execution win- dow. live inputs verified, entire data flow region notified nonspeculative status retire without ever executing. This546 MODERN PROCESSOR DESIGN significantly reduce structural dependences contention functional units programs reusable data flow regions make significant portion instructions executed. 10.4.4.4 Misprediction Recovery via Refetch. two approaches recovering value mispredictions: refetch selective reissue. already men- tioned, refetch-based recovery builds branch misprediction recovery mecha- nism present almost ev ery modern superscalar processor. approach, value mispredictions treated exactly branch mispredictions: instructions follow mispre dicted instruction program order flushed processor, instruction fetch redirected refetch instruct ions. architected state r estored instruction boundary following mispredicted instruction, refetched instructions guaranteed polluted mispredicted values, since mispredicted values survive refetch. attractive featur e refetch-base misprediction recovery requires changes processor, assuming mechanism already place redirecting mispredicted branches. hand, obvious drawback misprediction penalty quite severe. Studies shown processor refetch policy recovering fro value mispredictions, highly accurate value prediction quirement gaining performance benefit. Without highly accurate value prediction—usually brought high- threshold confidence mechanism—performance fact degrade due excessive refetches . Unfortunately, high-threshold confidence mechanism also inevitably reduces prediction coverage, resultin g processor design fails capture potential performance benefit value prediction. 10.4.4.5 Misprediction Recovery via Selective Reissue. Selective reissue pro- vides potential solution performance limitations refetch-based recovery. selective reissue, instructions dat dependent mispre- dicted value required reissue. Implementing selective rei ssue requires mech- anism propagating misprediction information data flow graph dependent instructions. case propagating rification information, reissue informa tion also propagated serially parallel. serial mecha- nism easily piggyback existing result bus used wake depen- dent instructions out-of-order processor. serial propagation, delay communicating reissue condition proportional data flow distance misprediction instruction must reissue. conceivable, although unlikely, reissue message never catch speculative operand wavefront illustrate Figure 10.8, since propagate data flow graph rate one level per cycle. Furthermore, even reissue mes- sage eventually reach speculative operand wavefront, serial propagation delay reissue ssage cause excessive wasted execution along specu- lative operand wavefront. Henc e, researchers also proposed broadcast-based mechanisms communicate reissue commands parallel dependent instructions.ADVANCED GISTER DATA FLOW TECHNIQUES 543 parallel mechanism, speculative value-predicte operands pro- vided unique tag, depende nt instructions execute operands must p ropagate tags dependent instructions. mispre- diction, tag corresponding mi spredicted operand broadcast data flow descendants realiz e must reissue reexecute new operand. Figure 10.9 illustrates possible implementation value prediction parallel- broadcast selective reissue. Misprediction Pen alty Selective Reissue. refetch-based mispredic- tion recovery, misprediction penalty comparable branch misprediction penalty run dozen cycles recent processors deep Fetch Disp Exec Complete/ VerifyPredicted CT PC VPT Dispatch buffer Reservation stationDependent Predict? XX7 Result bus Completion bufferDispatch buffer j - <—f=\ T~t i— ~\ReleaseV— > DataReservation station Reissue XX7 Invalidate Completion buffer - Actual value - «- Predicted v alue dependent instruction shown right uses predicted result instruction left, able issue execute same.cycle. VP Unit predicts values fetch dispatch, forwards speculatively subsequent dependent instructions via rename buffer. dependent instruction able issue execute immediately, prevented completing architecturally retains possession r eservation station inputs longer speculative. Speculatively forwarded values tagged uncommitted register writes depend on, tags propagated results subsequent dependent instructions. Meanwhile, predicted instruction executes right, predicted value verified comparison actual value. prediction verified, tag broadcast active instructions, dependent instructions either release reservation stations proceed completion unit (in case correct prediction), restart execution correct register values (if prediction incorrect). Figure 10.9 Example Value Prediction Selective Reissue.iTiT tlx K J>48 MODERN PROCESSOR DESIGN pipelines. goal selective reissue mitigate penalty reducing number cycles elapse determining mis prediction occurred correcdy re-executing data-dependent instructions. Assuming single addi- tional cycle prediction verification, apparent best case would single cycle misprediction penalty. say, dependent instruction executes., one cycle later would value prediction. penalty occurs dependent instruction already executed speculatively w aiting reservation station one predicted inputs verified. Since value comparison takes extra cycle beyond pipeline result latency, dependent instruction reissue execute correct value one cycle later would en prediction. addition, earlier incorrect speculative issue may caused structural hazard pre-. vented useful instructions dispatching executing. cases, dependent instruction yet executed (due structural unresolved data dependences), penalty, since dependent instruction issue soon actual computed value available, parallel th*i value comparison rifies prediction. Data Flow Eager Execution. possible reduce misprediction penalty' zero cycles employing data flow eager execution. scheme, dependent instruc tion speculatively re-executed soon nonspeculative operand becomes available. words, second shadow issue dependent instruc tion earlier speculative one. parallel second issue, prediction verified, case correct prediction,' shadow issue squashed. Otherwise , shadow ssue allowed continue,' execution continues value prediction never occurred, effec- tively zero cycles misprediction penalty. course, data flow eager shadow issue instructions depend value pre dictions consumes significant" additional execution resources, potentially overwhelming available functional'' units slowing computation. However, given wide machine suffi- cient execution resources, may viable alternative reducing mispre- diction penalty. Prediction confidence could also used gate data flow eager ; execution. cases prediction confidence high, eager execution dis-' abled; cases confidence low, ea ger execution used mitigate misprediction penalty. Effect Scheduling Latency. canonical out-of-order processor implements modern equivalent Tomasulo's algorithm, instruction scheduling decisions made single cycle immediately preceding actual execution instructions selected execution. scheme allows scheduler react immediately dynamic events, dete ction store-to-load aliases cache misses, ssue alternative, independent instructions subsequent cycles. However, cycle time cons traints led recent esigns abandon thi property, resulting instruction sched ulers create executio n schedule several cycles advance actual execution. effect, called scheduling latency, inhibits scheduler's ability react dynamic events. course, value mispredictionADVANCED REGISTER DATA FLOW TECHNIQUES 549 detection dynamic event, fact several modern processor designs (e.g., Alpha 21264 Intel Pentium 4) multicycle scheduling la tency nec- essarily increase value misprediction penalty machines. short, value misprediction penalty fact sum scheduling latency veri- fication laten cy. Hence, processor three-cycle scheduling latency one- cycle verification latency would value misprediction latency four cy cles. However, even desi gns possible reduce th e misprediction penalty via dataflow eager execution. course, likelihood execution resources overwhelmed approach increases scheduling latency, since number eagerly executed squashed instructions proportional latency. 10.4.4.6 Imp lications Selective Reissue Memory Data Dependences. Selective reissue require data-dependent in- structions reissue following value misprediction. fairly straight- forward identify register dat dependences issue dependent instructions, memory data dependences cause subde problem. Namely, memory data dependences de fined register values mselves; register values prove incorrect due value misprediction, memory data dependence information may need reconstructed guarantee correctnes determine additional instructions fact dependent value misprediction. e xample, mispredicted value used either directiy indirectly compute address load store instruction, load/store queue alias reso- lution mechanism within processor may incorrectly concluded load store al iased store load within proces- sor's instruction window. cases, care must taken ensure memory dependence information recomputed load store whose address polluted value misprediction. Alternatively, processor disallow use value-predicted operands address generation loads stores. course, severely limit ability value prediction improve memory-level parallelism. Note problem occur refetch recovery policy, since memory dependence information ex plicitly recomputed instruction following value misprediction. Changes cheduling Logic. Reissuing instructions requir es nontrivial changes scheduling logic conventional processor. normal operation, instructions issue one time, input operands become available functional unit availab le. However, selective reissue, instruction may hav e issue multi- ple times, specu lative operands corrected operands. practical out-of-order implementations partition the, active instruction window two disjoint sets: instructions waiting issue (these instructions till reser- vation stations issue queues), instructions issued waiting retire. partitioning driven cycl e-time demands limit total number instructions considered issue single cycle. Since instructions already issued need considered reissue, moved reserva- tion stations ssue queues second partition (in structions waiting retire).550 MODERN PROCESSOR DESIGN Unfortunately, selective reissue , clean partition longer possible, since instructions issued speculative operands may need reissue, hence le ave issue queue reservation station. two solu- tions problem: either remove speculatively issued instructions res- ervation stations, pr ovide additional mechanism reinsert need reissue; keep reservation stations input operands longer speculative. former solution introduces significant additional complexity front-end control data paths must also deal pos- sible deadlock scenario. One scenario occurs reservation station entries full newer instructions data dependent older instruction needs reinserted reservation station reissue. Since reservation stations available, none ever become available since newer instructions waiting older instruction, older instruction cannot make forward progress, never r etire, leading de adlocked sys- tem. Note refetch- based recovery problem, since newer data-dependent instructions flushed reservation stations upon misprediction recovery. Hence, latter solution forcing speculatively issued instructions retain reservation stat ion entries proposed often. course, approach requires mechanism promoting speculative operands nonspeculative status. parallel serial mechanism like ones described Section 10.4.4.2 suffice purpose. addition complexity introduced track verification status operands, solution additional slight problem increases occup ancy reservation station entries. Without value predic- tion, dependent struction releases reservation station cycle issues, cycle fol lowing computation last input operand. proposed scheme, even though instruction may issue much earlier value-predicted operand, reservation station occupied one additional cycle beyond operand availability, since entry released predicted operand verified, one c ycle later computed. Existing Support Data Speculation. Note existing processors implement value prediction, support forms data speculation (for example, speculating load al iased prior store), may already support limited form selective rei ssue. Intel Pentium 4 one processor implements selective reissue recover cache hit speculation; here, data cache access forwarded dependent instructions tag match validates cache hit completed. tag mismatch, dependent instructions selectively reissued. kind selective reissue scheme already exists, also used support value misprediction recovery. How- ever, likelihood able reuse ex isting mechanism reduced fact existing mechanisms selective reissue often tailored specula- tive conditions resolved within small number cycles (e.g., tag mis- match alias resolution). fact speculation window extends cycles allows speculative operand wavefront (see Figure 10.8) propagateADVANCED REGISTER DATA FLOW TECHNIQUES 5: levels data flow graph, turn limits total number instructions issue speculatively. selec tive reissu e mecha- nism exploits property somehow restricted handling small number dependent operations, useful value prediction, since speculation window value prediction extend tens hundreds cycles (e.g., load misses cache value predicted) encompass processor's entire instruction window. However, converse hold: pro- cessor im plements selective reissue support value prediction, th e mecha- nism reused support recovery forms data speculation. 10.4.5 Performance Value Prediction Numerous published studies examined performanc e potential value prediction. results varied widely, reported performance effects ranging minor slowdowns sp eedups 100% more. Achievable perfor- mance depends heavily many factors already mentioned, including par- ticular details machin e model pipeline structure, well workload choice. factors affecting performance  degree value locality present programs workloads.  dynamic dependence distance corre ctly pr edicted instructions instructions consume results. compiler already scheduled dependent instructions far apart, redu cing result latency value prediction may provide much benefit.  instruction fetch rate achieved machine model. fetch rate fast relative pipeline's execution rate, value prediction signifi- cantly improve execution throughput. Howe ver, pipeline fetch- limited, value prediction help much.  coverage achieved value prediction unit. Clearly, instructions predicted, performance benefit possible. Con- versely, poor coverag e results limited opportunity.  accuracy value prediction unit. Achieving high ratio correct incor rect predictions critical reaping significant perfor- mance benefit, ince mispredictions slow proce ssor down.  misprediction penalty pipeline implementation. discussed, recovery policy (refetch versus reissue) efficiency recovery policy severely affect performance impact mispredic- tions. Generally speaking, deeper pipelines require speculative sched- uling greater misprediction penalties wili sensitive effect.  degree program limited data flow dep endences. program primarily performance-limited something data dependences, eliminating data dependences via value prediction not552 MODERN PROCESSOR DESIGN result much benefit. example, program limited instruction fetch, branch mispredictions, struct ural hazards, memory bandwidth, unlikely tha value prediction help performance. summary, performance effects value prediction yet fully under- stood. clear large variety instruction sets, benchmark programs, machine models, isprediction recovery schemes, nontrivial speedup achievable reported literature. indication performance potential value prediction, som e per- formance results idealized machine model shown Figure 10.10. idealized machine model measures one possible data flow limit, since, prac- tical purposes, parallel ssue model restricted following three factors:  Branch prediction accuracy, minimum redirect penalty three cycles  Fetch bandwidth (single taken branch per cycle)  Data flow dependences  value misprediction penalty one cycle machine model reflects idealized performance respects, since misprediction penalties low structural hazards. However, consider history-based value predictors only, later studies employed £r <& & 6 6^ Simple configuration employs straightforward last-value predictor. IPerfCT, 4PerfCT, 8PerfCT configurations use perfect confidence, eliminating mispredictions maximizing coverage, choosing value history 1.4, 8 last values, respectively. Per fect configuration eliminates true data dependences indicates overall perf ormance potential. Figure 10.10 Value Prediction Speedup Idealized Machine Model.ADVANCED REGISTER DATA FLOW TECHNIQUES 55: Table 10.1 Value predictio n unit configurations Value Prediction Table Confidence Table Configuration Simple IPerfCT 4PerfCT 8PerfCT PerfectDirect-Mapped Entries 4096 4096 4096 4096History Depth l 4/Perfect selector 8/Perfect selector PerfectDirect-Mapped Entries 1024Bits/Entry 2-bit up-down saturating counter Perfect Perfect Perfect Perfect computational hybrid predictors shown dramatically higher potential speedup. Figure 10.10 shows speedup five diffe rent value prediction unit configurations, summarized Table 10.1. Attributes marked perfect Table 10.1 indicat e behavior analogous perfect caches; is, mechanism always produces right result assumed. specifically, IPerfCT, 4PerfCT, 8PerfCT configurations, assume oracle confi- dence table (CT) able correcdy identify predictable unpredictable register writes. Furthermore, 4PerfCT 8PerfCT configurations, assume perfect mechanism choosing four (or eight) values stored value history correct one. Note idealized version last-n predictor proposed Burtscher Zorn [1999]. Moreover, assume perfect configuration always correctly predict value every register write, effectively removing data dependences execution. configurations, value prediction unit configuration know build simple one, four merely included measure potential contribution improvements value prediction table (VPT) CT predictio n accuracy. results Figure 10.10 clearly demonstrate even simple predictors capable achieving significant speedup. di fference simple IPerfCT configurations demonstrates accuracy vitally important, since increase speedup factor 50% limit. 4PerfCT 8PerfCT cases show marginal benefit gained history-based predic- tors track multiple values. Finally, p erfect configuration shows dra- matic speedups possible benchmarks limited data flow. 10.4.6 Concluding Remarks summary, various schemes speculative execution based value prediction proposed. Researchers described techniques improving prediction accuracy coverage focusing predictor scope value predictions are554 MODERN PROCESSOR DESIGN perceived useful. Many implementation issues, predictor design well effective microarchitectural support value-s peculat ive execution studied. time, numerous unanswered questions unex- plored issues remain. real designs incorporate value prediction yet emerged; time tell demonstrated performance potential value prediction compen sate additional complexity required effective implementation. 10.5 Summary chapter explored speculative nonspeculative tec hniques improving register data flow beyond classical data flow limit. tech- niques based program characteristic value locality, describes likelihood previously seen operand values recur later executions static program instructions. property exploited remove computations program's dynamic data flow graph, potentially reducing height tree allowing compressed execution schedule permits instructions execute sooner position data flow graph might indicate. Whenever scenario ccurs, program said executing beyond data flow limit, rate computed dividing number instructions data flow graph height graph. Since height reduced techniques, rate execution increases beyond data flow limit. nonspeculative tech niques range memoization, program- ming technique stores reuses results side-effect free computations; instruction reuse, impleme nts memoization instruction level reusing previously executed instructions whenever operands match cur- rent instance; block, trace, data flow region reuse, extend instruction reuse large r groups instructions based control data flow relationships. techniques share characteristic invoked known safe c orrectness; safety determ ined applying reuse test guaran- tees correctness. contrast, remaining value locality-based technique examined—value prediction—is speculative nature, removes computation data dependence graph whenever correctly predict outcome computation. Value prediction introduces additional microarchitectural com- plexity, since speculative executi on, misprediction detection, recovery mecha- nisms must provided. None techniques yet implemented real processor design. published studies indicate dramatic performance improvement possible, appears industry practitioners found incremental implementations techniques augment existing designs provide enough performance improvement merit additional cost complexity. time tell future microarchitectures, perhaps amenable adapta- tion techniques, actually reap benefits described literature.ADVANCED REG ISTER DATA FLOW TECHNIQUES 5 REFERENCES Burtscher, M., B. Zorn: "Prediction outcome history -based confidence estimation load value ptediction,n Journal Instruction Level Parallelism, 1, 1999. Calder, B., P. Feller, A. Eustace: "Value profiling," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1997, pp. 259-269. Calder, B., G. Reinman, D. Tullsen: "Sel ective value prediction," Proc. 26th Annual Int. Symposium Computer Architecture (ISCA'99), 27, 2 Computer Architecture News, 1999, pp. 64-74, New York, ACM Press. Connors, D. A., W. mei W. Hwu: "Compiler-directed dynamic computation reuse: Rationale initial results," Int. Symposium Microarchitecture, 1999, pp. 158-169. Fields, B., S. Rubin, R. Bodik: "Focusing processor policies via critical-path predic- tion," Proc. 28th Int Symposium Computer Architecture, 2001, pp. 74-85. Gabbay, F., A. Mendelson: "Can program profiling support value prediction," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1997, pp. 2 70-280. Gabbay, F., A. Mendelson: "The effect instruction fetch bandwidth value predic- tion," Proc. 25th Annual Int. Symposium Computer Architecture, Barcelona, Spain, 1998a, pp. 272-281. Gabbay, F., A. Mendelson : "Using value prediction increase power specula- tive execution hardware," ACM Trans, Computer Systems, 16,3, 1998b, pp. 234-270. Gonzalez, A., J. Tub ella, C. Molina : "Trace-level reuse," Proc. Int. Conference Par- allel Processing, 1999, pp. 30-37. Huang, J., D. J. Lilja: "Exploiting basic block value l ocality block reuse," HPCA, 1999, pp. 106-114. Lee, S.-J., P.-C. Yew: "On table bandwidth update delay value pre diction wide-issue ILP processors," IEEE Trans, Computers, 50, 8, 2001, pp. 847-852. Lipasti, M. H.: "Value Locality Speculative Execution," PhD thesis, Carnegie Mellon University, 1997. Lipasti, M. H, J. P. Shen: "Exceeding dataflow limit via value prediction," Proc. 29th Annual ACM/IEEE Int. Symposium Microarchitecture, 1996, pp. 226-237. Lipasti, M. H., I. P. Shen: "Superspeculative microarchitecture beyond AD 2000," Computer, 30,9,1997. pp. 59-66. Lipasti, M. H., C. B. Wilkerson, J. P. Shen: "Value locality load value pre diction," Proc. Seventh Int. Conference Architectural Support Programming Languages Operating Systems (ASPLOS-VII). 1996, pp. 138-147. Martin, M. M. K., D. J. Sorin. H. W. Cain, M. D. Hill, M. H. Lipasti: "Correctly imple- menting value prediction microprocessors support multithreading mu ltiprocess- ing," Proc. MICRO-34. 2001, pp. 328-337. Mendelson, A., F. Gabbay: "Speculative execution based value prediction, " Techni- cal report, Technion, 1997. Sazeides, Y.: "An Analysis Value Predictability Application Superscalar Pro- cessor," PhD Thesis, University Wisconsin, Madison, WI, 1999. Sazeides, Y, J. E. Sm ith: "The predictability data values," Proc. 30th Annual ACM/ IEEE Int. Symposium Microarchitecture, 1997, pp. 248-258.556 MODERN PROCESSOR DESIGN Sodani, A.: "Dynamic Instruction Reuse," PhD thesis. University Wisconsin, 2000. Sodani, A., G. S. Sohi: "Dynamic instruction reuse," Proc. 24th Annual Int. Sympo- sium Computer Architecture, 1997, pp. 194—205. Sodani, A., G. S. Sohi: "Understanding differences value prediction instruction reuse," Proc. 31st Annual ACM/IEEE Int. Symposium Microarchitecture (MICRO-31), 1998, pp. 205-215, Los Alamitos, IEEE Computer Society. Tjaden, G. S., M. J. Flynn: "Detection parallel execution independent instruc- tions," IEEE Trans, Computers, C19, 10,1970, pp. 889-895. Tomasulo, R.: "An ef ficient algorithm exploiting multiple arithimetic units," IBM Jour- nal Research Development, 11,1967, pp. 25-33. Tullsen, D., J. Seng: "Storageless value prediction usin g prior register values," Proc. 26th Annual Int. Symposium Computer Architecture (ISCA'99), vol. 27, 2 Computet Architecture News, 1999, pp. 270-281, New York, ACM Press. Wang, K., M. Franklin: "Highly accurate data value prediction using hybrid predictors," Proc. 30th Annual ACM/IEEE Int. Symposium Microarchitecture, 1999, pp. 281-290. HOMEWORK PROBLEMS P10.1 Figure 10.1 suggests possible improve IPC 1 4 employing techniques instruction reuse value prediction collapse true data dependences. However, publications de scribing techniques show speedups ranging percent tens percent. Identify describe one program characteristic inhibits speedups. P10.2 Problem 10.1, identify describe least one implementation constraint tha prevents best-case speedups occurring. P10 3 Assume implementing instruction reuse integer instructions PowerPC 620. Assume want perform reuse test based value dispatch stage. IJescribe many additional read write ports need integer architected register file (ARF) rename bu ffers. P10.4 Problem 10.3, assume implementing instruction reuse PowerPC 620, wish perform reuse test value dispatch stage. Show design reuse buffer integrates 620 pipeline. many read /write ports structure need? P10.5 Assume building instruction reuse mechanism attempts reuse load instructions p erforming reuse test name PowerPC 620 dispatc h stage. Since addresses prior in-flight stores may known time, several design choices: (1) either disallow load reuse stores unknown addresses still flight, (2) delay dispatch reused loads prior stores computed addresses, (3) go ahead allow loads beADVANCED REGISTER DATA FLOW TECHNIQUES 557 reused, relying mechanism guarantee correctness. Discuss three alternatives performance perspective. P10.6 Given assumptions Problem 10.5, describe ex isting microar- chitectural feature PowerPC 620 could used guarantee cor- rectness third case. choose third ption, instruction reuse scheme still nons peculative? P10.7 Given scenario described Problem 10.5, comment likely effectiveness load instruction reuse 5-stage pipeline like PowerPC 620 versus 20-stage pipeline like Intel Pentium 4. thre e options outlined likely work best future deeply pipelined processor? Why? P10.8 Construct sequence lo ad value outcomes last-value predic- tor perform better FCM predictor stride predictor. Com- pute prediction rate type predictor sequence. P10.9 Construct sequence load value outcomes FCM predictor perform better last-value predictor stride predictor. Com- pute prediction rate ty pe predictor fo r sequence. P10.10 Construct sequence load value outcomes stride predictor perform better FCM predictor last-value predictor. Compute prediction rate type predictor sequence. P10.ll Consider interaction value predictors branch predictors. Given stride value predictor tw o-level GAg branch predictor 10-bit branch history register, write C-code program snippet stride value predictor correct branch branch pre- dictor mispredicts. P10.12 Consider nteraction value predictors branch predictors. Given last-value predictor two-level GAg branch predictor 10-bit branch history register, write C-code program snippet last-value predictor incorrectly resolves branch branch predictor predicts correctly. P10.13 Given value predictor incorrectly redirect correctly predicted branches, suggest discuss least two microarchitectural alterna- tives dealin g problem. P10.14 Assume implementing value prediction integer instructions PowerPC 620. Describe many additional read write ports need integer architected register file (ARF) rename b uffers. P10.15 Problem 10.14, assume implementing value prediction PowerPC 620. concluded need selective reissue via global broadcast recovery chanism. mechanism,.58 MODERN PROCESSOR DESIGN in-flight instruction must know precisely earlier instruc- tions dep ends on, either directiy indirectiy multiple levels data flow graph. PowerPC 620, design RAM/CAM hardware tructure tracks information enables direct selective reissue misprediction detected. many write ports structure need? P10.16 hardware structure Problem 10.15, determine size hardware structure (number bit cells needs store). Describe size would vary ag gressive microarchitecture like Intel P6, allows 40 instructions flight one time. P10.17 Based data Figure 10.10, provide justify one possible explanation gawk benchmark achieve higher speedups aggre ssive value prediction schemes. P10.18 Based data Figure 10.10, provide justify one possible explanation swm256 benchmark achieves dramatically higher speedup p erfect value prediction scheme. P10.19 Based data F igures 10.3 10.10, explain apparent contra- diction benchmark sc. even though roughly 60% register writes predictable, speedup obtained implementing value prediction. Discuss least two reasons might case. P10.20 Given answer Problem 10.19, propose set experiments could conduct validate hypotheses. P10.21 Given deadlock scenario described ection 10.4.4.5, describe possible solu tion prevents dea dlock without requiring specula- tively issued instructions retain reservation stations. Compare proposed solution alternative solution forces instructions retain reservation stations deemed nonspec ulative.CHAPTER 11 Executing Multiple Threads CHAPTER OUTLINE 11.1 troduction 11.2 Synchronizing Shared-Memory Threads 113 Introduction Multiprocessor Systems 11.4 ExplicitK'MuWtrn-eac^ 11S Implicitly Multithreaded Processors 11.6 Executing Thread 11.7 Summary References Homework Problems 11.1 Introduction Thus far exploration high-performance processors, focused exclusively techniques ac celerate processing single thread execu- tion. say, concentrated compressing latency execution, beginning end, single serial program. Fir st discussed Chapter 1, three fundamental interrelated terms affect latency: processor cycle time, available instruction-level parallelism, number instructions per program. Reduced cycle time brought combination circuit design techniques, improvements circuit technology, archi tectural tradeoffs. Available instruction-level parallelism affected advances compilation technology, redu ctions structural hazards, aggressive microarchitectural techniques branch value prediction tha mitigate negative effects control data dependences. Finally, number instructions per program determined algorithmic advances, improvements compilation technology, fundamental characteristics instruction set executed. 559560 MODERN PROCESSOR DESIGN factors assume single thread ex ecution, processor traverses static control flow graph program serial fashion beginning end, aggressively r esolving control data dependences always maintaining illusion sequential execution. chapter, broaden scope consider alternative source per- formance widely exploited real systems. source, called thread-level parallelism, primarily used improve throughput instruction processing bandwidth processor collection process ors. Exploitation thread-level parallelism roots early time-sharing mainframe computer systems. early systems coupled relatively fast CPUs relatively slow input/output (I/O) devices (the slowest I/O device human programmer opera- tor sitting terminal). Since CPUs expensive, slow I/O devices terminals relatively inexpensive, operating system developers invented concept time-sharing, allowed multiple I/O devices con- nect share, time-sliced fashion, single CPU r esource. allowed expensive CPU switch contexts alternative user thread whenever current thread encountered long-latency I/O event (e.g., reading disk waiting terminal user enter keystrokes). nce, mos expensive resource system—the CPU—was kept busy long users threads waiting execute instructions. time-slicing policies—which also included time quanta enforced fair access CPU—were implemented operating system using software, hence introd uced additional execution-time overhead switching contexts. Hence, latency single thread execution (or latency perceived single user) would actually increase , since would include context-sw itch operating system policy nagement overhead. However, overal l instruction throughput processor would increase due fac instructions executed alternative threads other- wise idle CPU would waiting long-latency I/O event complete. microarchitectural st andpoint, ty pes time-sharing workloads provide interesting challenge processor designer. Since interleave execution multiple independent threads, wreak havoc caches structures rely spatial temporal loca lity exhibited refer- ence stream single thread. Furthermore, interthread conflicts branch value predictors si gnificantly increase pressure structures reduce efficacy, particularly structures adequately sized. Finally, large aggregate working set large numbers threads (there tens thousands hundreds thousands active threads modern, high-end time-shared system) easily overwhelm capacity bandwidth provided conventional memory subsystems, leading designs large secondary tertiary caches extremely high memory bandwidth. effects illus- trated Figure 3.31. Time-shared workloads share data concurrently active processes must serialize access shared data well-defined repeatable manner. Otherwise, workloads generate nondeterminisfic even erroneous results. consider simple widely used schemes serialization orEXECUTING MULTIPLE THREADS 56' synchronization Section 11.2; schemes rely hardware support atomic operations. operat ion considered atomic suboperations per- formed indivisible unit; say, either performed without interference operations processes, none performed. Mod- em processors support primitives used implement various atomic operations enable multiple proc esses threads synchronize correctly. standpoint system architecture , time-shared workloads create additional opportunity building syst ems provide scalable throughput. Namely, availability large numbers active independent threads execution motivates construction sy stems multiple processors them, since operating system distribute ready threads multiple proces- sors quite easily. Building multiprocessor system requires designer resolve number tradeoffs related prima rily memory subsystem pro- vides processor coherent consistent view memory. discuss issues Section 11.2 briefly describe key attributes coherence interface odem processor must supply order support view memory. hi addition systems simultaneously execute multiple threads control physically separate processors, processors provide efficient, fine-grained support interleaving multiple threads single physical processor also proposed built. multithreaded processors come various flavors, ranging fine-grained multithreading, switches multiple thread contexts every cycle every cycles; coarse-grained multithreading, switches contexts long-latency events cache misses; simulta- neous multithreading, away con text switching allowing indi- vidual instructions multiple threads intermingled processed simultaneously within out-of-order processor's execution window. discuss tradeoffs implementation challenges proposed real multi- threaded processors Section 11.4. availability sy stems multiple processors also spawned large body research parallel alg orithms use multiple collaborating threads arrive answer quickly single serial thread. Many important problems, particularly ones apply regular computations massive data sets, quite amenable parallel implementations. However, holy grail research— automated parallelization serial programs —has yet materialize. automated parallelization certain cl asses algorith ms demon- strated, success largely limited scientific numeric applica- tions pred ictable control flow (e.g., nested loop structures statically determined itera tion counts) statically analy zable memory access patterns (e.g., sequential walks large multidimensional arrays floating-point data). applications, parallelizing compiler decompose total amount computation multiple independent threads distrib uting partitions data set total set loop iterations across multiple threads. Naturally, partitioning algorithm must take care avoid violating data dependences across parallel threads may need incorporate synchronization primitives across the562 MODERN PROCESSOR DESIGN threads guarantee correctness cases. Successful automatic parallelization scientific numeric applications demonstrated years fact commercial use many applications domain. However, many difficulties extracting thread-level parallelism typical non-numeric serial applications automatically parallelizing com- pile time. Namely, applications irregular control flow, ones tend access data unpredictable patterns, ones replete accesses pointer-based data structures make difficult statically de termine memory data depen- dences various portions original sequential program. utomatic par- allelization codes difficult partitioning serial algorithm multiple parallel independent threads becomes virtually impossible without exact compile-time knowledge control flow data dependence relationships. Recendy, several researchers proposed shifting process automatic parallelization serial algorithms compile time run time, least pro- viding e fficient hardware support solving thorny problems associ- ated efficient extraction multiple threads execution. implicit multithreading proposals range approaches dynamic multithreading [Akkary Driscoll, 1998], advocates pur e hardware approach auto- matically identifies spawns speculative implicit threads execution, multiscalar [Sohi et al., 1995] paradigm uses combination hardware support aggressive compilation achieve purpose, thread-level speculation [Steffan etal., 1997; 2 000; Steffan Mowry, 1998; Hammond et al., 1998; Krishnan Torrellas, 2001], relies compiler create parallel hreads provides simple hardware support detecting data depen- dence violations bet ween threads. discuss proposals implicit multithreadin g Section 11.5. another variation theme, researchers proposed preexecution, uses second runahead thread execute critical portions main execution thread order prefetch data instructions resolve difficult- to-predict conditional branches main thread encounters them. similar approach also en suggested fault detection fault-tolerant execution. discuss proposals associated implementation chal- lenges Section 11.6. 11.2 Synchronizing Shared-Memory Threads Time-shared workloads share data concurrently active processes must serialize access shared data well-defined repeatable manner. Other- wise, workloads wil l nondeterministic even erroneous results. Figure 11.1 illustrates four possible interleavings loads stores performed shared variable tw threads. four interleavings possi- ble time-shared system alternating execution two threads. Assum- ing initial value = 0, depending interleaving, final value either 3 [Figure 11.1(a)], 4 [Figure 11.1(b) (c)], 1 [Figure 11.1(d)]. course, well-written pr ogram predictable repeatable outcome, instead one determined operating sy stem's task dispatching policies.EXECUTING MULTIPLE THREADS 563 Thread 0 Thread 1 load rl, addirl,rl ,3 loadrl, addirl.rl, 1 store rl, store rl, (a)Thread 0 Thread 1 load rl, addirl.rl, 1 store rl, loadrl, addi rl, rl, 3 store rl. (b) Thread 0 load rl, addirl.rl, 1 store rl, Thread 1 load rl, addirl.rl, 3 store rl, Thread 0 loadrl, addirl, rl. 1 Thread 1 loadrl, addirl.rl, 3 store rl, store rl, (c) (d) figure shows four possible interleavings references made two threads shared variable A, resulting 3 different final values A. Figure 11.1 Need Synchronization. __j L_ Table 11.1 common synchronization primitives Primitive Fetch-and-add Compare-and-swap Load-linked/store- conditionalSemantic Atomic load —> add —> store operation Atomic load —» compare —> conditional store Atomic load —> conditional storeComments Permits atomic increment; used synthesize locks mutual exclusion Stores load returns expected value Stores load/store pair atomic; is, intervening store simple example motivates need well-defined synchronization shared-memory threads. Modern processors supply primitives used implement various atomic operations enable multiple proc esses threads synchronize correctly. primitives guarantee hardware support atomic operations. operation considered atomic suboperations performed indivisible unit; say, either performed without interference ther operations processes, none performed. Table 11.1 ummarizes three commonly implemented primitives used synchronize shared-memory threads.564 MODERN PROCESSOR DESIGN Thread 0 Thread 1 fetchadd A, 1 fetchadd A, 3 (a)Thread 0 spin: cmpswp AL. 1 bfail spin loadrl, addi rl, rl, store rl, store 0, ALThread 1 spin: cmpswp AL, 1 bfail spin load rl, addirl,rl ,3 store rl, store 0, AL (b)ThreadO spin: 11 rl, addirl.rl, 1 stcrl, bfail spin Figure 11.2 Synchronization (a) Fetch-and-Add, (b) Compare-and-Swap, (c) Load-Linked/Store-Conditional.Thread 1 spin: llrl.A addirl.rl, 3 stcrl, bfail spin (c) first primitive Table 1 1.1, fetch-and-add, simply loads value memory locat ion, adds operan it, stores result back memory location. hardware guarantees seq uence occurs atomically; effect, processor must continue retry sequence succeeds storing sum thread overwritten fetched value shared loc ation. shown Figure 11.2(a) , code snippets Figure 11.1 could rewritten "fetchadd , 1" n "fetchadd , 3 " f r h e h r e 0 n 1 , respectively, resulting deterministic, repeatable shared-memory program. case, allowable outcome would = 4. second primitive, compare-and-swap, simply loads value, compares supplied operand, stores operan memory location loaded value matches operand. primitive allows programmer atomically swap register value value memory location whenever memory location contains expected value. compare fails, condition flag set reflect failure. primitive used implement mutual exclusion critical sections protected locks. Critical sections simply arbitrary sequences instructions executed atomically guaranteeing thread enter section thread currently executing critical section completed entire section. example, updates snippets Figure 11.1 could made atomic performing within critical section protecting critical section additional lock variable. illustrated Figure 11.2(b), cmpswp instruction checks AL lock variable. set 1, cmpswp fails, thread repeats cmpswp instruction u ntil su cceeds, branching back repeatedly (this known spinning lock). cmpswp succeeds, thread enters critical section performs load, add, store atomically (since mutual exclusion guarantees processor concurrently executing critical section protected lock). Finally, thread stores 0 lock variable AL indicate done critical section. third primitive, load-linked/store-conditional (11/stc), simply loads value, performs othe r arbitrary operations, attempts store back address loaded from. intervening store another thread cause storeEXECUTING MULTIPLE THREADS 565 conditional fail. However, store ad dress occurred, load/store pair execute atomically store succeeds. Figure 11.2(c) illustrates shared memory snippets rewritten use 11/stc pairs. example, 11 instruction loads current value A, adds it, attempts store sum back c instruction. stc fails, thread spins back 11 instruction pair eventually succeeds, guaranteeing atomic update. three examples Figure 11.2 guarantee final result memory location always e qual 4, regardless two threads execute memory references interleaved. propert guaran- teed atomicity property primitives employed. implementation standpo int, 11/stc p r h e tractive three. Since closely matches load store instructions already supported, fits nicely pipelined superscalar implementations detailed earlier chapters. two, fetch-and-add compare-and-swap, not, since require two memory references must performed indivisibly. Hence, require substantially specialized handling processor pipeline. Modern instruction sets MIPS, PowerPC, Alpha, IA-64 provide 11/stc primitives synchronization. fairly easy implement; additional semantic supported 11 instruction must, side effect, reme mber ad dress loaded from. subsequent stores (including stores performed remote processors multiprocesso r system) must check addresses linked address must clear match. Finally, c executes, must check address linked address. matches, c allowed proceed; not, c must fail set condition code reflects failure. changes fairly incremental beyond support already place standard loads stores. Hence, 11/stc easy implement still powerful enough synthesize fetch-and-add compare-and-swap well many atomic primitives. summary, proper synchronization necessary correct, repeatable exe- cution shared-memory prog rams multiple threads execution. true pr ograms running time- shared uniprocessor, also programs running multiprocessor systems multithreaded proce ssors. 113 Introduction Multiprocessor Systems Building multiprocessor systems ttractive proposition system vendors number reasons. First all, provide natural, incrementa l upgrade path customers growing computational demands. long key user applications provide thread-level parallelism, adding processors system replacing smaller system larger one contains processors pro- vides customer straightforward efficient way add computing capacity. Second, multiprocessor systems allow system vendor amortize cost single microprocessor design across wide variety system design points tha provide varying levels performance scalability. Finally, multi- processors provide coherent shared memory provide programming model566 MODERN PROCESSOR DESIGN compatible time-shared uniprocessors, making easy customers deploy existing applications develop new ones. systems, hardware operating system oftware collaborate provide user programmer appearance four multiprocessor idealisms:  Fully shared memory means processors system eq uiva- lent access physica l memory system.  Unit latency means requests memory satisfied single cycle.  Lack contention means thanhe forward progress one processor's memory references never slowed affected memory references another proce ssor.  Instantaneous propagation writes means changes memory image made one processor's write immediately visible processors system. Naturally, system processor designers must strive approximate idealisms closely possible satisfy performance correctness expectations user. Obviously, factors cost scalability play large role easy reach goals, bu well-designed system in. fact maintain illusion idealisms quite successfully. 11.3.1 Fully Shared Memory, Unit Latency, Lack Contention shown Figure 11.3, conventional shared-memory multiprocessors provide uniform memory access (UMA) usually built using dancehall organi- zation, set memory modules banks connected set processors Uniform Memory (dancehall)Uniform memory latencyProcessor Processor 1 — | Cache 1 ProcessorHCache 1 1 Interconnection network MexnoryJ Memory "| Memory "j Long remote memory latency Nonuniform Short Memory local Access latencyMemory MemoryJ ProcessorMemory J—| Cache | Processor |—| Cache | Processor j —| Cache | | Processor"HCache] Interconnection network Figure 11.3 U versus NUMA Multiprocessor ArchitectureEXECUTING MULTIPLE HREADS 567 via crossbar interconnect, processor incurs uniform latency accessing memory bank crossbar. downsides approach cost crossbar, increases square number processors memory banks, fact every memory reference must traverse crossbar. alternative, many system vendors build systems nonuni- form memory access (NUMA), processors still connected via crossbar interconnect, processor local bank memory much lower access latency. NUMA configuration, references remote memory must pay latency penalty traversing crossbar. UMA NUMA systems, uniprocessor systems, idealism unit latency approximated use caches able satisfy refer- ences local remote (NUMA) memories. Similarly, traffic filtering effect caches used mitigate c ontention memory banks, use intelligent memory controllers combine reorder requests minimize latency. Hence, caches, already learned dispensable unipro- cessor systems, similarly effe ctive multiproc/jgor systems well. However, presence caches multiprocessor system create additional diffi- culties dealing memory writes, since must somehow made visible propagated processors system. 11.3.2 Instantaneous Propa gation Writes time-shared uniprocessor system, one thread updates memory location writing new value it, tha thread well threa eventually executes wil l instantaneously see new value, since stored cache hierarchy uniprocessor. Unfortunately, multiprocessor system, property hold, since subsequent ferences address may originate different processors. Since processors caches may contain private copies cache line, may see effects proce ssor's write. example, Figure 11.4(a), processor PI writes " 1 " memory location A. coherence support, copy memory location P2's cache updated reflect new value, load P2 would still observe stale value "0." known classic cache coherence problem, solve it, system must provide cache coherence protocol ens ures processors system gain visibility processors' writes, processor coherent view contents memory [Censier Feautrier, 1978]. two fundamental approaches cache coherence—update protocols invalidate protocols—and dis- cussed briefly Sec tion 11.3.3. illustrated Figure 11.4(b) (c). 11.3.3 Coherent Shared Memory coherent view memory hard requirement shared-memory multipro- cessors. Without it, programs share memory would behave unpredictable ways, since value returned read would vary depending processor performed r ead. already stated, coherence problem caused fact writes automatically instantaneously propagated processors'568 MODERN PROCESSOR DESIGN Time JA:0 j AJ j    (a) coherence protocol: stale copy P2 E3A:0 5A:0'*yA:0  (b) Update protocol writes copies   (c) Invalidate protocol eliminates stale remote copy update protocol updates remote copies, invalidate protocol removes remote copies. Figure 11.4 Update Invalidate Protocols. caches. ensure writes made visible othe r processors, two classes coherence protocols exist. 11.3.3.1 Update Protocols. earliest proposed multiprocessors employed  straightforward approach maintaining cache coherence. systems, processors' caches used wr ite-through policy, writes per- formed cache processor perfo rming write, aiso main memory. protocol illustrated Figure 11. 4(b). Since aucXECUTING MULTIPLE THREADS 569 processors connected electric ally shared bus also co nnected main mem ory, processors wer e able observe write- throughs occurred able directiy update copies data (if copies) snooping new values shared bus. effect, update protocols based broadcast write-through policy; is, every write every processor written through, main mem- ory, also copy existed processor's cache. Obviously, protocol scalable beyond small number processors, since write-through traffic multiple processors quickly overwhelm band- width available memory bus. straightforward optimization allowed use writeback caching private data, writes performed locally processor's cache changes written back main memory cache line evicted processor's cache. protocol, however, writes shared cache lines (i.e., lines present processor's cache) still broadcast bus th e sharing processors could update copies. Furthermore, remote read line dirty local cache required dirty line flushed back memory remote read could satisfied. Unfortunately, excessive bandwidth demands update protocols led virtual extinction, modern multiprocessor sys tems use update protocol maintain cache coherence. 11.3.3 J2 Invalidate Protocols. Today's modern shared-memory multiproces- sors use invalidate protocols maintain coh erence. fundamental premise invalidate protocol simple: single processor allowed write cache line point time (such protocols also often called single-writer protocols). policy enforced ensuring pr ocessor wishes write cache line must first establish copy cache line valid copy system. oth er copies must invalidated f rom proces- sors' caches (hence term invalidate protocol). protocol illustrated Figure 1 1.4(c). short, processor pe rforms write, checks see copies line elsewhere system. are, sends messages invalidate them; finally, performs write private exclusive copy. Subsequent writes line treamlined, since check outstanding remote copies required. again, uniprocessor writeback caches, updated line written back memory evicted processor's cache. However, coherence protocol must keep track fact modi fied copy line exists must p revent processors attempting read stale version memory. Furthermore, must sup- port flushing mod ified data processor's cache remote refer- ence satisfied up-to-date copy line. Minimally, invalidate protocol requires cache directory maintain least two states cached line: modified (M) invalid (I). invalid state, requested address present must fetched memory. In570 MODERN PROCESSOR DESIGN modified state, processor knows copies system (i.e., local copy exclusive one), hence processor able per- form reads writes line. Note line evicted mod- ified state must written back main memory, since processor may performed write it. -simple optimization incorporates dirty bit cache line's state, allows processor differentiate lines exclusive processor (usually called E state) ones exclusive dirtiecTby write ( usually called state). IBM/ Motorola PowerPC G3 processors used Apple's Macintosh desktop systems implement MEI coherence protocol. Note three states (MEI), cache line allowed exist one processor's cache time. solve problem, allow readable copies line multiple processors' caches, invali- date protocols also include shared state (S). state indicates one remote readable copies line exist. proc essor wishes perform write line state, must first upgrade line state invali- dating remote copies. Figure 11.5 shows state table tran sition diagram straightforward MESI coherence protocol. Ea ch row corresponds one four states (M, E, S, I), column sum marizes the'aetions coherence controller must per- form response type bus event. transition state cache line caused either local reference (read write), remote reference (bus read, bus wr ite, bus upgrade), l ocal capacity-induced eviction. cache directory tag array maintains MESI state line cache. Note allows cache line different state point time; enabling lines contain trictiy private data stay E state, lines contain shared data simultaneously e xist multiple caches state. ESI coherence protocol supports single -writer principle guaran- tee coherence also allows efficient sh aring read-only data well silent upgrades exclusive (E) state modified (M) state local writes (i.e., bus upgrade message required). common enhancement MESI protocol achieved adding O, owned state protocol, res ulting MOESI protocol. state entered following remote read dirty block state. state sig- nifies multiple valid copies block exist, since remote requestor received valid copy satisfy read, local processor also kept copy. However, di ffers conventional state avoiding writeback memory, hence leaving stale copy memory. state also known shared-dirty, since block shared, still dirty respect memory. owned block evicted cache must written back, like dirty block M.state, since copy main memory must made up-to-date. system impl ements state place either requesting processor processor upplies dirty data state, placing copy th e state, since single copy needs marked dirty.EXECUTING MULTIPLE THREADS Event Local Coherence controller Responses Actloi {$' refers next state) Current StatesLocal Read (LR)Local Write (LW)Local Eviction (EV)Bus Read (BR)Bus Write (BW)Bus U pgr (BU) Invalid 0)Issue bus read sharers s' = E else s' = Issue bus write s' = s'=l nothing nothing nothing Shared (S)Do nothing Issue bus upgrade s' = s' = l Respond shareds'=l s' = l Exclusive (E)Do nothing s' = s' = l Respond shared s' = s' = l Error Modified (M)Do nothing nothing Write data back;s' = l Respond dirty; Write data back; s' = Respond dirty; Write data back; s' = l Error response local bus events co herence controller may need change local coherence state line, may also need fetch supply cache line data. Figure 11.5 Sample MESI Cache Coherence Protocol. 11.3.4 Implem enting Cache C oherence Maintaining cache c oherence requires mechanism tracks tate (e.g., MESI) active cache line sys tem, references lines handled appropriately. convenient place store coherence state cache tag array, since state information must maintained line cache anyway . However, l ocal coherence tate cache line needs available processors system references line572 MODERN PROCESSOR DESIGN correctly satisfied w ell. Hence, cache coherence implementation must provide means distributed access coherence state lines cache. two verall approaches so: snooping implementations directory implementations. ' ( § 11.3.4.1 Snooping Implementations. straightforward approach implementing coherence consistency via snooping. snooping implemen- tation, off-chip address events evoked coherence protocol (e.g., cache misses invalidates invalidate protocol) made visible pro- cessors system via shared address bus. small-scale systems, address bus electrically shared processor sees processors' commands placed bus. advanced point-to-point interconnect schemes avoid slow ultidrop busses also support snooping reflecting commands processors via hierarchical snoop interconnect. notational convenience, simply refer scheme address bus. snooping implementation, coherence protocol specifies processor must react commands observes address bus. exam- ple, remot e processor's read cache line tha' currently modified local cache must cause cache controller flush line local cache transmit either directiy requester and/or back main memory, requester receive latest copy. Similarly, remote processor's invalidate request cache line currentiy shared loc al cache must cause con- troller update directory entry mark line inv alid. prevent future local reads consuming stale data cache. main shortcoming snoop ing implementations cache coherence scalability systems many processors. assume tha processor system generates address bus transactions rate, see fre- quency inbound address bus transactions must sno oped directiy pro- portional number processors system. Outbound snoop rate = s0 = (cache miss rate) + (bus upgrade r ate) (11.1) Inbound snoop rate = s, = nxj, (11-2) say, processor generates sB address transactions per second (consisting read requests cache misses upgrade requests stores shared lines), n processors system, processor must also snoop nsa transactions per second. Since snoop minimally requires local cache directory lookup check see proc essor needs react snoop (refer Figure 11.5 typical reactions) , aggregate lookup band- width required large n quickly become prohibitive. Similarly, available link ban dwidth connecting processor rest system easily overwhelmed traffic; fact, many snoop-based multiprocessors performance-limited addr ess-bus bandwidth. Snoop-based implementations shown scale sev eral dozen processors (up 64 case Sun Enterprise 10000 [Charlesworth, 1997]), scaling beyond number quires expensive investment increased address bus bandwidth.EXECUTING MULTIPLE THREADS Large-scale snoop-based systems also suffer dramatic increases mem- ory latency compared systems designed fewer processors, since memory latency determ ined latency coherence response, rather DRAM data interconnect latency. words, large «, often takes longer snoop collect snoop responses processors system fetch data DRAM, even NTJMA configura- tion long remote memory latencies. Even data memory transmitted speculati vely requester, known valid processors system responded up-to-date dirty copy line. Hence, snoop response latency often determines quickly cache miss resolved , rather latency retrieve cache line local remote storage location. 11.3.4.2 Directory Implementation. common solution scal- ability memory latency problems sn ooping implementations use direc- tories. directory implementation, coherence maintained keeping copy cache line's coherence state collocated main memory. c oherence state, stored directory resides next main memory, ndicates line currently cached anywhere system, also includes pointers cached copies sharing list sharing vector. Sharing lists either pre- cise (meaning sharer individually indicated list) coarse (meaning multiple processors share entry list, entry indicates one processors shared copy line) stored linked lists fixed-size presence vectors. Precise sharing vectors draw- back significant storage overhead, particularly systems large numbers processors, since cache line-size block main memory requires directory storage proportional number processors. large system 64-byte cache lines 512 processors, overhead 100% haring vector. Bandwidth Scaling. main benefit directory approach directory bandwidth scales memory bandwidth: Adding memory bank supply memory data ba ndwidth also adds directory bandwidth. Another benefit demand address bandwidth reduced filtering commands dire ctory. director implementation, address commands sent directory first forwarded remote processors necessary (e.g., line dirty remote cache writing line shared remote cache). Hence, frequ ency inbound address commands processor longer proportional number processors system, rather proportional degree data sha ring, since processor receives address command owns shared copy l ine question. Hence, systems dozens hundreds processors h ave built. Memory Latency. Finally, latency misses satisfied memory significantly reduced, ince memory bank respond nonspecula- tive data soon checked directory. particularly advantageous in574 MODERN PROCESSOR DESIGN NUMA configuration perating run -time systems opti- mized place private nonshared data processor's local memory. Since latency local memory usually low confi guration, misses resolved dozens nanoseconds instead hundreds nanoseconds. Communication Miss Latency. main drawback directory-based systems additional latency incurred cache misses found dirty remote processor's cache (called communication misses dirty misses). snoop-based system, dirty miss satisfied directly, since read request transmitted directly responder dirty data. directory implementation, request first sent directory forwarded current owner line; results additional traversal proc essor/memory interconnect increases latency. Applications database transaction processing thatshare data intensively sensitive dirty miss latency perform poorly directory-based systems. Hybrid snoopy/directory systems also proposed built. example, Sequent NUMA-Q system uses conventional bus-based snooping maintain co herence within four-processor quads, extends cache coherence across multiple quads directory protocol built scalable coherent interface (SCI) standard [Lovett Clapp, 1996]. Hybrid schemes obtain many scalability benefits directory schemes st ill maintaining low average latency communication misses satisfied within local snoop domain. 11.3.5 Multilevel Caches, Inclusion, Virtual Memory modern processors implement multiple levels cache trade capacity miss rate access latency bandwidth: level-1 primary cache relatively small allows one- two-cycle access, frequently multiple banks ports, level-2 secondary cache provides much greater capacity multic ycle access usually single port. design multilevel cache hierarchies exercise b alancing implementation cost complexity achieve lowest average memory latency references hit miss caches. shown Equation (11.3), average memory reference latency latavg computed weighted sum latencies n levels cache hierarchy, latency lat, weighted fraction refer- e n c e ref,- satisfied level: lat„vg = JTref, x lat, (11.3) course,-such average l atency measure less meaningful context out-of-order processors, miss latencies econdary cache often overlapped useful work, reducing importance high hit rates primary cache. Besides reducing average latency, primary ob jective primary caches reduce bandwidth required secondary cache. SinceEXECUTING MULTIPLE THREADS 5] majority references satisfied reaso nably sized primary cache, small subset need serviced secondary cache, en abling much narrower usually single-ported access path cache. Guaranteeing cache coherence design multiple levels cache incrementally complex base case single level cache; benefit obtained maintaining inclusion levels cache forcing line resides higher level cache also reside lower level. Noninclusive Caches. straightforward approach multilevel cache c oherence require inclusion treats cache hierarchy peer coherence sc heme, implying tha coherence maintained independently level. snooping implementation, implies levels cache hierarchy must snoop address commands traversing system's address bus. lead excessive bandwidth demands level-1 tag array, since processor core inbound address bus generate high rate references tag array. IBM Northstar/Pulsar design [Storino et al., 1998], noninclusive employs snoop-based coherence, maintains two copies level-1 tag array provide effectively dual-ported access structure. noninclusive directory implementation, sharing vector must maintain separate entries level processor (if sharing vector precise), revert coarse haring scheme implies messages must forwarded levels cache processor copy line. Inclusive Caches. common alternative maintaining coherence independently level cache guarantee coherence state line upper level cache con sistent lower private levels maintaining inclusion. example, system two levels cache, cache hierarchy must ensure line resides level-1 cache also resides (or included in) level-2 cache con sistent state. Maintainin g inclusion fairly straightforward: Whenever line enters level-1 cache, must also placed level-2 cache. Similarly, whenever line leaves level-2 cache (is evicted due replacement invalidated), must also leave level-1 cache. inclusion maintained, th e lower level cache hierarchy needs partici- pate directly cache coherence scheme. definition, coherence oper ation pertains lines level-1 cache also pertains corresponding line level-2 cache, cache hierarchy, upon finding line le vel-2 cache, must apply operation th e level-1 cache well. effect, snoop lookups tag array level-2 cache serve filter prevent coherence operations relevant requiring lookup level-1 tag array. snoop-based implementations lots address tra ffic, significant advantage, since tag array references mostly partitioned two dis- joint groups: 90% processor core references satisfied level-1 tag array cache hits, 90% address bus commands satis- fied level-2 tag array misses. level-1 misses require level-2 tag lookup, coherence hits shared lines require accesses level-1 tag576 MODERN PROCESSOR DESIGN array. approach avoids maintain multiple copies dual-porting level-1 tag array. Cache Coherence Virtual Memory. Additi onal complexity introduced fact nearly modem processors implement virtual memory provide access protection demand paging. virtual memory, effective virtual address gen erated user pro gram translated physical address using map- ping maintained operating system. Usually, address translation performed prior accessing cache hierarchy, but, cycle time capacity reasons, processors imple ment primary caches virtually indexed tagged. access time virtually addressed cache lower since cache indexed parallel address tr anslation. However, since cache coherence typically handled using physical addresses virtual addresses; performing coherence-induced tag lookups cache poses challenge. mechanism p erforming reverse address translation must exist; accomplished separate rev erse address translation table keeps track referenced real addresses heir co rresponding virtual addresses, or—in multilevel hierarchy—with pointers level-2 tag array point corre- sponding level-1 entries. Alternatively, coherence controller search level-1 entries congruence c lass corresponding particular real address . case large set-associative virtually ddressed cache, alternative prohibitively expensive, since congruence class quite large. interested reader referred classic paper Wang et al. [1989] topic. 11.3.6 Memory Consistency addition providing coherent view mem ory, multiprocessor system must also provide suppor predefined memory consistency model. consistency model specifies agreed-upon convention ordering memory references one processor respect references another processor integral part instruction set architecture specification multiprocessor-capable system [Lamport, 1 979]. Consistent orderin g memory references across process sors important correct operation multithreaded applications share memory, since without architected set rules ordering references; programs could c orrectly reliably synchronize threads behave repeatable, predictable manner. example, Figure 11.6 shows simple Reorder load storeProcO st A=l (load B==0) { ...critical section }st B=l (load A=-0) { ...critical section } either processor reorders load executes store, processors enter mutt exclusive critical section simultaneously. Figure 11.6 Dekker's Algorithm Mutual Exclusion.EXECUTING MULTIPLE THREADS 577 serialization scheme guarantees mutually exclusive access critical section, may updating shared datum. Dekker's mutual exclusion scheme two processors consists processor 0 setting variable A, testing another variable B, performin g mutually exclusive access (the variable names reversed processor 1). long processor sets variable tests processor's variable, mutual exclusion guaranteed. However, without c onsistent ordering memory references performed here, two processors could easily get confused whether ther entered critical secti on. Imagine scenario tested other's variables time, neither yet observed other's write, e ntered critical section, continuing conflicting updates shared object. scenario possible processors allowed reorder memory references loads execute independent stores (termed load bypassing Chapter 5). 11.3.6.1 Sequential Consistency. simplest co nsistency model called sequential consistency, requires imposing total order among references performed processor [Lamport, 1979]. Conceptually, sequentially consistent (SC) system behaves processors take turns accessing shared memory, creating int erleaved, totally ordered stream references also obeys program order ndividual p rocessor. approach illus trated Figure 11.7 principle similar interleaving references multiple threads executing single time-shared processor. similarity, easier programmers reason behavior shared-memory programs SC systems, since multithreaded programs operate correctly time- shared uniprocessors also usuall operate corr ectly sequentially consistent multiprocessor. However, sequential consistency challenging implement efficiently. Consider imposing total order requires load store must issue program order, effectively crippling odem out-of-order processor, reference must also ordered respect processors system , naively requiring ry-high-bandwidth interconnect establishing processo r accesses memory program order, accesses processors interleaved memory serviced requests one processor time. Figure 11.7 Sequentially Consistent Memory Reference Ordering. Source: Lamport, 1979.578 MODERN PROCESSOR DESIGN global order. Fortunately, principle allows us relax instruction ordering within out-of-order processor also allows us relax requirement creating total memory reference order. Namely, sequential execution instruction stream overspecification strictly required correctness, SC total orde r also overly rigorous strictly necessary. out-of-order processor, register renaming reorder buffer enable relaxe execution order, gated true data dependences, maintaining illusion sequential execution. Similarly, SC total order relaxed references must ordered enforc e data dependences fact ordered, while-others proceed order. allows programs expect SC total order still run cor- recdy, since th e failures occur reference order one processor exposed another via data dependences expr essed accesses shared locations. 11.3.6.2 High-Perf ormance Implementation Sequential Consistency. number factors enable efficient im plementation relax- ation SC total order. first presence caches cache coherence mechanism. Since cache coherence guarantees processor visibility processors' writes, also reveals us interprocessor data depende nces; namely!' read one processor data-dependent write another processor [i.e., read-after-wri te (RAW) dependence], coherence mechanism must intervene satisfy dependence first invalidating address ques- tion reader's cache (to guarantee single-writer coherence) then, upon subsequent read misses invalidated line, supplying dated line flushing writer's cache transmitting reader's cache. Conveniently, absenc e coherence activity (invalidates and/or cache misses), know dependence exists. Since vast majority memory references satisfied cache hits require interven- tion, safely relax reference rder cache hits. decomposes problem reference ordering local problem (ordering local ferences respect boundaries formed cache misses remote invalidat e requests) global problem (ordering cache misses invalidate requests). former accomplished augmenting processor's loa d/store queue monitor global address events addition processor-local addresses, monitors anyway enforce local stor e-to-load dependences. Cache misses upgrades ordered providing global ordering point somewhere system. small-scale systems shared address bus, arbitration single shared bus establishes global order misses invalidates, must raverse bus. directory implementation, commands rdered upon arrival directory shared point system's interconnect. However, must still solve local problem ordering references respect cohe rence events. Naively, quires must ensure prior memory references result cache hit perform current reference. Clearly, degenerates in-order execution memory refer- ences precludes high-performance out-of-order execution. Here, apply speculation solve problem enable relaxation thi ordering requirement.EXECUTING MULTIPLE THREADS 5; Out-of-order processor coreLoad queueSystem address bus Bus writes Bus upgrades In-order commitOther processors Loads issue order, loaded addresses tracked load queue. remote stores occur loads retire snooped load queue. Address matches indicate potential ordering violation trigger refetch-based recovery load attempts commit. Figure 11.8 Read Set Tracking Detect Consistency Violations. Namely, speculate particu lar reference fact need ordered, execute speculatively, recover speculation cases determine needed ordered. Since canonical out-of-order processor already supports speculation recovery, need add mecha- nism detects ordering violations initiates recovery cases. straightforward approach fo r detecting ordering violations monitor global address events check see conflict local specula- tively executed memory references. Since speculatively executed memory refer- ences already tracked process or's load/store queue, simple mechanism checks global address events (inv alidate messages correspond remote writes) unretired lo ads sufficient sh Figure 11.8, matching address causes load marked potential ordering violation. instruc- tions retired program order completion time, checked ordering violations. th e processor attempts retire load, proc essor treats load wer e branch misprediction refetches load subsequent instructions. U pon re-execution, load ordered conflicting remote write. mecha nism similar one guaranteeing adherence memory consistency model implemented MIPS R10000 [Yeager, 1996], HP PA-8000, Intel Pentium Pro processors later derivatives. 11.3.6.3 Relaxed Consistency Models. architectural alternative sequential consistency specify relaxed consistency model programmer. broad variety relaxed consistency (RC) models proposed imple- mented, various subtie differences. intereste reader referred Adve Gharachorlo o's [1996] consistency model tutorial detailed disc ussion several relaxed models. underlying motivation RC models simplify implementation hardware r equiring programmer identify label ferences need ordered, allowing hardware proceed unordered execution unlabeled references.5 8 0 MODERN PROCESSOR DESIGN Memory Barriers. common practical way lab eling ordered references require programmer insert memory barrier instructions fences code impose ordering r equirements. Typical memory barrier semantics (e.g., sync instruction PowerPC instruction set) require memory references precede barrie r complete subsequent memory ferences allowed begin. simple practical implementation memory ba rrier stalls instruction ssue earlier memory instructions completed. Care must taken ascertain memory instructions fact completed; exampl e, many processors retire stor e instructions store queue, may arbitrarily delay performing stores. Hence, checking reorder buffer contain stores sufficient; checking must extended queue retired stores. Furthermore, invalidate messages corre- sponding store may still flight coheren ce interconnect, may even delayed invalidate queue remote processor chip, even though store already performed local cache removed store queue. correctness, system guarantee invalidates origi- nating stores preceding memory b arrier actually applied, hence preventing remote accesses stale copies line, refere nces follow- ing memory barrier allowe issue. Needless say, take long time, even hundreds processor cycles systems large numbers processors. main drawback r elaxed models additional burden placed programmer identify label references need ordered. Reasoning correctness multithreaded programs difficult challenge begin with; imposing subtle sometimes counterintuitive correctness rules programmer hurt programmer productivity increase likelihood subtle errors problematic race conditions. Benefits Relaxed Consistency. main advantage r elaxed models better performance simpler hardware. advantage disappear memory bar- riers frequent enough require implementations efficient simply stal ling issue waiting pending memory references complete. efficient imple mentation memory barriers look much like invalidation trackin g scheme illustrated Figure 11.8; load addresses snooped invalidate essages, violation triggered memory barrier retired violating load retired. accomplished marking load load/store queue twice: first, conflicting invalidate occurs, second, local memory barrier retired first mark > already present. load attempts retire, refetch triggered marks present, indicating load may retrieved stale value data cache. fundamental advantage relaxed models absence mem- ory barriers, hardware greater freedom overlap latency store misses execution subsequent instructions. SC execution scheme outlined Section 11.3.6.2, overlap limited size out-of-orderEXECUTING MULTIPLE THREADS ! instruction window; window full, instructions executed pending store completed. RC system, store retired store queue, subsequent instructions retired instruction win- dow make room new ones. relative benefit distinction depends frequency memory barriers. limiting case, store followed memory barrier, RC provide performance benefit all, since instruction window full whenever would full equivalent SC sys- tem. However, even applications relati onal databases significant degree data sharing, memory barriers much less frequent stores. Assuming relatively infr equent memory barriers, performance advantage relaxed models varies size instruction window ability instruction fetch unit keep filled useful instructions, well rel- ative latency retiring store instruction. Recent trend indicate former growing better branch predictors larger order buffers, latter also increasing due increased clock freq uency systems many processors interconnected multistage networks. Given know, clear fundamental advantage RC systems translate significant performance advantage future. fact, researchers recently argued relaxed models, due difficulty reasoning correctness [Hill, 1998]. Nevertheless, recently introduced instruction sets specify relaxed consistency (Alpha, PowerPC, IA-64) serve existence proofs relative difficulty reasoning program c orrectness relaxed consistency means insurmountable problem programming community. 11.3.7 Coherent Memory Interface simple uniprocessor interfaces memory via bus allows processor issue read write commands single atomic bus transactions. simple bus, processor successfully arbitrated bus, places appropriate command bus, holds bus receives data address responses, signaling completion transaction. advanced uniprocessors add support split transactions, requests responses separated expose gre ater concurrency allow better utilization bus. split-transaction bus, processor issues request releases bus receives data response memory controller, subse- quent requests issued overlapped response latency. Further- more, requests split coherence sponses well, releasing address bus coherence responses returned. Figure 11.9 illustrates benefits split-transaction bus. Figure 11.9(a), simple bus serializes request, snoop response, DRAM fetch, data transmission latencies two requests, one address one address B. Figure 11.9(b) shows split- transaction bus releases bus every request, receives snoop responses data responses separate busses, satisfy four requests pipelined fashion less time simple bus satisfy two requests. course, design significantly complex, since multiple con current split transactions flight tracked coherence controller.582 MODERN PROCESSOR DESIGN Reg | Rsp~A | Read DRAM | XmitA (a) Simple bus atomic transactionsReqB \ R sp-B | Read B DRAM | XmitB Reg | Rsp -A jRead DRAM | j XmitA ReqB | | R sp~B | Read B DRAM | ! ReqC | XmitB Rsp-C | Read C f r DRAM | XmitC ReqD Rsp-D .Read Dfrom DRAM| XmitD (b) Split-transaction bus wit h separate requests responses split-transaction bus enables higher throughput pipelining requests, res ponses, data transmission. Figure 11.9 Simple Versus Split-Transaction Busses. Usually, tag unique systemwide associated outstanding transaction; tag, significantly sho rter physical address, used identify subsequent coherence data messages providing additional signal lines message headers data res ponse busses. outstanding transaction tracked miss-status handling register (MSHR), keeps track mi ss address , critical word information, rename register informa- tion needed restart execution memory controller returns data needed missing reference. MSHRs also used merge multiple requests line prevent transmitting request multiple times. addition, writeback bu ffers used delay writing back evicted dirty lines cache co rresponding demand miss satisfied; fill buffers used collect packetized data response whole cache line, written cache. example advanced split-transaction bus interface shown Figure 11.10. relatively simple uniprocessor nterface must augmented several ways handle coherence multiprocessor system. First all, bus arbitra- tion mechanism enhanced support multiple requesters bus masters. Second , must support handling inbound address commands originate processors system. snooping implementation, commands placed bus processors, directory implementatio n commands forwarded directory. Minimally, command set mu st provide functionality probing processor's tag array check current state line, flushing modified data cache, invalidating line. earlier microprocessor designs required external board-level co herence controllers issued low-level commands pro- cessor's cache, virtually modern processors sup port glueless multiprocessing integrating coherence controller directly p rocessor chip. on-chipEXECUTING MULTIPLE THREADS St MSHROut-of-order processor Level 1 tag array Level 2 tag array Snoop queueLoadQ Store Q Storethrough Q Critical word bypass Level 1 data array WB buffer Level 2 data array WB buffer Fill buffer System address response bus System data bus processor may communicate memory two levels cache, load queue, stor e queue, store- queue (needed LI write-through). MSHR (miss-status handling registers), snoop queue, fill buffers, wnte-back buffers. shown complex control logic coordinates activity. Figure 11.10 Processor-Memory Interface. coherence controller reacts higher-level commands observed address bus (e.g., remote read, read exclusive, invalidate), issues appropriate low-level commands local cache. expose much concurrency possible, modern processors implement snoop queues (see Figure 11.10) accept snoop commands bus process semantics pipelined fashion. 11.3.8 Concluding Remarks Systems integrate multiple processors provide coherent consistent view memory enjoyed tremendous success marketplace. provide obvious advantages system vendors customers enabling scalable, high- performance systems straightforward use write programs provide growth path entry-level enterprise-class systems. abundance thread-level parallelism many important applications key enabler systems. demand performance scalability continues grow, designers systems faced myriad tradeoffs implementing584 MODERN PROCESSOR DESIGN cache coherence shared memory inimizing th e latency communica- tion misses misses memory. 11.4 Explicitly Multithreaded Pro cessors Given prevale nce applications plentiful thread-level parallelism, obvious next step evolution microprocessors make pr ocessor chip capable executing single thread. primary motivation increase utilization expensive execution resources processor chip. time-sharing operating systems enable better utiliza- tion CPU swapp ing another thread current thread waits long-latency I/O event (illustrated Figure 3.31), c hips execute multiple threads able keep processor resources busy even one thread stalled cache miss branch isprediction. Th e straightforward approach achieving capability integrating multiple processor cores single pro- cessor chip [Olukotun et al., 1996]; least two g eneral-purpose microprocessor designs announced (the IBM POWER4 [Tendler et al., 2001] Hewlett-Packard PA-8900). relatively straightforward, inter- esting design questions arise chip mul tiprocessors. Also, discuss Section 11.5, several researchers proposed extending chip multiprocessors support speculative parallelization single-threaded programs. chip multiprocessors (CMPs) provide one extreme supporting execu- tion one thread per processor chip replicating entire processor core thread, less costly alternatives exist well. Various approaches multithreading single proce ssor core proposed even realized commercial products. hese range fine-grained multithreading (FGMT), interleaves execution multiple threads single execution core cycle-by-cycle basis; coarse-grained multithreading (CGMT), also interleaves multiple threads, coarser boundaries delimited long-latency events like cache isses; simultaneous multithreading (SMT), eliminates context switching multiple threads allowing instructions multiple simul- taneously active threads occupy proc essor's e xecution window. Table 11.2 summarizes context switch mechanism degree resource sharing sev- eral approaches on-chip multithreading. assignme nt execution resources sc hemes illustrated Figure 11.11. 11.4.1 Chip Multiprocessors Historically , improvements transistor density made possible incorpo- rate increasingly complex area-intensive architectural features ut-of- order execution, highly accurate branch predict ors, even sizable secondary caches directly onto processor chip. Recent designs also integrated coher- ence controllers enable glueless multiproce ssing, tag arrays large off-chip cache mem ories, well memory controllers direct connection DRAM. System-on-a-chip designs integrate graphics controllers, I/O devices, I/O bus interface directly chip. obvious next step, tran sistor dimensionsEXECUTING MULTIPLE THREADS 58! Table 11.2 Various approaches resource sharing context switching MT Approach Resources Shared betwee n Threads None Everything Fine-grained Everything register file control logic/state Coarse-grained Everything l-fetch buffers, register file, control logic/state SMT Everything instruction fetch buffers, return address stack, architected register file, control logic/state, reorder buffer, store queue, etc. C P Secondar cache, system interconnectContext witch Mechanism Explicit operating system context switch Switch every cycle Switch pipeline stall contexts concurrently active; switching contexts concurrently active; switching Static partitioning exec ution resources Dynamic partitioning execution resources Spatial partition Temporal partition Per cycle Per functional unit (a) CMP (b) FGMT ( c)CGMT (d) SMT Four possible alternatives are: chip multiprocessing (a), statically partitions execution bandwidth; fine-grained multiprocessing (b), executes different thread alternate cycles; coarse-grained multithreading (c), switches threads tolerate long-latency events; simultaneous multithreading (d), intermingles instructions fro multiple threads. CMP FGMT approaches partition execution resources statically, either spatial partition assigning fixed number resources processor, temporal partition time-multiplexes multiple threads onto set resources. CGMT SMT approaches allow dynamic partitioning, either per-cycle temporal partition CGMT approach, per functional unit partition SM approach. greatest flexibility highest resource utilization instruction throughput achieved SMT approach. Figure 11.11 Running Multiple Threads One Chip. Source: Tullsen etal., 1996.586 MODERN PROCESSOR DESIGN continue shrink, incorporate multiple processor cores onto piece silicon. Chip multiprocessors provide se veral obvious advantages system designers: Integrating multiple processor cores single chip ease physical challenges packaging interconnecting multiple processors; tight integra- tion reduces off-chip signaling results reduced latencies p rocessor-to- processor communication synchronization; final ly, chip-scale integration provides interesting oppor tunities rethinking perhaps sharing elements cache hierarchy coherence interface [Olukotun et al., 1996]. Shared Caches. One ob vious design choice CMPs share on- off- chip cache memory multiple cores (both IBM P0WER4 HP PA- 8800 so). approach reduces latency communication misses on-chip processors, since off-chip signaling needed resolve misses. course, sharing misses remote processors still problem, although frequency reduced. Unfortunately, also true processors executing un related threads shar e data, shared cache over- whelmed conflict misses. operating system's task scheduler mitigate conflicts reduce off-chip sharing misses scheduling processor affinity; is, scheduling related tasks processors sharing cache. Shared Coherence Interface. Another obvious choice share coh erence interface rest system. cost interface amortized two processors, likely efficiently utilized, sinc e multiple indepen- dent threads driving creating additional memory-level parallelism. course, underengineered coherence interface likely even over- whelmed traffic two processors single processor. Hence, designers must pay care ful attention make sure b andwidth demands multiple processors satisfied coherence interface. different tack, assuming on-chip shared cache plenty available signaling band- width, designers ought reevaluate write-through update-based protocols maintaining coherence chip. short, reason assume on-chip coherence maintai ned using approach chip-to-chip coherence maintained. Similarly, advanced schemes synchronization on-chip processors investigated. CMP Drawbacks. ever, CMP designs drawbacks well. First all, one always argue given equivalent silicon technology, one always build uniprocessor executes single thread faster CMP cost, since th e available die area dedicated better branch prediction, larger caches, execution resources. Furthermore, area cost multiple cores easily lead large die may cause yield manufacturability issues, particularly comes speed-binning parts high frequency; empirical evidence su ggests CMP part, even though designed nominal target frequency, may suffer yield-induced frequ ency disadvan- tage. Finally, many argue operating system oftware scalability con- straints place ceiling total number processors system wellEXECUTING MULTIPLE THR EADS 58 one imposed pa ckaging physical constraints. Hence, one might conclude CMP left niche approach may make sense cost/performance perspective subset system vendor's product range, offers fundamental advantage high end low end. Nevertheless, several system vendors announced CMP designs, offer compelling advantages, particulariy commercial server market applications con- tain plenty thread-level parallelism. IBM POWER4. Figure 11.12 llustrates IBM POWER4 chip multiprocessor. processor chip contains two deeply pipeline out-of-order proce ssor cores, private 64K- byte level-1 instruction cache priv ate 32K-byte data cache. level-1 data caches write-through; writes processors collected combined store queues within bank shared level-2 cache (shown P0 STQ PI STQ). store queues four 64-byte entries allow arbitrary write combining. three level-2 banks approximately 512K bytes size con tains multiple MSHRs tracking outstanding transactions, multiple P0 STQ PI STQ L2 tags L2 data MSHR -512KB L2 slicePower4 coreO Power4 corel LI 1$ LI D$ LIDS LI 1$ Crossbar interconnect POSTQ PI STQ L2tags L2daia MSHR SNPQ WBQ gig -512KB L2 sliceCoherent I/O interfaceCoherent I/O interface POSTQ PI STQ g g L2tags L2data MSHR SNPQ WBQPOSTQ PI STQ L2tags L2dala MSHR SNPQ WBQggg Address/ Address/ Address/ response data r esponse data respons e data interconnect inter connect interconnect Figure 11.12 IBM POWER4: Example Chip Mul tiprocessor. Source: Tendler et al, 2001.588 MODERN PROCESSOR DESIGN writeback buffers, multiple snoop queue entries handling incoming coher- ence requests. processors also share coherence interface proces- sors system, separate interface coherent I/O subsystem, well interface th e off-chip level-3 cache add on- chip tag array. store- policy level-1 data caches, coherence r equests remote pro- cessors well reads on-chip core satisfied level-2 cache. level-2 tag array maintains sharing vector two on-chip proces- sors records two cores contains shared copy cache line inclusive level-2 cache. sharing vector referenced whenever one local cores remote proc essor issues write shared line; invalidate mes- sage forwarded one local cores guarantee singl e-writer cache coherence. POWER4 design supplies tremendous ba ndwidth (in excess 100 Gbytes/s) level-2 processor cores, also provides multiple high-bandwidth interfaces (each excess 10 Gbytes/s) level-3 cache surrounding processor chips multipiocessor configuration. 11.4.2 Fine-Grained Multithreading fine-grained multithreaded processor provides two thread contexts chip switches one thread next fixed , fine-grained schedule, usually processing instructions different thread every cycle. origins fine-grained multithreading traced way back mid-1960s, Seymour Cray designed CDC-6600 supercomputer [Thornton, 1964]. CDC-6600, 10 I/O processors shared single central processor round- robin fashion, interleaving work I/O processors central processing unit. 1970s, Burton Smith proposed built DenelcorHEP, first true multithreaded processor, interleaved instructions hand- ful thread contexts single pipeline mask memory latency avoid need detec resolve terinstruction dependences [Smith, 1 991]. recent yet similar machine Burton Smith, Tera MTA, focused maximizing utilization memory access path interleaving references multiple threads path [Tera Computer Company, 1998]. recent MTA design targeted high-end scientific computing inv ested heavily high- bandwidth, low-latency path access memory. fact, memory bandwidth pro- vided MTA machine expensive resource system; hence, reasonable design processor maximize utilization. MTA machine fine-grained ultithreaded processor; is, switches threads fixed schedule, every processor clock cy cle. enough register contexts (128) fully mask main memory latency, making data cache unnecessary. path memory fully pipelined, allowing 128 threads outstanding access main memory times. main advertised benefit achine lack data cache; since cache, threads access memory uniform latency, need algorithmic compiler transformations restructure access patterns maximize utilization data cache hierarchy. Instead, com- piler concentrates identifying independent threads computation (e.g., do-across loops scientific programs) schedule 128 contexts. early performance success reported Tera MTA machine, future isEXECUTING ULTIPLE THREADS 51 currently uncertain due delays second-generation CMOS implementation (the first generation used exotic gallium arsenid e technology). Single-Thread Performance. main drawback fine-grained multithreaded processors like Tera MTA sac rifice sing le-thread performance overall throughput. Since memory reference takes 12 8 cycles complete, latency complete th e execution single thread MTA longer factor 100 compared conventional cache-based design, majority references satisfied cache fe w cycles. course, programs poor cache locality, MTA perform worse cache- based system similar memory la tency achieve much higher throughput entire set threads. Unfortu nately, many applications single- thread performance important. example, commercia l workloads restrict access shared data limiting shared references critical sections pro- tected locks. maintain high throughput software systems frequent sharing (e.g., relational database systems), important execute criti- cal sections quickly possible reduce occurrence lock contention. fine-grained multithreaded processor like MTA, one would expect contention locks increase point system throughput would dramatically adversely affected. nce, unlikely fine-grai ned multithreading suc- cessfully applied g eneral-purpose computin g domain unless somehow combined conventional means masking memory lat ency (e.g., caches). However, fine-grained multi threading specific pipe stages play important role hybrid multithr eaded designs, see Section 11.4.4. 11.4.3 Coarse-Grained Multithreading Coarse-grained multithreading (CGMT) intermediate approach multithread- ing enjoys many benefits fine-grained approach without imposing severe limits single-thread performance. CGMT, first proposed Massachu- setts Institute Technology incorporated several research chines [Agarwal etal., 1990; Fillo etal., 1995], successfully commercialized Northstar Pulsar PowerPC processors IBM [Eickemeyer et al., 1996; Storino et al., 1998]. CGMT proce ssor also provides multiple thread contexts within processor core, differs fine-grained multithreading switching contexts currently active thread stalls long-latency event, cache miss. approach makes sense in-order processor would normally stall pipeline cache iss. Rather stall, pipeline filled ready instructions alternate thread, until, rum, one threads also misses cache. manner, execution two thread contexts interleaved processor, resulting better utilization processor's execut ion resources effectively masking large fraction cache miss latency. Thread-Switch Penalty. One key design issue CGMT processor cost performing context switch threads. Since context switches occur response dynamic events cache misses, may detected late pipeline, naive context-switch implementation incu r several penalty590 MODERN PROCESSOR DESIGN cycles. Since instructions following mi ssing ins truction may already pipeline, need drained pipeline. Similarly, instructions new thread reach execution stage traversed ear- lier pipeline stages. Depending length pipeline, results one pipeline bubbles. straightforward approach avoiding thread-switch penalty replicate proce ssor's pipeline registers thread save current state pipeline context switch. Hence, alternate thread context switched back next cycle, avoiding pipeline bubbles (a similar approach employed Motorola 88000 p rocessor reduce interrupt latency). course, ar ea complexity cost shadowing pipeline state considerable. fairly short pipeline context-switch penalty three cycles, IBM Northstar/Pulsar designers found complexity merited; eliminating three-cycle switch penalty provided marginal performance benefit. reasonable, since switches trig- gered cover latency cache misses take hundred proces- sor cycles resolve; saving cycles hundreds translate worthwhile performance gain. course, design longer pipeline larger switch penalty could face different tradeoff may need shadow pipeline reg isters mitigate switch penalty fashion. Guaranteeing Fairn ess. One challenges building CGMT processor provide guarantee fa irness allocation execution resources prevent starvation occurring. long thread comparable cache miss rates, processor pipeline shared fairly among thread contexts, since thread surrender CPU alternate thread com- parable rate. However, cache miss rate thread property eas- ily controlled programmer operating system; hence, additional features needed provide fairness avoid starvation. Standard techniques operating system scheduling policies adopted: Threads low miss rates preempted time slice expires, forcing thread switch; hard- ware enforce minimum quantum avoid starvation caused premature preemption. Beyond guaranteeing fairness, CGMT proce ssor provide scheme minimizing useless execution bandwidth also maximizing execution bandwidth situations wher e single-thread throughput critical perfor- mance. former occur whenever thread bus y-wait state (e.g., spin- ning lock held thread processor) thread enters operating system idle loop. Clearly, thes e cases, available execution resources dedicated alternate thread useful work, instead expending busy-wait idle loop. latter occur whe never* thread holding critical reso urce (e.g., highly contested lock) threads system waiting source released. sce- nario, execution high-priority thread preempted, even stalled cache miss, since alternate threads may slow primary thread either directly (due thread-switch penalty overhead) indirectly (by causing additional conflict misses co ntention memory hierarchy).EXECUTING MULTIPLE THREADS 591 Thread Priorities. CGMT pr ocessor avoid pitfalls performance architecting priority scheme assigns least three levels priority—high, medium, low—to active threads. Note tha priorities operating system sense, thread process fixed priority set operating system system administrator. Rather, thread priorities vary dynamically reflect relative importance execution current execu- tion phase thread. Hence, programmer intervention required notify hardware whenever thread undergoes priority transition. example, thread enters critical section acquiring lock, transition high pri- ority; conversely, exits, reduce priority level. Similarly, thread enters idle loop begins spin lock currently held another thre ad, lower priority. course, communication r equires interface specified, usually hrough special instructions ISA identify phase transitions, also requires pro grammers place ins truc- tions appropriate locations pro grams. Alternatively, implici pattern- matching mechanisms recognize execution sequences usually accompany transitions also devised. former approach employed IBM Northstar/Pulsar processors, w specially encoded NOP instructions used indicate thread priority level. Fortunately, required software changes concentrated relatively locations perating system middle- ware (e.g., database ) realized min imal effort. Thread-Switch tate Machine. Figure 11.13 illustrates simple thread-switch state machine CGMT processor. shown, four possible states processor thr ead: running, ready, stalled, swapped. Threads tr ansition states whenever cache miss initiated completed, thread switch logic decides switch alternate thread. well-designed CGMT processor, following conditions cause thread switch occur:  cache miss occurred primary thread, alternate thread ready state.  primary thread entered idle loop, alternate nonidle thread ready state. Thread active Thread inactive Thread switch -t- Preemption( Ready ) ( Stalled Y-Cache miss | 1- Thread switchThread ready run Miss complete (^Swapped ^ Thread stalled Figure 11.13 CGMT Thread Switch State achine.592 MODERN PROCESSOR DESIGN  primary thread entered synchronization spin loop (busy wait) alternate noni dle thread ready state.  swapped thread transitioned ready state, swapped thread higher priority primary thread.  alternate ready, nonidle thread retired instruction last n cycles (avo iding starvation). Finally, forward progress guaranteed preventing preemptive thread switch occurring running thread active less fixed number cycles. Performance Cost. CGMT shown cost-effective technique improving instruction throughput. IBM reports Northstar/ Pulsar line processors gains 30% additional instruction throughput expense less 10% die area negligible effect cycle time. complexity introduced CGMT incarnation control complexity managing thread switches thread priorities, well doubling architected register file hold two threa contexts instead one. Finally, minor software changes required implement thread priorities must also fig- ured cost equation. 11.4.4 Simultaneous Multithreading final sophist icated approach on-chip multithreading allow fine-grained dynamically varying interleaving instructions multiple threads across shared execution resources. technology recently com- mercialized Intel Pentium 4 processor first proposed 1995 researchers University Washington [Tullsen, 1996; Tullsen et al., 1996]. argued prior approaches multithreading shared hardware resources across threads inefficiently, since thread-switch paradigm restricted eithe r entire pipeline minimally pipeline stage contain instructions single thread. Since instruction-level parallelism unevenly distributed, led unused instruction slots stage pipeline reduced efficiency multithreading. nstead, proposed simultaneous mult ithreading (SMT), allows instructions interleaved within across pipeline stages maximize utilization processor's execution resources. Several attributes odem out-of-order proc essor enable efficient imple- mentation simultaneous multithreading. First instructions traverse intermediate pipeline stages order, decoupled program fetch order; enables instructions different threads mingle within pipe stages, allowing resources within pipe stages fully utilized. example, data dependences within one thread restrict wide superscalar pro- cessor fro issuing one two instructions per cycle, instructions alternate independent thread used fill empty issue slots. Second, architected registers renamed share common pool physical registers; renaming removes need tracking threads resolving dataEXECUTING MULTIPLE THREADS 51 dependences dynamically. rename table simply maps architected reg- ister thread different physical reg ister, standard out-of-order execution hardware takes care r est, since dependences resolved using renamed physical register names. Finally, extensive buffers (i .e., reorder buffer, issue queues, load/store queue, retired store queue) present out-of-order processor extract smooth uneven irregular instruction-level paral- lelism utilized effectively multiple threads, since serializing data control dependences starve processor affect portion instructions belong thread encountering dependence; instructions threads still available fill processor pipeline. 11A4.1 SMT Resource Sharing. primary goal SMT design improve processor resource utilization sharing resources across multiple active threads; f act, increased parallelism created multiple simulta- neously active threads used justify deeper wider pipelines, since additional resources likely useful SMT configuration. However, less clear resources shared perhaps cannot shared. Figure 11.14 illustrates alternat ives, ranging design left shares everything fetch retire stages, design right shares execute memory tages. Regardless design point chosen, instructions multiple threads joined pipeline stage resources shared must separated end pipeline preserve precise exceptions thread. Interstage Buffer Implementation. One key issues SMT design, superscalar processor design, implementation interstage buffers track instructions traverse pipeline. fetch decode stages FetchO | Fetchl Decode Rename Z3Z Issue Ex ~T~ Mem ReureO RetirelFetchO Fetchl J Decode Decode 1 Rename ~T~ Issue HZ Ex ZEZ Mem E=3 Retired | Retirel | FetchO Fetchl Figure 11.14 SMT Resource Sharing Alternatives.594 MODERN PROCESSOR DESIGN replicated, shown left middle options Figure 11.14, stage replicated pipelines meet must support multiple simultaneous writers buffer. complicate design bas eline non-SMT processor, since single writer case. Furthermore, load /store queue reorder buffer ( ROB), used track instructions program order, must also redesigne partitioned accommodate multiple threads. partitioned per thread, design similar analogous con- ventional structures. course, partitioned design preclude best-case single- thread performance, since single thread longer able occupy available slot s. Sharing reorder buffer among multiple threads ntroduces addi- tional complexity, si nce program order must tracked separately threat}, ROB must supp ort selective fl ushing nonconsecutive entries support per-thread branch misprediction rec overy. turn requires complex free-list management, since ROB longer managed circular queue. Simi- lar issues apply load/store queue, co mplicated mem- ory consistency model implications load/store queue resolves memory data dependences; discussed briefly here. SMT Sharing Pipeline Stages. number issues affect sensible feasible attempt share resources pipeline stage; discuss hese issues stage, based pipeline structure outline Figure 11.14.  Fetch. expensive resource instruction fetch stage instruction cache port. Since cache port limited accessing contigu- ous range addresses, would difficult share single porV multiple threads, unlikely one thread would fetching instructions contiguous even spatially loc al', addresses. Hence, SMT design would likely either provide dedi- cated fetch stage per thread would ti me-share single port fine-* grained coarse-grained manner. cost dual-porting instruction, cache quite high difficult justify, likely real SMT designs employ time-sharing approach. expensive resource branch predictor. Likewise, multiporting branch predictor is~ equivalent halving effective size, time-shared approach probably makes sense. However, certain elements modern branch predictors rely serial thread semantics perform well semantics oft multiple threads interleaved arbitrary fashion. example, the^ return address tack relies FIFO (first-in, first-out) behavior programs, calls returns work reliably calls returns multi- ple threads interleaved. Similarly, branch pr edictor relies global branch history register (BHR) shown perform poorly f * branch comes interleaved threads shifted arbitrarily the?. BHR. Hence, likely time-shared branch predictor design, atS least elements need replicated thread.EXECUTING MULTIPLE THREADS 595  Decode. simple RISC inst ruction sets, th e primary task decode stage identify source destination operands resolve dependences instructions decode group. involves logic 0(n2) complexity respect decode group width im plement operand speci- fier comparators priority decoders. Since are, definition, inter-instruction dependences tween instructions different threads, may make sense partition resource across threads order reduce complexity. examp le, two four-wide decoders could operate parallel two thre ads much less logic complexity single, shared eight-wide decoder. course, design tradeoff could compromise single-thread performance cases singl e thread actually able supply eight instructions decoding single cycle. CISC instruction set, decode tage much complex ince requires determining semantics complex instruction (usually) decomposing sequence simpler, RISC-like primitives. Since complex task, may make sense share decode stage threads. However, fetch stage, may sensible time-share fine-grained coarse-grained manner, rather attempting decode instructions multiple threads imultaneously.  Rename. rename stage responsible allocating physical registers mapping architected register names physical register names. Since physical registers likely allocated common pool, makes p erfect sense share logic manages free list SMT threads. However, mapping architected register names physical register names done indexing rename mapping table architected register number either updating pping (for destina- tion operands) reading (for source operands). Since architected register numbers disjoint across threads, rename table could partitioned across threads, thus providing high bandwidth table much lower cost true multiporting. However, would imply partitioning rename stage across threads and, decode stage, poten- tially limiting single-thread throughput program abundant instruction-level parallelism.  Issue. issue stage implements Tomasulo's algorithm dynamic scheduling instructions via two-phase wakeup-and-select process: waking instructions data-ready, selecting issue candi- dates data-ready pool satisfy structural dependences. Clearly, multiple threads simultaneously share functional units, selection process must involve instructions one thread. However, instruction wakeup clearly limited int ra thread interaction; is, instruction wakes response execu tion earlier instruc- tion thread. Hence, may make sense partition issuewindow across threads , since wakeup events never cross parti- tions anyway. course, th e earlier pipe stag es, partitioning can596 MODERN PROCESSOR DESIGN EXECUTING ULTIPLE THREADS 597 mappings (in physical register file-based design) copying rename reg- ister values architected registers (in rename gister-based design). either case, superscalar retirement requires checking prioritizing write- after-write (WAW) dependences (since last committed write register must win) multipl e ports rename table architected register file. again, partitioning hardware across threads ease im ple- mentation, since WAW dependences onl occur within thr ead, commit upd ates conflict across threads. viable alternative, provided retirement latency bandwidth cr itical, time-share retirement stage fine-grained coarse-grained manner. summary, research date make clear case resource-sharing alternatives discussed here. Based limited disclosure date, Pentium 4 SMT design appears simultaneously share ssue, execute, memory stages, performs coarse-grained sharing pr ocessor front end fine-grained haring retire pipe-stages. Hence, clearly compromise SMT idea l sharing many resources possible reality cycle-time complexity challenges presented attempting maximize sharing. SMT Support erializing Instructions. instruction sets contain ins truc- tions serializing semant ics; typically, instructions affect global state (e.g., changing processor privilege level invalidating address translation) impose ordering constraints memory operations (e.g., th e memory barriers discussed Section 11.3.6.3). instructions often implemented brute-force manner, draining processor pipeline active instructions, applying semantics instruction, resuming issue following instruction. brute-force approach used instructions rel- atively rare, hence even inefficient implementation affect perfor- mance much. Furthermore, semantics required instructions quite subtle difficult mplement correctly aggressive manner, mak- ing difficult justify aggre ssive implementation. However, SMT design, frequency serializing instructions increase dramatically, since proportional number threads. example, single-threaded processor, let's assume serializing instruction occurs every 600 cycles, four-threaded SMT processor achieves three times instruction throughput single-threaded processor, occur every 200 cycles. Obviously, efficient aggressive imple- mentation uch instructions may required sustain high performance, since draining pipeline every 200 cycles severely degrade performance. execution serializing instructions update th e global state stream- lined renaming global state, register renaming stre amlines execution removing false dependences betwee n instructions. global state renamed, subsequent instructions read state delayed, earlier instructions continue read earlier instance. Hence, inst ruc- tions serializing instruction intermingled processor's instruction window. However, renaming global state may easy sounds. example, serializing updates translation-lookasidehave negative impact single-thread performance. However, researchers argued issue window logic one critical cycle-time-limiting paths future process technologies. Partitioning logic exploit presenc e multiple data-flow-disjoint threads may enable much larger overall issue window fixed cycle-time budget, resulting better SMT throughput.  Execute. execute stage realizes semantics instructions executing instruction functional unit. Sharing functional units fairly straightforward, although even oppor- tunity multithread optimization: bypass network connects functional units llow back-to-back execution dependent instructions simplified, given instructions different th reads need never bypass results. ex ample, clustered microarchitecture along lines Alpha 21264, issue logic could modified direct instruc- tions thread cluster, hence reducing likeli- hood cross-cluster result bypassing. Alt ernatively, ssue logi c could prevent back-to-back issue dependent instructions, filling gaps independent instructions alternate threads, hence avoiding need cycle-time critical ALU-output-to-ALU-input bypass path. Again, optimization may compromise single-thread performance, except extent enable higher oper ating frequency.  Memory. memory stage performs cache acce sses satisfy load instructions also responsible resolving memory dependences loads stores performing memory-related book- keeping tasks. Sharing cache access ports threads maximize utilization one prime objectives SMT design accomplished fairly straightforward manner. However, sharing hardware detects resolves memory dependences complex. hardware con sists processor's load/s tore queue, keeps track loads stores program order detects later loads alias earlier stores. Extending load/store queu e handl e multiple threads requires understanding architected memory consistency model, since certain models (e.g., sequential consistency, see Section 11. 3.6) pro- hibit forwarding store value one thread load another. handle cases, load/store queue must enhanced thread- aware, forward values stall depen- dent load cannot. may simpler provide separate load/store queues eac h thread; course, reduce degree SMT processor sharing resources across threads restrict effective window size single thread capacity partition load/store queue.  Retire. retire pipeline stage, instruction results committed pro- gram order. involves checking e xceptions anomalous conditions hen committing instruction results updating rename598 MODERN PROCESSOR DESIGN buffer (TLB) address-translation protection structures may require wholesale targeted renaming large array structures. Unfortunately, increase latency accessing structures, access paths may already cycle-time-critical. Finally, streamlining execution memory barrier instructions, used serialize memory references, requires resolving numerous subtle issues related system's memory consistency model; issues discussed Section 11.3 .6.3. One possible approach memory barriers drain pipeline selectively thread, still allowing con- current execution threads. obvious implications reorder buffer de sign, well issue logic, must selectively block ssue instructions part icular thread allowing issue continue alternate threads. case , complexity implications nontrivial l argely unex- plored research literature. Managing Multiple Threads. Many issues discussed Section 11.43 coarse-grained mul tithreading also apply, least extent, SMT designs. Namely, processor's issuing policies must provide guarantee fairness forward progress active threads. Similarly, priority policies prevent useless instructions (spin loops, idle loop) consuming execution resources present; similarly, elevated prio rity level provides maximum throughput thread phases performance-critical may also needed. However, since pure SMT design notion thread-switching, mechanism implementing policies wil l different: rather switching low-priority thread switching high-priority thread, SMT design govern execution resource allocation much finer granularity, prioritizing particular thread issue logic's instruction selection phase. Alternatively, threads various priority levels prevented occupying fixed number entries processor's execution window gating instruction fetch threads. Similar restrictions placed dynamically allocated res ource within proc essor. Examples su ch resource limits load/ store queue occupancy, restrict thread's ability stress memory sub- system; MSHR occupancy, restrict number outstanding cache misses per thread; entries branch value prediction structure, order dedicate resources high-prio rity threads. SMT Performance Cost. Clearly, many subtle issues caa affect performance SMT design. One example interference threads caches, predictors, stru ctures. published evidence indt> cates interference excessive, particularly larger structures secondary caches, effect primary caches smaller structures less clear. date, definitive evidence performance potential SMT designs preliminary announcement Intel claims 16% 289b throughput imp rovement Pentium 4 design running server workloads abundant thread-level parallelism. following paragraph sum marizes details Pentium 4 SMT design released. Since Pentium 4 design limited machine parallelism, supports two threads,EXECUTING MULTIPLE HREADS 599 implements true SMT parts issue, execute, memory stages, perhaps surprising gain much less factor 2 3 improvement reported research literature. However, clear proposals described literature feasible, SMT designs deal real implementation issues discussed scalable beyond two perhaps three simultaneously active threads. Certainly cost implementing SMT, terms implementation complexity well resource duplication, understated research literature date. Pentium 4 Hybrid Multithreading Implementation. Intel Pentium 4 processor incorporates hybrid form multithreading enables two logical processors share execution resources processor. Intel's implementation—named hyperthreading —is conceptually similar SMT proposals ap peared academic literature, differs substantial ways. limite disclosure date indicates in-order portions Pen- tium 4 pipeline (i.e., front-end fetch decode engine commit stages) multithreaded fine-grained fashioa is, two logical threads fetch, decode, retir e instructions alternating cycles, unless one threads stalled reason. latter case single thread able consume fetch, decode, commit resources processor thread solves stall. scheme could also described coarse-grained single- cycle time quantum Pentium 4 also implem ents two-stage scheduling logic, instructions placed five ssue queues first stage issued functional units five issue queues second stage. again, first stage scheduling fine-grained multithreaded: one thread place instructions issue queues given cycle. again, onethread stalled, continue place instructions issue queues stall resolved. Similarly, stores retired thread alternat- ing cycles, unless one thread stalled. essence, Pentium 4 implements combination fine-grained coarse-graine multithreading pipe stages. However, Pentium 4 imp lement true simultaneous multithreading second issue stage well execute memory stages pipeline, allowing instructions threads interleaved arbitrary fashion. Resource sharing Pentium 4 also somewhat complicated. buffers out-of-order portion pipeline (i.e., reorder buffer, load qu eue, store queue) partitioned half rather arbitrarily shared. scheduler queues partitioned les rigid manner, high-water rks prevent either thread consuming available entries. disc ussed earlier, parti- tioning resources sacrifices maximum achievable single-thread performance order achieve high throughput two threads available. high level, partitioning work well two threads ar e largely symmetric behavior, result p oor performance asymmetric differing resource utilization needs. However, effect mitigated fact Pentium 4 supports sin gle-threaded mode resource partitioning disabled, enabling single active thread consume available resources.600 MODERN PROCESSOR DESIGN 11.5 Implicitly Multithreaded Pr ocessors , ffy far restricted discussion multithreaded processors multipro- cessor systems designs exploit explicit, programmer-created threads improve instruction throughput. However , many important applications single-thread performance still paramount importance. One approach improving performance single-threaded application break thread multiple threads execution executed concurrently Rather relying programmer explicitly create multiple threads manually parallelizing application , proposals implicit multithreading (IMT) describe techniques automatically spawning threads exploiting attributes program's control flow. contrast automatic compiler-based manual parallelization scientiflh numeric workloads, ypically attempt extract thread-level parallelism occupy dozens hundreds CPUs achieve orders magnitude speedup, implicit multithreading attempts sustain half-dozen dozen hreads simultaneously. difference scale driven primarily tightly coupled nature impl icit multithreading, caused threads execution tend relatively short (tens instructions) often need communi- cate large amounts state active threads resolve data control dependences. Furthermore, heavy use speculation proposed systems requires efficient recovery misspeculation, also requires ti ght cou- pling processing elements. factors conspire make difficult scale implicit multithreading beyond handful concurrently active threads. Nevertheless, im plicit multithreading proposals claimed nontrivial speedups applications amenable conventional approaches extracting instruction-level parallelism. IMT proposals motivated desire extract much instruction- level parallelism possi ble, achieve goal filling large shared execu- tion window instructions sequenced multiple disjoint locations program's control flow graph . IMT proposals advocate IMT means building scalable nstruction windows: Implicit threads inde pen- dently sequen ced assigned executed separate processing elements, eliminating need centralized, shared execution window poses many implementation challenges. course, su ch decentralized designs must still pro- vide means satisfying data dependences processing elements; much research focused efficient solutions problem. Fundamentally, three main challenges must faced when" designing IMT processor. surprisingly, challenges faced superscalar desi gn: resolving control dependences, resolving register data- dependences, resolving memory data dependences. However, due unique characteristics IMT designs, resolving substantially difficult. proposals rely purely hardware mechanisms resolving problems, whil e others rely heavily compilation technology supported critical hardware assists. discuss challenges describe solutions proposed literature.EXECUTING MULTIPLE THREADS 601 11.5.1 Resolving Control Dependences One main arguments IMT designs difficulty effectively con- structing traversing single thread execution large enough expose significant amounts instruction-level parallelism. conventional approach constructing single thread—using branch predictor speculatively traverse program's control flow graph—is severely limited effectiveness cumulative branch prediction accuracy. example, even 95% accurate branch predictor deteriorates cumulative prediction accuracy 60% 10 consecutive branch predictions. Since many important programs five six instruc- tions conditional branches, allows branch predictor construct window 50 60 instructions likelihood branch mispredic- tion becomes unacceptably high. obvious sol ution improving branch pre- diction accuracy continues active field research; however, effort hardware required incrementally improve accuracy predictors already 95% accurate prohibitive. Furthermore, clear significant improvements branch prediction accuracy possible. Control Independence. proposed IMT designs exploit program attribute control independence increase size instruction window beyond joins control flow graph. node prog ram's control flow graph said control-independent post-dominates current node, is, execution eventually reach node regardless tervening conditional branches resolved. Figure 11.15 illustrates several sources control independence programs. proposed IMT designs, implicit threads spawned joins control flow, subroutine return addresses, across loop iterations, loop fall-through point. thread often spawned nonspeculatively, since cont rol independence guarantees program eventually reach (a) Loop-closing (b) Control-now c onvergence (c) Call/return mu ltiple sourc es control independence: (a), block C eventually follows block B since loop finite number ite rations; (b) block E always follows B independent way branch r esolves; (c), block C eventually follows block B subroutine cal l E completes. Figure 11.15 Sources Control Independence.602 MODERN PROCESSOR DESIGN initiation points. However, also sp awned speculatively, encompass cases intervening control flow cannot fully determined time thread spawned . example, loop traverses linked list may data- dependent number iterations: Spawning speculative threads multiple itera- tions future often result better performance, even speculative threads need eventually squashed incorrect. > ^ «j Spawning implicit future thread ubsequent control-independent point program's control flow ha several advantages. Firs all, intermediate branch instructions may mispredicted direcdy affect control independent thread, since executed matter wha control flow path used reach it. Hence, ex ploiting control independence allows processor skip ahead past hard-to-predict branches find useful instructions. Seco nd, skip- ping ahead positive prefetching effect. say, act fetching instructions future point control flow effectively overlap useful work current thread instruction cache misses caused future thread. Conversely, current thread may also encounter instruction cache misses overlapped execution future hread. Note prefetching effe cts impossible conventional single-threaded execution, since current future thread's instruction fetches definition serialized prefetching effect substantial; Akkary reports DMT processor fetches 40% committed instructions beyond intervening instruction cache miss [Akkary Driscoll, 1998]. Disjoint Eager Execution. interesting alternative creating imp licit threads proposed disjoint eager execution (DEE) architecture [Uht Sindagi, 1995]. Conventional eager execution attempts overcome conditional branches executing paths following branch. course, results c ombinatorial explosion paths multiple branches traversed. DEE proposal, eager execution decision tree pruned comparing cumulative branch prediction rates along branch tree ch oosing th e branch path highest cumulative prediction rate next path follow; process illustrated Figure 11.16. branch prediction rates static branch estimated using profiling, cumulative rates computed multiplying rates branch used reach branch tree. Ho wever, practical implementation reasons, Uht found assuming uniform static prediction rate branch works quite well, resulting straightforward fetch policy always backtracks fixed number levels branch tree interleaves execution alternate paths main path provided conventional branch predictor. alternate paths introduced DEE core implicit threads. Table 11.3 summ arizes four IMT proposals terms control flow attributes exploit; sources implici threads are, created, sequenced, executed; dependences resolved. cases - threads created compiler, program control flow statically analyzed determine opportun e thread creation points. simply, thread-level specula- tion (TLS) proposals create thread iteration loop compile time harness parallelism [Steffan et al., 1997]. mult iscalar proposal allows muchEXECUTING MULTIPLE THREADS 603 Assuming branch predicted 75% accuracy, cumulative branch prediction rate shown; fetching branch paths 1, 2, 3, 4, next-highest cumulative rate along branch path 5, fetched next. Figure 11.16 Disjoint Eager Execution. Source: Uht Sindagi. 1 995. Table 11.3 Attributes severa l implicit multithre ading proposals Disjoint Eager Control flow attribute exploited Source implicit threads Thread creation mechanism Thread creation sequencingThread execution Register data dependencesMultiscalar Control independence Loop bodies, control flo w joins Software/compiler Program order Distributed processing elements Software hard- ware speculation supportExecution (DEE) Control independence, cumulative branch misprediction Loop bodies, control flo w joins, cumulative branch mispredictions Implicit har dware program order Shared processing elements Hardware; nospeculationDynamic Murtt- threadlnfl (DMT? Control independence Loop exits, subroutine returns Memory data Hardware-supported Hardware dependences speculationImplicit hardware program order Shared multithreaded processing elements Hardware; data dependence prediction speculation Hardware; prediction speculationThread-Level Speculation (TLS) Control independence Loop bodies Software/compiler Program order Separate CPUs Disallowed; compiler must avoid Dependence specula- tion; checked simple extension MESI coherence604 MODERN PROCESSOR DESIGN greater flexibility compiler providing architected primitives spawning threads (called tasks multiscalar lit erature) arbitrar points program's control flow [Sohi et al., 1995; Franklin, 1993]. DEE proposal dynamically detects control indepen dence exploits within single instruction window* also creates implicit threads backtracking branch prediction tree, illustrated Figure 11.16 [Uht Sindagi, 1995]. Finally, dynamic multithreading (DMT) pro posal uses hardware detection heur istics spawn threads procedure calls well backward loop branches [Akkary Driscoll, 1998]. cases execution continues simultaneously within procedure call well following it, return site, similarly, within next loop iteration well code follo wing loop exit. Out-of-Order Thread Creation. One challenge unique DMT ap- proach threads spawned program order. exa mple, case nested p rocedure calls, first call spawn thread executing call, well executing code subroutine return site, resulting two active threads. code called procedure en counters nested procedure call spawns additional thread execute call, resulting three active threads. However, thread, though created third, actually occurs sec- ond thread program order. result, logical order buffer used design support out-of-order insertion arbitrary number instruc- tions middle set lready active instructions. see, pro- cess resolving register memory data dependences also substantially complicated out-of-order threa creation. Whether approach feasible remains seen. Physical Organization. course, constructing large window instructions half battle; design atte mpts detect exploit parallelism window must demonstrate feasible build hardware accomplishes feat. Many IMT proposals partition execution resources processor thread executes independently partition, enabling distributed scalable extraction instruction-level parallelism. Since par* tition need co ntain struction window single thread, need aggressive c urrent-generatio n design. fact, may even less aggressive. Add itional parallelism extracted overlapping execution multiple windows. TLS proposals, partition actually independent microprocessor core system similar multiprocessor, chip multiprocessor (CMP, disc ussed Section 11.4.1). contrast, DMT pro- posal relies SM T-like multithreaded execution core tracks nter- leaves implici threads instead explicit threads. DM also proposes 1 hierarchical two-level reorder buffer enables large instruction window; threads finished execution cannot committed migrate sec- ond level reorder buffer fetched second level case need re-execute due data mispredictions. Finally, DEE processor - centralized ex ecution window tracks multiple implicit threads simuha- j neously organizing window basic static program structure rather 4 EXECUTING MULTIPLE THREADS 605 dynamic single path. say, instruction window DEE prototype design, Levo, captures static view program includes hardware simultaneously tracking multiple dynamic instances static control flow constructs (e.g., loop bodies). Finally, multiscalar proposal str uctured c ircular queue processing elements. tail q ueue considered nonspeculative executes cur- rent thread task; nodes executing future tasks speculative respect control data dependences. tail thread completes execution, results retired, next node becomes nonspeculative tail node. Sim ultaneously, new future thread spawned occupy processing element freed tail thread completed execution. way, overlapping execution acros multiple proc essing elements, additional parallelism exposed beyond extracted single processing element. Thread Sequencing Retirement. One challenging asp ects IMT designs control and/or prediction hardware must sequence threads retire program der. Relying compiler assistance creating threads ease task. Similarly, queue-based machine organization multiscalar ca n leas conceptually simplify task sequencing retiring tasks. However, proposals share need control logic determines thatno correctness violations occurred task allowed retire update architected state. Control dependenc e violations fairly straightfor- ward; long nonspeculative control flow eventually reaches thread ques- tion, long control flow leaves thread proceeds next speculative thread, thread safely retired. However , resolving data dependences quite bit complex discussed following. 11.5.2 Resolving Register Data Dependences Register data dependences consist name false (WAR WAW) depen- dences true data dependences (RAW). IMT designs, conventional superscalar processors, former solved via register rena ming in-order commit. compli cation in-order commit coordinated across multiple threads , easily resolved committing threads pro- gram order. True register data dependences broken two types: depen- dences within thread intrathread dependences, dependences across threads interthread dependences. Intrathread dependences resolved stan- dard techniques st udied earlier cha pters, since instructions within thread sequenced program order, renamed, bypassed, eventually com- mitted using conventional ans. Interthread dependences, however, compli- cated fact instructions sequenced program order. reason, difficult identify co rrect producer-consumer relation- ships, since producer register-writing struction may decoded yet time co nsumer register-reading instruction becomes candidate execution. example, happen register value is606 MODERN PROCESSOR DESIGN read near beginning new thread, last write register value occur near end prior thread. Since prior thread still busy executing older instructions, instruction performs last write even fetched yet. scenario, conventional renaming hardware fails correctly capture true dependence, since producing instruction updated renaming information reflect pending write. Hence, either simplifications pro gramming model sophisticated Tenanting solu- tions necessary maintain co rrect execution. easiest solution resolving interthread register data dependences simplify programming model disallowing compile-time. Thr ead- level speculation proposals take approach. compiler creates impl icit threads parallel execut ion, simply required communicate snared operands memory loads stores. Register depende nces tracked within threads only, using well-understood techniques like register renaming Tomasulo's algorithm, ingle-threaded uniprocessor. contrast, multiscalar proposal allows register communication implicit threads, also enlists compiler's help requiring identify interthread register dependences explicitly. done communicating future thread, created, registers register file pending writes them, also marking last instruction write register prior thread's processing element knows forward future tasks write occurs. Transitively, pending writes older threads must also for- warded future threads arrive processing element. compiler embeds information write mask provided future thread spawned. Thus, helpful assistance compiler, possible effectively implement distributed, scalable dependence resolution scheme relatively straightforward hardware implementation. DEE DMT proposals assume compder assistance, however* responsible dynamically resolving data dependences. DEE proposal constructs single, likely thread execution, fetches decodes instructions along path pro gram der. Henc e, identifying data dependences along path relatively straightforward. alternate eager execution paths, treat impli cit threads discussion, similar sequential semantics, forward dependence resolution possible. However, DEE pro- posal also detects control independe nce implementing minimal control depen- dences (MCD). hardware MCD capable id entifying resolving data dependences across divergent control flow paths eventually join, paths introduced execution window DEE fetch po licy. interested reader referred Uht Sindagi [1995] description novel hardware scheme. DMT proposal, hand, sequential instructioa stream work with. Hence, challenging task identifying last write register read future thread, since instruction performing write may fetched decoded yet. simplistic solution toassume registers written current thread delay registerEXECUTING MULTIPLE THREADS 607 reads future threads al l instructions current thread fetched decoded. course, result miserable performance. Hence, DMT proposal relies data dependence speculat ion, future threads assume register operands alre ady stored register file pro- ceed execute speculatively operands. course, future threads must recover re-executing instructions older thread performs write register. DMT proposal describes complex dependence resolution mechanisms enable re-execution whenever dependence violation detected. addition , researchers explored adaptive prediction mechanisms attempt identify pending register writes based historical information. Whenever predictor identifies pending write, dependent instructions future threads stalled, hence prevented misspeculating stale data. Furthermore, register dependence problem also eased employ- ing value prediction; cases pending unknown likely pending writes, operand's value predicted, forwarded dependent operands, later verified. Many issues discussed Chapter 10 regarding value prediction, verification, recovery apply design. 11.5.3 Resolving Memory Dat Dependences Finally, implicit multithreading design must also correctly resolve memory data dependences. again, useful decompose problem intrathread interthread memory dependences. Intrathrea memory dependences, intrathread register dependences, resolved conventional well- understood techniques prior chapters: WAW WAR dependences resolved buffering stores retirement, RAW dependences resolved stalling dependent loads forwarding l oad/store queue. Interthread false dep endences (WAR WAW) also solved straight- forward manner, buffering writes future threads com mitting threads retire. subtle differences among proposed alternatives. DEE DMT proposals use structures similar conventional load/store queues buffer writes commit. multiscalar design uses complex mechanism called address resolution buffer (ARB) buffer in-flight writes. Finally, TLS proposal extends conventional MESI cache coherence allow multiple instances cache lines written future threads. future instances tagged epoch number incremented new thread. epoch number appended cache line address, allow- ing conventional MESI coh erence support multiple modified instances line. retirement logic responsible committing modified lines writing back memory whenever thread becomes nonspeculative. True (RAW) interthread memory dependences ignificantly complex true register dependences, altho ugh conceptually similar. fundamental difficulty same: since instructions fetched decoded p rogram order, later loads unable obtain dependence information res pect earlier stores, since stores may computed target addresses yet may even fetched yet.608 MODERN PROCESSOR DESIGN EXECUTING MULTIPLE THREADS 609 renaming, since multiple instances memory location flight one time, dep endent loads satisfied correct instance long writes sequence issued present store queues. course, older store still pending, mechanism fail cap- ture dependenc e information correctly load proceed potentially incorrect data restarted missing store issue. DEE Memory RAW Resolution. DEE proposa l describes mechanism conceptually similar DMT approach described greater detail. DEE employs address-interleaved, high-throughput structure capable tracking program order detecting dependence violations whenever later load reads value written earlier store. Again, since loads stores performed order, mechanism must logically sort program order flag violations actually occurred. complicated fact implicit threads spawned DEE fetch policy also contain stores must tracked separately thread. Multiscalar ARB. multiscalar address resolution buffer ( ARB) central- ized, multiported, addr ess-interleaved st ructure allows multiple in-flight stores address correctly resolved loads future threads. structure allocates tracking entry speculative load performed future thread checks subseq uent stores older threads entries. hit flag violation cause th e violating thread future threads squashed restarted. Similarly, load checked prior unretired stores, also tracked ARB, resulting data dependence satisfied data prior store, rather data cache. noted prior stores also form visibility barriers ol der unexecuted stores, due WAW ordering. example, let's say future thread n + 1 stores address A. store placed ARB. Later on, future thread n + 2 reads add ress A; read satisfied ARB thread n + l's store entry. Eventually, current thread n performs store A. naive implementatio n would find future load thread n + 2, squash refetch thread n + 2 newer future threads. However, si nce thread n + 1 performed intewening store addr ess A, violation actually occurred thread n + 2 need squashed. Implementa tion Challenges. main drawback ARB similar, cen- tralized designs track reads writes scalability. Since processing element needs high bandwidth structure, scaling significant number processing elements becomes difficult. TLS proposal avoids scalabil- ity problem using standard caching protocols filter amount traffic needs tracked. Since cache misses cache upgrades need made visible outside cache, small portion references ordered checked processing eleme nts. Ordering within threads provided conventional load store queues within processor. analogous cache- based enhancement ARB, speculative versioning cache, also beenEXAMPLETLS Memory RAW solution. Again, simplest solution employed TLS design : Future threads simply assume dependence violations wiTj occur speculatively consume latest available value particular memory address. accomplished simple extension conventional snoop-based cache coherence: speculative thread execute load causes cache miss, caches processors searched reverse program order matching address. searching reverse program order (i.e., reverse thread cre- ation order), latest write, any, id entified used satisfy load. match found, load simply satisfied memory, holds conirnit- ted state cache line. effect TLS scheme predicting actual store load dependence occur far enough apart ol der thread already performed relevan store, resulting snoop hit newer thread issues load miss. cases w store load actually executed order across speculative threads result erroneous speculation, course, since TLS emplo ying simple data dependence specula- tion, mechanism needed detect recover violations may occur. Again, simple extension exi sting cache coherence protocol employed. two cases need handled: first, future load satisfied memory, second, future load satisfied modified-cache line written earlier thread. forme r case, cache line placed future thread's cache exclusive state, since copy system. Subsequently, lder thread p erforms store cache line, hence caus- ing potential dependence violation. order perform store, older thread must snoop caches the" system obtain exclusive access line. point, future thread's copy line discovered, thread squashed due violation. latter case, future thread's load satisfied modified lin e written older thread, similar. line placed future thread's cache shared state also downgraded shared state older thread's ca che. exactly would happen satisfying remote read mod ified line, shown earlier Figure 1).5 older thread writes line again, upgrade line snoop- ing processor's caches invalidate copies. point, again, future thread's shared, copy discovered violation triggered. recovery mechanism simple: thread squashed restarted. DMT Memory RAW Resolution. DMT proposal han dles true memory dependences tracking loads stores thread separate per- thread load store queues. queues used handle intrathread memory dependences conventional manner, also used resolve interthread dependences cond ucting cross-thread associative searches earlier threads* store queues whenever load issues later threads' load queue whenever.* store ssues. match former case forward store data depen- dent load; match latter case signa l violation, since load already executed stale data, cause later thread reissue load dependent instructions. Effectively, DMT mechanism achieves memory610 MODERN PROCESSOR DESIGN proposed multiscalar. course, corresponding drawback cach e-based filtering false dependences arise due address granularity. say, since cache coherence protocols operate blocks larger single word (usually 32 128 bytes), write one word block falsely trigger violation read different word block, causing additional recovery overhead would occur fine-grained dependence mechanism. problems involved memory dependence checking mun- dane. example, limited buffer space stall effective speculation, full load store q ueue stall instruction fetch supers calar processor. Similarly, conunit bandwidth cause l imitations, particularly TLS systems, since commit ypically nvolves writing modified lines back memory. specu- lative thread modifies large number lines, writeback bandwidth limit per- formance, sinc e future thread cannot spawned unti l commits performed. Finally, TLS proposals well fine-grained proposals suf- fer inherentiy serial process searching newest previous write resolving dependences. TLS proposal, accomplished serially snooping processors reverse thread creation order. IMT pro- posals suggest parallel associative lookups, faster, expensive difficult scale large numbers processing elements. 11.5.4 Concludin g Remarks date, implicit multithreading exists research proposals. Whil e shows dramatic potential fo r improving performance beyond achievable single-threaded execut ion, c lear implementation issues dis- cussed here, well others may discovered someone attempts real implementation, ultimately prevent adoption IMT. Certainly, chip multi processor designs become widespread, quite likely simple enhancements required thread-level speculation systems fact become available. However, changes benefit applications execution characteristics match TLS hardware recompiled exploit hardware. mor e complex schemes—DEE, DMT, ultiscalar— require much dramatic changes existing p rocessor implementations, hence must meet higher standard adop ted real designs. 11.6 Executing Thread far, discussed explicidy implicitly multithre aded processor designs attempt equence instructions multiple threads execution maximize pro cessor throughput. interesting alternative several researchers proposed execute instructions multiple contexts. Although may seem counterintuitive, several potential benefits approach first proposal suggest doin g [Rotenberg, 1 999], active-stream/redundant- stream simultaneous multithreading (AR-SMT), focused fault detection. executing nstruction stream twice separate thread contexts comparingEXECUTING MU LTIPLE THREADS 611 Main thread }Detect faults comparing results 4—I Redundant thread (a) Fault detection Runahead thread llllllllPrefetch caches, resolve branches Main thread (b) Preexecution Figure 11.17 Executing Thread. execution results acros threads, transient errors processing pipeline detected. say, pipeline hardware flips bit due soft error storage cell, likelihood bit fl ipped redundant stream low. Comparing results across threads likely detect many tran- sient errors. teresting observation grew work fault detection: namely, active redundant streams end helping execute effectively. say, prefetch memory references potentially resolve branch mispredictions well. cooperative effect exploited several research proposals. discuss proposals context hese benefits—fault detection, prefetching, branch resolution—in section. Figure 11.17 illustrates uses executing thread; Figure 11.17(a) shows redundant thread used check main thread transient faults, Figure 11.17(b) shows runahead thread prefetch cache misses resolve mispredicted branches main thread. 11.6.1 Fault Detection described, original work redundant execution thread based premise inconsistencies exec ution two thread instances could used detect transient faults. AR-SMT proposal assumes baseline SMT processor enhances front end SMT pipeline repli- cate fetched instruction stream two separate thread contexts. contexts execute dependently store results reorder b uffer. commit stage pipeline enhanced compare instruction outcomes, committed, check inconsistencies. inconsistencies used identify transient errors execution pipeline. similar approach used real processor designs p lace emphasis fault detection fault tolerance. example, IBM S/390 G5 processor also perfo rms redundant execution instructions, achieves replicating pipeline hardware chip and612 MODERN PROCESSOR DESIGN ninning pipelines lock step. Similar system-level designs available Hewlett Packard's Tandem division; designs, two physical processor chips coupled run threads lockstep manner, faults detected comparing results processors other. fact, long history designs, real proposed, th e fault-tolerant comput- ing domain. DIVA pro posal [Austin, 1999] builds AR-SMT concept, instead using two threads running SMT processor, employs simple processor dynamically checks computations complex processor re-executing instruction stream. first glance, appears throughput th e pair processors would limited simpler one, sulting poor performance. fact, however, simple processor easily keep complex processor exploits fact complex pr ocessor speculatively resolved con- trol data flow dependen ces. Since case, trivial parallelize code running simple processor, since dependences removed: con- ditional branches resolved, data dependences disappear since input output operand values lready known. simple processor need verify instruction isolation, execut ing provided inputs comparing output th e provided output. instruction verified manner, then, induction, entire instruction stream also verified. Since simple processor definition easy verify correctness, trusted check operation much complex design-error-prone runahead proces- sor. Hence, approach able cover design errors addition transient faults. Dynamic veri fication simple, slower processor one sh ort- coming adequately addressed literature. long checker proc essor used verify computation (i.e., ALU operations, memory references, branches), possible triv ially parallelize checking, ince computation checked independent othe rs. However, relies complex proc essor's ability provide correct operands hese computations. words, operand communicat ion occurs within complex processor checked, ince checker relies complex processor perform correctly. Since operand communication one worst sources complexity modern out-of-order processor, one could argue checker focusing wrong problem. words, terms fault co verage, one could argue checking communication much important checking computation, since relatively straightforward verify correctness LUs computational paths viewed com- binational delay paths. hand, verifying correctness complex renaming schemes associative operand bypassing extremely difficult. Fur- thermore, soft errors complex processor's register file would also be, detected DIVA checker check operand communication. resolve shortcoming, DIVA proposal also advocates checking operand communication separately che cker processor. checker decodes instruction, reads source operands register file, writes resultEXECUTING MULTIPLE THREADS 61 operands checker register file. However, process reading writing register operands may read-after-write ( RAW), write-after-read (WAR), write-a fter-write (WAW) depende nces instructions immediately preceding following instruction checked trivial parallelize. explained detail Chapter 5, dependence detected resolved com plex dependence-checking logic (X«2) complexity respect pipeline width n. Hence, parallelizing checking process require hardware eq uivalent complexity hardware complex processor. Furthermore, if, DIVA proposal advocates, checker processor runs slower baseline processor, support wider pipeline avoid becoming execution bottleneck. thi case, checker must actually im ple- ment complex logic processor checking. investigation needed determine much problem whether pre- vent adoption DIVA design technique enhancing fault tolerance processor performance. 11.6.2 Prefetching One positive side effect redundant execution prefetching, since threads generating stream instruc tion data memory referencesf Whenever one thread runs ahead other, prefetches useful instructions data processor's caches. result net speedup, since additional memory-level parallelism exposed. key extracting significant perfor- mance benefit maximize degree runahead, slip, two threads. slipstream processor proposal [Sundaramoorthy et al., 2000] exactly that, specializing runahead thread; instead redundantly executing instructions program, runahead thread stripped instructions considered nonessential remo ved execution. Nones- sential instructions ones effect program outcome con- tribute resolving predictable br anches. Since runahead thread longer needs execute structions, able get ahead control flow program, increasing lip two threads improving timeliness prefetches creates. principle maximizing slip ensure timeliness refined proposals preexecution [Roth, 2001; Zilles, 2002; Collins et al., 2001]. proposals, profiling information used identify problematic instructions like branch instructions frequently mispredicted load instructions frequently cause cache misses. backward dynamic data flow slice instructions th en constructed compile time. instructions composing backward slice form speculative preexecution thread spawned run time available thread context SMT-like processor. preexecuted slice precompute outcome problematic instruction issue prefetch memory misses. Subsequently, worker thread catches preexecuted instruction avoids cache miss. main benefit slipstreaming preexecution implicit multi- threading proposal discussed Section 11.5 streamlined r unahead614 MODERN PROCESSOR DESIGN thread corre ctness requirement. is, since serving generate prefetches "assist" thread's execution, effect architected program tate, genera ting executing thread much easier. None issues regarding control data dependence resolution solved exactly. course, precision dependence resolution likely result useful runahead thread, since less likely issue useless prefetches paths real thread never reaches; performance issue, rather correctness issue, solved much easily. Intel de scribed fully functional software impleme ntation preexecution Pentium 4 SMT processor. implementation, runahead thread spawned assigned physical processor main thread; runahead thread prefetches instructions data main thread, resulting measurable speedup programs. alternative historicall interesting approach uses redundant execu- tion data prefetching datascalar architecture [Burger et al., 1997]. architecture, memory partitioned across several processors execute program. processors connected fast broadcast network allows communicate memory operands quickly. processor responsible broadcasting ferences local partition memory processors. manner, reference broadcast once, processor able satisfy refer ences either local memory broadcast initiated owner remote memory. policy, remote memory references satisfied request-free manner. say, processor ever needs request copy memory location; available locally, processor need wait show broadcast interconnect, since remote processor owns memory eventually execute reference broadcast result. net result average memory latency longer inc ludes request latency, consists simply transfer latency broadcast interconnect. many respects, conceptually similar redundant-stream prefetching used slip- stream preexecution proposals. 11.6.3 Branch Resolution main benefit slipstreaming preexecution early resolution branch instructions hard predict conventiona l approaches branch prediction. case slips treaming, instructions data flow ante- cedents problematic branch instructions ar e considered essential therefore executed runahead thread. branch outcome forwarded real thread reaches branch, use precomputed outcome avoid misprediction. Similarly , preexecution constructs back ward program slice branch instruction spawns speculative threa preexecute slice. main implementation chal lenge early resolution branch outc omes stems syn chronizing two threads. instruction data prefetching, synchronization necessary, since real thread's instruction fetch memory reference benefit finding target instruction data cache, insteadEXECUTING MULTIPLE THREADS 615 experiencing cache miss. effect, threads synchronized instruction cache dat cache, tolerates degree inaccuracy fetch address (due spa tial locality) well timing (due temporal locality). long prefetches timely, say occur neither late (failing cove r entire miss latency) early (where prefetched line evicted cache real thread catches references it), beneficial. However, branch resolution, preexecuted branch outcome must exactly synchronized branch instance real thread; otherwise, applied wrong branch, early resolution-based prediction may fail. threads cannot simply synchronize based static branch (i.e., branch PC), since multiple dynamic instances th e static branch exist slip-induced window instructions two threads. Hence, reference- counting scheme must employed make sure branch resolved correct preexecuted branch outcome. reference-counting scheme must keep track exactly many instances static branch separate runahead thread main thread. outcome instance stored in-order queue separates two threads; runahead thread inserts new branch out- comes one end queue, main thread removes outc omes end. queue length incorrect, two threads become unsyn- chronized, predicted outcomes likely useful. Building queue associated control logic, well mechanisms fl ushing when- ever mispredictions detected, nontrivial problem satisfac- torily resolved lit erature date. Alternatively, branch outcomes communicated indirectl existing branch predictor allowing runahead thread update predictor's state. Hence, worker thread benefit updated branch predictor state performs branch predictions, since two threads synchronize implicitly branch predictor. However, likelihood runahead thread's predictor update timely accurate low, particularly modern branch predictors multiple levels history. 11.6.4 Concluding Remarks Redundant execution instructions proposed implemented fault detection. quite likely future fault-tolerant implementations employ redundant execution context SMT processors, since overhead quite reasonable fault coverage quite helpful, particu- larly smaller transistor dimensions lead increasing vulnerability soft errors. Exploiting redundant-stream e xecution enhance performance generating prefetches resolving branches early yet reached real designs. likely purely ftware-based redundant-stream prefetching materialize near future, since least theoretically possible achieve without hard- ware changes; however, performance benefits software-only scheme less clear. reported performance benefits advanced preexecution slipstream proposals certai nly attractive; assuming baseline SMT and616 MODERN PROCESSOR DESIGN CMP designs become commonplace future, extensions required sup- porting schemes incremental enough likely least partially adopted. > Jet 11.7 Summary chapter discusses wide range real proposed des igns execute multiple threads. Many important applications, particularly server domain, contain abundant thread-level parallelism eff iciently executed systems. discussed explicit multithreaded execution context multiprocessor systems multithreaded processors. Man challenges building multiprocessor systems revolve around providing coherent consistent view memory threads execution minimizing average memory latency. Multithreaded processors enable efficient designs sharing execu- tion resources either chip level chip multiprocessors (CMP), fine- grained coarse-grained time-sharing manner multithreaded processors alternate execution multiple threads, seamlessly simultaneous multithreaded (SMT) processors. Multiple thread contexts also used speed exe- cution serial programs. Proposals range complex hardware schemes implicit multithreading hybrid har dware/software schemes employ compiler transformations critical hardware ass ists parallelize sequential programs. approaches deal correctly control data dependences, numerous implementation challenges remain. Finally, multi- ple thread contexts also used redundant execution, detect tran- sient faults improve performance preexecuting problematic instruction sequences resolve branches issue prefetches memory. Many techniques already adopted real systems; many others exist research proposals. Future designs likely adopt least proposed techniques overcome many implementation chal- lenges associated building high-throughput, high-frequency, power-efficient computer systems. REFERENCES Adve, S. V., K. Gharachorloo: "Shared memory consistency models: tutorial," IEEE Computer, 29,12,1996. pp. 66-76. Agarwal, A., B. Lim. D. Kranz, J. Kubiatowiez: "APRIL: processor architecture multiprocessing," Proc. 1SCA-17, 1990, pp. 104-114. Akkary, H-, M. Driscoll: "A dynamic multithreading processor," Proc. 31st Annual Int. Symposium Microarchitecture, 1998, pp. 226-236. Austin, T.: "DIVA: reliable ubstrate deep-submicron processor design," Proc. 32nd Annual ACM/IEEE Int. Symposium Microarchitecture (MICRO-32). Los Alamitos, IEEE Computer Society, 1999. Burger, D., S. Kaxiras, J. Goodman: "Datascalar architectures," Proc. 24th Int. Sympo- sium Computer Architecture, 1997, pp. 338-349.EXECUTING MULTIPLE THREADS 617 Censier, L., P. Feautrier: "A new solution coherence problems multicache sys- tems," IEEE Trans, Computers. C-27,12,1978, pp. 1112-1118. Charlesworth, A.: "Starfire: extending SMP envelope." IEEE MICRO, vol. 18 no. 1, 1998, pp. 39-49. Collins, J., H. Wang, D. Tullsen, C. Hughes, Y. Lee, D. La very, J. Shen: "Speculative precomputation: Long-range prefetching delinquent loads," Proc. 28th Annual Int. Sym- posium Computer Architecture, 2001, pp. 14-25. Eickemeyer, R. J., R. E. Johnson, S. R. Kunkel, M. S. Squillante, S. Liu: "Evaluation multithreaded uniprocessors commercial application environments," Proc. 23rd Annual Int. Symposium Computer Architecture, Philadelphia, ACM SI GARCH IEEE Com- puter Society TCCA, 1996, pp. 203-212. Fillo, M., S. Keckler, W. Dally, N. Carter: "The M-Machine multicomputer," Proc. 28th Annual Int. Symposium Microarchitecture (MlCRO-28), 1995, pp. 146-156. Franklin, M.: "The multiscalar architecture," Ph.D. thesis. University Wisconsin- Madison, 1993. Hammond, L., M. Willey, K. Olukotun: "Data speculation support chip- multiprocessor," Proc. 8th Symposium Architectural Support Programming Lan- guages Operating Systems, 1998, pp. 58 -69. Hill, M.: "Multiprocessors support simple memory consistency models," IEEE Computer, 31, 8,1998 , pp. 28-34. Krishnan, V., J. Torrellas: "The need fast communication hardware-based specu- lative chip multiprocessors," Int. Journal Parallel Programming, 29,1, 2001, pp. 3-33. Lamport, L. : "How make multiprocessor computer correctly executes multiprocess programs," IEEE Trans, Computers, C-28,9, 1979, pp. 690-691. Lovett, T., R. Clapp: "STiNG: CC-NUMA Computer System Commercial Marketplace," Proc. 23rd Annual Int. Symposium Computer Architecture, 1996, pp. 308-317. Olukotun, K., B. A. Nayfeh, L. H ammond, K. Wilson, K. Chang: "The case single- chip multiprocessor," Proc. 7th Int. Conf. Architectural Support Programming Languages Operating Systems (ASPLOS-VII), 1996, pp. 2-11. Rotenberg, E.: "AR-SMT: microarchitectural appro ach fault tolerance microproces- sors," Proc. 29th Fault-Tolerant Computing Symposium, 1999, pp. 84-91. Roth, A.: "Pre-execution via speculative data-driven multithreading," Ph.D. Thesis, Univer- sity Wisconsin, Madison, WI, 2001. Smith, B.: "Architecture applications HEP multiprocessor computer ystem," Proc. Int. Society Optical Engineering, 1991, pp. 241- 248. Sohi, G., S. Breach, T. Vijaykumar: "Multiscalar pro cessors," Proc. 22nd Annual Int. Symposium Computer Architecture, 1995, pp. 414-425. Steffan, J., C. Colohan, T. Mowry: "Architectural support fo r thread-leve l data specu- lation," Technical report, School Computer Science, Carnegie Mellon University, 1997. Steffan, J. G., C. Colohan, A. Zhai, T. Mowry: "A scalable approach thread-level speculation," Proc. 27th Int. Symposium Computer Architecture, 2000. Steffan. J. G., T. C. Mowry: "The potential using thread-level data speculation facilitate autom atic parallelization," Proc. ofHPCA, 1998, pp. 2-13.61B MODERN PROCESSOR DESIGN Storino, S., A. Aipperspach , J. Borkenhagen, R Eickemeyer, S. Kunkel, S. Levenstein, G. Uhlmann: "A commercial multi-threaded RISC processor," Int. Solid-Slate Circuits Conference, 1998. Sundaramoorthy, K., Z. Purser., E, Rotenberg: " Slipstream processors: Improving performance fault tolerance," Proc. 9th Int. Conf. Architectural Support Program- ming Languages Operating Systems, 2000, pp. 257-268. Tendler, J. M., S. Dodson, S. Fields, B. Sinharoy: "IBM eserver POWER4 system microarchitecture," IB Whitepaper, 2001. Tera Computer Com pany: "Hardware characteristics Tera MTA," 1998. Thornton, J. E: "Parallel operation Control Data 6600," AFIPS Proc. FJCC, part 2, 26, 1964, pp. 33-40. Tullsen, D., S. Eggers, J. Emer, H. Levy, J. Lo, R. Stamm: "Exploiting choice: ins truction fetch issue implement able simultaneous multithreading processor," Proc. 23rd Annual Symposium Computer Architecture, 1996, pp. 191-202. Tullsen, D. M.: "Simultaneous multithreading," Ph.D. Thesis, University Washington, Seattle, WA, 1996. Uht, A. K., V. Sindagi: "Disjoint eager execution: optimal form speculative execu- tion," Proc. 28th Annual ACM/IEEE Int. Symposium Microarchitecture, 1995, pp. 313-325. Wang, W.-H., J.-L. Baer, H. Levy: "Organization performance two-level virtual- real cache hierarchy," Prdc. 16th Annual Int. Symposium Computer Architecture, 1989, pp. 140-148. Yeager, K.: "The MIPS R10000 superscalar microprocessor," IEEE Micro. 16, 2, 1996, pp. 28^10. Zilles, C: "Master/slave speculative parallelization approximate code," Ph.D. Thesis, University Wisconsin, Madison, WI, 2002. HOMEWORK PROBLEMS Pll.l Using syntax Figure 11.2, show use load-linked/store conditional primitives synthesize compare-and-swap operation. P11.2 Using syntax Figure 1 1.2, show use load-linked/store conditional primitives acquir e lock variable entering criti- cal section. P1L3 processor PowerPC G3, widely deployed Apple Macin- tosh systems, p rimarily intended use uniprocessor systems, hence simple MEI cache co herence protocol. Identify discuss one reason even uniprocessor design support cache coherence. MEI protocol th e G3 adequate purpose? not? P11.4 Apple marketed G3-based dual-processor system mostly used running asymmetric workloads. words, second processor used execute parts specific applications, Adobe Photos hop, rather used symmetric manner operating system execute read thread process. AssumingEXECUTING MULTIPLE THREADS 619 multiprocessor-capable operating system (which MacOS, time, not) , explain symmetric use G3-based dual-processor system might result poor p erformance. Propose oftware solution implemented operating system would mitigate problem, explain would help. Pll.S Given MESI protocol described Figure 11.5, create similar spec- ification (state table diagram) much simpler MEI protocol. Comment much easier would implement thi protocol. PI 1.6 Many modern systems use MOESI cache coherence protocol, semantics additional state line shared-dirty: i.e., multiple copies may exist, copies state, cache line state responsible writing line back evicted. Modify table tate diagram hown Figure 11.5 include state. PI 1.7 Explain benefit accrues addition state MESI protocol. P11.8 Real coherence controllers include numerous transient states addition ones shown Figure 11.5 support split-transaction busses. example, processor issues bus read invalid line (I), line placed transient state proce ssor received valid data r esponse cause line transition shared state (S). Given spli t-transaction bus separates bus command (bus read, bus write, bus upgrade) request response, aug- ment state table state transition diagram Figure 1 1.5 incorpo- rate necessar transient states bus responses. simpli city, assume bus command line transient state gets negative acknowledge (NAK) response forces retried delay. Pll.9 Given Problem 11.8, augment Figure 11.5 eliminate least three NAK responses adding necessary additional transient states. Comment complexity resulting coherence prot ocol. P11.10 Assuming processor frequency 1 GHz, target CPI 2, per- instruction level-2 cache miss rate 1% per instruction , snoop-based cache coherent system 32 processors, 8-byte address messages (including command snoop addresses), compute inbound outbound snoop bandwidth required processor node. PI 1.11 Given assumptions Problem 11.10, assume planning enhanced system 64 processors. current level-2 cache design single-ported tag array lookup latency 3 ns. 64-processor system adequate snoop bandwidth? not, describe alternative design will. P11.12 Using equation Section 11.3.5, compute average memory latency three-level hierarchy hits level-1 cache take one620 MODERN PROCESSOR DESIGN cycle, hits level-2 cache take 12 cycles, hits level-3 cache take 50 cycles, misses memory take 250 cycles. Assume level-1 miss rate 5% misses per program reference, level-2 miss rate 2% per program reference, level-3 miss rate 0.5% per program reference P11.13 G iven assumptions Problem 11.12, compute average memory latency system level-3 cache 200 cycle latency memory (since level-3 lookup longer perfor med initiat- ing fetch memory). system perfo rms better? breakeven miss rate pe r program reference two systems (i.e., level-3 miss rate systems provide performance)? P11.14 Assume processor similar Hewlett-Packard PA-8500, single level data cache. Assume cache virtually indexed physically tagged, fou r-way associative 128-byte lines, 512 KB size. rder snoop c oherenc e messages bus, reverse-address translation able used store physical -to-virtual address mappings stored cache. Assuming fully associative reverse-address translation table 4K- byte pages, many entries must contain map entire data cache? P11.15 Given assumptions Problem 11.14, describe reasonable set- associative organization RAT till able map entire data cache. P11.16 Given the' assumptions Problem 11.14, ex plain implications reverse-address translation table able map possible entries data cache. Describe sequence events must occur whenever reverse-address translation able entry displaced due replacement Problems 17 19 two-level cache hierarchy, often convenient maintain nclusion primary cache secondary cache. common mechanism tracking clusion level-2 cache maintain presence bits level-2 directory entry indicate line also present level-1 ca che. Given following assumptions, answer following questions:  Presence bit mechanism maintaining inclusion  4K virtual memory page size  Physically inde xed, physically tagged 2-Mb yte eight-way set-associative cache 64-byte lines P11.17 Given 32K-byte eight-way set-associative leve l-1 data cache 32-byte lines, outline steps level-2 controller must follow whenever removes cache line level-2 cache. specific, explain step, make sure level-2 controller informa- tion needs complete step.EXECUTING MULTIPLE THREADS 621 P11.18 Given virtually indexed, physically tagged 16K-byte direct-mapped level-1 data cache 32-byte lines, ho w level-2 controller's job change? P11.19 Given virtually indexed, virtually tagged 16 K-byte dir ect-mapped level-1 data cache 32-byte lines, presence bits still reasonable solution better one? not? P11.20 Figure 11.8 explains read-set tracking used high-performance implementations se quentially consistent multiprocessors. shown, potential ordering vio lation detected snooping load queue refetching marked load tempts commit. Explain processor refetch right away, soon violation detected, instead waiting load commit. P11.21 Given mechanism referenced Problem 11.20, fals e sharing (where remote processor writes lower half cache line, local processor reads upper half) cause additional pipeline refetches. Propose hardware scheme would eliminate refetches. Quantify hardware cost scheme. P11.22 Given Problem 11.21, describe software approach would derive benefit. P11.23 chip multiprocessor (CMP) impl ementation enables inter esting combi- nations on-chip off-chip coherence protocols. Discuss combina- tions following coherence protocols implementation approaches relative advanta ges disadvantages. On-chip, consider update invalidate protocols, implemented snooping directories. Off-chip, consider invalidate protocols, implemented snooping directories. combinations make sense? tradeoffs? P11.24 Assume building fine-grained multithreaded processor similar Tera MTA masks memory latency large number concurrently active threads. Assuming processor supports 100 concurrently active threads mask memory latency one hundred 1-ns processor cycles. assume using conventional DRAM chips implement memory subsystem. Assume DRAM chips using 30-ns command occupancy, i.e., com- mand (read write) occupies DRAM chip interface 30 ns. Com- pute min imum number independent DRAM chip interfaces memory controller must provide prevent processor stalling turning around DRAM request every processor cycle. P11.25 Assume escribed Problem 11.24. , assume DRAM chips support page mode, sequential accesses 8 bytes made 10 ns. is, first access requires 30 ns, subse- quent accesses th e 512-byte page satisfied 10 ns. scientific workloads processor executes tend perform uni stride622 MODERN PROCESSOR DESIGN accesses large arrays. Given memory reference behavior, many independent DRAM chip interfaces need prevent pr ocessor stalling? P11.26 Existing coarse-grained multithreaded processors IBM Nonhstar Pulsar processors provide in-order execution core. Explain coarse-grained multithreading would effective processor supports out-of-order execution. PI 1.27 Existing simultaneous multithreaded processors Intel Pen- tium 4 also support out-of-order execution instructions. Explain simultaneous multithreading would e ffective in-order processor. P11.28 IMT design distributed processing elements (e.g., multiscalar TLS) must perform type load balancing ensure processing element roughly amount work. Discuss hardware- software-based load balancing schemes comment might appropriate multiscalar TLS. P11.29 implicit multithreaded processor proposed DMT design must insert instructions reorder buffer program order. implies complex free-list management scheme tracking avail- able entries reorder b uffer. physical register fde used existing out-of-order processors also requires similar free-list man- agement scheme. Comment DMT ROB management differs, all, free-list management physical register file. Describe scheme detail, using diagram pseudocode implements management algorithm. P11.30 DEE proposal appears rely fairly uniform branch prediction rates limited eager execution effective. Describe hap- pens branch mispredictions clustered nonuniform distribution (i.e., mispredicted branch likely followed one mispredictions). happens effectiveness approach? Use example show whether DEE still effective. P11.31 recent study shows TLS architecture benefits significantly silen stores. Silent stores store instructions write value memory already stored location. Create detailed sample execution shows detecting eliminating silent store substantially improve performance TLS system. P11.32 Preexecution conditional branches redundant runahead thread allows speedy resolution mispredicted branches, long branch instances bot h threads properiy synchronized. Propose detailed design keep runahead thread main thread synchro- nized purpose. Identify design challenges quantify cost hardware unit.Index Aargon, J. L., 481 Abraham, Santosh G„ 472 accelerated graphics port ( AGP), 155, 162 accumulator, 7 Acosta, R.. 208.382 active-/redundant-stream SMT, 610-612 addressability, 524 address bellow, 418 address resolution buffer (ARB), 607,609 Adve, S. V., 267.579 affector branches, 489 affector register file (ARF), 4 89-490 Agarwal, A, 589 Agerwala, Tilak, 23,87,91,378, 380, 397,440 aggregate degree parallelism, 24 agree predictor, 477-478 Akkary, FL, 562,602,604 aliasing, 266,472 load bypassing, 269-270 reducing predictors, 472-481 virtual address, 142 Allen, Fran, 373, 376,440 Allen, M., 186, 187,224,423 allocator, 353-355 alloyed history predictors, 482-483 Alpert, Don, 381,405,407,440 Alpha (AXP) architecture, 62, 387-392 PowerPC vs.. 331-322 synchronization, 565translation isses, 145 value locality, 525 Alpha 21064,383, 386,388-389 Alpha 21064A, 387 Alpha 21164,322,389-390 Alpha 21264,236, 382, 385, 390-391,471,512 Alpha 21264A 387 Alpha 2 1364,391 Alpha 21464,383,391-392,477 Alsup, Mitch, 440 AltiVec extensions, 428,431 ALU forwarding paths, 79—84 ALU instructions, 62-65 leading, pipeline hazards, 79-80, 82-84 processing. See register data flow techniques RAW hazard worst-case penalties, 7 7-78, 80 specifications, 63,65 unifying procedure and, 66-67 ALU penalties deep pipelines, 94 reducing via forwarding paths, 79-84 Amdahl, Gene, 6,18,373-374, 376-377.440 Amdahl's law, 17-18, 21, 220 AMD 29000,410AMDAHL 470V/7,60-61 AMD K5,196,381,383, 385, 410-411 predecoding, 19 8,199AMD K6 (NexGen Nx686), 233, 411-412 AMD K7 (Athlon), 382, 383, 385, 412-413,454 AMD K8 (Opteron). 134-135, 383,417 analysis, di gital systems design, 4 Anderson, Willie, 422 Ando, Hisashige, 435anomalous decisions, 461-462 anti-dependence. See WAR data dependence Apollo DN10000, 382 Apple Computer Macintosh, L 56,570 PowerPC alliance, 302, 398, 424-425 architected register file (ARF), 239 architecture, 5-6. See also instruction set architecture (ISA) ring, 436 arithmetic operation tasks, 61-62 arithmetic pipelines, 40,44-48 A/R-SMT, 610-612 Asprey, T., 393 assembly instructions, 7 assembly language, 7 assign tag, 241 associative memory, 119-120.146 Astronautics ZS-1, 378-379 Athlon (AMD K7), 382,383,385, 412^*13,454 atomic operations, 561,563 AT&T Bell Labs, 381 Auslander, Marc, 380 623624 INDEX Austin. T., 277, 612 average memory reference latency, 574 average value locality, 526 Avnon, D., 407 B backwards branch, 456 Baer, J., 153.276 Bailey, D., 386 Ball, Thomas, 456-457 Ball/Larus heuristics, 456-457 bandwidth, 40, 107-110 defined, 108 improving, 273-274 caching, 117-118 cost constraints on, 109 infinite, 110 instruction fetch, 192-195, 223 measurement of, 108-109 peak vs. sustainable, 108-109 Bannon, Pete, 389,390,440 Barren, J., 402 baseline scalar pipelined machine, 28-29 Bashe. C, 370Bashteen, A., 30 Beard, Doug, 409 Becker, M., 425Benschneider, B., 390 Bendey, B., 440Bertram, Jack, 373 biasing bit, 477 bimodal branch predictor, 323-324, 459-462, 473^174 binding, 354-355 Bishop, J., 432 bit-line precharging, 130 Blaauw, Gerrit, 370 Black, B., 15Blanchard, T., 394 Blanck, Greg, 203,433 Blasgen, M.. 401Bloch, Erich, 39, 370,440 blocks. See caching/cache memory Bluhm, Mark, 407,440 Bodik. R., 537,541 bogus branch, 499Boney, Joel, 435,440 Borkenhagen, J., 432 Bose, Pradip, 380,440 Bowhill, W.. 390 brainiacs, 321-322. 384,386-387 branch address calculator (BAC), 339, 346 branch classification, 494-495 branch condition resolution penalties, 222-223 branch condition speculation, 223, 224-226,228-229 branch confidence prediction, 501-504 branch delay slot, 455-456 branch filtering, 480 branch folding, 227-228 branch hints, 456 branch history register (BHR), 232-233, 341-343,462-463 branch history shift register (BHSR), 232-233 branch history table (BHT), 230-231,465 PowerPC 620, 307-309 branch instruction address (BIA), 223 branch ins tructions, 62-63 anticipating, 375 conditional. See conditional branch instructions control dependences and, 72 IBM ACS-1 echniques, 374-375 leading, pipeline hazards, 81, 86-87 PC-relative addressing mode, 92-93 prepare-to-branch, 375 processing. See instruction flow techniques unifying procedure and, 66-67 branch penalties. See also instruction flow techniques deep pipelines, 94-95,453-454 reducing, 91-93 worst-case, RAW hazard, 77, 80 branch prediction, 223-228,453 advanced techniques, 231-236 backwards taken/forwards not-taken, 456bimodal, 323-324,459-462, 473-474 branch condition speculation, 223,224-226,228-229 branch folding, 227-228 branch target speculation, 223-224 branch validation, 229-230 correlated (two-level adaptive), ~ 232-236,462-469 counter-based predictors, 228 decoder, 346 dynamic. See dynamic branch prediction experimental studies on, 226-22JT gshare scheme, 235-236,323-324 history-based predictors, 225-228' hybrid branch predictors, 491-497 branch classification, 494-495 multihybrid predict or, 495-496 prediction fusion, 496-497 static predictor selection, 493 tournament predictor, 491-493" Intel P6, 341-343 misprediction recovery, 228-231 multiple branches and, 236 PowerPC 620,307-311 preexecution and, 614-615 siipstreaming and, 614 static. See static branch prediction taken branches and, 236-237 training period, 471 Yen's algorithm, 341-343 branch-resume cache, 421 branch target address ( BTA), u& 220-221.223 branch target address cache (BTAC). 230-231, 307-309.497 branch target buffer (BTB), 223-224.497-500 block diagram, 343 history-based branch prediction, 226 Intel P6, 339-343 branch target prediction, 497-501 branch target speculation, 223-224 Brennan, John, 418INDEX 625 broadcast write-through policy, 569 Brooks, Fred. 370,440 BTFNT branch prediction, 456 Bucholtz,W.,39,370 buffers branch target. See branch target buffer (BTB) cache, disk drives, 156-157 collapsing, 504-506 completion, 190,305,312,313 dispatch, 190,201-203 elastic history (EHB), 485 instruction bu ffer network, 195 PowerPC 620,303-304, 311,312 instruction reuse, 530-533 interpipeline-stage, 186-190 multientry, 187-190 out-of-order designs, 385 reorder. See reorder buffer (ROB) reservation stations. See reservation stations single-entry, 186-187,188 Translation lookaside. See translation lookaside buffer (TLB) Burger, D., 614 Burgess, Brad, 425,426,428,440 Burkhardt, B., 409Burtscher, M., 527, 539, 553 busses, 161—165 bus turnaround, 131 common data (CDB), 247-248 design parameters, 163-165 design trends, 165 I/O, 162 processor busses, 161-162 simple, 163 split-transaction, 163,581-582 storage, 162-163 busy vector, 374 C cache coherence, 567-576 average memory reference latency, 574 hardware cache coherence, 168implementation, 571-574 bandwidth scaling and, 573 communication (dirty) misses and, 574 directories, 573-574 hybrid snoopy/directory systems, 574 memory latency and, 573-574 snooping, 572-573 inclusive caches, 575-576 invalidating protocols, 568, 569-571 multilevel caches, 574-576 noninclusive caches, 575 relaxation SC total order, 578-579 software, 168 updating protocols, 568-569 virtual memory and, 576 caching/cache memory, 112, 115-127 attributes of, 111 average latency, 115, 574 bandwidth benefits, 117-118 blocks block (line) size, 118 block offset, 118-119 block organization, 119-120,123 evicting, 120-121FIFO replacement, 120 locating, 118-120 LRU (least recently used) replacement, 120-121 multiword per block, 146-147 NMRU (not recently used) replacement, 121 random replacement, 121replacement policies, 120 -121 single word per block. 146-147 updating, 121-122 branch-resume cache, 421 branch arget address cache, 230-231,307-309,497 cache buffer, 15 6-157 cache coherence. See cache coherencecache misses, 115,265 cache organization and, 125 miss classification, 122-125 miss rates, 115-117 conflict aliasing solution, 473 CPI estimates, 116-117 data cache. See D-cache design parameters, 123 direct-mapped, 119,146-147.509 dual-ported data cache, 273-274 fully-associative, 119-120, 147 global vs. local hit rates, 115-116 IBM ACS-1, 375-376 implementation, 146-147 instruction cache. See I-cache multilevel caches, 574-576 multiprocessor systems, 567 nonblocking cache, 274-275, 319-320 noninclusive cache, 575 organization design of, 118-122 PowerPC 620,318-320 prefetching cache, 274,275-277 reduce memory latency, 274-277 row buffer cache, 131 set-associative, 119,120,146-148 shared cac hes, 586 speculative versioning cache, 609-610 trace cache, 23 6-237,415, 506-508 translation lookaside buffer, 149-150 two-level example, 125-127 virtually indexed, 152-153 write-allocate policy, 121 writeback cache, 122 write-no-allocate policy, 1 21-122 write-through cache, 121-122 caching-inhibited (Ca) bit, 14 2-143 Calder, Brad, 261,262,458, 509, 527,541 call rule, 457 call-subgraph identities, 524 capacity, memory, 110 capacity aliasing, 473 capacity misses, 123-125626 INDEX Capek, Peter, 440 Carmean, D., 96 CDC 6400, 226 CDC 6600,29, 39, 185-186,372, 373, 376, 588 Censier, L., 567centralized reservation stations, 201-203 Chan, K., 395Chang, Al, 380 Chang, Po-Yung, 480, 493,494 change (Ch) bit, 142-143 Charlesworth, A., 572 Chen, C, 436 Chen, T., 276Chessin, Steve, 435Chin ChingKau, 429 chip multiprocessors (CMPs), 324, 584-588 cache sharing in, 586 coherence interface sharing, 586 drawbacks of, 586-587 chip select (CS) control line, 132 chip set, 162 choice PHT, 478-479 choice predictor, 473 Christie, Dave, 411,417 Chrysos, G., 273Circello, Joe, 423,424,440 CISC architecture, 9,16 instruction decoding in, 196-198 Intel 486 example, 89-91 pipelined, 39 superscalar retrofits, 384-385 Clapp, R., 574clock algorithm, 140 clock frequency deeper pipelining and, 94 increasing pipelining, 13,17 microprocessor evolution, 2 speed demon approach, 321-322, 384, 386 clustered reservation stations, 202 coarse-grained disk arrays, 157-158 coarse-grained multithreading (CGMT), 584, 585,5 89-592 cost of, 592 fairness, guaranteeing, 590 performance of, 592thread priorities, 591 thread-switch penalty, 589-590 thread switch state machine, 591-592 Cocke, John, 23,87,91,370,372, 373-374, 376,378.380, 397,440 code generation, 237-238 coherence interface, 561.586 Cohler, E., 379cold misses, 123-125 collapsing bu ffer, 504-506 Collins, J, 613 column address strobe (CAS), 130 Colwell, R., 9,329,413,440 common data bus (CDB), 247-248 communication misses, 574 Compaq Computer, 383, 387,493 compare-and-swap primitive, 563-564 completed store buffer, 268-269 completion buffer, 426 PowerPC 620,305,312,313 completion suige. See instruction completion ;tage complex instructions, 346 complex instruction set computer (CISC). See CISC architecture compulsory aliasing, 472-473 compulsory misses, 123-125 computed branches, 524 computer system overview, 106-107 conditional branch instructions control dependences and, 72 resolution penalties, 222-223 specifications, 63-65 target address generation penalties, 220-221 unifying procedure and, 66-67 conflict aliasing, 473conflict misses, 123-125 Connors, D. A., 534Come, homas M., 236, 504 contender stack, 374 control dependences, 72 examining pipeline hazards, 76-77resolving control h azards, 78 86-87 resolving IMT designs, 601-605 control flow graph (CFG), 218,535 control independence, 601-602 , * convergent algorithms, 524 Conway, Lynn, 374,440 correct/incorrect registers (CIRs),502 corrector predictor, 490correlated (two-level adaptive) branch predictors, 232-236, 462-469 Corrigan, Mike, 431 cost/performance tradeoff model deeper pipelines, 95-96 pipelined design, 43-44 Cox, Stan, 440 Crawford, John, 39, 87, 89,90, 181,405 Cray, Seymour, 588 CRAY-1,29 CRISP micr oprocessor, 381 CRT (cathode-ra tube) monitor, 107, 154 CSPI MAP 2 00,379 Culler, Glen, 379Culler-7,379cumulative multiply, 370 cycles per instruction (CPI), U deeper pipelining and, 94,96 perfect cache, 117 reducing, 12,91-93 cycle time, 11, 12-13 Cydrome Cydra-5,418 Cyrix 6x86 (Ml), 407-409 dancehall organization, 566-567 data cache. See D-cache data-captured scheduler, 260 data dependence graph (DDG), 244 data dependences, 71-72 false. See false dat dependences memory. See memory data dependences register. See register data dependences data flow branch predictor, 489-491INDEX 627 data flow eager execution, 548, 549 data flow execution model, 245data flow graph (DFG), 535 data flow limit, 244-245, 520-521 exceeding, 261-262 data flow region reuse, 534-535 data movement tasks, 61-62 data redundancy, 523 datascalar architecture, 614 data translation lookaside buffer (DTLB), 362, 364 Davies, Peter, 417 DAXPY example, 266-267 D-cache, 62 multibanked, 205 Pentium processor, 184 PowerPC 620,318-319 TYP instruction pipeline, 68-69 DEC Alpha. See Alpha (AXP) architecture DEC PDP-11,226 DEC VAX 11/780,6 DEC VAX architecture, 6,39, 380, 387 decoder shortstop, 414 decoupled access-execute architectures, 378-379 DEE. See disjoint eager execution (DEE) deeply pipelined processors, 94-97 branch penalties in. 94-95, 453-454 Intel P6 microarchitecture, 331 Dehnert, Jim, 418 Dekker's algorithm, 576-577 DeLano, E., 393delayed branches, 78 . demand paging, 137.138-141 Denelcor HEP, 588 Denman, Marvin, 427,440 dependence prediction, 273 destination allocate, 240,241 destructive interference, 477Diefendorff, Keith, 186,187, 224, 410,413,422,423,424-425, 428,437,440 Diep, T. A, 301, 302 digital sys tems design, 4-5 direction agreement bit, 477direct-mapped memory, 119, 146-147 direct-mapped TLB, 149-150 direct memory access (DMA), 168 directory approach, cache coherence, 573-574 DirectPath decoder, 412 dirty bit, 122 dirty misses, 574 disjoint eager execution (DEE), 236. 503,602-604 attributes of, 603 control flow techniques, 603,604 data dependences, resolving, 606interthread false dependences, resolving, 607 physical organization, 604-605 disk arrays, 157-161 disk drives. 111, 155-157 disk mirroring, 159 disks, 111, 153 dispatch buffers, 190,201-203dispatching. See instruction dispatching dispatch stack, 382 distributed reservation stations, 201-203 Ditzel, Dave, 381,435 DIVA proposal, 612-613 diversified pipelines, 179, 184-186 DLX processor, 71 Dohm, N., 440 Domenico, Bob, 373 done flag, 420 DRAM, 108,112 access latency, 130-131 addressing, 130 ,133-134 bandwidth measurement, 108-109capacity per chip, 128-129 chip organization, 127—132 memory controller organization, 132-136 Rambus (RDRAM), 131-132 synchronous (SDRAM), 129,131 2-level cache hierarchy, 126 Dreyer, Bob, 405 DriscoU, M. A., 562,602,604 dual inline memory module (DIMM), 127dual operation mode, 382 dual-ported data cache, 273-274 Dunwell, Stephen, 370 dynamic branch prediction, 228-231, 458-491 alternative contexts, 482-491 alloyed history predictors, 482-483 data flow pridictors, 489-491 DHLF predictors, 485-486 loop countin g predictors, 486-487 path history predictors, 483-485 perceptron predictors, 48 7-489 variable path length predictors, 485 basic algorithms, 459-472 global-history predictor, 462-465 gshare, 469-471 index-sharing, 469-471 local-history predictor, 465-^68 mispredictions, reasons for, 471-472 per-set branch history predictor, 468 pshare, 471 Smith's algorithm, 459-462 two-level prediction ables, 462-469 branch speculation, 22 8-229 branch validation, 229-230 interference-reducing predictors, 472-481 agree predictor, 477-478 W-mode predictor, 473-474 branch filtering, 480 gskewed predictor, 474-477 selective branch inversion, 480-482 YAGS predictor, 478-480 PowerPC 604 implementation, 230-231 dynamic execution core, 254-261 completion phase, 254,256 dispatching phase, 255 execution phase, 255628 INDEX dynamic execution cere— Cont. instruction cheduler, 260-261 instruction window, 259 micro-dataflow engine, 254 reorder buffer, 256, 258-259 reservation tations, 255-258 dynamic history length fining (DHLF), 485-486 dynamic instruction reuse, 262 dynamic instruction scheduler, 260-261 dynamic multithreading (DMT), 562 attributes of, 603control flow techniques, 603,604 interthread false dependences, 607 memory RAW resolution, 608-609 out-of-order thread creation, 604 physical organization, 604register data dependences, 606-607 dynamic pipelines, 180, 186-190 dynamic random-access memory. See DRAM dynamic-static interface (DSI), 8-10, 32 E Earle, John, 377 Earle latch, 41 Eden, M., 407Eden. N. A, 478 Edmondson, John, 389,390,440 Eickemeyer, R. J., 589 elastic history buffer (EHB), 485 11-stage instruction pipeline, 57-58, 59 Emer, Joel, 273,392 Emma, P. B., 500 engineering design components, 4 enhanced gs kewed predictor, 476-477 EPIC architecture, 383 error checking, 524 error-correction codes (ECCs), 159 Ethernet, 155 Evers, Marius, 471,495 exceptions, 207-208, 265, 266exclusive-OR hashing function, 460n. 469 execute permission (Ep) bit, 142 execute stage. See instruction execution (EX) stage execution-driven imulation, 14-15 explicitly parallel instruction computing, 383 external fragmentation, 50,53,61-71 F Faggin, Federico, 405 Fagin, B., 500 false data dependences IMT designs, 605 register reuse and, 237-239 resolved Tomasulo's algorithm, 253-254 write-after-read. See WAR data dependence write-after-write. See WAW data dependence fast Fourier transform (FFT), 244-245 FastPath decoder, 417 fast schedulers, 415 fault tolerance, disk arrays, 157-160 Favor, Greg, 410,411 Feautrier, P., 567 fetch-and-add primitive, 563-564 fetch group, 192-195 fetch stage. See instruction fetch (IF) stage Fetterman, Michael, 413 Fields, B., 537, 541 Fillo, M., 589fill time, 50 final reduction, floating-point multiplier, 46 fine-grained disk arrays, 157-158 fine-grained multithreading, 584, 585,588-589 fine-grained parallelism, 27 finished loa buffer , 272-273 finished store buffer, 268-269 finite-context-method (FCM) predictor, 539finite state machine (FSM), 225-226 first in, first (FIFO), 120,140 first opcode markers, 345 Fisher, Josh, 26,31,378,440,458 Fisher's optimism, 26five-stage instruction pipeline, 53-5£ Intel 486 example, 89-91 , r ; MIPS R2000/R3000 example, 87-89 floating-point buffers ( FLBs), 246 floating-point instruction *, specifications. 63,179 , „ < J floating-point mu ltiplier, 45-48 floating-point operation stack, ±J$ (FLOS), 247 floating-point registers (FLRs), 246 tag fields used in, 248-250 value locality of, 526 floating-point unit (FPU), 203-20$ IBM 360 design, 246-247 lBMJiS/6000 implementation, 242-243 Flynn, Mike, 9,25,45-48,373,374, 377,440,519 Flynn's bottleneck, 25 forwarding paths, 79-81 ALU, 79-84 critical, 81 load, 84-86 pipeline interlock and, 82-87 forward page tables, 143-144 Foster, C, 25,377 four-stage instruction pipeline, 28, 56-58 frame buffer, 154-155 Franklin, M., 262,273, 527, 540,604 free list (FL) queue, 243 Freudenberger, St ephan M., 458 Friendly, Daniel H., 506fully-associative memory, 119-120,146 fully-associative TLB, 150-151 functional simulators, 13,14-15 function calls, 500-501,524 fused multiply add (FMA) instructions, 243, 370 fusion table. 496-497 future thread, 602INDEX 629 G Gabbay, F„ 261, 527,540 Gaddis, N., 396, 397 Garibay, Ty, 407,409 Garner, Robert, 432,440 Gelsinger, Patrick, 405 generic computations, 61-62 GENERIC ( GNR) pipeline, 5 5-56 Gharachorloo, K., 267, 579 Gibson, G., 158Gieseke, B., 391Glew, Andy, 413,440 global completion table, 430 global (G) BHSR, 233,235-236 global (g) PHT, 233,469 global-history two-level branch predictor, 462-465 global hit rates, 115-116 Gloy. N. C, 234glue code, 524 glueless mul tiprocessing, 58 2-583 Gochman, Simcha, 416,417,486 Goldman, G., 438 Golla. Robert, 426 Gonzalez, A., 533Goodman, J., 118 Goodrich, R, 424Gowan, M., 391 graduation, 420 graphical dis play, 153, 154-155 Gray, R., 440 Green, Dan, 409 Greenley, D., 438 Grochowski, Ed, 405 Grohoski, Greg, 193,224,242,380, 397,399,401,409,440 Gronowski, P., 390Groves. R. D., 193,224,242 Grundmann, W., 386,440 Grunwald, Dirk, 493, 509 gskewed branch predictor, 474-477 guard rule, 457 Gwennap, L., 324, 409,411,415 H Haertel, Mike, 440 Halfhill. T, 411,412,415,421,431 Hall, C, 400HaL SPARC64, 382, 385,435-437 Hammond, L., 562Hansen, Craig, 417 Harbison, S. P., 262 hardware cache coherence. 168 hardware de scription language (HL),5 hardware instrumentation, 14 hardware RAID controllers, 160-161 hardware TLB miss handler, 145 Hartstein, A, 96,454 hashed page tables. 143, 144-145 Hauck, Jerry, 395 hazard register, 77 Heinrich, J., 455 Hennessy, John, 71 ,417,456 Henry, Dana S., 496 Henry, Glenn, 410 Hester, P., 401 high-performance substrate (HPS), 196,380-381,409 Hill, G., 435 Hill, M., 120, 1 23,472,581 Hinton, Glenn, 382,403,404,413, 415,416,454, 501, 508 history-based branch prediction, 225-228 hits, 115-116,135 Hochsprung, Ron, 424 Hoevel, L., 9 Hollenbeck, D., 394Hopkins, Marty, 380,440 Horel, T., 122,439 Horowitz, M., 380Horst, R.. 382 HP Precision Architecture (PA), 62, 392-397 HP PA-RISC Version 1.0, 392-395 PA7100,384,392,393,394PA 7100LC, 384, 393-394 PA 7200, 394-395 PA7300LC, 394 HP PA-RISC Version 2.0, 395-397 PA 8000,199, 324, 382, 385, 395-397. 579 PA 8200, 397PA 8500. 397 PA 8600,397 PA 8700, 397,478PA 8800, 397, 586 PA 8900, 397, 584 HP Tandem division, 612Hrishikesh, M. S., 454Hsu, Peter, 41 7,418,419 Huang, J., 533Huck, J., 395Huffman, Bill, 417 Hunt, D., 397Hunt, J., 397Hwu, W., 196, 380, 534 HyperSPARC, 384,434 hyperthreading, 599 IBM pipelined RISC machines, 91-93 PowerPC alliance, 302,383,398, 424-425 IBM 360, 6 IBM 360/85, 115 IBM 360/91,6,41,201.247-254IBM 360/370,7 IBM 370,7,226IBM 7030. See IBM Stretch computer IBM 7090, 372, 376IBM 7094,377 IBM ACS-1,369, 372-377 IBM ACS-360, 377 IBM America, 380IBM Cheetah, 380IBM ES/9000/520,377, 382 IBM/Motorola PowerPC. See PowerPC IBM Northstar, 302,432, 575, 589, 592 IBM OS/400,302 IBM Panther, 380 IBM POWER architecture, 62, 383, 397-402 brainiac approach, 386-387 PowerPC. See Power PC PowerPC-AS extension, 398, 431-432 RIOS pipelines, 399-401 RS/6000. See IBM RS/6000 RSC implementation, 401 IBM POWER2, 322, 387,401-402630 INDEX IBM POWER3, 301-302, 322-323, 385,429-430 IBM POWER4, 122, 136, 301-302, 381,382,430-431,584 branch prediction, 471 buffering choices, 385 chip multiprocessor, 587-588 key attributes of, 323-324 shared caches, 586 IBM pSeries 690, 109 IBM Pulsar, 575, 589, 592 IBM Pulsar/RS64-IU, 302,432 IBM 801 RISC, 380 IBM RS/6000, 375, 380 branch prediction, 224, 227 first superscalar workstation, 382 FPU register renaming, 242-243 I-cache, 193-195 MAF floating-point unit, 203 IBM S/360, 6, 372 IBM S/360 G5, 611-612 IBM S/360/85, 375, 377 IBM S/360/91, 372-373, 375,376, 380, 520 IBM S/390,167 IBM s-Star/RS64-IV, 302,432 IBM Star series, 302.432 IBM Stretch computer, 39, 369-372 IBM Unix (AIX), 302 IBM xSeries 445, 125-126 I-cache, 62 Intel P6, 338-341 TEM superscalar pipeline, 191-195 TYP instruction pipeline, 68-69 identical computations, 48, 50, 53, 54 idling pipeline stages. See external fragmentation IEEE Micro, 439 implementation, 2,5-6 implicit multithreading (IMT), 562, 600-610 control dependences, 601-605 control independence, 601-602 disjoint eager execution (DEE), 602-604out-of-order thread creation, 604 physical organization, 604-605 thread equencing/ retirement, 605 dynamic. See dynamic multithreading (DMT) memory data dependences, 607-610 implementation challenges, 609-610 multiscalar ARB, 607, 609 true (RAW) interthread dependences. 607-609 multiscalar. See multiscalar multithreading register data dependences, 605-607 thread-level speculation. See thread-level speculation (TLS) IMT. See implicit multithreading ,(IMT) inclusion, 575-576 independent computations, 48, 50-51,53,54 independent disk arrays, 157-158 indexed memory, 146 index-sharing branch predictors, 469-471 indirect branch target, 497 individual (P) BHSR, 233-235 individual (p) PHT, 233 inertia, 461 in-order retirement (graduation), 420 input/output (I/O) systems, 106-107, 153-170 attributes of, 153 busses, 160-165 cache coherence, 168 communication I/O devices, 165-168 control flow granularity, 167 data flow, 167-168 direct memory access, 168 disk arrays, 157-161 disk drives, 155-157 graphical display, 153,154-155inbound control flow, 166-167 interrupts, 167 I/O busses, 162 , '*2c keyboard, 153-154 LAN, 153.155 long latency I/O events, 169-170 magnetic disks, 153 memory hierarchy and, 168-170 memory-mapped I/O, 166modem, 153,155 mouse, 153-154 outbound control flow, 165-166 polling system, 166-167processor busses, 161-162 RAID levels, 158-161snooped commands, 168 "storage busses, 162-163 time sharing and, 169 instruction buffer network, 195 PowerPC 620,303-304, 311,312 instruction cache. See I-cache instruction completion stage completion buffer, 426 defined, 207 dynamic execution core, 254,256 PowerPC 620,305,312.313, 318-320 superscalar pipeline, 206-209 instruction control unit ( ICU), 412-413 instruction count, 11-12,17 instruction cycle, 51-52,55 instruction decode (ID) stage, 28, 55. See also instruction flow techniques . - Intel P6, 343-346 SMT design, 595in superscalar pipelines, 195-199 instruction dispatching, 199-203 dispatching, defined, 202-203 dynamic execution core, 254-255,256-257 PowerPC 620,304,311-M5> instruction execution (EX) stage, 28,55 dynamic execution core, 254,255INDEX 631 Intel P6, 355-357 multimedia applications, 204-205 PowerPC 620, 305,316-318 SMT design, 596 superscalar pipelines, 203-206 instruction fetch (IF ) stage, 28, 55 instruction flow techniques. See instruction flow techniques Intel P6, 334-336, 338-343 PowerPC 620, 303, 307-311 SMT design, 594 superscalar pipelines, 191-195 instruction fetch unit (IFU), 338-343 instruction flow techniques, 218-237,453-518 branch confidence prediction, 501-504 branch prediction. See branch prediction high-bandwidth fetch mechanisms, 504-508 collapsing buffer, 504-506 trace cache, 506-508 high-frequency fetch mechanisms, 509-512 line prediction, 509-510 overriding predictors, 510-512 way prediction, 510 performance penalties, 219-223, 453-454 condition resolution, 222-223 target address generation, 220-221 program control flow, 218-219 target prediction, 497-501 branch target buffers, 497-500 return address stack, 500-501 instruction grouper, 381-382 instruction groups, 430 instruction length decoder (TLD), 345 instruction-level parallelism (T LP), 3, 16-32 data flow limit and, 520-521 defined, 24 Fisher's optimism, 26Flynn's bottleneck, 25 limits of, 24-27 machines for, 27-32 baseline scalar pipelined machine, 28-29 Jouppi's classifications, 27-28 superpipelined machine, 29-31 superscalar achine, 31 VLIW, 31-32 scalar superscalar evolution, 16-24 Amdahl's law, 17-18 parallel processors, 17-19 pipelined processors, 19-22 superscalar pro posal, 22-24 studies of, 377-378 instruction loading, 381 instruction packet, 425 instruction pipelining, 40, 51-54 instruction retirement stage defined, 207 IMT designs, 605in SMT design, 596-597 superscalar pipelines, 206-209 instruction reuse buffer, 530-533 coherence mechanism, 532-533 indexing/updating, 530-531 organization of, 531 specifying liv e inputs, 531-532 instruction select, 258 instruction sequencing tasks, 61-62 instruction se architecture (ISA), 1-2,4, 6-8 design specifications, 7 DSI placement and, 8-10 innovations in, 7 instruction pipelining and, 53-54 instruction types and, 61-62 modem RISC processors, 62 processor design and, 4 software/hardware contract, 6-7 software portability , 6-7 instruction set processor (ISP), 1-2 instruction set processor (ISP) design, 4-10 architecture, 5-6. See also instruction set architecture (ISA) digital systems design, 4-5dynamic-static interface, 8-10 implementation, 5-6 realization, 5-6 instructions per cycle (IPC), 17 brainiac approach, 321-322,384, 386-387 microprocessor evolution, 2 instruction splitter, 378 instruction steering block (ISB), 343-344 instruction translation lookaside buffer (ITLB), 339-341 instruction type classification, 61—65 instruction wake up, 257instruction window, 259 integer functional units, 203-205 integer future file register file (IFFRF), 413 integer instruction specifications, 6: Intel 386, 89, 90,405 Intel 48 6,39,87, 89 -91,181-183, 405,455,456 Intel 860, 382 Intel 960,402-405 Intel 960 CA, 382, 3 84,403-404 Intel 960 CF, 405 Intel 960 Hx, 405 Intel 960 MM, 384,405 Intel 4004,2 Intel 8086, 89 Intel Celeron, 332 Intel IA32 architecture, 6, 7,89, 145,165. 329, 381. See also Intel P6 microarchitecture 64-bit ex tension, 417, 565, 581 decoding instructions, 196-198 decoupled app roaches, 4 09-41' AMD K5.410-411AMD K7 (Athlon), 412-tl. AMD K6 (NexGen Nx686) 411-412 Intel P6 core, 413-415 Intel Pentium 4,415-416 Intel Pentium M, 416-417 NexGen Nx586,410 WinChip series, 410 native approaches, 405-409 Cyrix 6x86 (Ml), 407^109 Intel Pentium, 405^107632 INDEX Intel Itanium, 383 Intel Itanium 2, 125-126 Intel P6 microarchitecture, 6, 196-197, 329-367,382 basic organization, 332-334 block diagram, 330 decoupled IA32 approach, 413-415 front-end pipeline, 334-336, 338-355 address translation, 340-341 allocator, 353-355 branch misspeculation recovery, 339 branch prediction, 341-343,467 complex instructions, 346 decoder branch prediction, 346 flow, 345 I-cache ITLB, 338-341 instruction decoder (ID), 343-346 MOB allocation, 354 register alias table. See register alias table (RAT) reservation station allocation, 354-355 ROB allocation, 353-354 Yeh's algorithm, 341-343 memory operations, 337, 361-364 deferring, 363-364 load operations, 363 memory access ordering, 362 memory ordering buffer, 361-362 page faults, 364store operations, 363 novel aspects of, 331 out-of-order core pipeline, 336-337, 355-357 cancellation, 356-357 data writeback, 356 dispatch, 355-356 execution unit data paths, 356 reservation station, 355-357 scheduling, 355 Pentium Pro block diagram, 331 pipeline stages, 414pipelining, 334-338 product packaging formats, 332 reorder buffer (ROB), 357-361 event detection, 360-361 implementation, 359-360 placement of, 357-358 retirement logic, 358-360 stages pipeline, 358-360 reservation station, 336, 355-357 retirement pipeline, 337-338. 357-361 atomicity rule, 337 external event handling, 337-338 Intel Pentium, 136, 181-184, 196, 382, 384,405-407 D-cache, 205pipeline stages, 406 Intel Pentium II, 329,413-415 Intel Pentium III, 329,413-415 Intel Pentium 4, 381, 382, 415-416 branch misprediction penalty, 454 buffering choices, 385 data speculation support, 550 hyp'erthreading, 599 preexecution, 614resource sharing, 599 SMT attributes of, 592, 597 trace caching, 236-237,508 Intel Pentium M, 416-417,486-487 Intel Pentium MMX, 407 Intel Pentium Pro, 233,329, 381,385 block diagram, 331 centralized reservation station, 201 instruction decoding, 196,197 memory consistency adherence, 579 P6 core, 413-415 Intel Xeon, 125 interference correction, 480-481 internal fragmentation, 49,53, 55-58 interrupts, 207 interthread memory dependences, 607-609 interthread register dependences, 605-607intrablock branch, 505-506 Intrater, Gideon, 440 intrathread memory dependences, 607 intrathread register dependences, 605 invalidate protocols, 568,569-571 inverted page tables, 143,144-145 I/O systems. See input/output (I/O) systems iron law processo r performance, 10-11,17,96 issue latency (IL), 27-28 issue parallelism (IP), 28 issue ports, 414 issuing defined, 202-203 SMT design, 595-596 J Jacobson, Erik, 502 Jimenez, Daniel A., 487,488, 510 Johnson, L., 394 Johnson, Mike, 217,271, 380, 410,426 Jouppi, Norm, 27-28, 276, 380,440 Jourdan, S., 206 Joy, Bill, 435Juan, Toni, 486 jump instructions, 63-65 K Kaeli, David R., 500 Kagan, M., 407Kahle, Jim, 425,430 Kaminski, T, 378 Kane, G, 56, 87,455 Kantrowitz, M., 440 Katz, R.. 158 Keller, Jim, 236,389.390,417 Keltcher, C, 134,417 Kennedy, A, 428 Kessler, R., 391,471,493, 512 keyboard, 153-154 Kilbum, T, 136 Killian, Eari, 417,421,440 Kissell, Kevin, 440INDEX 6: Klauser, A.. 503 Kleiman, Steve, 435 Knebel, P., 394 Kogge, Peter, 40,43 Kohn,Les,437Kolsky, Harwood, 370 Krewell, K.. 437Krishnan, V., 562 Kroft, D., 274Krueger. Steve. 203,433,435,440 Kuehler, Jack, 431 Kurpanek, G, 395 Laird, Michael, 440 Lamport, L., 267.577 LAN (local area network), 107, 153, 155 Larus, James R., 456-457 last committed serial number (CSN),436 last issued erial number (ISN), 436 last-n value predictor, 539 last value predictor, 5 38-539 latency, 107-110 average memory reference, 574 cache hierarchy, 115 defined, 108 disk dri ves, 156 DRAM access , 130-131 improving, 109 input/output systems, 169-170 issue (IL), 27-28 load instruction processing, 277 memory, 130-131,274-279. 573-574 operation (OL), 27override, 512 queueing, 156 rotational, 156 scheduling, 548-549 seek, 156 time-shared systems, 169-170 transfer, 156 . zero, 110 Lauterbach, Gary, 122,438,439 lazy allocation, 139 LCD monitor, 107,154least recently used (LRU) policy, 120-121, 140 Lee, Chih-Chieh, 473 Lee, J., 226, 342,497 Lee, R., 395,440 Lee, S.-J., 541 Leibholz, D., 391 Lempel, O., 405 Lesartre, G, 397 Lev, L., 438 Levitan, Dave, 301, 302,429 Levy, H., 153 Lichtenstein, Woody, 379,440 Lightner, Bruce, 434,435 Lilja, D. J., 533 Lin, Calvin, 487, 488linear address, 340linearly separable boolean functions, 488 line prediction, 509—510 LINPAC routines, 267 Lipasti, Mikko H., 261,278,523. 527,535 Liptay.J., 115,377, 382 live inputs, 529,5 31-532 live range, register value, 238 Livermore Automatic Research Computer ( LARC), 370 load address prediction, 277-278 load buffer (LB), 361 load forwarding paths, 84-86 load instructions bypassing/forwarding, 267-273, 577 leading, pipeline hazards, 80-81, 84-86 processing. See memory data flow techniques RAW hazard worst-case penalties, 77-78, 80 specifications, 63-65 unifying procedure and, 66-67 value locality of, 525-527 weak ordering of, 319, 321 load-linked/store-conditional (11/stc) primitive, 563-565 load penalties deep pipelines, 94reducing via forwarding paths, 79-81,84-86 load prediction table, 277,278 load/store queue ( LSQ), 533, 596 load value prediction, 278local-history two-level branch predictor, 465^168 local hit rate, 115-116 locality reference, 113 local miss rate, 116 Loh, Gabriel H., 496 long decoder, 411lookahead unit, 371 loop branch, 486 loop branch rule, 457 loop counting branch predictors, 486-487 loop exit rule, 457 loop header rule, 457Lotz,J., 396 ,397 Loven, T, 574Ludden. J.. 440 machine cycle, 51-52 machine parallelism (MP). 22, T. MacroOp, 412^*13 Mahon, Michael, 392, 395 main memory, 111-112, lZ7-13i computer system overview, 106-107 DRAM. See DRAM memory controller, 132-136 memory module organization 132-134 interleaved (ba nked), 133-134,136 parallel, 132-134 organization of, 128reference scheduling, 135-13 weak-ordering accesses, 319, Mangelsdorf, S.,440 Marine, Sri latha, 481 map table, 239,242-243 Markstein, Peter, 380Markstein, Vicky, 380 Martin, M. M. K., 542n Matson, M., 391634 INDEX Maturana, Guillermo, 4j May, C . 302May, D., 382 Mazor, Stanley, 405 McFarland, Mack, 410 McFarling, Scott, 234-235,456, 458,469,491 McGeady, S., 404,405 McGrath, Kevin, 417 McKee, S. A., 129 McLellan. H.. 381, 389 McMahan, S., 409Mehrotra, Sharad, 440 MEI coherence protocol, 570 Meier, Stephan, 412 Melvin, Steve, 380memoization, 522, 527-528 memory alias resolution, 524 memory barriers, 580 memory consistency models, 576-581 memory cycles per instruction (MCPI), 117 memory data dependences, 72 enforcing, 266-267 examining pipeline hazards, 75 predicting, 278-279 resolving IMT designs, 607-610 memory Oota flow techniques, 262-279 caching reduce latency, 274-277 high-bandwidth systems, 273-274 load address prediction, 277-278 load bypassing/forwarding, 267-273,577 load value prediction, 278 memory accessing ins tructions, 263-266 memory dependence prediction, 278-279 ordering memory accesses, 266-267 store instruction processing, 265-266 memory hierarchy, 110-136 cache memory. See caching/cache memorycomponents of, 111-113 computer system overview, 106-107 implementation, 145-153 accessing mechanisms, 146 cache memory, 146-147 TLB/cache interaction, 151-153 translation lookaside buffer (TLB), 149-153 locality, 113-114 magnetic disks, 111main memory. See main memory memory idealisms, 110,126-127 register file. See registe r file SMT sharing resources, 596 virtual memory. See virtual memory memory interface unit ( MIU), 361 memory-level parallelism (MLP), 3 memory order buffer (MOB), 354, 361-362 memory reference prediction table, 275 memory-time-per-instruction (MTPI), 116-117 memory wall, 129 MEM pipeline stage, 67,69 Mendelson, A., 261, 527, 540 Mergen, Mark, 380 MESI coherence protocol, 570-371,607 Metaflow Lightning Thunder, 434-435 meta-predictor M, 491-492 Meyer, Dirk, 412,454 Michaud, Pierre, 472,473,474 microarchitecture, 6 microcode read-only memory (UROM), 345 microcode sequence (MS), 345 micro-dataflow engine, 254 micro-operations (uops), 196-197, 413-416 Intel P6, 331,333-334 microprocessor evolution, 2-4 Microprocessor Reports, 387 Microsoft X Box, 131millicoding, 431Mills, Jack, 405 minimal control dependences (MCD), 606 minor cycle time, 29 MIPS architecture, 417-422 synchronization, 565 translation miss handling, 145 MIPS R2000/R3000,56.59-60,71, 87-89 MIPS R4000, 30-31 MIPS R5000,384,421-422MIPS R8000,418-419MIPS R10000, 199,202,324, 382, 419-421 buffering choices, 385 memory consistency adherence, 579 pipeline stages, 419 Mirapuri, S., 30,418 mismatch RAT stalls, 353 missed load queue, 275miss-status handling register (MSHR), 582 MMX instructions, 329,405,409 modem, 153,155 MOESI coherence protocol, 570 Monaco, Jim, 434,440 monitors, 154-155 Montanaro, J., 389Montoye, Bob, 380Moore, Charles, 40 1,425,430,439 Moore, Gordon, 2 Moore's Law, 2, 3 Moshovos, A, 273, 278 Motorola, 302, 383, 398,422-425 Motorola 68K/M68K, 6, 39 Motorola 68040, 6 Motorola 68060, 381,385, 423-424 Motorola 88110,186,187,204,224, 382,385,422-423 mouse. 153-154 Moussouris, J., 56, 87 Mowry, T. C, 562 Moyer, Bill, 422 MTPI metric, 116-117 Mudge, Trevor N., 478 Munich, John, 425 Multiflow TRACE computer, 26INDEX multihybrid branc h predictor, 495-496 multimedia applications, 204-205 multiple threads, executing, 559-622 explicit multithreading, 561, 584-599 chip multiprocessors, 324. 584-588 coarse-grained (CGMT), 584, 585,589-592 fine-grained (FGMT), 584, 585,588-589 SMT. See simultaneous multithreading (SMT) implicit multithreading. See implicit multithreading (IMT) multiprocessor systems. See multiprocessor systems multiscalar proposal. See multiscalar multithreading thread execution. See redundant ex ecution serial program parallelization, 561-562 synchronization, 561,562-565 multiply-add-fused (MAF) unit, 203-204 multiprocessor systems, 561,565-584 cache coherence. See cache coherence coherent memory interface, 581-583 glueless multiprocessing, 582-583 idealisms of, 566 instantaneous write propagation, 567 memory consistency, 576-581 memory barriers, 580 relaxed consistency, 579-581 sequential consistency, 577-579 uniform vs. nonuniform memory access, 566-567 multiscalar multithreading, 562 address resolution buffer, 607,609 attributes of, 603control flow techniques, 602-604physical organization, 605 register data dependences, 606 Myers, Glen, 402 N Nair, Ravi, 227- 228,484 National Semiconductor Swordfish, 381,395 negative interference, 477 neutral interference, 477 NexGen Nx586, 381,410 NexGen Nx686,233,411-412 Nicolau, A., 26Noack, L., 440nonblocking cache, 274-275, 319-320 noncommitted memory serial number pointer, 436 non-data-captured scheduler, 260-261 noninclusive caches, 575nonspeculative exploitation, value locality, 527-535 basic block/trace reuse, 533-534 data flow region reuse, 534-535 indexing/updating reuse buffer, 530-531 instruction reuse, 527,529-533 live inputs, specifying, 531-532 memoization, 522, 527-528 reuse buffer coherence mechanism, 532-533 reuse buffer organization, 531 reuse h istory mechanism, 529-533 reuse mechanism, 533 nonuniform memory access (NUMA), 566 -567 nonvolatility memory, 110 Normoyle, Kevin, 440 recently used (NMRU), 121 O'Brien, K, 400 O'Connell, F., 30 2,322,430 O'Connor, J. M., 438 Oehler, Rich, 193.224.242.397. 401,424 Olson, Tim, 440Olukotun, K„ 584,586 op code rule, 457operand fetch (OF), 55 operand store (OS), 55 operation latency (OL), 27 Opteron (AMD K8), 134-135, 383,417 out-of-order execution, 180. Se dynamic execution core output data dependence. See V data dependence override latency, 512 overriding predictors, 510-5 li P page faults, 138, 140, 141,26f Intel P6, 364 TLB miss, 151 page miss handler (PMH), 362 page-mode accesses, 131 page table base register (PTBF page tables. 142-145, 147-15: page walk, 364Paley, Max, 373 Pan, S. , 462 Papworth, Dave, 413,415 parallel pipelines, 179, 181—If See also superscalar mai partial p roduct generat ion, 45 partial product reduction, 45-" partial resolution, 500 partial update policy, PHT, 47 partial write RAT stalls, 352-: path history branch predictors 483^185 Patkar, N., 436Patt, Yale, 8, 196,232-233,3 380, 409,415,440,45f 468-469 pattern history table (PHT), 2j choice. 478-479 global (g), 233,469 individual (p), 233 organization alternatives, L partial update policy, 474, per-address (p), 469,471 per-set (s), 469 shared (s), 233, 234-235636 INDEX partem table (FT), 342 Patterson, David, 71,158, 160,432 PC mod 2m hashing function, 460n PC-relative addressing mode, 91-93 Peleg, A., 405 pending target return queue (PTRQ), 243 Peng, C. R., 429 per-address pattern tory table (pPHT),469,471 per-branch (P) BHSR, 233-235 perceptron branch predictor, 487-489 performance simulators, 13-16 trace-driven, 13-14, 306 VMW-generated, 301, 305-307 per-instruction miss rate, 116-117 peripheral component interface (PCI), 108 permission bits, 142-143 per-set branch history table (SBHT), 468 per-set pattern history table (sPHT), 469 persistence memory, 110 personal computer (PC), 3 phantom branch, 499PHT. See pattern history table (PHT) physical address, 136-137 physical address buffer (P AB), 361 physical destinations (PDst's), 351-352 pipelined processor design. 54-93 balancing pipeline stages, 53, 55-61 example instruction pipelines, 59-61 hardware requirements, 58-59 stage quantization, 53, 55-58  commercial pipelined processors, 87-93 CISC example, 89-91 RISC example, 87-89 scalar processor performance, 91-93 deeply pipelined processors, 94-97 optimum pipeline depth, 96pipeline stall minimization, 71-87 forwarding paths, 79-81 hazard identification, 73-77 hazard resolution, 77-78 pipeline interlock hardware, 82-87 program dependences, 71-73 pipelining fundamentals. See pipelining fundamentals pipelining idealism, 54 trends in, 61 unifying instruction types, 61-71 classification, 61-65instruction pipeline implementation. 68-71 optimization obj ectives, 67-68 procedure for, 65-68 resource requirements, 65-68 specifications, 63-65 pipelined processors, 39-104. See also pipelined processor design; pipelining fundamentals Amdahl's law, 21 commercial, 87-93 deep pipelines, 94-97 effective degree pipelining, 22 execution profiles, 19-20 performance of, 19-22 stall cycles. See pipeline talls superpipelined machines, 29-31 superscalar. See superscalar machines TYP pipeline, 21-22 pipeline hazards data dependences, 71-72 hazard register, 77 identifying, 73-77 resolving, 77-78, 82-87 TYP pipeline example, 75-77 pipeline interlock, 82-S7 pipeline stalls, 20-21, 51 dispatch stalls, 311-314 issue stalls, PowerPC 620, 316-317 minimizing, 53, 71-87RAT stalls, 352-353 rigid pipelines and, 179-180 pipelining fundamentals, 40-54 arithmetic pipelines, 40,44-48 nonpipelined floating-point multiplier, 45-46,47 pipelined float ing-point mul- tiplier, 46-48 instruction pipelining, 51-54 instruction pipeline design, 51-53 ISA impacts, 53-54 pipelining idealism and, 52-54 pipelined design, 40-44 cost/performance tradeoff, 43^14 limitations, 42-43 motivations for, 40—42 pipelining defined, 12-13 pipelining idealism. See pipelining idealism pipelining idealism, 40,48-51 identical computations, 48.50, 53,54 independent computations, 48, 50-51, 53, 54 instruction pipeline design and, 52-54 pipelined processor design and, 54 uniform sub computations, 48-49,53 Pleszkun, A, 208pointer rule, 457 polling algorithms, 524 pooled register file, 242-243 Popescu, V., 208,435 Potter, T, 425Poursepanj, A, 426 power consumption, 3 branch mispredictions and, 503 optimum pipeline depth and, 96-97 PowerPC, 6, 62,145, 302-305 32-bit architecture, 424-429 64-bit architecture, 429-431 relaxed memory consistency, 581 RISC attributes, 62INDEX 6 synchronization, 565 value locality, 525 PowerPC e500 Core, 428^*29 PowerPC 601, 3 02,382,425 PowerPC 603,302,425^126 PowerPC 603e, 426 PowerPC 604,6,230-231,302, 426-427 buffering choices, 385 pipeline stages, 427 PowerPC 604e, 427 PowerPC 620,199, 301-327,429 Alpha AXP vs., 321-322 architecture, 302-305 block diagram, 303 bottlenecks, 320-321 branch prediction, 307-311 buffering choices, 385 cache effects, 318-320 complete stage, 305,318-320 completion buffer. 305,312,313 conclusions/observations, 320-322 dispatch st age, 304, 311-315 execute stage, 305, 31 6-318 experimental framework, 305-307 fetch stage, 303, 307-311 IBM POWER3 vs., 322-323 IBM POWER4 vs., 323-324 instruction buffer, 303-304 instruction pipeline diagram, 304 latency, 317-318 parallelism, 315,317,318 reservation stations, 201, 304-305 SPEC 92 benchmarks, 30 5-307 weak-ordering memory access, 319.321 writeback stage, 305 PowerPC 750 (G3), 302, 385, 428,570 PowerPC 970 (G5), 112,431 PowerPC 7400 (G4), 302,428 PowerPC 7450 (G4+), 428 PowerPC-AS. 398,431^132 PowerPC-AS A10 (Cobra), 432 PowerPC-AS A30 (Muskie), 432 PowerPC-AS A35 (Apache, RS64), 432PowerPC-AS A50 (Star series). 432 precise exceptions, 208, 385 predecoding, 198-199 prediction fusion, 496-497 preexecution, 562,613-615 prefetching, 90,109 IBM POWER3, 323 prefetching cache, 274,275-277 prefetch queue, 275 redundant execution, 613-614 Prener, Dan, 380 Preston, R., 392 primary (Ll) cache, 111, 112,274 primitives, synchronization, 563-565 Probert, Dave, 440processor affinity, 586 processor performance, 17 Amdahl's law, 17-18, 21, 220 baseline scalar pipelined machine, 28-29 cost vs., 43-44, 95-96, 598-599 equation for, 10-11 evaluation methods, 13-16 iron law of, 10-11,17,96 optimizing, 11-13 parallel processors, 17-19 pipelined processors, 19-22 principles of, 10-16 scalar pipelined RISC machines, 91-93 sequential bottleneck and, 19 simulators. See performance simulators vectorizability and, 18-19 program constants, 524 program counter (PC), 76-77, 192 program parallelism, 22 Project X, 372-373 Project Y, 373 pseudo-operand, 250pshare algorithm, 471 Pugh, E., 373, 377 Puzak, Thomas R., 96,454 Q QED RM7000,421-422 quadavg instruction. 204-205queuing latency, 156 queuing time, 108 R RAID levels, 158-161 Rambus DRAM (RDRAM), 131- RAM digital-to-analog converter (RAMDAO, 154 Randell, Brian, 373 Rau, Bob. 378 RAW data dependence, 71-72 interthread dependences memory, 607-609 register, 605-607 intrathread dependences memory, 607 register. 605 load/store instructiot 266-267 memory controller, 135 register data flow and, 244-2- RAW hazard, 73 detecting, 83-84 necessary conditions for, 7 4 - penalty reduction, 79-81 resolving, 77-78 TYP pipeline, 76-77 worst-case penalties, 77, 80 Razdan, R., 391 read-after-write. See RAW data dependence read permission (Rp) bit, 142 ReadQ command, 134-135 realization, 5-6 Reches, S., 484 reduced instruction set computer See RISC architecture redundant arrays inexpensive disks. See RAID levels redundant ex ecution, 610-616 A/R-SMT, 610-612 branch resolution, 614-615 datascalar architecture, 614 DIVA proposal, 612-613 fault detection. 611-613 preexecution, 562, 613-615 prefetching, 613-614slipstreaming. 613-615638 INDEX reference (Ref) bit, 142 refetching, 546 register alias table (RAT), 333, 336, 346-353 basic operation, 349-351 block diagram, 347 floating-point overrides, 352 implementation details, 348-349 integer retirement overrides, 351 new PDst overrides, 351-352 stalls, 352-353 register data dependences, 72 IMT designs, 605-607 pipeline hazards of, 75-76 register data flow techniques, 237-262,519-558 data flow limits. 244-245 dynamic execution core. See dynamic execution core dynamic instruction reuse, 262 false data dependences, 237-239 register allocation, 237-238 register renaming. See register renaming register reuse problems, 237-239 Tomasulo's algorithm, 246-254 true data dependences, 244-245 value locality. See value locality value prediction, 261-262, 521-522 register file, 112-113,119. See also register data flow techniques attributes of, 111 definition (writing) of, 238pooled, 242-243 read port saturation, 312 TYP instruction pipeline interface, 69-70 use (reading) of, 238 register recycling, 237-239 register renaming, 239-244 destination allocate, 240. 241 dynamic execution core, 255 instruction scheduling and, 261 map table approach, 242-243 pooled register file, 242-243 register update, 240, 241-242rename register file (RRF), 239-240 registers in, 360saturation , 313 source read, 240-241 register spill code. 524 register transfer language (RTL), 5,15 register update, 240Reilly, M., 440Reininger, Russ, 425 relaxed consistency (RC) models, 579-581 reorder buffer (ROB), 208,209 dynamic execution core, 256, 258-259 Intel P6, 353-354,357-361 reservation station, combined, 259 RRF attached, 239-240 SMT design, 594 reservation stations, 201-203,209 dispatch step, 256-257 dynamic execution core, 255-258 entries, 255 IBM 360/91,246-248 instruction wake up, 257Intel P6, 336,355-357 issuing hazards, 316-317 issuing step, 258 PowerPC 620,304-305 reorder buffer, combined, 259 saturation of, 313tag fields used in, 248-250 waiting step , 257 resource recovery pointer (RRP), 436 response time, 106,108. See also latency RespQ command, 134 restricted data flow. 380 retirement st age. See instruction retirement stage return address stack (RAS), 500-501 return rule, 457return stack buffer (RSB), 501 reuse test, 554 Richardson, S. E., 262 Riordan, Tom, 417,421RISC architecture, 9 IBM study on, 91-93 instruction decoding in, 195-196 MIPS R2O0O/3OO0 example, 87-89 modem architecture, 62-65 predecoding, 198-199 RISC86 operation group, 411^112 RISC operations (ROPs), 196 superscalar retrofits, 384-385 Riseman, E., 25, 377 Robelen, Russ, 373 Rodman, Paul 418 Rosenblatt, F., 487 rotational latency, 156 Rotenberg, Eric, 236,506,610 Roth, A, 613 row address strobe (RAS), 130 row buffer cache, 131 Rowen, Chris, 419 row hits, 135 Rubin, S., 537, 541 Ruemmler, C, 157 Russell, K., 500Russell, R. M., 29 Ruttenberg, John, 418,440 Ryan, B., 409,410,429 safe instruction recognition, 407 Sandon, Peter, 431 saturating k-bit counter, 461-462 Sazeides, Y., 261, 527, 539 scalar com putation, 18 scalar pipelined processors, 16 limitations, 178-180 performance, 91-93, 179-180* pipeline rigidity, 179-180 scalar instruction pipeline, defined, 73 single-entry buffer, 186-187 unifying instruction types, 179 upper bound throughput, 178-179 scheduling latency, 548-549 scheduling matrices, 374 Schorr, Herb, 373, 374, 376INDEX SECDED codes, 159secondary (L2) cache, 111,112, 274 seek latency, 156 selective branch inversion (SBI), 480-482,502 selective eager execution, 503 selective reissue, 546-551 select logic, 258 Sell, John, 424 Seng, J., 541sense amp, 130 sequential bottleneck, 19, 22-23,220 sequential consistency model, 577-578 Sequent NUMA-Q syst em, 574 serialization constraints, 311, 316serializing instructions, 597-598 serial program parallelization, 561-562 service time, 108 set-associative mory. 119,120, 146-148 set-associative TLB, 150 set busy bit, 241Seznec, Andre, 392,477 shared-dirty state, 570 shared (s) PHT, 233.234-235 sharing list (vector), 573 Shebanow, Mike, 196,380,409, 435,437 Shen, John Paul, 15,261,384,523, 527,535 Shima, Masatoshi, 405Shippy, D., 402short decoders, 411 Shriver, B., 412Silha, E., 425Simone, M., 436simulators. See performance simulators simultaneous multithreading (SMT), 584,585,592-599 active-Zredundant-stream (A/R-SMT), 610-612 branch confidence, 504 cost of, 598-599 instruction serialization support, 597-598interstage buffer implementation, 593-594 multiple threads, managing, 598 Pentium 4 implementation, 599performance of, 598-599 pipeline stage sharing, 594-597 resource sharing, 59 3-599 Sindagi, V., 236,602,603,604,606 single-assignment code, 238 single-direction branch prediction, 455-456 single-instruction serialization, 311 single-thread performance, 589 single-writer protocols, 569—571 sink, 249 Sites, Richard, 387,389 six-stage instruction pipeline. See TYPICAL (TYP) instruction pipeline six-stage template (TEM) supers calar pipeline, 190-191 Skadron, Kevin, 482Slavenburg, G., 204 slipstreaming, 613-615 slotting stage, 389small computer system interface (SCSI), 108 Smith, Alan Jay, 120,226,342,497 Smith, Burton, 412,588 Smith, Frank, 403Smith, Jim E, 208,225,228,261, 378-379,387,401,402,425, 440,460,527,539 Smith, M., 234, 380 Smith's algorithm, 459-462 SMT. See simultaneous multithreading (SMT) snooping, 168,572-573 Snyder, Mike, 428 Sodani, A., 26 2,527,529, 534, 541,545 soft interrupts, 375 software cache coherence, 168 software ins trumentation, 13-14 software portability, 6-7 software RAID. 160 software TLB miss handler, 145 Sohi, G. S., 208,262,273,277,527, 529,541.545,562,604Soltis, F rank, 431,432 Song, Pe ter, 427,430,437,440 Sony Playstation 2,131 . source, 249source read, 240-241 SPARC Version 8.432-435 SPARC Version 9,435-439 spatial locality, 113-114 spatial parallelism, 181-182, 205-206 SPEC benchmarks, 26,227, 305-307 special-purpose register (mtspr instruction, 311 specification, 1-2,4-5 speculative exploitation, value locality, 535-554 computational predictors, 5 confidence estimation, 538 data flow region verificatio 545-546 history-based predictors, 539-540 hybrid predictors, 540implementation issues, 541 prediction accuracy, 538prediction coverage, 538-f prediction scope, 541-542 speculative execution usin; predicted va lues, 54 data flow eager execute data speculation suppoi 550-551 memory data dependenc misprediction penalty, selective reissue 547-548 prediction verification, 543-545 propagating verificatio results, 544-541 refetch-based recoveryscheduling latency efft 548-549 scheduling logic, chan 549-550 selective reissue recov 546-551 speculative verificatio640 INDEX speculative execution using predicted values— Com. straightforward value speculation, 542 value prediction. See value prediction weak dependence model, 535-536 speculative versioning cache, 609-610 speed demons, 321-322, 384, 386 spill code, 262-263 spinning lock, 564 split-transaction bus, 581-582 Sporer, M., 382 Sprangle, Eric, 96,477 SRAM, 112,130 SSE instructions, 329 stage quantization, 53,55-61 stall cycles. See pipeline stalls Standard Performance Evaluation Corp. benchmarks. See SPEC benchmarks Stark, Jared, 485 static binding load balancing, 355 static branch prediction, 346, 454-458 backwards taken/forwards not-taken, 456 Ball/Lams heuristics, 456-457 profile-based, 455,457-458 program-based, 456-457 rule-based, 455-457 single-direction prediction, 455^156 static predictor selection, 493 static random-access memory (SRAM), 112, 130 Steck, Randy, 329 Steffan, J. G., 562, 602 Stellar GS-1000, 382 Stiles, Dave, 410 Stone, Harold, 19 store address buffer (SAB), 361 store buffer (SB), 265-266, 268-272,361 store coloring, 362 store data buffer (SDB), 246-248, 361store instructions. See also memory data flow techniques processing, 265-266 senior, 354specifications, 63-65 unifying procedure, 66-67 weak ordering of, 319, 321 Storer, J., 379 store rule, 457 Storino, S., 302,575,589 streaming SIMD extension (SSE2), 416 stride predictor, 540 strong dependence model, 535-536 subcomputations ALU instructions, 63,65 branch instructions, 63-65 generic, 55 load/store instructions, 63-64 merging, 55-58 subdividing, 56-57 uniform, 48-49,53, 54 Sugumar, Rabin A, 472 Sundaramoorthy, K.,' 613 Sun Enterprise 10000, 572  Sun UltraSPARC. See UltraSPARC superpipelined machines, 29-31 superscalar machines, 16,31 brainiacs, 321-322, 384, 386-387 development of, 369-384 Astronautics ZS-1, 378-379 decoupled architectures access-execute, 378-379 microarchitectures, 380-382 IBM ACS-1, 372-377 IBM Cheetah/Panther/ America, 380 IBM Stretch, 369-372 ILP studies, 377-378 instruction fission, 380-381 instruction fusion, 381-382 multiple-decoding and, 378-379 1980s multiple-issue efforts, 382 superscalar design, 372-377 timeline, 383uniprocessor parallelism, 369-372 wide acceptance, 382-384 goal of, 24 instruction flow. See instruction flow techniques memory data flow. See memory data flow techniques pipeline organization. See superscalar pipeline organization recent design classifications, 384-387 register data flow. See register data flow techniques RISC/CISC retrofits, 384-385 dependent integer issue, 385 extensive out-of-order issue, 385 floating-point coprocessor style, 384 integer branch, 384 multiple function, precise exceptions, 385 multiple integer issue, 384 speed demons, 321-322, 384,386 verification of. 439-440 VLIW processors VS., 31-32 superscalar pipeline organization, 177-215 diversified pipelines, 184-186 dynamic pipelines, 186-190 fetch group misalignment, 191-195 instruction completion/ retirement, 206-209 exceptions, 207-208 interrupts, 207 instruction decoding, 195-199 instruction dispat ching, 199-203 instruction execution, 203-206 hardware complexity, 206 memory configurations, 205 optimal mix functional units, 205 parallelism and, 205-206 instruction fetching, 190-195 overview, 190-209INDEX parallelism, 181-184 predecoding, 198-199 reservation stations, 201-203 scalar pipeline limitations, 178-180 six-stage template, 19 0-191 SuperSPARC, 203,381-382, 385,433 Sussenguth, Ed, 373,374,375, 376,440 synchronization, 561,562-565 synchronous DRAM (SDRAM), 129,131 synthesis, 4 tag, 119 tag fields, 248-250 Talmudi, Ran, 440 Tandom Cyclone, 382 Tarlescu, Maria-Dana, 485 Taylor, S., 440 temporal locality, 113-114 temporal parallelism, 181-182, 205-206 Tendler, Joel M., 122,136, 302, 323-324,431,471,584,587 Tera MTA, 588-589 think time, 169 third level (L3) cache, 274 Thomas, Renju, 489 Thompson, T, 429 Thornton, J. E., 29,185, 588 thread-le vel parallelism (TLP), 3,560. See also multiple threads, executing thread-level speculation (TLS), 562 attributes of, 603 control flo w techniques, 602,603 memory RAW resolution, 608physical organization. 604 register data dependences, 606 thread switch state machine, 591-592 3 C's model, 123-125 throughput. See bandwidth time-sharing, 560-561 Tirumalai, P., 438 Tl SuperSPARC. See SuperSPARCTjaden, Gary, 25, 377, 519 TLB miss, 265Tobin, P., 394 Tomasulo, Robert, 201,373, 520 Tomasulo's algorithm, 246-254,535 common data bus, 246-248 IBM 360 FPU original design, 246-247 instruction sequence example, 250-254 reservation stations, 246-248 Torng, H. C, 382 Torreilas, J., 562 total sequential execution, 73 total update, 476 tournament branch predictor, 390, 491-493 Towle, Ross, 418,440 trace cache, 236-237,415,506-508 trace-driven simulation, 13-14,306 trace prediction, 508 trace scheduling, 26 training th reshold, 487 transfer latency, 156 transistor count. 2 translation lo okaside buffer (TLB), 145,149-153, 265 data cache interaction, 151-153 data (DTLB), 362, 364 fully-associative, 150—151 instruction (ITLB), ntel P6, 339-341 set-associative, 150 translation memory, 142-145, 147-153 Transputer T9000, 381-382 trap barrier instruction (TRAPB), 387 Tremblay, Marc, 437,438,440 TriMedia-1 processor, 204 TriMedia VLIW processor, 204 true dependence. See RAW data dependence Tsien, B., 440Tullsen, D. M., 541,585,592 Tumlin, T. J., 440Turumella, B., 440 two-level adaptive (correlated) branch prediction, 232-236, 462-469TYPICAL (TYP) instruction pipeline, 67—71 logical representation, 66 memory subsystem interfac MIPS R2000/R3000 vs., 89 physical organization, 68-< register file interface, 69 —71 unified instruction tyf. 65-68 U Uhilg, Richard, 473 Uht, A. K., 236,455,503, 602 604,606 UltraSPARC, 199 UltraSPARC-I, 382,437-438 UltraSPARC-Ill, 122,382,431 UltraSPARC-IV,439 Undy, S., 394 uniform memory access (UM/ 566-567 Univac A19,382 universal serial bus (USB), 15- pops. See micro-operations (U J update map table, 241 update protocols, 568-569 up-down counter, 461-462 V Vajapeyam, S., 208 value locality, 26 1,521, 523-i average, 526causes of, 523-525 nonspeculative exploitatiot nonspeculative explition, value locality quantifying, 525-527 speculative exploitation Si speculative ex ploitai value locality value prediction, 261-262,521 536-537 idealized machine model, 552-553 performance of, 551-553 value prediction table (VPT), 538-539642 INDEX value prediction unit (VPU), 537-542 Van Dyke, Korbin, 410 variable path length predictors, 485 Vasseghi, N.,421 vector computation, 18-19 vector decoder, 411 VectorPath decoder, 412,417 Vegesna, Raju, 434 large-scale integration (VLSI) processor, 455 long instruction word (VLIW) processor, 26, 31-32 virtual address, 136-137 virtual function calls, 524 virtually indexed data cache, 152-153 virtual memory, 127,136-145 accessing backing store, 140-141 address translation,. 1-36-137, 147-153, 263-264 Intel P6, 340-341 cache coherence and, 576 demand paging, 137, 138-141 evicting pages, 140 lazy allocation, 139 memory protection, 141-142 page allocation, 140 page faults, 138, 140, 141, 265 page table architectures, 142-145,147-153 permission bits, 142-143 translation memory, 142-145, 147-153 virtual address aliasing, 142 visual instruction set (VIS), 437 VMW-generated performance simulators, 301, 305-307W wakeup-and-select process, 595 wake-up logic, 258 Waldecker, Don, 429 Wall, D. W., 27, 380Wang, K., 262,527, 540 Wang, W., 153,576 WAN (wide area network), 107 WAR data dependence, 71-72 enforcing, 238-239 IMT designs, 605, 607 load/store instructions, 266-267 memory controller, 135pipeline hazard caused by, 73-74 resolved Tomasulo's algorithm, 252-254 TYP pipeline, 76 Waser, Shlomo, 45-48 Watson. Tom, 372 WAW data dependence, 72 enforcing, 238-239 IMT designs, 605,607 load/store instructions, 266-267 memory controller, 135 ' pipeline hazard caused by, 73-74 resolved Tomasulo's algorithm, 253-254 TYP pipeline, 75-76 Wayner, P., 438 way prediction, 510 weak dependence model, 535-536 Weaver. Dave, 435 Weber, Fred, 412,417 Weiser, Uri, 405,440 Weiss, S., 387,401,402,425,484 White, S., 302, 322,402,430 Wilcke, Winfried, 435,436Wilkerson, C. B 261 Wilkes, J., 157 Wilkes, M., 115Wflliams, T, 436Wilson, J„ 380 WinChip microarchitecture, 410 Witek, Rich, 387,389 Wolfe, A, 384word line, 129 Worley, Bill 392,440 Wottreng, Andy, 431 write-after-read. See WAR data dependence write-after-write. See WAW data dependence writeback cache, 122 write back (WB) stage, 28, 305write permission (Wp) bit, 142 WriteQ command, 134-135 write-through cache, 121-122 wrong-history mispredictions, 482 Wulf. W. A., 129 YAGS predictor, 478-180 Yates, John, 440 Yeager, K., 324,419,421,579 Yeh, T. Y, 232-233,341,458,462, 468-169 Yeh's algorithm, 341-343 Yew, P., 541 Young, C, 234Yung, Robert, 435,437 Z Zilles,C, 613 Zom, B., 527,539,553(continuedfrom front inside cover) Mikko Lipasti 1| Mikko Lipasti assistant professor Uni- versify Wisco nsin-Madison since 1999, ac- tively pursuing various research topics realms processor, system, memory architecture. He-has advised total 17 graduate students, including two completed Ph.D. theses numerous M.S. projects, published 30 papers top computer architecture conferences journals. well known seminal Ph.D. work value prediction. research program received excess $2 million support thr ough multiple grants National Science Foundation well financial support equipment donations IBM, Intel, AMD, Sun Microsystems. Eta Kappa Nu Elec trical Engineering Honor Society selected Mikko country's Outst anding Young Electrical Engineer 2002. also member IEEE Tau Beta Pi engineering honor society. received B.S. computer engineering Valparaiso University 1991, M.S. (1992) Ph.D. (1997) degrees electrical computer engineering Carnegie Mellon University. Prior beginning academic career, worked IBM Corporation software future proc essor system performance analysis design guidance, well operating system kernel implementation. IBM con- tributed system microarchitectural definition future IBM server computer systems. served numerous conference workshop program commit- tees co-organizer annual W orkshop Duplicating. Deconstructing, Debunking (WDDD). filed seven patent applications, six issued U.S. patents; Best Paper Award MICRO-29; received IBM Invention Achiev ement, Patent Issuance, Technical Recognition Awards. Mikko happily married since 1991 nine-year-old daughter six-year old son. spare time, enjoys regular exe rcise, family bike rides, reading, volunteering time local church campus English-language di scussion group leader International Friendship Center.ranch redi istructicj MODERN PROCESSOR DESIGN exciting new first edition John Shen Intel Corporation (formerly Car negie Mellon University) Mikko Lipasti University Wisconsin-Madison. book brings together numerous microarchitectural techniques harvesting instruction-level parallelism (I LP) achieve better processor performance proposed implemented real machines. advanced techniques recen research e fforts extend beyond ILP exploit th read-level parallelism (TLP) ^^K. Electrical & also compiled book. techniques, well foundational principles behind "™ | C omputer them, organized presented within clear framework allows ease comprehension. E ngineering KEY FEATURES OFTHIS BOOK INCLUDE: * first several chapters cover key fundamental topics lay foundation th e moder n topics. fundamentals include: art processor design, instruction set architecture specification processor, microarchitecture implementation processor; pipelining; superscalar organization. * New first edition! Chapter 3 Memory I/O Systems. chapter examine larger context computer systems incorporates advanced, high-performance processors. * Chapter 5 superscalar techniques heart book-this chapter presents issues related superscalar processor organization first, followed pr esentation specific techniques e nhancing instruction flow, register data flow memory data flow. * New first edition! Chapter 9, Advanced Instruction Flow Tec hniques. chapter focuses problem predicting whether conditional branch taken taken. brief discussion branch target prediction issues related effective instruction delivery. * Two case study chapters included give reader real-life examples concepts studied previous chapters. One case study chapters written lead architects Intel PB microarchitecture. hist oric microarchitecture provided foundation numerous hi ghly successfu l microprocessor designs. * Homework problems included end chapter provide reinforcement concepts presented. WEBSITE book's website, www.mhhe.com/siien includes downloadable version solutions manual, password-protected instructors. also contains PowerPoint slides, sample homework assignments olutions sample exams answers. McGraw-Hill EngineeringCS.com www.McGraw-HillEngineeringCS.com-Voir one-stop online shop McGr aw-Hill Engineering & Computer Science books, supplementa l materials, content, resources! tudent, professor professional, site houses Engineering Com puter Science needs. McGraw-Hill Companies mTata McGraw-Hill Publishing Company Limited 7 West Patel Nagar, New Delhi 110 008 Visit website at: www.tatamcgrawhill.com