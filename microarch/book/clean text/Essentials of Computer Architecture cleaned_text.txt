ESSENTIALS COMPUTER ARCHITECTURE Second EditionESSENTIALS COMPUTER ARCHITECTURE Second Edition DOUGLAS COMER SPARC registered trademark SPARC International, Inc. United States countries. ARM registered trademark ARM Limited United States countries. MIPS registered trademark MIPS Technologies, Inc. United States countries. Itanium Xeon trademarks of, Intel Pentium registered trademarks Intel Corporation. trademarks referred herein property respective owners. CRC Press Taylor & Francis Group 6000 Broken Sound Parkway NW, Suite 300 Boca Raton, FL 33487-2742 © 2017 Taylor & Francis Group, LLC CRC Press imprint Taylor & Francis Group, Informa business claim original U.S. Government works Printed acid-free paper Version Date: 20161102 International Standard Book Number-13: 978-1-138-62659-1 (Hardback) book contains information obtained authentic highly regarded sources. Reasonable efforts made publish reliable data information, author publisher cannot assume responsibility validity materials consequences use. authors publishers attempted trace copyright holders material reproduced publication apologize copyright holders permission publish form obtained. copyright material acknowledged please write let us know may rectify future reprint. Except permitted U.S. Copyright Law, part book may reprinted, reproduced, transmitted, utilized form electronic, mechanical, means, known hereafter invented, including photocopying, microfilming, recording, information storage retrieval system, without written permission publishers.For permission photocopy use material electronically work, please access www.copyright.com (http://www.copyright.com/) contact Copyright Clearance Center, Inc. (CCC), 222 Rosewood Drive, Danvers, 01923, 978-750-8400. CCC not-for-profit organization provides licenses registration variety users. organizations granted photocopy license CCC, separate system payment arranged. Trademark Notice: Product corporate names may trademarks registered trademarks, used identification explanation without intent infringe. Library Congress Cataloging-in-Publication Data Names: Comer, Douglas, author. Title: Essentials computer architecture / Douglas Comer. Description: Second edition. | Boca Raton : Taylor & Francis, CRC title, part Taylor & Francis imprint, member Taylor & Francis Group, academic division T&F Informa, plc, [2017] | Includes index. Identifiers: LCCN 2016041657 | ISBN 9781138626591 (hardback : alk. paper) Subjects: LCSH: Computer architecture. Classification: LCC QA76.9.A73 C625 2017 | DDC 004.2/2--dc23 LC record available https://lccn.loc.gov/2016041657 Visit Taylor & Francis Web site http://www.taylorandfrancis.com CRC Press Web site http://www.crcpress.comTo Chris, makes bits life meaningfulContents Preface Chapter 1 Introduction Overview 1.1 Importance Architecture 1.2 Learning Essentials 1.3 Organization Text 1.4 Omit 1.5 Terminology: Architecture Design 1.6 Summary PART Basics Chapter 2 Fundamentals Digital Logic 2.1 Introduction 2.2 Digital Computing Mechanisms 2.3 Electrical Terminology: Voltage Current 2.4 Transistor 2.5 Logic Gates 2.6 Implementation Nand Logic Gate Using Transistors 2.7 Symbols Used Logic Gates 2.8 Example Interconnection Gates 2.9 Digital Circuit Binary Addition 2.10 Multiple Gates Per Integrated Circuit 2.11 Need Combinatorial Circuits 2.12 Circuits Maintain State 2.13 Propagation Delay 2.14 Using Latches Create Memory2.15 Flip-Flops Transition Diagrams 2.16 Binary Counters 2.17 Clocks Sequences 2.18 Important Concept Feedback 2.19 Starting Sequence 2.20 Iteration Software Vs. Replication Hardware 2.21 Gate Chip Minimization 2.22 Using Spare Gates 2.23 Power Distribution Heat Dissipation 2.24 Timing Clock Zones 2.25 Clockless Logic 2.26 Circuit Size Moore’s Law 2.27 Circuit Boards Layers 2.28 Levels Abstraction 2.29 Summary Chapter 3 Data Program Representation 3.1 Introduction 3.2 Digital Logic Importance Abstraction 3.3 Definitions Bit Byte 3.4 Byte Size Possible Values 3.5 Binary Weighted Positional Representation 3.6 Bit Ordering 3.7 Hexadecimal Notation 3.8 Notation Hexadecimal Binary Constants 3.9 Character Sets 3.10 Unicode 3.11 Unsigned Integers, Overflow, Underflow 3.12 Numbering Bits Bytes 3.13 Signed Binary Integers 3.14 Example Two’s Complement Numbers 3.15 Sign Extension 3.16 Floating Point 3.17 Range IEEE Floating Point Values 3.18 Special Values 3.19 Binary Coded Decimal Representation 3.20 Signed, Fractional, Packed BCD Representations 3.21 Data Aggregates 3.22 Program Representation3.23 Summary PART II Processors Chapter 4 Variety Processors Computational Engines 4.1 Introduction 4.2 Two Basic Architectural Approaches 4.3 Harvard Von Neumann Architectures 4.4 Definition Processor 4.5 Range Processors 4.6 Hierarchical Structure Computational Engines 4.7 Structure Conventional Processor 4.8 Processor Categories Roles 4.9 Processor Technologies 4.10 Stored Programs 4.11 Fetch-Execute Cycle 4.12 Program Translation 4.13 Clock Rate Instruction Rate 4.14 Control: Getting Started Stopping 4.15 Starting Fetch-Execute Cycle 4.16 Summary Chapter 5 Processor Types Instruction Sets 5.1 Introduction 5.2 Mathematical Power, Convenience, Cost 5.3 Instruction Set Architecture 5.4 Opcodes, Operands, Results 5.5 Typical Instruction Format 5.6 Variable-Length Vs. Fixed-Length Instructions 5.7 General-Purpose Registers 5.8 Floating Point Registers Register Identification 5.9 Programming Registers 5.10 Register Banks 5.11 Complex Reduced Instruction Sets 5.12 RISC Design Execution Pipeline 5.13 Pipelines Instruction Stalls 5.14 Causes Pipeline Stalls5.15 Consequences Programmers 5.16 Programming, Stalls, No-Op Instructions 5.17 Forwarding 5.18 Types Operations 5.19 Program Counter, Fetch-Execute, Branching 5.20 Subroutine Calls, Arguments, Register Windows 5.21 Example Instruction Set 5.22 Minimalistic Instruction Set 5.23 Principle Orthogonality 5.24 Condition Codes Conditional Branching 5.25 Summary Chapter 6 Data Paths Instruction Execution 6.1 Introduction 6.2 Data Paths 6.3 Example Instruction Set 6.4 Instructions Memory 6.5 Moving Next Instruction 6.6 Fetching Instruction 6.7 Decoding Instruction 6.8 Connections Register Unit 6.9 Control Coordination 6.10 Arithmetic Operations Multiplexing 6.11 Operations Involving Data Memory 6.12 Example Execution Sequences 6.13 Summary Chapter 7 Operand Addressing Instruction Representation 7.1 Introduction 7.2 Zero, One, Two, Three Address Designs 7.3 Zero Operands Per Instruction 7.4 One Operand Per Instruction 7.5 Two Operands Per Instruction 7.6 Three Operands Per Instruction 7.7 Operand Sources Immediate Values 7.8 Von Neumann Bottleneck 7.9 Explicit Implicit Operand Encoding7.10 Operands Combine Multiple Values 7.11 Tradeoffs Choice Operands 7.12 Values Memory Indirect Reference 7.13 Illustration Operand Addressing Modes 7.14 Summary Chapter 8 CPUs: Microcode, Protection, Processor Modes 8.1 Introduction 8.2 Central Processor 8.3 CPU Complexity 8.4 Modes Execution 8.5 Backward Compatibility 8.6 Changing Modes 8.7 Privilege Protection 8.8 Multiple Levels Protection 8.9 Microcoded Instructions 8.10 Microcode Variations 8.11 Advantage Microcode 8.12 FPGAs Changes Instruction Set 8.13 Vertical Microcode 8.14 Horizontal Microcode 8.15 Example Horizontal Microcode 8.16 Horizontal Microcode Example 8.17 Operations Require Multiple Cycles 8.18 Horizontal Microcode Parallel Execution 8.19 Look-Ahead High Performance Execution 8.20 Parallelism Execution Order 8.21 Out-Of-Order Instruction Execution 8.22 Conditional Branches Branch Prediction 8.23 Consequences Programmers 8.24 Summary Chapter 9 Assembly Languages Programming Paradigm 9.1 Introduction 9.2 Characteristics High-level Programming Language 9.3 Characteristics Low-level Programming Language 9.4 Assembly Language9.5 Assembly Language Syntax Opcodes 9.6 Operand Order 9.7 Register Names 9.8 Operand Types 9.9 Assembly Language Programming Paradigm Idioms 9.10 Coding Statement Assembly 9.11 Coding IF-THEN-ELSE Assembly 9.12 Coding FOR-LOOP Assembly 9.13 Coding Statement Assembly 9.14 Coding Subroutine Call Assembly 9.15 Coding Subroutine Call Arguments Assembly 9.16 Consequence Programmers 9.17 Assembly Code Function Invocation 9.18 Interaction Assembly High-level Languages 9.19 Assembly Code Variables Storage 9.20 Example Assembly Language Code 9.21 Two-Pass Assembler 9.22 Assembly Language Macros 9.23 Summary PART III Memories Chapter 10 Memory Storage 10.1 Introduction 10.2 Definition 10.3 Key Aspects Memory 10.4 Characteristics Memory Technologies 10.5 Important Concept Memory Hierarchy 10.6 Instruction Data Store 10.7 Fetch-Store Paradigm 10.8 Summary Chapter 11 Physical Memory Physical Addressing 11.1 Introduction 11.2 Characteristics Computer Memory 11.3 Static Dynamic RAM Technologies 11.4 Two Primary Measures Memory Technology11.5 Density 11.6 Separation Read Write Performance 11.7 Latency Memory Controllers 11.8 Synchronous Multiple Data Rate Technologies 11.9 Memory Organization 11.10 Memory Access Memory Bus 11.11 Words, Physical Addresses, Memory Transfers 11.12 Physical Memory Operations 11.13 Memory Word Size Data Types 11.14 Byte Addressing Mapping Bytes Words 11.15 Using Powers Two 11.16 Byte Alignment Programming 11.17 Memory Size Address Space 11.18 Programming Word Addressing 11.19 Memory Size Powers Two 11.20 Pointers Data Structures 11.21 Memory Dump 11.22 Indirection Indirect Operands 11.23 Multiple Memories Separate Controllers 11.24 Memory Banks 11.25 Interleaving 11.26 Content Addressable Memory 11.27 Ternary CAM 11.28 Summary Chapter 12 Caches Caching 12.1 Introduction 12.2 Information Propagation Storage Hierarchy 12.3 Definition Caching 12.4 Characteristics Cache 12.5 Cache Terminology 12.6 Best Worst Case Cache Performance 12.7 Cache Performance Typical Sequence 12.8 Cache Replacement Policy 12.9 LRU Replacement 12.10 Multilevel Cache Hierarchy 12.11 Preloading Caches 12.12 Caches Used Memory 12.13 Physical Memory Cache12.14 Write Write Back 12.15 Cache Coherence 12.16 L1, L2, L3 Caches 12.17 Sizes L1, L2, L3 Caches 12.18 Instruction Data Caches 12.19 Modified Harvard Architecture 12.20 Implementation Memory Caching 12.21 Direct Mapped Memory Cache 12.22 Using Powers Two Efficiency 12.23 Hardware Implementation Direct Mapped Cache 12.24 Set Associative Memory Cache 12.25 Consequences Programmers 12.26 Summary Chapter 13 Virtual Memory Technologies Virtual Addressing 13.1 Introduction 13.2 Definition Virtual Memory 13.3 Memory Management Unit Address Space 13.4 Interface Multiple Physical Memory Systems 13.5 Address Translation Address Mapping 13.6 Avoiding Arithmetic Calculation 13.7 Discontiguous Address Spaces 13.8 Motivations Virtual Memory 13.9 Multiple Virtual Spaces Multiprogramming 13.10 Creating Virtual Spaces Dynamically 13.11 Base-Bound Registers 13.12 Changing Virtual Space 13.13 Virtual Memory Protection 13.14 Segmentation 13.15 Demand Paging 13.16 Hardware Software Demand Paging 13.17 Page Replacement 13.18 Paging Terminology Data Structures 13.19 Address Translation Paging System 13.20 Using Powers Two 13.21 Presence, Use, Modified Bits 13.22 Page Table Storage 13.23 Paging Efficiency Translation Lookaside Buffer 13.24 Consequences Programmers13.25 Relationship Virtual Memory Caching 13.26 Virtual Memory Caching Cache Flush 13.27 Summary PART IV Input Output Chapter 14 Input/Output Concepts Terminology 14.1 Introduction 14.2 Input Output Devices 14.3 Control External Device 14.4 Data Transfer 14.5 Serial Parallel Data Transfers 14.6 Self-Clocking Data 14.7 Full-Duplex Half-Duplex Interaction 14.8 Interface Throughput Latency 14.9 Fundamental Idea Multiplexing 14.10 Multiple Devices Per External Interface 14.11 Processor’s View I/O 14.12 Summary Chapter 15 Buses Bus Architectures 15.1 Introduction 15.2 Definition Bus 15.3 Processors, I/O Devices, Buses 15.4 Physical Connections 15.5 Bus Interface 15.6 Control, Address, Data Lines 15.7 Fetch-Store Paradigm 15.8 Fetch-Store Bus Size 15.9 Multiplexing 15.10 Bus Width Size Data Items 15.11 Bus Address Space 15.12 Potential Errors 15.13 Address Configuration Sockets 15.14 Question Multiple Buses 15.15 Using Fetch-Store Devices 15.16 Operation Interface15.17 Asymmetric Assignments Bus Errors 15.18 Unified Memory Device Addressing 15.19 Holes Bus Address Space 15.20 Address Map 15.21 Program Interface Bus 15.22 Bridging Two Buses 15.23 Main Auxiliary Buses 15.24 Consequences Programmers 15.25 Switching Fabrics Alternative Buses 15.26 Summary Chapter 16 Programmed Interrupt-Driven I/O 16.1 Introduction 16.2 I/O Paradigms 16.3 Programmed I/O 16.4 Synchronization 16.5 Polling 16.6 Code Polling 16.7 Control Status Registers 16.8 Using Structure Define CSRs 16.9 Processor Use Polling 16.10 Interrupt-Driven I/O 16.11 Interrupt Mechanism Fetch-Execute 16.12 Handling Interrupt 16.13 Interrupt Vectors 16.14 Interrupt Initialization Disabled Interrupts 16.15 Interrupting Interrupt Handler 16.16 Configuration Interrupts 16.17 Dynamic Bus Connections Pluggable Devices 16.18 Interrupts, Performance, Smart Devices 16.19 Direct Memory Access (DMA) 16.20 Extending DMA Buffer Chaining 16.21 Scatter Read Gather Write Operations 16.22 Operation Chaining 16.23 Summary Chapter 17 Programmer’s View Devices, I/O, Buffering17.1 Introduction 17.2 Definition Device Driver 17.3 Device Independence, Encapsulation, Hiding 17.4 Conceptual Parts Device Driver 17.5 Two Categories Devices 17.6 Example Flow Device Driver 17.7 Queued Output Operations 17.8 Forcing Device Interrupt 17.9 Queued Input Operations 17.10 Asynchronous Device Drivers Mutual Exclusion 17.11 I/O Viewed Application 17.12 Library/Operating System Dichotomy 17.13 I/O Operations OS Supports 17.14 Cost I/O Operations 17.15 Reducing System Call Overhead 17.16 Key Concept Buffering 17.17 Implementation Buffered Output 17.18 Flushing Buffer 17.19 Buffering Input 17.20 Effectiveness Buffering 17.21 Relationship Caching 17.22 Example: C Standard I/O Library 17.23 Summary PART V Advanced Topics Chapter 18 Parallelism 18.1 Introduction 18.2 Parallel Pipelined Architectures 18.3 Characterizations Parallelism 18.4 Microscopic Vs. Macroscopic 18.5 Examples Microscopic Parallelism 18.6 Examples Macroscopic Parallelism 18.7 Symmetric Vs. Asymmetric 18.8 Fine-grain Vs. Coarse-grain Parallelism 18.9 Explicit Vs. Implicit Parallelism 18.10 Types Parallel Architectures (Flynn Classification) 18.11 Single Instruction Single Data (SISD)18.12 Single Instruction Multiple Data (SIMD) 18.13 Multiple Instructions Multiple Data (MIMD) 18.14 Communication, Coordination, Contention 18.15 Performance Multiprocessors 18.16 Consequences Programmers 18.17 Redundant Parallel Architectures 18.18 Distributed Cluster Computers 18.19 Modern Supercomputer 18.20 Summary Chapter 19 Data Pipelining 19.1 Introduction 19.2 Concept Pipelining 19.3 Software Pipelining 19.4 Software Pipeline Performance Overhead 19.5 Hardware Pipelining 19.6 Hardware Pipelining Increases Performance 19.7 Pipelining Used 19.8 Conceptual Division Processing 19.9 Pipeline Architectures 19.10 Pipeline Setup, Stall, Flush Times 19.11 Definition Superpipeline Architecture 19.12 Summary Chapter 20 Power Energy 20.1 Introduction 20.2 Definition Power 20.3 Definition Energy 20.4 Power Consumption Digital Circuit 20.5 Switching Power Consumed CMOS Digital Circuit 20.6 Cooling, Power Density, Power Wall 20.7 Energy Use 20.8 Power Management 20.9 Software Control Energy Use 20.10 Choosing Sleep Awaken 20.11 Sleep Modes Network Devices 20.12 SummaryChapter 21 Assessing Performance 21.1 Introduction 21.2 Measuring Computational Power Performance 21.3 Measures Computational Power 21.4 Application Specific Instruction Counts 21.5 Instruction Mix 21.6 Standardized Benchmarks 21.7 I/O Memory Bottlenecks 21.8 Moving Boundary Hardware Software 21.9 Choosing Items Optimize, Amdahl’s Law 21.10 Amdahl’s Law Parallel Systems 21.11 Summary Chapter 22 Architecture Examples Hierarchy 22.1 Introduction 22.2 Architectural Levels 22.3 System-level Architecture: Personal Computer 22.4 Bus Interconnection Bridging 22.5 Controller Chips Physical Architecture 22.6 Virtual Buses 22.7 Connection Speeds 22.8 Bridging Functionality Virtual Buses 22.9 Board-level Architecture 22.10 Chip-level Architecture 22.11 Structure Functional Units Chip 22.12 Summary Chapter 23 Hardware Modularity 23.1 Introduction 23.2 Motivations Modularity 23.3 Software Modularity 23.4 Parameterized Invocation Subprograms 23.5 Hardware Scaling Parallelism 23.6 Basic Block Replication 23.7 Example Design (Rebooter) 23.8 High-level Rebooter Design23.9 Building Block Accommodate Range Sizes 23.10 Parallel Interconnection 23.11 Example Interconnection 23.12 Module Selection 23.13 Summary Appendix 1 Lab Exercises Computer Architecture Course A1.1 Introduction A1.2 Hardware Required Digital Logic Experiments A1.3 Solderless Breadboard A1.4 Using Solderless Breadboard A1.5 Power Ground Connections A1.6 Building Testing Circuits A1.7 Lab Exercises 1 Introduction account configuration 2 Digital Logic: Use breadboard 3 Digital Logic: Building adder gates 4 Digital Logic: Clocks decoding 5 Representation: Testing big endian vs. little endian 6 Representation: hex dump function C 7 Processors: Learn RISC assembly language 8 Processors: Function called C 9 Memory: Row-major column-major array storage 10 Input / Output: buffered I/O library 11 hex dump program assembly language Appendix 2 Rules Boolean Algebra Simplification A2.1 Introduction A2.2 Notation Used A2.3 Rules Boolean Algebra Appendix 3 Quick Introduction x86 Assembly Language A3.1 Introduction A3.2 x86 General-Purpose Registers A3.3 Allowable OperandsA3.4 Intel AT&T Forms x86 Assembly Language A3.5 Arithmetic Instructions A3.6 Logical Operations A3.7 Basic Data Types A3.8 Data Blocks, Arrays, Strings A3.9 Memory References A3.10 Data Size Inference Explicit Size Directives A3.11 Computing Address A3.12 Stack Operations Push Pop A3.13 Flow Control Unconditional Branch A3.14 Conditional Branch Condition Codes A3.15 Subprogram Call Return A3.16 C Calling Conventions Argument Passing A3.17 Function Calls Return Value A3.18 Extensions Sixty-four Bits (x64) A3.19 Summary Appendix 4 ARM Register Definitions Calling Sequence A4.1 Introduction A4.2 Registers ARM Processor A4.3 ARM Calling Conventions IndexPreface Hardware engineering shifted use discrete electronic components use programmable devices. Consequently, programming become much important. Programmers understand hardware operates basic hardware principles construct software systems efficient less prone errors. Consequently, basic knowledge computer architecture allows programmers appreciate software maps onto hardware make better software design choices. knowledge underlying hardware also valuable aid debugging helps programmers pinpoint source problems quickly. text suitable one-semester undergraduate course. many Computer Science programs, course computer architecture computer organization place curriculum students exposed fundamental concepts explain structure computers program. Unfortunately, texts computer architecture written hardware engineers aimed students learning design hardware. text takes different approach: instead focusing hardware design engineering details, focuses programmers explaining essential aspects hardware programmer needs know. Thus, topics explained programmer’s point view, text emphasizes consequences programmers. text divided five parts. Part covers basics digital logic, gates, data paths, data representation. students enjoy brief look underlying hardware (especially text labs avoid minute hardware details). Parts II, III, IV cover three primary aspects architecture: processors, memories, I/O systems. case, chapters give students enough background understand mechanisms operate consequences programmers without going many details. Finally, Part V covers advanced topics parallelism, pipelining, power energy, performance. Appendix 1 describes important aspect course: hands-on lab students learn doing. Although lab problems focus programming, students spend first weeks lab wiring gates breadboard. equipment inexpensive (we spent less fifteen dollars per student permanent equipment; students purchase set chips twenty dollars). Appendix 2 provides quick introduction x86 assembly language x64 extensions. Many professors teach x86 requested included. material appendix, means professors choose focus RISC assembly language (e.g., ARM architecture) use comparison.The second edition contains two new chapters well changes updates throughout. Chapter 3 data paths shows components computer describes data flows among components instructions executed. simplified example bridges gap digital logic Chapter 2 subsequent chapters describe processors. Chapter 20 power energy covers basics without going detail. explains dual-core chip core runs half speed takes less power single core chip runs full speed. set Web site accompany book at: http://www.eca.cs.purdue.edu text lab exercises used Purdue; students extremely positive both. receive notes thanks text course. many students, lab first experience hardware, enthusiastic. thanks many individuals contributed book. Bernd Wolfinger provided extensive reviews made several important suggestions topics direction. Professors students spotted typos first edition. George Adams provided detailed comments suggestions second edition. Finally, thank wife, Chris, patient careful editing valuable suggestions improve polish text. Douglas E. ComerAbout Author Dr. Douglas E. Comer, PhD, extensive background computer systems, worked hardware software. Comer’s work software spans aspects systems, including compilers operating systems. created complete operating system, including process manager, memory manager, device drivers serial parallel interfaces. Comer also implemented network protocol software network device drivers conventional computers network processors. operating system, Xinu, TCP/IP protocol stack used commercial products. Comer’s experience hardware includes work discrete components, building circuits logic gates, experience basic silicon technology. written popular textbooks network processor architectures, Bell Laboratories, Comer studied VLSI design fabricated VLSI chip. Comer Distinguished Professor Computer Science Purdue University, develops teaches courses engages research computer organization, operating systems, networks, Internets. addition writing series internationally acclaimed technical books computer operating systems, networks, TCP/IP, computer technologies, Comer created innovative laboratories students build measure systems operating systems IP routers; Comer’s courses include hands-on lab work. leave Purdue, Comer served inaugural VP Research Cisco Systems. continues consult lecture universities, industries, conferences around world. twenty years, Comer served editor-in-chief journal Software — Practice Experience. Fellow Association Computing Machinery (ACM), Fellow Purdue Teaching Academy, recipient numerous awards, including USENIX Lifetime Achievement Award. Additional information found at: www.cs.purdue.edu/people/comer information Comer’s books found at: www.comerbooks.com1 Introduction Overview Chapter Contents 1.1 Importance Architecture 1.2 Learning Essentials 1.3 Organization Text 1.4 Omit 1.5 Terminology: Architecture Design 1.6 Summary1.1 Importance Architecture Computers everywhere. Cell phones, video games, household appliances, vehicles contain programmable processors. systems depends software, brings us important question: someone interested building software study computer architecture? answer understanding hardware makes possible write smaller, faster code less prone errors. basic knowledge architecture also helps programmers appreciate relative cost operations (e.g., time required I/O operation compared time required arithmetic operation) effects programming choices. Finally, understanding hardware works helps programmers debug — someone aware hardware clues help spot source bugs. short, programmer understands underlying hardware, better creating software. 1.2 Learning Essentials hardware engineer tell you, digital hardware used build computer systems incredibly complex. addition myriad technologies intricate sets electronic components constitute technology, engineers must master design rules dictate components constructed interconnected form systems. Furthermore, technologies continue evolve, newer, smaller, faster components appear continuously. Fortunately, text demonstrates, possible understand architectural components without knowing low-level technical details. text focuses essentials, explains computer architecture broad, conceptual terms — describes major components examines role overall system. Thus, readers need background electronics electrical engineering understand subject. 1.3 Organization Text major topics cover? text organized five parts. Basics. first section covers two topics essential rest book: digital logic data representation. see case, issue same: use electronic mechanisms represent manipulate digital information. Processors. One three key areas architecture, processing concerns computation (e.g., arithmetic) control (e.g., executing sequence steps). learn basic building blocks, see blocks used modern Central Processing Unit (CPU).Memory. second key area architecture, memory systems, focuses storage access digital information. examine physical virtual memory systems, understand one important concepts computing: caching. I/O. third key area architecture, input output, focuses interconnection computers devices microphones, keyboards, mice, displays, disks, networks. learn bus technology, see processor uses bus communicate device, understand role device driver software. Advanced Topics. final section focuses two important topics arise many forms: parallelism pipelining. see either parallel pipelined hardware used improve overall performance. 1.4 Omit Paring topic essentials means choosing items omit. case text, chosen breadth rather depth — choice required, chosen focus concepts instead details. Thus, text covers major topics architecture, omits lesser- known variants low-level engineering details. example, discussion basic nand gate operates gives simplistic description without discussing exact internal structure describing precisely gate dissipates electrical current flows it. Similarly, discussion processors memory systems avoids quantitative analysis performance engineer needs. Instead, take high-level view aimed helping reader understand overall design consequences programmers rather preparing reader build hardware. 1.5 Terminology: Architecture Design Throughout text, use term architecture refer overall organization computer system. computer architecture analogous blueprint — architecture specifies interconnection among major components overall functionality component without giving many details. digital system built implements given architecture, engineers must translate overall architecture practical design accounts details architectural specification omits. example, design must specify components grouped onto chips, chips grouped onto circuit boards, power distributed board. Eventually, design must implemented, entails choosing specific hardware system constructed. design represents one possible way realize given architecture, implementation represents one possible way realize given design. point architectural descriptions abstractions, mustremember many designs used satisfy given architecture many implementations used realize given design. 1.6 Summary text covers essentials computer architecture: digital logic, processors, memories, I/O, advanced topics. text require background electrical engineering electronics. Instead, topics explained focusing concepts, avoiding low-level details, concentrating items important programmers.Part Basics Digital Logic Data Representation Fundamentals Computers Built2 Fundamentals Digital Logic Chapter Contents 2.1 Introduction 2.2 Digital Computing Mechanisms 2.3 Electrical Terminology: Voltage Current 2.4 Transistor 2.5 Logic Gates 2.6 Implementation Nand Logic Gate Using Transistors 2.7 Symbols Used Logic Gates 2.8 Example Interconnection Gates 2.9 Digital Circuit Binary Addition 2.10 Multiple Gates Per Integrated Circuit 2.11 Need Combinatorial Circuits 2.12 Circuits Maintain State 2.13 Propagation Delay 2.14 Using Latches Create Memory 2.15 Flip-Flops Transition Diagrams 2.16 Binary Counters 2.17 Clocks Sequences 2.18 Important Concept Feedback 2.19 Starting Sequence 2.20 Iteration Software Vs. Replication Hardware2.21 Gate Chip Minimization 2.22 Using Spare Gates 2.23 Power Distribution Heat Dissipation 2.24 Timing Clock Zones 2.25 Clockless Logic 2.26 Circuit Size Moore’s Law 2.27 Circuit Boards Layers 2.28 Levels Abstraction 2.29 Summary 2.1 Introduction chapter covers basics digital logic. goal straightforward — provide background sufficient reader understand remaining chapters. Although many low- level details irrelevant, programmers need basic knowledge hardware appreciate consequences software. Thus, need delve electrical details, discuss underlying physics, learn design rules engineers follow interconnect devices. Instead, learn basics allow us understand complex digital systems work. 2.2 Digital Computing Mechanisms use term digital computer refer device performs sequence computational steps data items discrete values. alternative, called analog computer, operates values vary continuously time. Digital computation advantage precise. digital computers become inexpensive highly reliable, analog computation relegated special cases. need reliability arises computation entail billions individual steps. computer misinterprets value single set fails, correct computation possible. Therefore, computers designed failure rates much less one billion. high reliability high speed achieved? One earliest computational devices, known abacus, relied humans move beads keep track sums. early twentieth century, mechanical gears levers used produce cash registers adding machines. 1940s, early electronic computers constructed vacuum tubes. Although much faster mechanical devices, vacuum tubes (which require afilament become red hot) unreliable — filament would burn hundred hours use. invention transistor 1947 changed computing dramatically. Unlike vacuum tubes, transistors require filament, consume much power, produce much heat, burn out. Furthermore, transistors could produced much lower cost vacuum tubes. Consequently, modern digital computers built electronic circuits use transistors. 2.3 Electrical Terminology: Voltage Current Electronic circuits rely physical phenomena associated presence flow electrical charge. Physicists discovered ways detect presence electrical charge control flow; engineers developed mechanisms perform functions quickly. mechanisms form basis modern digital computers. Engineers use terms voltage current refer quantifiable properties electricity: voltage two points (measured volts) represents potential energy difference, current (measured amperes amps) represents flow electrons along path (e.g., along wire). good analogy made water: voltage corresponds water pressure, current corresponds amount water flowing pipe given time. water tank develops hole water begins flow hole, water pressure drop; current starts flowing wire, voltage drop. important thing know electrical voltage voltage measured difference two points (i.e., measurement relative). Thus, voltmeter, used measure voltage, always two probes; meter register voltage probes connected. simplify measurement, assume one two points represents zero volts, express voltage second point relative zero. Electrical engineers use term ground refer point assumed zero volts. digital circuits shown text, assume electrical power supplied two wires: one wire ground wire, assumed zero volts, second wire five volts. Fortunately, understand essentials digital logic without knowing voltage current. need understand electrical flow controlled electricity used represent digital values. 2.4 Transistor mechanism used control flow electrical current semiconductor device known transistor. lowest level, digital systems composed transistors. particular, digital circuits use form transistor known Metal Oxide Semiconductor Field Effect Transistor (MOSFET), abbreviated FET. MOSFET formed crystalline silicon foundation composing layers P-type N-type silicon, silicon oxide insulating layer (a type glass), metal wires connect transistor rest circuit.The transistors used digital circuits function on/off switch operated electronically instead mechanically. is, contrast mechanical switch opens closes based mechanical force applied, transistor opens closes based whether voltage applied. transistor three terminals (i.e., wires) provide connections rest circuit. Two terminals, source drain, channel electrical resistance controlled. resistance low, electric current flows source drain; resistance high, current flows. third terminal, known gate, controls resistance. next sections, see switching transistors used build complex components used build digital systems. MOSFET transistors come two types; used digital logic circuits. Figure 2.1 shows diagrams engineers use denote two types†. Figure 2.1 two types transistors used logic circuits. type labeled (a) turns gate voltage positive; transistor labeled (b) turns gate voltage zero (or negative). diagram, transistor labeled (a) turns whenever voltage gate positive (i.e., exceeds minimum threshold). appropriate voltage appears gate, large current flow two connections. voltage removed gate, large current stops flowing. transistor labeled (b), small circle gate, works way: large current flows source drain whenever voltage gate threshold (e.g., close zero), stops flowing gate voltage high. two forms known complementary, overall chip technology known CMOS (Complementary Metal Oxide Semiconductor). chief advantage CMOS arises circuits devised use extremely low power. 2.5 Logic Gates digital circuits built? transistor two possible states — current flowing current flowing. Therefore, circuits designed using two-valued mathematical system known Boolean algebra. programmers familiar three basic Boolean functions: and, or, not. Figure 2.2 lists possible input values result function.Figure 2.2 Boolean functions result possible set inputs. logical value zero represents false, logical value one represents true. Boolean functions provide conceptual basis digital hardware. important, possible use transistors construct efficient circuits implement Boolean functions. example, consider Boolean not. Typical logic circuits use positive voltage represent Boolean 1 zero voltage represent Boolean 0. Using zero volts represent 0 positive voltage represent 1 means circuit computes Boolean constructed two transistors. is, circuit take input one wire produce output another wire, output always opposite input — positive voltage placed input, output zero, zero voltage placed input, output positive†. Figure 2.3 illustrates circuit implements Boolean not. drawing figure known schematic diagram. line schematic corresponds wire connects one component another. solid dot indicates electrical connection, small, open circle end line indicates external connection. addition two inputs output, circuit external connections positive zero voltages. Electronic circuits implement Boolean functions differ computer program significant way: circuit operates automatically continuously. is, power supplied (the + voltage figure), transistors perform function continue perform long power remains — input changes, output changes. Thus, unlike function program produces result called, output circuit always available used time. Figure 2.3 pair complementary transistors used implement Boolean not. understand circuit works, think transistors capable forming electrical connection source drain terminals. input positive, top transistor turns bottom transistor turns on, means output connected zero volts. Conversely, input voltage zero, top transistor turns bottom transistor turns off, means ouput connected positive voltage. Thus, ouput voltage represents logical opposite input voltage. detail adds minor complication Boolean functions: way electronic circuits work, takes fewer transistors provide inverse function. Thus, digital circuits implement inverse logical logical and: (which stands or) nand (which stands and). addition, circuits use exclusive (xor) function. Figure 2.4 lists possible inputs results function†. Figure 2.4 nand, nor, xor functions logic gates provide. 2.6 Implementation Nand Logic Gate Using Transistors remainder text, details transistors interconnection unimportant. need understand transistors used create Boolean functions described above, functions used create digital circuits form computers. leaving topic transistors, examine example: circuit uses four transistors implement nand function. Figure 2.5 contains circuit diagram. described above, use term logic gate describe resulting circuit. practice, logic gate contains additional components, diodes resistors, used protect transistors electrostatic discharge excessive electrical current; affect logical operation gate, extra components omitted diagram.Figure 2.5 Example four transistors interconnected circuit implements nand logic gate. understand circuit operates, observe inputs represent logical one, bottom two transistors turned on, means output connected zero volts (logical zero). Otherwise, least one top two transistors turned on, output connected positive voltage (logical one). course, circuit must designed carefully ensure output never connected positive voltage zero volts simultaneously (or transistors destroyed). diagram Figure 2.5 uses common convention: two lines cross indicate electrical connection unless solid dot appears. idea similar way vertices edges drawn graph: two edges cross indicate vertex present unless dot (or circle) drawn. circuit diagram, two lines cross without dot correspond situation physical connection; imagine wires positioned air gap exists (i.e., wires touch). help indicate connection, lines drawn slight space around crossing point. seen example gate created transistors, need consider individual transistors again. Throughout rest chapter, discuss gates without referring internal mechanisms. Later chapters discuss larger, complex mechanisms composed gates.2.7 Symbols Used Logic Gates design circuits, engineers think interconnecting logic gates rather interconnecting transistors. gate represented symbol, engineers draw diagrams show interconnections among gates. Figure 2.6 shows symbols nand, nor, inverter, and, or, xor gates. figure follows standard terminology using term inverter gate performs Boolean operation. Figure 2.6 symbols commonly used gates. Inputs gate shown left, output gate shown right. 2.8 Example Interconnection Gates electronic parts implement gates classified Transistor-Transistor Logic (TTL) output transistors gate designed connect directly input transistors gates. fact, output connect several inputs†. example, suppose circuit needed output true disk spinning user presses power-down button. Logically, output Boolean two inputs. said, however, designs limited nand, nor, inverter gates. cases, function created directly connecting output nand gate input inverter. Figure 2.7 illustrates connection. Figure 2.7 Illustration gate interconnection. output one logic gate connect directly input another gate.As another example gate interconnection, consider circuit Figure 2.8 shows three inputs. Figure 2.8 example circuit three inputs labeled X, Y, Z. Internal interconnections also labeled allow us discuss intermediate values. function circuit figure implement? two ways answer question: determine Boolean formula circuit corresponds, enumerate value appears output eight possible combinations input values. help us understand two methods, labeled input intermediate connection circuit well output. derive Boolean formula, observe input connected directly inverter. Thus, value corresponds Boolean function Y. gate takes inputs (from inverter) Z, value B corresponds Boolean function: combination nand gate followed inverter produces Boolean two inputs, output value corresponds to: formula also expressed as: Although described use Boolean expressions way understanding circuits, Boolean expressions also important circuit design. engineer start design finding Boolean expression describes desired behavior circuit. Writing expression help designer understand problem special cases. correct expression found, engineer translate expression equivalent hardware gates. use Boolean expressions specify circuits significant advantage: variety oftools available operate Boolean expressions. Tools used analyze expression, minimize expression†, convert expression diagram interconnected gates. Automated minimization especially useful reduce number gates required. is, tools exist take Boolean expression input, produce output equivalent expression requires fewer gates, convert output circuit diagram. summarize: Tools exist take Boolean expression input produce optimized circuit expression output. second technique used understand logic circuit consists enumerating possible inputs, finding corresponding values point circuit. example, circuit Figure 2.8 three inputs, eight possible combinations input exist. use term truth table describe enumeration. Truth tables often used debugging circuits. Figure 2.9 contains truth table circuit Figure 2.8. table lists possible combination inputs wires X, Y, Z along resulting values wires labeled A, B, C, output. Figure 2.9 truth table circuit Figure 2.8. table Figure 2.9 generated starting possible inputs, filling remaining columns one time. example, three inputs (X, Y, Z) set zero one. Consequently, eight possible combinations values columns X, Y, Z table. filled in, input columns used derive columns. example, point circuit represents output first inverter, inverse input Y. Thus, column filled reversing values column Y. Similarly, column B represents columns Z. truth table used validate Boolean expression — expression computed possible inputs compared values truth table. example, truth table Figure 2.9 used validate Boolean expression (2.1) equivalent expression:To perform validation, one computes value Boolean expression possible combinations X, Y, Z. combination, value expression compared value output column truth table. 2.9 Digital Circuit Binary Addition logic circuits implement integer arithmetic? example, consider using gates add two binary numbers. One apply technique learned elementary school: align two numbers column. Then, start least-significant digits add column digits. sum overflows given column, carry high-order digit sum next column. difference computers represent integers binary rather decimal. example, Figure 2.10 illustrates addition 20 29 carried binary. Figure 2.10 Example binary addition using carry bits. circuit perform addition needs one module column (i.e., bit operands). module low-order bits takes two inputs produces two outputs: sum bit carry bit. circuit, known half adder, contains gate exclusive gate. Figure 2.11 shows gates connected. Figure 2.11 half adder circuit computes sum carry two input bits. Although half adder circuit computes low-order bit sum, complex circuit needed bits. particular, successive computation three inputs: two input bits plus carry bit column right. Figure 2.12 illustrates necessarycircuit, known full adder. Note symmetry two input bits — either input connected sum circuit previous bit. Figure 2.12 full adder circuit accepts carry input well two input bits. figure shows, full adder consists two half adder circuits plus one extra gate (a logical or). connects carry outputs two half adders, provides carry output either two half adders reports carry. Although full adder eight possible input combinations, need consider six verifying correctness. see why, observe full adder treats bit 1 bit 2 symmetrically. Thus, need consider three cases: input bits zeros, input bits ones, one input bits one zero. presence carry input doubles number possibilities six. exercise suggests using truth table verify full adder circuit indeed give correct output input combination. 2.10 Multiple Gates Per Integrated Circuit logic gates described require many transistors, multiple gates use TTL manufactured single, inexpensive electronic component. One popular set TTL components implement logic gates known 7400 family†; component family assigned part number begins 74. Physically, many parts 7400 family consist rectangular package approximately one-half inch long fourteen copper wires (called pins) used connect part circuit; result known 14-pin Dual In-line Package (14-pin DIP). complex 7400-series chips require additional pins (e.g., use 16-pin DIP configuration). understand multiple gates arranged 7400-series chip, consider three examples. Part number 7400 contains four nand gates, part number 7402 contains four gates, part number 7404 contains six inverters. Figure 2.13 illustrates inputs outputs individual logic gates connect pins case.Figure 2.13 Illustration pin connections three commercially available integrated circuits implement logic gates. Although figure show gates connected pins 14 7, two pins essential supply power needed run gates — labels indicate, pin 14 connects plus five volts pin 7 connects ground (zero volts). 2.11 Need Combinatorial Circuits interconnection Boolean logic gates, circuits described above, known combinatorial circuit output simply Boolean combination input values. combinatorial circuit, output changes input value changes. Although combinatorial circuits essential, sufficient — computer requires circuits take action without waiting inputs change. example, user presses button power computer, hardware must perform sequence operations, sequence must proceed without input user. fact, user need hold power button continuously — startup sequence continues even user releases button. Furthermore, pressing button causes hardware initiate shutdown sequence. power button act power well power system? digital logic perform sequence operations without requiring input values change? digital circuit continue operate input reverts initial condition? answers involve additional mechanisms. Sophisticated arrangements logic gates provide needed functionality. rest requires hardware device known clock. next sections present examples sophisticated circuits, later sections explain clocks. 2.12 Circuits Maintain State addition electronic parts contain basic Boolean gates, parts also available contain gates interconnected maintain state. is, electronic circuits exist outputs function sequence previous inputs well current input. logic circuits known sequential circuits.A latch one basic sequential circuits. idea latch straightforward: latch input output. addition, latch extra input called enable line. long enable line set logical one, latch makes output exact copy input. is, enable line one, input changes, output changes well. enable line changes logical zero, however, output freezes current value change. Thus, latch “remembers” value input enable line set, keeps output set value. latch devised? Interestingly, combination Boolean logic gates sufficient. Figure 2.14 illustrates circuit uses four nand gates create latch. idea enable line logical zero, two nand gates right remember current value output. outputs two nand gates feed back other’s input, output value remain stable. enable line logical one, two gates left pass data input (on lower wire) inverse (on higher wire) pair gates right. Figure 2.14 Illustration four nand gates used implement one-bit latch. 2.13 Propagation Delay understand operation latch, one must know gate propagation delay. is, delay occurs time input changes output changes. propagation delay, output remains previous value. course, transistors designed minimize delay, delay less microsecond, finite delay exists. see propagation delay affects circuits, consider circuit Figure 2.15. Figure 2.15 inverter output connected back input. figure shows, output inverter connected back input. seem connection makes sense inverter’s output always opposite itsinput. Boolean expression circuit is: mathematical contradiction. Propagation delay explains circuit works. time, output 0, input inverter 0. propagation delay, inverter change output 1. output becomes 1, another propagation delay occurs, output become 0 again. cycle goes forever, say circuit oscillates generating output changes back forth 0 1 (known square wave). concept propagation delay explains operation latch Figure 2.14 — outputs remain propagation delay occurs. 2.14 Using Latches Create Memory see processors include set registers serve short-term storage units. Typically, registers hold values used computation (e.g., two values added together). register holds multiple bits; computers 32-bit 64-bit registers. circuit register illustrates important principle digital hardware design: circuit handle multiple bits constructed physically replicating circuit handles one bit. Figure 2.16 4-bit register formed four 1-bit latches.To understand principle, consider Figure 2.16 shows 4-bit register circuit constructed four 1-bit latches†. figure, enable lines four latches connected together form enable input register. Although hardware consists four independent circuits, connecting enable lines means four latches act unison. enable line set logical one, register accepts four input bits sets four outputs accordingly. enable line becomes zero, outputs remain fixed. is, register stored whatever value present inputs, output value change enable line becomes one again. point is: register, one key components processor, hardware mechanism uses set latches store digital value. 2.15 Flip-Flops Transition Diagrams flip-flop another circuit output depends previous inputs well current input. various forms. One form acts exactly like power switch computer: first time input becomes 1, flip-flop turns output on, second time input becomes 1, flip-flop turns output off. Like push-button switch used control power, flip-flop respond continuous input — input must return 0 value 1 cause flip-flop change state. is, whenever input transitions 0 1, flip-flop changes output current state opposite. Figure 2.17 shows sequence inputs resulting output. Figure 2.17 Illustration one type flip-flop reacts sequence inputs. flip-flop output changes input transitions 0 1 (i.e., zero volts five volts). responds sequence inputs, flip-flop simple combinatorial circuit. flip-flop cannot constructed single gate. However, flip-flop constructed pair latches.To understand flip-flop works, helpful plot input output graphical form function time. Engineers use term transition diagram plot. digital circuits, transitions coordinated clock, means transitions occur regular intervals. Figure 2.18 illustrates transition diagram flip-flop values Figure 2.17. line labeled clock Figure 2.18 shows clock pulses occur; input transition constrained occur one clock pulses. now, sufficient understand general concept; later sections explain clocks. Figure 2.18 Illustration transition diagram shows flip flop reacts series inputs Figure 2.17. Marks along x-axis indicate times; corresponds one clock tick. said flip-flop changes output time encounters one bit. fact, transition diagram shows exact details timing important circuit designers. example, transition diagram shows flip-flop triggered input rises. is, output change input transitions zero one. Engineers say output transition occurs rising edge input change; circuits transition input changes one zero said occur falling edge. practice, additional details complicate flip-flops. example, flip-flops include additional input named reset places output 0 state. addition, several variants flip-flops exist. example, flip-flops provide second output inverse main output (in circuits, inverse available results fewer gates). 2.16 Binary Counters single flip-flop offers two possible output values: 0 1. However, set flip-flops connected series form binary counter accumulates numeric total. Like flip- flop, counter single input. Unlike flip-flop, however, counter multiple outputs. outputs count many input pulses detected giving numerical total binary†. think outputs starting zero adding one time input transitions 0 1. Thus, counter three output lines accumulate total 0 7. Figure 2.19 illustrates counter, shows outputs change input changes.Figure 2.19 Illustration (a) binary counter, (b) sequence input values corresponding outputs. column labeled decimal gives decimal equivalent outputs. practice, electronic part implements binary counter several additional features. example, counter additional input used reset count zero, may also input temporarily stops counter (i.e., ignores input freezes output). important, fixed number output pins, counter maximum value represent. accumulated count exceeds maximum value, counter resets output zero uses additional output indicate overflow occurred. 2.17 Clocks Sequences Although seen basic building blocks digital logic, one additional feature absolutely essential digital computer: automatic operation. is, computer must able execute sequence instructions without inputs changing. digital logic circuits discussed previously use property respond changes one inputs; perform function input changes. digital logic circuit perform series steps? answer mechanism known clock allows hardware take action without requiring input change. fact, digital logic circuits said clocked, means clock signal, rather changes inputs, controls synchronizes operation individual components subassemblies ensure work together intended (e.g., guarantee later stages circuit wait propagation delay previous stages).What clock? Unlike common definition term, hardware engineers use term clock refer electronic circuit oscillates regular rate; oscillations converted sequence alternating ones zeros. Although clock created inverter†, clock circuits use quartz crystal, oscillates naturally, provide signal precise frequency. clock circuit amplifies signal changes sine wave square wave. Thus, think clock emitting alternating sequence 0 1 values regular rate. speed clock measured Hertz (Hz), number times per second clock cycles 1 followed 0. clocks high-speed digital computers operate speeds ranging one hundred megahertz (100 MHz) several gigahertz (GHz). example, present, clock used typical processor operates approximately 3 GHz. difficult human imagine circuits changing high rates. make concept clear, let’s consider clock available operates extremely slow rate 1 Hz. clock might used control interface human. example, computer contains LED flashes indicate computer active, slow clock needed control LED. Note clock rate 1 Hz means clock completes entire cycle one second. is, clock emits logical 1 one-half cycle followed logical zero one- half cycle. circuit arranges turn LED whenever clock emits logical 1, LED remain one-half second, one-half second. alternating sequence 0 1 values make digital circuits powerful? understand, consider simple clocked circuit. Suppose startup, computer must perform following sequence steps: Test battery Power test memory Start disk spinning Power screen Read boot sector disk memory Start CPU simplify explanation, assume step requires one second complete next step started. Thus, desire circuit that, started, perform six steps sequence, one-second intervals changes input. now, focus essence circuit, consider started later. circuit handle task performing six steps sequence built three building blocks: clock, binary counter, device known decoder/demultiplexor†, often abbreviated demux. already considered counter, assume clock available generates digital output rate exactly one cycle per second. last component, decoder/demultiplexor, single integrated circuit uses binary value map input set outputs. use decoding function select output. is, decoder takes binary value input, uses value choose output. one output decoder time; others — input lines represent value binary, decoder selects ith output. Figure 2.20 illustrates concept.Figure 2.20 Illustration decoder three input lines eight output lines. inputs x, y, z values 0, 1, 1, fourth output top selected. used decoder, device merely selects one outputs; used demultiplexor, device takes extra input passes selected output. decoder function complex demultiplexor function constructed Boolean gates. decoder provides last piece needed simplistic sequencing mechanism — combine clock, counter, decoder, resulting circuit execute series steps. example, Figure 2.21 shows interconnection output clock used input binary counter, output binary counter used input decoder. Figure 2.21 illustration clock used create circuit performs sequence six steps. Output lines counter connect directly input lines decoder. understand circuit operates, assume counter reset zero. counter output 000, decoder selects topmost output, used (i.e., connected). Operation starts clock changes logical 0 logical 1. counter accumulates count, changes output 001. input changes, decoder selects second output, labeled test battery. Presumably, second output wireconnects circuit performs necessary test. second output remains selected one second. second, clock output remains logical 1 one-half second, reverts logical 0 one-half second. clock output changes back logical 1, counter output lines change 010, decoder selects third output, connected circuitry tests memory. course, details important. example, decoder chips make selected output 0 outputs 1. Electrical details also matter. compatible devices, clock must use five volts logical 1, zero volts logical 0. Furthermore, directly connected, output lines binary counter must use binary representation input lines decoder. Chapter 3 discusses data representation detail; now, assume output input values compatible. 2.18 Important Concept Feedback simplistic circuit Figure 2.21 lacks important feature: way control operation (i.e., start stop sequence). clock runs forever, counter figure counts zero maximum value, starts zero. result, decoder repeatedly cycle outputs, output held one second moving next. digital circuits perform series steps repeatedly. arrange stop sequence six steps executed? solution lies fundamental concept: feedback. Feedback lies heart complex digital circuits allows results processing affect way circuit behaves. computer startup sequence, feedback needed steps. disk cannot started, example, boot sector cannot read disk. already seen feedback used maintain data value latch circuit Figure 2.14 output right-most nand gates feeds back input gate. another example feedback, consider might use final output decoder, call F, stop sequence. easy solution consists using value F prevent clock pulses reaching counter. is, instead connecting clock output directly counter input, insert logic gates allow counter input continue F value 0. terms Boolean algebra, counter input be: is, long F false, counter input equal clock; F true, however, counter input changes (and remains) zero. Figure 2.22 shows two inverters nand gate used implement necessary function.Figure 2.22 modification circuit Figure 2.21 includes feedback stop processing one pass output. feedback Figure 2.22 fairly obvious explicit physical connection last output combinatorial circuit input side. figure also makes easy see feedback mechanisms sometimes called feedback loops†. 2.19 Starting Sequence Figure 2.22 shows possible use feedback terminate process. However, circuit still incomplete contain mechanism allows sequence start. Fortunately, adding starting mechanism trivial. understand why, recall counter contains separate input line resets count zero. needed make circuit start another input (e.g., button user pushes) connected counter reset. user pushes button, counter resets zero, causes counter’s output become 000. receives input zeros, decoder turns first output, turns last output. last output turns off, nand gate allows clock pulses through, counter begins run. Although indeed start sequence, allowing user reset counter cause problems. example, consider happens user becomes impatient startup sequence presses button second time. counter resets, sequence starts beginning. cases, performing operation twice simply wastes time. cases, however, repeating operation causes problems (e.g., disk drives require one command issued time). Thus, production system uses complex combinatorial logic prevent sequence interrupted restarted completes. Although contains components, example demonstrates important concept: set Boolean logic gates clock sufficient allow execution sequence oflogical steps. point is: example circuit shows Boolean logic gates clock make possible build circuit which, started, performs logical sequence steps halts. one additional concept needed create general-purpose computer: programmability. Chapter 6 extends discussion hardware showing basic components described used build programmable processor uses program memory determine sequence operations. 2.20 Iteration Software Vs. Replication Hardware think hardware, important remember significant difference way software hardware handle operations must applied set items. software, fundamental paradigm handling multiple items consists iteration — programmer writes code repeatedly finds next item set applies operation item. underlying system applies operation one item time, programmer must specify number items. Iteration essential programming programming languages provide statements (e.g., loop) allow programmer express iteration clearly. Although hardware built perform iteration, difficult resulting hardware clumsy. Instead, fundamental hardware paradigm handling multiple items consists replication — hardware engineer creates multiple copies circuit, allows copy act one item. copies perform time. example, compute Boolean operation pair thirty-two bit values, hardware engineer designs circuit pair bits, replicates circuit thirty-two times. Thus, compute Boolean exclusive two thirty-two bit integers, hardware designer use thirty-two xor gates. Replication difficult programmers appreciate replication antithetical good programming — programmer taught avoid duplicating code. hardware world, however, replication three distinct advantages: elegance, speed, correctness. Elegance arises replication avoids extra hardware needed select individual item, move place, move result back. addition avoiding delay involved moving values results, replication increases performance allowing multiple operations performed simultaneously. example, thirty-two inverters working time invert thirty-two bits exactly amount time takes one inverter invert single bit. speedup especially significant computer operate sixty-four bits time. notion parallel operation appears throughout text; later chapter explains parallelism applies larger scale.The third advantage replication focuses high reliability. Reliability increased replication makes hardware easier validate. example, validate thirty-two bit operation works correctly, hardware engineer needs validate circuit single bit — remaining bits work circuit replicated. result, hardware much reliable software. Even legal system holds product liability standards higher hardware software — unlike software often sold “as is” without warranty, hardware (e.g., integrated circuit) sold within legal framework requires fitness intended purpose. summarize: Unlike software, uses iteration, hardware uses replication. advantages replication increased elegance, higher speed, increased reliability. 2.21 Gate Chip Minimization glossed many underlying engineering details. example, choose general design amount replication used, engineers seek ways minimize amount hardware needed. two issues: minimizing gates minimizing integrated circuits. first issue involves general rules Boolean algebra. example, consider Boolean expression: circuit implement expression consists two inverters connected together. course, know two operations identity function, expression replaced z. is, pair directly connected inverters removed circuit without affecting result. another example Boolean expression optimization, consider expression: Either x value 1, x value 1, means function always produce value, logical 0. Therefore, entire expression replaced value 0. terms circuit, would foolish use gate inverter compute expression circuit resulting two gates always logical zero. Thus, engineer writes Boolean expression formula, formula analyzed look instances subexpressions reduced eliminated without changing result. Fortunately, sophisticated design tools exist help engineers minimize gates. tools take Boolean expression input. design tool analyzes expression produces circuit implements expression minimum number gates. tools merely use Boolean and, or, not. Instead, understand gates available (e.g., nand), define circuit terms available electronic parts.Although Boolean formulas optimized mathematically, optimization needed overall goal minimization integrated circuits. understand situation, recall many integrated circuits contain multiple copies given type gate. Thus, minimizing number Boolean operations may optimize circuit optimization increases types gates required. example, suppose Boolean expression requires four nand gates, consider optimization reduces requirements three gates: two nand gates gate. Unfortunately, although total number gates lower, optimization increases number integrated circuits required single 7400 integrated circuit contains four nand gates, two integrated circuits required optimization includes nand gates. 2.22 Using Spare Gates Consider circuit Figure 2.22 carefully†. Assuming clock, counter, decoder require one integrated circuit, many additional integrated circuits required? obvious answer two: one needed nand gate (e.g., 7400) another two inverters (e.g., 7404). Surprisingly, possible implement circuit one additional integrated circuit. see how, observe although 7400 contains four nand gates, one needed. spare gates used? trick lies observing nand 1 0 1, nand 1 1 0. is, equivalent to: use nand gate inverter, engineer simply connects one two inputs logical one (i.e., five volts). spare nand gate used inverter. 2.23 Power Distribution Heat Dissipation addition planning digital circuits correctly perform intended function minimizing number components used, engineers must contend underlying power cooling requirements†. example, although diagrams chapter depict logical inputs outputs gates, every gate consumes power. amount power used single integrated circuit insignificant. However, hardware designers tend use replication instead iteration, complex digital systems contain many circuits. engineer must calculate total power required, construct appropriate power supplies, plan additional wiring carry power chip. laws physics dictate device consumes power generate heat. amount heat generated proportional amount power consumed, integrated circuit generates minimal amount heat. digital system uses hundreds circuits thatoperate small, enclosed space, total heat generated significant. Unless engineers plan mechanism dissipate heat, high temperatures cause circuits fail. small systems, engineers add holes chassis allow hot air escape replaced cooler air surrounding room. intermediate systems, personal computers, fans added move air surrounding room system quickly. largest digital systems, cool air insufficient — refrigeration system liquid coolant must used (e.g., circuits Cray 2 supercomputer directly immersed liquid coolant). 2.24 Timing Clock Zones quick tour digital logic omits another important aspect engineers must consider: timing. gate act instantly. Instead, gate takes time settle (i.e., change output input changes). examples, timing irrelevant clock runs incredibly slow rate 1 Hz gates settle less microsecond. Thus, gates settle long clock pulses. practice, timing essential aspect engineering digital circuits designed operate high speed. ensure circuit operate correctly, engineer must calculate time required gates settle. Engineers must also calculate time required propagate signals throughout entire system, must ensure system fail clock skew. understand clock skew, consider Figure 2.23 illustrates circuit board clock controls three integrated circuits system. Figure 2.23 Illustration three integrated circuits digital system controlled single clock. length wire clock integrated circuit determines clock signal arrives. figure, three integrated circuits physically distributed (presumably, integrated circuits occupy remaining space). Unfortunately, finite time required signal clock reach circuits, time proportional length wire clock given circuit. result, clock signal arrive integrated circuits sooner arrives others. rule thumb, signal requires onenanosecond propagate across one foot wire. Thus, system measures eighteen inches across, clock signal reach locations near clock nanosecond signal reaches farthest location. Obviously, clock skew cause problem parts system must operate parts. engineer needs calculate length path design layout avoids problem clock skew. consequence clock skew, engineers seldom use single global clock control system. Instead, multiple clocks used, clock controlling one part system. Often, clocks run highest rates used smallest physical areas. use term clock zone refer region given clock controls. idea limited physically large systems — integrated circuits, CPUs, become large complex multiple clock zones used chip. Although using multiple clock zones avoids problems clock skew, multiple clocks introduce another problem, clock synchronization: digital logic boundary two clock zones must engineered accommodate zones. Usually, accommodation means circuit slows takes multiple clock cycles move data. 2.25 Clockless Logic chips increase size complexity, problem clock skew division system clock zones become increasingly important. many systems, boundary clock zones forms bottleneck logic circuits boundary must wait multiple clock cycles output one clock zone forwarded another clock zone. problem zone synchronization become important researchers devised alternative approach: clockless logic. essence, clockless system uses two wires instead one represent Boolean value. use two wires means output indicate end bit unambiguously without depending clock. Figure 2.24 lists four possible combinations values two wires meanings. Figure 2.24 Meaning signals two wires clockless logic used transfer bits one chip another. idea sender sets wires zero volts bit reset receiver. reset, sender transfers logical 0 logical 1. receiver knows bit arrives exactly one two wires high (e.g., 5 volts). use clockless logic? addition eliminating problem clock zone coordination allowing higher speed data transfer among chips, clockless approach use less power. Clocked circuits need propagate clock signal continuously, even parts circuit inactive. Clockless logic avoid overhead propagating clock signals.Does clockless approach work practice? Yes. designing entire processor uses clockless logic, ARM, Inc. demonstrated approach scales large, complex circuits. Thus, clockless approach potential. Currently, chip designers still use clocked approach. 2.26 Circuit Size Moore’s Law digital circuits built Integrated Circuits (ICs), technology permits many transistors placed single silicon chip along wiring interconnects them. idea components IC form useful circuit. ICs often created using Complementary Metal Oxide Semiconductor (CMOS) technology. Silicon doped impurities give negative positive ionization. resulting substances known N-type silicon P-type silicon. arranged layers, N- type P-type silicon form transistors. IC manufacturers create single IC time. Instead, manufacturer creates round wafer twelve eighteen inches diameter contains many copies given IC design. wafer created, vendor cuts individual chips, packages chip plastic case along pins connect chip. ICs come variety shapes sizes; eight external connections (i.e., pins), others hundreds pins†. ICs contain dozens transistors, others contain millions. Depending number transistors chip, ICs divided four broad categories Figure 2.25 lists. Figure 2.25 classification scheme used integrated circuits. example, integrated 7400, 7402, 7404 circuits described chapter classified SSI. binary counter, flip-flop, demultiplexor classified MSI. definition VLSI keeps changing manufacturers devise new ways increase density transistors per square area. Gordon Moore, cofounder Intel Corporation, attributed observed density silicon circuits, measured number transistors per square inch, would double every year. observation, known Moore’s Law, revised 1970s, rate slowed doubling every eighteen months. number transistors single chip increased, vendors took advantage capability add functionality. vendors created multicore CPU chips placing multiple copies CPU (called core) single chip, providinginterconnections among cores. vendors took System Chip (SoC) approach single chip contains processors, memories, interfaces I/O devices, interconnected form complete system. Finally, memory manufacturers created chips larger larger amounts main memory called Dynamic Ram (DRAM). addition general-purpose ICs designed sold vendors, become possible build special-purpose ICs. Known Application Specific Integrated Circuits (ASICs), ICs designed private company, designs sent vendor manufactured. Although designing ASIC expensive time-consuming — approximately two million dollars nearly two years — design completed, copies ASIC inexpensive produce. Thus, companies choose ASIC designs products standard chips meet requirements company expects large volume product produced. 2.27 Circuit Boards Layers digital systems built using Printed Circuit Board (PCB) consists fiberglass board thin metal strips attached surface holes mounting integrated circuits components. essence, metal strips circuit board form wiring interconnects components. circuit board used complex interconnections require wires cross? Interestingly, engineers developed multilayer circuit boards solve problem. essence, multilayer circuit board allows wiring three dimensions — wire must cross another, designer arrange pass wire higher layer, make crossing, pass wire back down. may seem layers suffice circuit. However, large complex circuits thousands interconnections may need additional layers. uncommon engineers design circuit boards eighteen layers; advanced boards twenty-four layers. 2.28 Levels Abstraction chapter illustrates, possible view digital logic various levels abstraction. lowest level, transistor created silicon. next level, multiple transistors used along components, resistors diodes, form gates. next level, multiple gates combined form intermediate scale units, flip flops. later chapters, discuss complex mechanisms, processors, memory systems, I/O devices, constructed multiple intermediate scale units. Figure 2.26 summarizes levels abstraction. important point moving levels abstraction allows us hide details talk larger larger building blocks without giving internal details. describeprocessors, example, consider processor works without examining internal structure level gates transistors. Figure 2.26 example levels abstraction digital logic. item one level implemented using items next lower level. important consequence abstraction arises diagrams architects engineers use describe digital systems. seen, schematic diagrams represent interconnection transistors, resistors, diodes. Diagrams also used represent interconnection among gates. later chapters, use high-level diagrams represent interconnection processors memory systems. diagrams, small rectangular box represent processor memory without showing interconnection gates. looking architectural diagram, important understand level abstraction remember single item high-level diagram correspond arbitrarily large number items lower-level abstraction. 2.29 Summary Digital logic refers pieces hardware used construct digital systems computers. seen, Boolean algebra important tool digital circuit design — direct relationship Boolean functions gates used implement combinatorial digital circuits. also seen Boolean logic values described using truth tables. clock mechanism emits pulses regular intervals form signal alternating ones zeros. clock allows digital circuit output function time well logic inputs. clock also used provide synchronization among multiple parts circuit. Although think digital logic mathematical point view, building practical circuits involves understanding underlying hardware details. particular, besides basic correctness, engineers must contend problems power distribution, heat dissipation, clock skew. EXERCISES2.1 Use Web find number transistors VLSI chip physical size chip. entire die used, large would individual transistor be? 2.2 Digital logic circuits used smart phones battery-powered devices run five volts. Look battery smart phone search Web find voltage used. 2.3 Design circuit uses nand, inverter gates provide exclusive function. 2.4 Write truth table full adder circuit Figure 2.12. 2.5 Use Web read flip-flops. List major types characteristics. 2.6 Create circuit decoder nand, nor, inverter gates. 2.7 Look Web sources, Wikipedia, answer following question: chip manufacturer boasts uses seven nanometer chip technology, manufacturer mean? 2.8 maximum number output bits counter chip chip sixteen pins? (Hint: chip needs power ground connections.) 2.9 decoder chip five input pins (not counting power ground), many output pins have? 2.10 Design circuit takes three inputs, A, B, C, generates three outputs. circuit would trivial, except may use two inverters. may use arbitrary chips (e.g., nand, nor, exclusive or). 2.11 Assume circuit spare gate. useful functions created connecting one inputs logical one? logical zero? Explain. 2.12 Read clockless logic. used? †Technically, diagram depicts p-channel n-channel forms MOSFET. †Some digital circuits use 5 volts use 3.3 volts; rather specify voltage, hardware engineers write Vdd denote voltage appropriate given circuit. †A later section explains use term truth tables describe tables used figure. †The technology limits number inputs connected single output; use term fanout specify number inputs output supplies. †Appendix 2 lists set rules used minimize Boolean expressions. †In addition logic gates described section, 7400 family also includes sophisticated mechanisms, flip-flops, counters, demultiplexors, described later chapter. †Although diagram shows 4-bit register, registers used typical processors store 32 bits 64 bits. †Chapter 3 considers data representation detail. now, sufficient understand outputs represent number. †See Figure 2.15 page 24. †An alternate spelling demultiplexer also used. †A feedback loop also present among gates used construct flip-flop. †Figure 2.22 found page 31. †Chapter 20 considers power detail. †Engineers use term pinout describe purpose pin chip.3 Data Program Representation Chapter Contents 3.1 Introduction 3.2 Digital Logic Importance Abstraction 3.3 Definitions Bit Byte 3.4 Byte Size Possible Values 3.5 Binary Weighted Positional Representation 3.6 Bit Ordering 3.7 Hexadecimal Notation 3.8 Notation Hexadecimal Binary Constants 3.9 Character Sets 3.10 Unicode 3.11 Unsigned Integers, Overflow, Underflow 3.12 Numbering Bits Bytes 3.13 Signed Binary Integers 3.14 Example Two’s Complement Numbers 3.15 Sign Extension 3.16 Floating Point 3.17 Range IEEE Floating Point Values 3.18 Special Values 3.19 Binary Coded Decimal Representation 3.20 Signed, Fractional, Packed BCD Representations3.21 Data Aggregates 3.22 Program Representation 3.23 Summary 3.1 Introduction previous chapter introduces digital logic, describes basic hardware building blocks used create digital systems. chapter continues discussion fundamentals explaining digital systems use binary representations encode programs data. see representation important programmers well hardware engineers software must understand format underlying hardware uses, format affects speed hardware perform operations, addition. 3.2 Digital Logic Importance Abstraction seen, digital logic circuits contain many low-level details. circuits use transistors electrical voltage perform basic operations. main point digital logic, however, abstraction — want hide underlying details use high-level abstractions whenever possible. example, seen input output 7400-series digital logic chip restricted two possible conditions: zero volts five volts. computer architects use logic gates design computers, however, think details. Instead, use abstract designations logical 0 logical 1 Boolean algebra. Abstracting means complex digital systems, memories processors, described without thinking individual transistors voltages. important, abstraction means design used battery-operated device, smart phone, uses lower voltages reduce power consumption. programmer, important abstractions items visible software: representations used data programs. next sections consider data representation, discuss visible programs; later sections describe instructions represented. 3.3 Definitions Bit Byte data representation builds digital logic. use abstraction binary digit (bit) describe digital entity two possible values, assign mathematical names 0and 1 two values. Multiple bits used represent complex data items. example, computer system defines byte smallest data item larger bit hardware manipulate. big byte? size byte standard across computer systems. Instead, size chosen architect designs computer. Early computer designers experimented variety byte sizes, special-purpose computers still use unusual byte sizes. example, early computer manufactured CDC corporation used six-bit byte, computer manufactured BB&N used ten-bit byte. However, modern computer systems define byte contain eight bits — size become widely accepted engineers usually assume byte size equal eight bits, unless told otherwise. point is: Although computers designed size bytes, current computer industry practice defines byte contain eight bits. 3.4 Byte Size Possible Values number bits per byte especially important programmers memory organized sequence bytes. size byte determines maximum numerical value stored one byte. byte contains k bits represent one 2k values (i.e., exactly 2k unique strings 1s 0s exist length k). Thus, six-bit byte represent 64 possible values, eight-bit byte represent 256 possible values. example, consider eight possible combinations achieved three bits. Figure 3.1 illustrates combinations. Figure 3.1 eight unique combinations assigned three bits. given pattern bits represent? important thing understand bits intrinsic meaning — interpretation value determined way hardware software use bits. example, string bits could represent alphabetic character, string characters, integer, floating point number, audio recording (e.g., song), video, computer program. addition items computer programmer understands, computer hardware designed set bits represent status three peripheral devices. example: • first bit value 1 keyboard connected. • second bit value 1 camera connected.• third bit value 1 printer connected. Alternatively, hardware designed set three bits represent current status three pushbutton switches: ith bit 1 user currently pushing switch i. point is: Bits intrinsic meaning — meaning imposed way bits interpreted. 3.5 Binary Weighted Positional Representation One common abstractions used associate meaning combination bits interprets numeric value. example, integer interpretation taken mathematics: bits values positional number system uses base two. understand interpretation, remember base ten, possible digits 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, position represents power 10, number 123 represents 1 times 102 plus 2 times 101 plus 3 times 100. binary system, possible digits 0 1, bit position represents power two. is, positions represent successive powers two: 20, 21, 22, on. Figure 3.2 illustrates positional concept binary numbers. Figure 3.2 value associated first six bit positions using positional interpretation base two. example, consider binary number: 0 1 0 1 0 1 According figure, value interpreted as: 0 1 0 1 0 1 = 0×25 + 1×24 + 0×23 + 1×22 + 0×21 + 1×20 = 21 discuss specific forms integer representation (including negative numbers) later chapter. now, sufficient observe important consequence conventional positional notation: binary numbers represented k bits start zero instead one. use positional interpretation illustrated Figure 3.2, binary numbers berepresented three bits range zero seven. Similarly, binary numbers represented eight bits range zero two hundred fifty-five. summarize: set k bits interpreted represent binary integer. conventional positional notation used, values represented k bits range 0 2k–1. essential skill design software hardware, anyone working fields know basics. Figure 3.3 lists decimal equivalents binary numbers hardware software designers know. table includes entries 232 264 (an incredibly large number). Although smaller values table memorized, hardware software designers need know order magnitude larger entries. Fortunately, easy remember 232 contains ten decimal digits 264 contains twenty. 3.6 Bit Ordering positional notation Figure 3.2 may seem obvious. all, writing decimal numbers, always write least significant digit right significant digit left. Therefore, writing binary, makes sense write Least Significant Bit (LSB) right Significant Bit (MSB) left. digital logic used store integer, however, concepts “right” “left” longer make sense. Therefore, computer architect must specify exactly bits stored, least significant. idea bit ordering especially important bits transferred one location another. example, numeric value moved register memory, bit ordering must preserved. Similarly, sending data across network, sender receiver must agree bit ordering. is, two ends must agree whether LSB MSB sent first.Figure 3.3 Decimal values commonly used powers two. 3.7 Hexadecimal Notation Although binary number translated equivalent decimal number, programmers engineers sometimes find decimal equivalent difficult understand. example, programmer needs test fifth bit right, using binary constant 010000 makes correspondence constant bit much clearer equivalent decimal constant 16. Unfortunately, long strings bits unwieldy difficult understand decimal equivalent. example, determine whether sixteenth bit set following binary number, human needs count individual bits: 1 1 0 1 1 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 0 1To aid humans expressing binary values, compromise reached: positional numbering system larger base. base chosen power two, translation binary trivial. Base eight (known octal) used, base sixteen (known hexadecimal) become especially popular. Hexadecimal representation offers two advantages. First, representation substantially compact binary, resulting strings shorter. Second, sixteen power two, conversion binary hexadecimal straightforward involve complex arithmetic calculation (i.e., human perform transformation easily quickly, without need calculator tools). essence, hexadecimal encodes group four bits single hex† digit zero fifteen. Figure 3.4 lists sixteen hex digits along binary decimal equivalent each. figure examples follow use uppercase letters F represent hex digits nine. programmers programming languages use lowercase letters f instead; distinction unimportant programmers prepared use either form. Figure 3.4 sixteen hexadecimal digits equivalent binary decimal values. hex digit encodes four bits binary value. example hexadecimal encoding, Figure 3.5 illustrates binary string corresponds hexadecimal equivalent. Figure 3.5 Illustration relationship binary hexadecimal. hex digit represents four bits. 3.8 Notation Hexadecimal Binary Constants digits used binary, decimal, hexadecimal number systems overlap, constants ambiguous. solve ambiguity, alternate notation needed. Mathematicians textbooks add subscript denote base ten (e.g., 13516 specifies constant hexadecimal). Computer architects programmers tend follow programming language notation: hex constants begin prefix 0x, binary constants begin prefix 0b. Thus, denote 13516, programmer writes 0x135. Similarly, 32-bit constant Figure 3.5 written: 0xDEC90949 3.9 Character Sets said bits intrinsic meaning, hardware software must determine bit represents. important, one interpretation used — set bits created used one interpretation later used another. example, consider character data numeric symbolic interpretation. computer system defines character set† set symbols computer I/O devices agree use. typical character set contains uppercase lowercase letters, digits, punctuation marks. important, computer architects often choose character set character fits byte (i.e., bit patterns byte assigned one character). Thus, computer uses eight-bit byte two hundred fifty-six (28) characters character set, computer uses six-bit byte sixty-four (26) characters. fact, relationship byte size character set strong many programming languages refer byte character. bit values used encode character? computer architect must decide. 1960s, example, IBM Corporation chose Extended Binary Coded Decimal Interchange Code (EBCDIC) representation character set used IBM computers. CDC Corporation chose six-bit character set use computers. two character sets completely incompatible. practical matter, computer systems connect devices keyboards, printers, modems, devices often built separate companies. interoperate correctly, peripheral devices computer systems must agree bit pattern corresponds given symbolic character. help vendors build compatible equipment, American National Standards Institute (ANSI) defined character representation known American Standard Code Information Interchange (ASCII). ASCII character set specifies representation one hundred twenty-eight characters, including usual letters, digits, punctuation marks;additional values eight-bit byte assigned special symbols. standard widely accepted. Figure 3.6 lists ASCII representation characters giving hexadecimal value corresponding symbolic character. course, hexadecimal notation merely shorthand notation binary string. example, lowercase letter hexadecimal value 0x61, corresponds binary value 0b01100001. Figure 3.6 ASCII character set. entry shows hexadecimal value graphical representation printable characters meaning others. said conventional computer uses eight-bit bytes, ASCII defines one hundred twenty-eight characters (i.e., seven-bit character set). Thus, ASCII used conventional computer, one-half byte values unassigned (decimal values 128 255). additional values used? cases, — peripheral devices accept deliver characters merely ignore eighth bit byte. cases, computer architect programmer extends character set (e.g., adding punctuation marks alternate languages). 3.10 Unicode Although seven-bit character set eight-bit byte work well English European languages, suffice languages. Chinese, example, containsthousands symbols glyphs. accommodate languages, extensions alternatives proposed. One widely accepted extended character sets named Unicode. Unicode extends ASCII intended accommodate languages, including languages Far East. Originally designed sixteen-bit character set, later versions Unicode extended accommodate larger representations. Thus, future computers I/O devices may base character set Unicode. 3.11 Unsigned Integers, Overflow, Underflow positional representation binary numbers illustrated Figure 3.2† said produce unsigned integers. is, 2k combinations bits associated nonnegative numeric value. unsigned integers used computer finite size, operations like addition subtraction unexpected results. example, subtracting positive k-bit unsigned integer smaller positive k-bit unsigned integer yield negative (i.e., signed) result. Similarly, adding two k-bit unsigned integers produce value requires k bits represent. Hardware perform unsigned binary arithmetic handles problem interesting way. First, hardware produces result using wraparound (i.e., hardware adds two k-bit integers, takes k low-order bits answer). Second, hardware sets overflow underflow conditions indicate whether result exceeded k bits negative‡. example, overflow indicator corresponds value would appear k+1st bit (i.e., value commonly known carry). Figure 3.7 illustrates addition three-bit arithmetic results carry. Figure 3.7 Illustration addition unsigned integers produces overflow. overflow indicator, tells whether wraparound occurred, equal carry bit. 3.12 Numbering Bits Bytes set bits numbered? view set string, makes sense start numbering left, view set binary number, makes sense start numbering right (i.e., numerically least significant bit). Numbering especiallyimportant data transferred network sending receiving computers must agree whether least-significant most-significant bit transferred first. issue numbering becomes complicated consider data items span multiple bytes. example, consider integer consists thirty-two bits. computer uses eight-bit bytes, integer span four bytes, transferred starting least-significant most-significant byte. use term little endian characterize system stores transmits bytes integer least significant significant, term big endian characterize system stores transmits bytes integer significant least significant. Similarly, use terms bit little endian bit big endian characterize systems transfer bits within byte starting least-significant bit most-significant bit, respectively. think bytes integer stored array, endianness determines direction memory. Figure 3.8 uses example integer illustrate two byte orders, showing positional representation arrangement bytes memory using little endian order big endian order. Figure 3.8 (a) Integer 497,171,303 expressed 32-bit binary value, spaces used mark groups eight bits, (b) integer stored successive memory locations using little endian order, (c) integer stored successive memory locations using big endian order. big endian representation may seem appealing mimics order humans use write numbers. Surprisingly, little endian order several advantages computing. example, little endian allows programmer use single memory address refer four bytes integer, two low-order bytes, lowest-order byte. 3.13 Signed Binary IntegersThe positional representation described Section 3.5 provision negative numbers. accommodate negative numbers, need alternative. Three interpretations used: • Sign Magnitude. sign-magnitude representation, bits integer divided sign bit (1 number negative 0 otherwise) set bits gives absolute value (i.e., magnitude) integer. magnitude field follows positional representation illustrated Figure 3.2. • One’s Complement. set bits interpreted single field. positive integer uses positional representation illustrated Figure 3.2 restriction integer k bits, maximum positive value 2k-1. form negative value, invert bit (i.e., change 0 1 vice versa). significant bit tells sign integer (1 negative integers, 0 otherwise). • Two’s Complement. set bits interpreted single field. positive integer uses positional representation illustrated Figure 3.2 restriction integer k bits, maximum positive value 2k-1–1. Thus, positive integers representation one’s complement. form negative number, start positive number, subtract one, invert bit. one’s complement, significant bit tells sign integer (1 negative integers, 0 otherwise). interpretation interesting quirks. example, sign-magnitude interpretation makes possible create value negative zero, even though concept correspond valid mathematical concept. one’s complement interpretation provides two values zero: zero bits complement, one bits. Finally, two’s complement interpretation includes one negative value positive values (to accommodate zero). interpretation best? Programmers debate issue interpretation works well cases. However, programmers cannot choose computer architects make decision build hardware accordingly. three representations used least one computer. Many hardware architectures use two’s complement scheme. two reasons. First, two’s complement makes possible build low-cost, high-speed hardware perform arithmetic operations. Second, next section explains, hardware two’s complement arithmetic also handle unsigned arithmetic. 3.14 Example Two’s Complement Numbers said k bits represent 2k possible combinations. Unlike unsigned representation combinations correspond continuous set integers starting zero, two’s complement divides combinations half. combination first half (zero 2k- 1–1) assigned value unsigned representation. Combinations second half, high-order bit equal one, correspond negative integers. Thus, exactly one-half way possible combinations, value changes largest possible positive integer negative integer largest absolute value.An example clarify two’s complement assignment. keep example small, consider four-bit integer. Figure 3.9 lists sixteen possible bit combinations decimal equivalent using unsigned, sign magnitude, one’s complement, two’s complement representations. Figure 3.9 decimal value assigned combination four bits using unsigned, sign-magnitude, one’s complement, two’s complement interpretations. noted above, unsigned two’s complement advantage except overflow, hardware operations work representations. example, adding one binary value 1001 produces 1010. unsigned interpretation, adding one nine produces ten; two’s complement interpretation, adding one negative seven produces negative six. important point is: computer use single hardware circuit provide unsigned two’s complement integer arithmetic; software running computer choose interpretation integer. 3.15 Sign ExtensionAlthough Figure 3.9 shows four-bit binary strings, ideas extended arbitrary number bits. Many computers include hardware multiple integer sizes (e.g., single computer offer sixteen bit, thirty-two bit, sixty-four bit representations), allow programmer choose one sizes integer data item. computer contain multiple sizes integers, situation arise value copied smaller-size integer larger-size integer. example, consider copying value sixteen-bit integer thirty-two-bit integer. placed extra bits? two’s complement, solution consists copying least significant bits extending sign bit — original value positive, extending high-order bit fills significant bits larger number zeros; original value negative, extending high-order bit fills significant bits larger number ones. either case, integer bits interpreted numeric value integer fewer bits†. summarize: Sign extension: two’s complement arithmetic, integer Q composed k bits copied integer k bits, additional high-order bits made equal top bit Q. Extending sign bit ensures numeric value two interpreted two’s complement value. two’s complement hardware gives correct results performing arithmetic operations unsigned values, may seem software could use hardware support unsigned operations. However, sign extension provides exception rule: hardware always perform sign extension, may unexpected results. example, unsigned integer copied larger unsigned integer, copy numeric value high-order bit 1. point is: two’s complement hardware performs sign extension, copying unsigned integer larger unsigned integer change value. 3.16 Floating Point addition hardware performs signed unsigned integer arithmetic, general-purpose computers provide hardware performs arithmetic floating point values. Floating point representation used computers derives scientific notation value represented mantissa exponent. example, scientific notation expresses value –12345 –1.2345×104. Similarly, chemist might write well-known constant, Avogadro’s number, as:6.023 × 1023 Unlike conventional scientific notation, floating point representation used computers based binary. Thus, floating point value consists bit string divided three fields: bit store sign, group bits stores mantissa, third group bits stores exponent. Unlike conventional scientific notation, everything floating point based powers two. example, mantissa uses binary positional representation store value, exponent integer specifies power 2 rather power 10. scientific notation, think exponent specifying many digits shift decimal point; floating point, exponent specifies many bits shift binary point. optimize space, many floating point representations include optimizations: value normalized. significant bit mantissa implicit. exponent biased simplify magnitude comparison. first two optimizations related. floating point number normalized adjusting exponent eliminate leading zeros mantissa. decimal, example, 0.003×104 normalized 3×101. Interestingly, normalizing binary floating point number always produces leading bit 1 (except special case number zero). Therefore, increase number bits precision mantissa, floating point representations need store significant bit mantissa value stored memory. Instead, floating point number computation required, hardware concatenates 1 bit onto mantissa. example clarify concepts. example use IEEE† standard 754, widely used computer industry. standard specifies single precision double precision numbers. According standard, single precision value occupies thirty-two bits, double precision value occupies sixty-four bits. Figure 3.10 illustrates IEEE standard divides floating point number three fields. Figure 3.10 format (a) single precision (b) double precision floating point number according IEEE Standard 754, lowest bit field labeled. Fields consist sign bit, exponent, mantissa.Bit numbering figure follows IEEE standard, least significant bit assigned bit number zero. single precision, example, twenty-three rightmost bits, constitute mantissa, numbered zero twenty-two. next eight bits, constitute exponent, numbered twenty-three thirty, significant bit, contains sign, bit number thirty-one. double precision, mantissa occupies fifty-two bits exponent occupies eleven bits. 3.17 Range IEEE Floating Point Values IEEE standard single precision floating point allows normalized values exponent ranges negative one hundred twenty-six one hundred twenty-seven. Thus, approximate range values represented is: 2-126 2127 which, decimal, approximately: 10-38 1038 IEEE standard double precision floating point provides enormously larger range single precision. range is: 2-1022 21023 which, decimal, approximately: 10-308 10308 make magnitude comparison fast, IEEE standard specifies exponent field stores exponent (a power two) plus bias constant. bias constant used single precision one hundred twenty-seven, bias constant used double precision one thousand twenty-three†. example, store exponent three, exponent field single precision value assigned value one hundred thirty, exponent negative five represented one hundred twenty-two. example floating point, consider 6.5 represented. binary, 6 110, .5 single bit following binary point, giving us 110.1 (binary). use binary scientific notation normalize value, 6.5 expressed: 1.101 × 22To express value IEEE single precision floating point number, sign bit zero, exponent must biased 127, making 129. binary, 129 is: 10000001 understand value mantissa, recall leading 1 bit stored, means instead 1101 followed zeros, mantissa stored as: 10100000000000000000000 Figure 3.11 shows fields combine form single-precision IEEE floating point representation 6.5. Figure 3.11 value 6.5 (decimal) represented single-precision IEEE floating point constant. 3.18 Special Values Like floating point representations, IEEE standard follows implicit leading bit assumption — mantissa assumed leading one bit stored. course, representation strictly enforces assumption leading one bit useless representation cannot store value zero. handle zero, IEEE standard makes exception — bits zero, implicit assumption ignored, stored value taken zero. IEEE standard includes two special values reserved represent positive negative infinity: exponent contains ones mantissa contains zeros. point including values infinity digital systems facilities handle errors arithmetic overflow. systems, important value reserved software determine floating point operation failed. 3.19 Binary Coded Decimal Representation computers employ binary representations integers floating point numbers described above. underlying hardware uses digital logic, binary digits 0 1 map directly onto hardware. result, hardware compute binary arithmetic efficiently combinations bits valid. However, two disadvantages arise use binary representations. First, range values power two rather power ten (e.g., therange unsigned 32-bit integer zero 4,294,967,295). Second, floating point values rounded binary fractions rather decimal fractions. use binary fractions unintended consequences, use suffice computations. example, consider bank account stores U.S. dollars cents. usually represent cents hundredths dollars, writing 5.23 denote five dollars 23 cents. Surprisingly, one hundredth (i.e., one cent) cannot represented exactly binary floating point number turns repeating binary fraction. Therefore, binary floating point arithmetic used bank accounts, individual pennies rounded, making totals inaccurate. scientific sense, inaccuracy bounded, humans demand banks keep accurate records — become upset bank preserves significant digits account loses pennies. accommodate banking computations decimal required, Binary Coded Decimal (BCD) representation used. computers (notably IBM mainframes) hardware support BCD arithmetic; computers, software performs arithmetic operations BCD values. Although variety BCD formats used, essence always same: value represented string decimal digits. simplest case consists character string byte contains character single digit. However, use character strings makes computation inefficient takes space needed. example, computer uses ASCII character set, integer 123456 stored six bytes values†: 0x31 0x32 0x33 0x34 0x35 0x36 character format used, ASCII character (e.g., 0x31) must converted equivalent binary value (e.g., 0x01) arithmetic performed. Furthermore, operation performed, digits result must converted binary back character format. make computation efficient, modern BCD systems represent digits binary rather characters. Thus, 123456 could represented as: 0x01 0x02 0x03 0x04 0x05 0x06 Although use binary representation advantage making arithmetic faster, also disadvantage: BCD value must converted character format displayed printed. general idea arithmetic performed frequently I/O, keeping binary form improve overall performance. 3.20 Signed, Fractional, Packed BCD Representations description BCD omits many details found commercial systems. example, implementation may limit size BCD value. handle fractions, BCD must either include explicit decimal point representation must specify location decimal point. Furthermore, handle signed arithmetic, BCD representation must include sign. Interestingly,one widely used BCD conventions places sign byte right-hand end BCD string. Thus -123456 might represented sequence: 0x01 0x02 0x03 0x04 0x05 0x06 0x2D 0x2D value used indicate minus sign. advantage placing sign right arises scanning required arithmetic performed — bytes except last byte string correspond decimal digits. final detail used BCD encodings arises observation using byte digit inefficient. digit requires four bits, placing one digit eight-bit byte wastes half byte. reduce storage space needed BCD, packed representation used digit occupies nibble (i.e., four bits). packed version BCD, integer -123456 represented four bytes: 0x01 0x23 0x45 0x6D last nibble contains value 0xD indicate number negative†. 3.21 Data Aggregates far, considered representation individual data items characters, integers, floating point numbers. programming languages allow programmer specify aggregate data structures contain multiple data items, arrays, records, structs. values stored? general, aggregate value occupies contiguous bytes. Thus, computer uses eight-bit byte, data aggregate consists three sixteen-bit integers occupies six contiguous bytes Figure 3.12 illustrates. Figure 3.12 data aggregate consisting three sixteen-bit integers arranged successive bytes memory numbered 0 5. see later memory systems permit arbitrary data types contiguous. Thus, reconsider data aggregates discuss memory architecture. 3.22 Program Representation Modern computers classified stored program computers programs well data placed memory. discuss program representation storage nextchapters, including structure instructions computer understands storage memory. now, sufficient understand computer defines specific set operations format stored. computers, example, instruction size instructions; computers, instruction size varies. see typical computer, instruction occupies multiple bytes. Thus, bit byte numbering schemes computer uses data values also apply instructions. 3.23 Summary underlying digital hardware two possible values, logical 0 logical 1. think two values defining bit (binary digit), use bits represent data programs. computer defines byte size, current systems use eight bits per byte. set bits used represent character computer’s character set, unsigned integer, single double precision floating point value, computer program. Representations chosen carefully maximize flexibility speed hardware keeping cost low. two’s complement representation signed integers particularly popular single piece hardware constructed performs operations either two’s complement integers unsigned integers. cases decimal arithmetic required, computers use Binary Coded Decimal values number represented string specifies individual decimal digits. Organizations, ANSI IEEE, created standards representation; standards allow hardware manufactured two separate organizations interoperate exchange data. EXERCISES 3.1 Give mathematical proof string k bits represent 2k possible values (hint: argue induction number bits). 3.2 value following binary string hexadecimal? 1101 1110 1010 1101 1011 1110 1110 1111 3.3 Write computer program determines whether computer running uses big endian little endian representation integers. 3.4 Write computer program prints string zeros ones represents bits integer. Place blank bit, add extra space every four bits. 3.5 Write computer program determines whether computer running uses one’s complement, two’s complement, (possibly) representation signed integers.3.6 Write computer program determines whether computer running uses ASCII EBCDIC character set. 3.7 Write computer program takes set integers input integer prints two’s complement, one’s complement, sign-magnitude representation integer. 3.8 Write C program prints table possible eight-bit binary values two’s complement interpretation each. 3.9 Write computer program adds one largest possible positive integer uses result determine whether computer implements two’s complement arithmetic. 3.10 Write computer program display value byte hexadecimal, apply program array bytes. Add extra space every four bytes make output easier read. 3.11 Extend hexadecimal dump program previous exercise also print character representation printable character. characters printable representation, arrange program print period. 3.12 programmer computes sum two unsigned 32-bit integers. resulting sum less either two values? Explain. 3.13 Suppose given computer hardware perform 32-bit arithmetic, asked create functions add subtract 64-bit integers. perform 64-bit computations 32-bit hardware? (To simplify problem, limit answer unsigned arithmetic.) 3.14 C Programming language allows programmer specify constants decimal, binary, hexadecimal, octal. Write program declares 0, 5, 65, 128, -1 -256 decimal, binary, hexadecimal, octal, uses printf show values correct. easiest representation? 3.15 Create form Binary Coded Decimal similar one described text, write computer program uses form add two arbitrary length integers. 3.16 Extend previous program include multiplication. 3.17 financial industry uses “bankers” rounding algorithm. Read algorithm, implement program uses decimal arithmetic compute sum two decimal values bankers rounding conventional rounding. †Programmers use term hex abbreviation hexadecimal. †Names character sets pronounced, spelled out. example, EBCDIC pronounced ebb’sedick, ASCII pronounced ass’key. †Figure 3.2 found page 47. ‡The term underflow denotes value less representation hold. negative result unsigned integer arithmetic classified underflow negative values cannot represented. †Because division multiplication powers two implemented shift operations, sign extension occurs right-shift operation, results correct value. Thus, shifting integer -14 right one bit results -7, shifting integer 14 right one bit results 7. †IEEE stands Institute Electrical Electronics Engineers, organization creates standards used electronic digital systems. †The bias constant always 2k-1–1, k number bits exponent field. †Although examples use ASCII, BCD typically used IBM computers employ EBCDIC character set.†To aid BCD arithmetic, x86 architecture condition code bit indicates whether 4-bit addition overflows.Part II Processors Engines Drive Computation4 Variety Processors Computational Engines Chapter Contents 4.1 Introduction 4.2 Two Basic Architectural Approaches 4.3 Harvard Von Neumann Architectures 4.4 Definition Processor 4.5 Range Processors 4.6 Hierarchical Structure Computational Engines 4.7 Structure Conventional Processor 4.8 Processor Categories Roles 4.9 Processor Technologies 4.10 Stored Programs 4.11 Fetch-Execute Cycle 4.12 Program Translation 4.13 Clock Rate Instruction Rate 4.14 Control: Getting Started Stopping 4.15 Starting Fetch-Execute Cycle 4.16 Summary4.1 Introduction Previous chapters describe basic building blocks used construct computer systems: digital logic representations used data types characters, integers, floating point numbers. chapter begins investigation one three key elements computer system: processor. chapter introduces general concept, describes variety processors, discusses relationship clock rate processing rate. next chapters extend basic description explaining instruction sets, addressing modes, functions general-purpose CPU. 4.2 Two Basic Architectural Approaches Early history computers, architects experimenting new designs considered organize hardware. Two basic approaches emerged named groups proposed them: Harvard Architecture Von Neumann Architecture see two share ideas, differ programs data stored accessed. 4.3 Harvard Von Neumann Architectures term Harvard Architecture† refers computer organization four principal components: processor, instruction memory, data memory, I/O facilities, organized Figure 4.1 illustrates.Figure 4.1 Illustration Harvard Architecture uses two memories, one hold programs another store data. Although includes basic components, Von Neumann Architecture‡ uses single memory hold programs data. Figure 4.2 illustrates approach. Figure 4.2 Illustration Von Neumann Architecture. programs data stored memory. chief advantage Harvard Architecture arises ability one memory unit optimized store programs another memory unit optimized store data. chief disadvantage arises inflexibility: purchasing computer, owner must choose size instruction memory size data memory. computer purchased, owner cannot use part instruction memory store data usepart data memory store programs. Although fallen favor general-purpose computers, Harvard Architecture still sometimes used small embedded systems specialized designs. Unlike Harvard Architecture, Von Neumann Architecture offers complete flexibility: time, owner change much memory devoted programs much data. approach proven valuable become widely adopted: offers flexibility, Von Neumann Architecture, uses single memory hold programs data, become pervasive: almost computers follow Von Neumann approach. say computer follows Von Neumann Architecture employs stored program approach program stored memory. More, important programs loaded memory like data items. Except noted, remainder text implicitly assumes Von Neumann Architecture. two primary exceptions Chapters 6 12. Chapter 6, explains data paths, uses simplified Harvard Architecture example. Chapter 12, explains caching, discusses motivation using separate instruction data caches. 4.4 Definition Processor remainder chapter considers processor component present Harvard Von Neumann Architectures. next sections define term characterize processor types. Later sections explore subcomponents complex processors. Although programmers tend think conventional computer often use term processor synonym Central Processing Unit (CPU), computer architects much broader meaning includes processors used control engine automobile, processors hand-held remote control devices, specialized video processors used graphics equipment. architect, processor refers digital device perform computation involving multiple steps. Individual processors complete computers; merely one building blocks architect uses construct computer system. Thus, although compute combinatorial Boolean logic circuits examined Chapter 2, processor need large fast. particular, processors significantly less powerful general-purpose CPU found typical PC. next sections help clarify definition examining characteristics processors explaining ways used. 4.5 Range ProcessorsBecause processors span broad range functionality many variations exist, single description adequately captures properties processors. Instead, help us appreciate many designs, need divide processors categories according functionality intended use. example, use four categories explain whether processor adapted new computations. categories listed order flexibility: Fixed logic Selectable logic Parameterized logic Programmable logic fixed logic processor, least flexible, performs single task. important, functionality needed perform operation built processor created, functionality cannot altered without changing underlying hardware†. example, fixed logic processor designed compute function, sine(x), perform graphics operation needed video game. selectable logic processor slightly flexibility fixed logic processor. essence, selectable logic processor contains facilities needed perform one function; exact function specified processor invoked. example, selectable logic processor might designed compute either sine(x) cosine(x). parameterized logic processor adds additional flexibility. Although computes predetermined function, processor accepts set parameters control computation. example, consider parameterized processor computes hash function, h(x). hash function uses two constants, p q, computes hash x computing remainder x multiplied p divided q. example, p 167 q 163, h(26729) remainder 4463743 divided 163, 151‡. parameterized processor hash function allows constants p q changed time processor invoked. is, addition input, x, processor accepts additional parameters, p q, control operation. programmable logic processor offers flexibility allows sequence steps changed time processor invoked — processor given program run, typically placing program memory. 4.6 Hierarchical Structure Computational Engines large processor, modern, general-purpose CPU, complex human understand entire processor single unit. control complexity, computer architects use hierarchical approach subparts processor designed tested independently combined final design. independent subparts large processor sophisticated fit definition processor — subpart perform computation involves multiple steps. example, general-purpose CPU instructions sine cosine might beconstructed first building testing trigonometry processor, combining trigonometry processor pieces form final CPU. describe subpiece large, complex processor acts independently performs computation? engineers use term computational engine. term engine usually implies subpiece fills specific role less powerful overall unit. example, Figure 4.3 illustrates CPU contains several engines. Figure 4.3 example CPU includes multiple components. large arrow center figure indicates central interconnect mechanism components use coordinate. CPU figure includes special-purpose graphics engine. Graphics engines, sometimes called graphics accelerators, common video game software popular many computers need graphics engine drive graphics display high speed. example, graphics engine might include facilities repaint surface graphical figure moved (e.g., response joystick movement). CPU illustrated Figure 4.3 also includes query engine. Query engines closely related pattern engines used database processors. query engine examines database record high speed determine record satisfies query; pattern engine examines string bits determine string matches specified pattern (e.g., test whether document contains particular word). either case, CPU enough capability handle task, special-purpose processor perform task much faster. 4.7 Structure Conventional ProcessorAlthough imaginary CPU described previous section contains many engines, processors not. Two questions arise. First, engine(s) found conventional processor? Second, engines interconnected? section answers questions broadly, later sections give detail. Although practical processor contains many subcomponents complex interconnections among them, view processor five conceptual units: Controller Arithmetic Logic Unit (ALU) Local data storage (typically, registers) Internal interconnection(s) External interface(s) (I/O buses) Figure 4.4 illustrates concept. Controller. controller forms heart processor. Controller hardware overall responsibility program execution. is, controller steps program coordinates actions hardware units perform specified operations. Arithmetic Logic Unit (ALU). think ALU main computational engine processor. ALU performs computational tasks, including integer arithmetic, operations bits (e.g., left right shift), Boolean (logical) operations (e.g., Boolean and, or, exclusive or, not). However, ALU perform multiple steps initiate activities. Instead, ALU performs one operation time, relies controller specify exactly operation perform operand values. Local Data Storage. processor must least local storage hold data values operands arithmetic operations result. see, local storage usually takes form hardware registers — values must loaded hardware registers used computation.Figure 4.4 five major units found conventional processor. external interface connects rest computer system. Internal Interconnection(s). processor contains one hardware mechanisms used transfer values hardware units. example, interconnection hardware used move data values local storage ALU move results ALU local storage. Architects sometimes use term data path describe internal interconnection. External Interface(s). external interface unit handles communication processor rest computer system. particular, external interface manages communication processor external memory I/O devices. 4.8 Processor Categories Roles Understanding range processors especially difficult someone encountered hardware design processors used variety roles. may help consider ways hardware devices use processors processors function role. four examples: Coprocessors Microcontrollers Embedded system processors General-purpose processorsCoprocessors. coprocessor operates conjunction control another processor. Usually, coprocessor consists special-purpose processor performs single task high speed. example, CPUs use coprocessor known floating point accelerator speed execution arithmetic operations — floating point operation occurs, CPU automatically passes necessary values coprocessor, obtains result, continues execution. architectures running program know operations performed directly CPU operations performed coprocessor, say operation coprocessor transparent software. Typical coprocessors use fixed selectable logic, means functions coprocessor perform determined coprocessor designed. Microcontrollers. microcontroller consists programmable device dedicated control physical system. example, microcontrollers run physical systems engine modern automobile, landing gear airplane, automatic door grocery store. many cases, microcontroller performs trivial function require much traditional computation. Instead, microcontroller tests sensors sends signals control devices. Figure 4.5 lists example steps typical microcontroller programmed perform: Figure 4.5 Example steps microcontroller performs. cases, microcontrollers dedicated trivial control tasks. Embedded System Processors. embedded system processor runs sophisticated electronic devices wireless router smart phone. processors used embedded systems usually powerful processors used microcontrollers, often run protocol stack used communication. However, processor may contain functionality found general-purpose CPUs. General-purpose Processors. General-purpose processors familiar need little explanation. example, CPU PC general-purpose processor. 4.9 Processor TechnologiesHow processors created? 1960s, processors created digital logic circuits. Individual gates connected together circuit board, plugged chassis form working computer. 1970s, large-scale integrated circuit technology arrived, meant smallest least powerful processors — used microcontrollers — could implemented single integrated circuit. integrated circuit technology improved number transistors chip increased, single chip became capable holding powerful processors. Today, many powerful general- purpose processors consist single integrated circuit. 4.10 Stored Programs said processor performs computation involves multiple steps. Although processors series steps built hardware, not. Instead, programmable (i.e., rely mechanism known programming). is, sequence steps performed comprise program placed location processor access; processor accesses program follows specified steps. Computer programmers familiar conventional computer systems use main memory location holds program. program loaded memory time user runs application. chief advantage using main memory hold programs lies ability change program. next time user runs program changed, altered version used. Although conventional notion programming works well general-purpose processors, types processors use alternative mechanisms easy change. example, program microcontroller usually resides hardware known Read Memory (ROM†). fact, ROM contains program may reside integrated circuit along microcontroller runs program. example, microcontroller used automobile may reside single integrated circuit also contains program microcontroller runs. important point programming broad notion: computer architect, processor classified programmable if, level detail, processor separate program runs. user, may appear program processor integrated, may possible change program without replacing processor. 4.11 Fetch-Execute Cycle programmable processor access perform steps program? data path description Chapter 6 explains basic idea. Although details vary among processors, allprogrammable processors follow fundamental paradigm. underlying mechanism known fetch-execute cycle. implement fetch-execute, processor instruction pointer automatically moves program memory, performing step. is, programmable processor executes two basic functions repeatedly. Algorithm 4.1 presents two fundamental steps†. Algorithm 4.1 Repeat forever { Fetch: access next step program location program stored. Execute: perform step program. } Algorithm 4.1 Fundamental Steps Fetch-Execute Cycle important point is: level, every programmable processor implements fetch-execute cycle. Several questions arise. Exactly program represented memory, representation created? processor identify next step program? possible operations performed execution phase fetch-execute cycle? processor perform operation? next chapters answer questions detail. remainder chapter concentrates three questions: fast processor operate, processor begin first step program, happens processor reaches end program? 4.12 Program Translation important question programmers concerns program converted form processor expects. programmer uses High Level Language (HLL) create computer program. say programmer writes source code. programmer uses tool translate source code representation processor expects. Although programmer invokes single tool, gcc, multiple steps required perform translation. First, preprocessor expands macros, producing modified source program. modified source program becomes input compiler, translates theprogram assembly language. Although closer form needed processor, assembly language read humans. assembler translates assembly language program relocatable object program contains combination binary code references external library functions. linker processes relocatable object program replacing external function references code functions. so, linker extracts name function, searches one libraries find binary code function. Figure 4.6 illustrates translation steps software tool performs step. Figure 4.6 steps used translate source program binary object code representation used processor. 4.13 Clock Rate Instruction Rate One primary questions processors concerns speed: fast fetch- execute cycle operate? answer depends processor, technology used store program, time required execute instruction. one hand, processor used microcontroller actuate physical device (e.g., electric door) relatively slow response time one-tenth second seems fast human. hand, processor used highest-speed computers must fast possible goal maximum performance. saw Chapter 2, processors use clock control rate underlying digital logic operates. Anyone purchased computer knows sales personnel push customers purchase fast clock argument higher clock rate result higher performance. Although higher clock rate usually means higher processing speed, important realize clock rate give rate fetch-execute cycleproceeds. particular, systems, time required execute portion cycle depends instruction executed. see later operations involving memory access I/O require significantly time (i.e., clock cycles) not. time also varies among basic arithmetic operations: integer multiplication division requires time integer addition subtraction. Floating point computation especially costly floating point operations usually require clock cycles equivalent integer operations. Floating point multiplication division stands especially costly — single floating point division require orders magnitude clock cycles integer addition. now, sufficient remember general principle: fetch-execute cycle may proceed fixed rate time taken execute instruction depends operation performed. operation multiplication requires time operation addition. 4.14 Control: Getting Started Stopping far, discussed processor running fetch-execute cycle without giving details. need answer two basic questions. processor start running fetch- execute cycle? happens processor executes last step program? issue program termination easiest understand: processor hardware designed stop. Instead, fetch-execute cycle continues indefinitely. course, processor permanently halted, sequence used power computer — normal operations, processor continues execute one instruction another. cases, program uses loop delay. example, microcontroller may need wait sensor indicate external condition met proceeding. processor merely stop wait sensor. Instead, program contains loop repeatedly tests sensor. Thus, hardware point view, fetch-execute cycle continues. notion indefinite fetch-execute cycle direct consequence programming: software must planned processor always next step execute. case dedicated system microcontroller controls physical device, program consists infinite loop — finishes last step program, processor starts first step. case general-purpose computer, operating system always present. operating system load application memory, direct processor run application. keep fetch-execute cycle running, operating system must arrange regain control application finishes. application running, operating system enters loop wait input (e.g., touch screen, keyboard, mouse). summarize:Because processor runs fetch-execute cycle indefinitely, system must designed ensure always next step execute. dedicated system, program executes repeatedly; general-purpose system, operating system runs application running. 4.15 Starting Fetch-Execute Cycle processor start fetch-execute cycle? answer complex depends underlying hardware. example, processors hardware reset. processors, engineers arrange combinatorial circuit apply voltage reset line system components ready operate. voltage removed reset line, processor begins executing program fixed location. processors start executing program found location zero memory processor reset. systems, designer must guarantee valid program placed location zero processor starts. steps used start processor known bootstrap. embedded environment, program run usually resides Read Memory (ROM). conventional computer, hardware reads copy operating system I/O device, disk, places copy memory starting processor. either case, hardware assist needed bootstrap signal must passed processor causes fetch-execute cycle begin. Many devices soft power switch, means power switch actually turn power off. Instead, switch acts like sensor — processor interrogate switch determine current position. Booting device softswitch different booting devices. power first applied (e.g., battery installed), processor boots initial state. initial state consists loop interrogates soft power switch. user presses soft power switch, hardware completes bootstrap process. 4.16 Summary processor digital device perform computation involving multiple steps. Processors use fixed, selectable, parameterized programmable logic. term engine identifies processor subpiece complex processor. Processors used various roles, including coprocessors, microcontrollers, embedded processors, general-purpose processors. Although early processors created discrete logic, modern processor implemented single VLSI chip. processor classified programmable level, processor hardware separate sequence steps processor performs; point view enduser, however, might possible change program without replacing processor. programmable processors follow fetch-execute cycle; time required one cycle depends operation performed. fetch-execute processing continues indefinitely, designer must construct program way processor always instruction execute. set software programs used translate source program, written programmer, binary representation processor requires. set includes preprocessor, compiler, assembler, linker. EXERCISES 4.1 Neither Figure 4.1 Figure 4.2 storage major component. storage (e.g., flash electro-mechanical disk) fit figures? 4.2 Consider System-on-Chip (SoC) approach described Chapter 2. Besides processor, memory, I/O facilities, SoC need? 4.3 Consult Wikipedia learn early computers. much memory Harvard Mark computer have, year created? much memory IBM 360/20 computer have, year created? 4.4 Although CPU manufacturers brag graphics accelerators chips, video game designers choose keep graphics hardware separate processor. Explain one possible motivation keeping separate. 4.5 Imagine smart phone employs Harvard Architecture. purchase phone, would need specify normally specify? 4.6 aspect Von Neumann Architecture makes vulnerable hackers Harvard Architecture? 4.7 access gcc, read man page learn command line argument allows run preprocessor place preprocessed program file viewed. changes made source program? 4.8 Extend previous exercise placing assembly language output compiler file viewed. 4.9 Write computer program compares difference execution times integer division floating point division. test program, execute operation 100,000 times, compare difference running times. †The name arises approach first used Harvard Mark relay computer. ‡The name taken John Von Neumann, mathematician first proposed architecture. †Engineers use term hardwired functionality cannot changed without altering underlying wiring. ‡Hashing often applied strings. example, number 26729 decimal value two characters string “hi” treated unsigned short integer. †Later chapters describe memory detail. †Note algorithm presented simplified form; discuss I/O, see algorithm extended handle device interrupts.5 Processor Types Instruction Sets Chapter Contents 5.1 Introduction 5.2 Mathematical Power, Convenience, Cost 5.3 Instruction Set Architecture 5.4 Opcodes, Operands, Results 5.5 Typical Instruction Format 5.6 Variable-Length Vs. Fixed-Length Instructions 5.7 General-Purpose Registers 5.8 Floating Point Registers Register Identification 5.9 Programming Registers 5.10 Register Banks 5.11 Complex Reduced Instruction Sets 5.12 RISC Design Execution Pipeline 5.13 Pipelines Instruction Stalls 5.14 Causes Pipeline Stalls 5.15 Consequences Programmers 5.16 Programming, Stalls, No-Op Instructions 5.17 Forwarding 5.18 Types Operations 5.19 Program Counter, Fetch-Execute, Branching 5.20 Subroutine Calls, Arguments, Register Windows5.21 Example Instruction Set 5.22 Minimalistic Instruction Set 5.23 Principle Orthogonality 5.24 Condition Codes Conditional Branching 5.25 Summary 5.1 Introduction previous chapter introduces variety processors explains fetch-execute cycle programmable processors use. chapter continues discussion focusing set operations processor perform. chapter explains various approaches computer architects chosen, discusses advantages disadvantages each. next chapters extend discussion describing various ways processors access operands. 5.2 Mathematical Power, Convenience, Cost operations processor offer? mathematical point view, wide variety computational models provide equivalent computing power. theory, long processor offers basic operations, processor sufficient power compute computable function†. Programmers understand although minimum set operations necessary, minimum neither convenient practical. is, set operations designed convenience rather mere functionality. example, possible compute quotient repeated subtraction. However, program uses repeated subtraction compute quotient runs slowly. Thus, processors operations include hardware basic arithmetic operation: addition, subtraction, multiplication, division. computer architect, choosing set operations processor perform represents tradeoff. one hand, adding additional arithmetic operation, multiplication division, provides convenience programmer. hand, additional operation adds hardware makes processor design difficult. Adding hardware also increases engineering considerations chip size, power consumption, heat dissipation. Thus, smart phone designed conserve battery power, processors used smart phones typically fewer built-in operations processors used powerful mainframe computers.The point considering set operations given processor provides, need remember choice represents complex tradeoff: set operations processor provides represents tradeoff among cost hardware, convenience programmer, engineering considerations power consumption. 5.3 Instruction Set Architecture architect designs programmable processor, architect must make two key decisions: Instruction set: set operations processor provides Instruction representation: format operation use term instruction set refer set operations hardware recognizes, refer operation instruction. assume iteration fetch-execute cycle, processor executes one instruction. definition instruction set specifies details instructions, including exact specification actions processor takes executes instruction. Thus, instruction set defines values instruction operates results instruction produces. definition specifies allowable values (e.g., division instruction requires divisor nonzero) error conditions (e.g., happens addition results overflow). term instruction representation (instruction format) refers binary representation hardware uses instructions. instruction representation important defines key interface: interface software generates instructions places memory hardware executes instructions. software (e.g., compiler, linker, loader) must create image memory uses exactly instruction format processor hardware expects. say definition instruction set corresponding representation define Instruction Set Architecture (ISA). is, ISA defines syntactic semantic aspects instruction set. IBM Corporation pioneered approach 1960s developed ISA System/360 line computers — minor exceptions, computers line shared basic instruction set, individual models differed widely (approximately 1:30 ratio) size memory, processor speed, cost. 5.4 Opcodes, Operands, ResultsConceptually, instruction contains three parts specify: exact operation performed, value(s) use, place result(s). following paragraphs define idea precisely. Opcode. term opcode (short operation code) refers exact operation performed. opcode number; instruction set designed, operation must assigned unique opcode. example, integer addition might assigned opcode five, integer subtraction might assigned opcode twelve. Operands. term operand refers value needed perform operation. definition instruction set specifies exact number operands instruction, possible values (e.g., addition operation takes two signed integers). Results. architectures, one operands specify processor place results instruction (e.g., result arithmetic operation); others, location result determined automatically. 5.5 Typical Instruction Format instruction represented binary string. processors, instruction begins field contains opcode, followed fields contain operands. Figure 5.1 illustrates general format. Figure 5.1 general instruction format many processors use. opcode beginning instruction determines exactly operands follow. 5.6 Variable-Length Vs. Fixed-Length Instructions question arises: instruction size (i.e., occupy number bytes) length depend quantity type operands? example, consider integer arithmetic operations. Addition subtraction operates two values, negation operates single value. Furthermore, processor handle multiple sizes operands (e.g., processor instruction adds pair sixteen-bit integers well instruction adds pair thirty-two bit integers). one instruction shorter another? use term variable-length characterize instruction set includes multiple instruction sizes, term fixed-length characterize instruction set every instruction size. Programmers expect variable-length instructions software usually allocates space according size object (e.g., strings “Hello” “bye” appear program, compiler allocate 5 3 bytes, respectively). hardwarepoint view, however, variable-length instructions require complex hardware fetch decode. comparison, fixed-length instructions require less complex hardware. Fixed-length instructions allow processor hardware operate higher speed hardware compute location next instruction easily. Thus, many processors force instructions size, even instructions represented fewer bits others. point is: Although may seem inefficient programmer, using fixed-length instructions make processor hardware less complex faster. processor uses fixed-length instructions handle cases instruction need operands? example, fixed-length instruction set accommodate addition negation? Interestingly, hardware designed ignore fields needed given operation. Thus, instruction set may specify instructions, specific bits unused†. summarize: fixed-length instruction set employed, instructions contain extra fields hardware ignores. unused fields viewed part hardware optimization, indication poor design. 5.7 General-Purpose Registers seen, register small, high-speed hardware storage device found processor. register fixed size (e.g., 32 64 bits) supports two basic operations: fetch store. see later registers operate variety roles, including instruction pointer (also called program counter) gives address next instruction execute. now, restrict attention simple case well known programmers: general-purpose registers used temporary storage mechanism. processor usually small number general-purpose registers (e.g., thirty-two), register usually size integer. example, processor provides thirty-two bit arithmetic, general-purpose register holds thirty-two bits. result, general-purpose register hold operand needed arithmetic instruction result instruction. many architectures, general-purpose registers numbered 0 N–1. processor provides instructions store value (or fetch value from) specified register. General-purpose registers semantics memory: fetch operation returns value specified previous store operation. Similarly, store operation replaces contents register new value.5.8 Floating Point Registers Register Identification Processors support floating point arithmetic often use separate set registers hold floating point values. Confusion arise general-purpose registers floating point registers usually numbered starting zero — instruction determines registers used. example, registers 3 6 specified operands integer instruction, processor extract operands general-purpose registers. However, registers 3 6 specified operands floating point instruction, floating point registers used. 5.9 Programming Registers Many processors require operands placed general-purpose registers instruction executed. processors also place results instruction general- purpose register. Thus, add two integers variables X place result variable Z, programmer must create series instructions move values corresponding registers. example, general-purpose registers 3, 6, 7 available, program might contain four instructions perform following steps: Load copy variable X memory register 3 Load copy variable memory register 6 Add value register 3 value register 6, place result register 7 Store copy value register 7 variable Z memory see moving value memory register relatively expensive, performance optimized leaving values registers value used again. processor contains small number registers, programmer (or compiler) must decide values keep registers time; values kept memory†. process choosing values registers contain known register allocation. Many details complicate register allocation. One common arises instruction generates large result, called extended value. example, integer multiplication produce result contains twice many bits either operand. processors offer facilities double precision arithmetic (e.g., standard integer thirty-two bits wide, double precision integer occupies sixty-four bits). handle extended values, hardware treats registers consecutive. processors, example, instruction loads double precision integer register 4 place half integer register 4 half register 5 (i.e., value register 5 change even though instruction contains explicit reference). choosing registers use, programmer must plan instructions place extended data values consecutive registers.5.10 Register Banks additional hardware detail complicates register allocation: architectures divide registers multiple banks, require operands instruction come separate banks. example, processor uses two register banks, integer add instruction may require two operands separate banks. understand register banks, must examine underlying hardware. essence, register banks allow hardware operate faster bank separate physical access mechanism mechanisms operate simultaneously. Thus, processor executes instruction accesses two operands registers, operands obtained time. Figure 5.2 illustrates concept. Figure 5.2 Illustration eight registers divided two banks. Hardware allows processor access banks time. Register banks interesting consequence programmers: may possible keep data values registers permanently. understand why, consider following assignment statements typical used conventional programming language, assume want implement statements processor two register banks Figure 5.2 illustrates. perform first addition, X must separate register banks. Let’s assume X register bank A, register bank B. subtraction, Z must theopposite register bank X (i.e., Z must register bank B). third assignment, Z must different banks. Unfortunately, first two assignments mean Z located bank. Thus, possible assignment X, Y, Z registers works three instructions. say register conflict occurs. happens register conflict arises? programmer must either reassign registers insert instruction copy values. example, could insert extra instruction copies value Z register bank final addition performed. 5.11 Complex Reduced Instruction Sets Computer architects divide instruction sets two broad categories used classify processors†: Complex Instruction Set Computer (CISC) Reduced Instruction Set Computer (RISC) CISC processor usually includes many instructions (typically hundreds), instruction perform arbitrarily complex computation. Intel’s x86 instruction set classified CISC processor provides hundreds instructions, including complex instructions require long time execute (e.g., one instruction manipulates graphics memory others compute sine cosine functions). contrast CISC, RISC processor constrained. Instead arbitrary instructions, RISC design strives minimum set sufficient computation (e.g., thirty-two instructions). Instead allowing single instruction compute arbitrary function, instruction performs basic computation. achieve highest possible speed, RISC designs constrain instructions fixed size. Finally, next section explains, RISC processor designed execute instruction one clock cycle‡. Arm Limited MIPS Corporation created RISC architectures limited instructions executed one clock cycle. ARM designs especially popular smart phones low-power devices. summarize: processor classified CISC instruction set contains instructions perform complex computations require long times; processor classified RISC contains small number instructions execute one clock cycle. 5.12 RISC Design Execution PipelineWe said RISC processor executes one instruction per clock cycle. fact, accurate version statement is: RISC processor designed processor complete one instruction clock cycle. understand subtle difference, important know hardware works. said processor performs fetch-execute cycle first fetching instruction executing instruction. fact, processor divides fetch- execute cycle several steps, typically: Fetch next instruction Decode instruction fetch operands registers Perform arithmetic operation specified opcode Perform memory read write, needed Store result back registers enable high speed, RISC processors contain parallel hardware units perform one step listed above. hardware arranged multistage pipeline†, means results one hardware unit passed next hardware unit. Figure 5.3 illustrates pipeline. Figure 5.3 example pipeline five hardware stages used perform fetch-execute cycle. figure, instruction moves left right pipeline. first stage fetches instruction, next stage examines opcode, on. Whenever clock ticks, stages simultaneously pass instruction right. Thus, instructions move pipeline like assembly line: time, pipeline contains five instructions. speed pipeline arises stages operate parallel — fourth stage executes instruction, third stage fetches operands next instruction. Thus, stage never needs delay instruction always ready clock cycle. Figure 5.4 illustrates set instructions pass five-stage pipeline.Figure 5.4 Instructions passing five-stage pipeline. pipeline filled, stage busy clock cycle. figure clearly illustrates although RISC processor cannot perform steps needed fetch execute instruction one clock cycle, parallel hardware allows processor finish one instruction per clock cycle. summarize: Although RISC processor cannot perform steps fetch-execute cycle single clock cycle, instruction pipeline parallel hardware provides approximately performance: pipeline full, one instruction completes every clock cycle. 5.13 Pipelines Instruction Stalls say instruction pipeline transparent programmers instruction set contain explicit references pipeline. is, hardware constructed results program whether pipeline present. Although transparency advantage, also disadvantage: programmer understand pipeline inadvertently introduce inefficiencies. understand effect programming choices pipeline, consider program contains two successive instructions perform addition subtraction operands results located registers label A, B, C, D, E: Although instruction K proceed pipeline beginning end, instruction K+1 encounters problem operand C available time. is, hardware mustwait instruction K finish fetching operands instruction K+1. say stage pipeline stalls wait operand become available. Figure 5.5 illustrates happens pipeline stall. Figure 5.5 Illustration pipeline stall. Instruction K+1 cannot proceed operand instruction K becomes available. figure shows normal pipeline running clock cycle 3, Instruction K+1 reached stage 2. Recall stage 2 fetches operands registers. example, one operands instruction K+1 available, available instruction K writes results register. pipeline must stall instruction K completes. code above, value C computed, stage 2 cannot fetch value C. Thus, stages 1 2 remain stalled clock cycles 4 5. clock cycle 6, stage 2 fetch operand, pipeline processing continues. rightmost column Figure 5.5 shows effect stall performance: final stage pipeline produce results clock cycles 6, 7, 8. stalls occurred, instruction K+1 would completed clock cycle 6, stall means instruction complete clock cycle 9. describe delay cause stall time output stops, say bubble passes pipeline. course, bubble apparent someone observing pipeline’s performance correctness affected. is, instruction always passes directly next stage soon one stage completes, means instructions executed order specified. 5.14 Causes Pipeline StallsIn addition waiting operands, pipeline stall processor executes instruction delays processing disrupts normal flow. example, stall occur processor: Accesses external storage Invokes coprocessor Branches new location Calls subroutine sophisticated processors contain additional hardware avoid stalls. example, processors contain two copies pipeline, allows processor start decoding instruction executed branch taken well instruction executed branch taken. two copies operate branch instruction executed. time, hardware knows copy pipeline follow; copy ignored. processors contain special shortcut hardware passes copy result back previous pipeline stage. 5.15 Consequences Programmers achieve maximum speed, program RISC architecture must written accommodate instruction pipeline. example, programmer avoid introducing unnecessary branch instructions. Similarly, instead referencing result register immediately following instruction, reference delayed. example, Figure 5.6 shows code rearranged run faster. Figure 5.6 (a) list instructions, (b) instructions reordered run faster pipeline. Reducing pipeline stalls increases speed. figure, optimized program separates references computation. example, original program, second instruction references value C, produced previous instruction. Thus, stall occurs first second instructions. Moving subtraction later point program allows processor continue operate without stall.Of course, programmer choose view pipeline automatic optimization instead programming burden. Fortunately, programmers need perform pipeline optimizations manually. Instead, compilers high-level languages perform optimizations automatically. Rearranging code sequences increase speed processing hardware uses instruction pipeline; programmers view reordering optimization increase speed without affecting correctness. 5.16 Programming, Stalls, No-Op Instructions cases, instructions program cannot rearranged prevent stall. cases, programmers document stalls anyone reading code understand stall occurs. documentation especially helpful program modified programmer performs modification reconsider situation attempt reorder instructions prevent stall. programmers document stall? One technique obvious: insert comment explains reason stall. However, code generated compilers read humans problems occur special optimization required. cases, another technique used: places stalls occur, insert extra instructions code. extra instructions show items inserted without affecting pipeline. course, extra instructions must innocuous — must change values registers otherwise affect program. cases, hardware provides solution: no-op. is, instruction absolutely nothing except occupy time. point is: processors include no-op instruction reference data values, compute result, otherwise affect state computer. No-op instructions inserted document locations instruction stall occurs. 5.17 Forwarding mentioned above, hardware special facilities improve instruction pipeline performance. example, ALU use technique known forwarding solve problem successive arithmetic instructions passing results. understand forwarding works, consider example two instructions operands A, B, C, D, E registers:We said sequence causes stall pipelined processor. However, processor implements forwarding avoid stall arranging hardware detect dependency automatically pass value C instruction K directly instruction K+1. is, copy output ALU instruction K forwarded directly input ALU instruction K+1. result, instructions continue fill pipeline, stall occurs. 5.18 Types Operations computer architects discuss instruction sets, divide instructions basic categories. Figure 5.7 lists one possible division. Figure 5.7 example categories used classify instructions. general-purpose processor includes instructions categories. 5.19 Program Counter, Fetch-Execute, Branching Recall Chapter 4 every processor implements basic fetch-execute cycle. cycle, control hardware processor automatically moves instructions — finishes executing one instruction, processor automatically moves past current instruction memory fetching next instruction. implement fetch-execute cycle move next instruction, processor uses special-purpose internal register known instruction pointer program counter†. fetch-execute cycle begins, program counter contains address instruction executed. instruction fetched, program counter updated theaddress next instruction. update program counter fetch-execute cycle means processor automatically move successive instructions memory. Algorithm 5.1 specifies fetch-execute cycle moves successive instructions. Algorithm 5.1 Assign program counter initial program address. Repeat forever { Fetch: access next step program location given program counter. Set internal address register, A, address beyond instruction fetched. Execute: Perform step program. Copy contents address register program counter. } Algorithm 5.1 Fetch-Execute Cycle algorithm allows us understand branch instructions work. two cases: absolute relative. absolute branch computes memory address, address specifies location next instruction execute. Typically, absolute branch instruction known jump. execute step, jump instruction computes address loads value internal register Algorithm 5.1 specifies. end fetch-execute cycle, hardware copies value program counter, means address used fetch next instruction. example, absolute branch instruction: jump 0x05DE causes processor load 0x05DE internal address register, copied program counter next instruction fetched. words, next instruction fetch occur memory location 0x05DE. Unlike absolute branch instruction, relative branch instruction specify exact memory address. Instead, relative branch computes positive negative increment program counter. example, instruction: br +8 specifies branching location eight bytes beyond current location (i.e., beyond current value program counter).To implement relative branching, processor adds operand branch instruction program counter, places result internal address register A. example, relative branch computes -12, next instruction executed found address twelve bytes current instruction. compiler might use relative branch end short while- loop. processors also provide instruction invoke subroutine, typically jsr (jump subroutine). terms fetch-execute cycle, jsr instruction operates like branch instruction key difference: branch occurs, jsr instruction saves value address register, A. finishes executing, subroutine returns caller. so, subroutine executes absolute branch saved address. Thus, subroutine finishes, fetch- execute cycle resumes instruction immediately following jsr. 5.20 Subroutine Calls, Arguments, Register Windows High-level languages use subroutine call instruction, jsr, implement procedure function call. calling program supplies set arguments subroutine uses computation. example, function call cos(3.14159) floating point constant 3.14159 argument. One principal differences among processors arises way underlying hardware passes arguments subroutine. architectures use memory — arguments stored stack memory call, subroutine extracts values stack referenced. architectures, processor uses either general-purpose special-purpose registers pass arguments. Using either special-purpose general-purpose registers pass arguments much faster using stack memory registers part local storage processor itself. processors provide special-purpose registers argument passing, general-purpose registers typically used. Unfortunately, general-purpose registers cannot devoted exclusively arguments also needed computation (e.g., hold operands arithmetic operations). Thus, programmer faces tradeoff: using general- purpose register pass argument increase speed subroutine call, using register hold data value increase speed general computation. Thus, programmer must choose arguments keep registers store memory†. processors include optimization argument passing known register window. Although processor large set general-purpose registers, register hardware exposes subset registers time. subset known window. window moves automatically time subroutine invoked, moves back subroutine returns. important, windows available program subroutine overlap — registers visible caller visible subroutine. caller places arguments registers overlap calling subroutine subroutine extracts values registers. Figure 5.8 illustrates concept register windowFigure 5.8 Illustration register window (a) subroutine call, (b) call. Values A, B, C, correspond arguments passed. figure, hardware 16 registers, 8 registers visible time; others unavailable. program always references visible registers numbers 0 window size minus one (0 7 example). subroutine called, hardware changes set registers visible sliding window. example, registers numbered 4 7 call become 0 3 call. Thus, calling program places arguments registers 4 7, subroutine finds arguments registers 0 3. Registers values xi available calling program. advantage register window approach registers current window retain values had. So, called subroutine returns, window slide back, registers values xi exactly call. illustration Figure 5.8 uses small window size (eight registers) simplify diagram. practice, processors use register window typically larger windows. example, Sparc architecture one hundred twenty-eight one hundred forty-four physical registers window size thirty-two registers; however, eight registers window overlap (i.e., eight registers used pass arguments). 5.21 Example Instruction Set example instruction set help clarify concepts described above. selected MIPS processor example two reasons. First, MIPS processor popular use embedded systems. Second, MIPS instruction set classic example instruction set offered RISC processor. Figure 5.9 lists instructions MIPS instruction set.A MIPS processor contains thirty-two general-purpose registers, instructions require operands results registers. example, add instruction takes three operands registers: instruction adds contents first two registers places result third. addition integer instructions listed Figure 5.9, MIPS architecture defines set floating point instructions single precision (i.e., thirty-two bit) double precision (i.e., sixty-four bit) floating point values. hardware provides set thirty- two floating point registers. Although numbered zero thirty-one, floating point registers completely independent general-purpose registers. handle double precision values, floating point registers operate pairs. is, even-numbered floating point register specified operand target floating point instruction — hardware uses specified register plus next odd-numbered register combined storage unit hold double precision value. Figure 5.10 summarizes MIPS floating point instruction set.Figure 5.9 example instruction set. table lists instructions offered MIPS processor.Figure 5.10 Floating point (FP) instructions defined MIPS architecture. Double precision values occupy two consecutive floating point registers. 5.22 Minimalistic Instruction Set may seem instructions listed Figure 5.9 insufficient additional instructions needed. example, MIPS architecture include instruction copies contents register another register, architecture include instructions add value memory contents register. understand choices, important know MIPS instruction set supports two principles: speed minimalism. First, basic instruction set designed carefully ensure high speed (i.e., architecture property pipeline used, one instruction complete every clock cycle). Second, instruction set minimalistic — contains fewest possible instructions handle standard computation. Limiting number instructions forms key piece design. Choosing thirty-two instructions means opcode needs five bits combination bits wasted.One feature MIPS architecture, also used RISC processors, helps achieve minimalism: fast access zero value. case MIPS, register 0 provides mechanism — register reserved always contains value zero. Thus, test whether register zero, value compared register zero. Similarly, register zero used instruction. example, copy value one register another, add instruction used one two operands register zero. 5.23 Principle Orthogonality addition technical aspects instruction sets discussed above, architect must consider aesthetic aspects design. particular, architect strives elegance. Elegance relates human perception: instruction set appear programmer? instructions combine handle common programming tasks? instructions balanced (if set includes right-shift, also include left-shift)? Elegance calls subjective judgment. However, experience instruction sets often helps engineers programmers recognize appreciate elegance. One particular aspect elegance, known orthogonality, concentrates eliminating unnecessary duplication overlap among instructions. say instruction set orthogonal instruction performs unique task. orthogonal instruction set important advantages programmers: orthogonal instructions understood easily, programmer need choose among multiple instructions perform task. Orthogonality important become general principle processor design. summarize: principle orthogonality specifies instruction perform unique task without duplicating overlapping functionality instructions. 5.24 Condition Codes Conditional Branching many processors, executing instruction results status, processor stores internal hardware mechanism. later instruction use status decide proceed. example, executes arithmetic instruction, ALU sets internal register known condition code contains bits record whether result positive, negative, zero, arithmetic overflow occurred. conditional branch instruction follows arithmetic operation test one condition code bits, use result determine whether branch. example clarify condition code mechanism used†. understand paradigm, consider program tests equality two values. simple example,suppose goal set register 3 zero contents register 4 equal contents register 5. Figure 5.11 contains example code. Figure 5.11 example using condition code. ALU operation sets condition code, later conditional branch instruction tests condition code. 5.25 Summary processor defines instruction set consists operations processor supports; set chosen compromise programmer convenience hardware efficiency. processors, instruction size, processors size varies among instructions. processors include small set general-purpose registers high-speed storage mechanisms. program using registers, one loads values memory registers, performs computation, stores result register memory. optimize performance, programmer leaves values used registers. architectures, registers divided banks, programmer must ensure operands instruction come separate banks. Processors classified two broad categories CISC RISC depending whether include many complex instructions minimal set instructions. RISC architectures use instruction pipeline ensure one instruction complete clock cycle. Programmers optimize performance rearranging code avoid pipeline stalls. implement conditional execution (e.g., if-then-else), many processors rely condition code mechanism — ALU instruction sets condition code, later instruction (a conditional branch) tests condition code. EXERCISES 5.1 debugging program, programmer uses tool allows show contents memory. programmer points tool memory location contains instruction, tool prints three hex values labels: OC=0x43 OP1=0xff00 OP2=0x0324What labels abbreviate? 5.2 arithmetic hardware computer requires operands separate banks, instruction sequence needed compute following? 5.3 Assume designing instruction set computer perform Boolean operations and, or, not, exclusive or. Assign opcodes indicate number operands instruction. instructions stored memory, many bits needed hold opcode? 5.4 computer add, subtract, multiply, divide 16-bit integers, 32-bit integers, 32-bit floating point values, 64-bit floating point values, many unique opcodes needed? (Hint: assume one op code operation data size.) 5.5 computer architect boasted able design computer every instruction occupied exactly thirty-two bits. advantage design? 5.6 Classify ARM architecture owned ARM Limited, SPARC architecture owned Oracle Corporation, Intel Architecture owned Intel Corporation CISC RISC. 5.7 Consider pipeline N stages stage takes time ti. Assuming delay stages, total time (start finish) pipeline spend handling single instruction? 5.8 Insert nop instructions following code eliminate pipeline stalls (assume pipeline illustrated Figure 5.5). loadi r7, 10 # put 10 register 7 loadi r8, 15 # put 15 register 8 loadi r9, 20 # put 20 register 5 addrr r10, r7, r8 # add registers 7 8; put result register 10 movr r12, r9 # copy register 9 register 12 movr r11, r7 # copy register 7 register 11 addri r14, r11, 27 # add 27 plus register 11; put result register 14 addrr r13, r12, r11 # add registers 11 12; put results register 13 †In mathematical sense, three operations needed compute computable function: add one, subtract one, branch value nonzero. †Some hardware requires unused bits zero. †The term register spilling refers moving value register back memory make register available new value. †Instead using full name, engineers use acronyms, pronounced sisk risk. ‡Recall Chapter 2 clock, pulses regular intervals, used control digital logic. †The terms instruction pipeline execution pipeline used interchangeably refer multistage pipeline used fetch-execute cycle. †The two terms equivalent.†Appendix 3 describes calling sequence used x86 architecture, Appendix 4 explains ARM architecture passes arguments registers memory. †Chapter 9 explains programming condition codes shows examples.6 Data Paths Instruction Execution Chapter Contents 6.1 Introduction 6.2 Data Paths 6.3 Example Instruction Set 6.4 Instructions Memory 6.5 Moving Next Instruction 6.6 Fetching Instruction 6.7 Decoding Instruction 6.8 Connections Register Unit 6.9 Control Coordination 6.10 Arithmetic Operations Multiplexing 6.11 Operations Involving Data Memory 6.12 Example Execution Sequences 6.13 Summary6.1 Introduction Chapter 2 introduces digital logic describes basic hardware building blocks used create digital systems. chapter covers basic gates, shows gates constructed transistors. chapter also describes important concept clock, demonstrates clock allows digital circuit perform series operations. Successive chapters describe data represented binary cover processors instruction sets. chapter explains digital logic circuits combined construct computer. chapter reviews functional units, arithmetic-logic units memory, shows units interconnected. Finally, chapter explains units interact perform computation. Later sections text expand discussion examining processors memory systems detail. 6.2 Data Paths topic hardware organized create programmable computer complex. Rather look details large design, architects begin describing major hardware components interconnection. high level, interested instructions read memory instruction executed. Therefore, high-level description ignores many details shows interconnections across data items move instructions executed. example, consider addition operation, see data paths across two operands travel reach Arithmetic Logic Unit (ALU) data path carries result another unit. diagrams show details, power ground connections control connections. Computer architects use terms data paths describe idea data path diagram describe figure depicts data paths. make discussion data paths clear, examine simplified computer. simplifications include: instruction set contains four instructions assume program already loaded memory ignore startup assume processor running assume data item instruction occupies exactly 32 bits consider integer arithmetic completely ignore error conditions, arithmetic overflow Although example computer extremely simple, basic hardware units examine exactly conventional computer. Thus, example sufficient illustrate main hardware components, example interconnection sufficient illustrate data paths designed.6.3 Example Instruction Set previous chapter describes, new computer design must begin design instruction set. details instructions specified, computer architect design hardware performs instructions. illustrate hardware organized, consider imaginary computer following properties: set sixteen general-purpose registers† memory holds instructions (i.e., program) separate memory holds data items register hold thirty-two bit integer value. instruction memory contains sequence instructions executed. described above, ignore startup, assume program already placed instruction memory. data memory holds data values. also assume memories computer byte-addressable, means byte memory assigned address. Figure 6.1 lists four basic instructions imaginary computer implements. Figure 6.1 Four example instructions, operands uses, meaning instruction. add instruction easiest understand — instruction obtains integer values two registers, adds values together, places result third register. example, consider add instruction specifies adding contents registers 2 3 placing result register 4. register 2 contains 50 register 3 contains 60, add instruction place 110 register 4 (i.e., sum integers registers 2 3). assembly language, instruction specified giving instruction name followed operands. example, programmer might code add instruction described previous paragraph writing: add r4, r2, r3 notation rX used specify register X. first operand specifies destination register (where result placed), two specify source registers (where instruction obtains values sum). load store instructions move values data memory registers. Like many commercial processors, imaginary processor requires operands addinstruction registers. Also like commercial computers, imaginary processor large data memory, registers. Consequently, add two integers memory, two values must loaded registers. load instruction makes copy integer memory places copy register. store instruction moves data opposite direction: makes copy value currently register places copy integer memory. One operands load store specifies register loaded stored. operand interesting illustrates feature found many commercial processors: single operand combines two values. Instead using single constant specify memory address, memory operands contain two parts. One part specifies register, part specifies constant often called offset. instruction executed, processor reads current value specified register, adds offset, uses result memory address. example clarify idea. Consider load instruction loads register 1 value memory. instruction might written as: load r1, 20(r3) first operand specifies value loaded register 1. second operand specifies memory address computed adding offset 20 current contents register 3. processors designed operands specify register plus offset? Using form makes easy efficient iterate array. address first element placed register, bytes element accessed using constant part operand. move next element array, register incremented element size. now, need understand operands used, consider design hardware implements them. example, suppose register 3 contains value 10000, load instruction shown specifies offset 20. instruction executed, hardware adds 10000 20, treats result memory address, loads integer location 10020 register 1. fourth instruction, jump controls flow execution giving processor address instruction memory. Normally, imaginary processor works like ordinary processor executing instruction moving next instruction memory automatically. encounters jump instruction, however, processor move next instruction. Instead, processor uses operand jump instruction compute memory address, starts executing address. Like load store instructions, jump instruction allows register offset specified operand. example, instruction jump 60(r11) specifies processor obtain contents register 11, add 60, treat result address instruction memory, make address next location instruction isexecuted. important understand processors contain jump instruction — need understand hardware handles move new location program. 6.4 Instructions Memory said instruction memory imaginary computer contains set instructions processor execute, instruction occupies thirty-two bits. computer designer specifies exact format instruction specifying bit means. Figure 6.2 shows instruction format imaginary computer. Figure 6.2 binary representation four instructions listed Figure 6.1. instruction thirty- two bits long. Look carefully fields used instruction. instruction exactly format, even though fields needed instructions. uniform format makes easy design hardware extracts fields instruction. operation field instruction (sometimes called opcode field) contains value specifies operation. example, add instruction operation field set 1, load instruction operation field set 2, on. Thus, picks instruction, hardware use operation field decide operation perform. three fields term reg name specify three registers. add instruction needs three registers; instructions, one two register fields used. hardware ignores unused fields executing instruction add. order operands instructions may seem unexpected inconsistent code above. example, code add instruction destination (the register contain result) left, two registers added right. instruction, fields specify two registers added precede field specifies destination. Figure 6.3 shows statement written programmer instruction converted bits memory. summarize point:The order operands assembly language program chosen convenient programmer; order operands instruction memory chosen make hardware efficient. Figure 6.3 (a) example add instruction appears programmer, (b) instruction stored memory. figure, field labeled reg contains 2 specify register 2, field labeled reg B contains 3 specify register 3, field labeled dst reg contains 4 specify result placed register 4. examine hardware, see binary representation used instructions capricious — format chosen simplify hardware design. example, instruction operand specifies memory address, register operand always assigned field labeled reg A. Thus, hardware must add offset register, register always found field reg A. Similarly, value must placed register, register found field dst reg. 6.5 Moving Next Instruction Chapter 2 illustrates clock used control timing fixed sequence steps. Building computer requires one additional twist: instead fixed sequence steps, computer programmable means although computer hardware perform every possible instruction, exact sequence instructions perform predetermined. Instead, programmer stores program memory, processor moves memory, extracting executing successive instructions one time. next sections illustrate digital logic circuits arranged enable programmability. pieces hardware needed execute instructions memory? One key element known instruction pointer. instruction pointer consists register (i.e., set latches) processor holds memory address next instruction execute. example, imagine computer thirty-two-bit memory addresses, instruction pointer hold thirty-two-bit value. execute instructions, hardware repeats following three steps.Use instruction pointer memory address fetch instruction Use bits instruction control hardware performs operation Move instruction pointer next instruction One important aspects processor execute instructions arises mechanism used move next instruction. extracts instruction instruction memory, processor must compute memory address instruction immediately follows. Thus, given instruction executed, processor ready execute next sequential instruction. example computer, instruction occupies thirty-two bits memory. However, memory byte-addressable, means instruction executed, hardware must increment instruction pointer four bytes (thirty two bits) move next instruction. essence, processor must add four instruction pointer place result back instruction pointer. perform computation, constant 4 current instruction pointer value passed thirty-twobit adder. Figure 6.4 illustrates basic components used increment instruction pointer shows components interconnected. Figure 6.4 Hardware increments program counter. circuit figure appears infinite loop simply run wild incrementing program counter continuously. understand circuit works, recall clock used control synchronize digital circuits. case program counter, clock lets increment occur instruction executed. Although clock shown, assume component circuit connected clock, component acts according clock. Thus, adder compute new value immediately, program counter updated clock pulses. Throughout discussion, assume clock pulses per instruction. line figure represents data path consists multiple parallel wires. figure, data path thirty-two bits wide. is, adder takes two inputs, thirty-two bits. value instruction pointer obvious instructionpointer thirty-two bits. input, marked label 4 represents thirty-two-bit constant numeric value 4. is, imagine thirty-two wires zero except third wire. adder computes sum produces thirty-two-bit result. 6.6 Fetching Instruction next step constructing computer consists fetching instruction memory. simplistic example, assume dedicated instruction memory holds program executed, memory hardware unit takes address input extracts thirty- two bit data value specified location memory. is, imagine memory array bytes set input lines set output lines. Whenever value placed input lines, memory uses value input decoder, selects appropriate bytes, sets output lines value found bytes. Figure 6.5 illustrates value program counter used address instruction memory. Figure 6.5 data path used instruction fetch value program counter used memory address. 6.7 Decoding Instruction instruction fetched memory, consists thirty-two bits. next conceptual step execution consists instruction decoding. is, hardware separates fields theinstruction operation, registers specified, offset. Recall Figure 6.2 bits instruction organized. used separate bit fields item, instruction decoding trivial — hardware simply separates wires carry bits operation field, three register fields, offset field. Figure 6.6 illustrates output instruction memory fed instruction decoder. Figure 6.6 Illustration instruction decoder connected output instruction memory. figure, individual outputs instruction decoder thirty-two bits. operation consists five bits, outputs correspond registers consist four bits each, output labeled offset consists fifteen bits. Thus, think line data path diagram indicating one bits data. important understand output decoder consists fields instruction. example, path labeled offset contains fifteen offset bits instruction. Similarly, data path labeled reg merely contains four bits reg field instruction. point data reg specifies register use, carry value currently register. summarize: example instruction decoder merely extracts bit fields instruction without interpreting fields. Unlike imaginary computer, real processor may multiple instruction formats (e.g., fields arithmetic instruction may different locations fields memoryaccess instruction). Furthermore, real processor may variable length instructions. result, instruction decoder may need examine operation decide location fields. Nevertheless, principle applies: decoder extracts fields instruction passes field along data path. 6.8 Connections Register Unit register fields instruction used select registers used instruction. example, jump instruction uses one register, load store instruction uses two, add instruction uses three. Therefore, three possible register fields must connected register storage unit Figure 6.7 illustrates. Figure 6.7 Illustration register unit attached instruction decoder. 6.9 Control Coordination Although three register fields connect register unit, unit always use three. Instead, register unit contains logic determines whether given instruction reads existing values registers writes data one registers. particular, load add instructions write result register, jump store instructions not. may seem operation portion instruction passed register unit allow unit know act. understand figure show connectionbetween remaining fields instruction register unit, remember examining data paths (i.e., hardware paths along data flow). actual computer, units illustrated figure additional connections carry control signals. example, unit must receive clock signal ensure coordinates take action correct time (e.g., ensure data memory store value correct address computed). practice, computers use additional hardware unit, known controller, coordinate overall data movement functional units. controller must one connections units, must use operation field instruction determine unit operate perform instruction. diagram, example, connection controller register unit would used specify whether register unit fetch values one two registers, whether unit accept data placed register. now, assume controller exists coordinate operation units. 6.10 Arithmetic Operations Multiplexing example set instructions illustrates important principle: hardware designed re-use functional units. Consider arithmetic. add instruction performs arithmetic explicitly. real processor several arithmetic logical instructions (e.g., subtract, shift, logical and, etc), use operation field instruction decide ALU perform. instruction set also implicit arithmetic operation associated load, store, jump instructions. instructions requires addition operation performed instruction executed. Namely, processor must add offset value, found instruction itself, contents register. resulting sum treated memory address. question arises: processor separate hardware unit compute sum needed address, single ALU used general arithmetic address arithmetic? questions form basis key decisions processor design. Separate functional units advantage speed ease design. Re-using functional unit multiple purposes advantage taking less power. design illustrates re-use. Like many processors, design contains single Arithmetic Logic Unit (ALU) performs arithmetic operations†. sample instruction set, inputs ALU come two sources: either pair registers register offset field instruction. hardware unit choose among multiple sources input? mechanism accommodates two possible inputs known multiplexor. basic idea multiplexor K data inputs, one data output, set control lines used specify input sent output. understand multiplexor used, consider Figure 6.8, shows multiplexor register unit ALU. viewing figure, remember line diagram represents data path thirty-two bits. Thus, eachinput multiplexor contains thirty-two bits output. multiplexor selects thirty-two bits one two inputs sends output. Figure 6.8 Illustration multiplexor used select input ALU. figure, inputs multiplexor come register unit offset field instruction. multiplexor decide input pass along? Recall diagram shows data path. addition, processor contains controller, units connected controller. processor executes add instruction, controller signals multiplexor select input coming register unit. processor executes instructions, controller specifies multiplexor select input comes offset field instruction. Observe operation field instruction passed ALU. permits ALU decide operation perform. case arithmetic logical instruction (e.g., add, subtract, right shift, logical and), ALU uses operation select appropriate action. case instructions, ALU performs addition. 6.11 Operations Involving Data Memory executes load store operation, computer must reference item data memory. operations, ALU used add offset instruction contents register, result used memory address. simplified design, memory used store data separate memory used store instructions. Figure 6.9 illustrates data paths used connect data memory.Figure 6.9 Illustration data paths including data memory. 6.12 Example Execution Sequences understand computation proceeds, consider data paths used instruction. following paragraphs explain sequence. case, program counter gives address instruction, passed instruction memory. instruction memory fetches value memory, passes bits value instruction decoder. decoder separates fields instruction passes units. remainder operation depends instruction. Add. add instruction, register unit given three register numbers, passed along paths labeled reg A, reg B, dst reg. register unit fetches values first two registers, passed ALU. register unit also prepares write third register. ALU uses operation code determine addition required. allow reg B output register unit reach ALU, controller (not shown) must set multiplexor M2 pass value B register unit ignore offset value decoder. controller must set multiplexor M3 pass output ALU register unit’s data input, must set multiplexor M1 ignore output ALU. output ALU reaches input connection register unit, register unit stores value register specified path labeled dst reg, operation complete. Store. store instruction fetched memory decoded, register unit fetches values registers B, places output lines. Multiplexor M2 setto pass offset field ALU ignore value register B. controller instructs ALU perform addition, adds offset contents register A. resulting sum passed data memory address. Meanwhile, register B value (the second output register unit) passed data connection data memory. controller instructs data memory perform write operation, writes value register B location specified value address lines, operation complete. Load. load instruction fetched decoded, controller sets multiplexor M2 ALU receives contents register offset field instruction. store, controller instructs ALU perform addition, result passed data memory address. controller signals data memory perform fetch operation, means output data memory value location given address input. controller must set multiplexor M3 ignore output ALU pass output data memory along data path register unit. controller signals register unit store input value register specified register dst reg. register unit stores value, execution instruction complete. Jump. jump instruction fetched decoded, controller sets multiplexor M2 pass offset field instruction, instructs ALU perform addition. ALU adds offset contents register A. use result address, controller sets multiplexor M3 pass output ALU ignore output data memory. Finally, controller sets multiplexor M1 pass value ALU program counter. Thus, result ALU becomes input 32-bit program counter. program counter receives stores value, instruction complete. Recall program counter always specifies address memory fetch next instruction. Therefore, next instruction executes, instruction extracted address computed previous instruction (i.e., program jump new location). 6.13 Summary computer system programmable, means instead entire sequence operations hardwired digital logic, computer executes instructions memory. Programmability provides substantial computational power flexibility, allowing one change functionality computer loading new program memory. Although overall design computer executes instructions complex, basic components difficult understand. computer consists multiple hardware components, program counter, memories, register units, ALU. Connections among components form computer’s data path. examined set components sufficient execute basic instructions, reviewed hardware steps instruction fetch, decode, execute, including register data access. encoding used instructions selected make hardware design easier — fields instruction extracted passed hardware units. addition data path, controller connections hardware units. multiplexor important mechanism allows controller route data among thehardware units. essence, multiplexor acts switch allows data one several sources sent given output. instruction executes, controller uses fields instruction determine set multiplexors execution. Multiplexors permit single ALU compute address offsets well compute arithmetic operations. reviewed execution basic instructions saw multiplexors along data path computer control values pass given hardware unit. saw, example, multiplexor selects whether program counter incremented four move next instruction value replaced output ALU (to perform jump). EXERCISES 6.1 example system follow Von Neumann Architecture? not? 6.2 Consult Figure 6.3, show individual bit following instruction stored memory: add r1, r14, r9 6.3 Consult Figure 6.3, show individual bit following instruction stored memory: load r7, 43(r15) 6.4 following instruction invalid? jump 40000(r15) Hint: consider storing instruction memory. 6.5 example presented chapter uses four instructions. Given binary representation Figure 6.2, many possible instructions (opcodes) created? 6.6 Explain circuit Figure 6.5 merely infinite loop runs wildly. 6.7 jump instruction executed, operation ALU perform? 6.8 data path diagram, diagram Figure 6.9 hides many details. example changed every instruction sixty-four bits long, trivial change must made figure? 6.9 Make table instructions show multiplexors set instruction executed. 6.10 Modify example system include additional operations right shift subtract. 6.11 Figure 6.9, input multiplexor M1 forward add instruction? 6.12 Figure 6.9, instructions multiplexor M3 select input ALU? 6.13 Redesign computer system Figure 6.9 include relative branch instruction. Assume offset field contains signed value, add value current program counter toproduce next value program counter. 6.14 system Figure 6.9 handle multiplication? not? †Hardware engineers often use term register file refer hardware unit implements set registers; simply refer registers. †Incrementing program counter special case.7 Operand Addressing Instruction Representation Chapter Contents 7.1 Introduction 7.2 Zero, One, Two, Three Address Designs 7.3 Zero Operands Per Instruction 7.4 One Operand Per Instruction 7.5 Two Operands Per Instruction 7.6 Three Operands Per Instruction 7.7 Operand Sources Immediate Values 7.8 Von Neumann Bottleneck 7.9 Explicit Implicit Operand Encoding 7.10 Operands Combine Multiple Values 7.11 Tradeoffs Choice Operands 7.12 Values Memory Indirect Reference 7.13 Illustration Operand Addressing Modes 7.14 Summary7.1 Introduction previous chapters discuss types processors consider processor instruction sets. chapter focuses two details related instructions: ways instructions represented memory ways operands specified. see form operands especially relevant programmers. also understand representation instructions determines possible operand forms. next chapter continues discussion processors explaining Central Processing Unit (CPU) operates. see CPU combines many features discussed large, unified system. 7.2 Zero, One, Two, Three Address Designs said instruction usually stored opcode followed zero operands. many operands needed? discussion Chapter 5 assumes number operands determined operation performed. Thus, add instruction needs least two operands addition involves least two quantities. Similarly, Boolean instruction needs one operand logical inversion involves one quantity. However, example MIPS instruction set Chapter 5 employs additional operand instruction specifies location result. Thus, example instruction set, add instruction requires three operands: two specify values added third specifies location result. Despite intuitive appeal processor instruction arbitrary number operands, many processors permit scheme. understand why, must consider underlying hardware. First, arbitrary number operands implies variable-length instructions, fetching decoding instructions less efficient using fixed- length instructions. Second, fetching arbitrary number operands takes time, processor run slower processor fixed number operands. may seem parallel hardware solve inefficiency. Imagine, example, parallel hardware units fetch one operand instruction. instruction two operands, two units operate simultaneously; instruction four operands, four units operate simultaneously. However, parallel hardware uses space chip requires additional power. addition, number pins chip limits amount data outside chip accessed parallel. Thus, parallel hardware attractive option many cases (e.g., processor portable phone operates battery power).Can instruction set designed without allowing arbitrary operands? so, smallest number operands useful general computation? Early computers answered question using scheme instruction one operand. Later computers introduced instruction sets limited instruction two operands. Surprisingly, computers also exist instructions operands instruction itself. Finally, seen previous chapter, processors limit instructions three operands. 7.3 Zero Operands Per Instruction architecture instructions operands known 0-address architecture. architecture allow instructions specify operands? answer operands must implicit. is, location operands already known. 0-address architecture also called stack architecture operands kept run-time stack. example, add instruction takes two values top stack, adds together, places result back stack. course, exceptions, instructions stack computer allow programmer specify operand. example, zero-address architectures include push instruction inserts new value top stack, pop instruction removes top value stack places value memory. Thus, stack machine, add seven variable X, one might use sequence instructions similar example Figure 7.1. chief disadvantage stack architecture arises use memory — takes much longer fetch operands memory registers processor. later section discusses concept; now, sufficient understand computer industry moved away stack architectures. Figure 7.1 example instructions used stack computer add seven variable X. architecture known zero-address architecture operands instruction add found stack. 7.4 One Operand Per Instruction architecture limits instruction single operand classified 1-address design. essence, 1-address design relies implicit operand instruction: special register known accumulator†. One operand instruction processor uses value accumulator second operand. operation performed,the processor places result back accumulator. think instruction operating value accumulator. example, consider arithmetic operations. Suppose addition instruction operand X: encounters instruction, processor performs following operation: course, instruction set 1-address processor includes instructions allow programmer load constant value memory location accumulator store current value accumulator memory location. 7.5 Two Operands Per Instruction Although works well arithmetic logical operations, 1-address design allow instructions specify two values. example, consider copying value one memory location another. 1-address design requires two instructions load value accumulator store value new location. design especially inefficient system moves graphics objects display memory. overcome limitations 1-address systems, designers invented processors allow instruction two addresses. approach known 2-address architecture. 2-address processor, operation applied specified value instead merely accumulator. Thus, 2-address processor, specifies value X added current value Y: allows instruction specify two operands, 2-address processor offer data movement instructions treat operands source destination. example, 2- address instruction copy data directly location Q location R†: 7.6 Three Operands Per InstructionAlthough 2-address design handles data movement, optimization possible, especially processors multiple general-purpose registers: allow instruction specify three operands. Unlike 2-address design, key motivation 3-address architecture arise operations require three input values. Instead, point third operand specify destination. example, addition operation specify two values added well destination result: specifies assignment of: 7.7 Operand Sources Immediate Values discussion focuses number operands instruction without specifying exact details operand. know instruction bit field operand, questions arise bits interpreted. type operand represented instruction? operands use representation? semantic meaning given representation? understand issue, observe data value used operand obtained many ways. Figure 7.2 lists possibilities operands 3-address processor‡. Figure 7.2 Examples items operand reference 3-address processor. source operand specifies value destination operand specifies location. figure indicates, architectures allow operand constant. Although operand field small, explicit constant important programs use smallconstants frequently (e.g., increment loop index 1); encoding constant instruction faster requires fewer registers. use term immediate value refer operand constant. architectures interpret immediate values signed, interpret unsigned, others allow programmer specify whether value signed unsigned. 7.8 Von Neumann Bottleneck Recall conventional computers store programs data memory classified following Von Neumann Architecture. Operand addressing exposes central weakness Von Neumann Architecture: memory access become bottleneck. is, instructions stored memory, processor must make least one memory reference per instruction. one operands specify items memory, processor must make additional memory references fetch store values. optimize performance avoid bottleneck, operands must taken registers instead memory. point is: computer follows Von Neumann Architecture, time spent accessing memory limit overall performance. Architects use term Von Neumann bottleneck characterize situation, avoid bottleneck choosing designs operands found registers. 7.9 Explicit Implicit Operand Encoding operand represented instruction? instruction contains bit field operand, architect must specify exactly bits mean (e.g., whether contain immediate value, number register, memory address). Computer architects used two interpretations operands: implicit explicit. next sections describe approaches. 7.9.1 Implicit Operand Encoding implicit operand encoding easiest understand: opcode specifies types operands. is, processor uses implicit encoding contains multiple opcodes given operation — opcode corresponds one possible combination operands. example, Figure 7.3 lists three instructions addition might offered processor uses implicit operand encoding.Figure 7.3 example addition instructions 2-address processor uses implicit operand encoding. separate opcode used possible combination operands. figure illustrates, operands need interpretation. example, consider add immediate signed instruction. instruction takes two operands: first operand interpreted register number, second interpreted signed integer. 7.9.2 Explicit Operand Encoding chief disadvantage implicit encoding apparent Figure 7.3: multiple opcodes needed given operation. fact, separate opcode needed combination operands. processor uses many types operands, set opcodes extremely large. alternative, explicit operand encoding associates type information operand. Figure 7.4 illustrates format two add instructions architecture uses explicit operand encoding. figure shows, operand field divided two subfields: one specifies type operand specifies value. example, operand references register begins type field specifies remaining bits interpreted register number. Figure 7.4 Examples operands architecture uses explicit encoding. operand specifies type well value. 7.10 Operands Combine Multiple ValuesThe discussion implies operand consists single value extracted register, memory, instruction itself. processors indeed restrict operand single value. However, processors provide hardware compute operand value extracting combining values multiple sources. Typically, hardware computes sum several values. example help clarify hardware handles operands composed multiple values. One approach known register-offset mechanism. idea straightforward: instead two subfields specify type value, operand consists three fields specify register-offset type, register, offset. fetches operand, processor adds contents offset field contents specified register obtain value used operand. Figure 7.5 shows example add instruction register-offset operands. Figure 7.5 example add instruction operand consists register plus offset. operand fetch, hardware adds offset specified register obtain value operand. figure, first operand specifies contents register 2 minus constant 17, second operand specifies contents register 4 plus constant 76. discuss memory, see allowing operand specify register plus offset especially useful referencing data aggregate C language struct pointer structure left register offsets used reference individual items. 7.11 Tradeoffs Choice Operands discussion unsatisfying — seems listed many design possibilities focused approach adopted. fact, best choice, operand style discussed used practice. hasn’t one particular style emerged optimal? answer simple: style represents tradeoff ease programming, size code, speed processing, complexity hardware. next paragraphs discuss several potential design goals, explain relates choice operands. Ease Programming. Complex forms operands make programming easier. example, said allowing operand specify register plus offset makes data aggregate references straightforward. Similarly, 3-address approach provides explicit target means programmer need code separate instructions copy results final destination. course, optimize ease programming, architect needs trade aspects. Fewer Instructions. Increasing expressive power operands reduces number instructions program. example, allowing operand specify register anoffset means program need use extra instruction add offset register. Increasing number addresses per instruction also lowers count instructions (e.g., 3-address processor requires fewer instructions 2-address processor). Unfortunately, fewer instructions produce tradeoff instruction larger. Smaller Instruction Size. Limiting number operands, set operands types, maximum size operand keeps instruction small fewer bits needed identify operand type represent operand value. particular, operand specifies register smaller operand specifies register offset. result, smallest, least powerful processors limit operands registers — except load store operations, value used program must come register. Unfortunately, making instruction smaller decreases expressive power, therefore increases number instructions needed. Larger Range Immediate Values. Recall Chapter 3 string k bits hold 2k possible values. Thus, number bits allocated operand determines numeric range immediate values specified. Increasing range immediate values results larger instructions. Faster Operand Fetch Decode. Limiting number operands possible types operand allows hardware operate faster. maximize speed, example, architect avoids register-offset designs hardware fetch operand register much faster compute value register plus offset. Decreased Hardware Size Complexity. amount space integrated circuit limited, architect must decide use space. Decoding complex forms operands requires hardware decoding simpler forms. Thus, limiting types complexity operands reduces size circuitry required. course, choice represents tradeoff: programs larger. point is: Processor architects created variety operand styles. single form optimal processors choice represents compromise among functionality, program size, complexity hardware required fetch values, performance, ease programming. 7.12 Values Memory Indirect Reference processor must provide way access values memory. is, least one instruction must operand hardware interprets memory address†. Accessing value memory significantly expensive accessing value register. Although may make programming easier, design instruction references memory usually results lower performance. Thus, programmers usually structure code keep values used often registers reference memory needed.Some processors extend memory references permitting various forms indirection. example, operand specifies indirection register 6 causes processor perform two steps: Obtain A, current value register 6. Interpret memory address, fetch operand memory. One extreme form operand involves double indirection, indirection memory location. is, processor interprets operand memory address M. However, instead loading storing value address M, processor assumes contains memory address value. cases, processor performs following steps: Obtain M, value operand itself. Interpret memory address, fetch value memory. Interpret another memory address, fetch operand memory. Double indirection goes one memory location another useful program follow linked list memory. However, overhead extremely high (execution single instruction entails multiple memory references). 7.13 Illustration Operand Addressing Modes processor usually contains special internal register, called instruction register, used hold instruction instruction decoded. possible types operand addresses cost envisioned considering location operand references needed fetch value. immediate value least expensive value located instruction register (i.e., instruction itself). general-purpose register reference slightly expensive immediate value. reference memory expensive reference register. Finally, double indirection, requires two memory references, expensive. Figure 7.6 lists possibilities, illustrates hardware units involved resolving each.Figure 7.6 Illustration hardware units accessed fetching operand various addressing modes. Indirect references take longer direct references. figure, modes 3 5 require instruction contain memory address. Although available earlier computers, modes become unpopular require instruction quite large. 7.14 Summary designing processor, architect chooses number possible types operands instruction. make operand handling efficient, many processors limit number operands given instruction three fewer. immediate operand specifies constant value; possibilities include operand specifies using contents register value memory. Indirection allows register contain memory address operand. Double indirection means operand specifies amemory address value address pointer another memory location holds value. type operand encoded implicitly (i.e., opcode) explicitly. Many variations exist choice operand number type represents tradeoff among functionality, ease programming, engineering details speed processing. EXERCISES 7.1 Suppose computer architect designing processor computer extremely slow memory. Would architect choose zero-address architecture? not? 7.2 Consider size instructions memory. architecture allows immediate operands large numeric values, instruction takes space memory. Why? 7.3 Assume stack machine keeps stack memory. Also assume variable p stored memory. many memory references needed increment p seven? 7.4 Assume two integers, x stored memory, consider instruction sets z sum x+y. many memory references needed two-address architecture? Hint: remember include instruction fetch. 7.5 many memory operations required perform add operation 3-address architecture operand specifies indirect memory reference? 7.6 programmer increments variable value greater maximum immediate operand, optimizing compiler may generate two instructions. example, computer allows immediate values 127 less, incrementing variable x 140 results sequence: load r7, x add_immediate r7, 127 add_immediate t7, 13 store r7, x doesn’t compiler store 140 memory add value register 7? 7.7 Assume memory reference takes twelve times long register reference, assume program executes N instructions 2-address architecture. Compare running time program operands registers running time operands memory. Hint: instruction fetch requires memory operation. 7.8 Consider type operand Figure 7.6 illustrates, make table contains expression number bits required represent operand. Hint: number bits required represent values zero N is: 7.9 Name one advantage using higher number addresses per instruction.7.10 Consider two-address computer uses implicit operands. Suppose one two operands five operand types Figure 7.6, except immediate value. List add instructions computer needs. 7.11 compilers contain optimization modules choose keep frequently used variables registers rather writing back memory. term characterizes problem optimization module attempting overcome? †The general-purpose registers discussed Chapter 5 considered extension original accumulator concept. †Some architects reserve term 2-address instructions operands specify memory location, use term 1 ½- address situations one operand memory operand register. ‡To increase performance, modern 3-address architectures often limit operands one operands given instruction refers location memory; two operands must specify registers. †The third section text describes memory memory addressing.8 CPUs: Microcode, Protection, Processor Modes Chapter Contents 8.1 Introduction 8.2 Central Processor 8.3 CPU Complexity 8.4 Modes Execution 8.5 Backward Compatibility 8.6 Changing Modes 8.7 Privilege Protection 8.8 Multiple Levels Protection 8.9 Microcoded Instructions 8.10 Microcode Variations 8.11 Advantage Microcode 8.12 FPGAs Changes Instruction Set 8.13 Vertical Microcode 8.14 Horizontal Microcode 8.15 Example Horizontal Microcode8.16 Horizontal Microcode Example 8.17 Operations Require Multiple Cycles 8.18 Horizontal Microcode Parallel Execution 8.19 Look-Ahead High Performance Execution 8.20 Parallelism Execution Order 8.21 Out-Of-Order Instruction Execution 8.22 Conditional Branches Branch Prediction 8.23 Consequences Programmers 8.24 Summary 8.1 Introduction Previous chapters consider two key aspects processors: instruction sets operands. chapters explain possible approaches, discuss advantages disadvantages approach. chapter considers broad class general-purpose processors, shows many concepts previous chapters applied. next chapter considers low-level programming languages used processors. 8.2 Central Processor Early history computers, centralization emerged important architectural approach — much functionality possible collected single processor. processor, became known Central Processing Unit (CPU), controlled entire computer, including calculations I/O. contrast early designs, modern computer system follows decentralized approach. system contains multiple processors, many dedicated specific function hardware subsystem. example, see I/O device, disk, include processor handles disk transfers. Despite shift paradigm, term CPU survived one chip contains hardware used perform computations coordinate control processors. essence, CPU manages entire computer system telling processors start, stop, do. discuss I/O, see CPU controls operation peripheral devices processors.8.3 CPU Complexity must handle wide variety control processing tasks, modern CPU extremely complex. example, Intel makes CPU chip contains 2.5 billion transistors. CPU complex? many transistors needed? Multiple Cores. fact, modern CPU chips contain one processor. Instead, contain multiple processors called cores. cores function parallel, permitting multiple computations proceed time. Multicore designs required high performance single core cannot clocked arbitrarily high speeds. Multiple Roles. One aspect CPU complexity arises CPU must fill several major roles: running application programs, running operating system, handling external I/O devices, starting stopping computer, managing memory. single instruction set optimal roles, CPU often includes many instructions. Protection Privilege. computer systems incorporate system protection gives subsystems higher privilege others. example, hardware prevents application program directly interacting I/O devices, operating system code protected inadvertent deliberate change. Hardware Priorities. CPU uses priority scheme actions assigned higher priority others. example, see I/O devices operate higher priority application programs — CPU running application program I/O device needs service, CPU must stop running application handle device. Generality. CPU designed support wide variety applications. Consequently, CPU instruction set often contains instructions used type application (i.e., CISC design). Data Size. speed processing, CPU designed handle large data values. Recall Chapter 2 digital logic gates operate single bit data gates must replicated handle integers. Thus, operate values composed sixty-four bits, digital circuit CPU must sixty-four copies gate. High Speed. final, perhaps significant, source CPU complexity arises desire speed. Recall important concept discussed earlier: Parallelism fundamental technique used create high-speed hardware. is, achieve highest performance, functional units CPU must replicated, design must permit replicated units operate simultaneously. large amount parallel hardware needed make modern CPU operate highest rate also means CPU requires many transistors. see explanations later chapter. 8.4 Modes ExecutionThe features listed combined implemented separately. example, given core granted access parts memory without higher priority. CPU accommodate features way allows programmers understand use without becoming confused? CPUs, hardware uses set parameters handle complexity control operation. say hardware multiple modes execution. given time, current execution mode determines CPU operates. Figure 8.1 lists items usually associated CPU mode execution. Figure 8.1 Items typically controlled CPU mode execution. characteristics CPU change dramatically mode changes. 8.5 Backward Compatibility much variation execution modes introduce? principle, modes available CPU need share much common. one extreme case, CPUs mode provides backward compatibility previous model. Backward compatibility allows vendor sell CPU new features, also permits customers use CPU run old software. Intel’s line processors (i.e., 8086, 186, 286,...) exemplifies backward compatibility used. Intel first introduced CPU operated thirty-twobit integers, CPU included compatibility mode implemented sixteen-bit instruction set Intel’s previous CPU. addition using different sizes integers, two architectures different numbers registers different instructions. two architectures differ significantly easiest think design two separate pieces hardware execution mode determining two used time. summarize: CPU uses execution mode determine current operational characteristics. CPUs, characteristics modes differ widely think CPU separate hardware subsystems mode determining piece hardware used current time.8.6 Changing Modes CPU change execution modes? two ways: Automatic (initiated hardware) Manual (under program control) Automatic Mode Change. External hardware change mode CPU. example, I/O device requests service, hardware informs CPU. Hardware CPU changes mode (and jumps operating system code) automatically servicing device. learn consider I/O works. Manual Mode Change. essence, manual changes occur control running program. often, program operating system, changes mode executes application. However, CPUs also provide multiple modes applications use, allow application switch among modes. mechanism used change mode? Three approaches used. simplest case, CPU includes instruction set current mode. cases, CPU contains special-purpose mode register control mode. change modes, program stores value mode register. Note mode register storage unit normal sense. Instead, consists hardware circuit responds store command changing operating mode. Finally, mode change occur side effect another instruction. CPUs, example, instruction set includes instruction application uses make operating system call. mode change occurs automatically whenever instruction executed. accommodate major changes mode, additional facilities may needed prepare new mode. example, consider case two modes execution share general-purpose registers (e.g., one mode registers sixteen bits another mode registers contain thirty-two bits). may necessary place values alternate registers changing mode using registers. cases, CPU provides special instructions allow software create modify values changing mode. 8.7 Privilege Protection mode execution linked CPU facilities privilege protection. is, part current mode specifies level privilege CPU. example, services I/O device, CPU must allow device driver software operating system interact device perform control functions. However, arbitrary application program must prevented accidentally maliciously issuing commands hardware performing control functions. Thus, executes application program, operating system changes mode reduce privilege. running less privileged mode, CPU permit direct control I/O devices (i.e., CPU treats privileged operation like invalid instruction).8.8 Multiple Levels Protection many levels privilege needed, operations allowed level? subject discussed hardware architects operating system designers many years. CPUs invented offer protection, CPUs invented offer eight levels, privilege previous level. idea protection help prevent problems using minimum amount privilege necessary time. summarize: using protection scheme limit operations allowed, CPU detect attempts perform unauthorized operations. Figure 8.2 illustrates concept two privilege levels. Figure 8.2 Illustration CPU offers two levels protection. operating system executes highest privilege, application programs execute less privilege. Although protection scheme suffices CPUs, designers generally agree minimum two levels CPU runs application programs: CPU runs applications needs least two levels protection: operating system must run absolute privilege, application programs run limited privilege. discuss memory, see issues protection memory access intertwined. important, see memory access mechanisms, part CPU mode, provide additional forms protection.8.9 Microcoded Instructions complex CPU implemented? Interestingly, one key abstractions used build complex instruction set comes software: complex instructions programmed! is, instead implementing instruction set directly digital circuits, CPU built two pieces. First, hardware architect builds fast, small processor known microcontroller†. Second, implement CPU instruction set (called macro instruction set), architect writes software microcontroller. software runs microcontroller known microcode. Figure 8.3 illustrates two-level organization, shows level implemented. Figure 8.3 Illustration CPU implemented microcontroller. macro instruction set CPU provides implemented microcode. easiest way think microcode imagine set functions implement one CPU macro instructions. CPU invokes microcode instruction execution. is, obtained decoded macro instruction, CPU invokes microcode procedure corresponds instruction. macro- micro architectures differ. example, suppose CPU designed operate data items thirty-two bits macro instruction set includes add32 instruction integer addition. suppose microcontroller offers sixteen-bit arithmetic. implement thirty-two-bit addition, microcode must add sixteen bits time, must add carry low-order bits high-order bits. Figure 8.4 lists microcode steps required:Figure 8.4 example steps required implement thirty-two-bit macro addition microcontroller sixteen-bit arithmetic. macro- micro architectures differ. exact details unimportant; figure illustrates architecture microcontroller macro instruction set differ dramatically. Also note macro instruction implemented microcode program, macro instruction perform arbitrary processing. example, possible single macro instruction implement trigonometric function, sine cosine, move large blocks data memory. course, achieve higher performance, architect choose limit amount microcode corresponds given instruction. 8.10 Microcode Variations Computer designers invented many variations basic form microcode. example, said CPU hardware implements fetch-execute cycle invokes microcode procedure instruction. CPUs, microcode implements entire fetch- execute cycle — microcode interprets opcode, fetches operands, performs specified operation. advantage greater flexibility: microcode defines aspects macro system, including format macro instructions form encoding operand. chief disadvantage lower performance: CPU cannot instruction pipeline implemented hardware.As another variation, CPU designed uses microcode extensions. is, CPU complete macro instruction set implemented directly digital circuits. addition, CPU small set additional opcodes implemented microcode. Thus, vendor manufacture minor variations basic CPU (e.g., version special encryption instruction intended customers implement security software version special pattern matching instruction intended customers implement text processing software). extra instructions used particular version CPU, vendor insert microcode makes undefined (i.e., microcode raises error undefined instruction executed). 8.11 Advantage Microcode microcode used? three motivations. First, microcode offers higher level abstraction, building microcode less prone errors building hardware circuits. Second, building microcode takes less time building circuits. Third, changing microcode easier changing hardware circuits, new versions CPU created faster. summarize: design uses microcode less prone errors updated faster design use microcode. course, microcode disadvantages balance advantages: • Microcode overhead hardware implementation. • executes multiple micro instructions macro instruction, microcontroller must run much higher speed CPU. • cost macro instruction depends micro instruction set. 8.12 FPGAs Changes Instruction Set microcontroller internal mechanism intended help designers, micro instruction set usually hidden final design. microcontroller microcode typically reside integrated circuit along rest CPU, used internally. macro instruction set available programmers. Interestingly, CPUs designed make microcode dynamic accessible customers purchase CPU. is, CPU contains facilities allow underlying hardware changed chip manufactured.Why would customers want change CPU? motivations flexibility performance: allowing customer make changes CPU instructions defers decision macro instruction set, allows CPU’s owner tailor instructions specific use. example, company sells video games might add macro instructions manipulate graphics images, company makes networking equipment might create macro instructions process packet headers. Using underlying hardware directly (e.g., microcode) result higher performance. One technology allows modification become especially popular. Known Field Programmable Gate Array (FPGA), technology permits gates altered chip manufactured. Reconfiguring FPGA time-consuming process. Thus, general idea reconfigure FPGA once, use resulting chip. FPGA used hold entire CPU, FPGA used supplement holds extra instructions. summarize: Technologies like dynamic microcode FPGAs allow CPU instruction set modified extended CPU purchased. motivations flexibility higher performance. 8.13 Vertical Microcode question arises: architecture used microcontroller? point view someone writes microcode, question becomes: instructions microcontroller provide? discussed notion microcode microcontroller consists conventional processor (i.e., processor follows conventional architecture). see shortly designs possible. fact, microcontroller cannot exactly standard processor. must interact hardware units CPU, microcontroller needs special hardware facilities. example, microcontroller must able access ALU store results general-purpose registers macro instruction set uses. Similarly, microcontroller must able decode operand references fetch values. Finally, microcontroller must coordinate rest hardware, including memory. Despite requirements special features, microcontrollers created follow general approach used conventional processors. is, microcontroller’s instruction set contains conventional instructions load, store, add, subtract, branch, on. example, microcontroller used CISC processor consist small, fast RISC processor. say microcontroller vertical architecture, use term vertical microcode characterize software runs microcontroller. Programmers comfortable vertical microcode programming interface familiar. important, semantics vertical microcode exactly programmerexpects: one micro instruction executed time. next section discusses alternative vertical microcode. 8.14 Horizontal Microcode hardware perspective, vertical microcode somewhat unattractive. One primary disadvantages arises performance requirements. macro instructions require multiple micro instructions, means executing macro instructions rate K per second requires microcontroller execute micro instructions rate N×K per second, N average number micro instructions per macro instruction. Therefore, hardware associated microcontroller must operate high speed (e.g., memory used hold microcode must able deliver micro instructions high rate). second disadvantage vertical microcode arises vertical technology cannot exploit parallelism underlying hardware. Computer engineers invented alternative known horizontal microcode overcomes limitations vertical microcode. Horizontal microcode advantage working well hardware, providing familiar interface programmers. is: Horizontal microcode allows hardware run faster, difficult program. understand horizontal microcode, recall data path description Chapter 6: CPU consists multiple functional units, data paths connecting them. Operation units must controlled, unit controlled independently. Furthermore, moving data one functional unit another requires explicit control two units: one unit must instructed send data across data path, unit must instructed receive data. example clarify concept. make example easy understand, make simplifying assumptions restrict discussion six functional units. Figure 8.5 shows six functional units interconnected.Figure 8.5 illustration internal structure within CPU. Solid arrows indicate hardware path along data move. major item shown figure Arithmetic Logic Unit (ALU) performs operations addition, subtraction, bit shifting. remaining functional units provide mechanisms interface ALU rest system. example, hardware units labeled operand 1 operand 2 denote operand storage units (i.e., internal hardware registers). ALU expects operands placed storage units operation performed, places result operation two hardware units labeled result 1 result 2†. Finally, register access unit provides hardware interface general-purpose registers. figure, arrows indicate paths along data pass moves one functional unit another; arrow data path handles multiple bits parallel (e.g., 32 bits). arrows connect data transfer mechanism, serves conduit functional units (a later chapter explains data transfer mechanism depicted called bus). 8.15 Example Horizontal Microcode functional unit controlled set wires carry commands (i.e., binary values hardware interprets command). Although Figure 8.5 show command wires, imagine number command wires connected functional unit depends type unit. example, unit labeled result 1 needs single command wire unit controlled single binary value: zero causes unit stop interacting units, one causes unit send current contents result unit data transfer mechanism. Figure 8.6 summarizes binary control values passed functional unit example, gives meaning each.Figure 8.6 Possible command values meaning example functional units Figure 8.5. Commands carried parallel wires. Figure 8.6 shows, register access unit special case command two parts: first two bits specify operation, last four bits specify register used operation. Thus, command 010011 means value register three moved data transfer mechanism. understand hardware organized, see horizontal microcode works. Imagine microcode instruction consists commands functional units — executes instruction, hardware sends bits instruction functionalunits. Figure 8.7 illustrates bits microcode instruction correspond commands example. Figure 8.7 Illustration thirteen bits horizontal microcode instruction correspond commands six functional units. 8.16 Horizontal Microcode Example horizontal microcode used perform sequence operations? essence, programmer chooses functional units active time, encodes information bits microcode. example, suppose programmer needs write horizontal microcode adds value general-purpose register 4 value general- purpose register 13 places result general-purpose register 4. Figure 8.8 lists operations must performed. Figure 8.8 example sequence steps functional units must execute add values general-purpose registers 4 13, place result general-purpose register 4. steps expressed single micro instruction example system. instruction bits set specify functional unit(s) operate instruction executed. example, Figure 8.9 shows microcode program corresponds four steps. figure, row corresponds one instruction, divided fields correspond functional unit. field contains command sent functional unit instruction executed. Thus, commands determine functional units operate step.Figure 8.9 example horizontal microcode program consists four instructions thirteen bits per instruction. instruction corresponds step listed Figure 8.8. Consider code figure carefully. first instruction specifies two hardware units operate: unit operand 1 register interface unit. fields correspond four units contain zero, means units operate first instruction executed. first instruction also uses data transfer mechanism — data sent across transfer mechanism register interface unit unit operand 1†. is, fields instruction cause register interface send value across transfer mechanism, cause operand 1 unit receive value. 8.17 Operations Require Multiple Cycles Timing among important aspects horizontal microcode. hardware units take longer operate others. example, multiplication take longer addition. is, functional unit given command, results appear immediately. Instead, program must delay accessing output functional unit. programmer writes horizontal microcode must ensure hardware unit given correct amount time complete task. code Figure 8.9 assumes step accomplished one micro instruction cycle. However, micro cycle may short hardware units complete task. example, ALU may require two micro instruction cycles complete addition. accommodate longer computation, extra instruction inserted following third instruction. extra instruction merely specifies ALU continue previous operation; units affected. Figure 8.10 illustrates extra microcode instruction inserted create necessary delay. Figure 8.10 instruction inserted add delay processing wait ALU complete operation. Timing delay crucial aspects horizontal microcode. 8.18 Horizontal Microcode Parallel Execution basic understanding hardware operates general idea horizontal microcode, appreciate important property: use parallelism. Parallelism possible underlying hardware units operate independently. programmer specify parallel operations instruction contains separate fields control one hardware units. example, consider architecture ALU plus separate hardware units hold operands. Assume ALU requires multiple instruction cycles complete operation. ALU accesses operands first cycle, hardware units used hold operands remain unused successive cycles. Thus, programmer insert instruction simultaneously moves new value operand unit ALU operation continues. Figure 8.11 illustrates instruction. Figure 8.11 example instruction simultaneously continues ALU operation loads value register seven operand hardware unit one. Horizontal microcode makes parallelism easy specify. point is: horizontal microcode instructions contain separate fields control one hardware unit, horizontal microcode makes easy specify simultaneous, parallel operation hardware units. 8.19 Look-Ahead High Performance Execution practice, microcode used CPUs much complex simplistic examples chapter. One important sources complexity arises desire achieve high performance. silicon technology allows manufacturers place billions transistors single chip, possible CPU include many functional units operate simultaneously.A later chapter considers architectures make parallel hardware visible programmer. now, consider architectural question: multiple functional units used improve performance without changing macro instruction set? particular, internal organization CPU arranged detect exploit situations parallel execution produce higher performance? already seen trivial example optimization: Figure 8.11 shows horizontal microcode allow ALU operation continue time data value transferred hardware unit holds operand. However, example requires programmer explicitly code parallel behavior creating microcode. understand CPU exploits parallelism automatically, imagine system includes intelligent microcontroller multiple functional units. Instead working one macro instruction time, intelligent controller given access many macro instructions. controller looks ahead instructions, finds values needed, directs functional units start fetching computing values. example, suppose intelligent controller finds following four instructions 3-address architecture: say intelligent controller schedules instructions assigning necessary work functional units. example, controller assign operand functional unit fetches prepares operand values. operand values available instruction, controller assigns instruction functional unit performs operation. instructions listed assigned ALU. Finally, operation completes, controller assign functional unit task moving result appropriate destination register. point is: CPU contains enough functional units, intelligent controller schedule four macro instructions executed time. 8.20 Parallelism Execution Order description intelligent microcontroller overlooks important detail: semantics macro instruction set. essence, controller must ensure computing values parallel change meaning program. example, consider following sequence instructions: Unlike previous example, operands overlap. particular, first instruction specifies register one destination, third instruction specifies register one operand. macro instruction set semantics dictate sequential processing instructions, means first instruction place value register one third instruction references value. preserve sequential semantics, intelligent controller must understand accommodate overlap. essence, controller must balance two goals: maximize amount parallel execution, preserving original (i.e., sequential) semantics. 8.21 Out-Of-Order Instruction Execution controller schedules parallel activities handle case operand one instruction depends results previous instruction? controller uses mechanism known scoreboard tracks status instruction executed. particular, scoreboard maintains information dependencies among instructions original macro instruction sequence execution. Thus, controller use scoreboard decide fetch operands, execution proceed, instruction finished. short, scoreboard approach allows controller execute instructions order, reorders results reflect order specified code. achieve highest speed, modern CPU contains multiple copies functional units permit multiple instructions executed simultaneously. intelligent controller uses scoreboard mechanism schedule execution order preserves appearance sequential processing. 8.22 Conditional Branches Branch Prediction Conditional branches pose another problem parallel execution. example, consider following computation: translated machine instructions, computation contains conditional branch directs execution either code Q code R. condition depends value Y, computed first step. consider running code CPU uses parallel execution instructions. theory, reaches conditional branch, CPU must wait results comparison — CPU cannot start schedule code R Q knows one selected. practice, two approaches used handle conditional branches. first, known branch prediction, based measurements show code, branch taken approximately sixty percent time. Thus, building hardware schedules instructions along branch path provides optimization hardware schedules instructions along non-branch path. course, assuming branch occur may incorrect — CPU eventually determines branch taken, results branch path must discarded CPU must follow path. second approach simply follows paths parallel. is, CPU schedules instructions outcomes conditional branch. branch prediction, CPU must eventually decide result valid. is, CPU continues execute instructions, holds results internally. value condition known, CPU discards results path valid, proceeds move correct results appropriate destinations. course, second conditional branch occur either Q R; scoreboard mechanism handles details. point is: CPU offers parallel instruction execution handle conditional branches proceeding precompute values one branches, choosing values use later time computation branch condition completes. may seem wasteful CPU compute values discarded later. However, goal higher performance, elegance. also observe CPU designed wait conditional branch value known, hardware merely sit idle. Therefore, high- speed CPUs, manufactured Intel AMD, designed parallel functional units sophisticated scoreboard mechanisms. 8.23 Consequences Programmers understanding CPU structured help programmers write faster code? cases, yes. Suppose CPU designed use branch prediction CPU assumes branch taken. programmer optimize performance arranging code common cases take branch. example, programmer knows commonfor less Z, instead testing > Z, programmer rewrite code test whether ≤ Z. 8.24 Summary modern CPU complex processor uses multiple modes execution handle complexity. execution mode determines operational parameters operations allowed current privilege level. CPUs offer least two levels privilege protection: one operating system one application programs. reduce internal complexity, CPU often built two levels abstraction: microcontroller implemented digital circuits, macro instruction set created adding microcode. two broad classes microcode. microcontroller uses vertical microcode resembles conventional RISC processor. Typically, vertical microcode consists set procedures correspond one macro instruction; CPU runs appropriate microcode fetch-execute cycle. Horizontal microcode, allows programmer schedule functional units operate cycle, consists instructions bit field corresponds functional unit. third alternative uses Field Programmable Gate Array (FPGA) technology create underlying system. Advanced CPUs extend parallel execution scheduling set instructions across multiple functional units. CPU uses scoreboard mechanism handle cases results one instruction used successive instruction. idea extended conditional branches allowing parallel evaluation path proceed, then, condition known, discarding values along path taken. EXERCISES 8.1 quad-core CPU chip contains 2 billion transistors, approximately many transistors needed single core? 8.2 List seven reasons modern CPU complex. 8.3 text says CPU chips include backward compatibility mode. mode offer advantage user? 8.4 Suppose addition hardware, CPU used smart phone contains additional hardware three previous versions chip (i.e., three backward compatibility modes). disadvantage user’s point view? 8.5 Virtualized software systems used cloud data centers often include hypervisor runs controls multiple operating systems, applications run one operating systems. levels protection used systems differ conventional levels protection?8.6 manufacturers offer chip contains processor basic set instructions plus attached FPGA. owner configure FPGA additional instructions. chip provide conventional software cannot? 8.7 Read FPGAs, find “programmed.” languages used program FPGA? 8.8 Create microcode algorithm performs 32-bit multiplication microcontroller offers 16-bit arithmetic, implement algorithm C using short variables. 8.9 offered two jobs salary, one programming vertical microcode programming horizontal microcode. choose? Why? 8.10 Find example commercial processor uses horizontal microcode, document meaning bits instruction similar diagram Figure 8.7. 8.11 motivation scoreboard mechanism CPU chip, functionality provide? 8.12 Las Vegas casinos computed odds program execution, odds would give branch taken? Explain answer. †The small processor also called microprocessor, term somewhat misleading. †Recall arithmetic operation, multiplication, produce result twice large operand. †For purposes simplified example, assume data transfer mechanism always operates require control.9 Assembly Languages Programming Paradigm Chapter Contents 9.1 Introduction 9.2 Characteristics High-level Programming Language 9.3 Characteristics Low-level Programming Language 9.4 Assembly Language 9.5 Assembly Language Syntax Opcodes 9.6 Operand Order 9.7 Register Names 9.8 Operand Types 9.9 Assembly Language Programming Paradigm Idioms 9.10 Coding Statement Assembly 9.11 Coding IF-THEN-ELSE Assembly 9.12 Coding FOR-LOOP Assembly 9.13 Coding Statement Assembly 9.14 Coding Subroutine Call Assembly 9.15 Coding Subroutine Call Arguments Assembly 9.16 Consequence Programmers 9.17 Assembly Code Function Invocation 9.18 Interaction Assembly High-level Languages9.19 Assembly Code Variables Storage 9.20 Example Assembly Language Code 9.21 Two-Pass Assembler 9.22 Assembly Language Macros 9.23 Summary 9.1 Introduction Previous chapters describe processor instruction sets operand addressing. chapter discusses programming languages allow programmers specify details instructions operand addresses. chapter tutorial language particular processor. Instead, provides general assessment features commonly found low-level languages. chapter examines programming paradigms, explains programming low-level language differs programming conventional language. Finally, chapter describes software translates low-level language binary instructions. Low-level programming low-level programming languages strictly part computer architecture. consider here, however, languages closely related underlying hardware two cannot separated easily. Subsequent chapters return focus hardware examining memory I/O facilities. 9.2 Characteristics High-level Programming Language Programming languages divided two broad categories: High-level languages Low-level languages conventional programming language, Java C, classified high-level- language language exhibits following characteristics: One-to-many translation Hardware independence Application orientation General-purpose Powerful abstractionsOne-To-Many Translation. statement high-level language corresponds multiple machine instructions. is, compiler translates language equivalent machine instructions, statement usually translates several instructions. Hardware Independence. High-level languages allow programmers create program without knowing details underlying hardware. example, high-level language allows programmer specify floating point operations, addition subtraction, without knowing whether ALU implements floating point arithmetic directly uses separate floating point coprocessor. Application Orientation. high-level language, C Java, designed allow programmer create application programs. Thus, high-level language usually includes I/O facilities well facilities permit programmer define arbitrarily complex data objects. General-Purpose. high-level language, like C Java, restricted specific task specific problem domain. Instead, language contains features allow programmer create program arbitrary task. Powerful Abstractions. high-level language provides abstractions, procedures, allow programmer express complex tasks succinctly. 9.3 Characteristics Low-level Programming Language alternative high-level language known low-level language following characteristics: One-to-one translation Hardware dependence Systems programming orientation Special-purpose abstractions One-To-One Translation. general, statement low-level programming language corresponds single instruction underlying processor. Thus, translation machine code one-to-one. Hardware Dependence. statement corresponds machine instruction, low- level language created one type processor cannot used another type processor. Systems Programming Orientation. Unlike high-level language, low-level language optimized systems programming — language facilities allow programmer create operating system software directly controls hardware. Special-Purpose. focus underlying hardware, low-level languages used cases extreme control efficiency needed. example, communication coprocessor usually requires low-level language. Abstractions. Unlike high-level languages, low-level languages provide complex data structures (e.g., strings objects) control statements (e.g., if-then-else while). Instead,the language forces programmer construct abstractions low-level hardware mechanisms†. 9.4 Assembly Language widely used form low-level programming language known assembly language, software translates assembly language program binary image hardware understands known assembler. important understand phrase assembly language differs phrases Java language C language assembly refer single language. Instead, given assembly language uses instruction set operands single processor. Thus, many assembly languages exist, one processor. Programmers might talk MIPS assembly language Intel x86 assembly language. summarize: assembly language low-level language incorporates specific characteristics processor, instruction set, operand addressing, registers, many assembly languages exist. consequence programmers obvious: moving one processor another, assembly language programmer must learn language. side, instruction set, operand types, register names often differ among assembly languages. positive side, assembly languages tend follow basic pattern. Therefore, programmer learns one assembly language, programmer learn others quickly. important, programmer understands basic assembly language paradigm, moving new architecture usually involves learning new details, learning new programming style. point is: Despite differences, many assembly languages share fundamental structure. Consequently, programmer understands assembly programming paradigm learn new assembly language quickly. help programmers understand concept assembly language, next sections focus general features programming paradigms apply assembly languages. addition specific language details, discuss concepts macros. 9.5 Assembly Language Syntax Opcodes9.5.1 Statement Format assembly language low-level, single assembly language statement corresponds single machine instruction. make correspondence language statements machine instructions clear, assemblers require program contain single statement per line input. general format is: label gives optional label statement (used branching), opcode specifies one possible instructions, operand specifies operand instruction, whitespace separates opcode items. 9.5.2 Opcode Names assembly language given processor defines symbolic name instruction processor provides. Although symbolic names intended help programmer remember purpose instruction, assembly languages use extremely short abbreviations instead long names. Thus, processor instruction addition, assembly language might use opcode add. However, processor instruction branches new location, opcode instruction typically consists single letter, b, two-letter opcode br. Similarly, processor instruction jumps subroutine, opcode often jsr. Unfortunately, global agreement opcode names even basic operations. example, architectures include instruction copies contents one register another. denote operation, assembly languages use opcode mov (an abbreviation move), others use opcode ld (an abbreviation load). 9.5.3 Commenting Conventions Short opcodes tend make assembly language easy write difficult read. Furthermore, low-level, assembly language tends require many instructions achieve straightforward task. Thus, ensure assembly language programs remain readable, programmers add two types comments: block comments explain purpose major section code, detailed comment individual line explain purpose line. make easy programmers add comments, assembly languages often allow comments extend end line. is, language defines character (or sequence characters) starts comment. One commercial assembly language defines pound sign character (#) start comment, second uses semicolon denote start comment, third adopted C++ comment style uses two adjacent slash characters. block comment created line begins comment character, detailed comment added line program. Programmers often add additional characters surround block comment. example, pound sign signals start acomment, block comment explains section code searches list find memory block given size: ################################################################ # # # Search linked list free memory blocks find block # # size N bytes greater. Pointer list must # # register 3, N must register 4. code also # # destroys contents register 5, used # # walk list. # # # ################################################################ programmers place comment line assembly code explain instruction fits algorithm. example, code search memory block might begin: ld r5,r3 # load address list r5 loop_1: cmp r5,r0 # test see end list bz notfnd # reached end list go notfnd ... Although details example may seem obscure, point relatively straightforward: block comment section code explains code accomplishes, comment line code explains particular instruction contributes result. 9.6 Operand Order One frustrating difference among assembly languages causes subtle problems programmers move one assembly language another: order operands. given assembly language usually chooses consistent operand order. example, consider load instruction copies contents one register another register. example code above, first operand represents target register (i.e., register value placed), second operand represents source register (i.e., register value copied). interpretation, statement: ld r5,r3 # load address list r5 copies contents register 3 register 5. mnemonic aid help remember right-to-left interpretation, programmers told think assignment statement expression right target assignment left. alternative example code, assembly languages specify opposite order — source register left target register right. assembly languages, code written operands opposite order: ld r3,r5 # load address list r5As mnemonic aid help remember left-to-right interpretation, programmers told think computer reading instruction. text read left right, imagine computer reading opcode, picking first operand, depositing value second operand. course, underlying hardware process instruction left-to- right right-to-left — operand order assembly language syntax. Operand ordering complicated several factors. First, unlike examples above, many assembly language instructions two operands. example, instruction performs bitwise complement needs one operand. Furthermore, even instruction two operands, notions source destination may apply (e.g., comparison). Therefore, programmer unfamiliar given assembly language may need consult manual find order operands given opcode. course, significant difference programmer writes resulting binary value instruction assembly language merely uses notation convenient programmer. assembler reorder operands translation. example, author worked computer two assembly languages, one produced computer’s vendor another produced researchers Bell Labs. Although languages used produce code underlying computer, one language used left- to-right interpretation operands, used right-to-left interpretation. 9.7 Register Names typical instruction includes reference least one register, assembly languages include special way denote registers. example, many assembly languages, names consist letter r followed one digits reserved refer registers. Thus, reference r10 refers register 10. However, universal standard register references. one assembly language, register references begin dollar sign followed digits; thus, $10 refers register 10. assemblers flexible: assembler allows programmer choose register names. is, programmer insert series declarations define specific name refer register. Thus, one might find declarations as: # # Define register names used program # r1 register 1 # define name r1 register 1 r2 register 2 # r2, r3, r4 r3 register 3 r4 register 4 chief advantage allowing programmers define register names arises increased readability: programmer choose meaningful names. example, suppose program manages linked list. Instead using numbers names like r6, programmer give meaningful names registers: # # Define register names linked list program# listhd register 6 # holds starting address list listptr register 7 # moves along list course, allowing programmers choose names registers also lead unexpected results make code difficult understand. example, consider reading program programmer used following declaration: r3 register 8 # define name r3 register 8! points summarized: registers fundamental assembly language programming, assembly language provides way identify registers. languages, special names reserved; others, programmer assign name register. 9.8 Operand Types Chapter 7 explains, processor often provides multiple types operands. assembly language processor must accommodate operand types hardware offers. example, suppose processor allows operand specify register, immediate value (i.e., constant), memory location, memory location specified adding offset instruction contents register. assembly language processor needs syntactic form possible operand type. said assembly languages often use special characters names distinguish registers values. many assembly languages, example, 10 refers constant ten, r10 refers register ten. However, assembly languages require special symbol constant (e.g., #10 refer constant ten). assembly language must provide syntactic forms possible operand type. Consider, example, copying value source target. processor allows instruction specify either register (direct) memory location (indirect) source, assembly language must provide way programmer distinguish two. One assembly language uses parentheses distinguish two possibilities: mov r2,r1 # copy contents reg. 1 reg. 2 mov r2,(r1) # treat r1 pointer memory # copy mem. location reg. 2 point is: assembly language provides syntactic form possible operand type processor supports, including reference register, immediate value, indirect reference memory.9.9 Assembly Language Programming Paradigm Idioms programming language provides facilities programmers use structure data code, language impact programming process resulting code. Assembly language especially significant language provide high-level constructs language enforce particular style. Instead, assembly language gives programmer complete freedom code arbitrary sequences instructions store data arbitrary memory locations. Experienced programmers understand consistency clarity usually important clever tricks optimizations. Thus, experienced programmers develop idioms: patterns use consistently. next sections use basic control structures illustrate concept assembly language idioms. 9.10 Coding Statement Assembly use term conditional execution refer code may may executed, depending certain condition. conditional execution fundamental part programming, high-level languages usually include one statements allow programmer express conditional execution. basic form conditional execution known statement. assembly language, programmer must code sequence statements perform conditional execution. Figure 9.1 illustrates form used conditional execution typical high-level language equivalent form used typical assembly language. Figure 9.1 (a) Conditional execution specified high-level language, (b) equivalent assembly language code. figure indicates, processors use condition code fundamental mechanism conditional execution. Whenever performs arithmetic operation comparison, ALU sets condition code. conditional branch instruction used test conditioncode execute branch condition code matches instruction. Note case emulating statement, branch instruction must test opposite condition (i.e., branch taken condition met). example, consider statement: assume b stored registers five six, equivalent assembly language is: cmp r5, r6 # compare values b set cc bne lab1 # branch previous comparison equal code x ... lab1: code next statement 9.11 Coding IF-THEN-ELSE Assembly if-then-else statement found high-level languages specifies code executed case condition true condition false. Figure 9.2 shows assembly language equivalent if-then-else statement. Figure 9.2 (a) if-then-else statement used high-level language, (b) equivalent assembly language code. 9.12 Coding FOR-LOOP Assembly term definite iteration refers programming language construct causes piece code executed fixed number times. typical high-level language uses statement implement definite iteration. Figure 9.3 shows assembly language equivalent statement.Definite iteration illustrates interesting difference high-level language assembly language: location code. assembly language, code implement control structure divided separate locations. particular, although programmer thinks initialization, continuation test, increment specified header statement, equivalent assembly code places increment code body. 9.13 Coding Statement Assembly programming language terminology, indefinite iteration refers loop executes zero times. Typically, high-level language uses keyword indicate indefinite iteration. Figure 9.4 shows assembly language equivalent statement. Figure 9.3 (a) statement used high-level language, (b) equivalent assembly language code using register 4 index. Figure 9.4 (a) statement used high-level language, (b) equivalent assembly language code. 9.14 Coding Subroutine Call AssemblyWe use term procedure subroutine refer piece code invoked, perform computation, return control invoker. terms procedure call subroutine call refer invocation. key idea subroutine invoked, processor records location call occurred, resumes execution point subroutine completes. Thus, given subroutine invoked multiple points program control always passes back location invocation occurred. Many processors provide two basic assembly instructions procedure invocation. jump subroutine (jsr) instruction saves current location branches subroutine specified location, return subroutine (ret) instruction causes processor return previously saved location. Figure 9.5 shows two assembly instructions used code procedure declaration two invocations. Figure 9.5 (a) declaration procedure x two invocations high-level language, (b) assembly language equivalent. 9.15 Coding Subroutine Call Arguments Assembly high-level language, procedure calls parameterized. procedure body written references parameters, caller passes set values procedure known arguments. procedure refers parameter, value obtained corresponding argument. question arises: arguments passed procedure assembly code? Unfortunately, details argument passing vary widely among processors. example, following three schemes used least one processor†: processor uses stack memory arguments processor uses register windows pass arguments processor uses special-purpose argument registersAs example, consider processor registers r1 r8 used pass arguments procedure call. Figure 9.6 shows assembly language code procedure call architecture. 9.16 Consequence Programmers consequence variety argument passing schemes clear: assembly language code needed pass reference arguments varies significantly one processor another. important, programmers free invent new mechanisms argument passing optimize performance. example, memory references slower register references. Thus, even hardware designed use stack memory, programmer might choose increase performance passing arguments general-purpose registers rather memory. Figure 9.6 (a) declaration parameterized procedure x two invocations high-level language, (b) assembly language equivalent processor passes arguments registers. point is: single argument passing paradigm used assembly languages variety hardware mechanisms exist argument passing. addition,programmers sometimes use alternatives basic mechanism optimize performance (e.g., passing values registers). 9.17 Assembly Code Function Invocation term function refers procedure returns single-value result. example, arithmetic function created compute sine(x) — argument specifies angle, function returns sine angle. Like procedure, function arguments, function invoked arbitrary point program. Thus, given processor, function invocation uses basic mechanisms procedure invocation. Despite similarities functions procedures, function invocation requires one additional detail: agreement specifies exactly function result returned. argument passing, many alternative implementations exist. Processors built provide separate, special-purpose hardware register function return value. processors assume program use one general-purpose registers. case, executing ret instruction, function must load return value location processor uses. return occurs, calling program extracts uses return value. 9.18 Interaction Assembly High-level Languages Interaction possible either direction code written assembly language code written high-level language. is, program written high-level language call procedure function written assembly language, program written assembly language call procedure function written high-level language. course, programmer control assembly language code high- level language code, assembly program must follow calling conventions high- level language uses. is, assembly code must use exactly mechanisms high- level language uses store return address, invoke procedure, pass arguments, return function value. would programmer mix code written assembly language code written high- level language? cases, assembly code needed high-level language allow direct interaction underlying hardware. example, computer special graphics hardware may need assembly code use graphics functions. cases, however, assembly language used optimize performance — programmer identifies particular piece code bottleneck, programmer writes optimized version code assembly language. Typically, optimized assembly language code placed procedure function; rest program remains written high-level language. result, common case interaction code written high-level language code written assembly language consists program written high-level language calling procedure function written assembly language.The point is: writing application programs assembly language difficult, assembly language reserved situations high-level language insufficient functionality results poor performance. 9.19 Assembly Code Variables Storage addition statements generate instructions, assembly languages permit programmer define data items. initialized uninitialized variables declared. example, assembly languages use directive .word declare storage sixteen-bit item, directive .long declare storage thirty-twobit item. Figure 9.7 shows declarations high-level language equivalent assembly code. Figure 9.7 (a) Declaration variables high-level language, (b) equivalent variable declarations assembly language. keywords .word .long known assembly language directives. Although appears location opcode appears, directive correspond instruction. Instead, directive controls translation. directives figure specify storage locations reserved hold variables. assembly languages, directive reserves storage also allows programmer specify initial value. Thus, directive: x: .word 949 reserves sixteen bit memory location, assigns location integer value 949, defines x label (i.e., name) programmer use refer location. 9.20 Example Assembly Language CodeAn example help clarify concepts show assembly language idioms apply practice. help compare x86 ARM architectures, use example architecture. make example clear, begin C program show algorithm implemented assembly language. Instead using long, complex program show idioms, use trivial example demonstrates basics. particular, show indefinite iteration conditional execution. example consists piece code prints initial list Fibonacci sequence. first two values sequence 1. successive value computed sum preceding two values. Thus, sequence 1, 1, 2, 3, 5, 8, 13, 21, on. ensure example relates concepts computer architecture, arrange code print values Fibonacci sequence fit two’s complement thirty-two-bit signed integer. sequence generated, code count number values greater 1000, print summary. 9.20.1 Fibonacci Example C Figure 9.8 shows C program computes value Fibonacci sequence fits thirty-two-bit signed integer. program uses printf print value. also counts number values greater 1000, uses printf print total well summary final values variables used computation.Figure 9.8 example C program computes prints values Fibonacci sequence fit thirty- two-bit signed integer. Figure 9.9 shows output results program runs. last line output gives value variables a, b, tmp loop finishes. Variable (1,836,311,903 decimal) 6D73E55F hex. Notice variable tmp value B11924E1, high-order bit set. Chapter 3 explains, tmp interpreted signed integer, value negative, loop terminated. Also note variable n, counts number Fibonacci values final value 30; value verified counting lines output values greater 1000.Figure 9.9 output results running program Figure 9.8.9.20.2 Fibonacci Example x86 Assembly Language Figure 9.10 shows x86 assembly code generates output program Figure 9.8. code uses gcc calling conventions call printf. Figure 9.10 x86 assembly language program follows C program shown Figure 9.8. 9.20.3 Fibonacci Example ARM Assembly Language Figure 9.11 shows ARM assembly code generates output program Figure 9.8. Neither x86 ARM code optimized. case, instructions eliminated keeping variables registers. example, small amount optimization done ARM code: registers r4 r8 initialized contain addresses variables a, b, n, tmp, format string fmt1. registers remain unchanged program runs called subprograms required save restore values. Thus, calling printf print variable a, code use single instruction move address format first argument register (r0): code also use single instruction load value second argument register (r1): Exercises suggest ways improve code.Figure 9.11 ARM assembly language program follows algorithm shown Figure 9.8.9.21 Two-Pass Assembler use term assembler refer piece software translates assembly language programs binary code processor execute. Conceptually, assembler similar compiler takes source program input produces equivalent binary code output. assembler differs compiler, however, compiler significantly responsibility. example, compiler choose allocate variables memory, sequence instructions use statement, values keep general- purpose registers. assembler cannot make choices source program specifies exact details. difference assembler compiler summarized: Although compiler assembler translate source program equivalent binary code, compiler freedom choose values kept registers, instructions used implement statement, allocation variables memory. assembler merely provides one-to-one translation statement source program equivalent binary form. Conceptually, assembler follows two-pass algorithm, means assembler scans source program two times. understand two passes needed, observe many branch instructions contain forward references (i.e., label referenced branch defined later program). assembler first reaches branch statement, assembler cannot know address associated label. Thus, assembler makes initial pass, computes address label final program, stores information table known symbol table. assembler makes second pass generate code. Figure 9.12 illustrates idea showing snippet assembly language code relative location statements. Figure 9.12 snippet assembly language code locations assigned statement hypothetical processor. Locations determined assembler’s first pass. first pass, assembler computes size instructions without actually filling details. assembler completed first pass, assembler recorded location statement. Consequently, assembler knows value label program. figure, example, assembler knows label4 starts location 0x20 (32 decimal). Thus, second pass assembler encounters statement: assembler generate branch instruction 32 immediate operand. Similarly, code generated branch instructions second pass location label known. important understand details assembler, merely know that: Conceptually, assembler makes two passes assembly language program. first pass, assembler assigns location statement. second pass, assembler uses assigned locations generate code. understand assembler works, discuss one chief advantages using assembler: automatic recalculation branch addresses. see automatic recalculation helps, consider programmer working program. programmer inserts statement program, location successive statement changes. result, every branch instruction refers label beyond insertion point must changed. Without assembler, changing branch labels tedious prone errors. Furthermore, programmers often make series changes debugging program. assembler allows programmer make change easily — programmer merely reruns assembler produce binary image branch addresses updated. 9.22 Assembly Language Macros assembly language low-level, even trivial operations require many instructions. important, assembly language programmer often finds sequences code repeated minor changes instances. Repeated sequences code make programming tedious, lead errors programmer uses cut-and-paste approach. help programmers avoid repetitious coding, many assembly languages include parameterized macro facility. use macro facility, programmer adds two types items source program: one macro definitions one macro expansions. Note: C programmers recognize assembly language macros operate like C preprocessor macros.In essence, macro facility adds extra pass assembler. assembler makes initial pass macros expanded. important concept macro expansion pass parse assembly language statements handle translation instructions. Instead, macro processing pass takes input assembly language source program contains macros, produces output assembly language source program macros expanded. is, output macro preprocessing pass becomes input normal two-pass assembler. Many assemblers option allows programmer obtain copy expanded source code use debugging (i.e., see macro expansion proceeding programmer planned). Although details assembly language macros vary across assembly languages, concept straightforward. macro definition usually bracketed keywords (e.g., macro endmacro), contains sequence code. example, Figure 9.13 illustrates definition macro named addmem adds contents two memory locations places result third location. Figure 9.13 example macro definition using keywords macro endmacro. Items macro refer parameters a, b, c. macro defined, macro expanded. programmer invokes macro supplies set arguments. assembler replaces macro call copy body macro, substituting actual arguments place formal parameters. example, Figure 9.14 shows assembly code generated expansion addmem macro defined Figure 9.13. Figure 9.14 example assembly code results expansion macro addmem. important understand although macro definition Figure 9.13 resembles procedure declaration, macro operate like procedure. First, declaration macro generate machine instructions. Second, macro expanded, called. is, complete copy macro body copied assembly program. Third, macroarguments treated strings replace corresponding parameter. literal substitution arguments especially important understand yield unexpected results. example, consider Figure 9.15 illustrates illegal assembly program result macro expansion. Figure 9.15 example illegal program result expansion macro addmem. assembler substitutes arguments without checking validity. figure shows, arbitrary string used argument macro, means programmer inadvertently make mistake. warning issued assembler processes expanded source program. example, first argument example consists string 1+, syntax error. expands macro, assembler substitutes specified string results in: Similarly, substitution second argument, %*J, results in: makes sense. However, errors detected macro expander run assembler attempts assemble program. important, macro expansion produces source program, error messages refer line numbers reference lines expanded program, original source code programmer submits. point is: macro expansion facility preprocesses assembly language source program produce another source program macro invocation replaced text macro. macro processor uses textual substitution, incorrect arguments detected macro processor; errors detected assembler macro processor completes. 9.23 SummaryAssembly languages low-level languages incorporate characteristics processor, instruction set, operand addressing modes, registers. Many assembly languages exist, one type processor. Despite differences, assembly languages follow basic structure. statement assembly language corresponds single instruction underlying hardware; statement consists optional label, opcode, operands. assembly language processor defines syntactic form type operand processor accepts. Although assembly languages differ, follow basic paradigm. Therefore, specify typical assembly language sequences conditional execution, conditional execution alternate paths, definite iteration, indefinite iteration. processors include instructions used invoke subroutine function return caller. details argument passing, return address storage, return values caller differ. processors place arguments memory, others pass arguments registers. assembler piece software translates assembly language source program binary code processor execute. Conceptually, assembler makes two passes source program: one assign addresses one generate code. Many assemblers include macro facility help programmers avoid tedious coding repetition; macro expander generates source program assembled. uses textual substitution, macro expansion result illegal code detected reported two main passes assembler. EXERCISES 9.1 State explain characteristics low-level language. 9.2 might programmer expect find comments assembly language program? 9.3 program contains if-then-else statement, many branch instructions performed condition true? condition false? 9.4 assembly language used implement repeat statement? 9.5 Name three argument passing mechanisms used commercial processors. 9.6 Write assembly language function takes two integer arguments, adds them, returns result. Test function calling C. 9.7 Write assembly language program declares three integer variables, assigns 1, 2, 3, calls printf format print values. 9.8 Programmers sometimes mistakenly say assembler language. confused, term use? 9.9 Figure 9.12, instruction inserted following label4 jumps label2, address jump? address change new instruction inserted label1? 9.10 Look Figure 9.8 see example Fibonacci program written C. program redesigned faster? How?9.11 Optimize Fibonacci programs Figures 9.10 9.11 choosing keep values registers rather writing memory. Explain choices. 9.12 Compare x86 ARM versions Fibonacci program Figures 9.10 9.11. version expect require code? Why? 9.13 Use -S option gcc generate assembly code C program. example, try program Figure 9.8. Explain extra code generated. 9.14 chief disadvantage using assembly language macro instead function? †Computer scientist Alan Perlis quipped programming language low-level programming requires attention irrelevant details. point applications need direct control, using low-level language creates overhead application programmer without real benefit. †The storage used return address (i.e., location ret instruction branch) often related storage used arguments.Part III Memories Program Data Storage Technologies10 Memory Storage Chapter Contents 10.1 Introduction 10.2 Definition 10.3 Key Aspects Memory 10.4 Characteristics Memory Technologies 10.5 Important Concept Memory Hierarchy 10.6 Instruction Data Store 10.7 Fetch-Store Paradigm 10.8 Summary10.1 Introduction Previous chapters examine one major components used computer systems: processors. chapters review processor architectures, including instruction sets, operands, structure complex CPUs. chapter introduces second major component used computer systems: memories. Successive chapters explore basic forms memory: physical memory, virtual memory, caches. Later chapters examine I/O, show I/O devices use memory. 10.2 Definition programmers think memory, usually focus main memory found conventional computer. programmer’s point view, main memory holds running programs well data programs use. broader sense, computer systems use storage hierarchy includes general-purpose registers, main memory, secondary storage (e.g., disk flash storage). Throughout text, use term memory refer specifically main memory, generally use term storage broader hierarchy abstractions programmers use hierarchy. architect views memory solid-state digital device provides storage data values. next sections clarify concept examining variety possibilities. 10.3 Key Aspects Memory architect begins design memory system, two key choices arise: Technology Organization Technology refers properties underlying hardware mechanisms used construct memory system. learn many technologies available, see examples properties. also learn basic technologies operate, understand technology appropriate. Organization refers way underlying technology used form working system. see many choices combine one-bit memory cell multibit memory cells, learn multiple ways map memory address underlying units. essence, memory technology refers lowest-level hardware pieces (i.e., individual chips), memory organization refers pieces combined create meaningful storage systems. see aspects contribute cost performance memory system.10.4 Characteristics Memory Technologies Memory technology easy define wide range technologies invented. help clarify broad purpose intent given type memory, engineers use several characteristics: Volatile nonvolatile Random sequential access Read-write read-only Primary secondary 10.4.1 Memory Volatility memory classified volatile contents memory disappear power removed. main memory used computers (RAM) volatile — computer shut down, running applications data stored main memory vanish. contrast, memory known nonvolatile contents survive even power removed†. example, flash memory used digital cameras Solid State Disks (SSDs) nonvolatile — data stored camera disk remains intact power turned off. fact, data remains even storage device removed camera computer. 10.4.2 Memory Access Paradigm common forms memory classified random access, means value memory accessed fixed amount time independent location sequence locations accessed. term Random Access Memory (RAM) common consumers look RAM purchase computer. alternative random access sequential access time access given value depends location value memory location previously accessed value (typically, accessing next sequential location memory much faster accessing location). example, one type sequential access memory consists FIFO† queue implemented hardware. 10.4.3 Permanence Values Memory characterized whether values extracted, updated, both. primary form memory used conventional computer system permits arbitrary value memory accessed (read) updated (written) time. forms memory provide permanence. example, memory characterized Read Memory (ROM) memory contains data values accessed, cannot changed. form ROM, Programmable Read Memory (PROM), designed allow data values stored memory accessed many times. extreme case, PROMcan written — high voltage used alter chip permanently. Intermediate forms permanence also exist. example, flash memory commonly used smart phones solid state disks represents compromise permanent ROM technologies little permanence — although retains data power removed, items flash memory last forever. exercise asks reader research flash technologies discover long data last flash device sits idle. 10.4.4 Primary Secondary Memory terms primary memory secondary memory qualitative. Originally, terms used distinguish fast, volatile, internal main memory computer slower, nonvolatile storage provided external electromechanical device hard disk. However, many computer systems use solid-state memory technologies primary secondary storage. particular, Solid State Disks (SSDs) used secondary storage. 10.5 Important Concept Memory Hierarchy notions primary secondary memory arise part memory hierarchy computer system. understand hierarchy, must consider performance cost: memory highest performance characteristics also expensive. Thus, architect must choose memory satisfies cost constraints. Research memory use led interesting principle: given cost, optimal performance achieved using one type memory throughout computer. Instead, set technologies arranged conceptual memory hierarchy. hierarchy small amount highest performance memory, slightly larger amount slightly slower memory, on. example, architect selects small number general-purpose registers, larger amount primary memory, even larger amount secondary memory. summarize principle: optimize memory performance given cost, set technologies arranged hierarchy contains relatively small amount fast memory larger amounts less expensive, slower memory. Chapter 12 examines concept memory hierarchy. chapter presents scientific principle behind hierarchical structure, explains memory mechanism known cache uses principle achieve higher performance without high cost. 10.6 Instruction Data StoreRecall earliest computer systems used Harvard Architecture separate memories programs data. Later, architects adopted Von Neumann Architecture single memory holds programs data. Interestingly, advent specialized solid state memory technologies reintroduced separation program data memory — special-purpose systems sometimes use separate memories. Memory used hold program known instruction store, memory used hold data known data store. One motivations separate instruction store comes notion memory hierarchy: many systems, overall performance increased increasing speed instruction store. understand why, observe high-speed instructions designed operate values general-purpose registers rather values memory. Thus, optimize speed, data kept registers whenever possible. However, instruction must accessed iteration fetch-execute cycle. Thus, instruction store experiences activity data store. important, although data accesses tend follow random pattern accessing variable another variable, processor typically accesses instruction store sequentially. is, instructions placed one another memory, processor moves one next unless branch occurs. Separating two memories allows designer optimize instruction store sequential access. summarize: Although modern computer systems place programs data single memory, possible separate instruction store data store. allows architect select memory performance appropriate activity. 10.7 Fetch-Store Paradigm see, memory technologies use single paradigm known fetch-store. now, important understand two basic operations associated paradigm: fetching value memory storing value memory. fetch operation sometimes called read load. think memory array locations, view reading value memory array index operation memory address used: analogy also holds store operations, sometimes called write operations. is, view storing value memory storing value array: next chapter explains idea detail. Later chapters I/O explain fetch- store paradigm used input output devices, underlying memory access relates I/O. 10.8 Summary two key aspects memory underlying technology organization. variety technologies exist; characterized volatile nonvolatile, random sequential access, permanent nonpermanent (read-only read-write), primary secondary. achieve maximal performance given cost, architect organizes memory conceptual hierarchy. hierarchy contains small amount high performance memory large amount lower performance memory. Memory systems use fetch-store paradigm. memory hardware supports two operations: one retrieves value memory another stores value memory. EXERCISES 10.1 Define storage hierarchy give example. 10.2 two key choices architect makes designing memory system? 10.3 Read RAM SSD technologies typical computer. approximate financial cost per byte type memory? 10.4 Extend previous exercise finding speed (access times) memories comparing financial cost performance. 10.5 type memory secure (i.e., less susceptible someone trying change contents), flash memory ROM? 10.6 data stored volatile memory, happens data power removed? 10.7 Suppose NVRAM replace DRAM. characteristic memory technologies becomes less important (or even disappears)? 10.8 Compare performance NVRAM traditional RAM. much slower NVRAM? 10.9 Research flash technology used typical USB flash drives also called jump drives thumb drives. flash drive left unused, long data persist? surprised answer? 10.10 Registers much faster main memory, means program could run much faster data kept registers instead main memory. designers create processors registers? 10.11 computer follows Harvard Architecture, expect find two identical memories, one instructions one data? not? 10.12 terms fetch-execute fetch-store refer concept? Explain. †An emerging technology known NonVolatile RAM (NVRAM) operates like traditional main memory, retains values power removed. †FIFO abbreviates First-In-First-Out.11 Physical Memory Physical Addressing Chapter Contents 11.1 Introduction 11.2 Characteristics Computer Memory 11.3 Static Dynamic RAM Technologies 11.4 Two Primary Measures Memory Technology 11.5 Density 11.6 Separation Read Write Performance 11.7 Latency Memory Controllers 11.8 Synchronous Multiple Data Rate Technologies 11.9 Memory Organization 11.10 Memory Access Memory Bus 11.11 Words, Physical Addresses, Memory Transfers 11.12 Physical Memory Operations 11.13 Memory Word Size Data Types 11.14 Byte Addressing Mapping Bytes Words 11.15 Using Powers Two 11.16 Byte Alignment Programming 11.17 Memory Size Address Space 11.18 Programming Word Addressing 11.19 Memory Size Powers Two 11.20 Pointers Data Structures11.21 Memory Dump 11.22 Indirection Indirect Operands 11.23 Multiple Memories Separate Controllers 11.24 Memory Banks 11.25 Interleaving 11.26 Content Addressable Memory 11.27 Ternary CAM 11.28 Summary 11.1 Introduction previous chapter introduces topic memory, lists characteristics memory systems, explains concept memory hierarchy. chapter explains basic memory system operates. chapter considers underlying technologies used construct typical computer memory organization memory bytes words. next chapter expands discussion consider virtual memory. 11.2 Characteristics Computer Memory Engineers use term Random Access Memory (RAM) denote type memory used primary memory system computers. name implies, RAM optimized random (as opposed sequential) access. addition, RAM offers read-write capability makes access update equally inexpensive. Finally, see RAM volatile — values persist computer powered down. 11.3 Static Dynamic RAM Technologies technologies used implement Random Access Memory divided two broad categories. Static RAM (SRAM†) easiest type programmers understand straightforward extension digital logic. Conceptually, SRAM stores data bit latch, miniature digital circuit composed multiple transistors similar latch discussed Chapter 2. Although internal implementation beyond scope text, Figure 11.1 illustrates three external connections used single-bit RAM.Figure 11.1 Illustration miniature static RAM circuit stores one data bit. circuit contains multiple transistors. figure, circuit two inputs one output. write enable input (i.e., logical 1), circuit sets output value equal input (0 1); write enable input (i.e., logical 0), circuit ignores input keeps output last setting. Thus, store value, hardware places value input, turns write enable line, turns enable line again. Although performs high speed, SRAM significant disadvantage: high power consumption (which generates heat). miniature SRAM circuit contains multiple transistors operate continuously. transistor consumes small amount power, therefore, generates small amount heat. alternative static RAM, known Dynamic RAM (DRAM‡), consumes less power. internal working dynamic RAM surprising confusing. lowest level, store information, DRAM uses circuit acts like capacitor, device stores electrical charge. value written DRAM, hardware charges discharges capacitor store 1 0. Later, value read DRAM, hardware examines charge capacitor generates appropriate digital value. conceptual difficulty surrounding DRAM arises way capacitor works: physical systems imperfect, capacitor gradually loses charge. essence, DRAM chip imperfect memory device — time passes, charge dissipates one becomes zero. important, DRAM loses charge short time (e.g., cases, second). DRAM used computer memory values quickly become zero? answer lies simple technique: devise way read bit memory charge time dissipate, write value back again. Writing value causes capacitor start appropriate charge. So, reading writing bit reset capacitor without changing value bit. practice, computers use DRAM contain extra hardware circuit, known refresh circuit, performs task reading writing bit. Figure 11.2 illustrates concept.Figure 11.2 Illustration bit dynamic RAM. external refresh circuit must periodically read data value write back again, charge dissipate value lost. refresh circuit complex figure implies. keep refresh circuit small, architects build one refresh circuit bit. Instead, single, small refresh mechanism designed cycle entire memory. reaches bit, refresh circuit reads bit, writes value back, moves on. Complexity also arises refresh circuit must coordinate normal memory operations. First, refresh circuit must interfere delay normal memory operations. Second, refresh circuit must ensure normal write operation change bit time refresh circuit reads bit time refresh circuit writes value back. Despite need refresh circuit, cost power consumption advantages DRAM beneficial computer memory composed DRAM rather SRAM. 11.4 Two Primary Measures Memory Technology Architects use several quantitative measures assess memory technology; two stand out: Density Latency cycle times 11.5 Density strict sense, term density refers number memory cells per square area silicon. practice, however, density often refers number bits represented standard size chip plug-in module. example, Dual In-line Memory Module (DIMM) might contain set chips offer 128 million locations 64 bits per location, equals 8.192 billion bits one Gigabyte. Informally, known 1 gig module. Higher density usually desirable means memory held physical space.However, higher density disadvantages increased power utilization increased heat generation. density memory chips related size transistors underlying silicon technology, followed Moore’s Law. Thus, memory density tends double approximately every eighteen months. 11.6 Separation Read Write Performance second measure memory technology focuses speed: fast memory respond requests? may seem speed easy measure, not. example, previous chapter discusses, memory technologies take much longer write values read them. choose appropriate memory technology, architect needs understand cost access cost update. Thus, principle arises: many memory technologies, time required fetch information memory differs time required store information memory, difference dramatic. Therefore, measure memory performance must give two values: performance read operations performance write operations. 11.7 Latency Memory Controllers addition separating read write operations, must decide exactly measure. may seem important measure latency (i.e., time elapses start operation completion operation). However, latency simplistic measure provide complete information. see latency suffice measure memory performance, need understand hardware works. addition memory chips themselves, additional hardware known memory controller† provides interface processor memory. Figure 11.3 illustrates organization. Figure 11.3 Illustration hardware used memory access. controller sits processor physical memory. access memory, device (typically processor) presents read write request controller. controller translates request signals appropriate underlying memory, passes signals memory chips. minimize latency, controller returns answer quickly possible (i.e., soon memory responds). However, responds device, controller may need additional clock cycle(s) reset hardware circuits prepare next operation. second principle memory performance arises: memory system may need extra time operations, latency insufficient measure performance; performance measure needs measure time required successive operations. is, assess performance memory system, need measure fast system perform sequence operations. Engineers use term memory cycle time capture idea. Specifically, two separate measures used: read cycle time (abbreviated tRC) write cycle time (abbreviated tWC). summarize: read cycle time write cycle time used measures memory system performance assess quickly memory system handle successive requests. 11.8 Synchronous Multiple Data Rate Technologies Like digital circuits computer, memory system uses clock controls exactly read write operation begins. Figure 11.3 indicates, memory system must also coordinate processor. controller may also coordinate I/O devices. happens processor’s clock differs clock used memory? system still works controller hold request processor response memory side ready. Unfortunately, difference clock rates impact performance — although delay small, delay occurs every memory reference, accumulated effect large. eliminate delay, memory systems use synchronous clock system. is, clock pulses used memory system aligned clock pulses used run processor.As result, processor need wait memory references complete. Synchronization used DRAM SRAM; two technologies named: practice, synchronization effective; computers use synchronous DRAM primary memory technology. many computer systems, memory bottleneck — increasing memory performance improves overall performance. result, engineers concentrated finding memory technologies lower cycle times. One approach uses technique runs memory system multiple normal clock rate (e.g., double quadruple). clock runs faster, memory deliver data faster. technologies sometimes called fast data rate memories, typically double data rate quadruple data rate. Fast data rate memories successful, standard computer systems, including consumer systems laptops. Although covered highlights, discussion RAM memory technology begin illustrate range choices available architect detailed differences among them. example, Figure 11.4 lists commercially available RAM technologies: Figure 11.4 Examples commercially available RAM technologies. Many technologies exist. 11.9 Memory Organization Recall two key aspects memory: underlying technology memory organization. seen, architect choose variety memory technologies; consider second aspect. Memory organization refers internal structureof hardware external addressing structure memory presents processor. see two related. 11.10 Memory Access Memory Bus understand memory organized, need examine access paradigm. Recall Figure 11.3 memory controller provides interface physical memory processor uses memory†. Several questions arise. structure connection processor memory? values pass across connection? processor view memory system? achieve high performance, memory systems use parallelism: connection processor controller consists many wires used simultaneously. wire transfer one data bit time. Figure 11.5 illustrates concept. Figure 11.5 parallel connection processor memory. connection contains N wires allows N bits data transferred simultaneously. technical name hardware connection processor memory bus (more specifically, memory bus). learn buses chapters I/O; now, sufficient understand bus provides parallel connections. 11.11 Words, Physical Addresses, Memory Transfers parallel connections memory bus pertinent programmers well computer architects. architectural standpoint, using parallel connections improve performance. programming point view, parallel connections define memory transfer size (i.e., amount data read written memory single operation). see transfer size crucial aspect memory organization. permit parallel access, bits comprise physical memory divided blocks N bits per block, N memory transfer size. block N bits sometimes called word, transfer size called word size width word. think memory organized array. entry array assigned unique index known physical memory address; approach known word addressing. Figure 11.6 illustrates idea shows physical memory address exactly like array index.Figure 11.6 Physical memory addressing computer word thirty-two bits. think memory array words. 11.12 Physical Memory Operations controller physical memory supports two operations: read write. case read operation, processor specifies address; case write operation, processor specifies address well data written. fundamental idea controller always accepts delivers entire word; physical memory hardware provide way read write less complete word (i.e., hardware allow processor access alter part word). point is: Physical memory organized words, word equal memory transfer size. read write operation applies entire word. 11.13 Memory Word Size Data Types Recall parallel connection processor memory designed high performance. theory, performance increased adding parallel wires. example, interface 128 wires transfer data twice rate interface 64 wires. question arises: many wires architect choose? is, wordsize optimal? question complicated several factors. First, memory used store data, word size accommodate common data values (e.g., word large enough hold integer). Second, memory used store programs, word size accommodate frequently used instructions. Third, connection processor memory requires pins processor, adding wires interface increases pin requirements (the number pins limiting factor design CPU chip). Thus, word size chosen compromise performance various considerations. word size thirty-two bits popular, especially low-power systems; many high- performance systems use sixty-four-bit word size. cases, architect designs parts computer system work together. Thus, architect chooses memory word size equal thirty-two bits, architect make standard integer single-precision floating point value occupy thirty-two bits. result, computer system often characterized stating word size (e.g., thirty-two-bit processor). 11.14 Byte Addressing Mapping Bytes Words Programmers use conventional computer may surprised learn physical memory organized words programmers familiar alternate form addressing known byte addressing. Byte addressing especially convenient programming gives programmer easy way access small data items characters. Conceptually, byte addressing used, memory must organized array bytes rather array words. choice byte addressing two important consequences. First, byte memory assigned address, byte addressing requires addresses word addressing. Second, byte addressing allows program read write single byte, memory controller must support byte transfer. larger word size results higher performance many bits transferred time. Unfortunately, word size equal eight-bit byte, eight bits transferred one time. is, memory system built byte addressing lower performance memory system built larger word size. Interestingly, even byte addressing used, many transfers processor memory involve multiple bytes. example, instruction occupies multiple bytes, integer, floating point value, pointer. devise memory system combines higher speed word addressing programming convenience byte addressing? answer yes. so, need intelligent memory controller translate two addressing schemes. controller accepts requests processor specify byte address size. controller uses word addressing access appropriate word(s) underlying memory extract specified bytes. Figure 11.7 shows example mapping used byte addressing word addressing word size thirty-two bits.Figure 11.7 Example byte address assigned byte memory even though underlying hardware uses word addressing thirty-two-bit word size. implement mapping shown figure, controller must convert byte addresses issued processor word addresses used memory system. example, processor requests read operation byte address 17, controller must issue read request word 4, extract second byte word. memory transfer entire word time, byte write operation expensive. example, processor writes byte 11, controller must read word 2 memory, replace rightmost byte, write entire word back memory. Mathematically, translation addresses straightforward. translate byte address, B, corresponding word address, W, controller divides B N, number bytes per word, ignores remainder. Similarly, compute byte offset, O, within word, controller computes remainder B divided N. is, word address given by: offset given by: example, consider values Figure 11.7, N = 4. byte address 11 translates word address 2 offset 3, means byte 11 found word 2 byte offset 3†. 11.15 Using Powers TwoPerforming division computing remainder time consuming requires extra hardware (e.g., Arithmetic Logic Unit). avoid computation, architects organize memory using powers two. means hardware perform two computations simply extracting bits. Figure 11.7, example, N=22, means offset computed extracting two low-order bits, word address computed extracting everything except two low-order bits. Figure 11.8 illustrates idea: Figure 11.8 example mapping byte address 17 word address 4 offset 1. Using power two number bytes per word avoids arithmetic calculations. summarize: avoid arithmetic calculations, division remainder, physical memory organized number bytes per word power two, means translation byte address word address offset performed extracting bits. 11.16 Byte Alignment Programming Knowing underlying hardware works helps explain concept programmers encounter: byte alignment. say integer value aligned bytes integer correspond word underlying physical memory. Figure 11.7, example, integer composed bytes 12, 13, 14, 15 aligned, integer composed bytes 6, 7, 8, 9 not. architectures, byte alignment required — processor raises error program attempts integer access using unaligned address. processors, arbitrary alignment allowed, unaligned accesses result lower performance aligned accesses. understand unaligned address requires accesses physical memory: memory controller must convert processor request operations underlying memory. integer spans two words, controller must perform two read operations obtain requested bytes. Thus, even processor permits unaligned access, programmers strongly encouraged align data values. summarize:The organization physical memory affects programming: even processor allows unaligned memory access, aligning data boundaries correspond physical word size improve program performance. 11.17 Memory Size Address Space large memory be? may seem memory size economic issue — memory simply costs money. However, size turns essential aspect memory architecture overall memory size inherently linked design choices. particular, addressing scheme determines maximum memory size. Recall data paths processor consist parallel hardware. processor designed, designer must choose size data path, register, hardware units. choice places fixed bound size address generated passed one unit another. Typically, address size integer size. example, processor uses thirty-two-bit integers uses thirty-two-bit addresses processor uses sixty-four-bit integers uses sixty-four-bit addresses. Chapter 3 points out, string k bits represent 2k values. Thus, thirty-two-bit value represent: 232 = 4,294,967,296 unique addresses (i.e., addresses 0 4,294,967,295). use term address space denote set possible addresses. tradeoff byte addressing word addressing clear: given fixed address size, amount memory addressed depends whether processor uses byte addressing word addressing. Furthermore, word addressing used, amount memory addressed depends word size. example, computer uses word addressing four bytes per word, thirty-twobit value hold enough addresses 17,179,869,184 bytes (i.e., four times much byte addressing used). 11.18 Programming Word Addressing Many processors use byte addressing byte addressing provides convenient interface programmers. However, byte addressing maximize memory size. Therefore, specialized systems, processors designed numeric processing, use word addressing provide access maximum amount memory given address size. processor uses word addressing, software must handle details byte manipulation. essence, software performs function memory controller byte- addressed architecture. example, extract single byte, software must read appropriateword memory, extract byte. Similarly, write byte, software must read word containing byte, update correct byte, write modified word back memory. optimize software performance, logical shifts bit masking used manipulate address rather division remainder computation. Similarly, shifts logical operations used extract bytes word. example, extract leftmost byte thirty-two- bit word, w, programmer code C statement: ( w >> 24 ) & 0xff code performs logical constant 0xff ensure low-order eight bits kept shift performed. understand logical needed, recall Chapter 3 right shift propagates sign bit. Thus, w contains 0xa1b2c3d2, expression w >> 24 produce 0xffffffa1. logical and, result 0xa1. 11.19 Memory Size Powers Two said physical memory architecture characterized follows: Physical memory organized set words contain N bytes; make controller hardware efficient, N chosen powers two. use powers two word address space size interesting consequence: maximum amount memory always power two rather power ten. result, memory size measured powers two. example, Kilobyte (Kbyte) defined consist 210 bytes, Megabyte (MB) defined consist 220 bytes, Gigabyte (GB) defined consist 230 bytes. terminology confusing exception. computer networking, example, measure Megabits per second refers base ten. Thus, one must careful mixing memory size measures (e.g., although eight bits per byte, one Kilobyte data memory eight times larger one Kilobit data sent across network). summarize: used refer memory, prefixes kilo, mega, giga defined powers 2; used aspects computing, computer networking, prefixes defined powers 10. 11.20 Pointers Data StructuresMemory addresses important form basis commonly used data abstractions, linked lists, queues, trees. Consequently, programming languages often provide facilities allow programmer declare pointer variable holds memory address, assign value pointer, dereference pointer obtain item. C programming language, example, declaration: char *cptr; declares variable cptr pointer character (i.e., byte memory). compiler allocates storage variable cptr equal size memory address, allows variable assigned address arbitrary byte memory. autoincrement statement: cptr + + ; increases value cptr one (i.e., moves next byte memory). Interestingly, C programming language heritage byte word addressing. performing arithmetic pointers, C accommodates size underlying item. example, declaration: int *iptr; declares variable iptr pointer integer (i.e., pointer word). compiler allocates storage variable iptr equal size memory address (i.e., size allocated cptr above). However, program compiled run processor defines integer four bytes, autoincrement statement: iptr + + ; increases value iptr four. is, iptr declared byte address beginning word memory, autoincrement moves byte address next word memory. fact, examples assume byte-addressable computer. compiler generates code increment character pointer one integer pointer four. Although C facilities allow pointer move one word next, language intended use byte-addressable memory. 11.21 Memory Dump trivial example help us understand relationship pointers memory addresses. Consider linked list Figure 11.9 illustrates.Figure 11.9 Example linked list. pointer list corresponds memory address. create list, programmer must write declaration specifies contents node, must allocate memory hold list. trivial example, node list contain two items: integer count pointer next node list. C, struct declaration used define contents node: struct node { int count; struct node *next; } Similarly, variable named head serves head list defined as: struct node *head; understand list appears memory, consider memory dump Figure 11.10 illustrates†. Figure 11.10 Illustration small portion memory dump shows contents memory. address column gives memory address leftmost byte line, values shown hexadecimal. example figure taken processor uses byte addressing. line figure corresponds sixteen contiguous bytes memory divided four groups four bytes. group contains eight hexadecimal digits represent values four bytes. address beginning line specifies memory address first byte line. Therefore, address line sixteen greater address previous line. Assume head linked list found address 0x0001bde4, located first line dump. first node list starts address 0x0001bdf8, located second line dump, contains integer 192 (hexadecimal constant 000000c0). processor uses byte addressing, bytes memory contiguous. figure, spacing inserted divide output groups bytes improve readability.Specifically, example shows groups four-byte units, implies underlying word size four bytes (i.e., thirty-two bits). 11.22 Indirection Indirect Operands discussed operands addressing modes Chapter 7, topic indirection arose. understand memory organization, understand processor evaluates indirect operand. example, suppose processor executes instruction operand specifies immediate value 0x1be1f, specifies indirection. suppose processor designed use thirty-two-bit values. operand specifies immediate value, processor first loads immediate value (hexadecimal 1be1f). operand specifies indirection, processor treats resulting value address memory, fetches word address. values memory correspond values shown Figure 11.10, processor load value rightmost word last line figure, final operand value 6. 11.23 Multiple Memories Separate Controllers discussion physical memory assumed single memory single memory controller. practice, however, architectures contain multiple physical memories. multiple memories used, hardware parallelism employed provide higher memory performance. Instead single memory single controller, memory system multiple controllers operate parallel, Figure 11.11 illustrates. figure, interface hardware receives requests processor. interface uses address request decide memory used, passes request appropriate memory controller†. help multiple memories, controller? Remember memory accessed, hardware must reset next access occur. two memories available, programmer arrange access one resets, increasing overall performance. is, memory controllers operate parallel, using two memory controllers allows memory accesses occur per unit time. Harvard Architecture, example, higher performance results instruction fetch interfere data access vice versa.Figure 11.11 Illustration connections two memory modules separate controllers. 11.24 Memory Banks Multiple physical memories also used Von Neumann Architecture convenient way form large memory replicating small memory modules. idea, known memory banks, uses interface hardware map addresses onto two physical memories. example, suppose two identical memory modules designed physical addresses 0 M–1. interface arrange treat two banks form contiguous large memory twice addresses Figure 11.12 illustrates. Figure 11.12 logical arrangement two identical memory banks form single memory twice size. Figure 11.12, addresses 0 M–1 assigned one bank addresses 2M – 1 assigned second bank. Chapter 13, see mapping address extremely efficient.Although banks arranged give illusion one large memory, underlying hardware configured Figure 11.11 shows. result, controllers two memory banks operate parallel. Thus, instructions placed one bank data another, higher performance result instruction fetch interfere data access vice versa. memory banks appear programmer? architectures, memory banks transparent — memory hardware automatically finds exploits parallelism. embedded systems special-purpose architectures, programmer may responsible placing items separate memory banks increase performance. example, programmer may need place code low memory addresses data items high memory addresses. 11.25 Interleaving related optimization used physical memory systems known interleaving. understand optimization, observe many programs access data sequential memory locations. example, long text string copied one place memory another program searches list items, sequential memory locations referenced. banked memory, sequential locations lie memory bank, means successive accesses must wait controller reset. Interleaving uses idea separate controllers, instead organizing memories banks, interleaving places consecutive words memory separate physical memory modules. Interleaving achieves high performance sequential memory accesses word fetched memory previous word resets. Interleaving usually hidden programmers — programmer write code without knowing underlying memory system mapped successive words separate memory modules. memory hardware handles details automatically. use terminology N-way interleaving describe number underlying memory modules (to make scheme efficient, N chosen power two). example, Figure 11.13 illustrates words memory assigned memory modules four-way interleaving scheme. interleaving achieved efficiently? answer lies thinking binary representation. Figure 11.13, example, words 0, 4, 8, lie memory module 0. addresses common? represented binary, values two low-order bits equal 00. Similarly, words assigned module 1 low-order bits equal 01, words assigned module 2 low-order bits equal 10, words assigned module 3 low-order bits equal 11. Thus, given memory address, interface hardware extracts low-order two bits, uses select module.Figure 11.13 Illustration 4-way interleaving illustrates successive words memory placed memory modules optimize performance. Interestingly, accessing correct word within module equally efficient. modules standard memory modules provide array words addressed 0 K– 1, value K. interface ignores two low-order bits address uses rest bits index memory module. see works, write 0, 4, 8, ... binary, remove two low-order bits. result sequence 0, 1, 2,... Similarly, removing two low-order bits 1, 5, 9... also results sequence 0, 1, 2,... summarize: number modules power two, hardware N-way interleaving extremely efficient low-order bits address used select module remaining bits used address within module. 11.26 Content Addressable Memory unusual form memory exists blends two key aspects discussed: technology memory organization. form known Content Addressable Memory (CAM). see, CAM much merely store data items — includes hardware high- speed searching. easiest way think CAM view memory organized two-dimensional array. row, used store item, called slot. addition allowing processor place value slot, CAM allows processor specify search key exactly long one slot. search key specified, hardware canperform search table determine whether slot matches search key. Figure 11.14 illustrates organization CAM. Figure 11.14 Illustration Content Addressable Memory (CAM). CAM provides memory technology memory organization. basic form CAM, search mechanism performs exact match. is, CAM hardware compares key slot, reports whether match found. Unlike iterative search performed conventional processor, CAM reports results instantly. essence, slot CAM contains hardware performs comparison. imagine wires leading bits key slots. slot contains gates compare bits key bits value slot. hardware slots operates parallel, time required perform search depend number slots. course, parallel search hardware makes CAM extremely expensive search mechanism must replicated slot. also means CAM consumes much power (and produces much heat) conventional memory. Thus, architect uses CAM lookup speed important cost power consumption. example, high-speed Internet router, system must check incoming packet determine whether packets arrived previously source. handle high-speed connections, designs use CAM store list source identifiers. CAM allows search performed fast enough accommodate packets arriving high rate (i.e., high-speed network). 11.27 Ternary CAMAn alternative form CAM, known Ternary CAM (TCAM), extends idea CAM provide partial match searches. essence, bit slot three values: zero, one, “don’t care.” Like standard CAM, TCAM performs search operation parallel examining slots simultaneously. Unlike standard CAM, TCAM performs match bits value zero one. Partial matching allows TCAM used cases two entries CAM overlap — TCAM find best match (e.g., longest prefix match). 11.28 Summary examined two aspects physical memory: underlying technology memory organization. Many memory technologies exist. Differences among include permanence (RAM ROM), clock synchronization, read write cycle times. Physical memory organized words accessed controller. Although programmers find byte addressing convenient, underlying memory systems use word addressing. intelligent memory controller translate byte addressing word addressing. avoid arithmetic computation controller, memory organized address space bytes per word powers two. Programming languages, C, provide pointer variables pointer arithmetic allow programmer obtain manipulate memory addresses. memory dump, shows contents memory along memory address location, used relate data structures program values memory runtime. Memory banks interleaving employ multiple memory modules. Banks used organize large memory smaller modules. Interleaving places successive words memory separate modules speed sequential access. Content Addressable Memory (CAM) combines memory technology memory organization. CAM organizes memory array slots, provides high-speed search mechanism. EXERCISES 11.1 Smart phones portable devices typically use DRAM rather SRAM. Explain why. 11.2 Explain purpose DRAM refresh mechanism. 11.3 Assume computer physical memory organized 64-bit words. Give word address offset within word following byte addresses: 0, 9, 27, 31, 120, 256. 11.4 Extend exercise writing computer program computes answer. program take series inputs consist two values: word size specified bits byte address. input, program generate wordaddress offset within word. Note: although specified bits, word size must power two bytes. 11.5 ARM processor, attempting load integer memory result error address multiple 4 bytes. term use refer error? 11.6 computer 64-bit addresses, address corresponds one byte, many gigabytes memory computer address? 11.7 Compute number memory operations required two-address instruction instruction operands unaligned. 11.8 Write C function declares static integer array, M, implements fetch store operations use shift Boolean operations access individual bytes. 11.9 Find memory PC, identify type chips used, look vendor’s specification chips determine memory type speed. 11.10 Redraw Figure 11.13 8-way interleaved memory. 11.11 Emulate physical memory. Write C program declares array, M, array 10,000 integers (i.e., array words). Implement two functions, fetch store, use array emulate byte-addressable memory. fetch(i) returns ith byte memory, store(i,ch) stores 8-bit character ch ith byte memory. use byte pointers. Instead, use ideas chapter write code computes correct word contains specified byte offset within word. 11.12 Simulate TCAM. Write program matches input string set patterns. simulation, use characters instead bits. Allow pattern contain string characters, interpret asterisk “wild card” matches character. find way make match proceed faster iterating patterns? †SRAM pronounced “ess-ram.” ‡DRAM pronounced “dee-ram.” †We learn memory controller later chapter. †In later chapters, learn I/O devices also access memory memory controller; now, use processor examples. †The offset measured zero. †As figure shows, programmer initialize memory hexadecimal value makes easy identify items memory dump. example, programmer used value deadbeef. †Chapter 13 explains interface acts Memory Management Unit (MMU), explains functionality detail.12 Caches Caching Chapter Contents 12.1 Introduction 12.2 Information Propagation Storage Hierarchy 12.3 Definition Caching 12.4 Characteristics Cache 12.5 Cache Terminology 12.6 Best Worst Case Cache Performance 12.7 Cache Performance Typical Sequence 12.8 Cache Replacement Policy 12.9 LRU Replacement 12.10 Multilevel Cache Hierarchy 12.11 Preloading Caches 12.12 Caches Used Memory 12.13 Physical Memory Cache 12.14 Write Write Back 12.15 Cache Coherence 12.16 L1, L2, L3 Caches 12.17 Sizes L1, L2, L3 Caches 12.18 Instruction Data Caches 12.19 Modified Harvard Architecture 12.20 Implementation Memory Caching12.21 Direct Mapped Memory Cache 12.22 Using Powers Two Efficiency 12.23 Hardware Implementation Direct Mapped Cache 12.24 Set Associative Memory Cache 12.25 Consequences Programmers 12.26 Summary 12.1 Introduction previous chapter discusses physical memory systems, focusing underlying technologies used build memory systems organization address spaces. chapter also discusses organization physical memory words. chapter takes different view problem: instead concentrating technologies used construct memory systems, chapter focuses technology used improve memory system performance. chapter presents fundamental concept caching, shows caching used memory systems, explains caching essential, describes caching achieves high performance low cost. 12.2 Information Propagation Storage Hierarchy Recall Chapter 10 storage mechanisms organized hierarchy includes general-purpose registers, main memory, secondary storage. Data items migrate hierarchy, usually control software. general, items move hierarchy read, hierarchy written. example, generating code arithmetic computation, compiler arranges move items memory registers. computation finishes, result may moved back memory. item must kept program finishes, programmer arrange copy item memory secondary storage. see caching fits storage hierarchy, learn memory cache uses hardware rather software move items part hierarchy. 12.3 Definition CachingThe term caching refers important optimization technique used improve performance hardware software system retrieves information. memory systems, caching reduce Von Neumann bottleneck†. cache acts intermediary. is, cache placed path mechanism makes requests mechanism answers requests, cache configured intercept handle requests. central idea caching high-speed, temporary storage: cache keeps local copy selected data, answers requests local copy whenever possible. Performance improvement arises cache designed return answers faster mechanism normally fulfills requests. Figure 12.1 illustrates cache positioned mechanism makes requests mechanism answers requests. Figure 12.1 Conceptual organization cache, positioned path mechanism makes requests storage mechanism answers requests. 12.4 Characteristics Cache description purposefully vague caching broad concept appears many forms computer communication systems. section clarifies definition explaining concept detail; later sections give examples caching used. Although variety caching mechanisms exist, share following general characteristics: Small Active Transparent Automatic Small. keep economic cost low, amount storage associated cache much smaller amount storage needed hold entire set data items. cache sizes less ten percent main storage size; many cases, cache holds less one percent much data store. Thus, one central design issues revolves around selection data items keep cache.Active. cache contains active mechanism examines request decides respond. Activities include: checking see requested item available cache, retrieving copy item data store item available locally, deciding items keep cache. Transparent. say cache transparent, means cache inserted without making changes requester data store. is, interface cache presents requester exactly interface data store presents, interface cache presents data store exactly interface requester presents. Automatic. cases, cache mechanism receive instructions act data items store cache storage. Instead, cache implements algorithm examines sequence requests, uses requests determine manage cache. 12.5 Cache Terminology Although caching used variety contexts, terminology related caching universal acceptance across types caching systems. cache hit (abbreviated hit) defined request satisfied cache without access underlying data store. Conversely, cache miss (abbreviated miss) defined request cannot satisfied cache. Another term characterizes sequence references presented cache. say sequence references exhibits high locality reference sequence contains repetitions requests; otherwise, say sequence low locality reference. see high locality reference leads higher performance. Locality refers items cache. Therefore, cache stores large data items (e.g., pages memory), repeated requests need identical long reference item cache (e.g., memory references items page). 12.6 Best Worst Case Cache Performance said data item stored cache, cache mechanism return item faster data store. Figure 12.2 shows, represent costs retrieval requester’s view. Figure 12.2 Illustration access costs using cache. Costs measured respect requester. figure, Ch cost item found cache (i.e., hit), Cm cost item found cache (i.e., miss). Interestingly, individual costs informative. Observe cache uses contents requests determine items keep, performance depends sequence requests. Thus, understand caching, must examine performance sequence requests. example, easily analyze best worst possible behavior sequence N requests. one extreme, request references new item, caching improve performance — cache must forward request data store. Thus, worst case, cost is: noted analysis ignores administrative overhead required maintain cache. divide N compute average cost per request, result Cm. extreme, requests sequence specify data item (i.e., highest locality reference), cache indeed improve performance. receives first request, cache fetches item data store saves copy; subsequent requests satisfied using copy cache. Thus, best case. cost is: Dividing N produces cost per request: N → ∞, first two terms approach zero, means cost per request best case becomes Ch. understand caching powerful tool: one ignores overhead, worst case performance caching worse cache present. best case, cost per request approximately equal cost accessing cache, lower cost accessing data store. 12.7 Cache Performance Typical Sequence estimate performance cache typical sequence requests, need examine cache handles sequence contains hits misses. Cache designers use termhit ratio refer percentage requests sequence satisfied cache. Specifically, hit ratio defined be: hit ratio value zero one. define miss ratio one minus hit ratio. course, actual hit ratio depends specific sequence requests. Experience shown many caches, hit ratio tends nearly across requests encountered practice. cases, derive equation cost access terms cost miss cost hit: r hit ratio defined Equation 12.4 above. cost accessing data store, given Cm equation, fixed. Thus, two ways cache designer improve performance cache: increase hit ratio decrease cost hit. 12.8 Cache Replacement Policy cache designer increase hit ratio? two ways: Increase cache size Improve replacement policy Increase cache size. Recall cache usually much smaller large data store. begins, cache keeps copy response. cache storage full, item must removed cache new item added. larger cache store items. Improve replacement policy. cache uses replacement policy decide item remove new item encountered cache full. replacement policy specifies whether ignore new item choose item evict make space new item. replacement policy chooses keep items referenced increase hit ratio. 12.9 LRU Replacement replacement policy used? two issues. First, increase hit ratio, replacement policy retain items referenced frequently. Second,the replacement policy inexpensive implement, especially memory cache. One replacement policy satisfies criteria become extremely popular. Known Least Recently Used (LRU), policy specifies replacing item referenced longest time past†. LRU easy implement. cache mechanism keeps list data items currently cache. item referenced, item moves front list; replacement needed, item back list removed. LRU works well many situations. cases set requests high locality reference (i.e., cache improve performance), items referenced again. LRU tends keep items cache, means cost access kept low. summarize: storage full new item arrives, cache must choose whether retain current set items replace one current items new item. Least Recently Used (LRU) policy popular choice replacement trivial implement tends keep items requested again. 12.10 Multilevel Cache Hierarchy One unexpected astonishing aspects caching arises use caching improve performance cache! understand optimization possible, recall insertion cache lowers cost retrieving items placing items closer requester. imagine additional cache placed requester existing cache Figure 12.3 illustrates. Figure 12.3 organization system additional cache inserted. second cache improve performance? Yes, provided cost access new cache lower cost access original cache (e.g., new cache closer requester). essence, cost equation becomes: r1 denotes fraction hits new cache, r2 denotes fraction hits original cache, Ch1 denotes cost accessing new cache, Ch2 denotes cost accessing original cache. one cache used along path requester data store, say system implements multilevel cache hierarchy. set Web caches provides example multilevel hierarchy. path browser running user’s computer pass cache user’s ISP well local cache mechanism used browser. point is: Adding additional cache used improve performance system uses caching. Conceptually, caches arranged multilevel hierarchy. 12.11 Preloading Caches cache performance improved further? Cache designers observe although many cache systems perform well steady state (i.e., system run awhile), system exhibits higher cost startup. is, initial hit ratio extremely low cache must fetch items data store. cases, startup costs lowered preloading cache. is, values loaded cache execution begins. course, preloading works cases cache anticipate requests. example, ISP’s Web cache preloaded hot pages (i.e., pages accessed frequently past day pages owner expects frequent access). alternative, caches use automated method preloading. one form, cache periodically places copy contents nonvolatile storage, allowing recent values preloaded startup. another form, cache uses reference prefetch related data. example, processor accesses byte memory, cache fetch 128 bytes. Thus, processor accesses next byte, likely, value come cache. Prefetching especially important Web pages. typical Web page contains references multiple images, page displayed, browser must download copy image cache copy user’s computer. page downloaded, browser scan references images, begin prefetch images without waiting entire page download. 12.12 Caches Used Memory understand basic idea caching, consider ways caches used memory systems. fact, concept caching originated computer memory systems†. original motivation higher speed low cost. memory bothexpensive slow, architects looked ways improve performance without incurring cost higher-speed memory. architects discovered small amount high-speed cache improved performance dramatically. result impressive 1980s, computer systems single cache located processor memory. Physically, memory one circuit board cache occupied separate circuit board, allowed computer owners upgrade memory cache independently. described above, caching hierarchy increase performance single cache. Therefore, see modern computers employ hierarchy memory caches use caching variety ways. next sections present examples. 12.13 Physical Memory Cache Caching become popular way achieve higher memory performance without significantly higher cost. Early computers used physical memory system. is, generated request, processor specified physical address, memory system responded physical address. Thus, inserted path processor memory, cache understand use physical addresses. may seem physical memory cache trivial. imagine memory cache receiving fetch request, checking see request answered cache, then, item present, passing request underlying memory. Furthermore, imagine item retrieved underlying memory, cache saves copy locally, returns value processor. fact, imagined scenario misleading — physical memory cache much complex description. understand why, must remember hardware achieves high speed parallelism. example, encounters fetch request, memory cache check cache access physical memory. Instead, cache hardware performs two tasks parallel: cache simultaneously passes request physical memory searches answer locally. finds answer locally, cache must cancel memory operation. find answer locally, cache must wait underlying memory operation complete. Furthermore, answer arrive memory, cache uses parallelism simultaneously saving local copy answer transferring answer back processor. Parallel activities make hardware complex. point is: achieve high performance, physical memory cache designed search local cache access underlying memory simultaneously. Parallelism complicates hardware. 12.14 Write Write BackIn addition parallelism, memory caches also complicated write (i.e., store) operations. two issues: performance coherence. Performance easiest understand: caching improves performance retrieval requests, storage requests. is, write operation takes longer write operation must change value underlying memory. important, addition forwarding request memory, cache must also check see whether item cache. so, cache must update copy well. fact, experience shown memory cache always keep local copy value written programs tend access value short time stored. Initial implementations memory caches handled write operations described above: cache kept copy forwarded write operation underlying memory. use term write-through cache describe approach. alternative, known write-back cache, keeps copy data item written, waits later update underlying physical memory. know whether underlying physical memory must updated, write-back cache keeps extra bit item known dirty bit. physical memory cache, dirty bit associated block cache. item fetched copy placed cache, dirty bit initialized zero. processor modifies item (i.e., performs write), dirty bit set one. needs eject block cache, hardware first examines dirty bit associated block. dirty bit one, copy block written memory. dirty zero, however, block simply overwritten data block exactly copy memory. point is: write-back cache associates dirty bit block record whether block modified since fetched. ejecting block cache, hardware writes copy dirty block memory, simply overwrites contents block dirty. understand write-back improves performance, imagine loop program increments variable memory iteration loop. write-back cache places variable cache first time variable referenced. successive iteration, changes variable affect cached copy. Assume loop ends, program stops referencing variable. Eventually, program generates enough references variable least recently used item cache, selected replacement. new item referenced cache slot needed, cache writes value variable underlying physical memory. Thus, although variable referenced changed many times, memory system one access underlying physical memory†. 12.15 Cache CoherenceMemory caches especially complex system multiple processors (e.g., multicore CPU). said write-back cache achieves higher performance write-through cache. multiprocessor environment, performance also optimized giving core cache. Unfortunately, two optimizations conflict. understand why, look architecture Figure 12.4, shows two processors private cache. Figure 12.4 Illustration two processors sharing underlying memory. processor separate cache, conflicts occur processors reference memory address. consider happens two caches use write-back approach. processor 1 writes memory location X, cache 1 holds value X. Eventually, needs space, cache 1 writes value underlying physical memory. Similarly, whenever processor 2 writes memory location, value placed cache 2 space needed. problem obvious: without additional mechanism, incorrect results occur processors simultaneously issue read write operations given address. avoid conflicts, caches access given memory must follow cache coherence protocol coordinates values. example, processor 2 reads address, A, coherence protocol requires cache 2 inform cache 1. currently holds address A, cache 1 writes physical memory cache 2 obtain recent value. is, read operation processor triggers write-back cache currently holds cached copy address. Similarly, processor issues write operation address, A, caches must informed discard cached values A. Thus, addition requiring additional hardware mechanism allows caches communicate, cache coherency introduces additional delay. 12.16 L1, L2, L3 Caches said arranging multiple caches hierarchy improve overall performance. Indeed, computer memory systems least two levels cache hierarchy. Tounderstand computer architects added second level cache memory hierarchy, must consider four facts: • traditional memory cache separate memory processor. • access traditional memory cache, processor used pins connect processor chip rest computer. • Using pins access external hardware takes much longer accessing functional units internal processor chip. • Advances technology made possible increase number transistors per chip, means processor chip contain hardware. conclusion clear. know adding second cache improve memory system performance, know placing second cache processor chip make cache access times much lower, know technology allows chip vendors add hardware chips. So, makes sense embed second memory cache processor chip itself. hit ratio high, data references never leave processor chip — effective cost accessing memory approximately cost accessing register. describe idea multiple caches, computer manufacturers originally adopted terms Level 1 cache (L1 cache) refer cache onboard processor chip, Level 2 cache (L2 cache) refer external cache, Level 3 cache (L3 cache) refer cache built physical memory. is, L1 cache originally on-chip L2 L3 cache off- chip. fact, chip sizes become large single chip contain multiple cores multiple caches. cases, manufacturers use term L1 cache describe cache associated one particular core, term L2 cache describe on-chip cache may shared, term L3 cache describe on-chip cache shared multiple cores. Typically, cores share L3 cache. Thus, distinction on-chip off-chip faded. summarize terminology: using traditional terminology multilevel cache hierarchy, L1 cache embedded processor chip, L2 cache external processor, L3 cache built physical memory. recent terminology defines L1 cache associated single core, whereas L2 L3 refer on-chip caches cores share. 12.17 Sizes L1, L2, L3 CachesMost computers employ cache hierarchy. course, cache top hierarchy fastest, also smallest. Figure 12.5 lists example cache memory sizes. L1 cache may divided separate instruction data caches, described next section. Figure 12.5 Example cache sizes 2016. Although absolute sizes continue change; readers focus amount cache relative RAM 4 GB 32 GB. 12.18 Instruction Data Caches memory references pass single cache? understand question, imagine instructions executed data accessed. Instruction fetch tends behave high locality — many cases, next instruction executed found adjacent memory address. Furthermore, time-consuming loops program usually small, means entire loop fit cache. Although data access programs exhibits high locality, data access others not. example, program accesses hash table, locations referenced appear random (i.e., location referenced one instant necessarily close location referenced next). Differences instruction data behavior raise question intermixing two types references affects cache. essence, random sequence requests becomes, worse cache performs (because cache save value, even though value needed again). state general principle: Inserting random references series requests tends worsen cache performance; reducing number random references occurs tends improve cache performance. 12.19 Modified Harvard Architecture performance optimized separate cache instructions data? simplistic answer obvious. data instructions placed cache, data references tend push instructions cache, lowering performance. Adding separate instruction cache improve performance.The simplistic answer insufficient, however, question whether additional hardware help, choose among tradeoffs. additional hardware generate heat, consume power, portable devices, deplete battery faster, architect must weigh costs additional cache. architect decide add cache hardware, question best use hardware. know, example, increasing size single cache increase performance avoiding collisions. cache becomes sufficiently large, intermixing instructions data references work fine. Would better add separate instruction cache retain single cache increase size? Many architects decided optimal way use modest amount additional hardware lies introducing new I-cache (instruction cache) using existing cache D-cache (data cache). Separating instruction data caches trivial Harvard Architecture I-cache associated instruction memory D-cache associated data memory. architects abandon Von Neumann Architecture? Many architects adopted compromise computer separate instruction data caches, caches lead single memory. use term Modified Harvard Architecture characterize compromise. Figure 12.6 illustrates modified architecture. Figure 12.6 Modified Harvard Architecture separate instruction data caches leading underlying memory. 12.20 Implementation Memory Caching Conceptually, entry memory cache contains two values: memory address value byte found address. practice, storing complete address entry inefficient. Therefore, memory caches use clever techniques reduce amount space needed. two important cache optimization techniques known as: Direct mapped memory cache Set associative memory cache see that, like virtual memory schemes, cache implementations use powers two avoid arithmetic computation.12.21 Direct Mapped Memory Cache direct mapped memory cache uses mapping technique avoid overhead. Although memory caches used byte-addressable memories, cache record individual bytes. Instead, cache divides memory cache set fixed-size blocks, block size, B (measured bytes), chosen power two. hardware places entire block cache whenever byte block referenced. Using cache terminology, refer block cache cache line; size direct mapped memory cache often specified giving number cache lines times size cache line. example, size might specified 4K lines 8 bytes per line. envision cache, think bytes memory divided 8-byte segments assigned lines cache. Figure 12.7 illustrates bytes memory would assigned block size eight cache four lines. (Note: memory cache usually holds many four lines; small cache size chosen merely simplified example figure.) Observe blocks memory numbered modulo C, C number slots cache. is, blocks numbered zero C – 1 (C 4 figure). Interestingly, using powers two means arithmetic required map byte address block number. Instead, block number found extracting set bits. figure, block number computed extracting fourth fifth bits address. example, consider byte address 57 (111001 binary, shown forth fifth bits underlined). bits 11 3 decimal, agrees block number figure. address 44 (101100 binary), fourth fifth bits 01 block number 1. express mapping programming language terms as: b = (byte_address >> 3) & 0x03; terms memory cache, computation needed — hardware places value internal register extracts appropriate bits form block number. Figure 12.7 example assignment block numbers memory locations cache four blocks eight bytes per block.The key understanding direct mapped memory cache arises following rule: memory block numbered placed cache slot i. example, block addresses 16 23 placed slot 2, block addresses 48 55. multiple memory blocks placed given slot, cache know block currently slot? cache attaches unique tag group C blocks. example, Figure 12.8 illustrates tag values assigned memory blocks example cache four slots. Figure 12.8 example memory cache space four blocks memory divided conceptual blocks 8 bytes. group four blocks memory assigned unique tag. identify block currently slot cache, cache entry contains tag value. Thus, slot zero cache contains tag K, value slot zero corresponds block zero area memory tag K. use tags? cache must uniquely identify entry slot. tag identifies large group blocks rather single byte memory, using tag requires fewer bits identify section memory using full memory address. Furthermore, next section explains, choosing block size size memory identified tag powers two makes cache lookup extremely efficient. 12.22 Using Powers Two EfficiencyAlthough direct mapping described may seem complex, using powers two simplifies hardware implementation. fact, hardware elegant extremely efficient. Instead modulo arithmetic, tag block number computed extracting groups bits memory address. high-order bits address used tag, next set bits forms block number, final set bits gives byte offset within block. Figure 12.9 illustrates division. Figure 12.9 Illustration using powers two allows cache divide memory address three separate fields correspond tag, block number, byte offset within block. know values obtained via bit extraction, algorithm lookup direct-mapped memory cache straightforward. Think cache array. idea extract block number address, use block number index array. entry array contains tag value. tag address matches tag cache slot, cache returns value. tag match, cache hardware must fetch block memory, place copy cache, return value. Algorithm 12.1 summarizes steps. algorithm omits important detail. slot cache valid bit specifies whether slot used. Initially (i.e., computer boots), valid bits set 0 (to indicate none slots contain blocks memory). stores block slot, cache hardware sets valid bit 1. examines tag slot, hardware reports mismatch valid bit set, forces copy block loaded memory. Algorithm 12.1 Given: memory address Find: data byte address Method: Extract tag number, t, block number, b, offset, o, address selecting appropriate bit fields Examine tag slot b cache tag slot b cache matches { Use select appropriate byte block slot b, return byte } else { /* Update cache */ Fetch block b underlying memory Place copy slot b Set tag slot b Use select appropriate byte block slot b, return byte }Algorithm 12.1 Cache Lookup Direct Mapped Memory Cache 12.23 Hardware Implementation Direct Mapped Cache Algorithm 12.1 describes cache lookup cache array separate steps taken extract items index array. fact, slots cache stored array memory. Instead, implemented hardware circuits, circuits work parallel. example, first step extracts items address implemented placing address internal register (a hardware circuit consists set latches), arranging bit address represented output one latch. is, address placed register, bit address represented separate wire. items t, b, address obtained merely dividing output wires groups. second step Algorithm 12.1 requires cache hardware examine one slots. hardware uses decoder select exactly one slots. slots connected common output wires; hardware arranged selected slot puts output wires. comparator circuit used compare tag address tag selected slot. Figure 12.10 gives simplified block diagram hardware perform cache lookup.Figure 12.10 Block diagram hardware used implement lookup memory cache. circuit takes memory address input, produces two outputs. valid output 1 specified address found cache (i.e., cache returns value). value output contents memory specified address. figure, slot divided valid bit, tag, value indicate separate hardware circuits used field. Horizontal lines decoder slot indicate connection used activate circuits slot. time, however, decoder selects one slot (in figure, selected slot shown shaded). Vertical lines slots indicate parallel connections. Hardware slot connects wires, selected slot places value vertical wires. Thus, example, input gate comes V circuit selected slot, input comparator comes Tag circuit selected slot, value output comes Value circuit selected slot. key point cache lookup performed quickly combinatorial circuits.12.24 Set Associative Memory Cache chief alternative direct mapped memory cache known set associative memory cache. essence, set associative memory cache uses hardware parallelism provide flexibility. Instead maintaining single cache, set associative approach maintains multiple underlying caches, provides hardware search simultaneously. important, provides multiple underlying caches, set associative memory cache store one block number. trivial example, consider set associative cache two copies underlying hardware. Figure 12.11 illustrates architecture. Figure 12.11 Illustration set associative memory cache two copies underlying hardware. cache includes hardware search copies parallel. understand advantages set associative approach, consider reference string program alternately references two addresses, A1 A2, different tags, block number zero. direct mapped memory cache, two addresses contend single slot cache. reference A1 loads value A1 slot 0 cache, reference A2 overwrites contents slot 0 value A2. Thus, alternating sequence references, every reference results cache miss. set associative memory cache, however, A1 placed one two underlying caches, A2 placed other. Thus, every reference results cache hit. amount parallelism increases, performance set associative memory cache increases. extreme case, cache classified fully associative, underlying caches contains one slot, slot hold arbitrary value. Note amount parallelism determines point continuum: parallelism, direct mapped memory cache, full parallelism, equivalent Content Addressable Memory (CAM). 12.25 Consequences ProgrammersExperience shown caching works well computer programs. code programmers produce tends contain loops, means processor repeatedly execute small set instructions moving another set. Similarly, programs tend reference data items multiple times moving new data item. Furthermore, compilers aware caching, help optimize generated code take advantage cache. Despite overwhelming success caching, programmers understand cache works write code exploits cache. example, consider program must perform many operations element large array. possible perform one operation time (in case program iterates array many times) perform operations single element array moving next element (in case program iterates array once). point view caching, latter preferable element remain cache. 12.26 Summary Caching fundamental optimization technique used many contexts. cache intercepts requests, automatically stores values, answers requests quickly, whenever possible. Variations include multilevel cache hierarchy preloaded caches. Caches provide essential performance optimization memories. computer systems employ multilevel memory cache. Originally, L1 cache resided integrated circuit along processor, L2 cache located external processor, L3 cache associated memory. integrated circuits became larger, manufacturers moved L2 L3 caches onto processor chip, using distinction L1 cache associated single core L2/L3 caches shared among multiple cores. technology known direct mapped memory cache handles lookup without keeping list cached items. Although think lookup algorithm performing multiple steps, hardware implementation direct mapped memory cache use combinatorial logic circuits perform lookup without needing processor. set associative memory cache extends concept direct mapping permit parallel access. EXERCISES 12.1 term transparent mean applied memory cache? 12.2 hit ratio given piece code 0.2, time required access cache 20 nanoseconds, time access underlying physical memory 1 microsecond, effective memory access time piece code? 12.3 physicist writes C code iterate large 2-dimensional array: float a[32768, 32768], sum; ... (i=0; i<32768; i++) { (j=0; j<32768; j++) { sum += a[j,i]; } } physicist complains code running slowly. simple change make increase execution speed? 12.4 Consider computer memory address thirty-two-bits long memory system cache holds 4K entries. naive cache used entry cache stores address byte data, much total storage needed cache? direct mapped memory cache used entry stores tag block data consists four bytes, much total storage needed? 12.5 Extend previous exercise. Assume size cache fixed, find alternative naive solution allows storage data items. Hint: values placed cache processor always accesses four-byte integers memory? 12.6 Consult vendors’ specifications find cost memory access cost cache hit modern memory system (Ch Cm Section 12.6). 12.7 Use values obtained previous exercise plot effective memory access cost hit ratio varies zero one. 12.8 Using values Ch Cm obtained Exercise 12.6, value hit ratio, r, needed achieve improvement 30% mean access time memory system (as compared memory system without cache)? 12.9 State two ways improve hit ratio cache. 12.10 cache coherence, type system needs it? 12.11 Write computer program simulate direct mapped memory cache using cache 64 blocks block size 128 bytes. test program, create 1000 x 1000 array integers. Simulate address references program walks array row-major order column-major order. hit ratio cache case? 12.12 hardware diagram Figure 12.10 shows circuits needed lookup. Extend diagram include circuits load value cache memory. †The Von Neumann bottleneck defined page 131. †Note “least recently” always refers long ago item last referenced, number accesses. †In addition introducing use microcode, Maurice Wilkes credited inventing concept memory cache 1965. †An optimizing compiler improve performance using general-purpose register hold variable loop finishes (another form caching).13 Virtual Memory Technologies Virtual Addressing Chapter Contents 13.1 Introduction 13.2 Definition Virtual Memory 13.3 Memory Management Unit Address Space 13.4 Interface Multiple Physical Memory Systems 13.5 Address Translation Address Mapping 13.6 Avoiding Arithmetic Calculation 13.7 Discontiguous Address Spaces 13.8 Motivations Virtual Memory 13.9 Multiple Virtual Spaces Multiprogramming 13.10 Creating Virtual Spaces Dynamically 13.11 Base-Bound Registers 13.12 Changing Virtual Space 13.13 Virtual Memory Protection 13.14 Segmentation 13.15 Demand Paging 13.16 Hardware Software Demand Paging 13.17 Page Replacement 13.18 Paging Terminology Data Structures13.19 Address Translation Paging System 13.20 Using Powers Two 13.21 Presence, Use, Modified Bits 13.22 Page Table Storage 13.23 Paging Efficiency Translation Lookaside Buffer 13.24 Consequences Programmers 13.25 Relationship Virtual Memory Caching 13.26 Virtual Memory Caching Cache Flush 13.27 Summary 13.1 Introduction previous chapters discuss physical memory caching. chapter physical memory considers hardware technologies used create memory systems, organization physical memory words, physical addressing scheme used access memory. chapter caching describes memory cache organized, explains memory cache improves performance dramatically. chapter considers important concept virtual memory. examines motivation, technologies used create virtual address spaces, mapping virtual physical memory. Although focus primarily hardware systems, learn operating system uses virtual memory facilities. 13.2 Definition Virtual Memory use term Virtual Memory (VM) refer mechanism hides details underlying physical memory provide convenient memory environment. essence, virtual memory system creates illusion — address space memory access scheme overcome limitations physical memory physical addressing scheme. definition may seem vague, need encompass wide variety technologies uses. next sections define concept precisely giving examples virtual memory systems created technologies used implement each. learn variety virtual memory schemes arises single scheme optimal cases. already seen example memory system fits definition virtual memory Chapter 11: intelligent memory controller provides byte addressing underlying physical memory uses word addressing. implementation consists acontroller allows processor specify requests using byte addressing. saw choosing sizes powers two avoids arithmetic computation makes translation byte addresses word addresses trivial. 13.3 Memory Management Unit Address Space Architects use term Memory Management Unit (MMU) describe intelligent memory controller. MMU creates virtual address space processor. addresses processor uses virtual addresses MMU translates address underlying physical memory. classify entire mechanism virtual memory system part underlying physical memory. Informally, help distinguish virtual memory physical memory, engineers use adjective real refer physical memory. example, might use term real address refer physical address, term real address space refer set addresses recognized physical memory. 13.4 Interface Multiple Physical Memory Systems MMU map byte addresses underlying word addresses extended create complex memory organizations. example, Intel designed network processor used two types physical memory: SRAM DRAM. Recall SRAM faster DRAM, costs more, system smaller amount SRAM (intended items accessed frequently) large amount DRAM (intended items accessed frequently). Furthermore, SRAM physical memory organized four bytes per word DRAM physical memory organized eight bytes per word. Intel’s network processor used embedded RISC processor could access memories. important, RISC processor used byte addressing. However, rather using separate instructions operand types access two memories, Intel design followed standard approach: integrated physical memories single virtual address space. implement uniform virtual address space two dissimilar physical memory systems, memory controller must perform necessary translations. essence, MMU must supply abstraction hides details underlying memory systems. Figure 13.1 illustrates overall architecture.Figure 13.1 Illustration architecture two dissimilar memories connect processor. processor use either memory. figure, processor connects Memory Management Unit. MMU receives memory requests processor, translates request, forwards request controller physical memory 1 controller physical memory 2. controllers two physical memories operate described Chapter 11 — controller accepts request specifies byte addressing, translates request operations use word addressing. hardware Figure 13.1 provide virtual address space? answer related memory banks described Chapter 11. Conceptually, MMU divides address space two parts, MMU associates physical memory 1 physical memory 2. example, physical memory contains gigabyte (0x40000000 bytes) RAM, MMU create virtual address space maps addresses 0 0x3fffffff first memory addresses 0x40000000 0x7fffffff second memory. Figure 13.2 illustrates resulting virtual memory system. 13.5 Address Translation Address Mapping underlying memory systems Figure 13.2 operates like independent physical memory — hardware expects requests reference addresses beginning zero. Thus, two memories recognizes set addresses. memory 1, virtual addresses associated memory cover range hardware expects. memory 2, however, processor generates virtual addresses starting 0x40000000, MMU mustmap address lower range (i.e., real addresses 0 0x3fffffff) passing request memory 2. say MMU translates address. Figure 13.2 Illustration virtual memory system divides address space among two physical memories†. MMU uses address decide memory access. Mathematically, address mapping memory 2 straightforward: MMU merely subtracts 0x40000000 address. Figure 13.3 explains concept. Figure 13.3 sequence steps used Memory Management Unit create virtual memory depicted Figure 13.2. MMU maps virtual address space onto two physical memories. point is: MMU combine multiple underlying physical memory systems create virtual address space provides processor illusion single, uniform memory system. underlying memory uses addresses start zero,the MMU must translate addresses generated processor addresses used memory. 13.6 Avoiding Arithmetic Calculation practice, MMU use subtraction implement address translation subtraction requires substantial hardware (e.g., ALU) takes much time perform memory reference. solution consists using powers two simplify hardware. example, consider mapping Figure 13.2. Addresses 0 0x3fffffff map memory 1, addresses 0x40000000 0x7fffffff onto memory 2. Figure 13.4 shows expressed binary, addresses occupy thirty-one bits, ranges differ high- order bit. Figure 13.4 binary values addresses range 0 2 gigabytes. Except high-order bit, values 1 gigabyte below. example shows, choosing power two eliminate need subtraction low-order bits used physical address. example, mapping address one two underlying physical memories, MMU use high-order bit address determine physical memory receive request. form physical address, MMU merely extracts remaining bits virtual address. summarize: Dividing virtual address space boundary corresponds power two allows MMU choose physical memory perform necessary address translation without requiring arithmetic operations. 13.7 Discontiguous Address SpacesFigure 13.2 shows example contiguous virtual address space, address space addresses mapped onto underlying physical memory. is, processor reference address zero highest address address corresponds location one physical memories. Interestingly, computers designed flexible — physical memory designed allow computer’s owner determine much memory install. computer contains physical slots memory, owner choose populate slots memory chips leave slots empty. Consider consequence allowing owner install arbitrary amount memory. defined computer created, virtual address space includes address possible physical memory location (i.e., addresses maximum amount memory installed computer). owner decides omit memory, part virtual address space becomes unusable — processor references address correspond physical memory, error results. virtual address space contiguous regions valid addresses separated invalid addresses. example, Figure 13.5 shows virtual address space might appear virtual address space mapped onto two physical memories, part physical memory omitted. Figure 13.5 Example noncontiguous virtual address space N bytes mapped onto two physical memories. addresses correspond physical memory. part virtual address space map onto physical memory, say address space contains hole. Figure 13.5, example, virtual address space contains two holes†. summarize: virtual address space contiguous, case every address maps location underlying physical memory, noncontiguous, case theaddress space contains one holes. processor attempts read write address correspond physical memory, error results. Many possibilities exist mapping virtual address space onto physical memories. example, recall Chapter 11 two low-order bits address used interleave memory among four separate physical memory modules (i.e., banks), remaining bits address used select byte within module. One chief advantages interleaving bytes among set modules arises ability underlying hardware access separate physical memories simultaneously. Using low-order bits select module means successive bytes memory come different modules. particular, processor accesses data item composed thirty-two bits, underlying memory system fetch four bytes simultaneously. 13.8 Motivations Virtual Memory trivial examples show memory system present processor virtual address space differs underlying physical memory. rest chapter explores complex virtual memory schemes. cases, schemes incorporate extend concepts discussed above. learn four main motivations use complex virtual memory: Homogeneous integration hardware Programming convenience Support multiprogramming Protection programs data Homogeneous Integration Hardware. examples explain virtual memory system provide homogeneous interface set physical memories. important, scheme allows heterogeneity underlying memories. example, underlying physical memories use word size thirty-two bits, others use word size sixty-four bits. memories much faster cycle time others, memories consist RAM others consist ROM. MMU hides differences allowing processor access memories single address space. Programming Convenience. One chief advantages virtual memory system arises ease programming. separate physical memories integrated uniform address space, processor needs special instructions (or special operand formats) memory. Programming memory accesses becomes painful. important, programmer decides move item one memory another, program must rewritten, means decision cannot made run time. Support Multiprogramming. Modern computer systems allow multiple applications run time. example, user editing document leave word processoropen, temporarily launch Web browser check reference, listen music time. terms multiprogramming multiprocessing characterize computer system allows multiple programs run time. see virtual memory system needed support multiprogramming. Protection Programs Data. said CPU uses modes execution determine instructions allowed time. see virtual memory inherently linked computer’s protection scheme. 13.9 Multiple Virtual Spaces Multiprogramming Early computer designers thought multiprogramming impractical. understand why, consider instruction set works. operands specify indirection reference memory address. two programs loaded single memory run time, conflict occur programs attempt use memory location two different purposes. Thus, programs run together written avoid using memory addresses. common technology multiprogramming uses virtual memory establish separate virtual address space program. understand virtual memory system used, consider example. Figure 13.6 illustrates straightforward mapping. Figure 13.6 Illustration four partitions mapped onto single physical memory. virtual address space starts address zero.The mechanism figure divides physical memory equal-size areas known partitions. Partitioned memory systems used early mainframe computers 1960s 1970s, since replaced. One main drawbacks partitioned memory memory available given program fraction total physical memory computer. Figure 13.6 illustrates, systems used partitioned memory typically divided memory four partitions, meant one-fourth total memory dedicated program. diagram Figure 13.6 implies MMU translates multiple virtual address spaces onto single physical memory. practice, however, MMU hardware perform additional mappings. example, MMU translate virtual address space 1 intermediate virtual address space, translate intermediate virtual address space onto one underlying physical memories (which may implement translation byte addresses word addresses). 13.10 Creating Virtual Spaces Dynamically virtual memory system created? simplistic examples above, implied mapping virtual address space(s) physical memories chosen hardware built. Although small, special-purpose systems mappings designed hardware, general-purpose computer systems usually not. Instead, MMU general- purpose system changed dynamically run time. is, system boots, processor tells MMU exactly map virtual address space onto physical memory. program running processor change address space continue run? general, address space used part processor mode. processor begins running real mode, means processor passes memory references directly physical memory without using MMU. operating real mode, processor interact MMU establish mapping. mapping specified, processor execute instruction changes mode, enables MMU, branches specified location. MMU translates memory reference according mapping configured. next sections examine technologies used create dynamic virtual memory systems. consider three examples: Base-Bound Registers Segmentation Demand Paging 13.11 Base-Bound Registers mechanism known name base-bound among oldest easiest dynamic virtual memory schemes understand. essence, base-bound scheme creates single virtualaddress space maps space onto region physical memory. name refers pair registers part MMU; must loaded MMU enabled. base register holds address physical memory specifies map virtual address space, bound register holds integer specifies size address space. Figure 13.7 illustrates mapping. Figure 13.7 Illustration virtual memory uses base-bound mechanism. base register specifies location virtual address space, bound register specifies size. 13.12 Changing Virtual Space may seem base-bound mechanism uninteresting provides single virtual address space. must remember, however, base-bound mechanism dynamic (i.e., easy change). idea operating system use base-bound mechanism move among multiple virtual address spaces. example, suppose operating system loaded two application programs different locations memory. operating system, runs real mode, controls MMU. application, A, ready run, operating system configures MMU point A’s section memory, enables MMU mapping, branches application. Later, control returns operating system, operating system selects another application run, B, configures MMU point B’s memory, enables MMU, branches code B. application’s virtual address space starts zero; application remains unaware location physical memory. point operating system use base-bound mechanism provide much functionality static virtual memory mechanisms considered earlier. summarize:A base-bound mechanism uses two values MMU specify virtual address space maps onto physical address space. base-bound mechanism powerful operating system change mapping dynamically. 13.13 Virtual Memory Protection bound register used base-bound approach? answer protection: although base register sufficient establish mapping virtual address physical address, mapping prevent program accidentally maliciously referencing arbitrarily large memory locations. Figure 13.7, example, addresses higher lie beyond region allocated program (i.e., addresses may allocated another application). base-bound scheme uses bound register guarantee program exceed allocated space. course, implement protection, MMU must check memory reference raise error program attempts reference address greater M. protection offered base-bound mechanism provides example important concept: virtual memory system supports multiprogramming must also provide protection prevents one application reading altering memory allocated another application. 13.14 Segmentation memory mappings described intended map complete address space (i.e., memory needed application run, including compiled program data program uses). say virtual memory technology maps entire address space coarse granularity mapping. alternative, consists mapping parts address space, known fine granularity mapping. understand motivation fine granularity mapping, consider typical application program. program consists set functions, flow passes one function another procedure call. Early computer architects observed although memory scarce resource, coarse granularity virtual systems required entire application occupy memory. memory unused one function actively executing time. reduce amount memory needed, architects proposed program divided variable-size blocks, blocks program needed time loaded memory. is, pieces program kept external storage device, typically disk, one needed. time, operating system finds unused region memory large enough, loads piece memory. operating systemthen configures MMU establish mapping virtual addresses piece uses physical addresses used hold piece. program piece longer used, operating system copies piece back disk, thereby making memory available another piece. variable-size piece scheme known segmentation, pieces programs known segments. proposed, segmentation generated many questions. hardware support would needed make segmentation efficient? hardware dictate upper bound size segment? much research hardware experiments, segmentation faded. central problem segmentation arises operating system begins move blocks memory. segments variable size, memory tends toward situation unused memory divided many small blocks. Computer scientists use term fragmentation describe situation, say memory becomes fragmented†. summarize: Segmentation refers virtual memory scheme programs divided variable-size blocks, blocks currently needed kept memory. leads problem known memory fragmentation, segmentation seldom used. 13.15 Demand Paging alternative segmentation invented become extremely successful. Known demand paging, technique follows general scheme segmentation: divide program pieces, keep pieces external storage needed, load individual piece piece referenced. significant difference demand paging segmentation lies program divided. Instead variable-size segments large enough hold complete function, demand paging uses fixed-size blocks called pages. Initially, memories application programs much smaller, architects chose page size 512 bytes 1 Kbyte; current architectures use larger page sizes (e.g., Intel processors use 4 Kbyte pages). 13.16 Hardware Software Demand Paging combination two technologies needed effective virtual memory system supports demand paging: Hardware handle address mapping efficiently, record page used, detect missing pagesSoftware configure hardware, monitor page use, move pages external store physical memory Demand Paging Hardware. Technically, hardware architecture provides address mapping mechanism allows software handle demand aspect. is, software (usually operating system) configures MMU specify pages virtual address space present memory page located. Then, operating system runs application uses virtual address space configured. MMU translates memory address application references address available (i.e., address one pages present memory). reference missing page called page fault, treated error condition (e.g., like division zero). is, instead fetching missing page external storage, hardware merely informs operating system fault occurred allows operating system handle problem. Typically hardware arranged raise exception. hardware saves current state computation (including address instruction caused fault), uses exception vector. Thus, operating system’s point view, page fault acts like interrupt. fault handled, operating system instruct processor restart execution instruction caused fault. Demand Paging Software. Software operating system responsible management memory: software must decide pages keep memory keep external storage. important, software fetches pages demand. is, hardware reports page fault, paging software takes over. software identifies page needed, locates page secondary storage, locates slot memory, reads page memory, reconfigures MMU. page loaded, software resumes executing application, fetch-execute cycle continues another page fault occurs. course, paging hardware software must work together. example, page fault occurs, hardware must save state computation way values reloaded later execution resumes. Similarly, software must understand exactly configure MMU. 13.17 Page Replacement understand paging, must consider happens set applications running long time. applications reference pages, virtual memory system moves pages memory. Eventually, memory becomes full. operating system knows page needed application references page. difficult decision involves selecting one existing pages evict make space incoming page. Moving page external storage memory takes time, performance optimized choosing move page needed near future. process known page replacement. page replacement handled software, discussion algorithms heuristics beyond scope text. see, however, hardware provides mechanisms assist operating system making decision.13.18 Paging Terminology Data Structures term page refers block program’s address space, term frame refers slot physical memory hold page. Thus, say software loads page frame memory. page memory, say page resident, set pages address space currently memory called resident set. primary data structure used demand paging known page table. easiest way envision page table imagine one-dimensional array indexed page number. is, entries table index zero, one, on. entry page table either contains null value (if page resident) address frame physical memory currently holds page. Figure 13.8 illustrates page table. Figure 13.8 Illustration active page table entries pointing frames memory. null pointer page table entry (denoted Λ) means page currently resident memory. 13.19 Address Translation Paging System Items Figure 13.8 correspond frames, individual words. understand paging hardware, imagine address space divided fixed-size pages Figure 13.9 illustrates.Figure 13.9 Illustration virtual address space divided pages K bytes per page. see addresses associated page important. figure shows, page contains K bytes, bytes page zero addresses zero K–1, bytes page 1 addresses K 2K–1, on. Conceptually, translation virtual address, V, corresponding physical address, P, requires three steps: 1. Determine number page address V lies. 2. Use page number index page table find location frame memory holds page. 3. Determine far page V lies, move far frame memory. Figure 13.9 illustrates addresses associated pages. Mathematically, page number address lies, N, computed dividing address number bytes per page, K: Similarly, offset address within page, O, computed remainder division†. Thus, virtual address, V, translated corresponding physical address, P, using page number offset, N O, follows: 13.20 Using Powers Two Chapter 11 discusses, arithmetic operation, division, expensive perform memory reference. Therefore, like parts memory system, paging system designed avoid arithmetic computation. number bytes per page chosen power two, 2q, means address first byte frame q low-order bits equal zero. Interestingly, low-order bits frame address always zero, page table need store full address. consequence using power two division modulo operations specified mathematical equations replaced extracting bits. Furthermore, addition operation replaced logical or. result, instead using Equations 13.1, 13.2 13.3, MMU performs following computation translate virtual address, V, physical address, P: Figure 13.10 illustrates MMU hardware performs virtual address mapping. considering figure, remember hardware move bits parallel. Thus, arrow points low-order bits virtual address low-order bits physical address represents parallel data path — hardware sends bits time. Also, arrow page table entry high-order bits physical address means bits page table entry transferred parallel. Figure 13.10 Illustration MMU performs address translation paging system. Making page size power two eliminates need division remainder computation.13.21 Presence, Use, Modified Bits description paging hardware omits several details. example, addition value specifies frame page located, page table entry contains control bits hardware software use coordinate. Figure 13.11 lists three control bits found paging hardware. Figure 13.11 Examples control bits found page table entry actions hardware takes each. bits intended assist page replacement software operating system. Presence Bit. straightforward control bit called presence bit, specifies whether page currently memory. bit set software tested hardware. loaded page filled values page table entry, operating system sets presence bit one; removes page memory, operating system sets presence bit zero. translates address, MMU examines presence bit page table entry — presence bit one, translation proceeds, presence bit zero, hardware declares page fault occurred. Use Bit. use bit, provides information needed page replacement, initialized zero later tested software. bit set hardware. mechanism straightforward: whenever accesses page table entry, MMU sets use bit one. operating system periodically sweeps page table, testing use bit determine whether page referenced since last sweep. page referenced becomes candidate eviction; otherwise, operating system clears use bit leaves page next sweep. Modified Bit. modified bit initialized later tested software. bit set hardware. paging software sets bit zero page loaded. MMU sets bit one whenever write operation occurs page. Thus, modified bit one byte page written since page loaded. value used page replacement — page selected eviction, value modified bit tells operating system whether page must written back external storage discarded (i.e., whether page identical copy external storage). 13.22 Page Table StorageWhere page tables reside? systems store page tables special MMU chip external processor. course, memory references play essential role processing, MMU must designed work efficiently. particular, ensure memory references become bottleneck, processors use special-purpose, high-speed hardware interface access MMU. interface contains parallel wires allow processor MMU send many bits time. Surprisingly, many processors designed store page tables memory! is, processor (or MMU) contains special purpose register operating system uses specify location current page table. location page table must specified giving physical address. Typically, systems designed divide memory three regions Figure 13.12 illustrates. Figure 13.12 Illustration physical memory might divided architecture stores page tables memory. large area physical memory reserved frames. design figure illustrates one motivations memory system composed heterogeneous technologies: page tables used frequently, memory used store page tables needs high performance (e.g., SRAM). However, high performance memory expensive, overall cost reduced using lower-cost memory (e.g., DRAM) store frames. Thus, architect design system uses SRAM hold page tables DRAM frame storage. 13.23 Paging Efficiency Translation Lookaside Buffer central question underlies virtual memory architectures: efficient resulting system? understand question, important realize address translation must performed every memory reference: instruction fetch, operand references memory, store result. memory heavily used, mechanisms implement address translation must extremely efficient translation become bottleneck. Architects primarily concerned amount time MMU uses translate virtual address physical address; less concerned amount time takes operating system configure page tables. One technique used optimize performance demand paging system stands especially important. technique uses special, high-speed hardware known Translation Lookaside Buffer (TLB) achieve faster page table lookups. TLB form Content Addressable Memory stores recently used values page table. first translates address, MMU places copy page table entry TLB. successive lookups, hardware performs two operations parallel: standard address translation steps depictedin Figure 13.10 high-speed search TLB. requested information found TLB, MMU aborts standard translation uses information TLB. entry TLB, standard translation proceeds. understand TLB improves performance, consider fetch-execute cycle. processor tends fetch instructions successive locations memory. Furthermore, program contains branch, probability extremely high destination nearby, probably page. Thus, rather randomly accessing pages, processor tends fetch successive instructions page. TLB improves performance optimizes successive lookups avoiding indexing page table. difference performance especially dramatic architectures store page tables memory; without TLB, systems slow useful. summarize: special high-speed hardware device, called Translation Lookaside Buffer (TLB), used optimize performance paging system. virtual memory TLB unacceptably slow. 13.24 Consequences Programmers Experience shown demand paging works well computer programs. code programmers produce tends organized functions fit onto page. Similarly, data objects, character strings, designed data occupies consecutive memory locations, means page loaded, page tends remain resident multiple references. Finally, compilers understand paging, optimize performance placing data items onto pages. One way programmers affect virtual memory performance arises array access. Consider two-dimensional array memory. programming systems allocate array row-major order, means rows array placed contiguous memory Figure 13.13 illustrates. Figure 13.13 illustration two-dimensional array stored row-major order. row contiguous memory. figure shows, rows matrix occupy successive locations memory. Thus, two-dimensional array bytes, location [ , j ] given by: location(A) + i×Q + j Q number bytes per row.The chief alternative row-major order known column-major order. array stored column-major order, elements column occupy contiguous memory locations. choice row-major column-major order usually determined programming language compiler, programmer. programmer control program iterates array, good choice optimize virtual memory performance. example, large array characters, A[N,M], stored row-major order, nested loops shown here: = 1 N { j = 1 { [ i, j ] = 0; } } require less time execute loop varies indices opposite order: j = 1 { = 1 N { [ i, j ] = 0; } } difference time arises varying row index force virtual memory system move one page memory another reference, varying column index means successive references stay page. 13.25 Relationship Virtual Memory Caching Two key technologies virtual memory systems related caching: TLB demand page relacement. Recall TLB consists small, high-speed hardware mechanism improves performance demand paging system dramatically. fact, TLB nothing cache address mappings: whenever looks page table entry, MMU stores entry TLB. successive lookup page receive answer TLB. Like many cache systems, TLB usually uses Least Recently Used replacement strategy. Conceptually, entry referenced, TLB moves entry front list; new reference occurs cache full, TLB discards page table entry back list make space new entry. course, TLB cannot afford keep linked list memory. Instead, TLB contains digital circuits move values special-purpose Content Addressable Memory (CAM) high speed. Demand paging viewed form caching. cache corresponds main memory, data store corresponds external storage pages kept needed. Furthermore, page replacement policy serves cache replacement policy. face, paging borrows phrase replacement policy caching. Interestingly, thinking demand paging cache help us understand important concept: virtual address space much larger physical memory. Like cache,physical memory holds fraction total pages. analysis caching, know performance demand-paged virtual memory approach performance physical memory. words: analysis caching previous chapter shows using demand paging computer system small physical memory perform almost well computer physical memory large enough entire virtual address space. 13.26 Virtual Memory Caching Cache Flush caching used virtual memory, cache placed processor MMU MMU physical memory? is, memory cache store pairs virtual address contents physical address contents? answer complex. one hand, using virtual addresses increases memory access speed cache respond MMU translates virtual address physical address. hand, cache uses virtual addresses needs extra hardware allows cache interact virtual memory system. understand why, observe virtual memory system usually supplies address range running application (i.e., process addresses start zero). consider happens operating system performs context switch stops running one process runs another process. Suppose memory cache contains entry address 2000 switch occurs. cache unchanged context switch new process accesses location 2000, cache return value location 2000 old process. Therefore, changes one process another, operating system must also change items cache. cache engineered avoid problem ambiguity occurs multiple processes use range addresses? Architects use two solutions: cache flush operation disambiguating identifier Cache Flush. One way ensure cache report incorrect values consists removing existing entries cache. say cache flushed. architectures use flushing, operating system must flush cache whenever performs context switch move one application another. Disambiguation. alternative cache flushing involves use extra bits identify running process (or precisely, address space). processor contains extra hardware register contains address space ID. Many operating systems create address space associated process, use process ID (an integer) identify address space. Whenever switches application, operating system loads application’s process ID address space ID register. Figure 13.14 shows, cache prepends thecontents ID register onto virtual address stores item cache, means even process 1 process 2 reference address 0, two entries cache differ. Figure 13.14 Illustration ID register used disambiguate among set virtual address spaces. address space assigned unique number, operating system loads ID register. figure illustrates, cache designed use longer address memory system. passing request cache, processor creates artificially long address concatenating virtual address onto process ID. processor passes longer address cache. cache’s point view, ambiguity: even two applications reference virtual address, ID bits distinguish two addresses. 13.27 Summary Virtual memory systems present abstract address space processor application program running processor. virtual memory system hides details underlying physical memory. Several virtual memory architectures possible. virtual memory system hide details word addressing present uniform address space incorporates multiple, possibly heterogeneous, memory technologies. Virtual memory offers convenience programmers, support multiprogramming, protection. multiple programs run concurrently, virtual memory used provide program address space begins zero. Virtual memory technologies include base-bound, segmentation, demand paging; demand paging popular. demand paging system uses page tables map virtual address physical address; high-speed search mechanism known TLB makes page table lookup efficient. avoid arithmetic computation, virtual memory systems make physical memory page sizes power two. allows hardware translate addresses without using arithmetic logical operations. Either physical virtual addresses cached. cache uses virtual addresses, ambiguity problem arise multiple applications (processes) use range virtualaddresses. Two techniques used solve ambiguity problem: operating system flush cache whenever switches one application another, cache hardware designed use artificially long addresses high-order bits consist address space ID (typically, process ID). EXERCISES 13.1 Consider computer using virtual address space illustrated Figure 13.2. C programmer writes: char c; char *p; p = (char *)1073741826; c = *p; memory module referenced, module referenced byte located within memory? 13.2 traditional Intel PC hole memory address space 640 kilobytes 1 megabyte. Use Figure 13.5 example, draw figure scale showing hole PC address space PC 2 megabytes memory. 13.3 four motivations virtual memory help programmers? Explain. 13.4 demand paging require special hardware special software? Explain. 13.5 Conceptually, page table array. found element page table array, interpreted? 13.6 Consider presence, use, modified bits. bit, tell bit changes whether hardware software makes change. 13.7 Assuming page size 4K bytes, compute page number offset addresses 100, 1500, 8800, 10000. 13.8 Write computer program takes two input values, page size address, computes page number offset address. 13.9 Extend program previous exercise. page size power two, use division modulus operations computing answer. 13.10 Calculate amount memory needed hold example page table. Assume page table entry occupies thirty-two bits, page size 4 Kbytes, memory address occupies thirty-two bits. 13.11 Write computer program takes input page size address space size, performs calculation previous exercise. (You may restrict sizes powers two.) 13.12 page replacement, performed hardware software? 13.13 Consider two-level paging scheme. Assume high-order ten bits address used index directory table select page table. Assume page table contains 1024 entries next ten bits address used select page tableentry. Also assume final twelve bits address used select byte page. much memory required directory table page tables? 13.14 TLB, necessary? 13.15 Write program references locations large two-dimensional array stored row-major order. Compare execution times program iterates rows touches column within row time required program iterates columns touches row within column. Explain results. 13.16 memory system caches virtual addresses process virtual address space starts zero, must operating system changing one process another? Why? †We chosen label address space address zero bottom; documentation uses convention placing zero top address space. †We see examples address spaces contain holes discuss I/O. †To avoid memory fragmentation, architects experimented larger, fixed-size segments (e.g., 64 Kbytes per segment). †Note computation byte address within page similar computation byte address within word discussed page 212.Part IV Input Output External Connections Data Movement14 Input / Output Concepts Terminology Chapter Contents 14.1 Introduction 14.2 Input Output Devices 14.3 Control External Device 14.4 Data Transfer 14.5 Serial Parallel Data Transfers 14.6 Self-Clocking Data 14.7 Full-Duplex Half-Duplex Interaction 14.8 Interface Throughput Latency 14.9 Fundamental Idea Multiplexing 14.10 Multiple Devices Per External Interface 14.11 Processor’s View I/O 14.12 Summary 14.1 Introduction Previous chapters text describe two major components found computer system: processors memories. addition describing technologies used component, chapters illustrate processors memory interact. chapter introduces third major aspect architecture, connections computer external world. learn computers, connection processor I/O device uses basic paradigm connection processor memory. Furthermore, see although operate control processor, I/O devices interact directly memory. 14.2 Input Output Devices earliest electronic computers, consisted numerical processor plus memory, resembled calculator modern computer. human interface crude — values entered set manual switches, results calculations viewed series lights. late 1940s, become obvious better interfaces needed digital computers could useful basic calculations. Engineers began devising ways connect computers external devices, became known Input Output (I/O) devices. Modern I/O devices include cameras, earphones, microphones, well keyboards, mice, monitors, sensors, hard disks, DVD drives printers. 14.3 Control External Device earliest external devices attached computers consisted independent units operated control CPU. is, external device usually occupied separate physical cabinet, independent source electrical power, contained internal circuitry separate computer. small set wires connected computer external device, carried control signals (i.e., signals digital logic computer digital logic device). Circuitry device monitored control signals, changed device accordingly. Many early computers provided set lights displayed values. Typically, display contained one light bit computer’s accumulator — light bit set one, bit zero. However, possible connect light bulb directly accumulator circuit even small light bulb requires power adigital circuit deliver. Therefore, display unit contains circuitry receives set digital logic signals controls set light bulbs accordingly. Figure 14.1 illustrates hardware organized. Figure 14.1 Example external circuit controls set lights. device contains circuitry converts incoming digital logic signals signals needed operate set light bulbs. figure illustrates, think external device independent processor except digital signals pass them. practice, course, devices reside enclosure processor, receive power common source. ignore details concentrate control signals. computer interacts device two ways: computer controls device computer exchanges data device. example, processor start disk spinning, control volume external speaker, tell camera snap picture, turn printer. next chapter, learn computer passes control information external devices. 14.4 Data Transfer Although control external devices essential, devices, control functions secondary. primary function external devices data transfer. Indeed, architectural choices surrounding external devices focus mechanisms permit device processor exchange data. consider several questions regarding data transfer. First, exactly data communicated? Second, side initiates transfer (i.e., processor request transfer device)? Third, techniques mechanisms needed enable high-speed transfers? questions less pertinent programmers concern low-level details. voltages used communicate external device, data represented? answers depend type device, speed data must transferred, type cabling used, distance processor device. However, Figure 14.1illustrates, digital signals used internally processor insufficient drive circuits external device. voltages encodings used external connections differ used internally, special hardware needed translate two representations. use term interface controller refer hardware provides interface external device. Figure 14.2 illustrates interface controllers needed ends physical connection. Figure 14.2 Illustration controller hardware end external connection. voltages signals used external connection differ voltages used internally. 14.5 Serial Parallel Data Transfers I/O interfaces computer classified two broad categories: Parallel interface Serial interface Parallel Interface. interface computer external device classified parallel interface allows transfer multiple bits data simultaneously. essence, parallel interface contains many wires — instant, wire carries one bit data. use term interface width refer number parallel wires interface uses. Thus, one might hear engineer talk eight-bit interface sixteen-bit interface. learn interfaces use parallel wires next chapter. Serial Interface. alternative parallel interface one one bit data transferred time; interface transfers one bit time classified serial. chief advantages serial interface fewer wires less interference signals traveling time. principle, two wires needed serial data transmission — one carry signal second serve electrical ground voltage measured. chief disadvantage serial interface arises increased latency: sending multiple bits, serial hardware must wait one bit sent sending another.14.6 Self-Clocking Data Recall digital circuits operate according clock, signal pulses continuously. Clocks especially significant I/O I/O device processor separate clock rate (i.e., controller clock). Thus, one significant aspects external interface concerns interface accommodates differences clock rates. term self-clocking describes mechanism signals sent across interface contain information allows receiver determine exactly sender encoded data. example, external devices use method similar clockless logic mechanism Chapter 2 describes†. Others employ extra set wires pass clocking information: transmitting data, sender uses extra wires inform receiver location bit boundaries data. 14.7 Full-Duplex Half-Duplex Interaction Many external I/O devices provide bidirectional transfer means processor send data device device send data processor. example, disk drive supports read write operations. Interface hardware uses two methods accommodate bidirectional transfer: Full-duplex interaction Half-duplex interaction Full-Duplex Interaction. interface allows transfer proceed directions simultaneously known full-duplex interface. essence, full-duplex hardware consists two parallel devices two independent sets wires connecting them. One set used transfer data direction. Half-Duplex Interaction. alternative full-duplex interface, known half-duplex interface, allows transfer proceed one direction time. is, single set wires connects processor external device must shared. next chapter, see sharing requires negotiation — perform transfer, processor device must wait current transfer finish, must obtain exclusive use underlying wires. 14.8 Interface Throughput Latency throughput interface measured number bits transferred per unit time, usually measured Megabits per second (Mbps) Megabytes per second (MBps). may seem serial interface would always lower throughput serial transmission transfers one bit time, whereas parallel interface transfer multiplebits time. However, parallel wires close together, data rate must limited electromagnetic interference result. Therefore, cases, engineers able send bits serial interface higher throughput parallel interface. second major measure interface latency. Latency refers delay time bit sent time bit received (i.e., long takes transfer single bit), usually measured nanoseconds (ns). seen memories, must careful distinguish latency throughput devices need low latency others need high throughput. summarize: latency interface measure time required perform transfer, throughput interface measure data transferred per unit time. 14.9 Fundamental Idea Multiplexing may seem choosing interface trivial: want full-duplex, low latency, high throughput. Despite desire high performance, many factors make choice interfaces complex. Recall, example, integrated circuit fixed number pins provide external connections. wider interface uses pins, means fewer pins functions. interface provides full-duplex capability uses approximately twice many pins interface provides half-duplex capability. architects choose compromise external connections. connection limited parallelism, hardware uses technique known multiplexing send data. Although details complex, concept multiplexing easy understand. idea hardware breaks large data transfer pieces sends one piece time. use terms multiplexor demultiplexor describe hardware handles data multiplexing. example, Figure 14.3 illustrates multiplexing hardware divide sixty-four bits data sixteen-bit chunks transmit chunks interface width sixteen bits. one chunk sent given time.Figure 14.3 Illustration transfer sixty-four bits data sixteen-bit interface. Multiplexing hardware divides data sixteen-bit units sends one unit time. practice, physical connections processor external devices use multiplexing. allows processor transfer arbitrary amounts data without devoting many physical pins connection. next chapter, learn multiplexing also improves CPU performance. summarize: Multiplexing used construct I/O interface transfer arbitrary amounts data fixed number parallel wires. Multiplexing hardware divides data blocks, transfers block independently. Note definition applies equally serial transmission — simply interpret serial interface multiplexing transfers single wire. Thus, chunk size serial interface single bit. 14.10 Multiple Devices Per External Interface examples chapter imply external connection processor attaches one device. help conserve pins external connections, processors singledevice per external connection. Instead, set pins attaches multiple devices, hardware configured permit processor communicate one devices given time. next chapter explains concept detail gives examples. 14.11 Processor’s View I/O Recall interface controller hardware associated external connection. Thus, processor interacts external device, processor must controller. processor makes requests controller, receives replies; controller translates request appropriate external signals perform requested function external device. point processor interact interface controller external device. capture architectural concept, say controller presents programming interface processor. Interestingly, programming interface need model operations underlying device exactly. next chapter, see example widely used programming interface casts external interactions simplified paradigm. summarize: processor uses interface controller hardware interact device; controller translates requests appropriate external signals. 14.12 Summary Computer systems interact external devices either control device (e.g., change status) transfer data. external interface use serial parallel approach; number bits sent simultaneously known width parallel interface. bidirectional interface use full-duplex half-duplex interaction. two measures interface performance. Latency refers time required send bit given source given destination (e.g., memory printer), throughput refers number bits sent per unit time. number pins limited, processor arbitrarily wide external connections. Instead, interface hardware designed multiplex large data transfers fewer pins. addition, multiple external devices attach single external connection; interface controller hardware communicates device separately. EXERCISES14.1 speaker smart phone laptop is, fact, analog device volume proportional voltage supplied. mean processor must analog output audio? Explain. 14.2 primary secondary functions associated external devices? 14.3 device operates 3.3-volt digital signals connected processor operates 5-volt digital signals? Explain. 14.4 interface width 16, interface parallel serial? Explain. 14.5 USB classified serial interface. classification mean? 14.6 Suppose purchasing network I/O device, vendor gives choice half-duplex full-duplex interface. choose why? 14.7 interface processor storage device width thirty-two bits, processor transfer data item consists sixty-four bits? 14.8 Create parallel interface self-clocking send data one side other. Hint: two wires two ends use coordinate additional wires used transfer data. 14.9 Suppose serial interface latency 200 microseconds. long take transfer one bit interface? long take transfer sixty-four bits interface? 14.10 Suppose parallel interface width thirty-two bits latency 200 microseconds. long take transfer thirty-two bits interface? long take transfer sixty-four bits interface? Explain. †The description clockless logic found page 37.15 Buses Bus Architectures Chapter Contents 15.1 Introduction 15.2 Definition Bus 15.3 Processors, I/O Devices, Buses 15.4 Physical Connections 15.5 Bus Interface 15.6 Control, Address, Data Lines 15.7 Fetch-Store Paradigm 15.8 Fetch-Store Bus Size 15.9 Multiplexing 15.10 Bus Width Size Data Items 15.11 Bus Address Space 15.12 Potential Errors 15.13 Address Configuration Sockets 15.14 Question Multiple Buses 15.15 Using Fetch-Store Devices 15.16 Operation Interface 15.17 Asymmetric Assignments Bus Errors 15.18 Unified Memory Device Addressing 15.19 Holes Bus Address Space 15.20 Address Map15.21 Program Interface Bus 15.22 Bridging Two Buses 15.23 Main Auxiliary Buses 15.24 Consequences Programmers 15.25 Switching Fabrics Alternative Buses 15.26 Summary 15.1 Introduction chapters memory discuss external connection processor memory system. previous chapter discusses connections external I/O devices, shows processor uses connections control device transfer data. chapter reviews concepts serial parallel transfer, defines terminology, introduces idea multiplexing data transfer set wires. chapter extends ideas explaining fundamental architectural feature present computer systems, bus. describes motivation using bus, explains basic operation, shows memory I/O devices share common bus. learn bus defines address space understand relationship bus address space memory address space. 15.2 Definition Bus bus digital communication mechanism allows two functional units transfer control signals data. buses designed use inside single computer system; used within single integrated circuit. Many bus designs exist bus optimized specific purpose. example, memory bus intended interconnect processor memory system, I/O bus intended interconnect processor set I/O devices. see general-purpose designs possible. 15.3 Processors, I/O Devices, Buses notion bus broad enough encompass external connections (i.e., connection processor coprocessor). Thus, instead viewing connection processor device set wires (as Chapter 14), precise:the two units interconnected bus. Figure 15.1 uses graphic common engineering diagrams illustrate concept. Figure 15.1 Illustration bus used connect processor external device. Buses used external connections. summarize: bus digital communication mechanism interconnects functional units computer system. computer contains one buses interconnect processors, memories, external I/O devices. 15.3.1 Proprietary Standardized Buses bus design said proprietary design owned private company available use companies (i.e., covered patent). alternative proprietary bus known standardized bus, means specifications available. permit equipment two vendors communicate interoperate, standardized buses allow computer system contain devices multiple vendors. course, bus standard must specify details needed construct hardware, including exact electrical specifications (e.g., voltages), timing signals, encoding used data. Furthermore, ensure correctness, device attaches bus must implement bus standard precisely. 15.3.2 Shared Buses Access Protocol said bus used connect processor I/O device. fact, buses shared, means single bus used connect processor set I/O devices. Similarly, computer contains multiple processors, processors connect shared bus. permit sharing, architect must define access protocol used bus. access protocol specifies attached device determine whether bus available isin use, attached devices take turns using bus. 15.3.3 Multiple Buses typical computer system contains multiple buses. example, addition central bus connects processor, I/O devices, memory, computers special-purpose bus used access coprocessors. computers multiple buses convenience flexibility — computer several standard buses accommodate wider variety devices. Interestingly, computers also contain buses internal (i.e., visible computer’s owner). example, many processors one internal buses processor chip. circuit chip uses onboard bus communicate another circuit (e.g., onboard cache). 15.3.4 Parallel, Passive Mechanism Chapter 14 describes, interface either classified using serial data transfer parallel data transfer. buses used computer systems parallel. is, bus capable transferring multiple bits data time. straightforward buses classified passive bus contain electronic components. Instead, device attaches bus contains electronic circuits needed communicate bus. Thus, imagine bus consist parallel wires devices attach†. 15.4 Physical Connections Physically, bus consist tiny wires etched silicon single chip, cable contains multiple wires, set parallel metal wires circuit board. computers use third form I/O bus: bus implemented set parallel wires computer’s main circuit board, known mother board. addition bus, mother board contains processor, memory, functional units. set sockets mother board connects bus allow devices plugged removed easily (i.e., device connected bus merely plugging device socket). Typically, bus sockets positioned near edge mother board make easily accessible outside. Figure 15.2 illustrates bus sockets mother board.Figure 15.2 Illustration bus consists parallel wires connect sockets mother board. mother board contains components shown. 15.5 Bus Interface Attaching device bus nontrivial. operate correctly, device must adhere bus standard. Recall, example, bus shared bus specifies access protocol used determine given device access bus transfer data. implement access protocol, device must digital circuit connects bus follows bus standard. Known bus interface bus controller, circuit implements bus access protocol controls exactly device uses bus. bus protocol complicated, interface circuit large; many bus interfaces require multiple chips. physical connection bus interface circuit bus itself? Interestingly, sockets many buses chosen make possible plug printed circuit board directly socket. circuit board must region cut exact size socket, must metal fingers align exactly metal contacts socket. Figure 15.3 illustrates concept. figure helps us envision physical computer system constructed. mother board lies bottom cabinet, circuit boards individual devices plug mother board perpendicular, meaning device circuit boards vertical. key piece physical arrangement concerns placement sockets — locating sockets near edge mother board, designer guarantee device boards located adjacent side cabinet. Choosing location near side means short connection circuit board outside cabinet. arrangement used typical PC.Figure 15.3 Side view mother board illustrating printed circuit board plugs socket bus. Metal fingers circuit board press metal contacts socket. 15.6 Control, Address, Data Lines Although physical structure bus provides interesting engineering challenges, concerned logical structure. examine wires used, operations bus supports, consequences programmers. Informally, wires comprise bus called lines. three conceptual functions lines: Control bus Specification address information Transfer data help us understand bus operates, assume bus contains three separate sets lines correspond three functions†. Figure 15.4 illustrates concept. Figure 15.4 Conceptual division wires comprise bus lines control, addresses, data. figure implies, bus lines need divided equally among three uses. particular, control functions usually require fewer lines functions.15.7 Fetch-Store Paradigm Recall Chapter 10 memory systems use fetch-store paradigm processor either fetch (i.e., read) value memory store (i.e., write) value memory. bus uses basic paradigm. is, bus supports fetch store operations. unlikely seems, learn processor communicates device transfers data across bus, communication always uses fetch store operations. Interestingly, fetch-store paradigm used devices, including microphones, video cameras, sensors, displays, well storage devices, disks. see later possible control devices fetch-store paradigm. now, sufficient understand following: Like memory system, bus employs fetch-store paradigm; control data transfer operations performed either fetch store. 15.8 Fetch-Store Bus Size Knowing bus uses fetch-store paradigm helps us understand purpose three conceptual categories lines Figure 15.4 illustrates. three categories used either fetch store operation. Control lines used ensure one pair entities attempts communicate bus time, allow two communicating entities interact meaningfully. address lines used pass address, data lines used transfer data value. Figure 15.5 explains three categories lines used fetch store operation. figure lists steps taken operation, specifies group lines used step. said buses use parallel transfer — bus contains multiple data lines, simultaneously transfer one bit data line. Thus, bus contains K data lines, bus transfer K bits time. Using terminology Chapter 14, say bus width K bits. Thus, bus thirty-two data lines (i.e., transfer thirty-two bits time) called thirty-two-bit bus. course, buses serial rather parallel. serial bus transfer one bit time. Technically, serial bus width one bit. However, engineers usually talk bus width one bit; simply call serial bus.Figure 15.5 steps taken perform fetch store operation bus, group lines used step. 15.9 Multiplexing wide bus be? Recall Chapter 14 parallel interfaces represent compromise: although increasing width increases throughput, greater width also takes space requires electronic components attached devices. Furthermore, high data rates, signals parallel wires interfere one another. Thus, architect chooses bus width compromise space, cost, performance. One technique stands especially helpful reducing number lines bus: multiplexing. bus use multiplexing two ways: data multiplexing alone combination address data multiplexing. Data Multiplexing. Chapter 14, learned data multiplexing works. essence, device attached bus large amount data transfer, device divides data blocks exactly large bus wide. device uses bus repeatedly, sending one block time. Address Data Multiplexing. motivation multiplexing addresses reduce number lines. understand address multiplexing works, consider steps Figure 15.5 carefully. case fetch operation, address lines data lines never used time (i.e., step). Thus, architect use lines send address andreceive data. store operation, multiplexing used: bus hardware sends address first sends data†. buses make heavy use multiplexing. Thus, instead three conceptual sets lines, typical bus two: lines used control, set lines used send either address data. Figure 15.6 illustrates idea. Figure 15.6 Illustration bus single set lines used addresses data. Using one set lines helps reduce cost. Multiplexing offers two advantages. one hand, multiplexing allows architect design bus fewer lines. hand, number lines bus fixed, multiplexing produces higher overall performance. see why, consider data transfer. K lines bus reserved addresses, K lines cannot used data transfer. lines shared, however, additional K bits transferred bus cycle, means higher overall throughput. Despite advantages, multiplexing two disadvantages. First, multiplexing takes time store operation requires two bus cycles (i.e., one transfer address another transfer data item). Second, multiplexing requires sophisticated bus protocol, therefore, complex bus interface hardware. Despite disadvantages, many bus designs use multiplexing. extreme case, bus designed multiplexes control information set lines used data addresses. 15.10 Bus Width Size Data Items use multiplexing helps explain another aspect computer architecture: uniform size data objects, including addresses. see data transfers among processor, memories, devices occur bus. Furthermore, bus multiplexes transfers fixed number lines, data item exactly matches bus width transferred one cycle, item larger bus width requires multiple cycles. Thus, makes sense architect choose single size bus width, size general-purpose register, size data value ALU functional units use (e.g., size integer floating point value). important, addresses also multiplexed bus lines, makes sense architect choose size address data items. point is:In many computers, addresses data values multiplexed bus. optimize performance hardware, architect chooses single size data items addresses. 15.11 Bus Address Space memory bus (i.e., bus processor uses access memory) easiest form bus understand. Previous chapters discuss concepts memory access memory address space; see bus used implement concepts. Figure 15.7 illustrates, memory bus provides physical interconnection among processor one memories. Figure 15.7 Physical interconnections processor memory using memory bus. controller circuit device handles details bus access. figure shows, processor memory modules connected memory bus contain interface circuit. interface implements bus protocol, handles bus communication. interface uses control lines gain access bus, sends addresses data values carry operation. Thus, interface understands bus details, voltage use timing control signals. processor’s point view, bus interface provides fetch-store paradigm. is, processor perform two operations: fetch bus address store bus address. processor encounters instruction references memory, processor hardware invokes bus interface. example, many architectures, load instruction fetches value memory places value general-purpose register. processor executes load, hardware issues fetch instruction bus interface. Similarly, processor executes instruction deposits value memory, hardware uses store operation bus interface. programmer’s point view, bus interface invisible. programmer thinks bus defining address space. key creating single address space lies memoryconfiguration — memory configured respond specific set bus addresses. is, interface memory 1 assigned different set addresses interface memories 2, 3, 4, on. processor places fetch store request bus, memory controllers receive request. memory controller compares address request set addresses memory module assigned. address request lies within controller’s set, controller responds. Otherwise, ignores request. point is: request passes across bus, attached memory modules receive request. memory module responds address request lies range assigned module. 15.12 Potential Errors Figure 15.8. lists conceptual steps memory module interface implements. Figure 15.8 steps bus interface memory module follows. error bus hardware reports referred bus error; typical bus protocol includes mechanisms detect report type bus error. Allowing memory module act independently means two types bus errors occur: Address conflict Unassigned address Address Conflict. use term address conflict describe bus error results two interfaces misconfigured respond given address. bus hardware designed detect report address conflicts system boots. hardware designed prevent conflicts. case, bus protocols include test foraddress conflicts occur run-time — two interfaces attempt respond given request, bus hardware detects problem sets control line indicate error occurred. uses bus, processor checks bus control lines takes action error occurs. Unassigned Address. unassigned address bus error occurs processor attempts access address assigned interface. detect unassigned address, bus protocols use timeout mechanism — sending request bus, processor starts timer. interface responds, timer expires, causes processor hardware report bus error. timeout mechanism used detect unassigned addresses also detects malfunctioning hardware (e.g., memory module responding requests). 15.13 Address Configuration Sockets bus hardware prevents bus errors. Unassigned addresses pose thorny problem prevention. one hand, prevent bus errors, possible address must assigned memory module. hand, memory systems designed accommodate expansion. is, bus typically contains enough wires address memory installed computer (i.e., addresses unassigned). Fortunately, architects devised scheme helps avoid problem two modules answer given request: special sockets. idea straightforward. Memory manufactured small printed circuits plug socket mother board. avoid problems caused misconfiguration, memory boards identical, configuration required board plugged in. Instead, circuitry wiring added mother board first socket receives requests address 0 K–1, second socket receives requests address K 2K – 1, on. socket recognize address, socket passes low-order bits address memory. point is: avoid memory configuration problems, architects place memory small circuit boards plug socket mother board. owner install memory without configuring hardware socket configured range addresses memory respond. alternative, computers contain sophisticated circuitry allows MMU configure socket addresses computer boots. MMU determines sockets populated, assigns range addresses. Although adds cost, extra circuitry prevent conflicts makes installing memory much easier — owner purchase memory modules plug sockets without configuring modules danger conflicts.15.14 Question Multiple Buses computer system multiple buses? so, many? Computers designed high performance (e.g., mainframe computers) often contain several buses. bus optimized specific purpose. example, mainframe computer might one bus memory, another high-speed I/O devices, another slow-speed I/O devices. alternative, less powerful computers (e.g., personal computers) often use single bus connections. chief advantages single bus lower cost generality. processor need multiple bus interfaces, single bus interface used memory devices. course, designing single bus connections means choosing compromise. is, bus may optimal given purpose. particular, processor uses single bus access instructions data memory well perform I/O, bus easily become bottleneck. Thus, system uses single bus often needs large memory cache answer memory requests without using bus. 15.15 Using Fetch-Store Devices Recall bus used primary connection processor I/O device, operations bus must use fetch-store paradigm. two statements may seem contradictory — although works well data transfer, fetch-store appear handle device control. example, consider operation like testing whether wireless radio currently range access point moving paper printer. seems fetch store operations insufficient, devices require large set control commands. understand bus works, must remember bus provides way communicate set bits one unit another without specifying bit means. names fetch store mislead us thinking values memory. bus, however, interface hardware device provides unique interpretation bits. Thus, device interpret certain bits control operation rather request transfer data. example clarify relationship fetch-store paradigm device control. Imagine simplistic hardware device contains sixteen status lights, suppose want attach device bus. bus offers fetch store operations, need build interface hardware uses fetch-store paradigm control. engineer designs device interface begins listing operations performed. Figure 15.9 lists five functions imaginary device. Figure 15.9 example functionality needed imaginary status light display. function must implemented using fetch-store paradigm. cast control operations fetch-store paradigm, designer chooses set bus addresses used devices, assigns meanings address. example, imaginary status light device attached bus width thirty-two bits, designer might choose bus addresses 10000 10011, might assign meanings according Figure 15.10. Figure 15.10 Example assignment addresses, operations, meanings device control functions listed Figure 15.9. 15.16 Operation Interface Although bus operations named fetch store, device interface act like memory — data stored later recall. Instead, device treats address, operation, data bus request merely set bits. interface contains logic circuits compare address bits request addresses assigned device. match occurs, interface enables circuit responds fetch store operation. example, first item Figure 15.10 implemented circuit tests bits bus store request address 10000 uses data take action. essence, circuit performs following test: (address == 10000 && op == store && data != 0) turn_on_display; } else (address == 10000 && op == store && data == 0) { turn_off_display; } Although used programming language notation express operations, interface hardware perform test sequentially. Instead, interface constructed Booleancircuits test address, operation, data values parallel take appropriate action. 15.17 Asymmetric Assignments Bus Errors example Figure 15.10 define effect fetch store operations addresses. example, specification define fetch operation address 10004. capture idea fetch store operations need defined address, say assignment asymmetric. specification Figure 15.10 asymmetric processor store value four bytes starting address 10004, bus error results processor attempts read address 10004. 15.18 Unified Memory Device Addressing computers, single bus provides access memory I/O devices. architecture, assignment addresses bus defines processor’s view address space. example, imagine computer system single bus Figure 15.11 illustrates. Figure 15.11 Illustration computer architecture uses single bus. bus connects memories well devices. figure, bus defines single address space processor use. memory module device must assigned unique address range bus addresses. example, assume memories 1 Mbyte device requires twelve memory locations, four address ranges must assigned use bus Figure 15.12 illustrates.Figure 15.12 One possible assignment bus addresses set devices shown Figure 15.11. also imagine address space drawn graphically like illustrations memory address space Chapter 11. course space occupied device extremely small compared space occupied memory, means diagram show much detail. example, Figure 15.13 shows diagram results assignments Figure 15.12. Figure 15.13 Illustration address space results address assignments Figure 15.12. amount space taken device (twelve bytes) insignificant compared amount space taken memory (1 Mbyte). 15.19 Holes Bus Address Space address assignment Figure 15.12 said contiguous, means address ranges contain gaps — first byte assigned one range immediate successor last byte assigned previous range. Contiguous address assignment notrequired — software accidentally accesses address assigned, bus hardware detects problem reports bus error. Using terminology Chapter 13, say assignment addresses contiguous, assignment leaves one holes address space. example, bus may reserve lower addresses memory assign devices high addresses, leaving hole two areas. 15.20 Address Map part specification, bus standard specifies exactly type hardware used address. call specification address map. Note address map address assignment map specifies assignments possible. example, Figure 15.14 gives example address map sixteen-bit bus. Figure 15.14 One possible address map sixteen-bit bus. Two areas available memory, one area available devices. figure, two areas address space available memory contiguous. Instead, hole located them. Furthermore, hole located second memory area device area. computer system constructed, owner must follow address map. example, sixteen-bit bus Figure 15.14 allows two blocks memory total 32,768 bytes. owner choose install less full complement memory, more.The device space bus address map especially interesting space reserved devices often much larger necessary. particular, address maps reserve large piece address space devices, making possible bus accommodate extreme cases thousands devices. However, typical computer fewer dozen devices, typical device uses addresses. consequence is: typical computer, part address space available devices sparsely populated — small percentage available addresses used. 15.21 Program Interface Bus programmer’s point view, two ways use bus. Either processor provides special instructions used access bus processor interprets memory operations references bus. latter known memory mapped architecture. example using memory-mapped approach, consider address assignment imaginary light display described Figure 15.10†. turn device on, program must store nonzero value bytes 10000 100003. assume integer consists four bytes (i.e., thirty-two bits) processor uses little-endian byte order, program needs store nonzero value integer location 10000. programmer use following C code perform operation: int *ptr; /* declare ptr pointer integer */ ptr = (*int)10000; /* set pointer address 10000 */ *ptr = 1; /* store nonzero value addresses 10000 - 10003 */ summarize: processor use special instructions access bus use memory- mapped approach normal memory operations used communicate devices well memory. 15.22 Bridging Two Buses Although single bus offers advantage simplicity lower cost, given device may work specific bus. example, earphones require Universal Serial Bus (USB) Ethernet devices require Peripheral Component Interconnect (PCI) bus. Clearly, computer multiple buses accommodate greater variety devices. course, system multiple buses expensive complex. Therefore, designers havecreated inexpensive straightforward ways attach multiple buses computer. One approach uses hardware device, known bridge, interconnects two buses Figure 15.15 illustrates. Figure 15.15 Illustration bridge connecting two buses. bridge must follow standard bus. bridge uses set K addresses. bus chooses address range size K assigns bridge. two assignments usually same; bridge designed perform translation. Whenever operation one bus involves addresses assigned bridge, circuits bridge translate address perform operation bus. Thus, processor bus 1 performs store operation one bridged addresses, bridge hardware performs equivalent store operation bus 2. fact, bridging transparent sense processors devices unaware multiple bridges involved. 15.23 Main Auxiliary Buses Logically, bridge performs one-to-one mapping address space one bus address space another. is, bridge maps set addresses one bus address space other. Figure 15.16 illustrates concept address mapping. figure, bus address spaces start zero, address space auxiliary bus smaller address space main bus. important, architect chosen map small part auxiliary bus address space, specified maps onto region main bus reserved devices. result, device auxiliary bus responds addresses mapped region appears connected computer’s main bus.Figure 15.16 Illustration mapping bridge provide address space auxiliary bus address space main bus. bus addresses need mapped. understand bridging popular, consider common case new device must added computer already bus. interface new device match computer’s main bus, new adapter hardware created bridge used add auxiliary bus system. Using bridge two advantages: bridging simpler adding bus interface new device, bridge installed, computer owner add additional devices auxiliary bus without changing hardware further. summarize: bridge hardware device interconnects two buses maps addresses them. Bridging allows computer one auxiliary buses accessed computer’s main bus. 15.24 Consequences ProgrammersAs Figure 15.16 shows, sets mapped addresses need identical address spaces. goal make bridge transparent software know auxiliary bus. Unfortunately, programmer writes device driver software someone configures computers may need understand mapping. example, device installed auxiliary bus, device obtains bus address, A. part initialization sequence, device may report bus address driver software†. bridge translates addresses, communication device driver uses data lines changed. Thus, generate address main bus, driver software may need understand bridge maps addresses. 15.25 Switching Fabrics Alternative Buses Although bus fundamental computer systems, bus disadvantage: bus hardware perform one transfer time. is, although multiple hardware units attach given bus, one pair attached units communicate time. basic paradigm always consists three steps: wait exclusive use bus, perform transfer, release bus another transfer occur. buses extend paradigm permitting multiple attached units transfer N bytes data time obtain bus. situations bus architectures insufficient, architects invented alternative technologies permit multiple transfers occur simultaneously. Known switching fabrics, technologies use variety forms. fabrics designed handle attached units, fabrics designed handle hundreds thousands. Similarly, fabrics restrict transfers attached units initiate transfers time, fabrics permit many simultaneous transfers. One reasons variety architectures arises economics: higher performance (i.e., simultaneous exchanges) cost much more, higher cost may justified. Perhaps easiest switching fabric understand consists crossbar switch. imagine crossbar matrix N inputs outputs. crossbar contains N×M electronic switches connect input output. time, crossbar turn switches connect pairs inputs outputs Figure 15.17 illustrates.Figure 15.17 conceptual view crossbar switch N inputs outputs dot showing active connection. crossbar mechanism ensures one connection active given row given column time. figure helps us understand switching fabrics expensive. First, line diagram represents parallel data path composed multiple wires. Second, potential intersection input output requires electronic switch connect input output point. Thus, crossbar requires N×M switching components, must able switch parallel connection. comparison, bus requires N+M electronic components (one connect input output bus). Despite cost, switching fabrics popular high-performance systems. 15.26 Summary bus fundamental mechanism used interconnect memory, I/O devices, processors within computer system. buses operate parallel, meaning bus consists parallel wires permit multiple bits transferred simultaneously. bus defines protocol attached devices use access bus. bus protocols follow fetch-store paradigm; I/O device connected bus designed receive fetch store operations interpret control operations device. Conceptually, bus protocol specifies three separate forms information: control information, address information, data. practice, bus need independent wires form bus protocol multiplex communication small set wires.A bus defines address space may contain holes (i.e., unassigned addresses). computer system single bus memory I/O devices attach, multiple buses attach specific types devices. alternative, hardware device called bridge used add multiple auxiliary buses computer mapping part auxiliary bus address space onto address space computer’s main bus. chief alternative bus known switching fabric. Although achieve higher throughput using parallelism, switching fabrics restricted high-end systems switching fabric significantly expensive bus. EXERCISES 15.1 hardware architect asks choose single, thirty-two bit bus design multiplexes data address information across bus two sixteen-bit buses, one used send address information one used send data. design choose? Why? 15.2 computer, bus, connect? 15.3 friend claims computer special bus patented Apple. term use characterize bus design owned one company? 15.4 three conceptual categories wires bus? 15.5 fetch-store paradigm? 15.6 lines bus divided control lines lines, main two uses lines? 15.7 advantage separate socket memory chip? 15.8 Suppose device assigned addresses 0x4000000 0x4000003. Write C code stores value 0xff0001A4 addresses. 15.9 bus transfer 64 bits cycle runs rate 66 MHz, bus throughput measured megabytes per second? 15.10 switching fabric, chief advantage bus? 15.11 many simultaneous transfers occur crossbar switching fabric N inputs outputs? 15.12 Search Internet, make list switching fabric designs. 15.13 Look Internet explanation CLOS network, used switching fabrics, write short description. 15.14 bridge connect? †In practice, buses contain digital circuit known bus arbiter coordinates devices attached bus. However, details beyond scope text. †The description simplifies details; later section explains functionality achieved without physically separate groups wires. †Of course, device receives request multiplexed bus must store address data transferred. †Figure 15.10 appears page 301.†Chapter 17 explains device driver needs address information.16 Programmed Interrupt-Driven I/O Chapter Contents 16.1 Introduction 16.2 I/O Paradigms 16.3 Programmed I/O 16.4 Synchronization 16.5 Polling 16.6 Code Polling 16.7 Control Status Registers 16.8 Using Structure Define CSRs 16.9 Processor Use Polling 16.10 Interrupt-Driven I/O 16.11 Interrupt Mechanism Fetch-Execute 16.12 Handling Interrupt 16.13 Interrupt Vectors 16.14 Interrupt Initialization Disabled Interrupts 16.15 Interrupting Interrupt Handler 16.16 Configuration Interrupts 16.17 Dynamic Bus Connections Pluggable Devices 16.18 Interrupts, Performance, Smart Devices 16.19 Direct Memory Access (DMA) 16.20 Extending DMA Buffer Chaining16.21 Scatter Read Gather Write Operations 16.22 Operation Chaining 16.23 Summary 16.1 Introduction Earlier chapters introduce I/O. previous chapter explains bus provides connection processor set I/O devices. chapter discusses bus address space, shows address space hold combination memory I/O devices. Finally, chapter explains bus uses fetch-store paradigm, shows fetch store operations used interrogate control external device. chapter continues discussion. chapter describes compares two basic styles interaction processor I/O device. focuses interrupt-driven I/O, explains device driver software operating system interacts external device. next chapter takes different approach subject examining I/O programmer’s perspective. chapter looks individual devices, describes interact processor. 16.2 I/O Paradigms know previous chapter I/O devices connect bus, processor interact device issuing fetch store operations bus addresses assigned device. Although basic mechanics I/O easy specify, several questions remain unanswered. control operations device support? application software running processor access given device without understanding hardware details? interaction processor I/O devices affect overall system performance? 16.3 Programmed I/O earliest computers took straightforward approach I/O: external device consisted basic digital circuits controlled hardware response fetch store operations; CPU handled details. example, write data disk, CPU activated set ofcircuits device, one time. circuits positioned disk arm caused head write block data. capture idea early peripheral device consisted basic circuits respond commands CPU say device contained intelligence, characterize form interaction saying I/O programmed. 16.4 Synchronization may seem writing software perform programmed I/O trivial: program merely assigns value address bus. understand I/O programming, however, need remember two things. First, nonintelligent device cannot remember list commands. Instead, circuits device perform command precisely processor sends command. Second, processor operates much faster I/O device — even slow processor execute thousands instructions time takes motor mechanical actuator move physical mechanism. example mechanical device, consider printer. print mechanism spray ink across page, print small vertical band time. Printing starts top page. printing one horizontal band, paper must advanced next horizontal band printed. processor merely issues instructions print item, advance paper, print another item, second item may printed paper still moving, resulting smear. worst case, print mechanism designed operate paper advance mechanism operates, hardware may damaged. prevent problems, programmed I/O relies synchronization. is, issues command, processor must interact device wait device ready another command. summarize: processor operates orders magnitude faster I/O device, programmed I/O requires processor synchronize device controlled. 16.5 Polling basic form synchronization processor uses I/O device known polling. essence, polling requires processor ask repeatedly whether operation completed processor starts next operation. Thus, perform print operation, processor use polling step. Figure 16.1 shows example.Figure 16.1 Illustration synchronization processor I/O device. processor must wait step complete. 16.6 Code Polling software perform polling? bus follows fetch-store paradigm, polling must use fetch operation. is, one addresses assigned device correspond status information — processor fetches value address, device responds giving current status. understand polling appears programmer, need know exact details hardware device. Unfortunately, devices incredibly complex. example, many vendors sell three-in-one printers function scanners fax machines well printers. keep example simple, imagine simple printing device, create programming interface device. Although example device indeed much simpler commercial devices, general approach exactly same. Recall device assigned addresses address space, device engineered respond fetch store instructions addresses. device created, designer specify addresses used, instead writes relative specification giving addresses 0 though N–1. Later, device installed computer, actual addresses assigned. use relative addresses specification means programmer write software control device without knowing actual address. device installed, actual address passed software argument. example clarify concept. imaginary printer defines thirty-two contiguous bytes addresses. Furthermore, design grouped addresses eight words thirty-two bits long. use words typical. specification Figure 16.2 shows device interprets fetch store operations addresses.Figure 16.2 specification bus interface imaginary printing device. processor uses fetch store control device determine status. figure gives meaning fetch store operations addresses assigned imaginary I/O device. described above, addresses specification start zero relative. device connected bus, device assigned thirty-two bytes somewhere bus address space, software use actual addresses communicating device. programmer given hardware specification similar one Figure 16.2, writing code controls device straightforward. example, assume printing device assigned starting bus address 0x110000. Addresses 0 3 figure correspond actual addresses 0x110000 0x110003. determine whether printer powered on, processor merely needs access value addresses 0x110000 0x110003. value zero, printer off. code access device status appears memory fetch. C, status test code written: int *p = (int *)0x110000; (*p != 0){ /* Test whether printer */ /* printer */ } else { /* printer */ } example assumes integer size four bytes. code declares p pointer integer, initializes p 0x110000, uses *p obtain value address 0x110000. understand software communicates device, consider sequence steps synchronization. Figure 16.3 shows C code performs steps found Figure 16.1.Figure 16.3 Example C code uses polling carry steps Figure 16.1 imaginary printing device specified Figure 16.2. Code figure assumes device assigned address 0x110000, data structure mydata contains data printed exactly form printer expects. understand use pointers, remember C programming language defines pointer arithmetic: adding K integer pointer advances pointer KN bytes, N number bytes integer. Thus, variable p value 0x110000, p+1 equals 0x110004. example code illustrates another feature many devices may seem strange programmer: multiple steps single operation. example, data printed located memory two steps used specify data. first step, address data passed printer. second step, printer instructed load copy data. two steps may seem unnecessary — doesn’t printer start loading data automatically address specified? understand, remember fetch store instruction controls circuits device. device designer might choose design finds easier build hardware two separate circuits, one accept memory address one load data memory. Programmers written program control device may find code shocking contains four occurrences statement appear infinite loop. statement appeared conventional application program, statement would error loop continually tests value memory location without making anychanges. example, however, pointer p references device instead memory location. Thus, processor fetches value location p+6, request passes device, interprets request status information. So, unlike value memory, value returned device change time — processor polls enough times, device complete current operation return zero status value. point is: Although polling code appears contain infinite loops, code correct values returned device change time. 16.7 Control Status Registers use term Control Status Registers (CSRs) refer set addresses device uses. specifically, control register corresponds contiguous set addresses (usually size integer) respond store operation, status register corresponds contiguous set addresses respond fetch operation. practice, CSRs usually complicated simplified version listed Figure 16.2. example, typical status register assigns meanings individual bits (e.g., low- order bit status word specifies whether device motion, next bit specifies whether error occurred, on). important, conserve addresses, many devices combine control status functions single set addresses. is, single address serve functions — store operation address controls device, fetch operation address reports device status. final detail, devices interpret fetch operation request status information control operation. example, trackpad delivers bytes indicate motion user’s fingers. processor uses fetch operation obtain data trackpad. Furthermore, fetch automatically resets hardware measure next motion. 16.8 Using Structure Define CSRs example code Figure 16.3 uses pointer pointer arithmetic reference individual items. practice, programmers usually create C struct defines CSRs, use named members struct reference items CSRs. example, Figure 16.4 shows code Figure 16.3 appears struct used define CSRs.Figure 16.4 code Figure 16.3 rewritten use C struct. example shows, code uses struct much easier read debug. members struct given meaningful names, programmer reading code guess purpose, even intimately familiar device. addition, using struct improves program organization offsets individual CSRs specified one place instead embedded throughout code. summarize:Instead distributing CSR references throughout code, programmer improve readability declaring structure defines CSRs device referencing fields structure. 16.9 Processor Use Polling chief advantage programmed I/O architecture arises economic benefit: contain sophisticated digital circuits, devices rely programmed I/O inexpensive. chief disadvantage programmed I/O arises computational overhead: step requires processor interact I/O device. understand polling especially undesirable, must recall fundamental mismatch I/O devices computation: electromechanical, I/O devices operate several orders magnitude slower processor. Furthermore, processor uses polling control I/O device, amount time processor waits fixed, independent processor speed. important point summarized: typical processor much faster I/O device, speed system uses polling depends speed I/O device; using fast processor increase rate I/O performed. Turning statement around produces corollary: processor uses polling wait I/O device, using faster processor merely means processor execute instructions waiting device (i.e., loops, Figure 16.3, run faster). Thus, faster processor merely wastes cycles waiting I/O device — processor need poll, processor could performing computation instead. 16.10 Interrupt-Driven I/O 1950s 1960s, computer architects became aware mismatch speed processors I/O devices. difference particularly important first generation computers, used vacuum tubes, replaced second generation used solid-state devices. Although use solid-state devices (i.e., transistors) increased speed processors, speed I/O devices remained approximately same. Thus, architects explored ways overcome mismatch I/O processor speeds.One approach emerged superior, led revolution computer architecture produced third generation computers. Known interrupt mechanism, facility standard processor designs. central premise interrupt-driven I/O straightforward: instead wasting time polling, allow processor continue perform computation I/O device operates. device finishes, arrange device inform processor processor handle device. name implies, hardware temporarily interrupts computation progress handle I/O. device serviced, processor resumes computation exactly interrupted. practice, interrupt-driven I/O requires aspects computer system designed support interrupts, including: I/O device hardware Bus architecture functionality Processor architecture Programming paradigm I/O Device Hardware. Instead merely operating control processor, interrupt- driven I/O device must operate independently started. Later, finishes, device must able interrupt processor. Bus Architecture Functionality. bus must support two-way communication allows processor start operation device allows device interrupt processor operation completes. Processor Architecture. processor needs mechanism cause processor suspend normal computation temporarily, handle device, resume computation. Programming Paradigm. Perhaps significant change involves shift programming paradigm. Polling uses sequential, synchronous style programming programmer specifies step operation I/O device performs. see next chapter, interrupt-driven programming uses asynchronous style programming programmer writes code handle events. 16.11 Interrupt Mechanism Fetch-Execute term interrupt implies, device events temporary. device needs service (e.g., operation completes), hardware device sends interrupt signal bus processor. processor temporarily stops executing instructions, saves state information needed resume execution later, handles device. finishes handling interrupt, processor reloads saved state resumes executing exactly point interrupt occurred. is:An interrupt mechanism temporarily borrows processor handle I/O device. Hardware saves state computation interrupt occurs resumes computation interrupt processing finishes. application programmer’s point view, interrupt transparent, means programmer writes application code interrupts exist. hardware designed result computation interrupts occur, one interrupt occurs, many interrupts occur execution instructions. I/O hardware interrupt processor? fact, devices request service. Interrupts implemented modified fetch-execute cycle allows processor respond request. Algorithm 16.1 explains, interrupt occurs execution two instructions. Algorithm 16.1 Repeat forever { Test: device requested interrupt, handle interrupt, continue next iteration loop. Fetch: access next step program location program stored. Execute: Perform step program. } Algorithm 16.1 Fetch-Execute Cycle Handles Interrupts. 16.12 Handling Interrupt handle interrupt, processor hardware takes five steps Figure 16.5 lists. Figure 16.5 Five steps processor hardware performs handle interrupt. steps hidden programmer. Saving restoring state easiest understand: hardware saves information interrupt occurs (usually memory), special return interrupt instruction reloads saved state. architectures, hardware saves complete state information, including contents general-purpose registers. architectures, hardware saves basic information, instruction counter, requires software explicitly save restore values, general-purpose registers. case, saving restoring state symmetric operations — hardware designed instruction returns interrupt reloads exactly state information hardware saves interrupt occurs. say processor temporarily switches execution context handles interrupt. 16.13 Interrupt Vectors processor know device interrupting? Several mechanisms used. example, architectures use special-purpose coprocessor handle I/O. start device, processor sends requests coprocessor. device needs service, coprocessor detects situation interrupts processor. architectures use control signals bus inform processor interrupt needed. processor checks bus iteration fetch-execute cycle. detects interrupt request, interrupt hardware processor sends special command bus determine device needs service. bus arranged exactly one device respond time. Typically, device assigned unique number, device responds giving number. Numbers assigned devices random. Instead, numbers configured way allows processor hardware interpret number index array pointers reserved location memory. item array, known interrupt vector, pointer software handles device; say interrupts vectored. software known interrupt handler. Figure 16.6 illustrates data structure. figure shows simplest interrupt vector arrangement physical device assigned unique interrupt vector. practice, computer systems designed accommodate many devices often use variation multiple devices share common interrupt vector. interrupt occurs, code interrupt handler uses bus second time determine physical device interrupted. determines physical device, handler chooses interaction appropriate device. chief advantage sharing interrupt vector among multiple devices arises scale — processor fixed set interrupt vectors accommodate arbitrary number devices.Figure 16.6 Illustration interrupt vectors memory. vector points interrupt handler device. 16.14 Interrupt Initialization Disabled Interrupts values installed interrupt vector table? Software must initialize interrupt vectors neither processor device hardware enters modifies table. Instead, hardware blindly assumes interrupt vector table initialized — interrupt occurs, processor saves state, uses bus request vector number, uses value index table vectors, branches code address. matter address found vector, processor jump address attempt execute instruction. ensure interrupts occur table initialized, processors start mode interrupts disabled. is, processor continues run fetch-execute cycle without checking interrupts. Later, software (usually operating system) initialized interrupt vectors, software must execute special instruction explicitly enables interrupts. many processors, interrupt status controlled mode processor; interrupts automatically enabled processor changes initial startup mode mode suitable executing programs. 16.15 Interrupting Interrupt HandlerOnce interrupt occurs interrupt handler running, happens another device becomes ready requests interrupt? simplest hardware follows straightforward policy: interrupt occurs, interrupts automatically disabled current interrupt completes returns. Thus, never confusion. sophisticated processors offer multiple level interrupt mechanism also known multiple interrupt priorities. device assigned interrupt priority level, typically range 1 7. given time, processor said operating one priority levels. Priority zero means processor currently handling interrupt (i.e., running application); priority N greater zero means processor currently handling interrupt device assigned level N. rule is: operating priority level K, processor interrupted device assigned level K+1 higher. Note interrupt happens priority K, interrupts occur priority K lower. consequence one interrupt progress priority level. 16.16 Configuration Interrupts said device must assigned interrupt vector (possibly) interrupt priority. hardware device software running processor must agree assignments — device returns interrupt vector number, corresponding interrupt vector must point handler device. interrupt assignments made? Two approaches used: Manual assignment used small, embedded systems Automated assignment used computer systems Manual Assignment. small embedded systems still use method used early computers: manual approach computer owners configure hardware software. example, devices manufactured physical switches circuit board, switches used enter interrupt vector address. course, operating system must configured match values chosen devices. Automated Assignment. Automated interrupt vector assignment widely used approach eliminates manual configuration allows devices installed without requiring hardware modified. computer boots, processor uses bus determine devices attached. processor assigns interrupt vector number device, places copy appropriate device handler software memory, builds theinterrupt vector memory. course, automated assignment means higher delay booting computer. 16.17 Dynamic Bus Connections Pluggable Devices description buses interrupt configuration assumed devices attached bus computer powered down, interrupt vectors assigned startup, devices remain place computer operates. Early buses indeed designed described. However, recent buses invented permit devices connected disconnected computer running. say buses support pluggable devices. example, Universal Serial Bus (USB) permits user plug device time. USB operate? essence, USB appears single device computer’s main bus. computer boots, USB assigned interrupt vector usual, handler placed memory. Later, user plugs new device, USB hardware generates interrupt, processor executes handler. handler, turn, sends request USB bus interrogate devices determine device attached. identifies device, USB handler loads secondary device-specific handler. device needs service, device requests interrupt. USB handler receives interrupt, determines device interrupted, passes control device-specific handler. 16.18 Interrupts, Performance, Smart Devices interrupt mechanism cause revolution computer architecture? answer easy. First, I/O important aspect computing must optimized. Second, interrupt- driven I/O automatically overlaps computation I/O without requiring programmer take special action. is, interrupts adapt speed processor I/O devices automatically. programmer need estimate many instructions performed I/O operation, interrupts never underestimate overestimate. summarize: computer uses interrupts easier program offers better overall performance computer uses polling. addition, interrupts allow speed processor adapt speed I/O devices automatically. Interestingly, basic interrupt mechanism invented, architects realized improvements possible. understand improvements, consider disk device. underlying hardware requires several steps read data disk place memory. Figure 16.7 summarizes steps.Figure 16.7 Example steps required read block disk device. Early hardware required processor handle step starting operation waiting interrupt. example, processor verify disk spinning. disk idle, processor issue command started motor wait interrupt. key insight digital logic I/O device contains, less device needs rely processor. Informally, architects use term dumb device refer device requires processor handle step term smart device characterize device perform series steps own. smart version disk device contains sufficient logic (perhaps even embedded processor) handle steps involved reading block. Thus, smart device interrupt often, require processor handle step. Figure 16.8 lists example interaction processor smart disk device. Figure 16.8 interaction processor smart disk device reading disk block. discussion device interaction omitted many details. example, I/O devices detect report errors (e.g., disk spin flaw surface prevents hardware reading disk block). Thus, interrupt processing complex described: interrupt occurs, processor must interrogate CSRs associated disk determine whether operation successful error occurred. Furthermore, devices report soft errors (i.e., temporary errors), processor must retry operation determine whether error temporary permanent. 16.19 Direct Memory Access (DMA)The discussion implies smart I/O device transfer data memory without using CPU. Indeed, transfers possible key high-speed I/O. technology allows I/O device interact memory known Direct Memory Access (DMA). understand DMA, recall architectures, memory I/O devices attach central bus. Thus, direct path I/O device memory. imagine smart I/O device contains embedded processor, idea behind DMA clear: embedded processor I/O device issues fetch store requests memory responds. course, bus design must make possible multiple processors (the main processor embedded processor smart device) take turns sharing bus prevent sending multiple requests simultaneously. bus supports mechanism, I/O device transfer data memory device without using processor. summarize: technology known Direct Memory Access (DMA) allows smart I/O device access memory directly. DMA improves performance allowing device transfer data device memory without using processor. 16.20 Extending DMA Buffer Chaining may seem smart device using DMA sufficient guarantee high performance: data transferred device memory without using processor, device interrupt step operation. However, optimization discovered improves performance. understand DMA improved, consider high-speed network. Packets tend arrive network bursts, means set packets arrives back-to-back minimum time successive packets. network interface device uses DMA, device interrupt processor accepting incoming packet placing packet memory. processor must specify location buffer next packet restart device. sequence events must occur quickly (i.e., next packet arrives). Unfortunately, devices system may also generating interrupts, means processor may delayed slightly. highest-speed networks, processor may able service interrupt time capture next packet. solve problem back-to-back arrivals, smart I/O devices use technique known buffer chaining. processor allocates multiple buffers, creates linked list memory. processor passes list I/O device, allows device fill buffer. smart device use bus read values memory, device follow linked list place incoming packets successive buffers. Figure 16.9 illustrates concept†.Figure 16.9 Illustration buffer chaining. processor passes list buffers smart I/O device, device fills buffer list without waiting processor. network example given describes use buffer chaining high-speed input. buffer chain also used output: processor places data set buffers, links buffers list, passes address linked list smart I/O device, starts device. device moves list, taking data buffer memory sending data device. 16.21 Scatter Read Gather Write Operations Buffer chaining especially helpful computer systems buffer size used software smaller size data block used I/O device. input, chained buffers allow device divide large data transfer set smaller buffers. output, chained buffers allow device extract data set small buffers combine data single block. example, operating systems create network packet placing packet header one buffer packet payload another buffer. Buffer chaining allows operating system send packet without overhead copying bytes single, large buffer. use term scatter read capture idea dividing large block incoming data multiple small buffers, term gather write capture idea combining data multiple small buffers single output block. course, make buffer chaining useful, linked list output buffers must specify size buffer (i.e., number bytes write). Similarly, linked list input buffers must include length field device set specify many bytes deposited buffer. 16.22 Operation Chaining Although buffer chaining handles situations given operation repeated many buffers, optimization possible cases device perform multiple operations. understand, consider disk device offers read write operations individual blocks.To optimize performance, need start another operation soon current operation completes. Unfortunately, operations mixture reads writes. technology used start new operation without delay known operation chaining. Like buffer chaining, processor uses operation chaining must create linked list memory, must pass list smart device. Unlike buffer chaining, however, nodes linked list specify complete operation: addition buffer pointer, node contains operation necessary parameters. example, node list used disk might specify read operation disk block. Figure 16.10 illustrates operation chaining. Figure 16.10 Illustration operation chaining smart disk device. node specifies operation (R W), disk block number, buffer memory. 16.23 Summary Two paradigms used handle I/O devices: programmed I/O interrupt-driven I/O. Programmed I/O requires processor handle step operation polling device. processor much faster I/O device, processor spends many cycles waiting device. Third-generation computers introduced interrupt-driven I/O allows device perform complete operation informing processor. processor uses interrupts includes extra hardware tests execution fetch-execute cycle see whether device requested interrupt. Interrupts vectored, means interrupting device supplies unique integer processor uses index array pointers handlers. guarantee interrupts affect running program, hardware saves restores state information interrupt. Multilevel interrupts used give devices priority others. Smart I/O devices contain additional logic allows perform series steps without assistance processor. Smart devices use techniques buffer chaining operation chaining optimize performance. EXERCISES16.1 Assume RISC processor takes two microseconds execute instruction I/O device wait 1 millisecond interrupt serviced. maximum number instructions executed interrupts disabled? 16.2 List explain two I/O paradigms. 16.3 Expand acronym CSR explain means. 16.4 software engineer trying debug device driver, discovers appears infinite loop: (*csrptr->tstbusy != 0) ; /* nothing*/ software engineer shows code, respond? 16.5 Read devices bus interrupt priorities assigned each. disk mouse higher priority? Why? 16.6 systems, part device driver code must written assembly language. Why? 16.7 Conceptually, data structure interrupt vector, one find entry interrupt vector? 16.8 significant advantage device uses chained operations? 16.9 chief advantage interrupts polling? 16.10 Suppose user installs ten devices perform DMA single computer attempts operate devices simultaneously. components computer might become bottleneck? 16.11 smart disk device uses DMA blocks disk contain 512 bytes, many times disk interrupt processor transfers 2048 bytes (four separate blocks)? 16.12 device uses chaining, type data structure device driver places memory give set commands device? †Although figure shows three buffers, network devices typically use chain 32 64 buffers.17 Programmer’s View Devices, I/O, Buffering Chapter Contents 17.1 Introduction 17.2 Definition Device Driver 17.3 Device Independence, Encapsulation, Hiding 17.4 Conceptual Parts Device Driver 17.5 Two Categories Devices 17.6 Example Flow Device Driver 17.7 Queued Output Operations 17.8 Forcing Device Interrupt 17.9 Queued Input Operations 17.10 Asynchronous Device Drivers Mutual Exclusion 17.11 I/O Viewed Application 17.12 Library/Operating System Dichotomy 17.13 I/O Operations OS Supports 17.14 Cost I/O Operations 17.15 Reducing System Call Overhead 17.16 Key Concept Buffering 17.17 Implementation Buffered Output 17.18 Flushing Buffer17.19 Buffering Input 17.20 Effectiveness Buffering 17.21 Relationship Caching 17.22 Example: C Standard I/O Library 17.23 Summary 17.1 Introduction Previous chapters cover hardware aspects I/O. explain bus architecture used interconnect devices, processors, memory, well interrupt mechanism external device uses inform processor operation completes. chapter changes focus software, considers I/O programmer’s perspective. chapter examines software needed control device application software uses I/O facilities. understand important concept device driver, see driver implements operations like read write. learn devices divided two broad types: byte-oriented block-oriented, understand interaction used each. Although programmers write device drivers, understanding device driver operates low-level I/O occurs help programmers write efficient applications. looked mechanics device drivers, focus concept buffering, see essential programmers use buffering. 17.2 Definition Device Driver previous chapter explains basic hardware interrupt mechanism. ready consider low-level software uses interrupt mechanism perform I/O operations. use term device driver refer software provides interface application program external hardware device. cases, computer system device driver external device, applications access given device use driver. Typically, device drivers part computer’s operating system, means application running computer uses device driver communicating device. device driver understands details particular hardware device, say driver contains low-level code. driver interacts device bus, understands device’s Control Status Registers (CSRs), handles interrupts device.17.3 Device Independence, Encapsulation, Hiding primary purpose device driver device independence. is, device driver approach removes hardware details application programs relegates driver. understand device independence important, need know early software built. application program designed specific brand computer, specific memory size, specific set I/O devices. application contained code needed use bus communicate particular devices. Unfortunately, program written use specific set devices could used devices. example, upgrading printer newer model required application programs rewritten. device driver solves problem providing device-independent interface applications. example, applications use printer rely printer’s device driver, application detailed knowledge hardware built in. Consequently, changing printer requires changing device driver; applications remain unchanged. say device driver hides hardware details applications device driver encapsulates hardware details. summarize: device driver consists software understands handles low-level details communication particular device. device driver provides high-level interface applications, application program need change device changes. 17.4 Conceptual Parts Device Driver device driver contains multiple functions must work together, including code communicate bus, code handle device details, code interact application. Furthermore, device driver must interact computer’s operating system. help manage complexity, programmers think device driver partitioned three parts: lower half comprised handler invoked interrupt occurs upper half comprised functions invoked applications request I/O operations set shared variables hold state information used coordinate two halves names upper half lower half reflect view programmer writes device drivers: hardware low level application programs high level. Thus, programmer thinks applications top hierarchy hardware bottom. Figure 17.1 illustrates programmer’s view.Figure 17.1 conceptual division device driver three parts. device driver provides interface applications operate high level device hardware operates low level. 17.5 Two Categories Devices understand device drivers, need know interface hardware presents driver. Devices divided two broad categories, depending style interface device uses: Character-oriented devices Block-oriented devices character-oriented device transfers single byte data time. example, serial interface used connect keyboard computer transfers one character (i.e., byte) keystroke. device driver’s point view, character-oriented device generates interrupt time character sent received — sending receiving block N characters generates N interrupts. block-oriented device transfers entire block data time. cases, underlying hardware specifies block size, B, blocks must contain exactly B bytes. example, disk device defines block size equal disk’s sector size. cases, however, blocks variable size. example, network interface defines block aslarge packet (although places upper-bound packet size, packet switching hardware allows packet sizes vary one packet next). device driver’s point view, block-oriented device generates one interrupt time block sent received. 17.6 Example Flow Device Driver details programming device drivers beyond scope text. However, help us understand concept, consider device driver might handle basic output. example, assume application sends data Internet. application specifies data sent, protocol software creates packet transfers packet device driver network device. Figure 17.2 illustrates modules involved packet transfer, lists steps taken output. figure shows, even straightforward operation requires complex sequence steps. application sends data, application process enters operating system control passes protocol software creates packet. protocol software, turn, passes outgoing packet upper half appropriate device driver. device driver places packet shared variables section, starts device performing packet transmission, returns protocol software returns application process. Although control returned operating system, outgoing packet remains shared variables data area device use DMA access it. device completes sending packet, device interrupts control passes lower half. lower half removes packet shared area.Figure 17.2 simplified example steps occur application requests output operation. device driver located operating system handles communication device. 17.7 Queued Output Operations Although design used example driver feasible, approach inefficient use production system. particular, application sends another packet device finished sending first one, device driver must poll device finishes using packet. avoid waiting, device drivers used production systems implement queue ofrequests. output, upper half wait device ready. Instead, upper half deposits data written queue, ensures device generate interrupt, returns application. Later, device finishes current operation generates interrupt, lower half extracts next request queue, starts device, returns interrupt. Figure 17.3 illustrates conceptual organization. Figure 17.3 conceptual organization device driver uses queue requests. output, upper half deposits items request queue without waiting device, lower half controls device. device driver uses output queue elegant — queue requests provides coordination upper lower halves driver. Figure 17.4 lists steps half device driver takes output. Figure 17.4 steps upper lower halves device driver take output operation queueing used. upper half forces interrupt, start output device. figure indicates, steps half device driver straightforward. Notice lower half performs work: addition handling interrupts device, lower half checks queue and, queue empty, extracts next item starts thedevice. device interrupts time completes operation, lower half invoked per output operation, allows start next operation. Thus, lower half continue invoked queue empty. happens last item removed queue? lower half invoked last output operation completes, find queue empty. point, device idle. prevent useless interrupts, lower half controls device stop interrupts. Later, application calls upper half place new item queue, upper half start device interrupting again, output proceed. 17.8 Forcing Device Interrupt request queue used many device drivers, engineers designed hardware works well programming paradigm outlined Figure 17.4. particular, device often includes CSR bit processor set force device interrupt. Recall Chapter 16 code required set CSR bit trivial — consists single assignment statement. Software need check current device status. Instead, mechanism designed setting bit effect device already active: device CSR bit, B, used force device interrupt device idle, setting bit B causes device generate interrupt device currently performing operation, setting bit B effect words, interrupt already destined occur current operation completes, device waits operation complete generates interrupt usual; operation progress, device generates interrupt immediately. concept — arranging hardware setting CSR bit affect busy device operation completes — greatly simplifies programming. see why, look steps Figure 17.4 lists. upper half need test whether device busy (i.e., whether operation progress). Instead, upper half always sets CSR bit. operation already progress, device hardware ignores bit set, waits operation completes. device idle, setting bit causes device interrupt immediately, forces lower half select next request queue start device. 17.9 Queued Input Operations device driver also use queueing input. However, additional coordination required two reasons. First, device driver configured accept input application ready read input (e.g., case user types ahead). Therefore, input queue must created device initialized. Second, input arrive application reads, device driver must temporarily block application input arrive. Figure 17.5 lists steps device driver uses handle input queue present.Figure 17.5 steps upper lower halves device driver take input operation queueing used. upper half temporarily stops application data becomes available. Although description device drivers omits many details, gives accurate picture general approach device drivers use. summarize: production device driver uses input output queues store items. upper half places request queue, lower half handles details communication device. 17.10 Asynchronous Device Drivers Mutual Exclusion Chapter 16, said interrupt mechanism implies asynchronous programming model. understand why. Like conventional program, polling synchronous control passes code beginning end. device driver handles interrupts asynchronous programmer writes separate pieces code respond events. One upper-half routines invoked application requests I/O. lower- half routine invoked I/O operation occurs interrupt occurs, initialization routine invoked device started. Asynchronous programming challenging synchronous programming. events occur order, programmer must use shared variables encode current state computation (i.e., events occurred past effect). difficult test asynchronous programs programmer cannot easily control sequence events. important, applications running processor device hardware generate events simultaneously. Simultaneous events make programming asynchronous devicedrivers especially difficult. example, consider smart device uses command chaining. processor creates linked list operations memory, device follows list performs operations automatically. programmer must coordinate interaction processor smart device. understand why, imagine smart device extracting items list time upper half driver adding items. problem occur smart device reaches end list stops processing device driver adds new item. Similarly, two independent pieces hardware attempt manipulate pointers list simultaneously, links become invalid. avoid errors caused simultaneous access, device driver interacts smart device must implement mutual exclusion. is, device driver must ensure smart device access list changes completed, smart device must ensure device driver access list changes completed. variety schemes used ensure exclusive access. example, devices special CSR values processor set temporarily stop device accessing command list. systems facility allows processor temporarily restrict use bus (if cannot use bus, smart device cannot make changes list memory). Finally, processors offer test-and-set instructions used provide mutual exclusion. 17.11 I/O Viewed Application sections describe device driver programmed. said earlier programmers write device drivers. Thus, details CSR addresses, interrupt vectors, request queues remain hidden typical programmer. motivation considering device drivers low-level I/O background: helps us understand create applications use low-level services efficiently. tend use high-level languages, programmers invoke low-level I/O facilities directly — express I/O operations, programmer uses abstractions programming language offers. example, application programs seldom use disk device. Instead, programming language underlying system presents programmer high- level file abstraction. Similarly, instead exposing programmer display hardware, systems present programmer window abstraction. point is: many programming systems, I/O hidden programmer. Instead manipulating hardware devices, disks display screens, programmer uses abstractions files windows. Even embedded systems allow application programmers control I/O devices, software usually designed hide many details possible programmer. Inparticular, application specify generic, high-level I/O operations. compiler translates program binary form use specific computer, compiler maps high-level I/O operation sequence low-level steps. Interestingly, typical compiler translate I/O operation directly sequence basic machine instructions. Instead, compiler generates code invokes library functions perform I/O operations. Therefore, executed, program must combined appropriate library functions. use term run-time library refer set library functions accompany compiled program. course, compiler run-time library must designed work together — compiler must know functions available, exact arguments used function, meaning function. Application programmers seldom interact device drivers directly. Instead, rely run-time library act intermediary. chief advantage using run-time library intermediary arises flexibility ease change. run-time library functions understand use underlying I/O mechanisms (i.e., device drivers). I/O hardware and/or device drivers change, run-time library needs updated — compiler remain unchanged. fact, separating run-time library compiler allows code compiled combined various run-time libraries produce images one version operating system. 17.12 Library/Operating System Dichotomy know device driver resides operating system run-time library functions application uses perform I/O reside outside operating system (because linked application). Conceptually, imagine three layers software top device hardware Figure 17.6 illustrates.Figure 17.6 conceptual arrangement application code, run-time library code, device driver interfaces labeled. Several questions arise. services layer software provide? interface application run-time library, interface run-time library operating system? relative costs using two interfaces? 17.13 I/O Operations OS Supports begin examining interface run-time library operating system. low-level programming language C, operating system interface directly available applications. Thus, programmer choose use I/O library make operating system calls directly†. Although exact details I/O operations depend operating system, general approach become popular. Known open/read/write/close paradigm, approach offers six basic functions. Figure 17.7 lists functions names used Unix operating system. Figure 17.7 Six basic I/O functions comprise open/read/write/close paradigm. names taken Unix operating system.As example, consider device read write Digital Video Disk (DVD). open function used start drive motor ensure disc inserted. drive started, read function used read data disc, write function used write data onto disc. seek function used move new position (e.g., specific video segment), close function used power disc. Finally, ioctl function (an abbreviation I/O control) used functions (e.g., eject function). course, operations takes arguments specify details. example, write operation needs arguments specify device use, location data, amount data write. important, device driver must understand map operation arguments operations underlying device. example, driver receives control operation, eject, driver must know implement operation device hardware (e.g., assign values device’s CSR registers). 17.14 Cost I/O Operations application program invokes function run-time library, cost exactly calling function copy code library function incorporated application program built. Thus, cost invoking library functions relatively low. application program run-time library function invokes I/O operation read write, however, control must pass system call† appropriate device driver operating system. Unfortunately, invoking operating system function system call incurs extremely high overhead. three reasons. First, processor must change privilege mode operating system runs greater privilege application. Second, processor must change address space application’s virtual address space operating system’s address space. Third, processor must copy data application’s address space operating system’s address space. summarize: overhead involved using system call communicate device driver high; system call much expensive conventional function call, call used invoke library function. important, much system call overhead associated making call rather work performed driver. Therefore, optimize performance, programmers seek ways minimize number system calls.17.15 Reducing System Call Overhead understand reduce overhead system calls, consider worst-case example. Suppose application needs print document, suppose printing requires application send total N bytes data printer. highest cost occurs application makes separate system call transfer byte data application make total N system calls. alternative, application generates complete line text makes system call transfer entire line, overhead reduced N system calls L system calls, L number lines document (i.e., L <N). reduce overhead printing document? Yes, can. application redesigned allocate enough memory hold entire page document, generate page, make one system call transfer entire page device driver. result application makes P system calls, P number pages document (presumably P <<N). general principle stated: reduce overhead optimize I/O performance, programmer must reduce number system calls application invokes. key reducing system calls involves transferring data per system call. course, always possible reduce number system calls used I/O. example, application like text editor email composer displays characters user enters them. application cannot wait user enters entire line text entire page character must appear screen immediately. Similarly, input keyboard often requires program accept one character time without waiting user enter entire line page. Fortunately, applications often involve user interaction I/O relatively slow, optimization unimportant. 17.16 Key Concept Buffering discussion shows application programmer optimize I/O performance rewriting code way number systems calls lower. optimization important high-speed I/O incorporated computer software. Instead requiring programmer rewrite code, I/O runtime libraries designed handle optimization automatically. use term buffering describe concept accumulating data I/O transfer, term buffer refer area memory data placed.The buffering principle: reduce number system calls, accumulate data buffer, transfer large amount data time system call made. automate buffering, library routines need scheme works application. Thus, instead lines pages, library functions use fixed-size buffer. take advantage buffering, application must call library functions instead operating system. case programming language contains built-in I/O facilities, run-time library implements buffering, compiler generates code invokes appropriate library routines; case programming language built-in I/O facilities, programmer must call buffering library routines instead system calls. Library routines implement buffering usually provide five conceptual operations Figure 17.8 lists. Figure 17.8 conceptual operations provided typical library offers buffered I/O. operations listed figure analogous operating system offers interface device. fact, see least one implementation buffered I/O library uses function names variants open, read, write, close. Figure 17.8 uses alternate terminology help clarify distinction. 17.17 Implementation Buffered Output understand buffering works, consider application uses buffered output functions Figure 17.8. begins, application calls setup function initialize buffering. implementations provide argument allows application specify buffer size; implementations, buffer size constant†. case, assume setup allocates buffer, initializes buffer empty. buffer initialized, application call output function transfer data. call, application supplies one bytes data. Finally, finishes transferring data, application calls terminate function. (Note: later section describes use function flush). amount code required implement buffered output trivial. Figure 17.9 describes steps used implement output function. language C, step implemented one two lines code.The motivation terminate function clear: output buffered, buffer may partially full application finishes. Therefore, application must force remaining contents buffer written. Figure 17.9 steps taken achieve buffered output. 17.18 Flushing Buffer may seem output buffering cannot used applications. example, consider application allows two users communicate computer network. emits message, application assumes message transmitted delivered end. Unfortunately, buffering used, message may wait buffer unsent. course, programmer rewrite application buffer data internally make system calls directly. However, designers general-purpose buffering libraries devised way permit applications use buffered I/O specify system call needed. mechanism consists flush function application call force data sent even buffer full. Programmers use phrase flushing buffer describe process forcing output partially full buffer. buffer empty application calls flush, call effect. buffer contains data, however, flush function makes system call write data, resets global pointer indicate buffer empty. Figure 17.10 lists steps flush operation.Figure 17.10 steps required implement flush function buffered I/O library. Flush allows application force data written buffer full. Look back implementation terminate function given Figure 17.9. library offers flush function, first step terminate replaced call flush function. summarize: programmer uses flush function specify outgoing data buffer sent even buffer full. flush operation effect buffer currently empty. 17.19 Buffering Input descriptions explain buffering used output. many cases, buffering also used reduce overhead input. understand how, consider reading data sequentially. application reads N bytes data, one byte time, application make N system calls. Assuming underlying device allows transfer one byte data, buffering used reduce number system calls. application (or run-time library) allocates large buffer, makes one system call fill buffer, satisfies subsequent requests buffer. Figure 17.11 lists steps required. output buffering, implementation straightforward. language C, step implemented trivial amount code. 17.20 Effectiveness Buffering buffering important? even small buffer large effect I/O performance. see why, observe buffered I/O used, system call needed per buffer†. result, buffer N bytes reduces number system calls factor ofN. Thus, application makes system calls, buffer 8 K bytes reduces number system calls / 8192. Figure 17.11 steps required achieve buffered input. Buffering limited run-time libraries. technique important device drivers often implement buffering. example, disk drivers, driver maintains copy disk block memory, allows application read write data block. course, buffering operating system eliminate system calls. However, buffering improve performance external data transfers slower system calls. important point buffering used reduce I/O overhead whenever less expensive operation substituted expensive operation. summarize importance buffering: Using buffer N bytes reduce number calls underlying system factor N. large buffer mean difference I/O mechanism fast one intolerably slow. 17.21 Relationship Caching Buffering closely related concept caching described Chapter 12. chief difference arises way items accessed: cache system optimized accommodate random access, buffering system optimized sequential access.In essence, cache stores items referenced, buffer stores items referenced (assuming sequential references). Thus, virtual memory system, cache stores entire pages memory — byte page referenced, entire page placed cache. contrast, buffer stores sequential bytes. Thus, byte referenced, buffering system preloads next bytes — referenced byte lies end page, buffering system preloads bytes next page. 17.22 Example: C Standard I/O Library One best-known examples buffering I/O library created C programming language Unix operating system. Known standard I/O library (stdio), library supports input output buffering. Figure 17.12 lists functions found Unix standard I/O library along purpose. Figure 17.12 Examples functions included standard I/O library used Unix operating system. library includes additional functions listed here. 17.23 Summary Two aspects I/O pertinent programmers. systems programmer writes device driver code must understand low-level details device, application programmer uses I/O facilities must understand relative costs. device driver divided three parts: upper half interacts application programs, lower half interacts device itself, set shared variables. function upper half receives control application reads writes data; lower half receives control device generates input output interrupt. fundamental technique programmers use optimize sequential I/O performance known buffering. Buffering used input output, often implemented run- time library. gives application control data transferred, flush operation allows buffering used arbitrary applications.Buffering reduces system call overhead transferring data per system call. Buffering provides significant performance improvement buffer N bytes reduces number system calls application makes factor N. EXERCISES 17.1 device driver provide, device drivers make easier write applications? 17.2 Name three conceptual parts device driver state used. 17.3 Explain use output queue device driver describing items inserted queue, well removed. 17.4 user invokes app writes file. app displays progress bar shows much file written. progress bar reaches 50%, battery fails device crashes. user reboots device, discovers less 20% file actually written. Explain app reported writing 50%. 17.5 program calls fputc, program invoke? 17.6 flush operation, needed? 17.7 increase performance app, programmer rewrites app instead reading one byte time, app reads eight thousand bytes processes them. technique programmer using? 17.8 Compare time needed copy large file using write fwrite. 17.9 standard I/O function fseek allows random access. Measure difference time required use fseek within small region file within large region. 17.10 Build output buffering routine, bufputc, accepts argument character printed. call bufputc, store character buffer, call write entire buffer. Compare performance buffered routine program uses write character. †A later section discusses standard I/O library used C. †Some computer architectures use term trap place system call. †Typical buffer sizes range 8 Kbytes 128 Kbytes, depending computer system. †Our analysis ignores situations application calls flush frequently.Part V Advanced Topics Fundamental Concepts Parallelism Pipelining18 Parallelism Chapter Contents 18.1 Introduction 18.2 Parallel Pipelined Architectures 18.3 Characterizations Parallelism 18.4 Microscopic Vs. Macroscopic 18.5 Examples Microscopic Parallelism 18.6 Examples Macroscopic Parallelism 18.7 Symmetric Vs. Asymmetric 18.8 Fine-grain Vs. Coarse-grain Parallelism 18.9 Explicit Vs. Implicit Parallelism 18.10 Types Parallel Architectures (Flynn Classification) 18.11 Single Instruction Single Data (SISD) 18.12 Single Instruction Multiple Data (SIMD) 18.13 Multiple Instructions Multiple Data (MIMD) 18.14 Communication, Coordination, Contention 18.15 Performance Multiprocessors 18.16 Consequences Programmers 18.17 Redundant Parallel Architectures 18.18 Distributed Cluster Computers 18.19 Modern Supercomputer 18.20 Summary 18.1 Introduction Previous chapters cover three key components computer architecture: processors, memory systems, I/O. chapter begins discussion fundamental concepts cross boundaries among architectural components. chapter focuses use parallel hardware, shows parallelism used throughout computer systems increase speed. chapter introduces terminology concepts, presents taxonomy parallel architectures, examines computer systems parallelism fundamental paradigm around entire system designed. Finally, chapter discusses limitations problems parallel architectures. next chapter extends discussion examining second fundamental technique: pipelining. see parallelism pipelining important high-speed designs. 18.2 Parallel Pipelined Architectures computer architects assert two fundamental techniques used increase hardware speed: parallelism pipelining. already encountered examples technique, seen used. architects take broader view parallelism pipelining, using techniques fundamental basis around system designed. many cases, architecture completely dominated one two techniques resulting system informally called parallel computer pipelined computer. 18.3 Characterizations Parallelism Rather classify architecture parallel nonparallel, computer architects use variety terms characterize type amount parallelism present given design. many cases, terminology describes possible extremes type parallelism. classify architecture stating architecture lies two extremes. Figure 18.1 lists key characterizations using nomenclature proposed Michael J. Flynn classic paper†. Later sections explain terms give examples.Figure 18.1 Terminology used characterize amount type parallelism present computer architecture. 18.4 Microscopic Vs. Macroscopic Parallelism fundamental; architect cannot design computer without thinking parallel hardware. Interestingly, pervasiveness parallelism means unless computer uses unusual amount parallel hardware, typically discuss parallel aspects. capture idea much parallelism computer remains hidden inside subcomponents, use term microscopic parallelism. Like microbes world around us, microscopic parallelism present, stand without closer inspection. point is: Parallelism fundamental virtually computer systems contain form parallel hardware. use term microscopic parallelism characterize parallel facilities present, especially visible. precise, say microscopic parallelism refers use parallel hardware within specific component (e.g., inside processor inside ALU), whereas macroscopic parallelism refers use parallelism basic premise around system designed. 18.5 Examples Microscopic Parallelism earlier chapters, seen examples using microscopic parallelism within processors, memory systems, I/O subsystems. following paragraphs highlight examples. ALU. Arithmetic Logic Unit handles logical arithmetic operations. ALUs perform integer arithmetic processing multiple bits parallel. Thus, ALU designed operate integers contains parallel hardware allows ALU compute Boolean function pair thirty-two bit values single operation. alternative consists ALU processes one bit time, approach known bit serial processing. shouldbe easy see bit serial processing takes much longer computing bits parallel. Therefore, bit serial arithmetic reserved special cases. Registers. general-purpose registers CPU make heavy use microscopic parallelism. bit register implemented separate digital circuit (specifically, latch). Furthermore, guarantee highest-speed computation, parallel data paths used move data general-purpose registers ALU. Physical Memory. another example microscopic parallelism, recall physical memory system uses parallel hardware implement fetch store operations — hardware designed transfer entire word operation. ALU, microscopic parallelism increases memory speed dramatically. example, memory system implements sixty-four bit words access store approximately sixty-four times much data time memory system accesses single bit time. Parallel Bus Architecture. seen, central bus computer usually uses parallel hardware achieve high-speed transfers among processor, memory, I/O devices. typical modern computer bus either thirty-two- sixty-four-bits wide, means either thirty-two sixty-four bits data transferred across bus single step. 18.6 Examples Macroscopic Parallelism examples previous section demonstrate, microscopic parallelism essential high-speed performance — without parallel hardware, various components computer system cannot operate high speed. Computer architects aware global architecture often greater impact overall system performance performance single subsystem. is, adding parallelism single subsystem may improve overall system performance†. achieve greatest impact, parallelism must span multiple components system — instead merely using parallelism improve performance single component, system must allow multiple components work together. use term macroscopic parallelism characterize use parallelism across multiple, large-scale components computer system. examples clarify concept. Multiple, Identical Processors. Systems employ macroscopic parallelism usually employ multiple processors one form another. example, PCs advertised dual core quad core computers, meaning PC contains two four copies processor single chip. chip arranged allow processors operate time. hardware control exactly cores used. Instead, operating system assigns code core. example, operating system assign one core task handling I/O (i.e., running device drivers), assign cores application programs run. Multiple, Dissimilar Processors. Another example macroscopic parallelism arises systems make extensive use special-purpose coprocessors. example, computer optimized high-speed graphics might four displays attached, special graphics processor running display. graphics processor, typically found interface card, doesnot use architecture CPU graphics processor needs instructions optimized graphics operations. 18.7 Symmetric Vs. Asymmetric use term symmetric parallelism characterize design uses replications identical elements, usually processors cores, operate simultaneously. example, multicore processors mentioned said symmetric cores identical. alternative symmetric parallel design parallel design asymmetric. name implies, asymmetric design contains multiple elements function time, differ one another. example, PC CPU, graphics coprocessor, math coprocessor, I/O coprocessor classified using asymmetric parallelism four processors operate simultaneously, differ one another internally†. 18.8 Fine-grain Vs. Coarse-grain Parallelism use term fine-grain parallelism refer computers provide parallelism level individual instructions individual data elements, term coarse-grain parallelism refer computers provide parallelism level programs large blocks data. example, graphics processor uses sixteen parallel hardware units update sixteen bytes image time said use fine-grain parallelism. contrast, dual core PC uses one core print document another core composes email message described using coarse-grain parallelism. 18.9 Explicit Vs. Implicit Parallelism architecture hardware handles parallelism automatically without requiring programmer initiate control parallel execution said offer implicit parallelism, architecture programmer must control parallel unit said offer explicit parallelism. consider advantages disadvantages explicit implicit parallelism later. 18.10 Types Parallel Architectures (Flynn Classification) Although many systems contain multiple processors one type another, term parallel architecture usually reserved designs permit arbitrary scaling. is, refer parallel architecture, architects usually mean design number processors arbitrarily large (or least reasonably large). example, consider computer canhave either one two processors. Although adding second processor increases parallelism, architecture usually classified dual-processor computer rather parallel architecture. Similarly, PC four cores classified quad-core PC. However, cluster thirty-two interconnected PCs scale one thousand twenty-four PCs classified parallel architecture. easiest way understand parallel architectures divide architectures broad groups, group represents type parallelism. course, division absolute — practical computer systems hybrids contain facilities one group. Nevertheless, use classification define basic concepts nomenclature allow us discuss characterize systems. popular way describe parallelism attributed Flynn considers whether processing data replicated. Known Flynn classification, system focuses whether computer multiple, independent processors running separate program single program applied multiple data items. Figure 18.2 lists terms used Flynn classification define types parallelism; next sections explain terminology give examples. Figure 18.2 Terminology used Flynn classification characterize parallel computers†. 18.11 Single Instruction Single Data (SISD) phrase Single Instruction Single Data stream (SISD) used describe architecture support macroscopic parallelism. term sequential architecture uniprocessor architecture often used place SISD emphasize architecture parallel. essence, SISD refers conventional (i.e., Von Neumann) architecture — processor runs standard fetch-execute cycle performs one operation time. term refers idea single, conventional processor executing instructions operate single data item. is, unlike parallel architecture, conventional processor execute one instruction time, instruction refers single computation. course, seen SISD computer use parallelism internally. example, ALU may able perform operations multiple bits parallel, CPU may invoke coprocessor, CPU may mechanisms allow fetch operands two banks memory time. However, overall effect SISD architecture sequential execution instructions operate one data item.18.12 Single Instruction Multiple Data (SIMD) phrase Single Instruction Multiple Data streams (SIMD) used describe parallel architecture instruction specifies single operation (e.g., integer addition), instruction applied many data items time. Typically, SIMD computer sufficient hardware handle sixty-four simultaneous operations (e.g., sixty-four simultaneous additions). Vector Processors. SIMD architecture useful applications word processing email. Instead, SIMD used applications apply operation set values. example, graphics applications scientific applications work well SIMD architecture apply operation large set values. architecture sometimes called vector processor array processor mathematical concept vectors computing concept arrays. example SIMD machine works, consider normalizing values vector, V, contains N elements. Normalization requires item vector multiplied floating point number, Q. sequential architecture (i.e., SISD architecture), algorithm required normalize vector consists loop Figure 18.3 shows. Figure 18.3 sequential algorithm vector normalization. SIMD architecture, underlying hardware apply arithmetic operation values array simultaneously (assuming size array exceed parallelism hardware). example, single step, hardware sixty-four parallel units multiply value array sixty-four elements constant. Thus, algorithm perform normalization array SIMD computer takes one step: course, vector V larger hardware capacity, multiple steps required. important point vector instruction SIMD architecture merely shorthand loop. Instead, underlying system contains multiple hardware units operate parallel provide substantial speedup; performance improvement significant, especially computations use large matrices. course, instructions SIMD architecture applied array values. Instead, architect identifies subset operations used vectors, defines special vector instruction each. example, normalization entire array possible architect chooses include vector multiplication instruction multiplies value vector constant.In addition operations use constant vector, SIMD computers usually provide instructions use two vectors. is, vector instruction takes one operands specify vector. example, SIMD architectures used problems involving matrix multiplication. SIMD machines, operand specifies vector gives two pieces information: location vector memory integer specifies size vector (i.e., number items vector). machines, vector instructions controlled special-purpose registers — address size vector loaded registers vector instruction invoked. case, software determines number items vector maximum size supported hardware†. Graphics Processors. SIMD architectures also popular use graphics. understand why, important know typical graphics hardware uses sequential bytes memory store values pixels screen. example, consider video game foreground figures move background scene stays place. Game software must copy bytes correspond foreground figure one location memory another. sequential architecture requires programmer specify loop copies one byte time. SIMD architecture, however, programmer specify vector size, issue single copy command. underlying SIMD hardware copies multiple bytes simultaneously. 18.13 Multiple Instructions Multiple Data (MIMD) phrase Multiple Instructions Multiple Data streams (MIMD) used describe parallel architecture processors performs independent computations time. Although many computers contain multiple internal processing units, MIMD designation reserved computers processors visible programmer. is, MIMD computer run multiple, independent programs time. Symmetric Multiprocessor (SMP). well-known example MIMD architecture consists computer known Symmetric Multiprocessor (SMP). SMP contains set N processors (or N cores) used run programs. typical SMP design, processors identical: instruction set, operate clock rate, access memory modules, access external devices. Thus, processor perform exactly computation processor. Figure 18.4 illustrates concept.Figure 18.4 conceptual organization symmetric multiprocessor N identical processors access memory I/O devices. researchers explored ways increase speed power silicon chips, researchers investigated symmetric multiprocessor form MIMD alternate way provide powerful computers. One well-known projects, conducted Carnegie Mellon University, produced prototype known Carnegie multiminiprocessor (C.mmp). 1980s, vendors first created commercial products, informally called multiprocessors, used SMP approach. Sequent Corporation (currently owned IBM) created symmetric multiprocessor runs Unix operating system, Encore Corporation created symmetric multiprocessor named Multimax. Asymmetric Multiprocessor (AMP). Although SMPs popular, forms MIMD architectures possible. chief alternative SMP design Asymmetric Multiprocessor (AMP). AMP contains set N programmable processors operate time, require processors identical capabilities. example, AMP design choose processor appropriate given task (i.e., one processor optimized management high-speed disk storage devices another processor optimized graphics display). cases, AMP architectures follow master-slave approach one processor (or cases set processors) controls overall execution invokes processors needed. processor controls execution known master, processors known slaves. theory, AMP architecture N processors many distinct types processors. practice, however, AMP designs two four types processors. Typically, general-purpose AMP architecture includes least one processor optimized overall control (the master), others optimized subsidiary functions arithmetic computation I/O. Math Graphics Coprocessors. Commercial computer systems created use asymmetric architecture. One widely known AMP designs became popular late 1980s early 1990s PC manufacturers began selling math coprocessors. idea math coprocessor straightforward: coprocessor special-purpose chip CPUcan invoke perform floating point computation. optimized one task, coprocessor perform task faster CPU. CDC Peripheral Processors. Control Data Corporation helped pioneer idea using AMP architecture mainframes created 6000 series mainframe computers. CDC architecture used ten peripheral processors handle I/O. Figure 18.5 illustrates conceptual organization peripheral processors CPU I/O devices. Interestingly, CDC’s peripheral processors limited I/O — peripheral processor resembled minicomputer general-purpose instruction set could used however programmer chose. peripheral processors access memory, meant peripheral processor could read store values location. Although much slower CPU, ten peripheral processors CDC could execute simultaneously. Thus, possible optimize program performance dividing tasks among peripheral processors well CPU. Although CDC computers longer manufactured, basic idea programmable I/O processors continues used. Surprisingly, multicore chips made general approach feasible many cores make possible dedicate one cores I/O. I/O Processors. mainframe computers use AMP architecture handle I/O high speed without slowing CPU. external I/O connection equipped dedicated, programmable processor. Instead manipulating bus handling interrupts, CPU merely downloads program programmable processor. processor handles details I/O. example, mainframe computers sold IBM Corporation use programmable I/O processors called channels. Figure 18.5 Illustration asymmetric architecture used CDC 6000 mainframe computers. 18.14 Communication, Coordination, Contention may seem obvious multiprocessor architecture always better performance uniprocessor architecture. Consider, example, symmetric multiprocessor, M. Intuitively, computer outperform uniprocessor perform N times many operations time. Moreover, chip vendor finds way make single processor run faster M, vendor sells merely replaces processors new chip faster multiprocessor. Indeed, many companies created multiprocessors made statements attract customers. Unfortunately, intuition computer performance misleading. Architects found three main challenges designing high-performance parallel architecture: Communication Coordination Contention Communication. Although may seem trivial envision computer dozens independent processors, computer must also provide mechanism allows processors communicate other, memory, I/O devices. important, communication mechanism must able scale handle large number processors. architect must spend significant amount effort create parallel computer system severe communication bottlenecks. Coordination. parallel architecture, processors must work together perform computation. Therefore, coordination mechanism needed allows processing controlled. said asymmetric designs usually designate one processors act master controls coordinates processing; symmetric designs also use master- slave approach. architectures use distributed coordination mechanism processors must programmed coordinate among without master. Contention. two processors attempt access resource time, say processors contend resource. Resource contention creates one greatest challenges designing parallel architecture contention increases number processors increases. understand contention problem, consider memory. set N processors access given memory, mechanism needed permits one processor access memory time. multiple processors attempt use memory simultaneously, hardware contention mechanism blocks except one them. is, N – 1 processors idle memory access. next round, N – 2 processors remain idle. obvious that:In parallel architecture, contention shared resources lowers performance dramatically one processor use given resource time; hardware contention mechanism forces processors remain idle wait access. 18.15 Performance Multiprocessors Multiprocessor architectures fulfilled promise scalable, high-performance computing. several reasons: operating system bottlenecks, contention memory, I/O. modern computer system, operating system controls processing, including allocating tasks processors handling I/O. one copy operating system run device cannot take orders multiple processors simultaneously. Thus, multiprocessor, one processor run operating system software time, means operating system shared resource processors must contend. consequence, operating system quickly becomes bottleneck processors access serially — K processors need access, K–1 must wait. Contention memory proven especially difficult problem. First, hardware multiported memory extremely expensive. Second, one important optimizations used memory systems, caching, causes problems used multiprocessor. cache shared, processors contend access. processor private cache, caches must coordinated update propagated caches. Unfortunately, coordination introduces overhead. Many multiprocessor architectures suffer another weakness: architecture outperforms uniprocessor performing intensive computation. Surprisingly, applications limited amount computation perform. Instead, applications I/O bound, means application spends time waiting I/O performing computation. example, delay common applications, word spreadsheets, video games, Web browsing, arises application waits I/O file network. Therefore, adding additional computational power underlying computer lower time required perform computation — extra processors sit idle waiting I/O. assess performance N-processor system, define notion speedup ratio performance single processor performance multiprocessor. Specifically, define speedup as: τ1 denotes execution time taken single processor, τN denotes execution time taken multiprocessor†. case, assume performance measured using bestalgorithm available (i.e., allow program rewritten take advantage parallel hardware). multiprocessors measured performing general-purpose computing tasks, interesting result emerges. ideal situation, would expect performance increase linearly processors added multiprocessor system. Experience shown, however, problems like memory contention, inter-processor communication, operating system bottlenecks mean multiprocessors achieve linear speedup. Instead, performance often reaches limit Figure 18.6 illustrates. Surprisingly, performance illustrated figure may achievable practice. multiprocessor designs, communication overhead memory contention dominate running time: processors added, performance starts decrease. example, particular symmetric multiprocessor design exhibited small speedup processors. However, sixty-four processors used, communication overhead made performance worse single processor system. summarize: used general-purpose computing, multiprocessor may perform well. cases, added overhead means performance decreases processors added. Figure 18.6 Illustration ideal typical performance multiprocessor number processors increased. Values y-axis list relative speedup compared single processor.18.16 Consequences Programmers Parallelism usually makes programming complex. programmer must aware parallel execution, must prevent one parallel activity interfering another. following sections describe mechanisms facilities programmers use. 18.16.1 Locks Mutual Exclusion Writing code uses multiple processors inherently complex writing code single processor. understand complexity, consider using shared variable. example, suppose two processors use variable x store count. programmer writes statement as: x = x + 1; compiler translates statement sequence machine instructions, sequence Figure 18.7. Figure 18.7 example sequence machine instructions used increment variable memory. architectures, increment entails load store operation. Unfortunately, two processors attempt increment x nearly time, value x might incremented instead twice. error arises two processors operates independently competes access memory. Thus, operations might performed order given Figure 18.8. Figure 18.8 sequence steps occur two independent processors cores access variable x shared memory.To prevent problems like one illustrated Figure 18.8, multiprocessor hardware provides hardware locks. programmer must associate lock shared item, use lock ensure processors change item update progress. example, lock 17 associated variable x, programmer must obtain lock 17 updating x. idea called mutual exclusion, say processor must gain exclusive use variable updating value. Figure 18.9 illustrates sequence instructions. Figure 18.9 Illustration instructions used guarantee exclusive access variable. separate lock assigned shared item. underlying hardware guarantees one processor granted lock time. Thus, two processors attempt obtain given lock time, one obtains access (i.e., continues execute) blocked. fact, arbitrary number processors blocked one processor holds lock. processor holds lock releases it, hardware selects blocked processor, grants processor lock, allows processor proceed. Thus, hardware ensures one processor hold given lock time. Locking adds nontrivial amount complexity programs several reasons. First, locking unusual, programmer accustomed programming multiprocessors easily forget lock shared variable, unprotected access may always result error, problem difficult detect. Second, locking severely reduce performance — K processors attempt access shared variable time, hardware keep K–1 idle wait access. Third, separate instructions used obtain release lock, locking adds overhead. Thus, programmer must decide whether obtain lock individual operation whether obtain lock, hold lock performing series operations variable, release lock. 18.16.2 Programming Explicit Implicit Parallel Computers important aspect parallelism programmer concerns whether software hardware responsible managing parallelism: system uses implicit parallelism significantly easier program system uses explicit parallelism. example, consider processor designed handle packets arriving computer network. implicit design, programmer writes code handle single packet, hardware automatically applies program N packets parallel. explicit design, programmer must plan read N packets, send different core, wait cores complete processing, extract resulting packets. many cases, code required control parallel cores determine finish complex code perform desired computation. Moreimportant, code control parallel hardware units must allow hardware operate arbitrary order. example, time required process packet depends packet’s contents, controller must ready hardware units complete processing arbitrary order. point is: programmer’s point view, system uses explicit parallelism significantly complex program system uses implicit parallelism. 18.16.3 Programming Symmetric Asymmetric Multiprocessors One important advantages symmetry arises positive consequences programmers: symmetric multiprocessor substantially easier program asymmetric multiprocessor. First, processors identical, programmer needs one compiler one language. Second, symmetry means programmer need consider tasks best suited type processor. Third, identical processors usually operate speed, programmer need worry time required perform task given processor. Fourth, processors use encoding instructions data, binary program data value moved one processor another. course, form multiprocessor introduces complication: addition everything else, programmer must consider coding decisions influence performance. example, consider computation processes packets arriving network. conventional program keeps global counter memory, updates counter packet arrives. shared memory architecture, however, updating value memory expensive processor must obtain lock updating shared value memory. Thus, programmer needs consider effect minor details, updating shared counter memory. 18.17 Redundant Parallel Architectures discussion focused use parallel hardware improve performance increase functionality. However, also possible use parallel hardware improve reliability prevent failure. is, multiple copies hardware used verify computation. term redundant hardware usually refers multiple copies hardware unit operate parallel perform operation. basic difference redundant hardware parallel architectures described arises data items used: parallel architecture arranges copy hardware operate separate data item; redundant architecture arranges copies perform exactly operation. point using redundant hardware verification computation correct. happens redundant copies hardware disagree? answer depends details andpurpose underlying system. One possibility uses votes: K copies hardware unit perform computation produce value. special hardware unit compares output, selects value appears often. Another possibility uses redundant hardware detect hardware failures: two copies hardware disagree, system displays error message, halts defective unit repaired replaced. 18.18 Distributed Cluster Computers parallel architectures discussed chapter called tightly coupled parallel hardware units located inside computer system. alternative, known loosely coupled architecture uses multiple computer systems interconnected communication mechanism spans longer distances. example, use term distributed architecture refer set computers connected computer network internet. distributed architecture, computer operates independently, computers communicate sending messages across network. special form distributed computing system known network cluster cluster computer. essence, cluster computer consists set independent computers, commodity PCs, connected high-speed computer network. Scientists use cluster computers run computations extremely large sets data, Internet search companies use clusters respond users’ search terms, cloud providers use cluster approach build cloud data centers. general idea cluster N computers, computation divided many ways. computers cluster flexible — dedicated solving single problem separate problems. Computers cluster run independently. working single problem, results may collected produce final output. special case cluster computing used construct high-capacity Web site handles many small requests. computer cluster runs copy Web server. special- purpose system known Web load balancer disperses incoming requests among computers cluster. time request arrives, load balancer chooses least-loaded computer cluster forwards request. Thus, Web site N computers cluster respond approximately N times many requests per second single computer. Another form loosely coupled distributed computing known grid computing. Grid computing uses global Internet communication mechanism among large set computers. computers (typically personal computers owned individuals) agree provide spare CPU cycles grid. computer runs software repeatedly accepts request, performs requested computation, returns result. use grid, problem must divided many small pieces. piece problem sent computer, computers execute simultaneously. 18.19 Modern SupercomputerInformally, term supercomputer used denote advanced computing system significantly processing power mainframe computers. often used scientific calculations, supercomputers typically assessed number floating point operations per second computer perform. Parallelism always played important role supercomputers. Early supercomputers 16 64 processors. modern supercomputer consists cluster many PCs interconnected high-speed Local Area Network. Furthermore, processor PC multiple cores. Modern supercomputers carry parallelism surprising extreme. example, Tianhe-2 supercomputer China consists cluster 16,000 Intel nodes. node memory set processors, multiple cores. resulting system total 3,120,000 cores. computational power computer 3 million cores difficult imagine. 18.20 Summary Parallelism fundamental optimization technique used increase hardware performance. components computer system contain parallel hardware; architecture classified parallel architecture includes parallel processors. Explicit parallelism gives programmer control use parallel facilities; implicit parallelism handles parallelism automatically. uniprocessor computer classified Single Instruction Single Data (SISD) architecture single instruction operates single data item given time. Single Instruction Multiple Data (SIMD) architecture allows instruction operate array values. Typical SIMD machines include vector processors graphics processors. Multiple Instructions Multiple Data (MIMD) architecture employs multiple, independent processors operate simultaneously execute separate program. Typical MIMD machines include symmetric asymmetric multiprocessors. Alternatives SIMD MIMD architectures include redundant, distributed, cluster, grid architectures. theory, general-purpose multiprocessor N processors perform N times faster single processor. practice, however, memory contention, communication overhead, coordination mean performance multiprocessor increase linearly number processors increases. extreme case, overhead means performance decrease additional processors added. Programming computer multiple processors challenge. addition considerations, programmer must use locks guarantee exclusive access shared items. modern supercomputer consists large cluster processors. problem partitioned subparts, processors supercomputer cluster work subparts parallel. EXERCISES18.1 Define macroscopic parallelism give example. 18.2 computer four cores plus two GPU cores, system symmetric parallelism, asymmetric parallelism, both? Explain? 18.3 Use Flynn classification scheme classify dual-core smart phone. 18.4 contention, affect performance? 18.5 C programmer writing code run multiple cores, must increment shared variable x. Instead writing: x = x + 1; C programmer writes: x++; second form guarantee two cores execute increment without interfering one another? Explain. 18.6 receive two job offers salary, one writing code system uses explicit parallelism another writing code system uses implicit parallelism. choose, why? 18.7 Consider multiplying two 10 x 20 matrices computer vector capability limits vector sixteen items. matrix multiplication handled computer, many vector multiplications required? 18.8 previous exercise, many scalar multiplications needed uniprocessor (i.e., SISD architecture)? ignore addition measure multiplication, speedup? speedup change multiplying 100 x 100 matrices? 18.9 access single-processor dual-processor computers use clock rate, write program consumes large amounts CPU time, run multiple copies computers, record running times. effective speedup? 18.10 previous question, change program reference large amounts memory (e.g., repeatedly set large array value x, set array value y, on). memory references affect speedup? 18.11 multiprocessor ever achieve speedup better linear? find out, consider encryption breaking algorithm must try twenty-four (four factorial) possible encryption keys must perform 1024 operations test key (stopping early answer found). assume multiprocessor requires K milliseconds perform 1024 operations, average much time processor spend solving entire problem? much time 32-processor MIMD machine spend solving problem? resulting speedup? 18.12 Search Web find list top 10 supercomputers. many cores have? †M. J. Flynn, “Some Computer Organizations Effectiveness,” IEEE Transactions Computers, C-21(9):948--960, September 1972. †Chapter 21 discusses performance detail. †Some architects also apply term asymmetric multicore design cores access memory I/O devices. †MISD specialized category reserved unusual hardware, pipeline architecture shown Figure 19.5 page 387 executes multiple instructions single piece data redundant processor used increase reliability. †An exercise considers speedup cases vectors exceed capacity hardware; definition speedup found Section 18.15. †Because expect processing time single processor greater processing time multiprocessor, expect speedup greater one.19 Data Pipelining Chapter Contents 19.1 Introduction 19.2 Concept Pipelining 19.3 Software Pipelining 19.4 Software Pipeline Performance Overhead 19.5 Hardware Pipelining 19.6 Hardware Pipelining Increases Performance 19.7 Pipelining Used 19.8 Conceptual Division Processing 19.9 Pipeline Architectures 19.10 Pipeline Setup, Stall, Flush Times 19.11 Definition Superpipeline Architecture 19.12 Summary 19.1 Introduction Earlier chapters present processors, memory systems, I/O fundamental aspects computer architecture. previous chapter shows parallelism used increase performance, explains variety parallel architectures. chapter focuses second major technique used increase performance: data pipelining. chapter discusses motivation pipelining, explains variety ways pipelining used, shows pipelining increase hardware performance. 19.2 Concept Pipelining term pipelining refers broadly architecture digital information flows series stations (e.g., processing components) inspect, interpret, modify information Figure 19.1 illustrates. Figure 19.1 Illustration pipeline concept. example pipeline four stages, information flows stage. Although primarily interested hardware architectures use pipelining within single computer system, concept limited hardware. Pipelining restricted single computer, particular type size digital information, specific length pipeline (i.e., particular number stages). Instead, pipelining fundamental concept computing used variety situations. help us understand concept, consider set characteristics. Figure 19.2 lists ways characterize pipelines, succeeding paragraphs explain characteristics.Figure 19.2 variety ways pipeline used digital systems. Hardware Software Implementation. Pipelining implemented either software hardware. example, Unix operating system provides pipe mechanism used form software pipeline — set processes creates pipes connect output one process input next process. examine hardware pipelines later sections. However, noted software hardware pipelines independent: software pipeline created computer use pipeline hardware architecture, pipeline hardware necessarily visible programmers. Large Small Scale. Stages pipeline range simplistic powerful, pipeline range length short long. one extreme, hardware pipeline contained entirely within small functional unit chip. extreme, software pipeline created passing data series programs run separate computer use Internet communicate. Similarly, short pipeline formed two stages, one generates information one absorbs it, long pipeline contain hundreds stages. Synchronous Asynchronous Flow. synchronous pipeline operates like assembly line: given time, stage processing amount information (e.g., byte). global clock controls movement, means stages simultaneously forward data (i.e., results processing) next stage. alternative, asynchronous pipeline, allows station forward information time. Asynchronous communication especially attractive situations amount time given stage spends processing depends data stage receives. However, asynchronous communication mean one stage delays long time, later stages must wait. Buffered Unbuffered Flow. conceptual diagram Figure 19.1 implies one stage pipeline sends data directly another stage. also possible construct pipeline buffer placed pair stages. Buffering useful asynchronous pipelines information processed bursts (i.e., pipeline stage repeatedly emits steady output, ceases emitting output, begins emitting steady output again).Finite Chunks Continuous Bit Streams. digital information passes pipeline consist sequence small data items (e.g., packets computer network) arbitrarily long bit stream (e.g., continuous video feed). Furthermore, pipeline operates individual data items designed data items size (e.g., disk blocks four Kbytes) size data items fixed (e.g., series Ethernet packets vary length). Automatic Data Feed Manual Data Feed. implementations pipelines use separate mechanism move information, implementations require stage participate moving information. example, synchronous hardware pipeline typically relies auxiliary mechanism move information one stage another. However, software pipeline usually requires stage write outgoing data read incoming data explicitly. Serial Parallel Path. large arrows Figure 19.1 imply parallel path used move information one stage another. Although hardware pipelines use parallel path, many use serial communication. Furthermore, communication stages need consist conventional communication (e.g., stages use computer network shared memory communicate). Homogeneous Heterogeneous Stages. Although Figure 19.1 uses size shape stage pipeline, homogeneity required. implementations pipelines choose type hardware appropriate stage. 19.3 Software Pipelining programmer’s point view, software pipeline attractive two reasons. First, software pipeline provides way handle complexity. Second, software pipeline allows programs re-used. essence, goals achieved software pipeline allows programmer divide large, complex task smaller, generic pieces. example software pipelining, consider pipeline facilities provided Unix shell (i.e., command interpreter). create software pipeline, user enters list command names separated vertical bar character specify programs run pipeline. shell arranges programs output one program becomes input next. program zero arguments control processing. example, following input shell specifies three programs, cat, sed, connected pipeline: cat x | sed ’s/friend/partner/g’ | example, cat program writes copy file x (presumably text file) output, becomes input sed program. sed program, middle pipeline, receives input cat sends output more. Sed argument specifies translating every occurrence word friend partner. final program pipeline, more, receives input sed displays input user’s screen.Although example trivial, illustrates software pipeline helps programmers. Decomposing program series smaller, less complex programs makes easier create debug software. Furthermore, division chosen carefully, pieces re-used among programs. particular, programmers often find using pipeline separate input output processing computation allows code performs computation re-used various forms input output. 19.4 Software Pipeline Performance Overhead may seem software pipelining results lower performance single program. operating system must run multiple application programs concurrently, must pass data pairs programs. Inefficiency especially high early stages pipeline pass large volumes data later discarded. example, consider following software pipeline contains one stage example above: additional invocation sed deletes line containing character W. cat x | sed ’s/friend/partner/g’ | sed ’/W/d’ | expect ninety-nine percent lines contain character W, first two stages pipeline perform unnecessary work (i.e., processing lines text discarded later stage pipeline). example, pipeline optimized moving deletion earlier stage. However, overhead using software pipeline appears remain: copying data one program another less efficient performing computation single program. Surprisingly, software pipeline perform better large, monolithic program, even underlying hardware use multiple cores. understand why, consider underlying architecture: processing, memory, I/O constructed independent hardware. operating system takes advantage independence automatically switching processor among application programs (i.e., processes): one application waiting I/O, another application runs. Thus, pipeline composed many small applications, operating system may able improve overall performance running one applications pipeline, another application waits I/O. 19.5 Hardware Pipelining Like software pipelining, hardware pipelining help designer manage complexity — complex task divided smaller, manageable pieces. However, important reason architects choose hardware pipeline increased performance. two distinct uses hardware pipelines provide high performance: Instruction pipelineData pipeline Instruction Pipeline. Chapter 5 explains fetch-execute cycle processor use pipeline decode execute instructions. precise, use term instruction pipeline describe pipeline information consists machine instructions stages pipeline decode execute instructions. instruction set operand types vary among processors, overall agreement number stages instruction pipeline exact operations performed given stage†. Data Pipeline. alternative instruction pipeline known data pipeline. is, instead passing instructions, data pipeline designed pass data stage stage. example, data pipeline used handle packets arrive computer network, packet passes sequentially stages pipeline. Data pipelining provides unusual interesting uses pipelining. see, data pipelining also potential greatest overall improvement performance. 19.6 Hardware Pipelining Increases Performance understand pipelining fundamental hardware design, need examine key point: pipelining dramatically increase performance. see how, compare data pipeline monolithic design. example, consider design Internet router used Internet Service Provider (ISP) forward packets customers Web sites. router connects multiple networks, lead customers least one leads Internet. Network packets arrive network, router’s job send packet toward destination. purposes example, assume router performs six basic operations packet listed Figure 19.3. important understand operations, appreciate example realistic. Figure 19.3 example series steps hardware Internet router performs forward packet. Consider design hardware implements steps figure. steps involve complex computation, may seem processor used perform packet forwarding. However, single processor fast enough high-speed networks. Thus, designs employ two optimizations described earlier chapters: smart I/O devices parallelism. smart I/O device transfer packet memory without using processor, parallel design uses separate processor handle input. parallel router design smart I/O interface means processor implements loop repeatedly executes six basic steps. Figure 19.4 illustrates processor connects input, shows algorithm processor runs. Figure 19.4 (a) Illustration connections processor used parallel implementation Internet router, (b) algorithm processor executes. processor handles input one network. Suppose parallel architecture, like one figure, still slow. is, suppose processor cannot execute steps algorithm next packet arrives interface faster processor available. achieve higher performance? One possibility higher speed lies data pipeline: use pipeline several processors place single processor Figure 19.5 illustrates†. Figure 19.5 Illustration pipeline used place single processor Internet router. may seem pipeline figure faster single processor Figure 19.4. all, pipeline architecture performs exactly operations packet single processor. Furthermore, processors Figure 19.5 speed theprocessor Figure 19.4, time perform given operation same. example, step labeled verify integrity take amount time architectures, step labeled check loops take amount time architectures, on. Thus, ignore delay introduced passing packets among stages pipeline, total time taken process packet exactly single processor architecture. is: data pipeline passes data series stages examine modify data. uses speed processors non-pipeline architecture, data pipeline improve overall time needed process given data item. total processing time required item pipelined non-pipelined architectures, advantage data pipeline? Surprisingly, even individual processors Figure 19.5 exactly speed processor Figure 19.4, pipeline architecture process packets per second. see why, observe individual processor executes fewer instructions per packet. Furthermore, operating one data item, processor moves next data item. Thus, data pipeline architecture allows given processor move next data item quickly nonpipeline architecture. result, data enter (and leave) pipeline higher rate. summarize: Even data pipeline uses speed processors nonpipeline architecture, data pipeline higher overall throughput (i.e., number data items processed per second). 19.7 Pipelining Used pipeline yield higher performance cases. Figure 19.6 lists conditions must met pipeline perform faster single processor. Figure 19.6 three key conditions must met data pipeline perform better computation single processor.Partionable Problem. must possible partition processing stages computed independent one another. Computations employ sequence steps work well pipeline, computations involve iteration often not. Equivalent processor speed. obvious processors used data pipeline slow enough, overall time required perform computation much higher single processor. Processors pipeline need faster single processor. merely require processor pipeline approximately fast single processor. is, time required perform given computation pipeline processor must exceed time required perform computation single processor. Low overhead data movement. addition time required perform computation, data pipeline additional overhead: time required move data item one stage pipeline next. moving data incurs extremely high latency, pipelining increase performance. requirements arise important principle: throughput pipeline limited stage takes time. example, consider data pipeline Figure 19.5. Suppose processors pipeline identical, assume pipeline processor takes exactly time execute instruction single processor. make example concrete, assume processor execute ten instructions microsecond. suppose four stages figure take fifty, one hundred, two hundred, one hundred fifty instructions, respectively, process packet. slowest stage requires two hundred instructions, means total time slowest stage takes process packet is: Looking another way, see maximum number packets processed per second inverse time per packet slowest stage. Thus, overall throughput example pipeline, Tp, given by: contrast, non-pipelined architecture must execute 500 instructions packet, means total time required packet 50 μsec. Thus, throughput non- pipelined architecture limited to:19.8 Conceptual Division Processing reason data pipelining improves performance arises pipelining provides special form parallelism. dividing series sequential operations groups handled separate stage pipeline, pipelining allows stages operate parallel. course, pipeline architecture differs conventional parallel architecture significant way: although stages operate parallel, given data item must pass stages. Figure 19.7 illustrates concept. Figure 19.7 (a) Processing conventional processor, (b) equivalent processing data pipeline. functions performed sequence divided among stages pipeline. point figure three stages operate parallel. Stage three performs function h one data item time stage two performs function g second data item stage one performs function f third data item. long pipeline full (i.e., delays items), overall system benefits N stages running parallel. 19.9 Pipeline Architectures Recall previous chapter distinguish hardware architectures merely use parallelism architectures parallelism forms central paradigm around entire architecture designed. make analogous distinction hardware architectures use pipelining architectures pipelining forms central paradigm around entire system designed. reserve name pipeline architectures latter. Thus, one might hear architect say processor given system uses instruction pipelining, architect characterize system pipeline architecture unless overall design centers around pipeline. hardware systems follow pipeline architecture dedicated special-purpose functions. instance, example describes pipelining used improve performance packet processing system. Pipelining especially important network systemsbecause high data rates used sending data optical fibers exceeds capacity conventional processors. Pipeline architectures less relevant general-purpose computers two reasons. First, applications decomposed set independent operations applied sequentially. Instead, typical application accesses items randomly keeps large volumes additional state information. Second, even situations functions performed data decomposed pipeline, number stages pipeline hardware needed implement stage usually known advance. result, general-purpose computers usually restrict pipeline hardware instruction pipeline processor special-purpose pipeline I/O device. 19.10 Pipeline Setup, Stall, Flush Times description pipelines overlooks many practical details. example, many pipeline implementations overhead associated starting stopping pipeline. use term setup time describe amount time required start pipeline idle period. Setup may involve synchronizing processing among stages passing special control token pipeline restart stage. software pipeline, setup especially expensive connections among various stages created dynamically. Unlike architectures, pipeline require significant time terminate processing. use term flush time refer amount time elapses input unavailable pipeline finishing current processing. say items currently pipeline must flushed pipeline shut down. need flush items pipeline arise two reasons. First, pipeline becomes idle input available first stage. Second, seen, later stages pipeline become idle one stage stalls (i.e., stage delays cannot complete processing). high-speed hardware pipeline, mundane operations memory reference I/O operation cause stage stall. Thus, high flush (or setup) times reduce pipeline performance significantly. 19.11 Definition Superpipeline Architecture final concept completes description pipelines. Architects use term superpipeline describe extension pipeline approach given stage pipeline subdivided set partial stages. Superpipelining often used instruction pipeline, concept applies data pipelines well. general idea is: dividing processing N stages increase overall throughput, adding stages increase throughput further. traditional instruction pipeline might five stages correspond to: instruction fetch, instruction decode, operand fetch, ALU operation, memory write. superpipeline architecture subdivides one stages multiple pieces. example, superpipelinemight subdivide operand fetch stage four steps: decode operand, fetch immediate values values registers, fetch values memory, fetch indirect operand values. standard pipelining, point subdivision higher throughput — substage takes less time, throughput superpipeline higher throughput standard pipeline. 19.12 Summary Pipelining broad, fundamental concept used hardware software. software pipeline, arranges set programs series data passing them, used hardware provide pipelining. hardware pipeline either classified instruction pipeline, used inside processor handle machine instructions, data pipeline, arbitrary data transferred pipeline. superpipeline technique, stage pipeline subdivided partial stages, often used instruction pipeline. data pipeline decrease overall time required process single data item. However, using pipeline increase overall throughput (items processed per second). stage pipeline requires time process item limits throughput pipeline. EXERCISES 19.1 scientist uses cluster PCs arranges software processor perform one step computation. processor reads 1 MB data (whatever available), processes data, passes output next processor 32- bit bus. characteristics Figure 19.2 arrangement have? 19.2 team given task moving video processing program old single-core processor new quad-core processor high-speed interconnect among cores. Conventional parallel approaches work frames video must processed order. technique suggest offers possible way use new hardware improve performance? 19.3 engineer builds data pipeline eight processors. measure performance, engineer runs software one processor measures time taken process single data item. engineer divides software eight stages, measures time taken process single data item. measurements show? 19.4 data pipeline hardware devoted specialized tasks (e.g., graphics processing). Would installing data pipeline computers increase performance programs? not? 19.5 manager notices company idle computers ten data centers. data centers spread across country, low-speed Internet connections usedto communicate among data centers. manager proposes rather using computer local data center, “giant data pipeline” set across ten data centers increase performance. tell manager idea? 19.6 given program runs one core, asked divide program pieces use eight cores data pipeline. divide program two ways. one, cores perform 680, 2000, 1300, 1400, 800, 1900, 1200, 200 instructions. other, cores perform 680, 1400, 1300, 1400, 1400, 1000, 1200, 1100 instructions. division choose, why? 19.7 maximum throughput homogeneous pipeline four processors handle one million instructions per second processing data item requires 50, 60, 40, 30 instructions, respectively? Assume constant execution time types instructions. 19.8 previous exercise, relative gain throughput compared architecture without pipelining? maximum speedup? 19.9 Extend previous exercise considering heterogeneous processors speeds 1.0, 1.2, 0.9, 1.0 million instructions per second, respectively. 19.10 asked apply superpipelining subdivide one stages existing pipeline, stage choose? Why? †The definition superpipeline, given later chapter, also relates instruction pipeline. †A pipeline provides example Flynn MISD type parallel architecture mentioned previous chapter.20 Power Energy Chapter Contents 20.1 Introduction 20.2 Definition Power 20.3 Definition Energy 20.4 Power Consumption Digital Circuit 20.5 Switching Power Consumed CMOS Digital Circuit 20.6 Cooling, Power Density, Power Wall 20.7 Energy Use 20.8 Power Management 20.9 Software Control Energy Use 20.10 Choosing Sleep Awaken 20.11 Sleep Modes Network Devices 20.12 Summary 20.1 Introduction related topics power consumption total energy consumption become increasingly important design computing systems. portable devices, designs strive balance maximizing battery life maximizing features users desire. large data centers, power consumed consequent cooling required critical factors design scale. brief chapter introduces topic without going much detail. defines terminology, explains types power digital circuits consume, describes relationship power energy. important, chapter describes software systems used shut parts system reduce power consumption. 20.2 Definition Power define power rate energy consumed (e.g., transferred transformed). electronic circuit, power product voltage current. Taking definitions physics, power measured units watts, watt defined one joule per second (J/s). higher wattage electronic device, power consumes; devices use kilowatts (103 watts) power. large data center cluster, aggregate power consumed computers cluster large measured megawatts (106 watts). small hand-held devices, cell phones, power requirements minimal measured milliwatts (10-3 watts). important note amount power system uses vary time. example smart phone uses less power display turned screen on. Therefore, precise, define instantaneous power time t, P(t), product voltage time t, V(t), current time t, I(t): see ability system vary power usage time important extremely large extremely small computing systems (e.g., powerful computers data center small battery powered devices). maximum power system uses especially important large systems, cluster computers data center. use term peak instantaneous power specify maximum power system need. Peak power especially important constructing large computing system designer must arrange meet peak power requirements.For example, planning data center, designer must guarantee electric utility supply sufficient power meet peak instantaneous power demand. 20.3 Definition Energy above, total energy system uses computed power consumed given time, measured joules. Electrical energy usually reported multiples watts multiplied unit time. Typically, time unit hour, multiples watts kilowatts, megawatts, milliwatts. Thus, energy consumed data center week might reported kilowatt hours (kWh) megawatt hours (MWh), energy consumed battery week might reported milliwatt hours (mWh). power utilization constant, energy consumed computed easily multiplying power utilization, P, time power used. example, time period t0 t1, energy used given by: system uses exactly 6 kilowatts hour energy consumption 6 kWh system energy consumption 3 kilowatts period two hours. described above, systems consume power constant rate. Instead, power consumption varies time. capture idea power varies continuously, define energy integral instantaneous power time: Although power defined instantaneous measure change time, electronic systems specify value known average power. Recall power rate energy used, means average power time interval computed taking amount energy used interval dividing time: 20.4 Power Consumption Digital Circuit Recall digital circuit created logic gates. lowest level, logic gates composed transistors, transistors consume power two ways†: Switching dynamic power (denoted Ps Pd)Leakage power (denoted Pleak) Switching Power. term switching refers change output response input. one inputs gate change, output may change. change output occur electrons flow transistors. Individual transistors consume power switching, means total power system increases. Leakage Power. Although think digital circuit binary value (on off), solid state physicists realize transistors imperfect switches. is, even transistor off, electrons penetrate semiconductor boundary. Therefore, whenever power supplied digital circuit, amount current always flow, even outputs switching. use term leakage refer current flows circuit operating. given transistor, amount leakage current insignificant. However, single processor billion transistors, meaning aggregate leakage current quite high. fact, digital systems, leakage current accounts half power utilization. point summarized: typical computing system, 40 60 percent power system uses leakage power. point important discussion power management. basic principle leakage always occurs power present: Leakage current eliminated removing power circuit. 20.5 Switching Power Consumed CMOS Digital Circuit focus using software manage power use digital circuit. understand power management techniques, need basic concepts. First, consider total energy consumed switching. energy, required single change gate denoted Ed, given by: C value capacitance depends underlying CMOS technology, Vdd voltage circuit operates†.To understand power consequences Equation 20.5, consider clock. clock generates square wave fixed frequency. Suppose clock signal connected inverter. inverter output change twice clock cycle, clock goes zero one clock goes one back zero. Therefore, clock period Tclock, average power used is: frequency clock inverse period: means rewrite Equation 20.6 terms clock frequency: One additional term used compute average power: fraction circuit whose outputs switching. use α denote fraction, 0≤α≤1, makes final form Equation 20.8 average power: Equation 20.9 captures three main components power pertinent following discussion. Constant C property underlying technology cannot changed easily. Thus, three components controlled are: fraction circuit active, α clock frequency, Fclock voltage circuit, Vdd 20.6 Cooling, Power Density, Power Wall Recall instantaneous power use often associated data centers large installations key aspect peak power utilization. addition question whether electric utility able deliver megawatts needed peak use, designers focus two aspects power use: cooling power density. Cooling. digital device operates, generates heat. huge power load means many devices operating, device generating heat. Consequently, heat produced related power consumed. electronic circuits must cooled circuits willoverheat burn out. smallest devices, enough heat escapes surrounding air cooling needed. medium-size devices, cooling requires fan blows cold air across circuits constantly; air must brought Heating, Ventilation, Air Conditioning (HVAC) system. extreme cases, air cooling insufficient, form liquid cooling required. Power Density. Although total amount heat circuit produces dictates total cooling capacity required, another aspect heat important: concentration heat small area. data center, example, many computers placed adjacent one another, overheat. Thus, spacing added computers racks computers permit cool air flow racks remove heat. Power density also important individual integrated circuit, power density refers amount power dissipated per given area silicon. many years, semiconductor industry followed Moore’s Law. size individual transistor continued shrink, every eighteen months, number transistors fit single chip doubled. However, following Moore’s Law negative aspect: power density also increased. power density increases, amount heat generated per unit area increases, means modern processor produces much heat per square centimeter earlier processors. Consequently, packing transistors closer together led major problem: reaching limits rate heat removed chip. Engineers refer limit power wall means power cannot increased. current cooling technologies, limit approximated: 20.7 Energy Use Unlike power, measures instantaneous flow current, energy measures total power consumed given time interval. focus energy especially pertinent portable devices use batteries. think battery bucket energy, imagine device extracting energy needed. total time battery power device (measured milliwatt hours) derived amount energy battery. Modeling battery bucket energy (analogous bucket water) overly simplistic. However, three aspects water buckets apply batteries. First, like water bucket, energy stored battery evaporate. case battery, chemical physical processes imperfect — internal resistance allows trivial amount current flow inside battery. Although flow almost imperceptible, allowing battery sit long time (e.g., year) result loss charge. Second, water poured bucket likely spill extracting energy battery, energy lost. Third, energy removed battery various rates, water extracted bucket various rates. important idea behind third property battery becomes efficient lowercurrent levels (i.e., lower power levels). Thus, designers look ways minimize power battery operated device consumes. 20.8 Power Management discussion shows reducing power consumption desirable cases. large data center, reducing power consumption reduces heat generated. small portable device, reducing power consumption extends battery life. Two questions arise: methods used reduce power consumption, power reduction techniques controlled software? Recall Equation 20.9†, three primary factors contribute power consumption: α, fraction circuit active, Fclock, clock frequency, Vdd, voltage used operate circuit. next sections describe voltage frequency used reduce power consumption; later section considers fraction circuit active. 20.8.1 Voltage Delay power utilization depends square voltage, lowering voltage produce largest reduction power. However, voltage independent variable. First, decreasing voltage increases gate delay, time gate takes change outputs inputs change. processor designed carefully hardware units operate according clock. delay single gate becomes sufficiently large, delay across entire hardware unit (many gates) exceed design specification. current technology, delay estimated by: Vdd voltage used, VTH threshold voltage determined underlying CMOS technology, K constant depends technology, β constant (approximately 1.3 current technology). second aspect power related voltage: leakage current. leakage current depends temperature circuit threshold voltage CMOS technology. Lowering voltage decreases leakage current, interesting consequence: lower voltage means increased delay, results total energy consumed. understand increasing leakage significant, recall leakage account 40% 60% power circuit uses. point is: Although power depends square voltage, reducing voltage increases delay increases total energy usage.Despite problems, voltage significant factor power reduction. Therefore, researchers work solid state physics silicon technologies devised transistors operate correctly much lower voltages. example, although early digital circuits operated 5 volts, current technologies used cell phones operate lower voltages. fully charged cell phone battery provides 4 volts, circuits continue operate battery discharges. fact, cell phones use NiMH battery technology still receive calls battery provides 1.2 volts, phone declares battery dead voltage falls 0.8 volts. (Lithium-based batteries tend die approximately 3.65 volts.) 20.8.2 Decreasing Clock Frequency Clock frequency forms second factor power utilization. theory, power proportional clock frequency, slowing clock save power. practice, reducing clock frequency lowers performance, may critical systems real-time requirements (e.g., system displays video plays music). Interestingly, adjusting clock frequency used conjunction reduction voltage. is, slower clock accommodate increased delays lower voltage causes. Thus, designer decreases clock frequency voltage decreased, performance suffer circuit operate correctly. clock frequency voltage reduced, resulting reduction power dramatic. one specific case, reducing frequency one-half original rate allowed voltage divided 1.7. voltage squared power equation (Equation 20.9), reducing voltage allows resulting power reduced dramatically. example, resulting power approximately 15% original power. Although savings depend technology used, general idea summarized: circuit deliver adequate performance reduced clock frequency, power cut dramatically reducing clock frequency also allows voltage reduced. Intel invented interesting twist reduced clock frequency permitting dynamic changes. idea straightforward. processor busy, operating system sets clock frequency high. processor exceeds preset thermal limit (i.e., overheats) power limit (e.g., would drain battery quickly), operating system reduces clock frequency processor operates within prescribed limits. example, clock frequency might increased decreased dynamically multiples 100 MHz. processor idle, clock frequency also reduced save energy. Instead advertising capability dynamic speed reduction, Intel marketing turns situation around advertises feature Turbo Boost.20.8.3 Slower Clock Frequency Multicore Processors early 2000s, time power utilization becoming problem, chip vendors introduced multicore processors. surface, shift multicore architectures seems counterproductive two cores require twice much power single core. course, cores may share circuitry (e.g., memory bus interface), means power consumption dual-core chip exactly double power consumption single core chip. However, second core adds substantial additional power requirements. would vendors introduce cores reducing power consumption important? understand, look carefully clock frequency. multicore chips appeared, clock frequency increased every years new processors appeared. know discussion slowing clock one-half original speed allows voltage lowered cuts power consumption significantly. consider dual-core chip. Suppose core runs one-half clock frequency single-core chip. computational power dual-core version still approximately single core runs twice fast. terms power utilization, however, voltage reduced, means two cores takes fraction, F, power required single-core version. result, multicore chip takes approximately 2F much power single core version. Provided F less 50%, slower dual-core chip consumes less power. example above, F 15%, means dual-core chip provide equivalent computational power 30% original power requirements. summarize: multicore chip core runs slower clock frequency lower voltage deliver approximately computational capability single core chip incurring significantly lower power utilization. course, discussion makes important assumption multicore processing. Namely, assumes computation divided among multiple cores. Unfortunately, Chapter 18 points experience parallelism promising. computations parallel approach feasible, slow clock make system unusable. Even cases parallelism feasible, memory contention inefficiencies result disappointing performance. parallel processing used handle multiple input items time, overall throughput two cores single, faster core. However, latency (i.e., time required process given item) higher. Finally, one remember discussion focused switching power — leakage still significant problem. 20.9 Software Control Energy UseSoftware system usually little ability make minor increases decreases voltage used. Instead, software often restricted two basic operations: Clock gating Power gating Clock Gating. term refers reducing clock frequency zero effectively stops processor. processor stopped, programmer must arrange way restart it. Typically, code image kept memory, memory retains power. Thus, image remains ready whenever processor restarts. Power Gating. term refers cutting power processor. special solid state device extremely low leakage current used cut power. clock gating, programmer must arrange restart, either saving restoring copy memory image ensuring memory remains powered image retained. Systems offer power gating capabilities apply gating across entire system. Instead, system divided islands, gating applied islands others continue operate normally. Memory cache forms particularly important power island — power removed memory cache, cached data lost. know Chapter 12 caching important performance. Therefore, memory cache placed power island shut power removed parts processor. processors extend idea provide set low power modes software use reduce power consumption. Vendors use variety names describe modes, sleep, deep sleep, hibernation. use generic names LPM0, LPM1, LPM2, LPM3, LPM4. general, low power modes arranged hierarchy. LPM0 turns least amount circuitry fastest recovery. LPM4, deepest sleep mode, turns almost entire processor. consequence, restarting LPM4 takes much longer low power modes. 20.10 Choosing Sleep Awaken Two questions must answered: system enter sleep mode, awaken? Choosing awaken sleep mode usually straightforward: wake demand. is, hardware waits event occurs requires processor, hardware moves processor sleep mode. example, screen saver restarts display whenever user moves mouse, touches touch-sensitive screen, presses key keyboard. question enter low power mode complex. motivation reduce power utilization. Therefore, want gate power subsystem (i.e., turn off) subsystem needed reasonably long time. usually cannot know future requirements, systems employ heuristic estimate subsystem needed: sufficiently long period inactivity occurs, assume subsystem remain inactive longer. Typically, processor device remains inactive N seconds, processoror device enters sleep mode. heuristic also applied cause deeper sleep — processor remains light sleep state K seconds, hardware moves processor deeper sleep state (i.e., additional parts processor turned off). value N used timeout sleep mode? Subsystems provide interaction human user typically allow user choose timeout. example, screen saver allows user specify long input devices remain idle screen saver runs. Allowing users specify timeout means user tailor system needs. Choosing timeout system involve human preference requires careful analysis. simplified model help illustrate calculation. model, assume two states: RUN state processor runs full power state power removed. processor makes transition, time elapses, denote Tshutdown Twakeup. Figure 20.1 illustrates simplified model. Figure 20.1 simplified model transitions among low power modes. Power used transition (i.e., save state information prepare I/O devices transition). make calculations easier, assume power used transition constant. Therefore, energy required transition calculated multipling power used time elapses: Understanding energy required transitions energy used system runs shut allows us assess potential energy savings. essence, shutting beneficial shutdown, sleep, later wakeup consume less energy continuing run time interval. Let time interval considered. assume power used running system constant, energy consumed system remains running time is: energy consumed system put sleep mode time consists energy required transitions plus Poff, energy used (if any) processor shut down: Shutting system beneficial if: using Equations 20.12, 20.13 20.15, inequality expressed terms single free variable, time interval t. Therefore, possible compute break-even point specifies minimum value shutting saves energy. course, analysis based simplified model. Power usage may remain constant; time power required transitions may depend state system. important, analysis focuses energy consumed switching ignores leakage. However, analysis illustrate basic point: Even simplified model one low power state, details energy used state transitions complicate decision move low power mode. 20.11 Sleep Modes Network Devices Many devices low power mode used save energy. example, printer usually sleeps N minutes inactivity. Similarly, wireless network adapters enter sleep mode reduce power consumption. network adapter, handling output (transmission) trivial adapter awakened whenever application generates outgoing packet. However, input (reception) poses difficult challenge low power mode computer cannot know another computer send packet. example, Wi-Fi (802.11) standard includes Power Saving Polling (PSP) mode. save power, laptops devices using Wi-Fi shut wake periodically. use term duty-cycle characterize repeated cycle device running shut down. radio must access point transmits. Wi-Fi base station periodically sends beacon includes list recipients base station undelivered packets. beacon frequent enough device guaranteed receive beacon part duty cycle awake. device finds recipient list, device remains awake receive packet.Two basic approaches used allow network adapter sleep without missing packets indefinitely. one approach, device synchronizes sleep cycles base station. approach, base station transmits packet many times receiver wakes receives it. 20.12 Summary Power instantaneous measure rate energy used; energy total amount power used given time. digital circuit uses dynamic switching power (i.e., output changes response change input) leakage power. Leakage account 40 60 percent power circuit consumes. Power consumption reduced making parts circuit inactive, reducing clock frequency, reducing voltage. Reducing voltage largest effect, also increases delay. Power density refers concentration power given space; power density related heat. power wall refers limit approximately 100 watts per cm2 gives maximum power density heat removed silicon chip using current cooling technologies. Clock gating power gating used turn circuit (or part circuit). devices use battery power, overall goal power management systems reduction total energy use. moving low power (sleep) mode consumes energy, sleeping justified energy required sleep mode less energy required remain running. simplified model shows computation involves cost shut cost wake up. Devices also use low-power modes. Network interfaces pose challenge interface must awake receive packets computer always know packets arrive. Wi-Fi standard includes Power Saving Polling mode. EXERCISES 20.1 Estimate amount power required Tianhe-2 supercomputer described page 376. Hint: start finding estimate number watts used single processor. 20.2 Suppose frequency clock reduced 10% parameters remain same. much power reduced? 20.3 Suppose voltage, Vdd, reduced 10% parameters remain same. much power reduced? 20.4 Use Equation 20.16 find break-even value t. 20.5 Extend model Figure 20.1 three-state system processor sleep mode deep sleep mode. †In addition two major sources, minor amount short-circuit power consumed CMOS transistors form brief connection power source ground switching. †The notation Vdd used specify voltage used operate CMOS circuit; notation V (voltage) used context understood. †Equation 20.9 found page 398.21 Assessing Performance Chapter Contents 21.1 Introduction 21.2 Measuring Computational Power Performance 21.3 Measures Computational Power 21.4 Application Specific Instruction Counts 21.5 Instruction Mix 21.6 Standardized Benchmarks 21.7 I/O Memory Bottlenecks 21.8 Moving Boundary Hardware Software 21.9 Choosing Items Optimize, Amdahl’s Law 21.10 Amdahl’s Law Parallel Systems 21.11 Summary 21.1 Introduction Earlier parts text cover three fundamental mechanisms computer architects use construct computer systems: processors, memories, I/O devices. characterize mechanism, explain salient features. Previous chapters consider two techniques used increase computational performance: parallelism pipelining. chapter takes broader view performance. examines performance measured, discusses architect evaluates instruction set. important, chapter presents Amdahl’s law, explains consequences computer architecture. 21.2 Measuring Computational Power Performance measure computational power? makes one computer system perform better another? questions engendered research scientific community, caused heated debate among representatives sales marketing departments commercial computer vendors, resulted variety answers. chief problem underlies performance assessment arises flexibility general-purpose computer system: computer designed perform variety tasks. important, optimization involves choosing among alternatives, optimizing architecture given task means architecture less optimal tasks. Consequently, performance computer system depends system used. summarize: computer designed perform wide variety tasks architecture optimal tasks, performance system depends task performed. dependency performance task performed two important consequences. First, means many computer vendors claim powerful computer. example, vendor whose computer performs matrix multiplication high speed uses matrix multiplication examples measuring performance, vendor whose computer performs integer operations high speed uses integer examples measuring performance. vendors claim computer performs best. Second, scientific point view, see single measure computer system performance suffices cases. point fundamental understanding performance assessment:A variety performance measures exist single measure suffices situations. 21.3 Measures Computational Power Recall early computer systems consisted central processor little I/O capability. consequence, early measures computer performance focused execution speed CPU. Even performance measures restricted CPU, however, multiple measures apply. important distinction arises computer systems optimized for: Integer computation Floating point computation scientific engineering calculations rely heavily floating point, applications employ floating point often called scientific applications, resulting computation known scientific computation. assessing computer performs scientific applications, engineers focus entirely performance floating point operations. ignore speed integer operations, measure speed floating point operations (specifically, floating point addition, subtraction, multiplication division). course, addition subtraction generally faster multiplication division, program contains instructions (e.g., instructions call functions control iteration). many computers, however, floating point operation takes much longer typical integer instruction floating point computation dominates overall performance program. Rather reporting time required perform floating point operation, engineers report number floating point operations performed per unit time. particular, primary measure given average number floating point operations hardware execute per second (FLOPS). course, floating point speed pertinent scientific computation; speed floating point hardware irrelevant programs use integers. important, measure FLOPS make sense RISC processor offer floating point instructions. Thus, alternative measuring floating point performance, vendor may choose exclude floating point report average number instructions processor execute per unit time. Typically, vendors measure millions instructions per second (MIPS). Simplistic measures performance MIPS FLOPS provide rough estimate performance. see why, consider time required execute instruction. example, consider processor floating point multiplication division takes twice long floating point addition subtraction. assume addition subtraction instruction takes Q nanoseconds weight four instruction types equally, average time computer takes perform floating point instruction, Tavg is:However, computer performs addition subtraction, time required Q nanoseconds per instruction (i.e., 33% less average). Similarly, performing multiplication division, computer requires 2×Q nanoseconds per instruction (i.e., 33% average). practice, times required addition division differ factor two, means actual performance vary 33%. exercise considers one possible ratio. point is: instructions take substantially longer execute others, average time required execute instruction provides crude approximation performance. actual time required depends instructions executed. 21.4 Application Specific Instruction Counts produce accurate assessment performance? One answer lies assessing performance specific application. example, suppose need know floating point hardware unit perform multiplying two N×N matrices. examining program, possible derive set expressions give number floating point additions, subtractions, multiplications, divisions performed function N. example, assume multiplying pair N×N matrices requires N3 floating point multiplications N3 – N2 floating point additions. addition requires Q nanoseconds multiplication requires 2×Q nanoseconds, multiplying two matrices require total of: alternative precise analysis, engineers use weighted average. is, instead calculating exact number times instruction executed, approximate percentage used. example, suppose graphics program run many input data sets, number floating point operations counted obtain list Figure 21.1. Figure 21.1 Example instruction counts graphics application run many input values. third column shows relative percentage instruction type. set instruction counts obtained, performance hardware assessed using weighted average. graphics application run hardware described above, expect average time floating point instruction be: example shows, weighted average differ significantly uniform average. case, weighted average 23% less average Equation 21.1 obtained using uniform instruction weights†. 21.5 Instruction Mix Although provides accurate measurement performance, weighted average example applies one specific application assesses floating point performance. give general assessment? One approach become popular: use large set programs obtain relative weights type instruction, use relative weights assess performance given architecture. is, instead focusing floating point, keep counter instruction type (e.g., integer arithmetic instructions, bit shift instructions, subroutine calls, conditional branches), use counts relative weights compute weighted average performance. course, weights depend specific programs chosen. Therefore, accurate possible, must choose programs represent typical workload. Architects choose instruction mix represents typical programs. addition helping assess performance computer, instruction mix helps architect design efficient instruction set. architect drafts tentative instruction set, assigns expected cost instruction, uses weights instruction mix see proposed instruction set perform. essence, architect uses instruction mix evaluate proposed architecture perform typical programs. performance unsatisfactory, architect change design. summarize: instruction mix consists set instructions along relative weights obtained counting instruction execution set example programs. architect use instruction mix assess proposed architecture perform.21.6 Standardized Benchmarks instruction mix used compare performance two architectures? answer question, need know computers used: programs computers intended run, type input programs receive. essence, need find set applications typical. Engineers architects use term benchmark refer programs — benchmark provides standard workload computer measured. course, devising benchmark difficult, community benefit vendor creates separate benchmark. solve problem, independent notfor-profit corporation formed 1980s. Named Standard Performance Evaluation Corporation (SPEC), corporation created “establish, maintain endorse standardized set relevant benchmarks applied newest generation high-performance computers”†. SPEC devised series standard benchmarks used compare performance. example, SPEC cint2006 benchmark used evaluate integer performance, SPEC cfp2006 benchmark used evaluate floating point performance. benchmarks produced SPEC primarily used measurement, design. is, benchmark consists set programs run measured. score results running SPEC benchmark, known SPECmark, often quoted industry vendor-independent measure computer performance. Interestingly, SPEC produced many benchmarks test one aspect performance. example, SPEC offers six separate benchmarks focus integer arithmetic another fourteen benchmarks focus various aspects floating point performance. addition, SPEC provides benchmarks assess power computers consume, performance Java environment, performance Unix systems running Network File System (NFS) remote file access software development tasks. 21.7 I/O Memory Bottlenecks CPU performance accounts part overall performance computer system. users personal computers realized, faster CPU cores guarantee faster response computing tasks. colleague author complains although CPU power increases order magnitude every ten years, time required launch application seems increase. prevents faster CPU increasing overall speed? already seen one answer: Von Neumann bottleneck (i.e., memory access). Recall speed memory affect rate instructions fetched well rate data accessed. Thus, rather merely measuring CPU performance, benchmarks designed measure memory performance. memory benchmark consists program repeatedly accesses memory. memory benchmarks designed test sequential access (i.e., accessto contiguous bytes), others designed test random access. important, memory benchmarks also make repeated references memory location test memory caching. chapters I/O point out, peripheral devices buses peripheral devices communicate also form bottleneck. Thus, benchmarks designed test performance I/O devices. example, benchmark test disk repeatedly execute write read operations transfer block data disk read data back. memory, disk benchmarks focus measuring performance accessing sequential data blocks, benchmarks focus measuring performance accessing random blocks. 21.8 Moving Boundary Hardware Software One fundamental principles underlies computer performance arises relative speed hardware software: hardware (especially hardware designed special purpose) faster software. consequence, moving given function hardware result higher performance executing function software. words, architect increase overall performance adding special-purpose hardware units. corollary arises equally important principle: software provides much flexibility hardware. consequence functionality implemented hardware cannot changed. Therefore, architect increase overall flexibility generality allowing software handle functions. recent use FPGAs example hardware functions moving software — instead building chip fixed gates, FPGA allows functions design programmed. point hardware software represent tradeoff: Performance increased moving functionality software hardware; flexibility increased moving functionality hardware software. 21.9 Choosing Items Optimize, Amdahl’s Law architect needs increase performance, architect must choose items optimize. Adding hardware design increases cost; special-purpose, high-speed hardware especially expensive. Therefore, architect cannot merely specify arbitrary amounts high-speed hardware used. Instead, careful choice must made select functions optimized high-speed hardware functions handled conventional hardware. choice made? computer architect, Gene Amdahl, observed waste resources optimize functions seldom used. example, consider thehardware used handle division zero circuitry used power computer system. little point optimizing hardware seldom used. Amdahl suggested greatest gains performance made optimizing functions account time. principle, known Amdahl’s law, focuses operations require extensive computation operations performed frequently. Usually, principle stated form refers potential speedup: Amdahl’s Law: performance improvement realized faster hardware technology limited fraction time faster technology used. Amdahl’s law expressed quantitatively giving overall speedup terms fraction time-enhanced hardware used speedup enhancement delivers. Equation 21.4 gives overall speedup: equation works two extremes. enhanced hardware never used (i.e., fraction 0), speedup, Equation 21.4 results ratio 1. enhanced hardware used 100% time (i.e., fraction 1), overall speedup equals speedup enhanced hardware. fractional values 0 1, overall speedup weighted according much enhanced hardware used. 21.10 Amdahl’s Law Parallel Systems Chapter 18 discusses parallel architectures, explains performance disappointing. particular, overhead communication among processors contention shared resources memory I/O buses limit effective speed system. result, parallel systems contain N processors achieve N times performance single processor. Interestingly, Amdahl’s Law applies directly parallel systems explains adding processors help. speedup achieved optimizing processing power (i.e., adding additional processors) limited amount time processors used. parallel system spends time waiting communication bus access rather using processors, adding additional processors produce significant increase performance.21.11 Summary variety performance measures exist. Simplistic measures processor performance include average number floating point operations computer perform per second (FLOPS) average number instructions computer execute per second (MIPS). sophisticated measures use weighted average instruction used often weighted heavily. Weights derived counting instructions program set programs; weights specific application(s) used. say weights, useful assessing instruction set, correspond instruction mix. benchmark refers standardized program set programs used assess performance; benchmark chosen represent typical computation. best-known benchmarks produced SPEC Corporation, known SPECmarks. addition measuring performance various aspects integer floating point performance, SPEC benchmarks available measure mechanisms remote file access. Amdahl’s Law helps architects select functions optimized (e.g., moved software hardware moved conventional hardware high-speed hardware). law states functions optimized account time. Amdahl’s Law explains parallel computer systems always benefit large number processors. EXERCISES 21.1 Write C program measures performance integer addition subtraction operators. Perform least 10,000 operations calculate average time per operation. 21.2 Write computer program measures difference execution times integer addition integer division. Execute operation 100,000 times, compare difference running times. Repeat experiment, verify activities computer interfere measurement. 21.3 Extend measurement previous exercise compare performance sixteen- bit, thirty-two-bit, (if computer supports it) sixty-four-bit integer addition. is, use short, int, long, long long variables needed. Explain results. 21.4 Computer professionals commonly use addition, subtraction, multiplication, division ways measure performance processor. However, many programs also use logical operations, logical and, logical or, bit complement, right shift, left shift, on. Measure operations, compare performance integer addition. 21.5 floating point addition subtraction take Q microseconds floating point multiplication division take 3Q microseconds, average time required four operations? 21.6 Extend previous exercise compute percentage difference time addition average time, percentage difference time multiplication average time.21.7 previous problem, repeat measurement compiler optimization enabled determine relative speedup. 21.8 Write program compares average time required perform integer arithmetic operations average time required reference memory. Calculate ratio memory cost integer arithmetic cost. 21.9 Write program compares average times required perform floating point operations integer operations. example, compare average time required perform 10,000 floating point additions average time required perform 10,000 integer additions. 21.10 programmer decides measure performance memory system. programmer finds according DRAM chip manufacturer, time needed access integer physical memory 80 nanoseconds. programmer writes assembly language program stores value memory location four billion times, measures time taken, computes average performance. Surprisingly, takes average 52 nanoseconds per store operation. result possible? 21.11 Turn previous exercise around, state accurate measurement physical memory difficult. 21.12 hashing function places values random loctions array called hash table. programmer finds even memory caching turned off, storing looking 50,000 values extremely large hash table (16 megabytes) worse performance using data smaller hash table (16 kilobytes). Explain why. †Equation 21.1 found page 413. †The description taken SPEC bylaws (see http://www.spec.org).22 Architecture Examples Hierarchy Chapter Contents 22.1 Introduction 22.2 Architectural Levels 22.3 System-level Architecture: Personal Computer 22.4 Bus Interconnection Bridging 22.5 Controller Chips Physical Architecture 22.6 Virtual Buses 22.7 Connection Speeds 22.8 Bridging Functionality Virtual Buses 22.9 Board-level Architecture 22.10 Chip-level Architecture 22.11 Structure Functional Units Chip 22.12 Summary 22.1 Introduction Earlier chapters explain concepts terminology essential understanding computer architecture. chapters discuss fundamental aspects processors, memory, I/O, explain role each. Previous chapters discuss parallelism pipelining used improve performance. chapter considers architecture examples. Instead introducing new ideas, chapter shows ideas previous chapters used describe explain various aspects digital systems. examples chosen show range possibilities. 22.2 Architectural Levels Recall earlier chapters architecture presented multiple levels abstraction. help us appreciate broadly architectural concepts apply digital systems, explore hierarchy architectural specifications. hierarchy ranges size complete computer system small functional unit single integrated circuit. use terms system-level architecture (sometimes called macroscopic architecture), board-level architecture, chip-level architecture (sometimes called microscopic architecture) characterize range. level, see concepts earlier chapters allow us understand basic components interconnection. Furthermore, see given level, possible specify logical (i.e., conceptual) architecture specify detailed implementation. Figure 22.1 summarizes levels consider. Figure 22.1 Conceptual levels architecture purpose each. 22.3 System-level Architecture: Personal Computer Conceptually, personal computer consists processor, memory, set I/O devices attach single bus. practice, however, even personal computer contains complex assortment buses interconnection mechanisms designed fill specific role. variety complexity underlying hardware arises special performance requirements cost. example, video card needs much higher data throughput floppy disk, high-resolution screen requires throughput low-resolution screen. Unfortunately, hardware interconnects device high-speed bus costs significantly hardware interconnects device low-speed bus, means using multiple buses lower overall cost system. second motivation multiple I/O buses arises vendor’s desire provide low- cost migration path newer, powerful systems. is, vendor strives create processor offers advantages higher performance capabilities, simultaneously retaining ability use existing peripheral devices. use term backward compatibility characterize ability use existing pieces hardware. Backward compatibility especially important bus architectures bus forms interconnection I/O device processor. computer vendor devise new, higher-speed bus still retaining ability attach older peripheral devices? One possibility consists creating processor multiple bus interfaces. much less expensive answer lies use bridging. 22.4 Bus Interconnection Bridging use bridging backward compatibility easy understand historical example. one point history, personal computers used Industry Standard Architecture (ISA) bus developed IBM Corporation. Peripheral devices PCs designed interface ISA bus. Later, higher-speed bus architecture developed: Peripheral Component Interconnect (PCI) bus. two standards PC buses incompatible — interface plugs ISA bus cannot connected PCI bus. Thus, user owns ISA devices, user less likely purchase computer accepts PCI devices. entice computer owners upgrade computers computer PCI bus, vendors created bridge interconnect new PCI bus older ISA bus. Logically, bridge provides interconnection Figure 22.2 illustrates.Figure 22.2 Conceptual view PC architecture uses bridge interconnect ISA bus PCI bus. bridge makes possible use older ISA devices newer processor. figure, CPU I/O devices PCI interface connect directly PCI bus. bridge provides connection ISA bus used I/O devices ISA interface. best case, interconnection provided bridge transparent. is, side uses local bus protocol communicate without knowing interconnection — CPU addresses ISA devices connected PCI bus, ISA device responds CPU connected ISA bus. 22.5 Controller Chips Physical Architecture Although architecture illustrated Figure 22.2 provides conceptual explanation PC architecture, implementation much complex figure indicates. First, although PC provides slots external devices use connect bus, PC use technology internally. Instead, PC usually contains two special-purpose controller chips provide bus memory interconnections. Second, controller chips configured give illusion multiple buses. understand need controller chips, consider functionality required PC. architect needs connect processor, memory, I/O bus (or buses). addition providing electrically compatible interconnections, architect must design mechanism allows one component communicate another. example, CPU I/O devices need access memory.Unfortunately, replicating hardware interfaces expensive. particular, architect cannot afford build system component multiple interface units handle communication one component. example, although processor I/O devices need access memory, cost prohibits architect providing memory interface device. save effort expense, architects often adopt approach using centralized controller chip. controller chip contains set K hardware interfaces, one type hardware, forwards requests among them. hardware unit needs access another hardware unit, request always goes controller. controller translates incoming request appropriate form, forwards request destination hardware unit. Similarly, controller translates reply. key idea is: Architects use controller chip provide interconnection among components computer less expensive equipping unit set interfaces building set discrete bridges interconnect buses. 22.6 Virtual Buses controller chip introduces interesting possibility. bus used communicate, expect two devices attached bus (e.g., processor disk). computer uses controller chip, however, reasonable create bus contains exactly one connected device. example, one device needs ISA bus others use PCI bus, controller chip created uses ISA protocol communicate ISA device uses PCI protocol communicate devices. Even controller chip uses ISA protocol communicate ISA device, computer need slots ISA devices physical ISA bus usual sense. is: controller chip provide illusion bus direct connection; need physical sockets wiring normally used bus. concept controller chip provide illusion bus direct connection allows architects generalize notion bus. Instead separate physical entities parallel wires, silicon chip used create appearance bus. use term virtual bus describe technology. example, controller created presents illusion one virtual bus per attached device. alternative, controller created combines one virtual buses connections one physical buses. Later sections show examples.Typically, PC architectures use two controller chips instead one. controllers known informally Northbridge Southbridge chips; Northbridge sometimes called system controller. Northbridge connects high-speed components, CPU, memories, streaming communications controllers, Advanced Graphics Port (AGP) interface used operate high-speed graphics display. Southbridge, attaches Northbridge, provides connectivity lower-speed components, PCI bus, Wi-Fi network interface†, audio device, keyboard, mouse, similar devices. Figure 22.3 illustrates physical interconnections PC architecture uses two controller chips. figure shows, controller chip must accommodate heterogeneity controller connect multiple bus technologies. figure, example, Southbridge provides connections PCI bus, USB bus, ISA bus. course, controller must follow rules bus. is, controller must adhere electrical specifications, ensure addresses lie within bus address space, obey protocol defines devices access use bus. Vendors manufacture CPUs usually offer set controller chips designed interconnect CPU standard buses. example, Intel Corporation offers 82865PE chip provides functionality Northbridge ICH5 chip provides functionality Southbridge. important, Intel processor chip Intel controller chips designed work together: chip contains interface allows chips directly interconnected, chip performs translation necessary connect heterogeneous devices.Figure 22.3 Example system-level architecture shows physical interconnections PC uses two controller chips. Components require highest speeds attach Northbridge controller. 22.7 Connection Speeds connections illustrated Figure 22.3 typically use parallel hardware interface fixed width engineered operate fixed clock rate deliver specified throughput. Figure 22.4 lists typical values clock rate, width, throughput major connections.Figure 22.4 Example clock rates, data widths, throughput connections architecture Figure 22.3 illustrates. comparison purposes, figure includes FCC’s definition Internet connection (25 megabits per second downstream, 3.1 megabytes per second) register file modern processor. Note transfers computer occur much faster broadband Internet connection, sustained throughput registers dwarfs throughputs listed figure. 22.8 Bridging Functionality Virtual Buses names Northbridge Southbridge imply, two controllers provide bridging functionality. example, Northbridge chip bridges memory, high-speed devices, Southbridge chip. Northbridge presents CPU single, unified address space includes above. Similarly, Southbridge combines PCI bus, ISA bus, USB bus single, unified address space, becomes part address space Northbridge presents processor. Interestingly, set controllers need bridge devices single address space. Instead, controller present CPU illusion multiple virtual buses. example, controller might allow CPU access two separate PCI buses: bus number zero contains CPU memory, bus number one contains I/O devices. alternative, controller might present illusion three virtual buses: one contains CPU memory, another contains high-speed graphics device, third corresponds external PCI slots arbitrary devices. Although particularly interesting programmer, separation crucial hardware designer interested performance controller chip contain parallel circuitry allows virtual buses operate time. 22.9 Board-level Architecture architecture Figure 22.3 includes Wi-Fi interface one units personal computer. role interface straightforward: provide physical connection betweenthe PC Wi-Fi radio, transfer data PC sends network well data arrives network. Physically, Wi-Fi interface integrated onto motherboard laptop reside circuit board desktop system. either case, logical interconnection remains same. network interface card contains surprising amount computational power. particular, interface usually contains embedded processor, instructions ROM, buffer memory, external host interface (e.g., PCI bus interface), connection radio transmitter receiver. interface cards use conventional RISC processor; others use specialized network processor optimized handling network packets. Figure 22.5 illustrates possible architecture LAN interface uses network processor. Figure 22.5 Example architecture network interface card used WiFi device. might Wi-Fi interface need two types memory? primary motivation cost: although faster, SRAM costs SDRAM. Thus, large SDRAM used hold packets, small SRAM used values must accessed updated frequently(e.g., instructions network processor execute). particular example, two memory connections chosen network processor described next section uses SRAM SDRAM. 22.10 Chip-level Architecture said chip-level architecture describes internal structure single integrated circuit. example, consider network processor board-level architecture illustrated Figure 22.5; figure uses rectangle depict network processor. move chip- level architecture, examine internal structure chip. Figure 22.6 shows chip- level architecture Netronome network processor†. Figure 22.6 Example chip-level architecture shows major internal components Netronome network processor. Access units provide connections outside chip. important remember entire figure refers single integrated circuit. figure shows, network processor chip contains many items, including various external interfaces, onboard scratch memory provides high-speed storage, multiple, independent processors. particular, chip contains set programmable RISC processors,known microengines†, operate parallel well XScale RISC processor. XScale provides general-purpose processor manages processors provides management interface. network processor operates, XScale runs conventional operating system, Linux. indicate processors part integrated circuit, say embedded. Details network processor processors irrelevant. important point understand detail revealed architectural level. case, seen although single integrated circuit contain many functional units, structure circuit revealed chip-level diagram; chip structure remains hidden board-level diagram. summarize: level architecture reveals details hidden higher levels. chip- level architecture specifies internal structure integrated circuit hidden board-level architecture. 22.11 Structure Functional Units Chip final example architectural levels, examine possible describe architecture one component chip. Figure 22.7 shows SRAM access unit Figure 22.6. internal structure memory access unit quite complex. 22.12 Summary architecture digital system viewed several levels abstraction. system- level architecture shows structure entire computer system, board-level architecture shows structure board, chip-level architecture shows internal structure integrated circuit. successive level, details revealed remain hidden previous levels. example, chapter presents hierarchy architectures shows structure personal computer, Wi-Fi network interface board computer, network processor interface board. Finally, saw chip-level architecture refined looking architecture embedded unit.Figure 22.7 internal structure SRAM access unit remains hidden Figure 22.6. successive level architectural hierarchy reveals details structure. EXERCISES 22.1 engineer offered job system architect, job entail? 22.2 motivation computer offers two buses? 22.3 computer USB port contains hardware known USB hub usually connects external ports PCI bus. Modify diagram Figure 22.2 show USB hub. 22.4 computer contains two buses connected transparent bridge, memory connects one bus devices connect other, devices able communicate memory? Explain. 22.5 purpose controller chip modern bus architectures? 22.6 computer one device uses old bus, normal sockets wires bus. situation possible? 22.7 PC, would super high-speed video system connect Northbridge chip Southbridge chip? Explain.22.8 takes 40 seconds transfer video USB 3.0 port, approximately long take transfer video Wi-Fi network operates 20 megabits per second? 22.9 network processor, one shown Figure 22.6, classified System Chip (SoC). Explain why. 22.10 many hardware design documents, rectangular boxes used represent subsystem. one tell looking diagram approximately many gates needed implement function box represents? Explain. †Networks operate gigabit speeds connect Northbridge. †Throughput reported Megabytes per second (MB/s) Gigabytes per second (GB/s), uppercase B emphasizes bytes instead bits. †Intel Corporation designed network processor, later sold design Netronome. †A advanced version chip provides sixteen microengines.23 Hardware Modularity Chapter Contents 23.1 Introduction 23.2 Motivations Modularity 23.3 Software Modularity 23.4 Parameterized Invocation Subprograms 23.5 Hardware Scaling Parallelism 23.6 Basic Block Replication 23.7 Example Design (Rebooter) 23.8 High-level Rebooter Design 23.9 Building Block Accommodate Range Sizes 23.10 Parallel Interconnection 23.11 Example Interconnection 23.12 Module Selection 23.13 Summary 23.1 Introduction Earlier chapters give overview hardware architectures without discussing design implementation details. brief chapter considers designs employ modularity. particular, chapter contrasts hardware modularity software modularity, considers common programming abstractions apply hardware. uses example illustrate basic hardware module designed flexible, replication basic module allows designer form scalable hardware design. 23.2 Motivations Modularity Modular construction two motivations: intellectual economic. intellectual perspective, modular approach allows designer break large complex problem smaller pieces. small piece easier understand complete solution. Consequently, easier designer ensure piece correct, easier designer optimize individual piece. economic motivation modularity arises cost designing testing products. many cases, company produce one isolated product. Instead, company creates set related products. One common reason multiple products arises size — company might sell set related products range size small large. example, company sells network equipment might offer four models network switch, models connect four computers, twenty-four computers, forty-eight computers, ninety-six computers. Alternatively, company may offer series products supply basic functionality, product special features. example, company sells network equipment may offer one model connects wireless Wi-Fi network another model connects wired Ethernet. designing product expensive, company save money basic module designed re-used multiple products. savings arise basic module tested thoroughly, successive designs use module assume works correctly. 23.3 Software Modularity Modularity played key role software design since early computers. principal abstraction consists subroutine (also called procedure, subprogram function). Theearly motivation using subroutines arose limited memory size — instead repeating sections code multiple places throughout program, single copy code could placed memory, used (i.e., called) several places program. software became complex, subprograms became important tool handling complexity. particular, use subprogram abstraction made possible expert build piece software programmers could use without understanding details. example, expert understands numerical mathematics create set trigonometric functions, sin(x) cos(x), efficient accurate. programmers invoke functions without writing code without needing understand algorithms used. raising level abstraction hiding details, subprograms allow programmers work higher level, meaning much productive resulting software contain fewer errors. 23.4 Parameterized Invocation Subprograms basic building block used multiple purposes? answer software well known. creating subprogram, programmer specifies set formal parameters. Then, writing code invokes subprogram, programmer specifies actual arguments substituted place formal parameters. key point is: building modularized software, single copy subprogram exists. change among invocations consists actual arguments supplied subprogram invoked. 23.5 Hardware Scaling Parallelism Although works well software, paradigm parameterized function invocation cannot used hardware. reason software invoke function iteratively, hardware requires separate physical instantiations controlled parallel. example, consider controlling set N items. software, items stored array, function written perform operation one item, program iterate array, calling function element. program scale larger array merely changing bound iteration. hardware created control set items, item requires hardware dedicated item. additional elements added set, additional hardware must added design. words, scaling hardware design always requires adding additional pieces hardware. consequence:When hardware designers think modular design, look ways make possible add additional hardware design, ways invoke given piece hardware iteratively. 23.6 Basic Block Replication fundamental technique used make possible scale hardware consists defining basic building block replicated needed. already seen trivial examples. instance, latch circuit replicated N times form N-bit register, full adder replicated N–1 times combined half adder build circuit compute sum two N-bit integers. trivial cases described above, replication involves small circuit (i.e., gates), number replications fixed. Although replication small circuit important aspect design, approach applied significantly larger circuits used scale design. example, chip manufacturer may use multicore architecture produce series products two cores, four cores, eight cores, on. Replication especially important designs number inputs outputs visible user varies across series products. 23.7 Example Design (Rebooter) example clarify idea. Rather choose hypothetical design, consider piece hardware used author’s lab. lab, used operating system networking research, large set backend computers available researchers students classes. lab facilities allow user create operating system, allocate backend computer, download operating system backend computer’s memory, start computer running. user interact backend computer. Unfortunately, experimental work operating systems often results crashes leaves computer hardware state cannot respond input. situations, backend computer must power-cycled regain control. Therefore, created special-purpose hardware system power-cycle individual backend computers needed. call system rebooter. Several generations rebooter hardware used lab; review one design. 23.8 High-level Rebooter Design principle, rebooter hardware follows straightforward approach. rebooter set outputs supply power backend computer. inputs rebooter consist abinary value specifies one outputs reboot plus enable input tells rebooter act. use rebooter, binary value placed input lines (to specify one outputs) enable input set 1, causes rebooter power-cycle specified output†. Figure 23.1 illustrates inputs outputs. Figure 23.1 conceptual organization rebooter hardware. many outputs rebooter have? question important rebooter needs physical connection output. Initially, lab one backend, size evolved quickly two eight. plan future, needed rebooter circuit accommodate least 40 backends, perhaps 100. situation illustrates standard hardware dilemma: • design outputs accommodate future needs • design many outputs wasteful 23.9 Building Block Accommodate Range Sizes Rather choose specific size, used modular approach. is, chose basic building block devised way interconnect basic blocks form larger rebooter. modular approach allowed us construct small rebooter, add additional outputs needed. basic building block consists sixteen-output rebooter Figure 23.2 illustrates.Figure 23.2 Illustration basic building block used rebooter. Look carefully figure. binary input value comprises eight bits, sixteen outputs. Thus, four bits needed select one outputs. extra input bits present? used allow multiple copies building block combined form larger rebooter. 23.10 Parallel Interconnection design uses parallel approach common many hardware systems. is, inputs connect modules parallel. Conceptually, building block passes copy inputs (including enable input) next building block. Figure 23.3 illustrates idea. Figure 23.3 Illustration basic building block passing inputs next stage rebooter. 23.11 Example Interconnection Figure 23.4 illustrates building blocks connected.Figure 23.4 example interconnection four copies basic building block provides 64 outputs. 23.12 Module Selection Figure 23.4 indicates, inputs passed parallel four modules. question arises: input specifies power-cycling computer number 5, module power-cycle fifth output? answer no. fifth output module 1 affected. understand modules respond inputs, necessary know module assigned unique ID (0, 1, 2, 3 example). module includes hardware checks four high-order bits input see match assigned ID. input match ID, input ignored. words, hardware interprets four high-order bits module selection four low-order bits output selection. example, Figure 23.5 illustrates hardware interprets input value 5 module 0 output 5. Figure 23.5 interpretation input 5 rebooter Figure 23.4. Figure 23.5 shows, input 5 means four high-order bits contain 0000 four low- order bits contain 0101. high order bits match ID assigned module 0, none modules. Therefore, module 0 responds input. Using high-order bits input select module makes hardware extremely efficient. module selection bits passed comparator chip along ID themodule. name implies, comparator compares two sets inputs, sets output line high two equal. Thus, little additional hardware needed perform module selection. 23.13 Summary hardware software engineers use modularity. software, fundamental abstraction modularity subprogram. hardware, fundamental abstraction replication basic building block. One method used accommodate range hardware sizes consists structuring module (i.e., building block) accept set N input lines control set 2N outputs. building blocks replicated, assigned unique ID. Additional input lines added design, means high-order bits input used select one modules, low-order bits used select output module. EXERCISES 23.1 enginnering, relationship modularity re-use? 23.2 ability pass arguments functions help programmers control complexity software? 23.3 software engineer hardware engineer think design crypto system processes 128-bit integers, start bias. software engineer might imagine algorithm iterates integer, working 32 bits time. hardware engineer envision? 23.4 Mathematically, one arbitrary number outputs module use arithmetic extract module number input module (e.g., seven outputs per module divide input value 7 get module number use remainder select ouput within module). However, hardware engineers always choose make outputs power two. Explain. 23.5 tradeoffs consider choosing many outputs piece hardware have? 23.6 Suppose basic building block contains 4 outputs, design must scale 64 outputs. many building blocks used? 23.7 building block contains 8 outputs input 16 bits, many total outputs controlled, many building block chips used? 23.8 previous exercise, draw diagram similar one Figure 23.5 shows bits input interpreted. 23.9 Look comparator chips. many pairs inputs single comparator have?23.10 previous exercise, suppose comparator chip compare K pairs inputs designer needs compare 2K pairs. multiple chips used? †The exact details rebooter circuit used irrelevant discussion follows; important understand basics.Appendix 1 Lab Exercises Computer Architecture Course A1.1 Introduction appendix presents set lab exercises undergraduate computer architecture course. labs designed students whose primary educational goal learning build software, hardware. Consequently, weeks introduction digital circuits, labs shift emphasis programming. facilities required lab minimal: small amount hardware needed early weeks, access computers running version Unix operating system (e.g., Linux) needed later labs. RISC architecture works best assembly language labs instructors find CISC architectures absorb arbitrary amounts class time assembly language details. One lab asks students write C program detects whether architecture big endian little endian. additional resources needed coding debugging performed one two architectures, trivial amount time required port test program other. A1.2 Hardware Required Digital Logic Experiments hardware labs covered first weeks require student following: Solderless breadboard Wiring kit used breadboard (22-gauge wire)Five-volt power supply Light-Emitting Diode (used measure output) NAND logic gates None hardware expensive. handle class 70 students, example, Purdue University spent less $1000 hardware. Smaller classes sharing lab reduce cost further. alternative, possible institute lab fee require students purchase copy hardware. A1.3 Solderless Breadboard solderless breadboard used rapidly construct electronic circuit without requiring connections soldered. Physically, breadboard consists block plastic (typically three inches seven inches) array small holes covering surface. holes arranged rows small gap running center extra holes around outside. hole breadboard socket large enough copper wire — wire inserted hole, metal contacts socket make electrical contact metal wire. size spacing sockets breadboard arranged match size spacing pins standard integrated circuit (technically IC uses standard DIP package), gap breadboard matches spacing across pins IC, means one integrated circuits plugged breadboard. is, pins IC plug directly holes breadboard. back breadboard contains metal strips interconnect various sockets. example, sockets side center given row interconnected. Figure A1.1 illustrates sockets breadboard electrical connections among sockets.Figure A1.1 (a) Illustration breadboard sockets wires inserted, (b) blue lines showing electrical connections among sockets. A1.4 Using Solderless Breadboard use breadboard, experimenter plugs integrated circuits onto breadboard along center, uses short wires make connections among ICs. wire plugged hole row connects corresponding pin IC plugged row. make connections, experimenter uses set pre-cut wires known wiring kit. individual wire wiring kit bare ends plug breadboard, otherwise insulated. Thus, many wires added breadboard insulated area wire rub insulation wires without making electrical contact. Figure A1.2 illustrates part breadboard contains 7400 IC, wires connecting gates IC.Figure A1.2 Illustrations (a) internal connections 7400 chip, (b) part breadboard blue lines indicating wires connecting 7400 chip. Using set sockets connect power ground wires allows additional connections added. A1.5 Power Ground Connections multiple chips plugged breadboard, chip must connections power ground (i.e., five volts zero volts). ensure power ground connections convenient keep wires short, experimenters choose devote outer sets sockets sides breadboard power ground. wires used connect power ground semi-permanent sense re-used many experiments. Thus, experimenters often use color wire indicate purpose, choose colors power ground used connections. example, red wires used power connections, black wires used ground connections, blue wires used connections. course, wires differ — color insulation merely helps human understand purpose wire. disassembling breadboard experiment finished, experimenter leave power ground connections later experiment. A1.6 Building Testing Circuits easiest approach building digital circuit consists constructing circuit stages testing stage circuit building proceeds. example, connecting power ground chip, gate chip tested verify chip working expected. Similarly, particular gate connected, input(s) output(s) gate measured determine whether connections working.Although possible use voltmeter measure output digital circuit, experimenters prefer easy inexpensive alternative: Light Emitting Diode (LED). idea choose LED powered directly†. LED glows connected logical one (i.e., five volts), input wire connects logical zero (i.e., zero volts). example, test circuit Figure A1.2, LED connected output (pin 11 integrated circuit). A1.7 Lab Exercises next pages contain series lab exercises. Although writeup specifies steps taken lab, additional details pertain local environment computer system must supplied lab instructor. example, first lab asks students establish computer account, including environment variables. set directories included path depend local computer system, set actual paths must supplied environment. Lab 1 Introduction Account Configuration Purpose learn lab facilities set computer account use lab semester. Background Reading Preparation Read bash shell available Linux, find set Linux environment variables. Overview Modify lab account environment set automatically log in. Procedure Details (checkmark completed)______1. Modify account startup file (e.g., .profile .bash_profile) PATH includes directories specified lab instructor. ______2. Log log again. ______3. Verify reach files compilers lab instructor specifies. Lab 2 Digital Logic: Use Breadboard Purpose learn wire basic breadboard use LED test operation gate. Background Reading Preparation Read Chapter 2 learn basic logic gates circuits, read beginning sections Appendix learn breadboards. Attend lecture properly use breadboard related equipment. Overview Place 7400 chip breadboard, connect power ground five-volt power supply, connect inputs gate four possible combinations zero one, use LED observe output. Procedure Details (checkmark completed) ______1. Obtain breadboard, power supply, wiring kit, parts box necessary logic gates. Also verify data sheet specifies pins 7400, quad, two-input NAND gate. copy pin diagram also found Figure 2.13 text, found page 22. ______2. Place 7400 breadboard shown Figure A1.2b page 448. ______3. Connect two wires five-volt power supply two separate sets sockets near edge board. ______4. Add wire jumper connects pin 14 7400 five volts. ______5. Add wire jumper connects pin 7 7400 zero volts. NOTE: sure reverse connections power supply chip damaged. ______6. Add wire jumper connects pin 1 7400 zero volts. ______7. Add wire jumper connects pin 2 7400 zero volts. ______8. Connect LED, lab kit, pin 3 7400 ground (zero volts). NOTE: LED must connected positive lead attached 7400. ______9. Verify LED lit (it lit inputs zero means output one). ______10. Move jumper connects pin 2 zero volts five volts, verify LED remains lit. ______11. Move jumper connects pin 2 back zero volts, move jumper connects pin 1 zero volts five volts, verify LED remains lit. ______12. Keep jumper pin 1 five volts, move jumper connects pin 2 five volts, verify LED goes out. Optional Extensions (checkmark completed) ______13. Wire breadboard shown Figure A1.2b (pin 3 connected pin 12, pin 13 acting additional input). ______14. Connect LED pin 11 ground. ______15. Record LED values possible combinations three inputs. ______16. Boolean function circuit represent? Lab 3 Digital Logic: Building Adder GatesPurpose learn basic logic gates combined perform complex tasks binary addition. Background Reading Preparation Read Chapter 2 basic logic gates circuits, read beginning sections Appendix learn breadboards. Overview Build half adder full adder circuit using basic logic gates. Combine circuits implement two-bit binary adder carry output. Procedure Details (checkmark completed) ______1. Obtain breadboard, power supply, wiring kit, parts box necessary logic gates well lab writeups describe chip pinouts logic diagram adder circuit. ______2. Construct binary half adder specified logic diagram lab instructor provides. ______3. Connect outputs LEDs, inputs switches, verify results displayed LED correct values one-bit adder. ______4. Construct binary full adder specified logic diagram lab instructor provides. ______5. Connect outputs LEDs, inputs switches, verify results displayed LED correct values full adder. ______6. Chain half adder circuit full adder circuit make two-bit adder. Verify circuit correctly adds pair two-bit numbers carry value correct. Optional Extensions (checkmark completed)______7. Draw logic diagram three-bit adder. ______8. Draw logic diagram four-bit adder. ______9. Give formula number gates required implement n-bit adder. __________________________________________________________________________ Notes Lab 4 Digital Logic: Clocks Decoding Purpose understand clock controls circuit allows series events occur. Background Reading Preparation Read Chapter 2 learn basic logic gates clocks. Concentrate understanding clock functions. Overview Use switch simulate clock, arrange clock operate decoder circuit (informally called demultiplexor circuit). Procedure Details (checkmark completed) ______1. Obtain breadboard, power supply, wiring kit, parts box necessary logic gates well lab writeups describe chip pinouts logic diagram decoder. ______2. Use switch simulate slow clock.______3. verify switch working, connect output switch LED, verify LED goes switch moved back forth. ______4. Connect simulated clock input four-bit binary counter (a 7493 chip). ______5. Use LED verify time switch moved one cycle, outputs counter move next binary value (modulo four). ______6. Connect four outputs binary counter inputs decoder chip (a 74154). ______7. Use LED verify switch moves one cycle, exactly one output decoder becomes active. Warning: 74154 counterintuitive active output low (logical zero) outputs high (logical one). Optional Extensions (checkmark completed) ______8. Use 555 timer chip construct 1-Hz clock, verify clock working. ______9. Replace switch clock circuit. ______10. Use multiple LEDs verify decoder continually cycles output. __________________________________________________________________________ Notes Lab 5 Representation: Testing Big Endian Vs. Little Endian Purpose learn integer representation used underlying hardware affects programming data layout. Background Reading PreparationRead Chapter 3 learn big endian little endian integer representations size integer. Overview Write C program examines data stored memory determine whether computer uses big endian little endian integer representation. Procedure Details (checkmark completed) ______1. Write C program creates array bytes memory, fills array zero, stores integer 0x04030201 middle array. ______2. Examine bytes array determine whether integer stored big endian little endian order. ______3. Compile run program (without changes source code) big endian little endian computer, verify correctly announces integer type. ______4. Add code program determine integer size (hint: start integer 1 shift left value zero). ______5. Compile run program (without changes source code) thirty- two bit sixty-four bit computer, verify program correctly announces integer size. Optional Extensions (checkmark completed) ______6. Find alternate method determining integer size. ______7. Implement alternate method determine integer size, verify program works correctly. ______8. Extend program announce integer format (i.e., one’s complement two’s complement). __________________________________________________________________________Notes Lab 6 Representation: Hex Dump Function C Purpose learn values memory presented hexadecimal form. Background Reading Preparation Read Chapter 3 data representation, find integer address sizes computer use†. Ask lab instructor exact specification output format. Overview Write C function produces hexadecimal dump memory ASCII. lab instructor give details format particular computer, general form follows: Address Words Hexadecimal ASCII characters --------- -------- -------- -------- -------- ---------------- aaaaaaaa xxxxxxxx xxxxxxxx xxxxxxxx xxxxxxxx cccccccccccccccc example, line corresponds set memory locations. string aaaaaaaa denotes starting memory address (in hexadecimal) values line, xxxxxxxx denotes value word memory (also hexadecimal), cccccccccccccccc denotes memory locations interpreted ASCII characters. Note: ASCII output displays printable characters; characters displayed blanks. Procedure Details (checkmark completed) ______1. Create function, mdump takes two arguments specify address memory. first argument specifies address dump start, second argument specifies highest address needs included dump. Test ensure starting address less ending address. ______2. Modify arguments specify appropriate word address (i.e., exact multiple four bytes). starting address, round nearest word address; ending address, round up. ______3. Test function verify addresses rounded correctly. ______4. Add code uses printf produce headings hexadecimal dump, verify headings correct. ______5. Add code iterates addresses produces lines hexadecimal values. ______6. verify function mdump outputs correct values, declare struct memory, place values fields, invoke mdump function dump items struct. ______7. Add code produces printable ASCII character values memory locations, shown above. ______8. Verify printable characters included output (i.e., verify non- printable character 0x01 mapped blank). Optional Extensions (checkmark completed) ______9. Extend mdump start stop byte address (i.e., omit leading values first line output trailing values last line). ______10. Modify mdump instead printing bytes ASCII, displays values words decimal. ______11. Modify mdump instead printing ASCII values, function assumes memory corresponds machine instructions gives mnemonic opcodes instruction. example, first word line corresponds load instruction, print load. ______12. Add argument function mdump selects among various forms output (ASCII characters, decimal, instructions). Lab 7 Processors: Learn RISC Assembly LanguagePurpose gain first-hand experience assembly language understand oneto-one mapping assembly language instructions machine instructions. Background Reading Preparation Read Chapters 5, 7, 9 learn concepts instruction sets operand types. Read specific instruction set available local computer. Consult assembler reference manual learn syntax conventions needed assembler. Also read assembler reference manual determine conventions used call external function. Overview Write assembly language program shifts integer value right calls C function display resulting value hexadecimal. Procedure Details (checkmark completed) ______1. Write C function, int_out, takes integer argument uses printf display argument value hexadecimal. ______2. Test function ensure works correctly. ______3. Write assembly language program places integer 4 register shifts contents register right one bit. ______4. Extend program pass result previous step argument external function int_out. ______5. Verify program produces 0x2 output. ______6. Load integer 0xBD5A register print result verify sign extension works correctly. ______7. Instead shifting integer 4 right one bit, load 0xBD5B7DDE 32-bit register, shift right one bit, verify output correct.Optional Extensions (checkmark completed) ______8. Rewrite external function int_out assembly language program pass multiple arguments. __________________________________________________________________________ Notes Lab 8 Processors: Function Called C Purpose learn write assembly language function called C program. Background Reading Preparation Read Chapter 9 learn concept subroutine calls assembly languages, read C assembler reference manuals determine conventions C uses call function local computer. Overview Write assembly language function called C program perform exclusive two integer values. Procedure Details (checkmark completed) ______1. Write C function xor takes two integer arguments returns exclusive-or arguments. ______2. Write C main program calls xor function two integer arguments displays result function.______3. Write axor, assembly language version C xor function behaves exactly like C version. (Do merely ask C compiler generate assembly file; write new version scratch.) ______4. Add printf call axor function use verify function correctly receives two values C program passes arguments (i.e., argument passing works correctly). ______5. Arrange C main program test axor verify code returns correct results reasonable range inputs. Hint: generate values randomly. Optional Extensions (checkmark completed) ______6. Modify C program axor function C program passes single structure argument instead two integers. Arrange structure contain two integer values. __________________________________________________________________________ Notes Lab 9 Memory: Row-Major Column-Major Array Storage Purpose understand storage arrays memory row-major order column-major order. Background Reading Preparation Read Chapters 10, 11, 12 13 learn basic memory organization difference storing arrays row-major order column-major order. OverviewInstead using built-in language facilities declare two-dimensional arrays, implement two C functions, two_d_store two_d_fetch, use linear storage implement two- dimensional array. Function two_d_fetch takes six arguments: base address memory region used two-dimensional array, size (in bytes) single entry array, two array dimensions, two index values. example, instead two lines: int d[10,20]; x = d[4,0]; programmer code: char d[200*sizeof(int)]; x = two_d_fetch(d, sizeof(int), 10, 20, 4, 0); Function two_d_store seven arguments. first six correspond six arguments two_d_fetch, seventh value stored. example, instead of: int d[10,20]; d[4,0] = 576; programmer code: char d[200*sizeof(int)]; two_d_store(d, sizeof(int), 10, 20, 4, 0, 576); Procedure Details (checkmark completed) ______1. Implement function two_d_store, using row-major order store array. ______2. Create area memory large enough hold array, initialize entire area zero, call two_d_store store specific values various locations. Use hex dump program created Lab 6 display result, verify correct values stored. ______3. Implement function two_d_fetch, using row-major order match order used two_d_store. ______4. Verify implementations two_d_store two_d_fetch work correctly. ______5. Test two_d_store two_d_fetch boundary conditions, minimum maximum array dimensions. ______6. Rewrite two_d_store two_d_fetch use column-major order. ______7. Verify code column-major order works correctly.Optional Extensions (checkmark completed) ______8. Verify functions two_d_store two_d_fetch work correctly array stores: characters, integers, double-precision items. ______9. Extend two_d_store two_d_fetch work correctly range array index. example, allow first index range -5 +15, allow second index range 30 40. Lab 10 Input / Output: Buffered I/O Library Purpose learn buffered I/O operates compare performance buffered unbuffered I/O. Background Reading Preparation Read Chapters 14, 15 16 learn I/O general, read Chapter 17 learn buffering. Overview Build three C functions, buf_in, buf_out, buf_flush implement buffered I/O. call, function buf_in delivers next byte data file descriptor zero. additional input needed device, buf_in reads sixteen kilobytes data buffer, allows successive calls return values buffer. call, function buf_out writes one byte data buffer. buffer full program invokes function buf_flush, data buffer written file descriptor one. Procedure Details (checkmark completed) ______1. Implement function buf_in.______2. Verify buf_in operates correctly input less sixteen kilobytes (i.e., less one buffer data). ______3. Redirect input file larger thirty-two kilobytes, verify buf_in operates correctly input requires buf_in fill buffer multiple times. ______4. Implement functions buf_out buf_flush. ______5. Verify buf_out buf_flush operate correctly output less one buffer (i.e., less sixteen kilobytes). ______6. Verify buf_out buf_flush operate correctly output spans multiple buffers. Optional Extensions (checkmark completed) ______7. Compare performance functions buf_in, buf_out, buf_flush performance unbuffered I/O (i.e., read write one byte) various size files. Plot results. ______8. Measure performance buf_in, buf_out, buf_flush various size buffers copying large file. Use buffers range size 4 bytes 100 Kbytes, plot results. __________________________________________________________________________ Notes Lab 11 Hex Dump Program Assembly Language Purpose gain experience coding assembly language. Background Reading PreparationReview Chapters 5, 7, 9, assembly language programs written earlier labs. Overview Rewrite hex dump program Lab 6 assembly language. Procedure Details (checkmark completed) ______1. Rewrite basic hex dump function Lab 6 assembly language. ______2. Verify assembly language version gives output C version. Optional Extensions (checkmark completed) ______3. Extend assembly language dump function start stop byte address (i.e., omit leading values first line output trailing values last line). ______4. Change function print values decimal instead ASCII character form. ______5. Modify dump function instead printing ASCII values, function assumes memory corresponds machine instructions gives mnemonic opcodes instruction. example, first word line corresponds load instruction, print load. ______6. Add argument dump function selects among various forms output (ASCII characters, decimal, instructions) __________________________________________________________________________ Notes †Warning: LED must electrical characteristics appropriate circuit — arbitrary LED draw much electrical power cause 7400-series integrated circuit burn out. †On computers, address size equals integer size.Appendix 2 Rules Boolean Algebra Simplification A2.1 Introduction Boolean expressions simplified applying rules Boolean algebra. Specifically, rules cover associative, reflexive, distributive properties. engineering perspective, motivation simplification implementation requires fewer gates. example, consider logical or. know either two expressions true, logical also true. Thus, expression X true replaced true. A2.2 Notation Used table follows, dot ( · ) denotes logical and, plus sign ( + ) denotes logical or, apostrophe ( ’ ) denotes logical not, 0 denotes false, 1 denotes true. Using notation, expression: (X + Y) · Z’ represents: (X Y) (not Z ) A2.3 Rules Boolean AlgebraFigure A2.1 lists nineteen rules Boolean algebra. Although many initial rules may seem obvious, included completeness. Figure A2.1 Rules Boolean algebra used simplify Boolean expressions.Appendix 3 Quick Introduction x86 Assembly Language A3.1 Introduction Engineers use term x86 refer series processors use architecture created Intel Corporation†. processor Intel series powerful predecessor. time, design changed 16-bit architecture 32-bit architecture. transition, Intel enforced backward compatibility guarantee newer chips series could execute code written earlier chips. Thus, fundamentals remain same. x86 undergone another transition, time 32-bit architecture 64-bit architecture; change led AMD, Intel competitor. again, backward compatibility key part transition. brief chapter, discuss 32-bit version first, describe 64-bit extensions. follows CISC approach, x86 processor large, complex instruction set. fact, instruction set huge — vendor’s manuals document instructions comprise nearly 3000 pages. x86 contain special instructions high-speed graphics operations, trigonometric functions, large set instructions operating system uses control processor modes, set protection, handle I/ O. addition 32-bit instructions used applications running recent processors, Intel x86 processor retains hardware supports previous versions. Consequently, cannot review entire instruction set brief appendix. Instead, provide overview introduces basics. programmer masters fundamentals, learning new instructions straightforward. A3.2 x86 General-Purpose RegistersAs result extensions, x86 architecture suffers confusing unexpected inconsistencies. example, architecture includes eight general-purpose registers, inconsistencies especially apparent way general-purpose registers named referenced. particular, initial design used four general-purpose 16-bit registers, assembly language provided names individual bytes register. registers extended thirty-two bits, extended register given name, architecture mapped original 16-bit registers onto low-order sixteen bits corresponding extended register. Thus, assembly language provides set names allows programmer reference entire 32-bit register, low-order 16-bit region register, individual bytes within 16-bit region. Unfortunately, names confusing. Initially, registers assigned specific purposes, names reflect historical use. Figure A3.1 illustrates eight general-purpose registers, lists historical purpose, gives names registers well subpart†. Figure A3.1 eight general-purpose registers x86 processor, historical purpose, names used reference register subparts.Although registers longer restricted original purpose, stack pointer (ESP) base pointer (EBP) still special meaning. use base stack pointers procedure call explained below. A3.3 Allowable Operands Operands specify values used operation location result. operand specify one registers, location memory, constant. instruction specifies combinations allowed. example, mov instruction copies data one location another. Mov copy constant register memory, copy data value register memory, memory register, one register another. However, mov cannot copy data one memory location directly another. Thus, copy data two memory locations, programmer must use two instructions. First, programmer uses mov copy data memory register, second, programmer uses mov copy data register new memory location. Figure A3.2 lists nomenclature used describe set operands allowed given instruction. Figure A3.2 Nomenclature used specify allowable operands. later section explains memory address computed. now, sufficient understand use terminology Figure A3.2 explain instructions. example, consider mov instruction, copies data item specified target operand location specified source operand. Figure A3.3 uses nomenclature Figure A3.2 list allowable operand combinations mov.Figure A3.3 Allowable operand combinations mov instruction. A3.4 Intel AT&T Forms x86 Assembly Language examine instructions, important know assembly language basics. example, assembly language employs fixed statement format one statement per line: label statement. optional, consists name used identify statement. statement defines data item, label specifies name item; statement contains code, label followed colon, used pass control statement. opcode field defines type data item specifies instruction; zero operands follow opcode give details data operation. Unfortunately, many x86 assemblers created, features distinguish others. Rather examining individual assembler, focus two major categories. first category employs syntax originally defined Intel adopted Microsoft; known informally Intel assembly language Microsoft-Intel assembly language. second category employs syntax originally defined AT&T Bell Labs Unix adopted open source community use Linux; known AT&T assembly language GNU assembly language (gas). Assemblers either category functionally equivalent allow programmer code arbitrary sequence x86 instructions declare arbitrary data items memory. Despite overall similarities, two types assemblers differ many details. example, order operands listed, way registers referenced, comment syntax differ. Although either type used, programmer may find one type intuitive, convenient, helps catch programming errors. types assemblers widely used industry, examine examples each. A3.4.1 Characteristics Intel Assembly Language Intel assembler following characteristics:• Operand order right-to-left, source right target left • Comments begin semicolon (;) • Register names coded without punctuation (e.g., eax) • Immediate constants written without punctuation • assembler deduces opcode type operand(s) remember operand order, programmer think assignment statement high- level language: expression right value assigned variable left. Thus, Intel assembly language, data movement operation written: mov target, source example, following code adds two contents register EBX places result register EAX: mov eax, ebx+2 x86 hardware implicit operand types, means run-time, opcode specifies types operands. example, instead one mov instruction, hardware contains opcode possible operand type. is, x86 opcode move byte, another opcode move word, on. hexadecimal values 88, 89, 8A, 8B, 8C, ... produces binary code, Intel assembler deduces correct opcode operand types. target single byte, assembler chooses opcode moves byte, target sixteen-bit register, assembler chooses opcode moves sixteen-bit value, on. instruction follows pattern — although programmer uses single mnemonic program (e.g., add addition sub subtraction), processor set opcodes operation, Intel assembler chooses opcode appropriate operands programmer specifies. A3.4.2 Characteristics AT&T Assembly Language AT&T assembler following characteristics: • Operand order left-to-right, source left target right • Comments enclosed /*... */ begin hash mark (#) • Register names preceded percent sign (e.g., %eax) • Immediate constants preceded dollar sign ($) • programmer chooses mnemonic indicates type well operationThe operand order exact opposite used Intel assembler. Thus, AT&T assembly language, data movement operation written: mov source, target example following code adds two contents register EBX places thirty-two-bit result register EAX. movl %ebx+2, %eax A3.5 Arithmetic Instructions Addition Subtraction. Many arithmetic logical operations x86 take two arguments: source target. target specifies location, register, source specifies location constant. processor uses two operands perform specified operation, places result target operand. example, instruction: Intel: add eax,ebx AT&T: add %ebx,%eax causes processor add values registers EAX EBX, place result register EAX. words, processor changes EAX adding value EBX. Figure A3.4 lists allowable combinations operands addition subtraction. Figure A3.4 allowable combinations operands add subtract. Increment Decrement. addition add sub, x86 offers increment decrement instructions add subtract one. instructions, opcodes inc dec (followed designator AT&T assembler), take single argument register memory location. example, instruction: Intel: inc ecx AT&T: incl %ecx increments value register ECX one. programmer must decide whether use inc add.The inclusion increment decrement instructions instruction set illustrates important principle architecture: CISC architecture, one used x86, often provides one instruction perform given computation. Multiplication Division. Integer multiplication division pose interesting challenge computer architects. product results pair registers multiplied exceed single register. fact, product twice long register. computers also permit dividend used integer division larger single register. x86 includes many variations integer multiplication division. variations multiplication allow programmer restrict result specific size (e.g., restrict product thirty-two bits). handle case product exceed one register, x86 uses combination two registers hold result. example, multiplying two thirty-two-bit values, x86 places sixty-four-bit result EDX EAX registers, EDX holding significant thirty-two bits EAX holding least significant thirty-two bits. x86 also permits integer division sixty-four-bit operand, stored pair registers. course, integer division also used small items. Even dividend occupy sixty-four bits, x86 use two registers hold result integer division: one holds quotient holds remainder. way capture remainder makes computations hashing efficient. x86 offers two basic forms multiplication. first form follows paradigm addition subtraction: multiplication instruction two arguments, result overwrites value first argument. second form takes three arguments, third constant. processor multiplies second third arguments, places result location specified first argument. example, Intel: imul eax,edi,42 AT&T: imul %edi,42,%eax multiplies contents register EDI 42, places result register EAX. A3.6 Logical Operations x86 processors offer logical operations treat data items string bits operate individual bits. Three logical operations perform bit-wise operations two operands: logical and, or, xor. fourth logical operation, not, performs bit inversion single operand. Figure A3.5 lists operand types used logical operations.Figure A3.5 allowable combinations operands and, or, xor, instructions. addition bit-wise logical operations, x86 supports bit shifting. Shifting applied register memory location. essence, shift takes current value, moves bits left right amount specified, places result back register memory location. shifting, x86 supplies zero bits fill needed. example, shifting left K bits, hardware sets low-order K bits result zero, shifting right K bits, hardware sets high-order K bits result zero. Figure A3.6 lists allowable operands used left right shift operations. Figure A3.6 Allowable operand combinations shift instructions. notation <cl> refers 8-bit register CL. A3.7 Basic Data Types Assembly language x86 allows programmer define initialized uninitialized data items. Data declarations must preceded .data assembler directive, tells assembler items treated data. programmer define individual data item define sequence unnamed data items occupy successive memory locations. Figure A3.7 lists basic data types available; figure assumes AT&T assembler set produce code thirty-two-bit processor.Figure A3.7 Basic data types used Intel AT&T assemblers. type assembler permits programmer assign initial value data items. Intel assembly program, label starts column 1, data type appears next, initial value item follows data type. Intel assembler uses question mark indicate data item uninitialized. AT&T assembly program, label ends colon, followed type initial value; initialization omitted, zero assumed. Figures A3.8 A3.9 illustrate declarations two types assemblers. Figure A3.8 Examples data declarations using Intel assembler. assembler places successive data items adjacent bytes memory. figures, item named u placed byte following item named v. Similarly, placed beyond z; z four bytes long, starts four bytes beyond location z starts. Figure A3.9 Examples data declarations using AT&T assembler. A3.8 Data Blocks, Arrays, StringsAlthough provide data aggregates, structs, x86 assembly languages allow programmer declare multiple occurrences data item occupy contiguous memory locations. example, declare three sixteen-bit items initialized 1, 2, 3, programmer write three separate lines declare one item list multiple items single line: Intel: q DW 1, 2, 3 AT&T: q: .hword 1, 2, 3 Intel assembler uses modifier K DUP(value) repeat data value multiple times; AT&T assembler uses .space fill specified size memory value. example, declare one thousand repetitions data byte initialized numeric value 220, one writes: Intel: DB 1000 DUP(220) AT&T: s: .space 1000, 220 AT&T assembler provides .rept macro declare repetitions larger items, dozen occurrences four-byte zero: Intel: DD 12 DUP(0) AT&T: .rept 12 .long 0 .endr addition numeric values, x86 assembly language allows programmer use ASCII characters initial values. Intel assembler encloses character constants single quote marks, allows multiple characters used form string. assembler add trailing zero (null termination). AT&T assembler surrounds string characters double quotes, uses directive .ascii .asciz declare string; .ascii add null termination byte, .asciz does. example, programmer declare byte memory initialized letter Q string contains characters hello world, without null termination. Intel: c DB ’Q’ DB ’hello world’ e DB ’hello world’, 0 AT&T: c: .ascii "Q" d: .ascii "hello world" e: .asciz "hello world" A3.9 Memory References seen, many x86 instructions permit operation reference memory, either fetch value use instruction store result. x86 hardware offers complex mechanism programmer use compute memory address: address formedby adding contents two general-purpose registers plus constant. Furthermore, one registers multiplied two, four, eight. examples illustrate possibilities. Data Names. straightforward form memory reference consists reference named data item. Intel assemblers use square brackets enclose name memory item, AT&T assemblers precede name dollar sign. either case, assembler computes memory address assigned item, substitutes constant instruction. example, assembly program contains declaration 16-bit data item named T, following instructions used move 16-bit value memory register DX: Intel: mov dx,[T] AT&T: movw $T,%dx Indirection Register. programmer compute numeric value, place value register, specify register used memory address. example, instruction: Intel: mov eax,[ebx] AT&T: movl (%ebx),%eax uses contents register EBX memory address, moves four bytes starting address register EAX. Expressions compute address permitted, provided adhere rule adding two registers constant, option multiplying one registers two, four, eight. example, possible form memory address adding contents EAX, contents ECX, constant 16, using address store value register EDI. Intel notation, operation written: mov [eax+exb+16],edi rules addresses difficult master first seem somewhat arbitrary. Figure A3.10 lists examples valid invalid memory references.Figure A3.10 Examples valid invalid memory references using Intel notation. A3.10 Data Size Inference Explicit Size Directives memory address calculated run time, address merely consists unsigned thirty-two-bit integer value. is, address specify size item memory. x86 assembler uses heuristics infer data size whenever possible. example, following instruction moves value memory register EAX, four bytes long, assembler infer memory address refers four-byte value. example, Intel notation, instruction written: mov eax,[ebx+esi+12] Similarly, name assigned data item declared single byte, assembler infers memory reference name refers one byte. cases, however, programmer must use explicit size directive specify size data item. example, suppose programmer wishes store –1 sixteen-bit word memory. programmer computes memory address, placed register EAX. assembler cannot know programmer thinks address pointing sixteen-bit (i.e., two-byte) data item, infer refers four-byte item. Therefore, programmer must add size directive memory reference, following example uses Intel notation: mov WORD PTR[eax],-1 good programming practice use size directive make intention clear doubt, even cases inference rules assembler produce correct result. Figure A3.11 summarizes three size directives available.Figure A3.11 Size directives prepended memory references Intel assembler. A3.11 Computing Address said integer value computed, placed register, used memory address. However, address computation begins known location memory, initial location array. example, Intel assembler, suppose array four-byte integers declared using name iarray initialized zero: iarray DB 1000 DUP(0) memory location ith element computed multiplying four (because element four bytes long) adding result address first byte array. running program obtain address first byte array? generally, program obtain memory address arbitrary variable? answer lies special instruction loads address register rather value. Specified name load effective address mnemonic lea, special instruction takes register memory location operands. Unlike mov instruction, lea access item memory. Instead, lea stops computes memory address, places address specified register. example, lea eax,[iarray] places memory address first byte item iarray register EAX. Observe computing offset ith element array four-byte integers straightforward. First, place register, example, EBX. index register, memory location corresponding element array computed single lea instruction: mov ebx,[i] ; obtain index variable memory lea eax,[4*ebx+iarray] ; place address ith element EAX A3.12 Stack Operations Push Pop x86 hardware includes instructions manipulate memory stack. stack operates Last-In-First-Out (LIFO) data structure, recently added item accessed first.Like stack processors, x86 stack grows downward, new items added successively lower memory addresses. Despite growing downward memory, say recently added item “top” stack. item added stack, say item pushed onto stack, top stack corresponds new item. top item removed stack, say stack popped. x86 stack always uses four-byte items — item pushed onto stack, four additional bytes memory used. Similarly, item popped stack, item contains four bytes, stack occupies four fewer bytes memory. x86, ESP register (stack pointer) contains current address top stack. Thus, although ESP appear explicitly, stack manipulation instructions always change value ESP. names stack instructions reflect generic terminology described above: push pop. Figure A3.12 lists allowable argument types. Figure A3.12 Operands allowed push pop instructions. register ESP set, adding items stack trivial. example, instruction: push eax pushes value register EAX onto stack, instruction: pop [qqqq] pops top stack places value memory location name qqqq. Similarly, instruction: push -1 pushes constant -1 onto stack. x86 hardware stack bound, means programmer must plan stack use carefully avoid situations stack grows downward memory area used variables. A3.13 Flow Control Unconditional Branch Normally, statement executed, processor proceeds next statement. x86 supports three types instructions change flow control:unconditional branch conditional branch procedure call return unconditional branch instruction easiest understand: opcode jmp (for “jump”), operand label statement. encounters jmp instruction, processor immediately moves specified label continues execution. example, jmp prntname means next instruction processor execute instruction label prntname. programmer must placed label instruction (presumably first instruction sequence prints name). Intel notation, programmer writes: prntname: mov eax,[nam] . . . A3.14 Conditional Branch Condition Codes arithmetic instruction sets internal value processor known condition code. conditional branch instruction uses value condition code choose whether branch continue execution next sequential statement. set conditional branch instructions exists; instruction encodes specific test. Figure A3.13 summarizes. Figure A3.13 Conditional branch instructions meaning each. example, following code Intel notation decrements register EBX jumps label atzero resulting value zero. dec ebx ;subtract 1 ebx jz atzero ;jump label atzero EBX reaches zero instructions Figure A3.13 require programmer compare two items. example, jge tests greater-than-or-equal. However, conditional branch instructions perform comparisons — single operand consists label specifying branch. arithmetic tests, conditional branches involving comparison rely condition code. Various instructions set condition code, means conditional branch executed immediately condition code set. conditional branch immediately follow instruction sets condition code, programmer must code extra instruction sets condition. x86 architecture includes two instructions used set condition code: test cmp. Neither two modifies contents registers memory. Instead, merely compare two values set condition code. cmp instruction checks equality. essence, cmp performs subtraction discards answer, keeping condition code. example, following code Intel notation tests whether four-byte value memory location var1 value 123, jumps label bb does. cmp DWORD PTR [var1], 123 ;compare memory item var1 123 jeq bb ;jump label bbb equal test instruction sophisticated: performs bit-wise two operands, sets various condition code bits accordingly. result, test sets conditions whether data value contains odd even parity. A3.15 Subprogram Call Return x86 hardware supports subroutine invocation (i.e., ability call subprogram subprogram return caller). Subroutine invocation forms key part run-time support needed high-level procedural language. Figure A3.14 summarizes two x86 instructions make subprograms possible: one instruction used invoke subprogram used subprogram return caller. Figure A3.14 Instructions used invoke subprogram: call invokes subprogram, ret returns caller. Subprogram call return use run-time stack. example, call instruction pushes return address stack. next section discusses details. A3.16 C Calling Conventions Argument PassingThe term calling conventions refers rules calling called programs use guarantee agreement details, location arguments. Calling conventions assign responsibilities calling program called subprogram. example, conventions specify exactly calling program pushes arguments stack subprogram use, exactly subprogram return value calling program use. high-level language defines set calling conventions. use popular C calling conventions examples. Although conventions intended allow C C++ programs invoke assembly language program assembly language program invoke C functions, C calling conventions also used assembly language program invokes assembly language subprogram. Thus, examples general. easiest way understand calling conventions visualize contents run-time stack subprogram invoked. example consists call passes three integer arguments (four bytes per argument) values 100, 200, 300 subprogram four local variables, thirty-two bits. calling conventions specify following call: • Caller Actions. caller pushes values registers EAX, ECX, EDX onto stack save them. caller pushes arguments onto stack reverse order. Thus, arguments 100, 200, 300, caller pushes 300, pushes 200, pushes 100. Finally, caller invokes call instruction, pushes return address (i.e., address immediately following call instruction) onto stack jumps subprogram. • Called Subprogram Actions. called subprogram pushes EBP register onto stack, sets EBP current top stack. caller pushes EBX, EDI, ESI registers onto stack, pushes local variable onto stack (or merely changes stack pointer allocate space local variable uninitialized). Figure A3.15 illustrates stack immediately subprogram call occurred (i.e., caller called subprogram followed conventions outlined above). understand figure, remember stack grows downward memory. is, push operation decrements stack pointer pop operation increments pointer.Figure A3.15 Illustration run-time stack subprogram called three arguments subprogram reserved space four local variables. finishes, called subprogram must undo actions taken call return caller. following specifies steps subprogram caller take return. • Called Subprogram Return Actions. called subprogram deallocates local variables stack. so, subprogram adds 4N bytes stack pointer, N number local variables (each local variable assumed four bytes long). subprogram restores ESI, EDI, EBX, EBP registers popping saved values stack. Finally, subprogram executes ret instruction pop return address stack jump back caller. • Caller Return Actions. called subprogram returns, caller deallocates arguments (e.g., adding constant stack pointer equal four times number arguments). Finally, caller restores values EDX, ECX EAX.A3.17 Function Calls Return Value Technically, set calling conventions applies procedure call. case function call, subprogram must return value caller. convention, return value passed register EAX. Therefore, function invoked, calling conventions modified caller restore saved value EAX. make sense caller save EAX stack calling function? function returns, EAX contain returned value. However, two reasons EAX saved. First, symbolic debugger expect stack layout procedure function called. Second, caller may choose continue computation saving result function. example, suppose compiler used EAX hold index variable loop. loop contains statement as: r = f(t); compiler may generate code save value EAX call, store return value memory location r immediately function f returns, restore EAX allow loop continue. A3.18 Extensions Sixty-four Bits (x64) x86 architecture expanded sixty-four-bit version. Interestingly, AMD Corporation defined extension scheme eventually adopted Intel vendors. Known x86-64, often shortened x64, architecture includes many changes. example, arithmetic logical instructions, instructions involve two registers, instructions involve register memory location, instructions involve two memory locations extended operate sixty-four-bit quantities. stack operations changed push pop sixty-four bits (eight bytes) time, pointers sixty-four- bits wide. two changes pertinent discussion involve general-purpose registers: • general-purpose register extended make sixty-four-bits long. • Eight additional general-purpose registers added, making total sixteen general-purpose registers. x86, x64 architecture attempts preserve backward compatibility. example, lower half sixty-four-bit register referenced thirty-two-bit register. Furthermore, possible reference sixteen-bit eight-bit parts first four registers exactly x86. Figure A3.16 illustrates general-purpose registers available x64; readers compare figure Figure A3.1 page 474.A3.19 Summary reviewed x86 fundamentals, including data declarations, registers, operand types, basic instructions, arithmetic logical instructions, memory references, stack operations, conditional unconditional branch, subprogram invocation. x86 architecture provides many instructions, programmer may choice multiple mechanisms perform given task. sixty-four-bit extension designed known name x64.Figure A3.16 General-purpose registers x64 architecture. †The name arises Intel assigned part numbers 8086, 80286, 80386, 80486.†Because assemblers distinguish uppercase lowercase, names eax EAX refer register. Programmers tend use lowercase; documentation tends use uppercase.Appendix 4 ARM Register Definitions Calling Sequence A4.1 Introduction previous appendix presents overview x86 x64 architectures. seen, x86 canonical example CISC instruction set. appendix continues discussion providing information ARM architecture. ARM provides canonical example RISC architecture. Although ARM defined set processors, appendix focuses features common thirty-two-bit ARM products. reader referred ARM documentation details specific models. A4.2 Registers ARM Processor ARM processor 16 general-purpose registers numbered 0 15, generally denoted names r0 r15. Registers r0 r3 used pass arguments called subroutine pass results back caller. Registers r4 r11 used hold local variables subroutine currently run. Register r12 intra-procedural call scratch register. Register r13 stack pointer. Register r14 link register, used subroutine call. Finally, register r15 program counter (i.e., instruction pointer). Thus, loading address r15 causes processor branch address. Figure A4.1 summarizes purpose registers, gives alternate names used gcc assembler.Figure A4.1 general-purpose registers ARM architecture, alternate name used assembly language, meaning assigned register. addition general-purpose registers, ARM processor thirty-two-bit Current Program Status Register (CPSR). CPSR divided many fields, including fields control processor mode operation, control interrupts, report condition code operation, report hardware errors, control endianness system. Figure A4.2 summarizes bit fields CPSR. A4.3 ARM Calling Conventions Programming languages support call mechanism piece code calls subroutine, subroutine executes, control passes back point call occurred. terms run-time environment, subroutine calls pushed onto run-time stack. say code becomes caller invokes subroutine, use term callee refer subroutine invoked. C Programming Language, subroutine known function; use term throughout remainder appendix. Although hardware places constraints function invocation, programmer compiler free choose details. Throughout chapter, describe calling conventions gcc follows, become widely accepted.Figure A4.2 Bits ARM CPSR meaning each. argument passing conventions ARM following characteristics: • Allow caller pass zero arguments callee • Optimize access first four arguments • Allow callee return set results caller • Specify registers callee change must unchanged call returns • Specify run-time stack used function called returns Many functions four fewer parameters. optimize access first four arguments, values passed general-purpose registers a1 a4 (i.e., registers r0 r3). Additional arguments placed stack memory. callee access first four arguments merely referencing register, access extremely fast. callee use registers a1 a4 return result called program. programming languages, function returns one result, found register a1. argument result larger 32 bits, value placed memory address passed argument register. Figure A4.3 shows example stack layout immediately function call. example clarify calling conventions explain register values preserved function call.Figure A4.3 Layout items run-time stack function calls function B six arguments. figure, function executing called function B, takes six arguments. first four arguments passed registers†, means appear stack.However, arguments beyond first four must passed stack. Therefore, function pushes arguments 5 6 onto runtime stack reverse order calling function B. figure shows, extra arguments last two items stack call occurs. caller expects values general-purpose registers preserved function call. is, caller expects called function disturb register values. course, functions need use registers. Therefore, called function saves register contents upon entry restores returning. figure illustrates, prelude code function B pushes link register (r14), frame pointer (r7), registers r4 r6, registers r8 r11 onto stack. prelude code function B reserves space stack local variables (if any). local storage allocated, function B ready run. function B returns, postlude code function runs. postlude code restores registers saved values stack, leaves stack exactly call. †Recall first four arguments passed registers a1 a4.Index Constants numeric items 0-address 128 1-address 129 14-pin Dual In-line Package 22 1 ½ - address 130 2-address 129 3-address 130 64-bit x86 architecture 491 7400 family 22 8086 473 abacus 12 absolute branch instruction 98 abstraction 343 access protocol 291 adders 453 address 210, 297 data multiplexing 295 conflict 298 mapping 253 space 214, 297, 298 space hole 304 translation 253 Advanced Graphics Port 427 aggregate 62 AGP 427 alignment 213 ALU 74, 119, 151, 361 AMP 366ampere (amp) 12 analog computer 11 ANSI 51 Application Specific Integrated Circuit 39 architecture 5 Harvard 70 Von Neumann 70 one-address 129 two-address 129 zero-address 128 argument passing 99, 174 Arithmetic Logic Unit 74, 119, 151 ARM processor 495 registers 495 array processor 364 ASCII 51, 52 ASIC 39 assembler 79, 165 assembly language 165, 461, 469 language function 463 asymmetric assignment 302 parallelism 362 Asymmetric Multiprocessor 366 asynchronous programming 321, 342 AT&T assembly language 476 autoincrement 216 auxiliary bus 307 Avogadro constant 58 AX register 474, 493 B backward compatibility 143, 424, 473 bank memory 219, 257 registers 90 base-bound 259 BCD 61 benchmark 415bias constant 60 bidirectional transfer 282 big endian 54, 457 binary counter 27, 455 digit 46 Binary Coded Decimal 61 bit 46 big endian 54 little endian 54 serial 361 block-oriented device 338 board-level architecture 423 Boolean algebra 14 bootstrap 81 branch prediction 158 breadboard 446, 451 bridge 306 bridging (between two buses) 424 bubble 94 buffer 347, 383 chaining 328 flushing 349 buffered I/O 467 buffering 347 burst packets 328 bus 151, 209, 289, 297 access protocol 291 arbiter 291 controller 292 interface 292 BX register 474, 493 byte 46 addressing 211 alignment 213 C C.mmp 366 cache 228 coherence protocol 236flushing 272 hierarchy 233 hit 229 locality 229 miss 229 preloading 233 replacement policy 231 transparency 229 write-back 235 write-through 235 caching 228 callee 496 caller 496 calling conventions 176, 463 conventions (ARM) 496 CAM 221, 246 capacitor 204 Carnegie multiminiprocessor 366 carry 53 carry bit 20 CDC6600 367 Central Processing Unit 71 chaining buffers 328 channel (IBM) 367 character set 51 character-oriented device 338 chip-level architecture 423 CISC 91 clock 28, 207, 282, 455 skew 36 synchronization 37 zone 36 clockless logic 37 close 346 cluster computer 375 CMOS 14, 38 coarse granularity mapping 261 coarse-grain parallelism 362 column-major order 270, 465 combinatorial circuit 22command interpreter 383 communication 369 compatibility mode 143 compiler 79 Complementary Metal Oxide Semiconductor 14, 38 Complex Instruction Set Computer 91 computational engine 73 condition code 104, 171, 487 conditional execution 171 statement 172 conflict (registers) 91 Content Addressable Memory 221, 246 contention 369 context switch 323 contiguous address space 304 addresses 256 virtual address space 255 Control Status Registers 318, 336 controller 119, 206, 281 (bus) 292 chips 426 cooling 399 coordination 369 coprocessor 76 core 38, 142 cosine 91 counter 27, 455 CPSR (ARM) 496 CPU 71 crossbar switch 308 CSR 318, 336 current 12 current program status register (ARM) 496 CX register 474, 493 D-cache 239 data(assembly language) 480 aggregate 62 cache 239 memory 110 multiplexing 295 path 75, 115 pipelining 381 store 198 transfer 281 dead beef 217 decoding 455 deep sleep 403 definite iteration 172 definition (assembly macro) 185 demand paging 262 demultiplexor 29, 284 demux 29 destination operand 130 device driver 336 independence 336 digital computer 11 Digital Video Disc 346 DIMM 206 DIP 22 direct mapped cache 240 Direct Memory Access 328 directive (assembly language) 177 directory table 274 dirty bit 235 disabling interrupts 324 DMA 328 double data rate 208 indirection 135 precision 58, 90 drain (transistor) 13 DRAM 204 driver 336 DualIn-line Memory Module 206 In-line Package 22 dual core 362 dual-processor computer 363 dumb device 327 dump 469 DVD 346 DX register 474, 493 dynamic RAM 204 E EAX, EBX, ECX, EDX registers 474, 493 EBCDIC 51 elegance 104 embedded 432 embedded systems processor 76 enable line 23 enabling interrupts 324 encapsulation 336 endian 457 endmacro 186 engine 73 ESI, EDI, ESP, EBP registers 474, 493 event 321 exact match search 222 exclusive use 372 execution pipeline 92 explicit operand encoding 132 exponent 58 extended value 89 F falling edge 27 fanout 17 fast data rate memory 208 feedback 31 FET 13 fetch 88 fetch-execute cycle 78, 322 fetch-store 199, 294fib-arm.s 182 fib-x86.s 180 fib.c 178 Fibonacci sequence 177 Field Effect Transistor 13 Field Programmable Gate Array 149 FIFO 197 file 343 fine granularity mapping 261 fine-grain parallelism 362 finger 292 First-In-First-Out 197 fixed logic processor 72 fixed-length instructions 88 flash memory 197 flip-flop 25 floating point 58 point accelerator 76 point operations per second 413 FLOPS 413 flush 348 flush time 391 flushing buffer 349 cache 272 Flynn 360 statement 172 for-loop assembly 172 forwarding 97 FPGA 149 fragmentation 262 full adder 21 full-duplex interaction 282 fully associative cache 245 function call assembly 174 invocation 488 G gas 476gate 14 (transistor) 13 delay 400 gather write 329 GB 215 general-purpose processor 76 register 88, 110, 198 gigabyte 215 GNU assembly language 476 graphics accelerator 73 engine 73 grid computing 375 ground 12 H half adder 20 half-duplex interaction 283 halting processor 80 hardware lock 372 pipeline 382 hardwired 72 Harvard Architecture 70 Mark 70 heat dissipation 399 Hertz 28 heterogeneous pipeline 383 hex dump 469 hexadecimal 49 hibernation 403 hiding hardware details 336 hierarchy 233 hierarchy memory 198 High Level Language 79 high-level language 163 programming language 163hit 229 hit ratio 231 HLL 79 hole 256, 304 homogeneous pipeline 383 Hz 28 I-cache 239 I/O library 467 IBM 86 IC 38 IEEE floating point 58 statement assembly 171 if-then-else 172 immediate operand 131 implicit operand encoding 132 parallelism 363 indefinite iteration 172 indirection 135 Industry Standard Architecture 425 input 279 instantaneous power 396 instruction cache 239 format 86 memory 110, 116 mix 415 pipeline 92, 385 pointer 88, 98, 114 register 136 representation 86 scheduling 156 set 86, 110 set architecture 86 stall 94 store 198 integrated circuit 38 Intel assembly language 476 interface(bus) 292 width 282 interleaved memory 220 internal bus 291 interrupt 320 disabling 324 fetch-execute 322 enabling 324 handler 323 mechanism 321 vector 323 inverter 17 invocation (subroutine) 488 ioctl 346 ISA 86, 425 ISP 385 I/O 279 bound 370 bus 289 J jsr instruction 99, 173 jump instruction 98 jump subroutine 173 K Kbyte (Kilobyte) 215 kilowatt 395 L L1, L2, L3 cache 237 label (assembly language) 166, 177 Last-In-First-Out 486 latch 23 latency 206, 283 lea 485 Least Recently Used 232 LED 449 level cache 237 library 79LIFO 486 Light Emitting Diode 449 limited parallelism 283 link register (ARM) 495 Linux 432, 476 little endian 54, 457 load 199 balancer 375 effective address 485 locality reference 229 logic gate 14, 453 long (assembly language) 176 longest prefix match 223 loosely coupled 375 low-level code 336 language 163 programming language 164 lower half (device driver) 337 LPM 403 LRU 232 LSB 48 macro 185 macro instruction set 146 macroscopic architecture 423 parallelism 360, 362 mantissa 58 mapping addresses 253 master-slave 367 math coprocessor 367 MB 215 Mbps MBps 283 Megabits per second 215, 283 megabyte 215 Megabytes per second 283 megawatt 395 memory 110, 195 address 210bank 219, 257 bus 209, 289, 297 cache 234 controller 206, 209 cycle time 207 dump 217 hierarchy 198 mapped architecture 305 module 257 organization 196, 209 technology 196 transfer size 209 Memory Management Unit 252 Metal Oxide Semiconductor 13 microcode 146 microcontroller 76, 146 microengines 432 microprocessor 146 microscopic architecture 423 parallelism 360, 361 millions instructions per second 413 milliwatt 395 MIMD 363, 366 minimalistic instruction set 103 MIPS 413 MISD 363 miss 229 miss ratio 231 MMU 252 mode execution 143, 258 modified bit 267 Modified Harvard Architecture 239 module 257 MOSFET 13 mother board 291 move instruction 297 MSB 48 multicore 38, 362 multicore processors 402multilayer board 39 multilevel cache hierarchy 233 Multiple Instructions Multiple Data 366 multiple level interrupts 325 multiplexing 283, 295 multiplexor 119, 284 multiprocessing 257 multiprogramming 257 mutual exclusion 343, 372 N n-channel MOSFET 13 N-type silicon 38 N-way interleaving 220 name (assembly language) 177 nand 15 network cluster 375 Network File System 416 NFS 416 nibble 62 no-op instruction 96 non-selfreferential 507 nonvolatile memory 196 15 normalized 58 Northbridge 427 octal 49 off-chip cache 237 on-chip cache 237 one-address 129 opcode 87, 113 opcode (assembly language) 166 open 346 open/read/write/close paradigm 345 operand 87, 166 operation chaining 330 organization computer 5of memory 196, 209 orthogonal instruction set 104 orthogonality 104 out-of-order execution 157 output 279 overflow 28, 53 P p-channel MOSFET 13 P-type silicon 38 packed BCD 62 page 262 fault 263 replacement 263 table 264 parallel data transfer 291 interface 281 parallelism 359 parameterized logic processor 72 macro 185 procedure 174 partial match search 223 passive 291 pattern engine 74 PCB 39 PCI 425 performance 385 Peripheral Component Interconnect 425 peripheral processors 367 physical memory address 210 memory cache 234 pin 22, 38, 283 pinout 38 pipe 382 pipeline 92, 381 architectures 390 characteristics 382 performance 385stage 382 pipelining 359 pluggable device 326 pointer 216 polling 315 pop 128, 486 power 395 density 399 80 Power Saving Polling 406 prefetch 233 preloading 233 preprocessor 79 presence bit 267 primary memory 197 principle orthogonality 104 printed circuit board 39 procedure call 173, 491 call assembly 174 invocation 488 processor 71 program counter 88, 98 counter (ARM) 495 programmable 77 programmable logic processor 72 Programmable Read Memory 197 programmed I/O 314 programming 77 programming interface 285 PROM 197 propagation delay 24 proprietary bus 290 protection 261 PSP 406 push 128, 486 Q quad core 362quad-processor PC 363 quadruple data rate 208 query engine 74 queue requests 339 R RAM 197, 203 Random Access Memory 197, 203 RAX, RBX, RCX, RDX registers 493 read 199, 294, 346 read cycle time 207 Read Memory 77, 197 real address 252 real mode 259 Reduced Instruction Set Computer 91 redundant hardware 374 refresh circuit 205 register 24, 74, 88, 110, 198 (x86) 493 allocation 89 bank 90 conflict 91 file 110 spilling 89 window 100 register-offset mechanism 133 relative branch instruction 99 replacement policy 231 request queue 339 reset 27, 81 resident set 264 ret instruction 173 return address 174 interrupt 323 subroutine 173 RISC 91 RISC assembly 461 rising edge 27 ROM 77, 197 row-major order 269, 465RSI, RDI, RSP, RBP registers 493 scaling 363 scatter read 329 scheduling 156 schematic diagram 14 scientific computation 412 notation 58 scoreboard 157 search key 221 secondary memory 197 seek 346 segment 262 segmentation 262 selectable logic processor 72 self-clocking 282 sequential 321 architecture 364 circuit 23 Sequential Access Memory 197 serial interface 282 set associative memory cache 245 setup time 390 shared bus 291 shell 383 sign-magnitude 55 silicon 38 SIMD 363, 364 sine 91 Single Instruction Multiple Data 364 Instruction Single Data 364 single precision 58 SISD 363, 364 size directive 484 slave 367 sleep 403 slot CAM 221smart device 327 SMP 366 SoC 38 soft error 327 power switch 81 software pipeline 382 solderless breadboard 446 solid state disk 196, 197 source (transistor) 13 code 79 operand 130 Southbridge 427 space SPEC 415 cfp2006 415 cint2006 415 SPECmark 415 speedup 370 spilling (registers) 89 SRAM 204 SSD 196, 197 stack architecture 128 pointer 495 stage pipeline 382 stall 391 standard I/O library 352 Standard Performance Evaluation Corporation 415 state 23 static RAM 204 status register 318 stdio 352 store 88, 199, 294 stored program 71 program computer 63 string 482 struct 217 subprogram invocation 488subroutine call 173 invocation 488 supercomputer 375 switching context 323 fabrics 308 symbol table 184 Symmetric Multiprocessor 366 symmetric parallelism 362 synchronization 314 synchronous memory 208 pipeline 382 programming 321, 342 system call 346 controller 427 System Chip 38 system-level architecture 423 System/360 86 TCAM 223 terminal (on transistor) 13 ternary CAM 223 test-and-set 343 threshold voltage 401 throughput 283 Tianhe-2 supercomputer 376 tightly coupled 375 timeout 299 timing 35 TLB 269, 270 transistor 13 Transistor-Transistor Logic 17 transition diagram 26 translation lookaside buffer 269 addresses 253 transparency interrupts 322transparent 93, 306, 425 bridge 308 cache 229 coprocessor 76 trap 346 tRC 207 truth table 15, 19 TTL 17 Turbo Boost (Intel) 402 tWC 207 two-pass assembler 183, 184 U unassigned address 299 unconditional branch 487 underflow 53 Unicode 53 uniprocessor 364 Universal Serial bus 326 Unix 416 unsigned integer 53 upper half (device driver) 337 USB 326 use bit 267 V valid bit 242 variable-length instructions 88 vector instruction 365 processor 364 vectored interrupt 323 vertical microcode 150 virtual address 252 address space 252 memory 251 memory system 252 VM 251 volatile memory 196voltage 12 voltmeter 12 Von Neumann Architecture 70 Von Neumann bottleneck 131 W wafer 38 Web load balancer 375 while-loop assembly 172 width bus 294 interface 282 word 210 window (register) 100 wiring 451 wiring kit 447 word (assembly language) 176 addressing 210 memory 210 size 210 width 210 write 199, 294, 346 write cycle time 207 write-back cache 235 write-through cache 235 X x64 491 x86 473 x86 general-purpose registers 474 x86-64 491 XScale 432 Z zero-address 128