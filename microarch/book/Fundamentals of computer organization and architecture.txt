FUNDAMENTALS OF
COMPUTERORGANIZATION ANDARCHITECTURE
Mostafa Abd-El-Barr
King Fahd University of Petroleum & Minerals (KFUPM)
Hesham El-Rewini
Southern Methodist University
A JOHN WILEY & SONS, INC PUBLICATIONFUNDAMENTALS OF
COMPUTER ORGANIZATION ANDARCHITECTUREWILEY SERIES ON PARALLEL AND DISTRIBUTED COMPUTING
SERIES EDITOR: Albert Y. Zomaya
Parallel & Distributed Simulation Systems /Richard Fujimoto
Surviving the Design of Microprocessor and Multimicroprocessor Systems:
Lessons Learned /Veljko Milutinovic
Mobile Processing in Distributed and Open Environments /Peter Sapaty
Introduction to Parallel Algorithms /C. Xavier and S.S. Iyengar
Solutions to Parallel and Distributed Computing Problems: Lessons fromBiological Sciences /Albert Y. Zomaya, Fikret Ercal, and Stephan Olariu (Editors)
New Parallel Algorithms for Direct Solution of Linear Equations /
C. Siva Ram Murthy, K.N. Balasubramanya Murthy, and Srinivas Aluru
Practical PRAM Programming /Joerg Keller, Christoph Kessler, and
Jesper Larsson Traeff
Computational Collective Intelligence /Tadeusz M. Szuba
Parallel & Distributed Computing: A Survey of Models, Paradigms, and
Approaches /Claudia Leopold
Fundamentals of Distributed Object Systems: A CORBA Perspective /
Zahir Tari and Omran Bukhres
Pipelined Processor Farms: Structured Design for Embedded Parallel
Systems /Martin Fleury and Andrew Downton
Handbook of Wireless Networks and Mobile Computing /Ivan Stojmenoviic
(Editor)
Internet-Based Workﬂow Management: Toward a Semantic Web /
Dan C. Marinescu
Parallel Computing on Heterogeneous Networks /Alexey L. Lastovetsky
Tools and Environments for Parallel and Distributed Computing Tools /
Salim Hariri and Manish ParasharDistributed Computing: Fundamentals, Simulations and Advanced Topics,
Second Edition /Hagit Attiya and Jennifer Welch
Smart Environments: Technology, Protocols and Applications /
Diane J. Cook and Sajal K. Das (Editors)
Fundamentals of Computer Organization and Architecture /M. Abd-El-Barr
and H. El-RewiniFUNDAMENTALS OF
COMPUTERORGANIZATION ANDARCHITECTURE
Mostafa Abd-El-Barr
King Fahd University of Petroleum & Minerals (KFUPM)
Hesham El-Rewini
Southern Methodist University
A JOHN WILEY & SONS, INC PUBLICATIONThis book is printed on acid-free paper. /C131
Copyright #2005 by John Wiley & Sons, Inc. All rights reserved.
Published by John Wiley & Sons, Inc., Hoboken, New Jersey.
Published simultaneously in Canada.
No part of this publication may be reproduced, stored in a retrieval system, or transmitted in any form or
by any means, electronic, mechanical, photocopying, recording, scanning, or otherwise, except aspermitted under Section 107 or 108 of the 1976 United States Copyright Act, without either the prior
written permission of the Publisher, or authorization through payment of the appropriate per-copy
fee to the Copyright Clearance Center, Inc., 222 Rosewood Drive, Danvers, MA 01923,
for permission should be addressed to the Permissions Department, John Wiley & Sons, Inc.,
111 River Street, Hoboken, NJ 07030; (201) 748-6011, fax (201) 748-6008.
Limit of Liability /Disclaimer of Warranty: While the publisher and author have used their best efforts
in preparing this book, they make no representations or warranties with respect to the accuracy or
completeness of the contents of this book and speciﬁcally disclaim any implied warranties of
merchantability or ﬁtness for a particular purpose. No warranty may be created or extended by salesrepresentatives or written sales materials. The advice and strategies contained herein may not besuitable for your situation. You should consult with a professional where appropriate. Neither the
publisher nor author shall be liable for any loss of proﬁt or any other commercial damages, including
but not limited to special, incidental, consequential, or other damages.
For general information on our other products and services please contact our Customer Care Department
within the U.S. at 877-762-2974, outside the U.S. at 317-572-3993 or fax 317-572-4002.
Wiley also publishes its books in a variety of electronic formats. Some content that appears in print,
however, may not be available in electronic format.
Library of Congress Cataloging-in-Publication Data:
Abd-El-Barr, Mostafa.
Fundamentals of computer organization and architecture /Mostafa Abd-El-Barr, Hesham El-Rewini
p. cm. — (Wiley series on parallel and distributed computing)
Includes bibliographical references and index.
ISBN 0-471-46741-3 (cloth volume 1) — ISBN 0-471-46740-5 (cloth volume 2)
1. Computer architecture. 2. Parallel processing (Electronic computers) I. Abd-El-Barr, Mostafa, 1950–
II. Title. III. Series.
QA76.9.A73E47 2004
004.2
02—dc22
2004014372
Printed in the United States of America
1 0987654321978-750-84 00,fax978-646-86 00,oronthewebatwww.copyrigh t.com. Requests tothePublisherTo my family members (Ebtesam, Muhammad, Abd-El-Rahman, Ibrahim, and Mai)
for their support and love
—Mostafa Abd-El-Barr
To my students, for a better tomorrow
—Hesham El-Rewini& CONTENTS
Preface xi
1. Introduction to Computer Systems 1
1.1. Historical Background 2
1.2. Architectural Development and Styles 41.3. Technological Development 51.4. Performance Measures 61.5. Summary 11Exercises 12
References and Further Reading 14
2. Instruction Set Architecture and Design 15
2.1. Memory Locations and Operations 15
2.2. Addressing Modes 182.3. Instruction Types 262.4. Programming Examples 312.5. Summary 33Exercises 34References and Further Reading 35
3. Assembly Language Programming 37
3.1. A Simple Machine 383.2. Instructions Mnemonics and Syntax 40
3.3. Assembler Directives and Commands 43
3.4. Assembly and Execution of Programs 44
3.5. Example: The X86Family 47
3.6. Summary 55Exercises 56References and Further Reading 57
4. Computer Arithmetic 59
4.1. Number Systems 594.2. Integer Arithmetic 63
vii4.3 Floating-Point Arithmetic 74
4.4 Summary 79Exercises 79References and Further Reading 81
5. Processing Unit Design 83
5.1. CPU Basics 835.2. Register Set 855.3. Datapath 895.4. CPU Instruction Cycle 915.5. Control Unit 955.6. Summary 104
Exercises 104
References 106
6. Memory System Design I 107
6.1. Basic Concepts 1076.2. Cache Memory 1096.3. Summary 130Exercises 131References and Further Reading 133
7. Memory System Design II 135
7.1. Main Memory 135
7.2. Virtual Memory 1427.3. Read-Only Memory 156
7.4. Summary 158
Exercises 158References and Further Reading 160
8. Input–Output Design and Organization 161
8.1. Basic Concepts 1628.2. Programmed I /O 164
8.3. Interrupt-Driven I /O 167
8.4. Direct Memory Access (DMA) 1758.5. Buses 1778.6. Input–Output Interfaces 1818.7. Summary 182
Exercises 183
References and Further Reading 183viii CONTENTS9 Pipelining Design Techniques 185
9.1. General Concepts 185
9.2. Instruction Pipeline 187
9.3. Example Pipeline Processors 201
9.4. Instruction-Level Parallelism 2079.5. Arithmetic Pipeline 2099.6. Summary 213Exercises 213References and Further Reading 215
10 Reduced Instruction Set Computers (RISCs) 215
10.1. RISC /CISC Evolution Cycle 217
10.2. RISCs Design Principles 21810.3. Overlapped Register Windows 220
10.4. RISCs Versus CISCs 221
10.5. Pioneer (University) RISC Machines 22310.6. Example of Advanced RISC Machines 22710.7. Summary 232Exercises 233References and Further Reading 233
11 Introduction to Multiprocessors 235
11.1. Introduction 23511.2. Classiﬁcation of Computer Architectures 23611.3. SIMD Schemes 244
11.4. MIMD Schemes 246
11.5. Interconnection Networks 252
11.6. Analysis and Performance Metrics 25411.7. Summary 254Exercises 255References and Further Reading 256
Index 259CONTENTS ix& PREFACE
This book is intended for students in computer engineering, computer science,
and electrical engineering. The material covered in the book is suitable for a one-
semester course on “Computer Organization & Assembly Language” and a one-semester course on “Computer Architecture.” The book assumes that studentsstudying computer organization and /or computer architecture must have had
exposure to a basic course on digital logic design and an introductory course on
high-level computer language.
This book reﬂects the authors’ experience in teaching courses on computer organ-
ization and computer architecture for more than ﬁfteen years. Most of the material
used in the book has been used in our undergraduate classes. The coverage in the
book takes basically two viewpoints of computers. The ﬁrst is the programmer’s
viewpoint and the second is the overall structure and function of a computer. Theﬁrst viewpoint covers what is normally taught in a junior level course on ComputerOrganization and Assembly Language while the second viewpoint covers what isnormally taught in a senior level course on Computer Architecture. In what follows,we provide a chapter-by-chapter review of the material covered in the book. In doingso, we aim at providing course instructors, students, and practicing engineers /scien-
tists with enough information that can help them select the appropriate chapter orsequences of chapters to cover /review.
Chapter 1 sets the stage for the material presented in the remaining chapters. Our
coverage in this chapter starts with a brief historical review of the development ofcomputer systems. The objective is to understand the factors affecting computingas we know it today and hopefully to forecast the future of computation. We alsointroduce the general issues related to general-purpose and special-purpose
machines. Computer systems can be deﬁned through their interfaces at a number
of levels of abstraction, each providing functional support to its predecessor. The
interface between the application programs and high-level language is referred toasLanguage Architecture. The Instruction Set Architecture deﬁnes the interface
between the basic machine instruction set and the Runtime and I/O Control .A
different deﬁnition of computer architecture is built on four basic viewpoints.These are the structure, the organization, the implementation, and the performance.The structure deﬁnes the interconnection of various hardware components, theorganization deﬁnes the dynamic interplay and management of the various com-
ponents, the implementation deﬁnes the detailed design of hardware components,
and the performance speciﬁes the behavior of the computer system. Architectural
xidevelopment and styles are covered in Chapter 1. We devote the last part of our cov-
erage in this chapter to a discussion on the different CPU performance measuresused.
The sequence consisting of Chapters 2 and 3 introduces the basic issues related to
instruction set architecture and assembly language programming. Chapter 2 coversthe basic principles involved in instruction set architecture and design. We start byaddressing the issue of storing and retrieving information into and from memory,
followed by a discussion on a number of different addressing modes. We also
explain instruction execution and sequencing in some detail. We show the appli-cation of the presented addressing modes and instruction characteristics in writingsample segment codes for performing a number of simple programming tasks.Building on the material presented in Chapter 2, Chapter 3 considers the issuesrelated to assembly language programming. We introduce a programmer’s viewof a hypothetical machine. The mnemonics and syntax used in representing thedifferent instructions for the machine model are then introduced. We follow that
with a discussion on the execution of assembly programs and an assembly language
example of the X86 Intel CISC family.
The sequence of chapters 4 and 5 covers the design and analysis of arithmetic cir-
cuits and the design of the Central Processing Unit (CPU). Chapter 4 introduces thereader to the fundamental issues related to the arithmetic operations and circuitsused to support computation in computers. We ﬁrst introduce issues such as numberrepresentations, base conversion, and integer arithmetic. In particular, we introduce
a number of algorithms together with hardware schemes that are used in performing
integer addition, subtraction, multiplication, and division. As far as ﬂoating-point arith-
metic, we introduce issues such as ﬂoating-point representation, ﬂoating-point oper-ations, and ﬂoating-point hardware schemes. Chapter 5 covers the main issuesrelated to the organization and design of the CPU. The primary function of the CPUis to execute a set of instructions stored in the computer’s memory. A simple CPU con-sists of a set of registers, Arithmetic Logic Unit (ALU), and Control Unit (CU). Thebasic principles needed for the understanding of the instruction fetch-execution
cycle, and CPU register set design are ﬁrst introduced. The use of these basic principles
in the design of real machines such as the 80 /C286 and the MIPS are shown. A detailed
discussion on a typical CPU data path and control unit design is also provided.
Chapters 6 and 7 combined are dedicated to Memory System Design. A typical
memory hierarchy starts with a small, expensive, and relatively fast unit, called the
cache . The cache is followed in the hierarchy by a larger, less expensive, and rela-
tively slow main memory unit. Cache and main memory are built using solid-state
semiconductor material. They are followed in the hierarchy by a far larger, less
expensive, and much slower magnetic memories that consist typically of the
(hard) disk and the tape. We start our discussion in Chapter 6 by analyzing the fac-
tors inﬂuencing the success of a memory hierarchy of a computer. The remainingpart of Chapter 6 is devoted to the design and analysis of cache memories. Theissues related to the design and analysis of the main and the virtual memory arecovered in Chapter 7. A brief coverage of the different read-only memory (ROM)implementations is also provided in Chapter 7.xii PREFACEI/O plays a crucial role in any modern computer system. A clear understanding
and appreciation of the fundamentals of I /O operations, devices, and interfaces are
of great importance. The focus of Chapter 8 is a study on input–output (I /O) design
and organization. We cover the basic issues related to programmed and Interrupt-
driven I /O. The interrupt architecture in real machines such as 80 /C286 and
MC9328MX1 /MXL AITC are explained. This is followed by a detailed discussion
on Direct Memory Access (DMA), busses (synchronous and asynchronous), and
arbitration schemes. Our coverage in Chapter 8 concludes with a discussion on
I/O interfaces.
There exists two basic techniques to increase the instruction execution rate of a
processor. These are: to increase the clock rate, thus decreasing the instructionexecution time, or alternatively to increase the number of instructions that can beexecuted simultaneously. Pipelining and instruction-level parallelism are examplesof the latter technique. Pipelining is the focus of the discussion provided in Chapter9. The idea is to have more than one instruction being processed by the processor at
the same time. This can be achieved by dividing the execution of an instruction
among a number of sub-units (stages), each performing part of the required oper-ations, i.e., instruction fetch, instruction decode, operand fetch, instructionexecution, and store of results. Performance measures of a pipeline processor areintroduced. The main issues contributing to instruction pipeline hazards are dis-cussed and some possible solutions are introduced. In addition, we present the con-cept of arithmetic pipelining together with the problems involved in designing such
pipeline. Our coverage concludes with a review of two pipeline processors, i.e., the
ARM 1026EJ-S and the UltraSPARC-III.
Chapter 10 is dedicated to a study of Reduced Instruction Set Computers (RISCs).
These machines represent a noticeable shift in computer architecture paradigm. The
RISC paradigm emphasizes the enhancement of computer architectures with theresources needed to make the execution of the most frequent and the most time-consuming operations most efﬁcient. RISC-based machines are characterized bya number of common features, such as, simple and reduced instruction set, ﬁxed
instruction format, one instruction per machine cycle, pipeline instruction fetch /exe-
cute units, ample number of general purpose registers (or alternatively optimized
compiler code generation), Load /Store memory operations, and hardwired control
unit design. Our coverage in this chapter starts with a discussion on the evolutionof RISC architectures and the studies that led to their introduction. Overlapped Reg-ister Windows, an essential concept in the RISC development, is also discussed. Weshow the application of the basic RISC principles in machines such as the BerkeleyRISC, the Stanford MIPS, the Compaq Alpha, and the SUN UltraSparc.
Having covered the essential issues in the design and analysis of uniprocessors
and pointing out the main limitations of a single stream machine, we provide anintroduction to the basic concepts related to multiprocessors in Chapter 11. Herea number of processors (two or more) are connected in a manner that allows themto share the simultaneous execution of a single task. The main advantage forusing multiprocessors is the creation of powerful computers by connecting manyexisting smaller ones. In addition, a multiprocessor consisting of a number ofPREFACE xiiisingle uniprocessors is expected to be more cost effective than building a high-
performance single processor. We present a number of different topologies usedfor interconnecting multiple processors, different classiﬁcation schemes, and atopology-based taxonomy for interconnection networks. Two memory-organizationschemes for MIMD (multiple instruction multiple data) multiprocessors, i.e., SharedMemory and Message Passing, are also introduced. Our coverage in this chapterends with a touch on the analysis and performance metrics for multiprocessors.
Interested readers are referred to more elaborate discussions on multiprocessors in
our book entitled Advanced Computer Architectures and Parallel Processing ,
John Wiley and Sons, Inc., 2005.
From the above chapter-by-chapter review of the topics covered in the book, it
should be clear that the chapters of the book are, to a great extent, self-containedand inclusive. We believe that such an approach should help course instructors toselectively choose the set of chapters suitable for the targeted curriculum. However,our experience indicates that the group of chapters consisting of Chapters 1 to 5 and
8 is typically suitable for a junior level course on Computer Organization and
Assembly Language for Computer Science, Computer Engineering, and ElectricalEngineering students. The group of chapters consisting of Chapters 1, 6, 7, 9–11is typically suitable for a senior level course on Computer Architecture. Practicingengineers and scientists will ﬁnd it feasible to selectively consult the material cov-ered in individual chapters and /or groups of chapters as indicated in the chapter-by-
chapter review. For example, to ﬁnd more about memory system design, interested
readers may consult the sequence consisting of Chapters 6 and 7.
ACKNOWLEDGMENTS
We would like to express our thanks and appreciation to a number of people who
have helped in the preparation of this book. Students in our Computer Organization
and Computer Architecture courses at the University of Saskatchewan (UofS),
SMU, KFUPM, and Kuwait University have used drafts of different chapters andprovided us with useful feedback and comments that led to the improvement ofthe presentation of the material in the book; to them we are thankful. Our colleaguesDonald Evan, Fatih Kocan, Peter Seidel, Mitch Thornton, A. Naseer, HabibAmmari, and Hakki Cankaya offered constructive comments and excellent sugges-
tions that led to noticeable improvement in the style and presentation of the book
material. We are indebted to the anonymous reviewers arranged by John Wiley
for their suggestions and corrections. Special thanks to Albert Y. Zomaya, the
series editor and to Val Moliere, Kirsten Rohstedt, and Christine Punzo of JohnWiley for their help in making this book a reality. Of course, responsibility forerrors and inconsistencies rests with us. Finally, and most of all, we want to thankour families for their patience and support during the writing of this book.
M
OSTAFA ABD-EL-BARR
HESHAM EL-REWINIxiv PREFACE& CHAPTER 1
Introduction to Computer Systems
The technological advances witnessed in the computer industry are the result of a
long chain of immense and successful efforts made by two major forces. Theseare the academia, represented by university research centers, and the industry,represented by computer companies. It is, however, fair to say that the current tech-nological advances in the computer industry owe their inception to universityresearch centers. In order to appreciate the current technological advances in thecomputer industry, one has to trace back through the history of computers andtheir development. The objective of such historical review is to understand the
factors affecting computing as we know it today and hopefully to forecast the
future of computation. A great majority of the computers of our daily use areknown as general purpose machines. These are machines that are built with no
speciﬁc application in mind, but rather are capable of performing computationneeded by a diversity of applications. These machines are to be distinguishedfrom those built to serve (tailored to) speciﬁc applications. The latter are knownasspecial purpose machines. A brief historical background is given in Section 1.1.
Computer systems have conventionally been deﬁned through their interfaces at
a number of layered abstraction levels, each providing functional support to its pre-decessor. Included among the levels are the application programs, the high-levellanguages, and the set of machine instructions. Based on the interface betweendifferent levels of the system, a number of computer architectures can be deﬁned.The interface between the application programs and a high-level language isreferred to as a language architecture. Theinstruction set architecture deﬁnes the
interface between the basic machine instruction set and the runtime andI/O control.
A different deﬁnition of computer architecture is built on four basic viewpoints.
These are the structure, the organization, the implementation, and the performance.
In this deﬁnition, the structure deﬁnes the interconnection of various hardware com-ponents, the organization deﬁnes the dynamic interplay and management of thevarious components, the implementation deﬁnes the detailed design of hardwarecomponents, and the performance speciﬁes the behavior of the computer system.Architectural development and styles are covered in Section 1.2.
1Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.A number of technological developments are presented in Section 1.3. Our discus-
sion in this chapter concludes with a detailed coverage of CPU performance measures.
1.1. HISTORICAL BACKGROUND
In this section, we would like to provide a historical background on the evolution of
cornerstone ideas in the computing industry. We should emphasize at the outset thatthe effort to build computers has not originated at one single place. There is everyreason for us to believe that attempts to build the ﬁrst computer existed in differentgeographically distributed places. We also ﬁrmly believe that building a computer
requires teamwork. Therefore, when some people attribute a machine to the name
of a single researcher, what they actually mean is that such researcher may haveled the team who introduced the machine. We, therefore, see it more appropriateto mention the machine and the place it was ﬁrst introduced without linking thatto a speciﬁc name. We believe that such an approach is fair and should eliminateany controversy about researchers and their names.
It is probably fair to say that the ﬁrst program-controlled (mechanical) computer
ever build was the Z1 (1938). This was followed in 1939 by the Z2 as the ﬁrst oper-
ational program-controlled computer with ﬁxed-point arithmetic. However, the ﬁrst
recorded university-based attempt to build a computer originated on Iowa StateUniversity campus in the early 1940s. Researchers on that campus were able tobuild a small-scale special-purpose electronic computer. However, that computerwas never completely operational. Just about the same time a complete design ofa fully functional programmable special-purpose machine, the Z3, was reported inGermany in 1941. It appears that the lack of funding prevented such design frombeing implemented. History recorded that while these two attempts were in progress,
researchers from different parts of the world had opportunities to gain ﬁrst-hand
experience through their visits to the laboratories and institutes carrying out the
work. It is assumed that such ﬁrst-hand visits and interchange of ideas enabled
the visitors to embark on similar projects in their own laboratories back home.
As far as general-purpose machines are concerned, the University of Pennsylvania
is recorded to have hosted the building of the Electronic Numerical Integrator andCalculator (ENIAC) machine in 1944. It was the ﬁrst operational general-purposemachine built using vacuum tubes. The machine was primarily built to help compute
artillery ﬁring tables during World War II. It was programmable through manual set-
ting of switches and plugging of cables. The machine was slow by today’s standard,
with a limited amount of storage and primitive programmability. An improved version
of the ENIAC was proposed on the same campus. The improved version of theENIAC, called the Electronic Discrete Variable Automatic Computer (EDVAC),was an attempt to improve the way programs are entered and explore the conceptof stored programs. It was not until 1952 that the EDVAC project was completed.Inspired by the ideas implemented in the ENIAC, researchers at the Institute for
Advanced Study (IAS) at Princeton built (in 1946) the IAS machine, which was
about 10 times faster than the ENIAC.2 INTRODUCTION TO COMPUTER SYSTEMSIn 1946 and while the EDVAC project was in progress, a similar project was
initiated at Cambridge University. The project was to build a stored-program com-
puter, known as the Electronic Delay Storage Automatic Calculator (EDSAC). Itwas in 1949 that the EDSAC became the world’s ﬁrst full-scale, stored-program,fully operational computer. A spin-off of the EDSAC resulted in a series of machinesintroduced at Harvard. The series consisted of MARK I, II, III, and IV. The lattertwo machines introduced the concept of separate memories for instructions and
data. The term Harvard Architecture was given to such machines to indicate the
use of separate memories. It should be noted that the term Harvard Architecture
is used today to describe machines with separate cache for instructions and data.
The ﬁrst general-purpose commercial computer, the UNIVersal Automatic
Computer (UNIVAC I), was on the market by the middle of 1951. It represented animprovement over the BINAC, which was built in 1949. IBM announced its ﬁrst com-puter, the IBM701, in 1952. The early 1950s witnessed a slowdown in the computerindustry. In 1964 IBM announced a line of products under the name IBM 360 series.
The series included a number of models that varied in price and performance. This led
Digital Equipment Corporation (DEC) to introduce the ﬁrst minicomputer , the PDP-8.
It was considered a remarkably low-cost machine. Intel introduced the ﬁrst micropro-
cessor , the Intel 4004, in 1971. The world witnessed the birth of the ﬁrst personal
computer (PC) in 1977 when Apple computer series were ﬁrst introduced. In 1977
the world also witnessed the introduction of the VAX-11 /780 by DEC. Intel followed
suit by introducing the ﬁrst of the most popular microprocessor, the 80 /C286 series.
Personal computers, which were introduced in 1977 by Altair, Processor
Technology, North Star, Tandy, Commodore, Apple, and many others, enhancedthe productivity of end-users in numerous departments. Personal computers fromCompaq, Apple, IBM, Dell, and many others, soon became pervasive, and changedthe face of computing.
In parallel with small-scale machines, supercomputers were coming into play.
The ﬁrst such supercomputer, the CDC 6600, was introduced in 1961 by ControlData Corporation. Cray Research Corporation introduced the best cost /performance
supercomputer, the Cray-1, in 1976.
The 1980s and 1990s witnessed the introduction of many commercial parallel
computers with multiple processors. They can generally be classiﬁed into twomain categories: (1) shared memory and (2) distributed memory systems. Thenumber of processors in a single machine ranged from several in a sharedmemory computer to hundreds of thousands in a massively parallel system.Examples of parallel computers during this era include Sequent Symmetry, InteliPSC, nCUBE, Intel Paragon, Thinking Machines (CM-2, CM-5), MsPar (MP),Fujitsu (VPP500), and others.
One of the clear trends in computing is the substitution of centralized servers by
networks of computers. These networks connect inexpensive, powerful desktopmachines to form unequaled computing power. Local area networks (LAN) ofpowerful personal computers and workstations began to replace mainframes andminis by 1990. These individual desktop computers were soon to be connectedinto larger complexes of computing by wide area networks (WAN).1.1. HISTORICAL BACKGROUND 3The pervasiveness of the Internet created interest in network computing and more
recently in grid computing. Grids are geographically distributed platforms of com-
putation. They should provide dependable, consistent, pervasive, and inexpensive
access to high-end computational facilities.
Table 1.1 is modiﬁed from a table proposed by Lawrence Tesler (1995). In this
table, major characteristics of the different computing paradigms are associated with
each decade of computing, starting from 1960.
1.2. ARCHITECTURAL DEVELOPMENT AND STYLES
Computer architects have always been striving to increase the performance of their
architectures. This has taken a number of forms. Among these is the philosophy thatby doing more in a single instruction, one can use a smaller number of instructions toperform the same job. The immediate consequence of this is the need for fewermemory read /write operations and an eventual speedup of operations. It was also
argued that increasing the complexity of instructions and the number of addressingmodes has the theoretical advantage of reducing the “semantic gap” between theinstructions in a high-level language and those in the low-level (machine) language.
A single (machine) instruction to convert several binary coded decimal (BCD)
numbers to binary is an example for how complex some instructions were intendedto be. The huge number of addressing modes considered (more than 20 in theVAX machine) further adds to the complexity of instructions. Machines followingthis philosophy have been referred to as complex instructions set computers
(CISCs). Examples of CISC machines include the Intel Pentium
TM, the Motorola
MC68000TM, and the IBM & Macintosh PowerPCTM.
It should be noted that as more capabilities were added to their processors,
manufacturers realized that it was increasingly difﬁcult to support higher clockrates that would have been possible otherwise. This is because of the increasedTABLE 1.1 Four Decades of Computing
Feature Batch Time-sharing Desktop Network
Decade 1960s 1970s 1980s 1990s
Location Computer room Terminal room Desktop Mobile
Users Experts Specialists Individuals Groups
Data Alphanumeric Text, numbers Fonts, graphs Multimedia
Objective Calculate Access Present Communicate
Interface Punched card Keyboard & CRT See & point Ask & tell
Operation Process Edit Layout Orchestrate
Connectivity None Peripheral cable LAN Internet
Owners Corporate computer
centersDivisional IS shops Departmental
end-usersEveryone
CRT, cathode ray tube; LAN, local area network.4 INTRODUCTION TO COMPUTER SYSTEMScomplexity of computations within a single clock period. A number of studies from
the mid-1970s and early-1980s also identiﬁed that in typical programs more than80% of the instructions executed are those using assignment statements, conditionalbranching and procedure calls. It was also surprising to ﬁnd out that simple assign-ment statements constitute almost 50% of those operations. These ﬁndings caused adifferent philosophy to emerge. This philosophy promotes the optimization ofarchitectures by speeding up those operations that are most frequently used while
reducing the instruction complexities and the number of addressing modes.
Machines following this philosophy have been referred to as reduced instructions
set computers (RISCs). Examples of RISCs include the Sun SPARC
TMand
MIPSTMmachines.
The above two philosophies in architecture design have led to the unresolved
controversy as to which architecture style is “best.” It should, however, be men-tioned that studies have indicated that RISC architectures would indeed lead tofaster execution of programs. The majority of contemporary microprocessor chips
seems to follow the RISC paradigm. In this book we will present the salient features
and examples for both CISC and RISC machines.
1.3. TECHNOLOGICAL DEVELOPMENT
Computer technology has shown an unprecedented rate of improvement. This
includes the development of processors and memories. Indeed, it is the advancesin technology that have fueled the computer industry. The integration of numbersof transistors (a transistor is a controlled on /off switch) into a single chip has
increased from a few hundred to millions. This impressive increase has beenmade possible by the advances in the fabrication technology of transistors.
The scale of integration has grown from small-scale (SSI) to medium-scale (MSI)
to large-scale (LSI) to very large-scale integration (VLSI), and currently to wafer-scale integration (WSI). Table 1.2 shows the typical numbers of devices per chipin each of these technologies.
It should be mentioned that the continuous decrease in the minimum devices
feature size has led to a continuous increase in the number of devices per chip,
TABLE 1.2 Numbers of Devices per Chip
Integration Technology Typical number of devices Typical functions
SSI Bipolar 10–20 Gates and ﬂip-ﬂops
MSI Bipolar & MOS 50–100 Adders & counters
LSI Bipolar & MOS 100–10,000 ROM & RAM
VLSI CMOS (mostly) 10,000–5,000,000 ProcessorsWSI CMOS .5,000,000 DSP & special purposes
SSI, small-scale integration; MSI, medium-scale integration; LSI, large-scale integration; VLSI, very
large-scale integration; WSI, wafer-scale integration.1.3. TECHNOLOGICAL DEVELOPMENT 5which in turn has led to a number of developments. Among these is the increase in
the number of devices in RAM memories, which in turn helps designers to trade offmemory size for speed. The improvement in the feature size provides golden oppor-tunities for introducing improved design styles.
1.4. PERFORMANCE MEASURES
In this section, we consider the important issue of assessing the performance of a
computer. In particular, we focus our discussion on a number of performance
measures that are used to assess computers. Let us admit at the outset that there
are various facets to the performance of a computer. For example, a user of acomputer measures its performance based on the time taken to execute a givenjob (program). On the other hand, a laboratory engineer measures the performanceof his system by the total amount of work done in a given time. While the userconsiders the program execution time a measure for performance, the laboratoryengineer considers the throughput a more important measure for performance. Ametric for assessing the performance of a computer helps comparing alternative
designs.
Performance analysis should help answering questions such as how fast can a
program be executed using a given computer? In order to answer such a question,
we need to determine the time taken by a computer to execute a given job. Wedeﬁne the clock cycle time as the time between two consecutive rising (trailing)edges of a periodic clock signal (Fig. 1.1). Clock cycles allow counting unit compu-tations, because the storage of computation results is synchronized with rising (trail-ing) clock edges. The time required to execute a job by a computer is often expressed
in terms of clock cycles.
We denote the number of CPU clock cycles for executing a job to be the cycle
count (CC), the cycle time by CT, and the clock frequency by f¼1/CT.T h e
time taken by the CPU to execute a job can be expressed as
CPU time ¼CC/C2CT¼CC=f
It may be easier to count the number of instructions executed in a given program ascompared to counting the number of CPU clock cycles needed for executing that
Figure 1.1 Clock signal6 INTRODUCTION TO COMPUTER SYSTEMSprogram. Therefore, the average number of clock cycles per instruction (CPI) has
been used as an alternate performance measure. The following equation showshow to compute the CPI.
CPI¼CPU clock cycles for the program
Instruction count
CPU time ¼Instruction count /C2CPI/C2Clock cycle time
¼Instruction count /C2CPI
Clock rate
It is known that the instruction set of a given machine consists of a number ofinstruction categories: ALU (simple assignment and arithmetic and logic instruc-
tions), load,store ,branch , and so on. In the case that the CPI for each instruction
category is known, the overall CPI can be computed as
CPI¼Pn
i¼1CPI i/C2Ii
Instruction count
where Iiis the number of times an instruction of type iis executed in the program and
CPI iis the average number of clock cycles needed to execute such instruction.
Example Consider computing the overall CPI for a machine A for which the
following performance measures were recorded when executing a set of benchmark
programs. Assume that the clock rate of the CPU is 200 MHz.
Instruction
categoryPercentage of
occurrenceNo. of cycles
per instruction
ALU 38 1
Load & store 15 3
Branch 42 4
Others 5 5
Assuming the execution of 100 instructions, the overall CPI can be computed as
CPI a¼Pn
i¼1CPI i/C2Ii
Instruction count¼38/C21þ15/C23þ42/C24þ5/C25
100¼2:76
It should be noted that the CPI reﬂects the organization and the instruction set archi-
tecture of the processor while the instruction count reﬂects the instruction set archi-tecture and compiler technology used. This shows the degree of interdependence
between the two performance parameters. Therefore, it is imperative that both the1.4. PERFORMANCE MEASURES 7CPI and the instruction count are considered in assessing the merits of a given
computer or equivalently in comparing the performance of two machines.
A different performance measure that has been given a lot of attention in recent
years is MIPS (million instructions-per-second (the rate of instruction execution
per unit time)), which is deﬁned as
MIPS ¼Instruction count
Execution time /C2106¼Clock rate
CPI/C2106
Example Suppose that the same set of benchmark programs considered above
were executed on another machine, call it machine B, for which the followingmeasures were recorded.
Instruction
categoryPercentage of
occurrenceNo. of cycles
per instruction
ALU 35 1
Load & store 30 2
Branch 15 3
Others 20 5
What is the MIPS rating for the machine considered in the previous example
(machine A) and machine B assuming a clock rate of 200 MHz?
CPI a¼Pn
i¼1CPI i/C2Ii
Instruction count¼38/C21þ15/C23þ42/C24þ5/C25
100¼2:76
MIPS a¼Clock rate
CPI a/C2106¼200/C2106
2:76/C2106¼70:24
CPI b¼Pn
i¼1CPI i/C2Ii
Instruction count¼35/C21þ30/C22þ20/C25þ15/C23
100¼2:4
MIPS b¼Clock rate
CPI a/C2106¼200/C2106
2:4/C2106¼83:67
Thus MIPS b.MIPS a.
It is interesting to note here that although MIPS has been used as a performance
measure for machines, one has to be careful in using it to compare machines
having different instruction sets. This is because MIPS does not track execution
time. Consider, for example, the following measurement made on two differentmachines running a given set of benchmark programs.8 INTRODUCTION TO COMPUTER SYSTEMSInstruction
categoryNo. of
instructions
(in millions)No. of
cycles per
instruction
Machine (A)
ALU 8 1
Load & store 4 3Branch 2 4
Others 4 3
Machine (B)
ALU 10 1
Load & store 8 2
Branch 2 4
Others 4 3
CPI a¼Pn
i¼1CPI i/C2Ii
Instruction count¼(8/C21þ4/C23þ4/C23þ2/C24)/C2106
(8þ4þ4þ2)/C2106ﬃ2:2
MIPS a¼Clock rate
CPI a/C2106¼200/C2106
2:2/C2106ﬃ90:9
CPU a¼Instruction count /C2CPI a
Clock rate¼18/C2106/C22:2
200/C2106¼0:198 s
CPI b¼Pn
i¼1CPI i/C2Ii
Instruction count¼(10/C21þ8/C22þ4/C24þ2/C24)/C2106
(10þ8þ4þ2)/C2106¼2:1
MIPS b¼Clock rate
CPI a/C2106¼200/C2106
2:1/C2106¼95:2
CPU b¼Instruction count /C2CPI a
Clock rate¼20/C2106/C22:1
200/C2106¼0:21 s
MIPS b.MIPS a and CPU b.CPU a
The example shows that although machine B has a higher MIPS compared to
machine A, it requires longer CPU time to execute the same set of benchmark
programs.
Million ﬂoating-point instructions per second, MFLOP (rate of ﬂoating-point
instruction execution per unit time) has also been used as a measure for machines’performance. It is deﬁned as
MFLOPS ¼Number of floating -point operations in a program
Execution time /C21061.4. PERFORMANCE MEASURES 9While MIPS measures the rate of average instructions, MFLOPS is only deﬁned for
the subset of ﬂoating-point instructions. An argument against MFLOPS is the factthat the set of ﬂoating-point operations may not be consistent across machinesand therefore the actual ﬂoating-point operations will vary from machine tomachine. Yet another argument is the fact that the performance of a machine fora given program as measured by MFLOPS cannot be generalized to provide asingle performance metric for that machine.
The performance of a machine regarding one particular program might not be
interesting to a broad audience. The use of arithmetic and geometric means arethe most popular ways to summarize performance regarding larger sets of programs(e.g., benchmark suites). These are deﬁned below.
Arithmetic mean ¼
1
nXn
i¼1Execution time i
Geometric mean ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃYn
i¼1Execution time ins
where execution time iis the execution time for the ith program and nis the total
number of programs in the set of benchmarks.
The following table shows an example for computing these metrics.
ItemCPU time on
computer A (s)CPU time on
computer B (s)
Program 1 50 10
Program 2 500 100
Program 3 5000 1000
Arithmetic mean 1835 370
Geometric mean 500 100
We conclude our coverage in this section with a discussion on what is known as the
Amdahl’s law for speedup ( SUo) due to enhancement. In this case, we consider
speedup as a measure of how a machine performs after some enhancement relative
to its original performance. The following relationship formulates Amdahl’s law.
SUo¼Performance after enhancement
Performance before enhancement
Speedup ¼Execution time before enhancement
Execution time after enhancement
Consider, for example, a possible enhancement to a machine that will reduce theexecution time for some benchmarks from 25 s to 15 s. We say that the speedup
resulting from such reduction is SU
o¼25=15¼1:67.10 INTRODUCTION TO COMPUTER SYSTEMSIn its given form, Amdahl’s law accounts for cases whereby improvement can be
applied to the instruction execution time. However, sometimes it may be possible to
achieve performance enhancement for only a fraction of time, D. In this case a new
formula has to be developed in order to relate the speedup, SUDdue to an enhance-
ment for a fraction of time Dto the speedup due to an overall enhancement, SUo.
This relationship can be expressed as
SUo¼1
(1/C0D)þ(D=SUD)
It should be noted that when D¼1, that is, when enhancement is possible at all
times, then SUo¼SUD, as expected.
Consider, for example, a machine for which a speedup of 30 is possible after
applying an enhancement. If under certain conditions the enhancement was onlypossible for 30% of the time, what is the speedup due to this partial applicationof the enhancement?
SU
o¼1
(1/C0D)þ(D=SUD)¼1
(1/C00:3)þ0:3
30¼1
0:7þ0:01¼1:4
It is interesting to note that the above formula can be generalized as shown below toaccount for the case whereby a number of different independent enhancements canbe applied separately and for different fractions of the time, D
1,D2,...,Dn, thus
leading respectively to the speedup enhancements SUD1,SUD2,...,SUDn.
SUo¼1
½1/C0(D1þD2þ/C1/C1/C1þ Dn)/C138þ(D1þD2þ/C1/C1/C1þ Dn)
(SUD1þSUD2þ/C1/C1/C1þ SUDn)
1.5. SUMMARY
In this chapter, we provided a brief historical background for the development of
computer systems, starting from the ﬁrst recorded attempt to build a computer,
the Z1, in 1938, passing through the CDC 6600 and the Cray supercomputers,and ending up with today’s modern high-performance machines. We then provideda discussion on the RISC versus CISC architectural styles and their impact onmachine performance. This was followed by a brief discussion on the technological
development and its impact on computing performance. Our coverage in this chapter
was concluded with a detailed treatment of the issues involved in assessing the per-formance of computers. In particular, we have introduced a number of performance
measures such as CPI, MIPS, MFLOPS, and Arithmetic /Geometric performance
means, none of them deﬁning the performance of a machine consistently. Possible1.5. SUMMARY 11ways of evaluating the speedup for given partial or general improvement measure-
ments of a machine were discussed at the end of this Chapter.
EXERCISES
1. What has been the trend in computing from the following points of view?
(a) Cost of hardware
(b) Size of memory
(c) Speed of hardware
(d) Number of processing elements
(e) Geographical locations of system components
2. Given the trend in computing in the last 20 years, what are your predictions
for the future of computing?
3. Find the meaning of the following:
(a) Cluster computing
(b) Grid computing
(c) Quantum computing
(d) Nanotechnology
4. Assume that a switching component such as a transistor can switch in zero
time. We propose to construct a disk-shaped computer chip with such a com-
ponent. The only limitation is the time it takes to send electronic signals from
one edge of the chip to the other. Make the simplifying assumption that elec-
tronic signals can travel at 300,000 kilometers per second. What is the limit-ation on the diameter of a round chip so that any computation result can byused anywhere on the chip at a clock rate of 1 GHz? What are the diameterrestrictions if the whole chip should operate at 1 THz ¼10
12Hz? Is such a
chip feasible?
5. Compare uniprocessor systems with multiprocessor systems in the following
aspects:
(a) Ease of programming
(b) The need for synchronization
(c) Performance evaluation
(d) Run time system
6. Consider having a program that runs in 50 s on computer A, which has a
500 MHz clock. We would like to run the same program on another machine,
B, in 20 s. If machine B requires 2.5 times as many clock cycles as machine
A for the same program, what clock rate must machine B have in MHz?
7. Suppose that we have two implementations of the same instruction set archi-
tecture. Machine A has a clock cycle time of 50 ns and a CPI of 4.0 for some
program, and machine B has a clock cycle of 65 ns and a CPI of 2.5 for the
same program. Which machine is faster and by how much?12 INTRODUCTION TO COMPUTER SYSTEMS8. A compiler designer is trying to decide between two code sequences for a
particular machine. The hardware designers have supplied the following
facts:
Instruction
classCPI of the
instruction class
A1
B3C4
For a particular high-level language, the compiler writer is considering two
sequences that require the following instruction counts:
Code
sequenceInstruction counts
(in millions)
ABC
12 1 2
24 3 1
What is the CPI for each sequence? Which code sequence is faster? By how
much?
9. Consider a machine with three instruction classes and CPI measurements as
follows:
Instruction
classCPI of the
instruction class
A2
B5
C7
Suppose that we measured the code for a given program in two different
compilers and obtained the following data:
Code
sequenceInstruction counts
(in millions)
AB C
Compiler 1 15 5 3Compiler 2 25 2 2EXERCISES 13Assume that the machine’s clock rate is 500 MHz. Which code sequence will
execute faster according to MIPS? And according to execution time?
10. Three enhancements with the following speedups are proposed for a new
machine: Speedup(a) ¼30, Speedup(b) ¼20, and Speedup(c) ¼15.
Assume that for some set of programs, the fraction of use is 25% for
enhancement (a), 30% for enhancement (b), and 45% for enhancement (c).
If only one enhancement can be implemented, which should be chosen to
maximize the speedup? If two enhancements can be implemented, which
should be chosen, to maximize the speedup?
REFERENCES AND FURTHER READING
J.-L. Baer, Computer architecture, IEEE Comput. , 17(10), 77–87, (1984).
S. Dasgupta, Computer Architecture: A Modern Synthesis , John Wiley, New York, 1989.
M. Flynn, Some computer organization and their effectiveness ,IEEE Trans Comput. , C-21,
948–960 (1972).
D. Gajski, V. Milutinovic, H. Siegel, and B. Furht, Computer Architecture: A Tutorial ,
Computer Society Press, Los Alamitos, Calif, 1987.
W. Giloi, Towards a taxonomy of computer architecture based on the machine data type view,
Proceedings of the 10th International Symposium on Computer Architecture, 6–15,
(1983).
W. Handler, The impact of classiﬁcation schemes on computer architecture, Proceedings of
the 1977 International Conference on Parallel Processing, 7–15, (1977).
J. Hennessy and D. Patterson, Computer Architecture: A Quantitative Approach , 2nd ed.,
Morgan Kaufmann, San Francisco, 1996.
K. Hwang and F. Briggs, Computer Architecture and Parallel Processing , 2nd ed.,
McGraw-Hill, New York, 1996.
D. J. Kuck, The Structure of Computers and Computations , John Wiley, New York, 1978.
G. J. Myers, Advances in Computer Architecture , John Wiley, New York, 1982.
L. Tesler, Networked computing in the 1990s, reprinted from the Sept. 1991 Scientiﬁc
American, The Computer in the 21st Century, 10–21, (1995).
P. Treleaven, Control-driven data-driven and demand-driven computer architecture (abstract),
Parallel Comput. , 2, (1985).
P. Treleaven, D. Brownbridge, and R. Hopkins, Data drive and demand driven computer
architecture, ACM Comput. Surv. , 14(1), 95–143, (1982).
Websites
http://www.gigaﬂop.demon.co.uk //C24wasel14 INTRODUCTION TO COMPUTER SYSTEMS& CHAPTER 2
Instruction Set Architecture
and Design
In this chapter, we consider the basic principles involved in instruction set architecture
and design. Our discussion starts with a consideration of memory locations and
addresses. We present an abstract model of the main memory in which it is consideredas a sequence of cells each capable of storing nbits. We then address the issue of stor-
ing and retrieving information into and from the memory. The information storedand/or retrieved from the memory needs to be addressed. A discussion on a
number of different ways to address memory locations (addressing modes) is thenext topic to be discussed in the chapter. A program consists of a number of instruc-tions that have to be accessed in a certain order. That motivates us to explain the issueof instruction execution and sequencing in some detail. We then show the applicationof the presented addressing modes and instruction characteristics in writing samplesegment codes for performing a number of simple programming tasks.
A unique characteristic of computer memory is that it should be organized in a hier-
archy. In such hierarchy, larger and slower memories are used to supplement smallerand faster ones. A typical memory hierarchy starts with a small, expensive, and rela-tively fast module, called the cache . The cache is followed in the hierarchy by a larger,
less expensive, and relatively slow main memory part. Cache and main memory are
built using semiconductor material. They are followed in the hierarchy by larger,less expensive, and far slower magnetic memories that consist of the (hard) diskand the tape. The characteristics and factors inﬂuencing the success of the memory
hierarchy of a computer are discussed in detail in Chapters 6 and 7. Our concentration
in this chapter is on the (main) memory from the programmer’s point of view. In par-ticular, we focus on the way information is stored in and retrieved out of the memory.
2.1. MEMORY LOCATIONS AND OPERATIONS
The (main) memory can be modeled as an array of millions of adjacent cells, each
capable of storing a binary digit (bit), having value of 1 or 0. These cells are
15Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.organized in the form of groups of ﬁxed number, say n, of cells that can be dealt
with as an atomic entity. An entity consisting of 8 bits is called a byte. In many
systems, the entity consisting of nbits that can be stored and retrieved in and out
of the memory using one basic memory operation is called a word (the smallest
addressable entity). Typical size of a word ranges from 16 to 64 bits. It is, however,
customary to express the size of the memory in terms of bytes. For example,the size of a typical memory of a personal computer is 256 Mbytes, that is,
256/C22
20¼228bytes.
In order to be able to move a word in and out of the memory, a distinct address
has to be assigned to each word. This address will be used to determine the location
in the memory in which a given word is to be stored. This is called a memory write
operation. Similarly, the address will be used to determine the memory locationfrom which a word is to be retrieved from the memory. This is called a memoryread operation.
The number of bits, l, needed to distinctly address Mwords in a memory is given
byl¼log
2M. For example, if the size of the memory is 64 M(read as 64 mega-
words), then the number of bits in the address is log2(64/C2220)¼log2(226)¼
26 bits. Alternatively, if the number of bits in the address is l, then the maximum
memory size (in terms of the number of words that can be addressed using theselbits) is M¼2
l. Figure 2.1 illustrates the concept of memory words and word
address as explained above.
As mentioned above, there are two basic memory operations. These are the
memory write and memory read operations. During a memory write operation a
word is stored into a memory location whose address is speciﬁed. During amemory read operation a word is read from a memory location whose address isspeciﬁed. Typically, memory read and memory write operations are performed bythecentral processing unit (CPU).
Figure 2.1 Illustration of the main memory addressing16 INSTRUCTION SET ARCHITECTURE AND DESIGNThree basic steps are needed in order for the CPU to perform a write operation
into a speciﬁed memory location:
1. The word to be stored into the memory location is ﬁrst loaded by the CPU
into a speciﬁed register, called the memory data register (MDR ).
2. The address of the location into which the word is to be stored is loaded by
the CPU into a speciﬁed register, called the memory address register (MAR ).
3. A signal, called write , is issued by the CPU indicating that the word stored in
the MDR is to be stored in the memory location whose address in loaded in
the MAR.
Figure 2.2 illustrates the operation of writing the word given by 7E (in hex) into the
memory location whose address is 2005. Part aof the ﬁgure shows the status of the reg-
isters and memory locations involved in the write operation before the execution of theoperation. Part bof the ﬁgure shows the status after the execution of the operation.
It is worth mentioning that the MDR and the MAR are registers used exclusively
by the CPU and are not accessible to the programmer.
Similar to the write operation, three basic steps are needed in order to perform a
memory read operation:
1. The address of the location from which the word is to be read is loaded into
the MAR.
2. A signal, called read, is issued by the CPU indicating that the word whose
address is in the MAR is to be read into the MDR.
3. After some time, corresponding to the memory delay in reading the speciﬁed
word, the required word will be loaded by the memory into the MDR readyfor use by the CPU.
Before execution After execution
Figure 2.2 Illustration of the memory write operation2.1. MEMORY LOCATIONS AND OPERATIONS 17Figure 2.3 illustrates the operation of reading the word stored in the memory
location whose address is 2010. Part aof the ﬁgure shows the status of the registers
and memory locations involved in the read operation before the execution of the
operation. Part bof the ﬁgure shows the status after the read operation.
2.2. ADDRESSING MODES
Information involved in any operation performed by the CPU needs to be addressed.
In computer terminology, such information is called the operand . Therefore, any
instruction issued by the processor must carry at least two types of information.
These are the operation to be performed, encoded in what is called the op-code
ﬁeld, and the address information of the operand on which the operation is to beperformed, encoded in what is called the address ﬁeld.
Instructions can be classiﬁed based on the number of operands as: three-address ,
two-address ,one-and-half-address ,one-address , and zero-address . We explain
these classes together with simple examples in the following paragraphs. It should
be noted that in presenting these examples, we would use the convention operation ,
source ,destination to express any instruction. In that convention, operation rep-
resents the operation to be performed, for example, add,subtract ,write ,o rread.
The source ﬁeld represents the source operand(s). The source operand can be a con-
stant, a value stored in a register, or a value stored in the memory. The destinationﬁeld represents the place where the result of the operation is to be stored, forexample, a register or a memory location.
A three-address instruction takes the form operation add-1 ,add-2 ,add-3 . In this
form, each of add-1 ,add-2 , and add-3 refers to a register or to a memory location.
Consider, for example, the instruction ADD R
1,R2,R3. This instruction indicates thatFigure 2.3 Illustration of the memory read operation18 INSTRUCTION SET ARCHITECTURE AND DESIGNthe operation to be performed is addition . It also indicates that the values to be added
are those stored in registers R1andR2that the results should be stored in register R3.
An example of a three-address instruction that refers to memory locations may take
the form ADD A ,B,C. The instruction adds the contents of memory location Ato the
contents of memory location Band stores the result in memory location C.
A two-address instruction takes the form operation add-1 ,add-2 . In this form,
each of add-1 andadd-2 refers to a register or to a memory location. Consider,
for example, the instruction ADD R 1,R2. This instruction adds the contents of regis-
terR1to the contents of register R2and stores the results in register R2. The original
contents of register R2are lost due to this operation while the original contents of
register R1remain intact. This instruction is equivalent to a three-address instruction
of the form ADD R 1,R2,R2. A similar instruction that uses memory locations instead
of registers can take the form ADD A ,B. In this case, the contents of memory location
Aare added to the contents of memory location Band the result is used to override
the original contents of memory location B.
The operation performed by the three-address instruction ADD A ,B,Ccan be per-
formed by the two two-address instructions MOVE B ,CandADD A ,C. This is
because the ﬁrst instruction moves the contents of location Binto location Cand
the second instruction adds the contents of location Ato those of location C(the con-
tents of location B) and stores the result in location C.
A one-address instruction takes the form ADD R 1. In this case the instruction
implicitly refers to a register, called the Accumulator R acc, such that the contents
of the accumulator is added to the contents of the register R1and the results are
stored back into the accumulator Racc. If a memory location is used instead of a reg-
ister then an instruction of the form ADD B is used. In this case, the instruction
adds the content of the accumulator Raccto the content of memory location Band
stores the result back into the accumulator Racc. The instruction ADD R 1is equival-
ent to the three-address instruction ADD R 1,Racc,Raccor to the two-address instruc-
tionADD R 1,Racc.
Between the two- and the one-address instruction, there can be a one-and-half
address instruction. Consider, for example, the instruction ADD B ,R1. In this case,
the instruction adds the contents of register R1to the contents of memory location
Band stores the result in register R1. Owing to the fact that the instruction uses
two types of addressing, that is, a register and a memory location, it is called a
one-and-half-address instruction. This is because register addressing needs a smaller
number of bits than those needed by memory addressing.
It is interesting to indicate that there exist zero-address instructions. These are the
instructions that use stack operation . A stack is a data organization mechanism in
which the last data item stored is the ﬁrst data item retrieved. Two speciﬁc oper-
ations can be performed on a stack. These are the push and the popoperations.
Figure 2.4 illustrates these two operations.
As can be seen, a speciﬁc register, called the stack pointer (SP), is used to indicate
the stack location that can be addressed. In the stack push operation, the SP value is
used to indicate the location (called the top of the stack) in which the value (5A) is tobe stored (in this case it is location 1023). After storing (pushing) this value the SP is2.2. ADDRESSING MODES 19incremented to indicate to location 1024. In the stack pop operation, the SP is ﬁrst
decremented to become 1021. The value stored at this location (DD in this case) isretrieved (popped out) and stored in the shown register.
Different operations can be performed using the stack structure. Consider, for
example, an instruction such as ADD (SP)þ,(SP). The instruction adds the contents
of the stack location pointed to by the SP to those pointed to by the SP þ1 and stores
the result on the stack in the location pointed to by the current value of the SP.Figure 2.5 illustrates such an addition operation. Table 2.1 summarizes the instruc-tion classiﬁcation discussed above.
The different ways in which operands can be addressed are called the addressing
modes . Addressing modes differ in the way the address information of operands is
speciﬁed. The simplest addressing mode is to include the operand itself in theinstruction, that is, no address information is needed. This is called immediate
addressing . A more involved addressing mode is to compute the address of the
operand by adding a constant value to the content of a register. This is called indexed
addressing . Between these two addressing modes there exist a number of other
addressing modes including absolute addressing, direct addressing, and indirectaddressing. A number of different addressing modes are explained below.
Figure 2.4 The stack push and pop operations
-52
39
1050-13
39
1050SP 1000
1001
10021000
1001
1002SP
Figure 2.5 Addition using the stack20 INSTRUCTION SET ARCHITECTURE AND DESIGN2.2.1. Immediate Mode
According to this addressing mode, the value of the operand is (immediately) avail-
able in the instruction itself. Consider, for example, the case of loading the decimal
value 1000 into a register Ri. This operation can be performed using an instruction
such as the following: LOAD #1000, Ri. In this instruction, the operation to be per-
formed is to load a value into a register. The source operand is (immediately) givenas 1000, and the destination is the register R
i. It should be noted that in order to indi-
cate that the value 1000 mentioned in the instruction is the operand itself and notits address (immediate mode), it is customary to preﬁx the operand by the specialcharacter (#). As can be seen the use of the immediate addressing mode is simple.
The use of immediate addressing leads to poor programming practice. This is
because a change in the value of an operand requires a change in every instructionthat uses the immediate value of such an operand. A more ﬂexible addressing modeis explained below.
2.2.2. Direct (Absolute) Mode
According to this addressing mode, the address of the memory location that holds
the operand is included in the instruction. Consider, for example, the case of loading
the value of the operand stored in memory location 1000 into register R
i. This oper-
ation can be performed using an instruction such as LOAD 1000, Ri. In this instruc-
tion, the source operand is the value stored in the memory location whose address is
1000, and the destination is the register Ri. Note that the value 1000 is not preﬁxed
with any special characters, indicating that it is the (direct or absolute) address of thesource operand. Figure 2.6 shows an illustration of the direct addressing mode. ForTABLE 2.1 Instruction Classiﬁcation
Instruction class Example
Three-address ADD R 1,R2,R3
ADD A ,B,C
Two-address ADD R 1,R2
ADD A ,B
One-and-half-address ADD B ,R1
One-address ADD R 1
Zero-address ADD (SP)þ,(SP)
Memory
OperandOperation Address
Figure 2.6 Illustration of the direct addressing mode2.2. ADDRESSING MODES 21example, if the content of the memory location whose address is 1000 was ( 2345) at
the time when the instruction LOAD 1000, Riis executed, then the result of execut-
ing such instruction is to load the value ( 2345) into register Ri.
Direct (absolute) addressing mode provides more ﬂexibility compared to the
immediate mode. However, it requires the explicit inclusion of the operand address
in the instruction. A more ﬂexible addressing mechanism is provided through the use
of the indirect addressing mode. This is explained below.
2.2.3. Indirect Mode
In the indirect mode, what is included in the instruction is not the address of the
operand, but rather a name of a register or a memory location that holds the (effec-tive) address of the operand. In order to indicate the use of indirection in the instruc-tion, it is customary to include the name of the register or the memory location in
parentheses. Consider, for example, the instruction LOAD (1000), R
i. This instruc-
tion has the memory location 1000 enclosed in parentheses, thus indicating indirec-
tion. The meaning of this instruction is to load register Riwith the contents of the
memory location whose address is stored at memory address 1000. Because indirec-
tion can be made through either a register or a memory location, therefore, we can
identify two types of indirect addressing. These are register indirect addressing ,i fa
register is used to hold the address of the operand, and memory indirect addressing ,
if a memory location is used to hold the address of the operand. The two types are
illustrated in Figure 2.7.
Figure 2.7 Illustration of the indirect addressing mode22 INSTRUCTION SET ARCHITECTURE AND DESIGN2.2.4. Indexed Mode
In this addressing mode, the address of the operand is obtained by adding a con-
stant to the content of a register, called the index register . Consider, for example,
the instruction LOAD X (Rind),Ri. This instruction loads register Riwith the contents
of the memory location whose address is the sum of the contents of registerR
indand the value X. Index addressing is indicated in the instruction by including
the name of the index register in parentheses and using the symbol Xto indicate
the constant to be added. Figure 2.8 illustrates indexed addressing. As can beseen, indexing requires an additional level of complexity over register indirectaddressing.
2.2.5. Other Modes
The addressing modes presented above represent the most commonly used modes in
most processors. They provide the programmer with sufﬁcient means to handle most
general programming tasks. However, a number of other addressing modes have
been used in a number of processors to facilitate execution of speciﬁc programmingtasks. These additional addressing modes are more involved as compared to thosepresented above. Among these addressing modes the relative ,autoincrement , and
theautodecrement modes represent the most well-known ones. These are explained
below.
Relative Mode Recall that in indexed addressing, an index register, R
ind, is used.
Relative addressing is the same as indexed addressing except that the program
counter (PC) replaces the index register. For example, the instruction LOAD X (PC),
Riloads register Riwith the contents of the memory location whose address
is the sum of the contents of the program counter (PC) and the value X. Figure 2.9
illustrates the relative addressing mode.
Autoincrement Mode This addressing mode is similar to the register indirect
addressing mode in the sense that the effective address of the operand is the content
of a register, call it the autoincrement register , that is included in the instruction.
Memory
+Operation Value X
operand Index Register ( Rind)
Figure 2.8 Illustration of the indexed addressing mode2.2. ADDRESSING MODES 23However, with autoincrement, the content of the autoincrement register is automati-
cally incremented after accessing the operand. As before, indirection is indicated byincluding the autoincrement register in parentheses. The automatic increment of theregister’s content after accessing the operand is indicated by including a ( þ) after
the parentheses. Consider, for example, the instruction LOAD (R
auto)þ,Ri. This
instruction loads register Riwith the operand whose address is the content of register
Rauto. After loading the operand into register Ri, the content of register Rautois
incremented, pointing for example to the next item in a list of items. Figure 2.10illustrates the autoincrement addressing mode.Memory
+Operation Value X
operand Program Counter (PC)
Figure 2.9 Illustration of relative addressing mode
Figure 2.10 Illustration of the autoincrement addressing mode24 INSTRUCTION SET ARCHITECTURE AND DESIGNAutodecrement Mode Similar to the autoincrement, the autodecrement mode
uses a register to hold the address of the operand. However, in this case the
content of the autodecrement register is ﬁrst decremented and the new contentis used as the effective address of the operand. In order to reﬂect the fact that thecontent of the autodecrement register is decremented before accessing the operand,a(2) is included before the indirection parentheses. Consider, for example, the
instruction LOAD/C0(R
auto),Ri. This instruction decrements the content of the
register Rautoand then uses the new content as the effective address of the operand
that is to be loaded into register Ri. Figure 2.11 illustrates the autodecrement addres-
sing mode.
The seven addressing modes presented above are summarized in Table 2.2. In
each case, the table shows the name of the addressing mode, its deﬁnition, and a gen-eric example illustrating the use of such mode.
In presenting the different addressing modes we have used the load instruction
for illustration. However, it should be understood that there are other types of
instructions in a given machine. In the following section we elaborate on the differ-
ent types of instructions that typically constitute the instruction set of a givenmachine.
Figure 2.11 Illustration of the autodecrement addressing mode2.2. ADDRESSING MODES 252.3. INSTRUCTION TYPES
The type of instructions forming the instruction set of a machine is an indication of
the power of the underlying architecture of the machine. Instructions can in generalbe classiﬁed as in the following Subsections 2.3.1 to 2.3.4.
2.3.1. Data Movement Instructions
Data movement instructions are used to move data among the different units of the
machine. Most notably among these are instructions that are used to move data
among the different registers in the CPU. A simple register to register movementof data can be made through the instruction
MOVE R
i,RjTABLE 2.2 Summary of Addressing Modes
Addressing
mode Deﬁnition Example Operation
Immediate Value of operand is included in
the instructionload #1000, Ri Ri 1000
Direct
(Absolute)Address of operand is included
in the instructionload 1000, Ri Ri M[1000]
Register
indirectOperand is in a memory
location whose address is in
the register speciﬁed in the
instructionload (Rj),Ri Ri M[Rj]
Memory
indirectOperand is in a memory
location whose address is inthe memory location
speciﬁed in the instructionload (1000), R
i Ri M[1000]
Indexed Address of operand is the sum
of an index value and thecontents of an index registerload X (R
ind),Ri Ri M[RindþX]
Relative Address of operand is the sum
of an index value and the
contents of the program
counterload X (PC),Ri Ri M[PCþX]
Autoincrement Address of operand is in a
register whose value isincremented after fetching
the operandload (R
auto)þ,RiRi M[Rauto]
Rauto Rautoþ1
Autodecrement Address of operand is in a
register whose value isdecremented before fetchingthe operandload2(R
auto),RiRauto Rauto21
Ri M[Rauto]26 INSTRUCTION SET ARCHITECTURE AND DESIGNThis instruction moves the content of register Rito register Rj. The effect of the instruc-
tion is to override the contents of the (destination) register Rjwithout changing the con-
tents of the (source) register Ri. Data movement instructions include those used to
move data to (from) registers from (to) memory. These instructions are usually referred
to as the load andstore instructions, respectively. Examples of the two instructions are
LOAD 25838 ,Rj
STORE R i,1024
The ﬁrst instruction loads the content of the memory location whose address is 25838into the destination register R
j. The content of the memory location is unchanged by
executing the LOAD instruction. The STORE instruction stores the content of the
source register Riinto the memory location 1024. The content of the source register
is unchanged by executing the STORE instruction. Table 2.3 shows some common
data transfer operations and their meanings.
2.3.2. Arithmetic and Logical Instructions
Arithmetic and logical instructions are those used to perform arithmetic and logical
manipulation of registers and memory contents. Examples of arithmetic instructionsinclude the ADD andSUBTRACT instructions. These are
ADD R
1,R2,R0
SUBTRACT R 1,R2,R0
The ﬁrst instruction adds the contents of source registers R1andR2and stores the
result in destination register R0. The second instruction subtracts the contents of
the source registers R1andR2and stores the result in the destination register R0.
The contents of the source registers are unchanged by the ADD and the SUBTRACT
instructions. In addition to the ADD andSUBTRACT instructions, some machines
have MULTIPLY andDIVIDE instructions. These two instructions are expensive
to implement and could be substituted by the use of repeated addition or repeatedsubtraction. Therefore, most modern architectures do not have MULTIPLY orTABLE 2.3 Some Common Data Movement Operations
Data movement
operation Meaning
MOVE Move data (a word or a block) from a given source
(a register or a memory) to a given destination
LOAD Load data from memory to a register
STORE Store data into memory from a register
PUSH Store data from a register to stack
POP Retrieve data from stack into a register2.3. INSTRUCTION TYPES 27DIVIDE instructions on their instruction set. Table 2.4 shows some common arith-
metic operations and their meanings.
Logical instructions are used to perform logical operations such as AND ,OR,
SHIFT ,COMPARE , and ROTATE . As the names indicate, these instructions per-
form, respectively, and, or, shift, compare, and rotate operations on register or
memory contents. Table 2.5 presents a number of logical operations.
2.3.3. Sequencing Instructions
Control (sequencing) instructions are used to change the sequence in which
instructions are executed. They take the form of CONDITIONAL BRANCHING
(CONDITIONAL JUMP ),UNCONDITIONAL BRANCHING (JUMP ), or CALL
instructions. A common characteristic among these instructions is that their
execution changes the program counter ( PC) value. The change made in the PC
value can be unconditional, for example, in the unconditional branching or the
jump instructions. In this case, the earlier value of the PCis lost and execution of
the program starts at a new value speciﬁed by the instruction. Consider, for example,the instruction JUMP NEW -ADDRESS . Execution of this instruction will cause the
PCto be loaded with the memory location represented by NEW-ADDRESS
whereby the instruction stored at this new address is executed. On the other hand,TABLE 2.4 Some Common Arithmetic Operations
Arithmetic operations Meaning
ADD Perform the arithmetic sum of two operands
SUBTRACT Perform the arithmetic difference of two operands
MULTIPLY Perform the product of two operands
DIVIDE Perform the division of two operands
INCREMENT Add one to the contents of a register
DECREMENT Subtract one from the contents of a register
TABLE 2.5 Some Common Logical Operations
Logical operation Meaning
AND Perform the logical ANDing of two operands
OR Perform the logical ORing of two operands
EXOR Perform the XORing of two operands
NOT Perform the complement of an operand
COMPARE Perform logical comparison of two operands and
set ﬂag accordingly
SHIFT Perform logical shift (right or left) of the content
of a register
ROTATE Perform logical shift (right or left) with
wraparound of the content of a register28 INSTRUCTION SET ARCHITECTURE AND DESIGNthe change made in the PCby the branching instruction can be conditional based on
the value of a speciﬁc ﬂag. Examples of these ﬂags include the Negative (N),Zero
(Z),Overﬂow (V), and Carry (C). These ﬂags represent the individual bits of a
speciﬁc register, called the CONDITION CODE (CC)REGISTER . The values of
ﬂags are set based on the results of executing different instructions. The meaning
of each of these ﬂags is shown in Table 2.6.
Consider, for example, the following group of instructions.
LOAD #100, R1
Loop :ADD (R2)þ,R0
DECREMENT R 1
BRANCH -IF-GREATER -THAN Loop
The fourth instruction is a conditional branch instruction, which indicates that if
the result of decrementing the contents of register R1is greater than zero, that is,
if the Zﬂag is not set, then the next instruction to be executed is that labeled by
Loop. It should be noted that conditional branch instructions could be used to exe-
cute program loops (as shown above).
TheCALL instructions are used to cause execution of the program to transfer to a
subroutine. A CALL instruction has the same effect as that of the JUMP in terms of
loading the PCwith a new value from which the next instruction is to be executed.
However, with the CALL instruction the incremented value of the PC(to point to the
next instruction in sequence) is pushed onto the stack. Execution of a RETURN
instruction in the subroutine will load the PCwith the popped value from the
stack. This has the effect of resuming program execution from the point wherebranching to the subroutine has occurred.
Figure 2.12 shows a program segment that uses the CALL instruction. This pro-
gram segment sums up a number of values, N, and stores the result into memory
location SUM . The values to be added are stored in Nconsecutive memory locations
starting at NUM . The subroutine, called ADDITION , is used to perform the actual
addition of values while the main program stores the results in SUM .
Table 2.7 presents some common transfer of control operations.TABLE 2.6 Examples of Condition Flags
Flag name Meaning
Negative (N) Set to 1 if the result of the most recent operation
is negative, it is 0 otherwise
Zero (Z) Set to 1 if the result of the most recent operation
is 0, it is 0 otherwise
Overﬂow (V) Set to 1 if the result of the most recent operation
causes an overﬂow, it is 0 otherwise
Carry (C) Set to 1 if the most recent operation results in a
carry, it is 0 otherwise2.3. INSTRUCTION TYPES 292.3.4. Input /Output Instructions
Input and output instructions (I /O instructions) are used to transfer data between the
computer and peripheral devices. The two basic I /O instructions used are the INPUT
andOUTPUT instructions. The INPUT instruction is used to transfer data from an
input device to the processor. Examples of input devices include a keyboard or a
mouse . Input devices are interfaced with a computer through dedicated input
ports . Computers can use dedicated addresses to address these ports. Suppose that
the input port through which a keyboard is connected to a computer carries the
unique address 1000. Therefore, execution of the instruction INPUT 1000 will
cause the data stored in a speciﬁc register in the interface between the keyboardand the computer, call it the input data register , to be moved into a speciﬁc register
(called the accumulator) in the computer. Similarly, the execution of the instructionOUTPUT 2000 causes the data stored in the accumulator to be moved to the data
output register in the output device whose address is 2000. Alternatively, the com-
puter can address these ports in the usual way of addressing memory locations. In
this case, the computer can input data from an input device by executing an instruc-tion such as MOVE R
in,R0. This instruction moves the content of the register Rin
into the register R0. Similarly, the instruction MOVE R 0,Rinmoves the contents
of register R0into the register Rin, that is, performs an output operation. ThisCLEAR R0
MOVE N,R1
MOVE   # NUM ,R2
CALL SUBROUTINE ADDITION 
MOVE R0,SUM
SUBROUTINE ADDITION
Loop:ADD (R2)+,R0
DEC R1
BRANCH-IF-GREATER  Loop 
RETURN ADDITION
Figure 2.12 A program segment using a subroutine
TABLE 2.7 Some Transfer of Control Operations
Transfer of control operation Meaning
BRANCH-IF-CONDITION Transfer of control to a new address if condition is true
JUMP Unconditional transfer of controlCALL Transfer of control to a subroutineRETURN Transfer of control to the caller routine30
INSTRUCTION SET ARCHITECTURE AND DESIGNlatter scheme is called memory-mapped Input /Output . Among the advantages of
memory-mapped I /O is the ability to execute a number of memory-dedicated
instructions on the registers in the I /O devices in addition to the elimination of
the need for dedicated I /O instructions. Its main disadvantage is the need to dedicate
part of the memory address space for I /O devices.
2.4. PROGRAMMING EXAMPLES
Having introduced addressing modes and instruction types, we now move on to
illustrate the use of these concepts through a number of programming examples.
In presenting these examples, generic mnemonics will be used. This is done in
order to emphasize the understanding of how to use different addressing modes inperforming different operations independent of the machine used. Applications ofsimilar principles using real-life machine examples are presented in Chapter 3.
Example 1 In this example, we would like to show a program segment that can be
used to perform the task of adding 100 numbers stored at consecutive memory loca-
tions starting at location 1000. The results should be stored in memory location 2000.
CLEAR R
0; R0 0
MOVE #100,R1; R1 100
CLEAR R 2; R2 0
LOOP :ADD 1000 (R2),R0; R0 R0þM(1000þR2)
INCREMENT R 2; R2 R2þ1
DECREMENT R 1; R1 R1/C01
BRANCH -IF.0L O O P ;GO TO LOOP if contents of R 1.0
STORE R 0,2000 ; M(2000 ) R0
In this example, use has been made of immediate ( MOVE #100,R1)a n di n d e x e d( ADD
1000(R 2),R0) addressing.
Example 2 In this example autoincrement addressing will be used to perform the
same task performed in Example 1.
CLEAR R 0; R0 0
MOVE #100,R1; R1 100
CLEAR R 2; R2 0
LOOP :ADD 1000( R2)þ,R0; R0 R0þM(1000þR2)&R2 R2þ1
DECREMENT R 1; R1 R1/C01
BRANCH -IF.0 LOOP ;GO TO LOOP if contents of R 1.0
STORE R 0,2000 ; M(2000 ) R0
As can be seen, a given task can be performed using more than one programmingmethodology. The method used by the programmer depends on his /her experience2.4. PROGRAMMING EXAMPLES 31as well as the richness of the instruction set of the machine used. Note also that the
use of the autoincrement addressing in Example 2 has led to a decrease in thenumber of instructions used to perform the same task.
Example 3 This example illustrates the use of a subroutine, SORT , to sort
Nvalues in ascending order (Fig. 2.13). The numbers are originally stored in a
list starting at location 1000. The sorted values are also stored in the same list
and again starting at location 1000. The subroutine sorts the data using the well-known “Bubble Sort” technique. The content of register R
3is checked at the end
of every loop to ﬁnd out whether the list is sorted or not.
Example 4 This example illustrates the use of a subroutine, SEARCH , to search
for a value VAL in a list of Nvalues (Fig. 2.14). We assume that the list is not orig-
inally sorted and therefore a brute force search is used. In this search, the value VAL
is compared with every element in the list from top to bottom. The content of register
R3is used to indicate whether VAL was found. The ﬁrst element of the list is located
at address 1000.
Example 5 This example illustrates the use of a subroutine, SEARCH , to search
for a value VAL in a list of Nvalues (as in Example 4) (Fig. 2.15). Here, we make use
of the stack to send the parameters VAL andN.
Figure 2.13 SORT subroutine32 INSTRUCTION SET ARCHITECTURE AND DESIGN2.5. SUMMARY
In this chapter we considered the main issues relating to instruction set design and
characteristics. We presented a model of the main memory in which the memory isabstracted as a sequence of cells, each capable of storing nbits. A number of addressing
modes were presented. Thes e include immediate, direct, indirect, indexed, autoincre-
ment, and autodecrement. Examples showing how to use these addressing modeswere then presented. We also presente d a discussion on instruction types, which include
data movement, arithmetic /logical, instruction sequencing, and Input /Output. Our dis-
cussion concluded with a presentation of a number of examples showing how to use the
principles and concepts discussed in the chapter in programming the solution of anumber of sample problems. In the next chapter, we will introduce the concepts involvedin programming the solution of real-life problems using assembly language.
Figure 2.14 SEARCH subroutine
Figure 2.15 Subroutine SEARCH using stack to send parameters VAL andN2.5. SUMMARY 33EXERCISES
1. Write a program using the addressing modes and the instruction types pre-
sented in Sections 2.2 and 2.3 to reverse the bits stored in a 16-bit register R0.
2. Consider a computer that has a number of registers such that the three reg-
isters R0¼1500, R1¼4500, and R2¼1000. Show the effective address
of memory and the registers’ contents in each of the following instructions
(assume that all numbers are decimal).
(a)ADD (R0)þ,R2
(b)SUBTRACT 2(R1),R2
(c)MOVE 500(R0),R2
(d)LOAD #5000, R2
(e)STORE R 0, 100( R2)
3. Assume that the top of the stack in a program is pointed to by the register SP.
You are required to write program segments to perform each of the followingtasks (assume that only the following addressing modes are available:indexed, autoincrement, and autodecrement).
(a) Pop the top three elements of the stack, add them, and push the result
back onto the stack.
(b) Pop the top two elements of the stack, subtract them, and push the results
back onto the stack.
(c) Push ﬁve elements (one at a time) onto the stack.
(d) Remove the top ﬁve elements from the top of the stack.
(e) Copy the third element from the top of the stack into register R
0.
4. You are required to write a program segment that can perform the operation
C AþBwhere each of AandBrepresents a set of 100 memory locations
each storing a value such that the set of values represented by Aare stored
starting at memory location 1000 and those represented by Bare stored start-
ing at memory location 2000. The results should be stored starting at memorylocation 3000. The above operation is to be performed using each of thefollowing instruction classes.
(a) A machine with one-address instructions
(b) A machine with one-and-half instructions
(c) A machine with two-address instructions
(d) A machine with three-address instructions
(e) A machine with zero-address instructions
5. Write program segments that perform the operation C CþA/C2Busing
each of the instruction classes indicated in Exercise 4 above. Assume that
A,B, and Care memory addresses.
6. Assume that a series of ﬁve tests has been offered to a class consisting of 50
students. The score obtained by students in each of the ﬁve tests are stored
sequentially in memory locations starting respectively at memory locations1000, 2000, 3000, 4000, and 5000. You are required to write a program34 INSTRUCTION SET ARCHITECTURE AND DESIGNthat calculates the average score obtained by each student in the ﬁve tests and
store the same in memory locations starting at memory location 6000. Eachstudent is identiﬁed by his /her student ID. You may assume that students’
IDs are sequential.
7. Repeat Exercise 6 above assuming that the memory used is byte addressable
while each score occupies 32-bit.
8. Rewrite the same program as in Exercise 6 above assuming that the students’
IDs are not sequential, that is, each student ID is to be used as a pointer tohis/her test scores.
9. Repeat Exercise 6 above assuming that the students scores are stored in an
array S(50,5), that is, each row holds the scores obtained by a student
(each score in a column of the same row) and that the ﬁrst element of thearray, that is, S (0,0) is stored in memory location 4000. The scores arestored rowwise, that is, one row after the other. The average score obtainedby each student is to be stored at a memory location pointed to by his /her ID.
10. Repeat Exercise 9 above assuming that your job is to write a subroutine
to perform the same task as in Exercise 9. Assume that the number of
students, the number of tests, and the location of the ﬁrst element in thearray are to be passed to the subroutine as parameters in registers R
1,R2,
andR3, respectively.
REFERENCES AND FURTHER READING
C. M. Gilmore, Microprocessors :Principles and Applications , 2nd ed., McGraw-Hill,
New York, 1996.
V. C. Hamacher, Z. G. Vranesic, and S. G. Zaky, Computer Organization , 5th ed.,
McGraw-Hill, New York, 2002
A. D. Patterson, J. L. Hennessy, Computer Organization & Design ;The Hardware /Software
Interface , Morgan Kaufmann, San Mateo, CA, 1994
B. Wilkinson, Computer Architecture :Design and Performance , 2nd ed., Prentice-Hall,
Hertfordshire, UK, 1996.REFERENCES AND FURTHER READING 35& CHAPTER 3
Assembly Language Programming
In Chapter 2 we introduced the basic concepts and principles involved in the design
of an instruction set of a machine. This chapter considers the issues related to assem-bly language programming. Although high-level languages and compiler technol-ogy have witnessed great advances over the years, assembly language remainsnecessary in some cases. Programming in assembly can result in machine codethat is much smaller and much faster than that generated by a compiler of a high-level language. Small and fast code could be critical in some embedded and portableapplications, where resources may be very limited. In such cases, small portions of
the program that may be heavily used can be written in assembly language. For the
reader of this book, learning assembly languages and writing assembly code can beextremely helpful in understanding computer organization and architecture.
A computer program can be represented at different levels of abstraction. A pro-
gram could be written in a machine-independent, high-level language such as Javaor Cþþ. A computer can execute programs only when they are represented in
machine language speciﬁc to its architecture. A machine language program for agiven architecture is a collection of machine instructions represented in binary
form. Programs written at any level higher than the machine language must be trans-
lated to the binary representation before a computer can execute them. An assemblylanguage program is a symbolic representation of the machine language program.Machine language is pure binary code, whereas assembly language is a direct map-ping of the binary code onto a symbolic form that is easier for humans to understandand manage. Converting the symbolic representation into machine language is per-formed by a special program called the assembler. An assembler is a program thataccepts a symbolic language program (source) and produces its machine language
equivalent (target). In translating a program into binary code, the assembler will
replace symbolic addresses by numeric addresses, replace symbolic operationcodes by machine operation codes, reserve storage for instructions and data, andtranslate constants into machine representation.
The purpose of this chapter is to give the reader a general overview of assembly
language and its programming. It is not meant to be a manual of the assemblylanguage for any speciﬁc architecture. We start the chapter with a discussion of a
37Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.simple hypothetical machine that will be referred to throughout the chapter. The
machine has only ﬁve registers and its instruction set has only 10 instructions.We will use this simple machine to deﬁne a rather simple assembly language thatwill be easy to understand and will help explain the main issues in assembly pro-gramming. We will introduce instruction mnemonics and the syntax and assemblerdirectives and commands. A discussion on the execution of assembly programs isthen presented. We conclude the chapter by showing a real-world example of the
assembly language for the X86 Intel CISC family.
3.1. A SIMPLE MACHINE
Machine language is the native language of a given processor. Since assembly
language is the symbolic form of machine language, each different type of processorhas its own unique assembly language. Before we study the assembly language of agiven processor, we need ﬁrst to understand the details of that processor. We need toknow the memory size and organization, the processor registers, the instruction
format, and the entire instruction set. In this section, we present a very simple
hypothetical processor, which will be used in explaining the different topics inassembly language throughout the chapter.
Our simple machine is an accumulator-based processor, which has ﬁve 16-bit
registers: Program Counter (PC), Instruction Register (IR), Address Register(AR), Accumulator (AC), and Data Register (DR). The PC contains the addressof the next instruction to be executed. The IR contains the operation code portionof the instruction being executed. The AR contains the address portion (if any) of
the instruction being executed. The AC serves as the implicit source and destination
of data. The DR is used to hold data. The memory unit is made up of 4096 words ofstorage. The word size is 16 bits. The processor is shown in Figure 3.1.
CPUALU
Figure 3.1 A simple machine38 ASSEMBLY LANGUAGE PROGRAMMINGWe assume that our simple processor supports three types of instructions: data
transfer, data processing, and program control. The data transfer operations are
load, store, and move data between the registers AC and DR. The data processinginstructions are add, subtract, and, and not. The program control instructions arejump and conditional jump. The instruction set of our processor is summarized inTable 3.1. The instruction size is 16 bits, 4 bits for the operation code and 12 bitsfor the address (when appropriate).
Example 1 Let us write a machine language program that adds the contents of
memory location 12 (00C-hex), initialized to 350 and memory location 14 (00E-hex),
initialized to 96, and store the result in location 16 (010-hex), initialized to 0.
The program is given in binary instructions in Table 3.2. The ﬁrst column gives
the memory location in binary for each instruction and operand. The second columnTABLE 3.1 Instruction Set of the Simple Processor
Operation code Operand Meaning of instruction
0000 Stop execution
0001 adr Load operand from memory (location adr) into AC
0010 adr Store contents of AC in memory (location adr)
0011 Copy the contents AC to DR
0100 Copy the contents of DR to AC
0101 Add DR to AC
0110 Subtract DR from AC
0111 And bitwise DR to AC
1000 Complement contents of AC
1001 adr Jump to instruction with address adr
1010 adr Jump to instruction adr if AC ¼0
TABLE 3.2 Simple Machine Language Program in Binary (Example 1)
Memory location
(bytes) Binary instruction Description
0000 0000 0000 0001 0000 0000 1100 Load the contents of location 12 in AC
0000 0000 0010 0011 0000 0000 0000 Move contents of AC to DR
0000 0000 0100 0001 0000 0000 1110 Load the contents of location 14 into AC
0000 0000 0110 0101 0000 0000 0000 Add DR to AC
0000 0000 1000 0010 0000 0001 0000 Store contents of AC in location 16
0000 0000 1010 0000 0000 0000 0000 Stop0000 0000 1100 0000 0001 0101 1110 Data value 350
0000 0000 1110 0000 0000 0110 0000 Data value is 96
0000 0001 0000 0000 0000 0000 0000 Data value is 03.1. A SIMPLE MACHINE 39lists the contents of the memory locations. For example, the contents of location 0 is
an instruction with opcode: 0001, and operand address: 0000 0000 1100. Please note
that in the case of operations that do not require operand, the operand portion of the
instruction is shown as zeros. The program is expected to be stored in the indicatedmemory locations starting at location 0 during execution. If the program will bestored at different memory locations, the addresses in some of the instructionsneed to be updated to reﬂect the new locations.
It is clear that programs written in binary code are very difﬁcult to understand
and, of course, to debug. Representing the instructions in hexadecimal will reduce
the number of digits to only four per instruction. Table 3.3 shows the same program
in hexadecimal.
3.2. INSTRUCTION MNEMONICS AND SYNTAX
Assembly language is the symbolic form of machine language. Assembly programs
are written with short abbreviations called mnemonics. A mnemonic is an abbrevi-ation that represents the actual machine instruction. Assembly language program-ming is the writing of machine instructions in mnemonic form, where each
machine instruction (binary or hex value) is replaced by a mnemonic. Clearly the
use of mnemonics is more meaningful than that of hex or binary values, whichwould make programming at this low level easier and more manageable.
An assembly program consists of a sequence of assembly statements, where
statements are written one per line. Each line of an assembly program is split intothe following four ﬁelds: label, operation code (opcode), operand, and comments.Figure 3.2 shows the four-column format of an assembly instruction.
Labels are used to provide symbolic names for memory addresses. A label is an
identiﬁer that can be used on a program line in order to branch to the labeled line. Itcan also be used to access data using symbolic names. The maximum length of aTABLE 3.3 Simple Machine Language Program in
Hexadecimal (Example 1)
Memory location
(bytes) Hex instruction
000 100C
002 3000
004 100E
006 5000
008 2010
00A 0000
00C 015E
00E 0060010 000040
ASSEMBLY LANGUAGE PROGRAMMINGlabel differs from one assembly language to another. Some allow up to 32 characters
in length, others may be restricted to six characters. Assembly languages for someprocessors require a colon after each label while others do not. For example, SPARCassembly requires a colon after every label, but Motorola assembly does not. TheIntel assembly requires colons after code labels but not after data labels.
The operation code (opcode) ﬁeld contains the symbolic abbreviation of a given
operation. The operand ﬁeld consists of additional information or data that the
opcode requires. The operand ﬁeld may be used to specify constant, label, immedi-
ate data, register, or an address. The comments ﬁeld provides a space for documen-tation to explain what has been done for the purpose of debugging and maintenance.
For the simple processor described in the previous section, we assume that the
label ﬁeld, which may be empty, can be of up to six characters. There is no colonrequirement after each label. Comments will be preceded by “ /”. The simple
mnemonics of the ten binary instructions of Table 3.1 are summarized in Table 3.4.
Let us consider the following assembly instruction:
START LD X \ copy the contents of location X into AC
The label of the instruction LD X isSTART , which means that it is the memory
address of this instruction. That label can be used in a program as a reference as
shown in the following instruction:
BRA START \ go to the statement with label STARTLabel
(Optional)Operation Code
(Required)Operand
(Required in some
instructions)Comment
(Optional)
Figure 3.2 Assembly instruction format
TABLE 3.4 Assembly Language for the Simple Processor
Mnemonic Operand Meaning of instruction
STOP Stop execution
LD x Load operand from memory (location x) into AC
ST x Store contents of AC in memory (location x)
MOVAC Copy the contents AC to DRMOV Copy the contents of DR to AC
ADD Add DR to AC
SUB Subtract DR from ACAND And bitwise DR to AC
NOT Complement contents of AC
BRA adr Jump to instruction with address adr
BZ adr Jump to instruction adrif AC¼03.2. INSTRUCTION MNEMONICS AND SYNTAX 41The jump instruction will make the processor jump to the memory address associ-
ated with the label START, thus executing the instruction LD X immediatelyafter the BRA instruction.
In addition to program instructions, an assembly program may also include
pseudo instructions or assembler directives. Assembler directives are commandsthat are understood by the assembler and do not correspond to actual machineinstructions. For example, the assembler can be asked to allocate memory storage.
In our assembly language for the simple processor, we assume that we can use
the pseudo instruction W to reserve a word (16 bits) in memory. For example,the following pseudo instruction reserves a word for the label X and initializingit to the decimal value 350:
X W 350 \ reserve a word initialized to 350Again, the label of the pseudo instruction W 350 is X, which means it is the memory
address of this value. The following is the assembly code of the machine language
program of Example 1 in the previous section.
LD X \ AC  X
MOVAC \ DR  AC
LD Y \ AC  Y
ADD \ AC  ACþDR
ST Z \ Z  AC
STOP
X W 350 \ reserve a word initialized to 350Y W 96 \ reserve a word initialized to 96Z W 0 \ result stored here
Example 2 In this example, we will write an assembly program to perform
the multiplication operation: Z  X
/C3Y, where X, Y, and Z are memory
locations.
As you know, the assembly of the simple CPU does not have a multiplication
operation. We will compute the product by applying the add operation multiple
times. In order to add Y to itself X times, we will use N as a counter that is initialized
to X and decremented by one after each addition step. The BZ instruction will beused to test for the case when N reaches 0. We will use a memory location tostore N but it will need to be loaded into AC before the BZ instruction is executed.
We will also use a memory location ONE to store the constant 1. Memory location Z
will have the partial products and eventually the ﬁnal result.
The following is the assembly program using the assembly language of
our simple processor. We will assume that the values of X and Y are
small enough to allow their product to be stored in a single word. For thesake of this example, let us assume that X and Y are initialized to 5 and 15,respectively.42 ASSEMBLY LANGUAGE PROGRAMMINGLD X \ Load X in AC
ST N \ Store AC (X original value) in N
LOOP LD N \ AC  N
BZ EXIT \ Go to EXIT if AC ¼0 (N reached 0)
LD ONE \ AC  1
MOVAC \ DR  AC
LD N \ AC  N
SUB \ subtract 1 from N
ST N \ store decrements N
LD Y \ AC  Y
MOVAC \ DR  AC
LD Z \ AC  Z (partial product)
ADD \ Add Y to Z
ST Z \ store the new value of ZBRA LOOP
EXIT STOPX W 5 \ reserve a word initialized to 5
Y W 15 \ reserve a word initialized to 15
Z W 0 \ reserve a word initialized to 0ONE W 1 \ reserve a word initialized to 1
N W 0 \ reserve a word initialized to 0
3.3. ASSEMBLER DIRECTIVES AND COMMANDS
In the previous section, we introduced the reader to assembly and machine languages.
We provided several assembly code segments written using our simple machine
model. In writing assembly language programs for a speciﬁc architecture, a numberof practical issues need to be considered. Among these issues are the following:
.Assembler directives
.Use of symbols
.Use of synthetic operations
.Assembler syntax
.Interaction with the operating system
The use of assembler directives , also called pseudo-operations , is an important issue
in writing assembly language programs. Assembler directives are commands that
are understood by the assembler and do not correspond to actual machine instruc-tions. Assembler directives affect the way the assembler performs the conversionof assembly code to machine code. For example, special assembler directives canbe used to instruct the assembler to place data items such that they have proper align-
ment. Alignment of data in memory is required for efﬁcient implementation of archi-
tectures. For proper alignment of data, data of n-bytes width must be stored at an3.3. ASSEMBLER DIRECTIVES AND COMMANDS 43address that is divisible by n, for example, a word that has a two-byte width has to be
stored at locations having addresses divisible by two.
In assembly language programs symbols are used to represent numbers, for
example, immediate data. This is done to make the code easier to read, understand,
and debug. Symbols are translated to their corresponding numerical values by theassembler.
The use of synthetic operations helps assembly programmers to use instructions
that are not directly supported by the architecture. These are then translated by theassembler to a set of instructions deﬁned by the architecture. For example, assem-blers can allow the use of (a synthetic) increment instruction on architectures forwhich an increment instruction is not deﬁned through the use of some other instruc-tions such as the add instruction.
Assemblers usually impose some conventions in referring to hardware com-
ponents such as registers and memory locations. One such convention is the preﬁx-ing of immediate values with the special characters (#) or a register name with the
character (%).
The underlying hardware in some machines cannot be accessed directly by a pro-
gram. The operating system (OS) plays the role of mediating access to resources
such as memory and I /O facilities. Interactions with operating systems (OS) can
take place in the form of a code that causes the execution of a function that ispart of the OS. These functions are called system calls.
3.4. ASSEMBLY AND EXECUTION OF PROGRAMS
As you know by now, a program written in assembly language needs to be trans-
lated into binary machine language before it can be executed. In this section, wewill learn how to get from the point of writing an assembly program to theexecution phase. Figure 3.3 shows three steps in the assembly and execution pro-cess. The assembler reads the source program in assembly language and generatesthe object program in binary form. The object program is passed to the linker. The
linker will check the object ﬁle for calls to procedures in the link library. The linker
will combine the required procedures from the link library with the object programand produce the executable program. The loader loads the executable program intomemory and branches the CPU to the starting address. The program beginsexecution.
Figure 3.3 Assembly and execution process44 ASSEMBLY LANGUAGE PROGRAMMING3.4.1. Assemblers
Assemblers are programs that generate machine code instructions from a source
code program written in assembly language. The assembler will replace symbolic
addresses by numeric addresses, replace symbolic operation codes by machine oper-
ation codes, reserve storage for instructions and data, and translate constants intomachine representation.
The functions of the assembler can be performed by scanning the assembly pro-
gram and mapping its instructions to their machine code equivalent. Since symbolscan be used in instructions before they are deﬁned in later ones, a single scanning ofthe program might not be enough to perform the mapping. A simple assembler scansthe entire assembly program twice, where each scan is called a pass. During the ﬁrst
pass, it generates a table that includes all symbols and their binary values. This table
is called the symbol table. During the second pass, the assembler will use the symboltable and other tables to generate the object program, and output some informationthat will be needed by the linker.
3.4.2. Data Structures
The assembler uses at least three tables to perform its functions: symbol table,
opcode table, and pseudo instruction table. The symbol table, which is generatedin pass one, has an entry for every symbol in the program. Associated with each
symbol are its binary value and other information. Table 3.5 shows the symbol
table for the multiplication program segment of Example 2. We assume that the
instruction LD X is starting at location 0 in the memory. Since each instructiontakes two bytes, the value of the symbol LOOP is 4 (004 in hexadecimal).Symbol N, for example, will be stored at decimal location 40 (028 in hexadecimal).The values of the other symbols can be obtained in a similar way.
The opcode table provides information about the operation codes. Associated
with each symbolic opcode in the table are its numerical value and other information
about its type, its instruction length, and its operands. Table 3.6 shows the opcode
TABLE 3.5 Symbol Table for the Multiplication
Segment (Example 2)
SymbolValue
(hexadecimal)Other
information
Loop 004
EXIT 01E
X 020
Y 022Z 024
ONE 026
N 0283.4. ASSEMBLY AND EXECUTION OF PROGRAMS 45table for the simple processor described in Section 3.1. As an example, we explain
the information associated with the opcode LD. It has one operand, which is a
memory address and its binary value is 0001. The instruction length of LD is
2 bytes and its type is memory-reference.
The entries of the pseudo instruction table are the pseudo instructions symbols.
Each entry refers the assembler to a procedure that processes the pseudo instructionwhen encountered in the program. For example, if END is encountered, the trans-lation process is terminated.
In order to keep track of the instruction locations, the assembler maintains a vari-
able called instruction location counter (ILC). The ILC contains the value of
memory location assigned to the instruction or operand being processed. The ILC
is initialized to 0 and is incremented after processing each instruction. The ILC isincremented by the length of the instruction being processed, or the number ofbytes allocated as a result of a data allocation pseudo instruction.
Figures 3.4 and 3.5 show simpliﬁed ﬂowcharts of pass one and pass two in a two-
pass assembler. Remember that the main function of pass one is to build the symboltable while pass two’s main function is to generate the object code.
3.4.3. Linker and Loader
The linker is the entity that can combine object modules that may have resulted from
assembling multiple assembly modules separately. The loader is the operatingsystem utility that reads the executable into memory and start execution.
In summary, after assembly modules are translated into object modules, the func-
tions of the linker and loader prepare the program for execution. These functions
include combining object modules together, resolving addresses unknown at assem-
bly time, allocating storage, and ﬁnally executing the program.TABLE 3.6 The Opcode Table for the Assembly of our Simple Processor
Opcode OperandOpcode value
(binary)Instruction length
(bytes) Instruction type
STOP — 0000 2 Control
LD Mem-adr 0001 2 Memory-reference
ST Mem-adr 0010 2 Memory-reference
MOVAC — 0011 2 Register-reference
MOV — 0100 2 Register-reference
ADD — 0101 2 Register-reference
SUB — 0110 2 Register-reference
AND — 0111 2 Register-reference
NOT — 1000 2 Register-reference
BRA Mem-adr 1001 2 Control
BZ Mem-adr 1010 2 Control46 ASSEMBLY LANGUAGE PROGRAMMING3.5. EXAMPLE: THE X86 FAMILY
In this section, we discuss the assembly language features and use of the X86 family.
We present the basic organizational features of the system, the basic programming
model, the addressing modes, sample of the different instruction types used, andStart
ENDProcess next instruction
Stop
NoYes
Opcode Lookup
Symbol Table Lookup Generate machine code
Figure 3.5 Simpliﬁed pass two in a two-pass assemblerStart
END
LabelILC← 0
Process next instruction
Increment ILCAdd to Symbol Table
with value = ILC Pass 2
No
NoYes
Yes
Figure 3.4 Simpliﬁed pass one in a two-pass assembler3.5. EXAMPLE: THE X86FAMILY 47ﬁnally examples showing how to use the assembly language of the system in
programming sample real-life problems.
In the late 1970s, Intel introduced the 8086 as its ﬁrst 16-bit microprocessor.
This processor has a 16-bit external bus. The 8086 evolved into a series of fasterand more powerful processors starting with the 80286 and ending with the Pentium.
The latter was introduced in 1993. This Intel family of processors is usually calledtheX86family. Table 3.7 summarizes the main features of the main members of
such a family.
The Intel Pentium processor has about three million transistors and its compu-
tational power ranges between two and ﬁve times that of its predecessor processor,the 80486. A number of new features were introduced in the Pentium processor,
among which is the incorporation of a dual-pipelined superscalar architecturecapable of processing more than one instruction per clock cycle.
The basic programming model of the 386, 486, and the Pentium is shown in
Figure 3.6. It consists of three register groups. These are the general purpose regis-
ters, the segment registers, and the instruction pointer (program counter) and the ﬂag
register. The ﬁrst set consists of general purpose registers A, B, C, D, SI (sourceindex), DI (destination index), SP (stack pointer), and BP (base pointer). It shouldbe noted that in naming these registers, we used Xto indicate eXtended. The
second set of registers consists of CS (code segment), SS (stack segment), andfour data segment registers DS, ES, FS, and GS. The third set of registers consistsof the instruction pointer (program counter) and the ﬂags (status) register. The
latter is shown in Figure 3.7. Among the status bits shown in Figure 3.7, the ﬁrst
ﬁve are identical to those bits introduced as early as in the 8085 8-bit microproces-
sor. The next 6–11 bits are identical to those introduced in the 8086. The ﬂags in thebits 12–14 were introduced in the 80286 while the 16–17 bits were introduced inthe 80386. The ﬂag in bit 18 was introduced in the 80486. Table 3.8 shows the mean-ing of those ﬂags.
In the X86family an instruction can perform an operation on one or two oper-
ands. In two-operand instructions, the second operand can be immediate data in
TABLE 3.7 Main Features of the Intel X86Microprocessor Family
Feature 8086 286 386 486 Pentium
Date introduced 1978 1982 1985 1991 1993
Data bus 8 bits 16 bits 32 bits 32 bits 64 bitsAddress bus 20 bits 24 bits 32 bits 32 bits 32 bitsOperating speed 5,8,10 MHz 6,8,10, 12.5,
16, 20 MHz16, 20,25, 33,
40, 50 MHz25, 33,
50 MHz50, 60, 66,
100 MHz
Instruction cache
sizeNA NA 16 bytes 32 bytes 8 Kbytes
Data cache size NA NA 256 bytes 8 Kbytes 8 KbytesPhysical memory 1 Mbytes 16 Mbytes 4 Gbytes 4 Gbytes 4 GbytesData word size 16 bits 16 bits 16 bits 32 bits 32 bits48 ASSEMBLY LANGUAGE PROGRAMMINGFigure 3.6 The base register sets of the X86programming model
31 18 17 16 15 14 13 12 11 10 9 8 1
Reserved AG VM RF 0 NT IOPL6543 2 0
ODI T S Z A P C7
Figure 3.7 TheX86ﬂag register
TABLE 3.8 X86Status Flags
Flag Meaning Processor Flag Meaning Processor
C Carry All P Parity All
A Auxiliary All Z Zero All
S Sign All T Trap All
I Interrupt All D Direction All
O Overﬂow All IOPL I /O privilege level 286
NT Nested task 286 RF Resume 386
VM Virtual mode 386 AC Alignment check 4863.5. EXAMPLE: THE X86FAMILY 492’s complement format. Data transfer, arithmetic and logical instructions can act on
immediate data, registers, or memory locations.
In the X86family, direct and indirect memory addressing can be used. In direct
addressing, a displacement address consisting of a 8-, 16-, or 32-bit word is used asthe logical address. This logical address is added to the shifted contents of the seg-ment register (segment base address) to give a physical memory address. Figure 3.8illustrates the direct addressing process.
Address indirection in the X86 family can be obtained using the content of a
base pointer register (BPR), the content of an index register, or the sum of a baseregister and an index register. Figure 3.9 illustrates indirect addressing using
the BPR.
TheX86family of processors deﬁnes a number of instruction types. Using the
naming convention introduced before, these instruction types are data movement,
arithmetic and logic, and sequencing (control transfer). In addition, the X86
family deﬁnes other instruction types such as string manipulation, bit manipulation,and high-level language support.
Data movement instructions in the X86 family include mainly four subtypes.
These are the general-purpose, accumulator-speciﬁc, address-object, and ﬂag
instructions. A sample of these instructions is shown in Table 3.9.
Arithmetic and logic instructions in the X86 family include mainly ﬁve subtypes.
These are addition, subtraction, multiplication, division, and logic instructions.
A sample of the arithmetic instructions is shown in Table 3.10.
Logic instructions include the typical AND ,OR,NOT,XOR, and TEST. The latter
performs a logic compare of the source and the destination and sets the ﬂags accord-
ingly. In addition, the X86family has a set of shift and rotate instructions. A sample
of these is shown in Table 3.11.Original 16 bits
7/15/31 15 0
Op Code Address
Segment base
Address Σ
Physical AddressDisplacement from2
nd word of instructionShifted 16 bits
Logical Address0
Figure 3.8 Direct addressing in the X86family50 ASSEMBLY LANGUAGE PROGRAMMINGControl transfer instructions in the X86 family include mainly four subtypes.
These are conditional, iteration, interrupt, and unconditional. A sample of these
instructions is shown in Table 3.12.
Processor control instructions in the X86 family include mainly three subtypes.
These are external synchronization, ﬂag manipulation, and general control instruc-
tions. A sample of these instructions is shown in Table 3.13.
Having introduced the basic features of the instruction set of the X86processor
family, we now move on to present a number of programming examples to showShifted 16 bits
2ndword of instruction Original 16 bits
7/15/31 0 15 0
Op Code Address
Logical AddressSegment base
Address
Physical Address BPRå åDisplacement from
Figure 3.9 Indirect addressing using BPR in the X86family
TABLE 3.9 Sample of the X86Data Movement Instructions
Mnemonic Operation Subtype
MOV Move source to destination General purpose
POP Pop source from stack General purpose
POPA Pop all General purpose
PUSH Push source onto stack General purpose
PUSHA Push all General purpose
XCHG Exchange source with destination General purpose
IN Input to accumulator Accumulator
OUT Output from accumulator Accumulator
XLAT Table lookup to translate byte Accumulator
LEA Load effective address in register Address-object
LMSW Load machine status word Address-object
SMSW Store machine status word Address-object
POPF Pop ﬂags off stack Flag
PUSHF Push ﬂags onto stack Flag3.5. EXAMPLE: THE X86FAMILY 51how the instruction set can be used. The examples presented are the same as those
presented at the end of Chapter 2.
Example 3 Adding 100 numbers stored at consecutive memory locations starting
at location 1000, the results should be stored in memory location 2000. LIST isTABLE 3.10 Sample of the X86Arithmetic Instructions
Mnemonic Operation Subtype
ADD Add source to destination Addition
ADC Add source to destination with carry Addition
INC Increment operand by 1 Addition
SUB Subtract source from destination Subtraction
SBB Subtract source from destination with borrow Subtraction
DEC Decrement operand by 1 Subtraction
MUL Unsigned multiply source by destination Multiply
IMUL Signed multiply source by destination Multiply
DIV Unsigned division accumulator by source Division
IDIV Signed division accumulator by source Division
TABLE 3.11 Sample of the X86Shift and Rotate
Instructions
Mnemonic Operation
ROR Rotate right
ROL Rotate left
RCL Rotate left through carry
RCR Rotate right through carry
SAR Arithmetic shift right
SAL Arithmetic shift left
SHR Logic shift right
SHL Logic shift left
TABLE 3.12 Sample of the X86Control Transfer Instructions
Mnemonic Operation Subtype
SET Set byte to true or false based on condition Conditional
JS Jump if sign Conditional
LOOP Loop if CX does not equal zero Iteration
LOOPE Loop if CX does not equal zero & ZF ¼1 Iteration
INT Interrupt Interrupt
IRET Interrupt return Interrupt
JMP Jump unconditional Unconditional
RET Return from procedure Unconditional52 ASSEMBLY LANGUAGE PROGRAMMINGdeﬁned as an array of Nelements each of size byte. FLAG is a memory variable used
to indicate whether the list has been sorted or not. The register CXis used as a coun-
ter with the Loop instruction. The Loop instruction decrements the CXregister and
branch if the result is not zero. The addressing mode used to access the array List
[BXþ1] is called based addressing mode. It should be noted that since we are
using BX and BXþ1 the CXcounter is loaded with the value 999 in order not to
exceed the list.
MOV CX, 1000 21 ; Counter ¼CX (1000 21)
MOV BX, Offset LIST ; BX  pointer to LIST
CALL SORT
......
SORT PROC NEAR
Again: MOV FLAG, 0 ; FLAG  0
Next: MOV AL, [BX]
CMP AL, [BXþ1] ;Compare current and next values
JLE Skip ;Branch if current ,next values
XCHG AL, [BXþ1] ;If not, Swap the contents of the
MOV [BXþ1], AL ;current location with the next one
MOV FLAG, 1 ;Indicate the swap
Skip: INC BX ; BX  BXþ1
LOOP Next ;Go to next value
CMP FLAG, 1 ;Was there any swapJE Again ;If yes Repeat processRET
SORT ENDP
Example 4 Here we implement the SEARCH algorithm in the 8086 instruction
set.LIST is deﬁned as an array of Nelements each of size word. FLAG is a
memory variable used to indicate whether the list has been sorted or not. The register
CXis used as a counter with the loop instruction. The Loop instruction decrementsTABLE 3.13 Sample of the X86Processor Control Instructions
Mnemonic Operation Subtype
HLT Halt External sync
LOCK Lock the bus External sync
CLC Clear carry ﬂag Flag
CLI Clear interrupt ﬂag Flag
STI Set interrupt ﬂag Flag
INVD Invalidate data cache General control3.5. EXAMPLE: THE X86FAMILY 53theCXregister and branch if the result is not zero. The addressing mode used to
access the array List [BX þ1] is called based addressing mode.
MOV CX, 1000 ; Counter ¼CX 1000
MOV BX, Offset
LIST;B X pointer to LIST
MOV SI, 0 ; SI used as an index
MOV AX, VAL ; AX  VAL
CALL SEARCH
; Test FLAG to check whether value found
......
SEARCH PROC NEAR
MOV FLAG, 0 ; FLAG  0
Next: CMP AX, [BX þSI] ;Compare current value to VAL
JE Found ;Branch if equal
ADD SI, 2 ; SI  SIþ2, next value
LOOP Next ;Go to next valueJMP Not_Found
Found: MOV FLAG, 1 ;Indicate value found
MOV POSITION, SI ;Return index of value in List
Not_Found: RET
SEARCH ENDP
Example 5 This is the same as Example 4 but using the stack features of the X86.
PUSH DS ;See Table 3.9
MOV CX, 1000 ;Counter ¼CX 1000
MOV BX, OFFSET LIST ;Point to beginning of LISTPUSH BXPUSH VAL ;VAL is a word variableCALL SEARCH
;Test FLAG to check whether value found
;If found get index from SI register
using
POP SI
......
SEARCH PROC NEAR
POP TEMP ;Save IPPOP AX ;AX  VAL. Value to search for
POP SI ;SI  OFFSET LIST and let BX ¼SI54 ASSEMBLY LANGUAGE PROGRAMMINGPOP ES ;Make ES ¼DS (See Table)
CLD ;Set auto-increment mode
REPNE SCASW ;Scan LIST for value in AX if not
found; increment SI by 2,
decrement CX and if; not zeroscan next location in LIST.
;If occurrence found Zero ﬂag
is set
JNZ Not_Found ;If value not branch to
Not_Found?
MOV FLAG, 1 ;Yes
SUB SI, BX
PUSH SI ;Save position
Not_Found: PUSH TEMP ;Restore IP
RET
SEARCH ENDP
It should be noted in the above example that when a call to a procedure is
initiated, the IPregister is the last value to be pushed on top of the stack. Therefore,
care should be made to avoid altering the value of the IPregister. The top of the
stack is thus saved to a temporary variable TEMP at procedure entry and restored
before exit.
3.6. SUMMARY
A machine language is a collection of the machine instructions represented in 0s and
1s. Assembly language provides easier to use symbolic representation, in which analphanumeric equivalent to machine language is used. There is a one-to-one corre-spondence between assembly language statements and machine instructions. Anassembler is a program that accepts a symbolic language program (source program)and produces its machine language equivalent (target program). Although assemblylanguage programming is difﬁcult compared to programming in high-level
languages, it is still important to learn assembly. In some applications, small por-
tions of a program that are heavily used may need to be written in assemblylanguage. Programming in assembly can result in machine code that is smallerand faster than that generated by a compiler of a high-level language. Assembly pro-grammers have access to all the hardware features of the target machine that mightnot be accessible to high-level language programmers. In addition, learning assem-bly languages can be of great help in understanding the low level details of computerorganization and architecture. In this chapter we provided a general overview of
assembly language and its programming. The programmer view of the X86 Intel
microprocessor family of processors was also introduced as a real-world example.
Examples were then presented showing how to use the X86instruction set in writing
sample programs similar to those presented in Chapter 2.3.6. SUMMARY 55EXERCISES
1. What is the difference between each of the following pairs?
.Compilers and assemblers
.Source code and target code
.Mnemonics and hexadecimal representation
.Pseudo instructions and instructions
.Labels and addresses
.Symbol table and opcode table
.Program counter (PC) and instruction location counter (ILC)
2. Using the assembly language of the simple processor in Section 3.1, write
assembly code segments to do the following operation:
.Swap two numbers
.Logical OR
.Negation
3. Add input /output instructions to the instruction set of the simple processor in
Section 3.1 and write an assembly program to ﬁnd the Fibonacci sequence.
4. Obtain the machine language code of the multiplication assembly program
given in Section 3.2.
5. With the great advances in high-level languages and compilers, some people
argue that assembly language is not important anymore. Give some argu-
ments for and against this view.
6. Write a program segment using the instruction of the X86 family to computeP200
i¼1XiYi, where XiandYiare signed 8-bit numbers. Assume that no over-
ﬂow will occur.
7. Write a subroutine using the X86instructions that can be called by a main
program in a different code segment. The subroutine will multiply a signed
16-bit number in CX by a signed 8-bit number in AL. The main programwill call this subroutine, store the result in two consecutive memory words,
and stop. Assume that SI and DI contain the signed 8-bit and 16-bit numbers,
respectively.
8. Write a program using the X86instructions to compare a source string of 100
words pointed to by an offset of 2000H in DS with a destination stringpointed to by an offset 4000H in DS.
9. Write a program using the X86instructions to generate the ﬁrst 10 numbers
of the Fibonacci series, that is, to generate the series 1, 1, 2, 3, 5, 8, 13, 21, 34.
10. Write a program using the X86instructions to convert a word of text from
upper case to lower case. Assume that the word consists of ASCII charactersstored in successive memory locations starting at location START and ending
at location FINISH.56 ASSEMBLY LANGUAGE PROGRAMMINGREFERENCES AND FURTHER READING
C. M. Gilmore, Microprocessors: Principles and Applications , 2nd ed., McGraw-Hill,
New York, 1996.
V. C. Hamacher, Z. G. Vranesic and S. G. Zaky, Computer Organization , 5th ed.,
McGraw-Hill, New York, 2002.
J. P. Hayes, Computer Architecture and Organization , McGraw-Hill, New York, 1998.
V. Heuring, and H. Jordan, Computer Systems Design and Architecture , Addison Wesley,
NJ, 1997.
K. R. Irvine, Assembly Language for Intel-Based Computers , 4th ed., Prentice Hall, NJ, 2003.
A. D. Patterson and J. L. Hennessy, Computer Organization & Design; The Hardware /
Software Interface , Morgan Kaufmann, San Mafeo, 1994.
W. Stallings, Computer Organization and Architectures: Designing for Performance , 4th ed.,
Prentice-Hall, NJ, U.S.A, 1996.
A. Tanenbaum, Structured Computer Organization , 4th ed., Prentice Hall, Paramus, NJ,
U.S.A, 1999.
J. Uffenbeck, The 80/C286 Family, Design, Programming, and Interfacing , 3rd ed., Prentice
Hall, Essex, UK, 2002.
B. Wilkinson, Computer Architecture: Design and Performance , 2nd ed., Prentice-Hall,
Hertfordshire, UK, 1996.REFERENCES AND FURTHER READING 57& CHAPTER 4
Computer Arithmetic
This chapter is dedicated to a discussion on computer arithmetic. Our goal is to
introduce the reader to the fundamental issues related to the arithmetic operationsand circuits used to support computation in computers. Our coverage starts withan introduction to number systems. In particular, we introduce issues such asnumber representations and base conversion. This is followed by a discussion oninteger arithmetic. In this regard, we introduce a number of algorithms togetherwith hardware schemes that are used in performing integer addition, subtraction,multiplication, and division. We end this chapter with a discussion on ﬂoating-
point arithmetic. In particular, we introduce issues such as ﬂoating-point represen-
tation, ﬂoating-point operations, and ﬂoating-point hardware schemes. The IEEEﬂoating-point standard is the last topic discussed in the chapter.
4.1. NUMBER SYSTEMS
A number system uses a speciﬁc radix (base). Radices that are power of 2 are widely
used in digital systems. These radices include binary (base 2), quaternary (base 4),
octagonal (base 8), and hexagonal (base 16). The base 2 binary system is dominant in
computer systems.
An unsigned integer number Acan be represented using ndigits in base b:
A¼(a
n/C01an/C02...a2a1a0)b. In this representation (called positional representation )
each digit aiis given by 0/C20ai/C20(b/C01). Using positional representation, the dec-
imal value of the unsigned integer number Ais given by A¼Pn/C01
i¼0ai/C2bi. Con-
sider, for example, the positional representation of the decimal number A¼106.
Using 8 digits in base 2, Ais represented as A¼0/C227þ1/C226þ1/C225þ
0/C224þ1/C223þ0/C222þ1/C221þ0/C220.
Using ndigits, the largest value for an unsigned number Ais given by
Amax¼bn/C01. For example, the largest unsigned number that can be obtained
using 4 digits in base 2 is 24/C01¼15. In this case, decimal numbers ranging
from 0 to 15 (corresponding to binary 0000 to 1111) can be represented. Similarly,the largest unsigned number that can be obtained using 4 digits in base 4 is
59Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.44/C01¼255. In this case, decimal numbers ranging from 0 to 255 (corresponding
to 0000 to 3333) can be represented.
Consider the use of ndigits to represent a real number Xin radix bsuch that the
most signiﬁcant kdigits represent the integral part while the least signiﬁcant mdigits
represents the fraction part. The value of Xis given by
X¼Xk/C01
i¼/C0mxi/C2bi¼xk/C01bk/C01þxk/C02bk/C02þ/C1/C1/C1þ x1b1
þx0b0þx/C01b/C01þ/C1/C1/C1þ x/C0mb/C0m
Consider, for example, the representation of the real number X¼25.375. This
number can be represented in binary using k¼5 and m¼3 as follows.
X¼X4
i¼/C03xi/C2bi¼x4/C224þx3/C223þx2/C222þx1/C221
þx0/C220þx/C01/C22/C01þx/C02/C22/C02þx/C032/C03
with x4¼1,x3¼1,x2¼0,x1¼0,x0¼1,x/C01¼0,x/C02¼1, and x/C03¼1.
It is often necessary to convert the representation of a number from a given base
to another, for example, from base 2 to base 10. This can be achieved using a number
of methods (algorithms). An important tool in some of these algorithms is the div-ision algorithm. The basis of the division algorithm is that of representing an integerain terms of another integer cusing a base b. The basic relation used is
a¼c/C2qþr, where qis the quotient and ris the remainder, 0/C20r/C20b/C01 and
q¼ba=cc. Radix conversion is discussed below.
4.1.1. Radix Conversion Algorithm
A radix conversion algorithm is used to convert a number representation in a given
radix, r
1, into another representation in a different radix, r2. Consider the conversion
of the integral part of a number X,Xint. The integral part Xintcan be expressed as
Xint¼½ /C1 /C1 /C1 (xk/C01r2þxk/C02)r2þ/C1/C1/C1þ x2)r2þx1/C138/C8/C9
r2þx0:
Dividing Xintbyr2will result in a quotient Xq¼{½/C1/C1/C1(xk/C01r2þxk/C02)r2þ/C1/C1/C1þ
x2)r2þx1/C138} and a remainder Xrem¼x0. Repeating the division process on the quo-
tient and retaining the remainders as the required digits until a zero quotient is
obtained will result in the required representation of Xintin the new radix r2.
Using a similar argument, it is possible to show that a repeated multiplication of
the fractional part of X(Xf)b y r2retaining the obtained integers as the required
digits, will result in the required representation of the fractional part in the new
radix, r2. It should, however, be noted that unlike the integral part conversion, the60 COMPUTER ARITHMETICfractional part conversion may not terminate after a ﬁnite number of repeated mul-
tiplications. Therefore, the process may have to be terminated after a number ofsteps, thus leading to some acceptable approximation.
Example Consider the conversion of the decimal number 67.575 into binary.
Here r
1¼10,r2¼2,Xint¼67, and Xf¼0.575. For the integral part Xint, a repeated
division by 2 will result in the following quotients and remainders:
Quotient 33 16 8 4 2 1 0
Remainder 1 1 0 0 0 0 1
Therefore the integral part in radix r2¼2i sXint¼(1000011). A similar method can
be used to obtain the fractional part (through repeated multiplication):
Fractional part 0.150 0.300 0.600 0.200 0.400 0.800 0.600 0.200 ...
Carry over bit 1 0 0 1 0 0 1 1 ...
The fractional part is Xf¼(.10010011 ...). Therefore, the resultant representation
of the number 67.575 in binary is given by (1000011.10010011 ...).
4.1.2. Negative Integer Representation
There exist a number of methods for representation of negative integers. These
include the sign-magnitude ,radix complement , and diminished radix complement.
These are brieﬂy explained below.
4.1.3. Sign-Magnitude
According to this representation, the most signiﬁcant bit (of the nbits used to
represent the number) is used to represent the sign of the number such that a “1”
in the most signiﬁcant bit position indicates a negative number while a “0” in themost signiﬁcant bit position indicates a positive number. The remaining ( n/C01)
bits are used to represent the magnitude of the number. For example, the negativenumber ( 218) is represented using 6 bits, base 2 in the sign-magnitude format, as
follows (110010), while a ( þ18) is represented as (010010). Although simple, the
sign-magnitude representation is complicated when performing arithmetic opera-tions. In particular, the sign bit has to be dealt with separately from the magnitudebits. Consider, for example, the addition of the two numbers þ18 (010010) and 219
(110011) using the sign-magnitude representation. Since the two numbers carrydifferent signs, then the result should carry the sign of the larger number in
magnitude, in this case the ( 219). The remaining 5-bit numbers are subtracted
(10011210010) to produce (00001), that is, ( 21).4.1. NUMBER SYSTEMS 614.1.4. Radix Complement
According to this system, a positive number is represented the same way as in the
sign-magnitude. However, a negative number is represented using the b’s comp-
lement (for base b numbers). Consider, for example, the representation of the
number ( 219) using 2’s complement. In this case, the number 19 is ﬁrst represented
as (010011). Then each digit is complemented, hence the name radix complement toproduce (101100). Finally a “1” is added at the least signiﬁcant bit position to resultin (101101). Now, consider the 2’s complement representation of the number ( þ18).
Since the number is positive, then it is represented as (010010), the same as inthe sign-magnitude case. Now, consider the addition of these two numbers. In thiscase, we add the corresponding bits without giving special treatment to the sign
bit. The results of adding the two numbers produces (111111). This is the 2’s comp-
lement representation of a ( 21), as expected. The main advantage of the 2’s comp-
lement representation is that no special treatment is needed for the sign of thenumbers. Another characteristic of the 2’s complement is the fact that a carrycoming out of the most signiﬁcant bit while performing arithmetic operations isignored without affecting the correctness of the result. Consider, for example,adding 219 (101101) andþ26 (011010). The result will be (1)(000111), which
is correct (þ7) if the carry bit is ignored.
4.1.5. Diminished Radix Complement
This representation is similar to the radix complement except for the fact that no “1”
is added to the least signiﬁcant bit after complementing the digits of the number, asis done in the radix complement. According to this number system representation, a
(219) is represented as (101100), while a ( þ18) is represented as (010010). If we
add these two numbers we obtain (111110), the 1’s complement of a ( 21). The
main disadvantage of the diminished radix representation is the need for a correction
factor whenever a carry is obtained from the most signiﬁcant bit while performingarithmetic operations. Consider, for example, adding 23 (111100) toþ18 (010010)
TABLE 4.1 The 2’s and the 1’s Complement
Representation of an 8-Bit Number
Number Representation Example
2’s Complement
x¼0 0 0 (00000000)
0,x,256 x 77 (01001101)
2128/C20x,0 256 2jxj 256 (11001000)
1’s Complement
x¼0 0 or 255 (11111111)
0,x,256 x 77 (01001101)
2127/C20x,0 255 2jxj 256 (11000111)62 COMPUTER ARITHMETICto obtain (1)(001110). If the carry bit is added to the least signiﬁcant bit of the result,
we obtain (001111), that is, ( þ15), which is a correct result.
Table 4.1 shows a comparison between the 2’s complement and the 1’s comp-
lement in the representation of an 8-bit number, x.
4.2. INTEGER ARITHMETIC
In this section, we introduce a number of techniques used to perform integer arith-
metic using the radix complement representation of numbers. Our discussion willfocus on the base “2” binary representation.
4.2.1. Two’s Complement (2’s) Representation
In order to represent a number in 2’s complement, we perform the following two steps.
1. Perform the Boolean complement of each bit (including the sign bit);
2. Add 1 to the least signiﬁcant bit (treating the number as an unsigned binary
integer), that is,/C0A¼
Aþ1
Example Consider the representation of ( 222) using 2’s complement.
22¼00010110
+
11101001 (1’s complement)
þ 1
11101010 (/C022)
4.2.2. Two’s Complement Arithmetic
Addition Addition of two n-bit numbers in 2’s complement can be performed
using an n-bit adder. Any carry-out bit can be ignored without affecting the correct-
ness of the results, as long as the results of the addition is in the range
/C02n/C01toþ2n/C01/C01.
Example Consider the addition of the two 2’s complement numbers ( 27) and
(þ4). This addition can be carried out as ( 27)þ(þ4)¼23, that is, 1001þ
(0100) ¼1101, a ( 23) in 2’s complement.
The condition that the result should be in the range /C02n/C01toþ2n/C01/C01i s
important because a result outside this range will lead to an overﬂow and hence awrong result. In simple terms, an overﬂow will occur if the result produced by a4.2. INTEGER ARITHMETIC 63given operation is outside the range of the representable numbers. Consider the fol-
lowing two examples.
Example Consider adding the two 2’s complement numbers ( þ7) and (þ6). The
addition can be done as ( þ7)þ(þ6)¼þ13, that is, 0111þ(0110) ¼1101, a
wrong result. This is because the result exceeds the largest value ( þ7).
Example Consider adding the two 2’s complement numbers ( 27) and ( 24). The
addition can be done as ( 27)þ(24)¼211, that is, 1001þ(1100)¼0101, a
wrong result. This is because the result is less than the smallest value ( 28).
Notice that the original numbers are negative while the result is positive.
From these two examples, we can make the following observation: when two
numbers (both positive or both negative) are added, then overﬂow can be detected
if and only if the result has an opposite sign to the added numbers.
Subtraction In 2’s complement, subtraction can be performed in the same way
addition is performed. For example, to perform B/C0A¼Bþ /C22A Aþ1, that is, sub-
tracting Afrom Bis the same as addition to the complement of AtoB.
Example Consider the subtraction 2 27¼25. This can be performed as
2þ7¯þ1¼0010þ1000þ0001¼1011 (25).
It should be noted that our earlier observation about the occurrence of overﬂow in
the context of addition applies in the case of subtraction as well. This is because sub-
traction is after all addition to the complement. Consider the following illustrative
example.
Example Consider the subtraction 7 2(27)¼14. This can be performed as
7þ7¯þ1¼0111þ1000þ0001¼(1) 0000, a wrong answer (result .7).
Hardware Structures for Addition and Subtraction of Signed
Numbers The addition of two n-bit numbers AandBrequires a basic hardware
circuit that accepts three inputs, that is, ai,bi, and ci/C01. These three bits represent
respectively the two current bits of the numbers AandB(at position i) and the
carry bit from the previous bit position (at position i21). The circuit should pro-
duce two outputs, that is, siandcirepresenting respectively the sum and the
carry, according to the following truth-table.
ai 00001111
bi 00110011
ci21 01010101
si 01101001
ci 0001011164 COMPUTER ARITHMETICThe output logic functions are given by si¼ai/C8bi/C8ci/C01and ci¼aibiþ
aici/C01þbici/C01. The circuit used to implement these two functions is called a
full-adder (FA) and is shown in Figure 4.1.
Addition of two n-bit numbers AandBcan be carried out using nconsecutive
FAs in an arrangement known as a carry-ripple through adder (CRT) , see Figure 4.2.
The n-bit CRT adder shown in Figure 4.2 can be used to add 2’s complement
numbers AandBin which the bn/C01andan/C01represent the sign bits. The same cir-
cuit can be used to perform subtraction using the relation B/C0A¼Bþ /C22A Aþ1.
Figure 4.3 shows the structure of a binary addition /subtraction logic network.
In this ﬁgure, the two inputs A and B represent the arguments to be added /sub-
tracted. The control input determines whether an add or a subtract operation is to be
performed such that if the control input is 0 then an add operation is performed whileif the control input is 1 then a subtract operation is performed. A simple circuit thatcan implement the Add /Sub block in Figure 4.3 is shown in Figure 4.4 for the case
of 4-bit inputs.
One of the main drawbacks of the CRT circuit is the expected long delay between
the time the inputs are presented to the circuit until the ﬁnal output is obtained. Thisis because of the dependence of each stage on the carry output produced by the pre-vious stage. This chain of dependence makes the CRT adder’s delay O(n), where nis
the number of stages in the adder. In order to speed up the addition process, it isnecessary to introduce addition circuits in which the chain of dependence amongthe adder stages must be broken. A number of fast addition circuits exist in the lit-erature. Among these the carry-look-ahead (CLA) adder is well known. The CLA
adder is introduced below.
Consider the CRT adder circuit. The two logic functions realized are s
i¼
ai/C8bi/C8ci/C01andci¼aibiþaici/C01þbici/C01. These two functions can be rewritten
in terms of two new subfunctions, the carry generate ,Gi¼aibiand the carry pro-
pagate ,Pi¼ai/C8bi. Using these two new subfunctions, we can rewrite the logic
equation for the carry output of any stage as ci¼GiþPici/C01. Now, we can write
Figure 4.1 The full-adder (FA) circuit4.2. INTEGER ARITHMETIC 65the series of carry outputs from the different stages as follows.
c0¼G0þP0c/C01
c1¼G1þP1c0¼G1þP1(G0þP0c/C01)¼G1þG0P1þP0P1c/C01
c2¼G2þP2c1¼G2þP2(G1þG0P1þP0P1c/C01)
¼G2þG0P1P2þG1P2þP0P1P2c/C01
c3¼G3þP3c2¼G3þP3(G2þG1P2þG0P1P2þP0P1P2c/C01)
¼G3þG2P3þG1P2P3þG0P1P2P3þP0P1P2P3c/C01
...
The sequence of carry outputs shows total independence among the different car-
ries (broken carry chain). Figure 4.5 shows the overall architecture of a 4-bit CLA
adder. There are basically three blocks in a CLA. The ﬁrst one is used to generate theG
is and the Pis, while the second is used to create all the carry output. The third
block is used to generate all the sum outputs. Regardless of the number of bits in
the CLA, the delay through the ﬁrst block is equivalent to a one gate delay, the
delay through the second block is equivalent to a two gate delay and the delaythrough the third block is equivalent to a one gate delay. In Figure 4.5, we showthe generation of some carry and sum outputs. The reader is encouraged to completethe design (see the Chapter Exercises).
Figure 4.2 n-Bit carry-ripple through (CRT) adder
CA B
Control = 0/1 Add/Sub
Figure 4.3 Addition /subtraction logic network66 COMPUTER ARITHMETICControl = 0/1 Input Argument A
Figure 4.4 The Add /Sub circuit
b3
P3 G3P2 G2 P1 P0 G1 G0
c-1a3 b2a2 b1a1 b0a0
AND-OR circuit for
realizing c3AND-OR
circuit for
realizing c2
c3
P3
s3 s2 s1 s0P2 P1 P0c2 c1 c0
Figure 4.5 A 4-bit CLA adder structure4.2. INTEGER ARITHMETIC 67Multiplication In discussing multiplication, we shall assume that the two input
arguments are the multiplier Qgiven by Q¼qn/C01qn/C02/C1/C1/C1q1q0and the multiplicand
Mgiven by M¼mn/C01mm/C02/C1/C1/C1m1m0. A number of methods exist for performing
multiplication. Some of these methods are discussed below.
The Paper and Pencil Method (for Unsigned Numbers) This is the simplest
method for performing multiplication of two unsigned numbers. The method is illus-
trated through the example shown below.
Example Consider the multiplication of the two unsigned numbers 14 and 10.
The process is shown below using the binary representation of the two numbers.
1110 (14) Multiplicand( M)
1010 (10) Multiplier(Q)
0000 (Partial Product)
1110 (Partial Product)
0000 (Partial Product)
1110 (Partial Product)
¼¼¼¼¼¼
10001100 (140) Final Product(P)
The above multiplication can be performed using an array of cells each consisting
of an FA and an AND. Each cell computes a given partial product. Figure 4.6 shows
the basic cell and an example array for a 4 /C24 multiplier array.
What characterizes this method is the need for adding npartial products regard-
less of the values of the multiplier bits. It should be noted that if a given bit of themultiplier is 0, then there should be no need for computing the corresponding partial
product. The following method makes use of this observation.
The Add-Shift Method In this case, multiplication is performed as a series of ( n)
conditional addition and shift operations such that if the given bit of the multiplier is
0 then only a shift operation is performed, while if the given bit of the multiplier is 1
then addition of the partial products and a shift operation are performed. The follow-ing example illustrates this method.
Example Consider multiplication of the two unsigned numbers 11 and 13. The
process is shown below in a tabular form. In this process, Ais a 4-bit register and
is initialized to 0s and C is the carry bit from the most signiﬁcant bit position.
The process is repeated n¼4 times (the number of bits in the multiplier Q). If
the bit of the multiplier is “1”, then A AþMand the concatenation of AQis
shifted one bit position to the right. If, on the other hand, the bit is “0”, then onlya shift operation is performed on AQ. The structure required to perform such an68 COMPUTER ARITHMETICFigure 4.6 Array multiplier for unsigned binary numbers
Figure 4.7 Hardware structure for add-shift multiplicationMC A Q
1011 0 0000 1101 Initial values
1011 0 1011 1101 Add First cycle
1011 0 0101 1110 Shift
1011 0 0010 1111 Shift Second cycle
1011 0 1101 1111 Add Third cycle
1011 0 0110 1111 Shift
1011 1 0001 1111 Add Fourth cycle
1011 0 1000 1111 Shift4.2. INTEGER ARITHMETIC 69operation is shown in Figure 4.7. In this ﬁgure, the control logic is used to determine
the operation to be performed depending on the least signiﬁcant bit in Q.A n n-bit
adder is used to add the contents of registers AandM.
In order to speed up the multiplication operation, a number of other techniques
can be used. These techniques are based on the observation that the larger thenumber of consecutive zeros and ones, the fewer partial products that have to be gen-erated. A group of consecutive zeros in the multiplier requires no generation of new
partial product. A group of kconsecutive ones in the multiplier requires the gener-
ation of fewer than knew partial products. One technique that makes use of the
above observation is the Booth’s algorithm. We discuss below the 2-bit Booth’s
algorithm.
The Booth’s Algorithm In this technique, two bits of the multiplier,
Q(i)Q(i21), (0/C20i/C20n/C01), are inspected at a time. The action taken depends
on the binary values of the two bits, such that if the two values are respectively
01, then A AþM; if the two values are 10, then A A/C0M. No action is
needed if the values are 00 or 11. In all four cases, an arithmetic shift right oper-ation on the concatenation of AQis performed. The whole process is repeated n
times ( nis the number of bits in the multiplier). The Booth’s algorithm requires
the inclusion of a bit Q(21)¼0 as the least signiﬁcant bit in the multiplier Qat
the beginning of the multiplication process. The Booth’s algorithm is illustratedin Figure 4.8.
The following examples show how to apply the steps of the Booth’s algorithm.
=10=01A= 0, Q( −1)= 0
M= Multiplicand
Q= Multiplier
n= Count
Q(0), Q( −1)?
11 or 00
A= A− M A= A+ M
ASR(A,Q)
n= n − 1
n= 0?No
Yes
Done
Figure 4.8 Booth’s algorithm70 COMPUTER ARITHMETICExample Consider the multiplication of the two positive numbers M¼0111 (7)
andQ¼0011 (3) and assuming that n¼4. The steps needed are tabulated below.
MA Q Q (21)
0111 0000 0011 0 Initial value
0111 1001 0011 0 A¼A2M
0111 1100 1001 1 ASR End cycle #1
---------------------------------------------------------------------
0111 1110 0100 1 ASR End cycle #2
---------------------------------------------------------------------0111 0101 0100 1 A¼AþM
0111 0010 1010 0 ASR End cycle #3
---------------------------------------------------------------------
0111 0001 0101 1 ASR End cycle #4
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ}
þ21 (correct result)
Example Consider the multiplication of the two numbers M¼0111 (7) and
Q¼1101 (23) and assuming that n¼4. The steps needed are tabulated below.
MA Q Q (21)
0111 0000 1101 0 Initial value
0111 1001 1101 0 A¼A2M
0111 1100 1110 1 ASR End cycle #1
---------------------------------------------------------------------
0111 0011 1110 1 A¼AþM
0111 0001 1111 0 ASR End cycle #2
---------------------------------------------------------------------
0111 1010 1111 0 A¼A2M
0111 1101 0111 1 ASR End cycle #3
---------------------------------------------------------------------0111 1110 1011 1 ASR End cycle #4
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ}
221 (correct result)
The hardware structure shown in Figure 4.9 can be used to perform the operations
required by the Booth’s algorithm. It consists of an ALU that can perform the add /
sub operation depending on the two bits Q(0)Q(21). A control circuitry is also
required to perform the ASR (AQ) and to issue the appropriate signals needed to
control the number of cycles.
The main drawbacks of the Booth’s algorithm are the variability in the number of
add/sub operations and the inefﬁciency of the algorithm when the bit pattern in Q4.2. INTEGER ARITHMETIC 71becomes a repeated pair of a 0 (1) followed by a 1(0). This last situation can be
improved if three rather than two bits are inspected at a time.
Division Among the four basic arithmetic operations, division is considered the
most complex and most time consuming. In its simplest form, an integer division oper-
ation takes two arguments, the dividend Xand the divisor D.It produces two outputs,
the quotient Qand the remainder R.The four quantities satisfy the relation X¼
Q/C2DþRwhere R,D. A number of complications arise when dealing with
division. The most obvious among these is the case D¼0. Another subtle difﬁculty
is the requirement that the resulting quotient should not exceed the capacity of the reg-ister holding it. This can be satisﬁed if Q,2
n/C01,w h e r e nis the number of bits in the
register holding the quotient. This implies that the relation X,2n/C01Dmust also be
satisﬁed. Failure to satisfy any of the above conditions will lead to an overﬂow
condition.
We will start by showing the division algorithm assuming that all values
involved, that is, divided, divisor, quotient, and remainder are interpreted as frac-
tions. The process is also valid for integer values as will be shown later.
In order to obtain a positive fractional quotient Q¼0q1q2/C1/C1/C1qn/C01, the division
operation is performed as a sequence of repeated subtractions and shifts. In each
step, the remainder should be compared with the divisor D. If the remainder
is larger, then the quotient bit is set to “1”; otherwise the quotient bit is set to“0”. This can be represented by the following equation r
i¼2ri/C01/C0qi/C2D
where riandri/C01are the current and the previous remainder, respectively, with r0¼
Xandi¼1, 2, ...,(n21).
Example Consider the division of a dividend X¼0.5¼(0.100000) and a divisor
D¼0.75¼(0.1100). The process is illustrated in the following table.
r0¼X 0† 100000 Initial values
2r0 01 † 00000 S e t q1¼1
2Dþ 11 † 010
r1¼2r02D 00 † 01000
2r1 00 † 1000 S e t q2¼0
Figure 4.9 Hardware structure implementing Booth’s algorithm72 COMPUTER ARITHMETICr2¼2r1 00 † 1000
2r2 01 † 0 0 0 Set q3¼1
2Dþ 11 † 010
r3¼2r22D 00 † 010
The resultant quotient Q¼(0.101)¼(5/8) and remainder R¼(1/32). These
values are correct since X¼QDþR¼(5/8)(3/4)þ1/32¼1/2.
Now we show the validity of the above process in the case of integer values. In
this case, the equation X¼QDþRcan be rewritten as 22n/C02Xf¼2n/C01Qf/C2
2n/C01Dfþ2n/C01Rf, leading to Xf¼Qf/C2Dfþ2/C0(n/C01)Rfwhere Xf,Df,Qf, and Rf
are fractions. We offer the following illustrative example.
Example Consider the division of a dividend X¼32¼(0100000) and a divisor
D¼6¼(0110). The process is illustrated in the following table.
r0¼X 0100000 Initial values
2r0 0100000 S e t q1¼1
2Dþ 11010
r1¼2r02D 0001000
2r1 001000 S e t q2¼0
r2¼2r1 001000
2r2 01000 S e t q3¼1
2Dþ 11010
r3¼2r22D 00010
2r3 0010 S e t q4¼0
The resultant quotient is Q¼0101¼(5) and the remainder R¼0010 (2). These are
correct values.
A hardware structure for binary division is shown in Figure 4.10. In this ﬁgure,
the divisor ( D) and the contents of register A are added using an ( nþ1)-bit adder. A
control logic is used to perform the required shift left operation (see Exercises).
The comparison between the remainder and the divisor is considered to be the
most difﬁcult step in the division process. The way used above to perform such com-
parison is to subtract Dfrom 2 ri/C01and if the result is negative, then we set qi¼0.
This required restoring the previous value by adding back the subtracted value(restoring division ).
The alternative is to use a non-restoring division algorithm:
Step #1: Do the following ntimes
1. If the sign of A is 0, shift left AQ and subtract D from A; otherwise shift
left AQ and add D to A.
2. If the sign of A is 0, set q
0¼1; otherwise set q0¼0.
Step #2: If the sign of A is 1, add D to A.4.2. INTEGER ARITHMETIC 73Example Consider the division of a dividend X¼8¼(1000) and a divisor
D¼3¼(0011) using the non-restoring algorithm. The process is illustrated in
the following table.
Initially 0 0 0 0 0 1 0 0 09
>>>=
>>>;First cycle00011
Shift 0 0 0 0 1 0 0 0
Subtract 1 1 1 0 1
Setx
0 1110 000 0
Shift 1 1 1 0 0 0 0 0
A d d 00011)
Second cycle
Setx0 11111 000 0
Shift 1 1 1 1 0 0 0 0
A d d 00011)
Third cycle
Setx0 00001 000 1
Shift 0 0 0 1 0 0 0 1Subtract 1 1 1 0 1)
Fourth cycle
Setx
0 11111 001 0
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ}
Quotient
A d d 11111)
Restore remainder 0001100010
Remainder
|ﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ{zﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄﬄ ﬄ}
4.3. FLOATING-POINT ARITHMETIC
Having considered integer representation and arithmetic, we consider in this section
ﬂoating-point representation and arithmetic.
4.3.1. Floating-Point Representation (Scientiﬁc Notation)
A ﬂoating-point (FP) number can be represented in the following form: +m/C3be,
where m, called the mantissa , represents the fraction part of the number and isDivisor  (D)
Shift Left(n+1)-bit AdderAdd
Control Logic
Register   A Dividend (X)C
Figure 4.10 Binary division structure74 COMPUTER ARITHMETICnormally represented as a signed binary fraction, erepresents the exponent, and b
represents the base (radix) of the exponent.
Example Figure 4.11 is a representation of a ﬂoating-point number having
m¼23 bits, e¼8 bits, and S(sign bit) ¼1 bit. If the value stored in Sis 0, the
number is positive and if the value stored in Sis 1, the number is negative.
The exponent in the above example, can only represent positive numbers 0
through 255. To represent both positive and negative exponents, a ﬁxed value,
called a bias, is subtracted from the exponent ﬁeld to obtain the true exponent.
Assume that in the above example a bias ¼128 is used, then true exponents in
the range 2128 (stored as 0 in the exponent ﬁeld) to þ127 (stored as 255 in the
exponent ﬁeld) can be represented. Based on this representation, the exponent þ4
can be represented by storing 132 in the exponent ﬁeld, while the exponent 212
can be represented by storing 116 in the exponent ﬁeld.
Assuming that b¼2, then an FP number such as 1.75 can be represented in any
of the forms shown in Figure 4.12.
To simplify performing operations on FP numbers and to increase their precision,
they are always represented in what is called normalized forms. An FP number is
said to be normalized if the leftmost bit of the mantissa is 1. Therefore, among
the three above possible representations for 1.75, the ﬁrst representation is normal-ized and should be used.
Since the most signiﬁcant bit (MSB) in a normalized FP number is always 1, then
this bit is often not stored and is assumed to be a hidden bit to the left of the radixpoint, that is, the stored mantissa is 1. m.Therefore, a nonzero normalized number
represents the value ( /C01)
s*(1:m)*2e/C0128.
Floating-Point Arithmetic Addition /Subtraction The difﬁculty in adding
two FP numbers stems from the fact that they may have different exponents.Therefore, before adding two FP numbers, their exponents must be equalized, thatis, the mantissa of the number that has smaller magnitude of exponent must bealigned.
Figure 4.11 Representation of a ﬂoating-point number
+0.111*21
+0.0111*22
+0.00000000000000000000111*2210 10000001 11100000000000000000000
0 10000010 01110000000000000000000
0 10010101 00000000000000000000111
Figure 4.12 Different representation of an FP number4.3. FLOATING-POINT ARITHMETIC 75Steps Required to Add /Subtract Two Floating-Point Numbers
1. Compare the magnitude of the two exponents and make suitable alignment to
the number with the smaller magnitude of exponent.
2. Perform the addition /subtraction.
3. Perform normalization by shifting the resulting mantissa and adjusting the
resulting exponent.
Example Consider adding the two FP numbers 1.1100 *24and 1.1000 *22.
1. Alignment: 1.1000 *22has to be aligned to 0.0110 *24
2. Addition: Add the two numbers to get 10.0010 *24.
3. Normalization: The ﬁnal normalized result is 0.1000 *26(assuming 4 bits
are allowed after the radix point).
Addition /subtraction of two FP numbers can be illustrated using the schematic
shown in Figure 4.13.
Multiplication Multiplication of a pair of FP numbers X¼mx*2aandY¼
my*2bis represented as X*Y¼(mx/C3my)*2aþb.
A general algorithm for multiplication of FP numbers consists of three basic
steps. These are:
1. Compute the exponent of the product by adding the exponents together.
2. Multiply the two mantissas.
3. Normalize and round the ﬁnal product.
Example Consider multiplying the two FP numbers X¼1.000 *222and
Y¼21.010 *221.
1. Add exponents: 22þ(21)¼23.
2. Multiply mantissas: 1.000 *21.010¼21.010000.
The product is 21.0100 *223.
Figure 4.13 Addition /subtraction of FP numbers76 COMPUTER ARITHMETICMultiplication of two FP numbers can be illustrated using the schematic shown in
Figure 4.14.
Division Division of a pair of FP numbers X¼mx*2aandY¼my*2bis
represented as X=Y¼(mx=my)*2a/C0b.
A general algorithm for division of FP numbers consists of three basic steps:
1. Compute the exponent of the result by subtracting the exponents.
2. Divide the mantissa and determine the sign of the result.3. Normalize and round the resulting value, if necessary.
Example Consider the division of the two FP numbers X¼1.0000 *2
22and
Y¼21.0100 *221.
1. Subtract exponents: 222(21)¼21.
2. Divide the mantissas: 1.0000 42 1.0100 ¼20.1101.
3. The result is 20.1101 *221.
Division of two FP numbers can be illustrated using the schematic shown in
Figure 4.15.
4.3.3. The IEEE Floating-Point Standard
There are essentially two IEEE standard ﬂoating-point formats. These are the basic
and the extended formats. In each of these, IEEE deﬁnes two formats, that is, the
single-precision and the double-precision formats. The single-precision format is
32-bit and the double-precision is 64-bit. The single extended format should have
at least 44 bits and the double extended format should have at least 80 bits.Exponent E2 Exponent E1
AddMantissa M2 Mantissa M1
Multiply
Result normalization and round logic
Result Exponent Result Mantissa
Figure 4.14 FP multiplication4.3. FLOATING-POINT ARITHMETIC 77In the single-precision format, base 2 is used, thus allowing the use of a hidden
bit. The exponent ﬁeld is 8 bits. The IEEE single-precision representation is shown
in Figure 4.16.
The 8-bit exponent allows for any of 256 combinations. Among these, two com-
binations are reserved for special values:
1.e¼0 is reserved for zero (with fraction m¼0) and denormalized numbers
(with fraction m=0).
2.e¼255 is reserved for +1(with fraction m¼0) and not a number (NaN)
(with fraction m=0).
m¼0 m=0
e¼0 0 Denormalized
e¼255 +1 NaN
The single extended IEEE format extends the exponent ﬁeld from 8 to 11 bits and
the mantissa ﬁeld from 23 þ1 to 32 or more bits (without a hidden bit). This results in
a total length of at least 44 bits. The single extended format is used in calculatingintermediate results.
4.3.4. Double-Precision IEEE Format
Here the exponent ﬁeld is 11 bits and the signiﬁcant ﬁeld is 52 bits. The format is
shown in Figure 4.17.
Similar to the single-precision format, the extreme values of e(0 and 2047) are
reserved for the same purpose.Exponent E2 Exponent E1
SubtractMantissa M2 Mantissa M1
Divide
Result normalization and round logic
Result Exponent Result Mantissa
Figure 4.15 FP division
Figure 4.16 IEEE single-precision representation78 COMPUTER ARITHMETICA number of attributes characterizing the IEEE single- and double-precision
formats are summarized in Table 4.2.
4.4. SUMMARY
In this chapter, we have discussed a number of issues related to computer arithmetic.
Our discussion started with an introduction to number representation and radix con-version techniques. We then discussed integer arithmetic and, in particular, we dis-cussed the four main operations, that is, addition, subtraction, multiplication, anddivision. In each case, we have shown basic architectures and organization. Thelast topic discussed in the chapter has been ﬂoating-point representation and arith-metic. We have also shown the basic architectures needed to perform basic ﬂoat-
ing-point operations such as addition, subtraction, multiplication, and division.
We ended our discussion in the chapter with the IEEE ﬂoating-point numberrepresentation.
EXERCISES
1. Represent the decimal values 26, 2123 as signed, 10-bit numbers using each
of the following binary formats:
(a) Sign-and-magnitude;
(b) 2’s complement.
2. Compute the decimal value of the binary number 1011 1101 0101 0110 if the
given number represents unsigned integer. Repeat if the number represents
2’s complement. Repeat if the number represents sign-magnitude integer.
Figure 4.17 Double-precision representation
TABLE 4.2 Characteristics of the IEEE Single and Double
Floating-Point Formats
Characteristic Single-precision Double-precision
Length in bits 32 64
Fraction part in bits 23 52
Hidden bits 1 1
Exponent length in bits 8 11
Bias 127 1023
Approximate range 2128/C253:8/C2103821024/C259:0/C210307
Smallest normalized number 2/C0126/C2510/C0382/C01022/C2510/C0308EXERCISES 793. Consider the binary numbers in the following addition and subtraction pro-
blems to be signed 6-bit values in the 2’s complement representation. Per-
form each of the following operations, specifying whether overﬂow occurs.
010110 011001 110111 100001 111111 011010
þ001001þ010000þ101011 2011101 2000111 2100010
4. Multiply each of the following pairs of signed 2’s complement numbers
using the 2-bit Booth algorithm.
M¼010111 M¼110011 M¼110101 M¼1111
Q¼110110 Q¼101100 Q¼011011 Q¼1111
5. Divide each of the following pairs of signed 20s complement numbers using
both the restoring and the nonrestoring algorithms.
X¼010111 X¼110011 X¼110101 X¼1111
D¼110110 D¼101100 D¼011011 D¼1111
6. Show how to perform addition, subtraction, multiplication, and division of
the following ﬂoating numbers.
A¼ 0 10001 011011
B¼ 1 01111 101010
The numbers are represented in a 12-bit format using a base b¼2, a 5-bit
exponent ewith a bias ¼16, and 6-bit normalized mantissa m.
7. Show a complete design (in terms of the logic equations) for a 4-bit adder /
subtractor using carry-look-ahead technique for all carries c1,c2,c3,c4.
Assume that the two 4-bit input numbers are A¼a4a3a2a1and
B¼b4b3b2b1.
8. Design a BCD adder using a 4-bit binary adder and the least number of logic
gates. The adder should receive two 4-bit numbers A and B and should pro-
duce 4-bit sum and a carry output.
9. Show a design of a 16-bit CLA that uses the 4-bit CLA block shown in
Figure 4.5. Compute the delay and the area (in terms of the number oflogic gates required).
10. Compare the longest path delay from input to output of a 32-bit adder using
4-bit CLA adder blocks in a multilevel architecture with that of a 32-bit CRTadder. Assume that a gate delay is given by T
g.
11. Convert each of the following decimal numbers to their IEEE single-
precision ﬂoating-point counterparts.
(a)276
(b) 0.92
(c) 5.312580 COMPUTER ARITHMETIC(d)20.000072
(e) 8.04/C21021
12. Convert the following IEEE single-precision ﬂoating-point numbers to their
decimal counterparts.
(a) 6589 00000
(b) 807B 00000H
(c) CDEF 0000H
13. Complete the logic design of the array multiplier shown in Figure 4.6.
14. Design the control logic shown in Figure 4.7.
15. Provide a complete logic design for the Control Logic indicated in
Figure 4.10.
REFERENCES AND FURTHER READING
C. Hamacher, Z. Vranesic and S. Zaky, Computer Organization , 5th ed., McGraw-Hill,
New York, 2002.
V. Heuring and H. Jordan, Computer Systems Design and Archiecture , Addison Wesley
Longman, NJ, USA, 1997.
K. Israel, Computer Arithmetic Algorithms , 2nd ed., A. K. Peters, Ltd., Massachusetts, 2002.
W. Stallings, Computer Organization and Architectures: Designing for Performance , 4th ed.,
Prentice-Hall, NJ, USA, 1996.
B. Wilkinson, Computer Architecture: Design and Performance , 2nd ed., Prentice-Hall,
Hertfordshire, UK, 1996.REFERENCES AND FURTHER READING 81& CHAPTER 5
Processing Unit Design
In previous chapters, we studied the history of computer systems and the fundamen-
tal issues related to memory locations, addressing modes, assembly language, andcomputer arithmetic. In this chapter, we focus our attention on the main componentof any computer system, the central processing unit (CPU). The primary function ofthe CPU is to execute a set of instructions stored in the computer’s memory. Asimple CPU consists of a set of registers, an arithmetic logic unit (ALU), and a con-trol unit (CU). In what follows, the reader will be introduced to the organization andmain operations of the CPU.
5.1. CPU BASICS
A typical CPU has three major components: (1) register set, (2) arithmetic logic
unit (ALU), and (3) control unit (CU). The register set differs from one computerarchitecture to another. It is usually a combination of general-purpose and special-purpose registers. General-purpose registers are used for any purpose, hence the
name general purpose. Special-purpose registers have speciﬁc functions within
the CPU. For example, the program counter (PC) is a special-purpose registerthat is used to hold the address of the instruction to be executed next. Anotherexample of special-purpose registers is the instruction register (IR), which isused to hold the instruction that is currently executed. The ALU provides the cir-cuitry needed to perform the arithmetic, logical and shift operations demanded ofthe instruction set. In Chapter 4, we have covered a number of arithmetic oper-ations and circuits used to support computation in an ALU. The control unit is
the entity responsible for fetching the instruction to be executed from the main
memory and decoding and then executing it. Figure 5.1 shows the main com-ponents of the CPU and its interactions with the memory system and the input /
output devices.
The CPU fetches instructions from memory, reads and writes data from and to
memory, and transfers data from and to input /output devices. A typical and
83Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.simple execution cycle can be summarized as follows:
1. The next instruction to be executed, whose address is obtained from the PC, is
fetched from the memory and stored in the IR.
2. The instruction is decoded.
3. Operands are fetched from the memory and stored in CPU registers, if needed.
4. The instruction is executed.
5. Results are transferred from CPU registers to the memory, if needed.
The execution cycle is repeated as long as there are more instructions to execute.
A check for pending interrupts is usually included in the cycle. Examples of inter-
rupts include I /O device request, arithmetic overﬂow, or a page fault (see Chapter 7).
When an interrupt request is encountered, a transfer to an interrupt handling routinetakes place. Interrupt handling routines are programs that are invoked to collect thestate of the currently executing program, correct the cause of the interrupt, and
restore the state of the program.
The actions of the CPU during an execution cycle are deﬁned by micro-orders
issued by the control unit. These micro-orders are individual control signals sent
over dedicated control lines. For example, let us assume that we want to execute aninstruction that moves the contents of register Xto register Y.Let us also assume
that both registers are connected to the data bus, D.The control unit will issue a con-
trol signal to tell register Xto place its contents on the data bus D.After some delay,
another control signal will be sent to tell register Yto read from data bus D.The acti-
vation of the control signals is determined using either hardwired control or micropro-gramming. These concepts are explained later in this chapter.RegistersControl UnitCPUMemory System
Input / OutputData Instructions
ALU
Figure 5.1 Central processing unit main components and interactions with the memory
and I /O84 PROCESSING UNIT DESIGNThe remainder of this chapter is organized as follows. Section 5.2 presents the
register set and explains the different types of registers and their functions. In Sec-
tion 5.3, we will understand what is meant by datapath and control. CPU instructioncycle and the control unit will be covered in Sections 5.4 and 5.5, respectively.
5.2. REGISTER SET
Registers are essentially extremely fast memory locations within the CPU that are
used to create and store the results of CPU operations and other calculations. Differ-ent computers have different register sets. They differ in the number of registers, reg-ister types, and the length of each register. They also differ in the usage of each
register. General-purpose registers can be used for multiple purposes and assigned
to a variety of functions by the programmer. Special-purpose registers are restrictedto only speciﬁc functions. In some cases, some registers are used only to hold dataand cannot be used in the calculations of operand addresses. The length of a dataregister must be long enough to hold values of most data types. Some machinesallow two contiguous registers to hold double-length values. Address registersmay be dedicated to a particular addressing mode or may be used as address generalpurpose. Address registers must be long enough to hold the largest address. The
number of registers in a particular architecture affects the instruction set design.
A very small number of registers may result in an increase in memory references.Another type of registers is used to hold processor status bits, or ﬂags. These bitsare set by the CPU as the result of the execution of an operation. The status bitscan be tested at a later time as part of another operation.
5.2.1. Memory Access Registers
Two registers are essential in memory write and read operations: the memory data
register (MDR) and memory address register (MAR). The MDR and MAR are used
exclusively by the CPU and are not directly accessible to programmers.
In order to perform a write operation into a speciﬁed memory location, the MDR
and MAR are used as follows:
1. The word to be stored into the memory location is ﬁrst loaded by the CPU into
MDR.
2. The address of the location into which the word is to be stored is loaded by the
CPU into a MAR.
3. A write signal is issued by the CPU.
Similarly, to perform a memory read operation, the MDR and MAR are used as
follows:
1. The address of the location from which the word is to be read is loaded into
the MAR.5.2. REGISTER SET 852. A read signal is issued by the CPU.
3. The required word will be loaded by the memory into the MDR ready for use
by the CPU.
5.2.2. Instruction Fetching Registers
Two main registers are involved in fetching an instruction for execution: the pro-
gram counter (PC) and the instruction register (IR). The PC is the register that con-
tains the address of the next instruction to be fetched. The fetched instruction is
loaded in the IR for execution. After a successful instruction fetch, the PC is updatedto point to the next instruction to be executed. In the case of a branch operation, thePC is updated to point to the branch target instruction after the branch is resolved,
that is, the target address is known.
5.2.3. Condition Registers
Condition registers, or ﬂags, are used to maintain status information. Some architec-
tures contain a special program status word (PSW) register. The PSW contains bitsthat are set by the CPU to indicate the current status of an executing program. Theseindicators are typically for arithmetic operations, interrupts, memory protectioninformation, or processor status.
5.2.4. Special-Purpose Address Registers
Index Register As covered in Chapter 2, in index addressing, the address of the
operand is obtained by adding a constant to the content of a register, called the index
register. The index register holds an address displacement. Index addressing is indi-
cated in the instruction by including the name of the index register in parentheses
and using the symbol Xto indicate the constant to be added.
Segment Pointers As we will discuss in Chapter 6, in order to support segmen-
tation, the address issued by the processor should consist of a segment number (base)and a displacement (or an offset) within the segment. A segment register holds theaddress of the base of the segment.
Stack Pointer As shown in Chapter 2, a stack is a data organization mechanism
in which the last data item stored is the ﬁrst data item retrieved. Two speciﬁc oper-
ations can be performed on a stack. These are the Push and the Popoperations. A
speciﬁc register, called the stack pointer (SP), is used to indicate the stack location
that can be addressed. In the stack push operation, the SP value is used to indicate thelocation (called the top of the stack). After storing (pushing) this value, the SP is
incremented (in some architectures, e.g. X86, the SP is decremented as the stack
grows low in memory).86 PROCESSING UNIT DESIGN5.2.5. 80 386 Registers
As discussed in Chapter 3, the Intel basic programming model of the 386, 486, and the
Pentium consists of three register groups. These are the general-purpose registers, the
segment registers, and the instruction pointer (program counter) and the ﬂag register.
Figure 5.2 (which repeats Fig. 3.6) shows the three sets of registers. The ﬁrst set
consists of general purpose registers A, B, C, D, SI (source index), DI (destinationindex), SP (stack pointer), and BP (base pointer). The second set of registers consistsof CS (code segment), SS (stack segment), and four data segment registers DS, ES,FS, and GS. The third set of registers consists of the instruction pointer (programcounter) and the ﬂags (status) register. Among the status bits, the ﬁrst ﬁve are iden-tical to those bits introduced as early as in the 8085 8-bit microprocessor. The next
6–11 bits are identical to those introduced in the 8086. The ﬂags in the bits 12–14
were introduced in the 80286 while the 16–17 bits were introduced in the 80386.The ﬂag in bit 18 was introduced in the 80486.
5.2.6. MIPS Registers
The MIPS CPU contains 32 general-purpose registers that are numbered 0–31.
Register xis designated by $x. Register $zero always contains the hardwired
value 0. Table 5.1 lists the registers and describes their intended use. Registers$at (1), $k0 (26), and $k1 (27) are reserved for use by the assembler and operatingsystem. Registers $a0–$a3 (4–7) are used to pass the ﬁrst four arguments to routines
Figure 5.2 The main register sets in 80 /C286 (80386 and above extended all 16 bit registers
except segment registers)5.2. REGISTER SET 87(remaining arguments are passed on the stack). Registers $v0 and $v1 (2, 3) are used
to return values from functions. Registers $t0–$t9 (8–15, 24, 25) are caller-saved
registers used for temporary quantities that do not need to be preserved acrosscalls. Registers $s0–$s7 (16–23) are calle-saved registers that hold long-livedvalues that should be preserved across calls.
Register $sp(29) is the stack pointer, which points to the last location in use on the
stack. Register $fp(30) is the frame pointer. Register $ra(31) is written with thereturn address for a function call. Register $gp(28) is a global pointer that pointsinto the middle of a 64 K block of memory in the heap that holds constants and
global variables. The objects in this heap can be quickly accessed with a single
load or store instruction.TABLE 5.1 MIPS General-Purpose Registers
Name Number Usage Name Number Usage
zero 0 Constant 0 s0 16 Saved temporary
(preserved across call)
at 1 Reserved for assembler s1 17 Saved temporary
(preserved across call)
v0 2 Expression evaluation
ands2 18 Saved temporary
(preserved across call)
v1 3 results of a function s3 19 Saved temporary
(preserved across call)
a0 4 Argument 1 s4 20 Saved temporary
(preserved across call)
a1 5 Argument 2 s5 21 Saved temporary
(preserved across call)
a2 6 Argument 3 s6 22 Saved temporary
(preserved across call)
a3 7 Argument 4 s7 23 Saved temporary
(preserved across call)
t0 8 Temporary (not
preserved across call)t8 24 Temporary (not
preserved across call)
t1 9 Temporary (not
preserved across call)t9 25 Temporary (not
preserved across call)
t2 10 Temporary (not
preserved across call)k0 26 Reserved for OS kernel
t3 11 Temporary (not
preserved across call)k1 27 Reserved for OS kernel
t4 12 Temporary (not
preserved across call)gp 28 Pointer to global area
t5 13 Temporary (not
preserved across call)sp 29 Stack pointer
t6 14 Temporary (not
preserved across call)fp 30 Frame pointer
t7 15 Temporary (not
preserved across call)ra 31 Return address (used by
function call)88 PROCESSING UNIT DESIGN5.3. DATAPATH
The CPU can be divided into a data section and a control section. The data section,
which is also called the datapath, contains the registers and the ALU. The datapath iscapable of performing certain operations on data items. The control section is basi-cally the control unit, which issues control signals to the datapath. Internal to theCPU, data move from one register to another and between ALU and registers.
Internal data movements are performed via local buses, which may carry data,
instructions, and addresses. Externally, data move from registers to memory andI/O devices, often by means of a system bus. Internal data movement among
registers and between the ALU and registers may be carried out using differentorganizations including one-bus, two-bus, or three-bus organizations. Dedicateddatapaths may also be used between components that transfer data between them-selves more frequently. For example, the contents of the PC are transferred to theMAR to fetch a new instruction at the beginning of each instruction cycle. Hence,
a dedicated datapath from the PC to the MAR could be useful in speeding up this
part of instruction execution.
5.3.1. One-Bus Organization
Using one bus, the CPU registers and the ALU use a single bus to move outgoing
and incoming data. Since a bus can handle only a single data movement withinone clock cycle, two-operand operations will need two cycles to fetch the operandsfor the ALU. Additional registers may also be needed to buffer data for the ALU.
This bus organization is the simplest and least expensive, but it limits the
amount of data transfer that can be done in the same clock cycle, which will slowdown the overall performance. Figure 5.3 shows a one-bus datapath consisting ofa set of general-purpose registers, a memory address register (MAR), a memorydata register (MDR), an instruction register (IR), a program counter (PC), andan ALU.
General
Purpose
RegistersPC
IR
MAR
MDRALUA B
Memory
BusProgram Counter (PC), and ALU
Figure 5.3 One-bus datapath5.3. DATAPATH 895.3.2. Two-Bus Organization
Using two buses is a faster solution than the one-bus organization. In this case, gen-
eral-purpose registers are connected to both buses. Data can be transferred from two
different registers to the input point of the ALU at the same time. Therefore, a two-
operand operation can fetch both operands in the same clock cycle. An additionalbuffer register may be needed to hold the output of the ALU when the two busesare busy carrying the two operands. Figure 5.4 ashows a two-bus organization.
In some cases, one of the buses may be dedicated for moving data into registers(in-bus ), while the other is dedicated for transferring data out of the registers
(out-bus ). In this case, the additional buffer register may be used, as one of the
ALU inputs, to hold one of the operands. The ALU output can be connected directly
to the in-bus, which will transfer the result into one of the registers. Figure 5.4 b
shows a two-bus organization with in-bus and out-bus.
5.3.3. Three-Bus Organization
In a three-bus organization, two buses may be used as source buses while the third
is used as destination. The source buses move data out of registers ( out-bus ), and
(a)
(b)PC
IR
MAR
MDRALUAIn-bus
Out-busGeneral
Purpose
Registers
General
Purpose
RegistersPC
IR
MAR
MDRALU
A
Memory
Memory BusBus  1 
Bus 2
Figure 5.4 Two-bus organizations. ( a) An Example of Two-Bus Datapath. ( b) Another
Example of Two-Bus Datapath with in-bus and out-bus90 PROCESSING UNIT DESIGNthe destination bus may move data into a register ( in-bus ). Each of the two out-buses is
connected to an ALU input point. The output of the ALU is connected directly to the
in-bus. As can be expected, the more buses we have, the more data we can movewithin a single clock cycle. However, increasing the number of buses will also increase
the complexity of the hardware. Figure 5.5 shows an example of a three-bus datapath.
5.4. CPU INSTRUCTION CYCLE
The sequence of operations performed by the CPU during its execution of instruc-
tions is presented in Fig. 5.6. As long as there are instructions to execute, the nextinstruction is fetched from main memory. The instruction is executed based onthe operation speciﬁed in the opcode ﬁeld of the instruction. At the completion of
the instruction execution, a test is made to determine whether an interrupt has
occurred. An interrupt handling routine needs to be invoked in case of an interrupt.
Figure 5.5 Three-bus datapath
Figure 5.6 CPU functions5.4. CPU INSTRUCTION CYCLE 91The basic actions during fetching an instruction, executing an instruction, or hand-
ling an interrupt are deﬁned by a sequence of micro-operations. A group of controlsignals must be enabled in a prescribed sequence to trigger the execution of a micro-operation. In this section, we show the micro-operations that implement instructionfetch, execution of simple arithmetic instructions, and interrupt handling.
5.4.1. Fetch Instructions
The sequence of events in fetching an instruction can be summarized as follows:
1. The contents of the PC are loaded into the MAR.
2. The value in the PC is incremented. (This operation can be done in parallel
with a memory access.)
3. As a result of a memory read operation, the instruction is loaded into the MDR.4. The contents of the MDR are loaded into the IR.
Let us consider the one-bus datapath organization shown in Fig. 5.3. We will see
that the fetch operation can be accomplished in three steps as shown in the table
below, where t
0,t1,t2. Note that multiple operations separated by “;” imply
that they are accomplished in parallel.
Step Micro-operation
t0 MAR /C0(PC); A /C0(PC)
t1 MDR /C0Mem[MAR]; PC /C0(A)þ4
t2 IR /C0(MDR)
Using the three-bus datapath shown in Figure 5.5, the following table shows the
steps needed.
Step Micro-operationt
0 MAR /C0(PC); PC /C0(PC)þ4
t1 MDR /C0Mem[MAR]
t2 IR /C0(MDR)
5.4.2. Execute Simple Arithmetic Operation
Add R 1,R2,R0This instruction adds the contents of source registers R1andR2,a n d
stores the results in destination register R0. This addition can be executed as follows:
1. The registers R0,R1,R2, are extracted from the IR.
2. The contents of R1andR2are passed to the ALU for addition.
3. The output of the ALU is transferred to R0.92 PROCESSING UNIT DESIGNUsing the one-bus datapath shown in Figure 5.3, this addition will take three steps
as shown in the following table, where t0,t1,t2.
Step Micro-operation
t0 A /C0(R1)
t1 B /C0(R2)
t2 R0 /C0(A)þ(B)
Using the two-bus datapath shown in Figure 5.4 a, this addition will take two steps
as shown in the following table, where t0,t1.
Step Micro-operation
t0 A /C0(R1)þ(R2)
t1 R0 /C0(A)
Using the two-bus datapath with in-bus and out-bus shown in Figure 5.4 b, this
addition will take two steps as shown below, where t0,t1.
Step Micro-operationt
0 A /C0(R1)
t1 R0 /C0(A)þ(R2)
Using the three-bus datapath shown in Figure 5.5, this addition will take only one
step as shown in the following table.
Step Micro-operationt
0 R0 /C0(R1)þ(R2)
Add X, R 0This instruction adds the contents of memory location Xto register R0
and stores the result in R0. This addition can be executed as follows:
1. The memory location Xis extracted from IR and loaded into MAR.
2. As a result of memory read operation, the contents of Xare loaded into MDR.
3. The contents of MDR are added to the contents of R0.
Using the one-bus datapath shown in Figure 5.3, this addition will take ﬁve steps
as shown below, where t0,t1,t2,t3,t4.5.4. CPU INSTRUCTION CYCLE 93Step Micro-operation
t0 MAR /C0X
t1 MDR /C0Mem[MAR]
t2 A /C0(R0)
t3 B /C0(MDR )
t4 R0 /C0(A)þ(B)
Using the two-bus datapath shown in Figure 5.4 a, this addition will take four
steps as shown below, where t0,t1,t2,t3.
Step Micro-operation
t0 MAR /C0X
t1 MDR /C0Mem[MAR]
t2 A /C0(R0)þ(MDR )
t3 R0 /C0(A)
Using the two-bus datapath with in-bus and out-bus shown in Figure 5.4 b, this
addition will take four steps as shown below, where t0,t1,t2,t3.
Step Micro-operation
t0 MAR /C0X
t1 MDR /C0Mem[MAR]
t2 A /C0(R0)
t3 R0 /C0(A)þ(MDR )
Using the three-bus datapath shown in Figure 5.5, this addition will take three
steps as shown below, where t0,t1,t2.
Step Micro-operationt
0 MAR /C0X
t1 MDR /C0Mem[MAR]
t2 R0 /C0R0þ(MDR )
5.4.3. Interrupt Handling
After the execution of an instruction, a test is performed to check for pending inter-
rupts. If there is an interrupt request waiting, the following steps take place:
1. The contents of PC are loaded into MDR (to be saved).
2. The MAR is loaded with the address at which the PC contents are to be saved.
3. The PC is loaded with the address of the ﬁrst instruction of the interrupt hand-
ling routine.94 PROCESSING UNIT DESIGN4. The contents of MDR (old value of the PC) are stored in memory.
The following table shows the sequence of events, where t1,t2,t3.
Step Micro-operation
t1 MDR /C0(PC)
t2 MAR /C0address1 (where to save old PC);
PC /C0address2 (interrupt handling routine)
t3 Mem[MAR] /C0(MDR)
5.5. CONTROL UNIT
The control unit is the main component that directs the system operations by sending
control signals to the datapath. These signals control the ﬂow of data within the CPUand between the CPU and external units such as memory and I /O. Control buses
generally carry signals between the control unit and other computer components
in a clock-driven manner. The system clock produces a continuous sequence of
pulses in a speciﬁed duration and frequency. A sequence of steps t
0,t1,t2,...,
Figure 5.7 Timing of control signals5.5. CONTROL UNIT 95(t0,t1,t2,...) are used to execute a certain instruction. The op-code ﬁeld of a
fetched instruction is decoded to provide the control signal generator with infor-
mation about the instruction to be executed. Step information generated by alogic circuit module is used with other inputs to generate control signals. Thesignal generator can be speciﬁed simply by a set of Boolean equations for itsoutput in terms of its inputs. Figure 5.7 shows a block diagram that describes howtiming is used in generating control signals.
There are mainly two different types of control units: microprogrammed and
hardwired. In microprogrammed control, the control signals associated with oper-
ations are stored in special memory units inaccessible by the programmer as controlwords. A control word is a microinstruction that speciﬁes one or more micro-operations. A sequence of microinstructions is called a microprogram, which isstored in a ROM or RAM called a control memory CM.
In hardwired control, ﬁxed logic circuits that correspond directly to the Boolean
expressions are used to generate the control signals. Clearly hardwired control is
faster than microprogrammed control. However, hardwired control could be very
expensive and complicated for complex systems. Hardwired control is more econ-omical for small control units. It should also be noted that microprogrammed controlcould adapt easily to changes in the system design. We can easily add new instruc-tions without changing hardware. Hardwired control will require a redesign of theentire systems in the case of any change.
Example 1 Let us revisit the add operation in which we add the contents of source
registers R
1,R2, and store the results in destination register R0. We have shown
earlier that this operation can be done in one step using the three-bus datapath
shown in Figure 5.5.
Let us try to examine the control sequence needed to accomplish this addition
at step t0. Suppose that the op-code ﬁeld of the current instruction was decoded to
Inst-x type. First we need to select the source registers and the destination register,then we select Add as the ALU function to be performed. The following tableshows the needed step and the control sequence.
Step Instruction type Micro-operation Control
t0 Inst-x R0 /C0(R1)þ(R2) Select R1as source 1 on
out-bus1 (R 1out-bus1)
Select R2as source 2 on
out-bus2 (R 2out-bus2)
Select R0as destination
on in-bus (R 0in-bus)
Select the ALU function
Add (Add)
Figure 5.8 shows the signals generated to execute Inst-x during time period t0.
The AND gate ensures that these signals will be issued when the op-code is decoded
into Inst-x and during time period t0. The signals (R 1out-bus 1), (R 2out-bus2),96 PROCESSING UNIT DESIGN(R0in-bus), and (Add) will select R1as a source on out-bus1, R2as a source on out-
bus2, R0as destination on in-bus, and select the ALUs add function, respectively.
Example 2 Let us repeat the operation in the previous example using the one-bus
datapath shown in Fig. 5.3. We have shown earlier that this operation can be carried
out in three steps using the one-bus datapath. Suppose that the op-code ﬁeld of the
current instruction was decoded to Inst-x type. The following table shows the neededsteps and the control sequence.
Step Instruction type Micro-operation
t0 Inst-x A /C0(R1) Select R1as source (R 1out)
Select Aas destination (A in)
t1 Inst-x B /C0(R2) Select R2as source (R 2out)
Select Bas destination (B in)
t2 Inst-x R0 /C0(A)þ(B) Select the ALU function Add (Add)
Select R0as destination (R 0in)
Figure 5.9 shows the signals generated to execute Inst-x during time periods t0,
t1, and t2. The AND gates ensure that the appropriate signals will be issued when the
op-code is decoded into Inst-x and during the appropriate time period. During t0, the
signals (R 1out) and (A in) will be issued to move the contents of R 1into A. Similarly
during t1, the signals (R 2out) and (B in) will be issued to move the contents of R 2
into B. Finally, the signals (R 0in) and (Add) will be issued during t2to add the con-
tents of A and B and move the results into R 0.
5.5.1. Hardwired Implementation
In hardwired control, a direct implementation is accomplished using logic cir-
cuits. For each control line, one must ﬁnd the Boolean expression in terms of theinput to the control signal generator as shown in Figure 5.7. Let us explain theIn-bus
Out-bus 1ALU
Out-bus 2t0
Inst-xR0
R1 R2R0in-bus
R2out-bus2R2out-bus1Add
Figure 5.8 Signals generated to execute Inst-x on three-bus datapath during time period t05.5. CONTROL UNIT 97implementation using a simple example. Assume that the instruction set of a
machine has the three instructions: Inst-x, Inst-y, and Inst-z; and A, B, C, D, E, F,
G, and H are control lines. The following table shows the control lines thatshould be activated for the three instructions at the three steps t
0,t1, and t2.
Step Inst-x Inst-y Inst-z
t0 D, B, E F, H, G E, H
t1 C, A, H G D, A, C
t2 G, C B, C
The Boolean expressions for control lines A, B, and C can be obtained as follows:
A¼Inst-x/C1t1þInst-z/C1t1¼(Inst-xþInst-z)/C1t1
B¼Inst-x/C1t0þInst-y/C1t2
C¼Inst-x/C1t1þInst-x/C1t2þInst-y/C1t2þInst-z/C1t1
¼(Inst-xþInst-z)/C1t1þ(Inst-xþInst-y)/C1t2
Figure 5.10 shows the logic circuits for these control lines. Boolean expressions
for the rest of the control lines can be obtained in a similar way. Figure 5.11 shows
the state diagram in the execution cycle of these instructions.
5.5.2. Microprogrammed Control Unit
The idea of microprogrammed control units was introduced by M. V. Wilkes in the
early 1950s. Microprogramming was motivated by the desire to reduce the complex-
ities involved with hardwired control. As we studied earlier, an instruction is
Figure 5.9 Signals generated to execute Inst-x on one-bus datapath during time period t0,t1,t298 PROCESSING UNIT DESIGNInst-x
Inst-z
Inst-x
Inst-y
Inst-y
t2t2
Inst-x
t0t1A
C
B
Figure 5.10 Logic circuits for control lines A, B, and C
Fetch the next
instruction
Decode
D, B, E
C, A, H
G, CF, H, G
G
B, CE, H
D, A, Ct0
t1
t2t0
t1
t2t0
t1Inst-x Inst-z Inst-y
Figure 5.11 Instruction execution state diagram5.5. CONTROL UNIT 99implemented using a set of micro-operations. Associated with each micro-operation
is a set of control lines that must be activated to carry out the corresponding micro-operation. The idea of microprogrammed control is to store the control signalsassociated with the implementation of a certain instruction as a microprogram ina special memory called a control memory (CM). A microprogram consists of asequence of microinstructions. A microinstruction is a vector of bits, where eachbit is a control signal, condition code, or the address of the next microinstruction.
Microinstructions are fetched from CM the same way program instructions are
fetched from main memory (Fig. 5.12).
When an instruction is fetched from memory, the op-code ﬁeld of the instruc-
tion will determine which microprogram is to be executed. In other words, theop-code is mapped to a microinstruction address in the control memory. Themicroinstruction processor uses that address to fetch the ﬁrst microinstruction inthe microprogram. After fetching each microinstruction, the appropriate controllines will be enabled. Every control line that corresponds to a “1” bit should be
turned on.Every control line that corresponds to a “0” bit should be left off.
After completing the execution of one microinstruction, a new microinstruction
will be fetched and executed. If the condition code bits indicate that a branchmust be taken, the next microinstruction is speciﬁed in the address bits of the cur-rent microinstruction. Otherwise, the next microinstruction in the sequence will befetched and executed.
The length of a microinstruction is determined based on the number of micro-
operations speciﬁed in the microinstructions, the way the control bits will be
interpreted, and the way the address of the next microinstruction is obtained. A
microinstruction may specify one or more micro-operations that will be activatedsimultaneously. The length of the microinstruction will increase as the number ofparallel micro-operations per microinstruction increases. Furthermore, when eachcontrol bit in the microinstruction corresponds to exactly one control line, thelength of microinstruction could get bigger. The length of a microinstructioncould be reduced if control lines are coded in speciﬁc ﬁelds in the microinstruction.
Decoders will be needed to map each ﬁeld into the individual control lines. Clearly,
using the decoders will reduce the number of control lines that can be activated sim-
ultaneously. There is a tradeoff between the length of the microinstructions and theamount of parallelism. It is important that we reduce the length of microinstructionsto reduce the cost and access time of the control memory. It may also be desirablethat more micro-operations be performed in parallel and more control lines can beactivated simultaneously.
External
inputControl
Control AddressControl data
Register SequencerControl
Memory
Figure 5.12 Fetching microinstructions (control words)100 PROCESSING UNIT DESIGNHorizontal Versus Vertical Microinstructions Microinstructions can be
classiﬁed as horizontal orvertical. Individual bits in horizontal microinstructions
correspond to individual control lines. Horizontal microinstructions are long and
allow maximum parallelism since each bit controls a single control line. Invertical microinstructions, control lines are coded into speciﬁc ﬁelds within amicroinstruction. Decoders are needed to map a ﬁeld of kbits to 2
kpossible com-
binations of control lines. For example, a 3-bit ﬁeld in a microinstruction could be
used to specify any one of eight possible lines. Because of the encoding, vertical
microinstructions are much shorter than horizontal ones. Control lines encodedin the same ﬁeld cannot be activated simultaneously. Therefore, vertical micro-
instructions allow only limited parallelism. It should be noted that no decoding
is needed in horizontal microinstructions while decoding is necessary in thevertical case.
Example 3 Consider the three-bus datapath shown in Figure 5.5. In addition to
the PC, IR, MAR, and MDR, assume that there are 16 general-purpose registers
numbered R
0–R15. Also, assume that the ALU supports eight functions (add, sub-
tract, multiply, divide, AND, OR, shift left, and shift right). Consider the add oper-
ation Add R1,R2,R0, which adds the contents of source registers R1,R2, and store
the results in destination register R0. In this example, we will study the format of the
microinstruction under horizontal organization.
We will use horizontal microinstructions, in which there is a control bit for
each control line. The format of the microinstruction should have control bits forthe following:
.ALU operations
.Registers that output to out-bus1 (source 1)
.Registers that output to out-bus2 (source 2)
.Registers that input from in-bus (destination)
.Other operations that are not shown here
The following table shows the number of bits needed for ALU, Source 1,
Source 2, and destination:
Purpose Number of bits Explanations
ALU 8 bits 8 functions
Source 1 20 bits 16 general-purpose registers þ4 special-
purpose registers
Source 2 16 bits 16 general-purpose registersDestination 20 bits 16 general-purpose registers þ4 special-
purpose registers
Figure 5.13 is the microinstruction for Add R1,R2,R0on the three-bus datapath.5.5. CONTROL UNIT 101Example 4 In this example, we will use vertical microinstructions, in which
decoders will be needed. We will use a three-bus datapath as shown in Figure 5.5.
Assume that there are 16 general-purpose registers and that the ALU supports eight
functions. The following tables show the encoding for ALU functions, registersconnected to out-bus 1 (Source 1), registers connected to out-bus 2 (Source 2),and registers connected to in-bus (Destination).
Purpose Number of bits Explanations
ALU 4 bits 8 functions þnone
Source 1 5 bits 16 general-purpose registers þ4 special-purpose
registersþnone
Source 2 5 bits 16 general-purpose registers þnone
Destination 5 bits 16 general-purpose registers þ4 special-purpose
registersþnone
Encoding ALU function
0000 None (ALU will connect out-bus1 to in-bus)
0001 Add
0010 Subtract
0011 Multiple
0100 Divide
0101 AND
0110 OR
0111 Shift left
1000 Shift right
Encoding Source 1 Destination Encoding Source 2
00000 R 0 R0 00000 R 0
00001 R 1 R1 00001 R 1
00010 R 2 R2 00010 R 2
00011 R 3 R3 00011 R 3
00100 R 4 R4 00100 R 4
00101 R 5 R5 00101 R 5
00110 R 6 R6 00110 R 6
00111 R 7 R7 00111 R 7
01000 R 8 R8 01000 R 8
01001 R 9 R9 01001 R 9
01010 R 10 R10 01010 R 10
Figure 5.13 Microinstruction for Add R1,R2,R0102 PROCESSING UNIT DESIGNEncoding Source 1 Destination Encoding Source 2
01011 R 11 R11 01011 R 11
01100 R 12 R12 01100 R 12
01101 R 13 R13 01101 R 13
01110 R 14 R14 01110 R 14
01111 R 15 R15 01111 R 15
10000 PC PC 10000 None
10001 IR IR
10010 MAR MAR
10011 MDR MDR
10100 NONE NONE
Figure 5.14 is the microinstruction for Add R1,R2,R0using the three-bus data-
path under vertical organization:
Example 5 Using the same encoding of Example 4, let us ﬁnd vertical microin-
structions used in fetching an instruction.
MAR PC First, we need to select PC as source 1 by using “10000” for source 1
ﬁeld. Similarly, we select MAR as our destination by using “10010” in the destina-
tion ﬁeld. We also need to use “0000” for the ALU ﬁeld, which will be decoded to
“NONE”. As shown in the ALU encoding table (Example 4), “NONE” means that
out-bus1 will be connected to in-bus. The ﬁeld source 2 will be set to “10000”,which means none of the registers will be selected. The microinstruction is shownin Figure 5.15.
Memory Read and Write Memory operations can easily be accommodated by
adding 1 bit for read and another for write. The two microinstructions in
Figure 5.16 perform memory read and write, respectively.
Fetch Fetching an instruction can be done using the three microinstructions of
Figure 5.17.
The ﬁrst and second microinstructions have been shown above. The third microin-
struction moves the contents of the MDR to IR (IR  MDR). MDR is selected as
source 1 by using “10011” for source 1 ﬁeld. Similarly, IR is selected as the destina-
tion by using “10001” in the destination ﬁeld. We also need to use “0000” (“NONE”)
Figure 5.14 Microinstruction for Add R1,R2,R0
Figure 5.15 Microinstruction for MAR /C0PC5.5. CONTROL UNIT 103in the ALU ﬁeld, which means that out-bus1 will be connected to in-bus. The ﬁeld
source 2 will be set to “10000”, which means none of the registers will be selected.
5.6. SUMMARY
The CPU is the part of a computer that interprets and carries out the instructions con-
tained in the programs we write. The CPU’s main components are the register ﬁle,
ALU, and the control unit. The register ﬁle contains general-purpose and special reg-
isters. General-purpose registers may be used to hold operands and intermediate results.The special registers may be used for memory access, sequencing, status information,or to hold the fetched instruction during decoding and execution. Arithmetic and logi-cal operations are performed in the ALU. Internal to the CPU, data may move from oneregister to another or between registers and ALU. Data may also move between theCPU and external components such as memory and I /O. The control unit is the com-
ponent that controls the state of the instruction cycle. As long as there are instructions to
execute, the next instruction is fetched from main memory. The instruction is executed
based on the operation speciﬁed in the op-code ﬁeld of the instruction. The control unitgenerates signals that control the ﬂow of data within the CPU and between the CPU andexternal units such as memory and I /O. The control unit can be implemented using
hardwired or microprogramming techniques.
EXERCISES
1. How many instruction bits are required to specify the following:
(a) Two operand registers and one result register in a machine that has 64
general-purpose registers?
Figure 5.16 Microinstructions for memory read and write
Figure 5.17 Microinstructions for fetching an instruction104 PROCESSING UNIT DESIGN(b) Three memory addresses in a machine with 64 KB of main memory?
2. Show the micro-operations of the load,store , and jump instructions using:
(a) One-bus system
(b) Two-bus system
(c) Three-bus system
3. Add control signals to all the tables in Section 5.4.
4. Data movement within the CPU can be performed in several different
ways. Contrast the following methods in terms of their advantages and
disadvantages:
(a) Dedicated connections
(b) One-bus datapath
(c) Two-bus datapath
(d) Three-bus datapath
5. Find a method of encoding the microinstructions described by the following
table so that the minimum number of control bits is used and all inherent
parallelism among the microoperations is preserved.
Microinstruction Control signals activated
I1 a,b,c,d,e
I2 a,d,f,g
I3 b,h
I4 c
I5 c,e,g,i
I6 a,h,j
6. Suppose that the instruction set of a machine has three instructions: Inst-1,
Inst-2, and Inst-3; and A, B, C, D, E, F, G, and H are the control lines.
The following table shows the control lines that should be activated forthe three instructions at the three steps T0, T1, and T2.
Step Inst-1 Inst-2 Inst-3
T0 D, B, E F, H, G E, H
T1 C, A, H G D, A, CT2 G, C B, C
(a) Hardwired approach:
(i) Write Boolean expressions for all the control lines A–G.
(ii) Draw the logic circuit for each control line.
(b) Microprogramming approach:
(i) Assuming a horizontal representation, write down the micropro-
gram for instructions Inst-1. Indicate the microinstruction size.EXERCISES 105(ii) If we allow both horizontal and vertical representation, what would
be the best grouping? What is the microinstruction size? Write the
microprogram of Inst-1.
7. A certain processor has a microinstruction format containing 10 separate
control ﬁelds C0:C9. Each Cican activate any one of nidistinct control
lines, where niis speciﬁed as follows:
i: 012 3 4 5 6789
ni:4431 191 67182 2
(a) What is the minimum number of control bits needed to represent the 10
control ﬁelds?
(b) What is the maximum number of control bits needed if a purely horizon-
tal format is used for all control information?
8. What are the main differences between the following pairs?
(a) Vertical and horizontal microinstructions
(b) Microprogramming and hardwired control
9. Using the single-bus architecture, generate the necessary control signals,
in the proper order (with minimum number of micro-instructions), forconditional branch instruction.
10. Write a micro-program for the fetch instruction using the one-bus datapath
and the two-bus datapath.
REFERENCES AND FURTHER READING
R. J. Baron and L. Higbie, Computer Architecture , Addison Wesley, Canada, 1992.
M. J. Flynn, Computer Architecture , Jones and Barlett, MA, USA, 1995.
J. P. Hayes, Computer Architecture and Organization , McGraw-Hill, New York, 1998.
J. Hennessy and D. Patterson, Computer Architecture: A Quantitative Approach , Morgan
Kaufmann, San Francisco, CA, 2003.
V. P. Heuring and H. F. Jordan, Computer Systems Design and Architecture , Addison Wesley,
NJ, 1997.
M. Murdocca and V. Heuring, Principles of Computer Architecture , Prentice Hall, NJ,
USA, 2000.
D. Patterson and J. Hennessy, Computer Organization and Design , Morgan Kaufmann,
San Mateo, CA, 1998.
W. Stallings, Computer Organization and Architecture: Designing for Performance , NJ, 1996.
A. S. Tanenbaum, Structured Computer Organization , Prentice Hall, NJ, USA, 1999.106 PROCESSING UNIT DESIGN& CHAPTER 6
Memory System Design I
In this chapter, we study the computer memory system. It was stated in Chapter 3
that without a memory no information can be stored or retrieved in a computer. Itis interesting to observe that as early as 1946 it was recognized by Burks, Goldstine,and Von Neumann that a computer memory has to be organized in a hierarchy. Insuch a hierarchy, larger and slower memories are used to supplement smaller andfaster ones. This observation has since then proven essential in constructing a com-puter memory. If we put aside the set of CPU registers (as the ﬁrst level for storingand retrieving information inside the CPU, see Chapter 5), then a typical memory
hierarchy starts with a small, expensive, and relatively fast unit, called the cache .
The cache is followed in the hierarchy by a larger, less expensive, and relatively
slow main memory unit. Cache and main memory are built using solid-state semi-
conductor material. They are followed in the hierarchy by far larger, less expensive,and much slower magnetic memories that consist typically of the (hard) disk and thetape. Our deliberation in this chapter starts by discussing the characteristics and fac-tors inﬂuencing the success of a memory hierarchy of a computer. We then direct ourattention to the design and analysis of cache memory. Discussion on the (main)
memory unit is conducted in Chapter 7. Also discussed in Chapter 7 are the
issues related to virtual memory design. A brief coverage of the different read-
only memory (ROM) implementations is also provided in Chapter 7.
6.1. BASIC CONCEPTS
In this section, we introduce a number of fundamental concepts that relate to the
memory hierarchy of a computer.
6.1.1. Memory Hierarchy
As mentioned above, a typical memory hierarchy starts with a small, expensive, and
relatively fast unit, called the cache , followed by a larger, less expensive, and rela-
tively slow main memory unit. Cache and main memory are built using solid-state
107Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.semiconductor material (typically CMOS transistors). It is customary to call the fast
memory level the primary memory . The solid-state memory is followed by larger, less
expensive, and far slower magnetic memories that consist typically of the (hard) diskand the tape. It is customary to call the disk the secondary memory , while the tape is con-
ventionally called the tertiary memory . The objective behind designing a memory hier-
archy is to have a memory system that performs as if it consists entirely of the fastest unitand whose cost is dominated by the cost of the slowest unit.
The memory hierarchy can be characterized by a number of parameters. Among these
parameters are the access type ,capacity ,cycle time ,latency ,bandwidth ,a n d cost.T h e
term access refers to the action that physically takes place during a read orwrite oper-
ation. The capacity of a memory level is usually measured in bytes. The cycle time isdeﬁned as the time elapsed from the start of a read operation to the start of a subsequentread. The latency is deﬁned as the time interval between the request for information andthe access to the ﬁrst bit of that information. The bandwidth provides a measure of thenumber of bits per second that can be acces sed. The cost of a memory level is usually
speciﬁed as dollars per megabytes. Figure 6.1 depicts a typical memory hierarchy.Table 6.1 provides typical values of the memory hierarchy parameters.
The term random access refers to the fact that any access to any memory location
takes the same ﬁxed amount of time regardless of the actual memory location and /or
the sequence of accesses that takes place. For example, if a write operation to
memory location 100 takes 15 ns and if this operation is followed by a read oper-
ation to memory location 3000, then the latter operation will also take 15 ns. This
is to be compared to sequential access in which if access to location 100 takes
500 ns, and if a consecutive access to location 101 takes 505 ns, then it is expected
that an access to location 300 may take 1500 ns. This is because the memory has tocycle through locations 100 to 300, with each location requiring 5 ns.
The effectiveness of a memory hierarchy depends on the principle of moving
information into the fast memory infrequently and accessing it many times beforereplacing it with new information. This principle is possible due to a phenomenoncalled locality of reference ; that is, within a given period of time, programs tend
to reference a relatively conﬁned area of memory repeatedly. There exist two
forms of locality: spatial and temporal locality. Spatial locality refers to the
CPU Registers
Cache
Latency
Main MemoryBandwidth
Secondary Storage (Disk)
Speed
Cost per bit TertiaryStorage (Tape)
Capacity (megabytes)
Figure 6.1 Typical memory hierarchy108 MEMORY SYSTEM DESIGN Iphenomenon that when a given address has been referenced, it is most likely that
addresses near it will be referenced within a short period of time, for example, consecu-tive instructions in a straightline program. Temporal locality , on the other hand, refers
to the phenomenon that once a particular memory item has been referenced, it is mostlikely that it will be referenced next, for example, an instruction in a program loop.
The sequence of events that takes place when the processor makes a request for an
item is as follows. First, the item is sought in the ﬁrst memory level of the memoryhierarchy. The probability of ﬁnding the requested item in the ﬁrst level is calledthehit ratio ,h
1. The probability of not ﬁnding (missing) the requested item in the
ﬁrst level of the memory hierarchy is called the miss ratio ,( 12h1). When the
requested item causes a “ miss,” it is sought in the next subsequent memory level.
The probability of ﬁnding the requested item in the second memory level, the hitratio of the second level, is h
2. The miss ratio of the second memory level is
(1/C0h2). The process is repeated until the item is found. Upon ﬁnding the requested
item, it is brought and sent to the processor. In a memory hierarchy that consists ofthree levels, the average memory access time can be expressed as follows:
t
av¼h1/C2t1þ(1/C0h1)½t1þh2/C2t2þ(1/C0h2)(t2þt3)/C138
¼t1þ(1/C0h1)½t2þ(1/C0h2)t3/C138
The average access time of a memory level is deﬁned as the time required to accesso n ew o r di nt h a tl e v e l .I nt h i se q u a t i o n , t
1,t2,t3represent, respectively, the access
times of the three levels.
6.2. CACHE MEMORY
Cache memory owes its introduction to Wilkes back in 1965. At that time, Wilkes
distinguished between two types of main memory: The conventional and the slave
memory . In Wilkes terminology, a slave memory is a second level of unconventional
high-speed memory, which nowadays corresponds to what is called cache memory
(the term cache means a safe place for hiding or storing things).
The idea behind using a cache as the ﬁrst level of the memory hierarchy is to keep
the information expected to be used more frequently by the CPU in the cacheTABLE 6.1 Memory Hierarchy Parameters
Access type Capacity Latency Bandwidth Cost /MB
CPU registers Random 64–1024 bytes 1–10 ns System clock
rateHigh
Cache memory Random 8–512 KB 15–20 ns 10–20 MB /s $500
Main memory Random 16–512 MB 30–50 ns 1–2 MB /s $20–50
Disk memory Direct 1–20 GB 10–30 ms 1–2 MB /s $0.25
Tape memory Sequential 1–20 TB 30–10,000 ms 1–2 MB /s $0.0256.2. CACHE MEMORY 109(a small high-speed memory that is near the CPU). The end result is that at any given
time some active portion of the main memory is duplicated in the cache. Therefore,when the processor makes a request for a memory reference, the request is ﬁrstsought in the cache. If the request corresponds to an element that is currently resid-ing in the cache, we call that a cache hit. On the other hand, if the request corre-sponds to an element that is not currently in the cache, we call that a cache miss.Acache hit ratio ,h
c, is deﬁned as the probability of ﬁnding the requested element
in the cache. A cache miss ratio (1/C0hc) is deﬁned as the probability of not ﬁnding
the requested element in the cache.
In the case that the requested element is not found in the cache, then it has to be
brought from a subsequent memory level in the memory hierarchy. Assuming thatthe element exists in the next memory level, that is, the main memory, then it hasto be brought and placed in the cache. In expectation that the next requested elementwill be residing in the neighboring locality of the current requested element (spatiallocality), then upon a cache miss what is actually brought to the main memory is a
block of elements that contains the requested element. The advantage of transferring
a block from the main memory to the cache will be most visible if it could be poss-
ible to transfer such a block using one main memory access time. Such a possibilitycould be achieved by increasing the rate at which information can be transferredbetween the main memory and the cache. One possible technique that is used toincrease the bandwidth is memory interleaving . To achieve best results, we can
assume that the block brought from the main memory to the cache, upon a cachemiss, consists of elements that are stored in different memory modules, that is,
whereby consecutive memory addresses are stored in successive memory modules.
Figure 6.2 illustrates the simple case of a main memory consisting of eight memorymodules. It is assumed in this case that the block consists of 8 bytes.
Having introduced the basic idea leading to the use of a cache memory, we would
like to assess the impact of temporal and spatial locality on the performance ofthe memory hierarchy. In order to make such an assessment, we will limit our
M7M6 M5M4 M3M2M1M0
Byte
Main memory
Block
Cache
Figure 6.2 Memory interleaving using eight modules110 MEMORY SYSTEM DESIGN Ideliberation to the simple case of a hierarchy consisting only of two levels, that is,
the cache and the main memory. We assume that the main memory access time is tm
and the cache access time is tc. We will measure the impact of locality in terms of the
average access time, deﬁned as the average time required to access an element (aword) requested by the processor in such a two-level hierarchy.
6.2.1. Impact of Temporal Locality
In this case, we assume that instructions in program loops, which are executed many
times, for example, ntimes, once loaded into the cache, are used more than once
before they are replaced by new instructions. The average access time, t
av,i sg i v e nb y
tav¼ntcþtm
n¼tcþtm
n
In deriving the above expression, it was assumed that the requested memory elementhas created a cache miss, thus leading to the transfer of a main memory block in timet
m. Following that, naccesses were made to the same requested element, each taking
tc. The above expression reveals that as the number of repeated accesses, n,i n c r e a s e s ,
the average access time decreases, a desirable feature of the memory hierarchy.
6.2.2. Impact of Spatial Locality
In this case, it is assumed that the size of the block transferred from the main
memory to the cache, upon a cache miss, is melements. We also assume that due
to spatial locality, all melements were requested, one at a time, by the processor.
Based on these assumptions, the average access time, tav, is given by
tav¼mtcþtm
m¼tcþtm
m
In deriving the above expression, it was assumed that the requested memory elementhas created a cache miss, thus leading to the transfer of a main memory block, con-sisting of melements, in time t
m. Following that, maccesses, each for one of the
elements constituting the block, were made. The above expression reveals that asthe number of elements in a block, m, increases, the average access time decreases,
a desirable feature of the memory hierarchy.
6.2.3. Impact of Combined Temporal and Spatial Locality
In this case, we assume that the element requested by the processor created a cache
miss leading to the transfer of a block, consisting of m elements , to the cache (that
take t
m). Now, due to spatial locality, all melements constituting a block were
requested, one at a time, by the processor (requiring mtc). Following that, the orig-
inally requested element was accessed ( n21) times (temporal locality), that is, a6.2. CACHE MEMORY 111total of ntimes access to that element. Based on these assumptions, the average
access time, tav, is given by
tav¼mtcþtm
m/C16/C17
þ(n/C01)tc
n¼tcþtm
m/C16/C17
þ(n/C01)tc
n¼tm
nmþtc
A further simplifying assumption to the above expression is to assume that tm¼mtc.
In this case the above expression will simplify to
tav¼mtc
nmþtc¼tcþtc
n¼nþ1
ntc
The above expression reveals that as the number of repeated accesses nincreases, the
average access time will approach tc. This is a signiﬁcant performance improvement.
It should be clear from the above discussion that as more requests for items that
do not exist in the cache (cache miss) occur, more blocks would have to be brought
to the cache. This should raise two basic questions: Where to place an incomingmain memory block in the cache? And in the case where the cache is totallyﬁlled, which cache block should the incoming main memory block replace? Place-ment of incoming blocks and replacement of existing blocks are performed accord-ing to speciﬁc protocols (algorithms). These protocols are strongly related to theinternal organization of the cache. Cache internal organization is discussed in the
following subsections. However, before discussing cache organization, we ﬁrst
introduce the cache-mapping function.
6.2.4. Cache-Mapping Function
Without loss of generality, we present cache-mapping function taking into consider-
ation the interface between two successive levels in the memory hierarchy: primarylevel and secondary level. If the focus is on the interface between the cache and mainmemory, then the cache represents the primary level, while the main memory rep-resents the secondary level. The same principles apply to the interface between anytwo memory levels in the hierarchy. In the following discussion, we focus our atten-tion to the interface between the cache and the main memory.
It should be noted that a request for accessing a memory element is made by the
processor through issuing the address of the requested element. The address issued
by the processor may correspond to that of an element that exists currently in thecache (cache hit); otherwise, it may correspond to an element that is currently resid-ing in the main memory. Therefore, address translation has to be made in order todetermine the whereabouts of the requested element. This is one of the functions
performed by the memory management unit (MMU). A schematic of the addressmapping function is shown in Figure 6.3.
In this ﬁgure, the system address represents the address issued by the processor
for the requested element. This address is used by an address translation functioninside the MMU. If address translation reveals that the issued address correspondsto an element currently residing in the cache, then the element will be made112 MEMORY SYSTEM DESIGN Iavailable to the processor. If, on the other hand, the element is not currently in the
cache, then it will be brought (as part of a block) from the main memory and placed
in the cache and the element requested is made available to the processor.
6.2.5. Cache Memory Organization
There are three main different organization techniques used for cache memory. The
three techniques are discussed below. These techniques differ in two main aspects:
1. The criterion used to place, in the cache, an incoming block from the main
memory.
2. The criterion used to replace a cache block by an incoming block (on cache full).
Direct Mapping This is the simplest among the three techniques. Its simplicity
stems from the fact that it places an incoming main memory block into a speciﬁcﬁxed cache block location. The placement is done based on a ﬁxed relation between
the incoming block number, i, the cache block number, j, and the number of cache
blocks, N:
j¼imod N
Example 1 Consider, for example, the case of a main memory consisting of 4K
blocks, a cache memory consisting of 128 blocks, and a block size of 16 words.
Figure 6.4 shows the division of the main memory and the cache according to thedirect-mapped cache technique.
As the ﬁgure shows, there are a total of 32 main memory blocks that map to
a given cache block. For example, main memory blocks 0, 128, 256, 384, ...,
3968 map to cache block 0. We therefore call the direct-mapping technique aThe requested elementMemory Management
Unit (MMU) SecondaryLevel
Block
Primary LevelAddress inPrimary MemoryMiss
SystemAddressTranslationFunction
Hit
Figure 6.3 Address mapping operation6.2. CACHE MEMORY 113many-to-one mapping technique. The main advantage of the direct-mapping tech-
nique is its simplicity in determining where to place an incoming main memoryblock in the cache. Its main disadvantage is the inefﬁcient use of the cache. Thisis because according to this technique, a number of main memory blocks may com-pete for a given cache block even if there exist other empty cache blocks. This dis-advantage should lead to achieving a low cache hit ratio.
According to the direct-mapping technique the MMU interprets the address issued
by the processor by dividing the address into three ﬁelds as shown in Figure 6.5. Thelengths, in bits, of each of the ﬁelds in Figure 6.5 are:
1. Word ﬁeld ¼log
2B, where Bis the size of the block in words.
2. Block ﬁeld ¼log2N, where Nis the size of the cache in blocks.
3. Tag ﬁeld ¼log2(M/N), where Mis the size of the main memory in blocks.
4. The number of bits in the main memory address ¼log2(B/C2M)
It should be noted that the total number of bits as computed by the ﬁrst three
equations should add up to the length of the main memory address. This can be
used as a check for the correctness of your computation.
Example 2 Compute the above four parameters for Example 1.
Word ﬁeld ¼log2B¼log216¼log224¼4 bits
Block ﬁeld ¼log2N¼log2128¼log227¼7 bits
Tag ﬁeld ¼log2(M/N)¼log2(22/C2210/27)¼5 bits
The number of bits in the main memory address ¼log2(B/C2M)¼log2
(24/C2212)¼16 bits.3Tag
1
0
31384Cache
1290
1
2
126
127 4095Main Memory
0
1
2128
129
130256
257
258384
385
3863968
127 255 383 4095
0123 3 1
Figure 6.4 Mapping main memory blocks to cache blocks
Figure 6.5 Direct-mapped address ﬁelds114 MEMORY SYSTEM DESIGN IHaving shown the division of the main memory address, we can now proceed to
explain the protocol used by the MMU to satisfy a request made by the processor for
accessing a given element. We illustrate the protocol using the parameters given inthe example presented above (Fig. 6.6).
The steps of the protocol are:
1. Use the Block ﬁeld to determine the cache block that should contain the element
requested by the processor. The Block ﬁeld is used directly to determine the
cache block sought, hence the name of the technique: direct-mapping.
2. Check the corresponding Tag memory to see whether there is a match between
its content and that of the Tag ﬁeld . A match between the two indicates that the
targeted cache block determined in step 1 is currently holding the main
memory element requested by the processor, that is, a cache hit .
3. Among the elements contained in the cache block, the targeted element can be
selected using the Word ﬁeld .
4. If in step 2, no match is found, then this indicates a cache miss . Therefore, the
required block has to be brought from the main memory, deposited in the cache,
and the targeted element is made available to the processor. The cache Tag
memory and the cache block memory have to be updated accordingly.
Figure 6.6 Direct-mapped address translation6.2. CACHE MEMORY 115The direct-mapping technique answers not only the placement of the incoming main
memory block in the cache question, but it also answers the replacement question.Upon encountering a totally ﬁlled cache while a new main memory block has to bebrought, the replacement is trivial and determined by the equation j¼imod N.
The main advantages of the direct-mapping technique is its simplicity measured
in terms of the direct determination of the cache block; no search is needed. It is alsosimple in terms of the replacement mechanism. The main disadvantage of the tech-
nique is its expected poor utilization of the cache memory. This is represented in
terms of the possibility of targeting a given cache block, which requires frequentreplacement of blocks while the rest of the cache is not used. Consider, for example,the sequence of requests made by the processor for elements held in the mainmemory blocks 1, 33, 65, 97, 129, and 161. Consider also that the cache size is32 blocks. It is clear that all the above blocks map to cache block number 1. There-fore, these blocks will compete for that same cache block despite the fact that theremaining 31 cache blocks are not used.
The expected poor utilization of the cache by the direct mapping technique is
mainly due to the restriction on the placement of the incoming main memoryblocks in the cache (the many-to-one property). If such a restriction is relaxed,that is, if we make it possible for an incoming main memory block to be placedin any empty (available) cache block, then the resulting technique would be so ﬂex-ible that efﬁcient utilization of the cache would be possible. Such a ﬂexible tech-nique, called the Associative Mapping technique, is explained next.
Fully Associative Mapping According to this technique, an incoming main
memory block can be placed in any available cache block. Therefore, the addressissued by the processor need only have two ﬁelds. These are the TagandWord
ﬁelds. The ﬁrst uniquely identiﬁes the block while residing in the cache. Thesecond ﬁeld identiﬁes the element within the block that is requested by the pro-cessor. The MMU interprets the address issued by the processor by dividing itinto two ﬁelds as shown in Figure 6.7. The length, in bits, of each of the ﬁelds inFigure 6.7 are given by:
1. Word ﬁeld ¼log
2B, where Bis the size of the block in words
2. Tag ﬁeld ¼log2M, where Mis the size of the main memory in blocks
3. The number of bits in the main memory address ¼log2(B/C2M)
It should be noted that the total number of bits as computed by the ﬁrst two equations
should add up to the length of the main memory address. This can be used as a check
for the correctness of your computation.
Figure 6.7 Associative-mapped address ﬁelds116 MEMORY SYSTEM DESIGN IExample 3 Compute the above three parameters for a memory system having the
following speciﬁcation: size of the main memory is 4K blocks, size of the cache is
128 blocks, and the block size is 16 words. Assume that the system uses associativemapping.
Word ﬁeld ¼log
2B¼log216¼log224¼4 bits
Tag ﬁeld ¼log2M¼log227/C2210¼12 bits
The number of bits in the main memory address ¼log2(B/C2M)¼log2
(24/C2212)¼16 bits.
Having shown the division of the main memory address, we can now proceed to
explain the protocol used by the MMU to satisfy a request made by the processor for
accessing a given element. We illustrate the protocol using the parameters given in
the example presented above (see Fig. 6.8). The steps of the protocol are:
1. Use the Tag ﬁeld to search in the Tag memory for a match with any of the tags
stored.
2. A match in the tag memory indicates that the corresponding targeted cache
block determined in step 1 is currently holding the main memory elementrequested by the processor, that is, a cache hit .
Figure 6.8 Associative-mapped address translation6.2. CACHE MEMORY 1173. Among the elements contained in the cache block, the targeted element can be
selected using the Word ﬁeld .
4. If in step 2, no match is found, then this indicates a cache miss . Therefore, the
required block has to be brought from the main memory, deposited in the ﬁrst
available cache block, and the targeted element (word) is made available tothe processor. The cache Tag memory and the cache block memory have to
be updated accordingly.
It should be noted that the search made in step 1 above requires matching the tag
ﬁeld of the address with each and every entry in the tag memory . Such a search, if
done sequentially, could lead to a long delay. Therefore, the tags are stored in an
associative (content addressable) memory . This allows the entire contents of the
tag memory to be searched in parallel (associatively), hence the name, associativemapping.
It should be noted that, regardless of the cache organization used, a mechanism is
needed to ensure that any accessed cache block contains valid information. The val-
idity of the information in a cache block can be checked via the use of a single bit for
each cache block, called the valid bit . The valid bit of a cache block should be
updated in such a way that if valid bit ¼1, then the corresponding cache block car-
ries valid information; otherwise, the information in the cache block is invalid.
When a computer system is powered up, all valid bits are made equal to 0, indicatingthat they carry invalid information. As blocks are brought to the cache, their statusesare changed accordingly to indicate the validity of the information contained.
The main advantage of the associative-mapping technique is the efﬁcient use of
the cache. This stems from the fact that there exists no restriction on where to placeincoming main memory blocks. Any unoccupied cache block can potentially beused to receive those incoming main memory blocks. The main disadvantage ofthe technique, however, is the hardware overhead required to perform the associat-ive search conducted in order to ﬁnd a match between the tag ﬁeld and the tagmemory as discussed above.
A compromise between the simple but inefﬁcient direct cache organization and
the involved but efﬁcient associative cache organization can be achieved by con-
ducting the search over a limited set of cache blocks while knowing ahead of
time where in the cache an incoming main memory block is to be placed. This isthe basis for the set-associative mapping technique explained next.
Set-Associative Mapping In the set-associative mapping technique, the cache
is divided into a number of sets. Each set consists of a number of blocks. A given
main memory block maps to a speciﬁc cache set based on the equations¼imod S, where Sis the number of sets in the cache, iis the main memory
block number, and sis the specﬁc cache set to which block imaps. However, an
incoming block maps to any block in the assigned cache set. Therefore, the address
issued by the processor is divided into three distinct ﬁelds. These are the Tag,Set,
andWord ﬁelds. The Setﬁeld is used to uniquely identify the speciﬁc cache set118 MEMORY SYSTEM DESIGN Ithat ideally should hold the targeted block. The Tagﬁeld uniquely identiﬁes the tar-
geted block within the determined set. The Word ﬁeld identiﬁes the element (word)
within the block that is requested by the processor. According to the set-associative
mapping technique, the MMU interprets the address issued by the processor bydividing it into three ﬁelds as shown in Figure 6.9. The length, in bits, of each ofthe ﬁelds of Figure 6.9 is given by
1. Word ﬁeld ¼log
2B, where Bis the size of the block in words
2. Set ﬁeld ¼log2S, where Sis the number of sets in the cache
3. Tag ﬁeld ¼log2(M/S), where Mis the size of the main memory in blocks.
S¼N=Bs, where Nis the number of cache blocks and Bsis the number of
blocks per set
4. The number of bits in the main memory address ¼log2(B/C2M)
It should be noted that the total number of bits as computed by the ﬁrst three
equations should add up to the length of the main memory address. This can be
used as a check for the correctness of your computation.
Example 4 Compute the above three parameters (Word, Set, and Tag) for a
memory system having the following speciﬁcation: size of the main memory is
4K blocks, size of the cache is 128 blocks, and the block size is 16 words.
Assume that the system uses set-associative mapping with four blocks per set.
S¼128
4¼32 sets :
1. Word ﬁeld ¼log2B¼log216¼log224¼4 bits
2. Set ﬁeld ¼log232¼5 bits
3. Tag ﬁeld ¼log2(4/C2210/32)¼7 bits
The number of bits in the main memory address ¼log2(B/C2M)¼log2
(24/C2212)¼16 bits.
Having shown the division of the main memory address, we can now proceed to
explain the protocol used by the MMU to satisfy a request made by the processor for
accessing a given element. We illustrate the protocol using the parameters given in
the example presented above (see Fig. 6.10). The steps of the protocol are:
1. Use the Set ﬁeld (5 bits) to determine (directly) the speciﬁed set (1 of the
32 sets).
Figure 6.9 Set-associative-mapped address ﬁelds6.2. CACHE MEMORY 1192. Use the Tag ﬁeld to ﬁnd a match with any of the (four) blocks in the deter-
mined set. A match in the tag memory indicates that the speciﬁed set deter-
mined in step 1 is currently holding the targeted block, that is, a cache hit .
3. Among the 16 words (elements) contained in hit cache block, the requested
word is selected using a selector with the help of the Word ﬁeld .Ta gCache
Block # 0Main Memory Address
Tag Field
75 4Set Field Word Field
03 1
Step #1
Step #2
Associative Search
Over the setBlock # 1
Block # 2
Block # 3
7 bitsSet 0
Set 1
Selector
Requested WordSet i
Set 311271 1290 384
Figure 6.10 Set-associative-mapped address translation120 MEMORY SYSTEM DESIGN I4. If in step 2, no match is found, then this indicates a cache miss . Therefore,
the required block has to be brought from the main memory, deposited in
the speciﬁed set ﬁrst, and the targeted element (word) is made available tothe processor. The cache Tag memory and the cache block memory have to
be updated accordingly.
It should be noted that the search made in step 2 above requires matching the tag
ﬁeldof the address with each and every entry in the tag memory for the speciﬁed set.
Such a search is performed in parallel (associatively) over the set, hence the name,set-associative mapping. The hardware overhead required to performing the associ-ative search within a set in order to ﬁnd a match between the tag ﬁeld and the tagmemory is not as complex as that used in the case of the fully associative technique.
The set-associative-mapping technique is expected to produce a moderate cache
utilization efﬁciency, that is, not as efﬁcient as the fully associative technique andnot as poor as the direct technique. However, the technique inherits the simplicity
of the direct mapping technique in terms of determining the target set.
An overall qualitative comparison among the three mapping techniques is shown
in Table 6.2. Owing to its moderate complexity and moderate cache utilization, the
set-associative technique is used in the Intel Pentium line of processors.
The discussion above shows how the associative-mapping and the set-associative
techniques answer the question about the placement of the incoming main memoryblock in the cache. The other important question that was posed at the beginning of
the discussion on cache memory is that of replacement. Speciﬁcally, upon encoun-
tering a totally ﬁlled cache while a new main memory block has to be brought, which
of the cache blocks should be selected for replacement? This is discussed below.
6.2.6. Replacement Techniques
A number of replacement techniques can be used. These include a randomly selected
block ( random selection ), the block that has been in the cache the longest ( ﬁrst-in-
ﬁrst-out, FIFO ), and the block that has been used the least while residing in the
cache ( least recently used, LRU ).
Let us assume that when a computer system is powered up, a random number
generator starts generating numbers between 0 and ( N21). As the name indicates,
TABLE 6.2 Qualitative Comparison Among Cache Mapping
Techniques
Mapping
technique SimplicityAssociative
tag searchExpected
cache
utilizationReplacement
technique
Direct Yes None Low Not neededAssociative No Involved High Yes
Set-associative Moderate Moderate Moderate Yes6.2. CACHE MEMORY 121random selection of a cache block for replacement is done based on the output of the
random number generator at the time of replacement. This technique is simple anddoes not require much additional overhead. However, its main shortcoming is that itdoes not take locality into consideration. Random techniques have been found effec-tive enough such that they have been ﬁrst used by Intel in its iAPX microprocessor
series.
The FIFO technique takes the time spent by a block in the cache as a measure for
replacement. The block that has been in the cache the longest is selected for replace-ment regardless of the recent pattern of access to the block. This technique requireskeeping track of the lifetime of a cache block. Therefore, it is not as simple as therandom selection technique. Intuitively, the FIFO technique is reasonable to usefor straightline programs where locality of reference is not of concern.
According to the LRU replacement technique, the cache block that has been
recently used the least is selected for replacement. Among the three replacementtechniques, the LRU technique is the most effective. This is because the history
of block usage (as the criterion for replacement) is taken into consideration. The
LRU algorithm requires the use of a cache controller circuit that keeps track of refer-ences to all blocks while residing in the cache. This can be achieved through anumber of possible implementations. Among these implementations is the use ofcounters. In this case each cache block is assigned a counter. Upon a cache hit,the counter of the corresponding block is set to 0, all other counters having a smallervalue than the original value in the counter of the hit block are incremented by 1, and
all counters having a larger value are kept unchanged. Upon a cache miss, the block
whose counter is showing the maximum value is chosen for replacement, the counter
is set to 0, and all other counters are incremented by 1.
Having introduced the above three cache mapping technique, we offer the follow-
ing example, which illustrates the main observations made about the three techniques.
Example 5 Consider the case of a 4 /C28 two-dimensional array of numbers, A.
Assume that each number in the array occupies one word and that the array elements
are stored column-major order in the main memory from location 1000 to location
1031. The cache consists of eight blocks each consisting of just two words. Assumealso that whenever needed, LRU replacement policy is used. We would like to exam-
ine the changes in the cache if each of the above three mapping techniques is used asthe following sequence of requests for the array elements are made by the processor:
a
0,0,a0,1,a0,2,a0,3,a0,4,a0,5,a0,6,a0,7
a1,0,a1,1,a1,2,a1,3,a1,4,a1,5,a1,6,a1,7
Solution The distribution of the array elements in the main memory is shown in
Figure 6.11. Shown also is the status of the cache before the above requests were made.
Direct Mapping Table 6.3 shows that there were 16 cache misses (not a single
cache hit) and that the number of replacements made is 12 (these are shown122 MEMORY SYSTEM DESIGN Itinted). It also shows that out of the available eight cache blocks, only four (0, 2, 4,
and 6) are used, while the remaining four are inactive all the time. This represents a50% cache utilization.
Fully Associative Mapping Table 6.4 shows that there were eight cache hits
(50% of the total number of requests) and that there were no replacements made.
It also shows a 100% cache utilization.
Set-Associative Mapping (With Two Blocks per Set) Table 6.5 shows that
there were 16 cache misses (not a single cache hit) and that the number of replace-
ments made is 12 (these are shown tinted). It also shows that of the available four
cache sets, only two sets are used, while the remaining two are inactive all the
time. This represents a 50% cache utilization.
Figure 6.11 Array elements in the main memory6.2. CACHE MEMORY 1236.2.7. Cache Write Policies
Having discussed the main issues related to cache mapping techniques and the repla-
cement policies, we would like to address a very important related issue, that is,cache coherence. Coherence between a cache word and its copy in the mainmemory should be maintained at all times, if at all possible. A number of policies
(techniques) are used in performing write operations to the main memory blocks
while residing in the cache. These policies determine the degree of coherence thatTABLE 6.3 Direct Mapping
RequestCache
hit/missMM
block
number
(i)Cache
block
number
(j)Cache status
BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7
A(0,0) Miss 0 0 0 1
00
A(0,1) Miss 2 2 0 1 0 1
00 11
A(0,2) Miss 4 4 0 1 0 1 0 1
00 11 22
A(0,3) Miss 6 6 0 1 0 1 0 1 0 1
00 11 22 33
A(0,4) Miss 8 0 0 1 0 1 0 1 0 1
44 11 22 33
A(0,5) Miss 10 2 0 1 0 1 0 1 0 1
44 55 22 33
A(0,6) Miss 12 4 0 1 0 1 0 1 0 1
44 55 66 33
A(0,7) Miss 14 6 0 1 0 1 0 1 0 1
44 55 66 77
A(1,0) Miss 0 0 0 1 0 1 0 1 0 1
00 55 66 77
A(1,1) Miss 2 2 0 1 0 1 0 1 0 1
00 11 66 66
A(1,2) Miss 4 4 0 1 0 1 0 1 0 1
00 11 22 66
A(1,3) Miss 6 6 0 1 0 1 0 1 0 1
00 11 22 33
A(1,4) Miss 8 0 0 1 0 1 0 1 0 1
44 11 22 33
A(1,5) Miss 10 2 0 1 0 1 0 1 0 1
44 55 22 33
A(1,6) Miss 12 4 0 1 0 1 0 1 0 1
44 55 66 33
A(1,7) Miss 14 6 0 1 0 1 0 1 0 1
44 55 66 77124 MEMORY SYSTEM DESIGN Ican be maintained between cache words and their counterparts in the main memory.
In the following paragraphs, we discuss these write policies. In particular, we dis-cuss two main cases: cache write policies upon a cache hit and the cache write pol-icies upon a cache miss. We also discuss the cache read policy upon a cache miss.Cache read upon a cache hit is straightforward.
Cache Write Policies Upon a Cache Hit There are essentially two possible
write policies upon a cache hit. These are the write-through and the write-back .TABLE 6.4 Fully Associative Mapping
RequestCache
hit/missMM
block
number
(i)Cache
block
numberCache status
BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7
A(0,0) Miss 0 0 0 1
00
A(0,1) Miss 2 1 0101
0011
A(0,2) Miss 4 2 010101
001122
A(0,3) Miss 6 3 01010101
00112233
A(0,4) Miss 8 4 0101010101
0011223344
A(0,5) Miss 10 5 010101010101
001122334455
A(0,6) Miss 12 6 01010101010101
00112233445566
A(0,7) Miss 14 7 0101010101010101
0011223344556677
A(1,0) Hit 0 0 0101010101010101
0011223344556677
A(1,1) Hit 2 1 0101010101010101
0011223344556677
A(1,2) Hit 4 2 0101010101010101
0011223344556677
A(1,3) Hit 6 3 0101010101010101
0011223344556677
A(1,4) Hit 8 4 0101010101010101
0011223344556677
A(1,5) Hit 10 5 0101010101010101
0011223344556677
A(1,6) Hit 12 6 0101010101010101
0011223344556677
A(1,7) Hit 14 7 0101010101010101
00112233445566776.2. CACHE MEMORY 125In the write-through policy, every write operation to the cache is repeated to the
main memory at the same time. In the write-back policy, all writes are made tothe cache. A write to the main memory is postponed until a replacement isneeded. Every cache block is assigned a bit, called the dirty bit , to indicate that at
least one write operation has been made to the block while residing in the cache.
At replacement time, the dirty bit is checked; if it is set, then the block is written
back to the main memory, otherwise, it is simply overwritten by the incomingTABLE 6.5 Set-Associative Mapping
RequestCache
hit/missMM
block
number
(i)Cache
block
numberCache status
Set # 0 Set # 1 Set # 2 Set # 3
BL0 BL1 BL2 BL3 BL4 BL5 BL6 BL7
A(0,0) Miss 0 0 0 1
00
A(0,1) Miss 2 2 0 1 0 1
00 11
A(0,2) Miss 4 0 0101 01
0022 11
A(0,3) Miss 6 2 0101 0101
0022 1133
A(0,4) Miss 8 0 0 1 0 1 0101
4422 1133
A(0,5) Miss 10 2 0101 01014422 5533
A(0,6) Miss 12 0 0 1 0 1 0101
4466 5533
A(0,7) Miss 14 2 0101 0101
4466 5577
A(1,0) Miss 0 0 0 1 0 1 0101
0066 5577
A(1,1) Miss 2 2 0101 0101
0066 1177
A(1,2) Miss 4 0 0 1 0 1 0101
0022 1177
A(1,3) Miss 6 2 0101 0101
0022 1133
A(1,4) Miss 8 0 0 1 0 1 0101
4422 1133
A(1,5) Miss 10 2 0101 0101
4422 5533
A(1,6) Miss 12 0 0 1 0 1 0101
4466 5533
A(1,7) Miss 14 2 0101 01014466 4422126
MEMORY SYSTEM DESIGN Iblock. The write-through policy maintains coherence between the cache blocks and
their counterparts in the main memory at the expense of the extra time needed towrite to the main memory. This leads to an increase in the average access time.On the other hand, the write-back policy eliminates the increase in the averageaccess time. However, coherence is only guaranteed at the time of replacement.
Cache Write Policy Upon a Cache Miss Two main schemes can be used.
These are write-allocate whereby the main memory block is brought to the cache
and then updated. The other scheme is called write-no-allocate whereby the
missed main memory block is updated while in the main memory and not brought
to the cache.
In general, write-through caches use write-no-allocate policy while write-back
caches use write-allocate policy .
Cache Read Policy Upon a Cache Miss Two possible strategies can be used.
In the ﬁrst, the main memory missed block is brought to the cache while the requiredword is forwarded immediately to the CPU as soon as it is available. In the second
strategy, the missed main memory block is entirely stored in the cache and the
required word is then forwarded to the CPU.
Having discussed the issues related to the design and analysis of cache memory,
we brieﬂy present formulae for the average access time of a memory hierarchy underdifferent cache write policies.
Case No. 1: Cache Write-Through Policy
Write-allocate In this case, the average access time for a memory system is given
by
t
a¼tcþ(1/C0h)tbþw(tm/C0tc)
where tbis the time required to transfer a block to the cache, ( tm/C0tc) is the
additional time incurred due to the write operations, wis the fraction of write oper-
ations. It should be noted that if the data path and organization allow, then tb¼tm;
otherwise, tb¼Btm, where Bis the block size in words.
Write-no-allocate In this case, the average access time can be expressed as
ta¼tcþ(1/C0w)(1/C0h)tbþw(tm/C0tc)
Case No. 2: Cache Write-Back Policy The average access time for a system
that uses a write-back policy is given by ta¼tcþ(1/C0h)tbþwb(1/C0h)tb, where wb
is the probability that a block has been altered while being in the cache.6.2. CACHE MEMORY 1276.2.8. Real-Life Cache Organization Analysis
Intel’s Pentium IV Processor Cache Intel’s Pentium 4 processor uses a two-
level cache organization as shown schematically in Figure 6.12. In this ﬁgure, L1
represents an 8 KB data cache. This is a four-way set-associative. The block size
is 64 bytes. Consider the following example (tailored after the L1 Pentium cache).
Example 6
Cache organization Set-associative
Main Memory size 16 MB
Cache L1 size 8 KB
Number of blocks per set Four
CPU addressing Byte addressable
The main memory address should be divided into three ﬁelds: Word, Set, and Tag
(Fig. 6.13). The length of each ﬁeld is computed as follows.
Number of main memory blocks M¼224=26¼218blocks
Number of cache blocks N¼213=26¼128 blocks
S¼128 =4¼32 sets
Set ﬁeld ¼log232¼5 bits
Word ﬁeld ¼log2B¼log264¼log226¼6 bits
Tag ﬁeld ¼log2(218/25)¼13 bits
Main memory address ¼log2(B/C2M)¼log2(26/C2218)¼24 bits
The second cache level in Figure 6.11 is L2. This is called the advanced transfer
cache . It is organized as an eight-way set-associative cache having a 256 KB total
size and 128-byte block size. Following a similar set of steps as shown above forthe L1 level, we obtain the following:
Number of main memory blocks M¼2
24=27¼217blocks
Processor
Cache
Level 1
L 1Cache
Level 2
L 2Main Memory
Figure 6.12 Pentium IV two-level cache128 MEMORY SYSTEM DESIGN INumber of cache blocks N¼218=27¼211blocks
S¼211=23¼28sets
Set ﬁeld ¼log228¼8 bits
Word ﬁeld ¼log2B¼log2128¼log227¼7 bits
Tag ﬁeld ¼log2(217/28)¼9 bits
The following tables summarize the L1 and L2 Pentium 4 cache performance in
terms of the cache hit ratio and cache latency.
CPUL1 Hit
ratioL2 Hit
ratioL1
LatencyL2
LatencyAverage
latency
Pentium 4 at 1.5 GHz 90% 99% 1.33 ns 6.0 ns 1.8 ns
PowerPC 604 Processor Cache The PowerPC cache is divided into data and
instruction caches, called Harvard Organization . Both the instruction and the data
caches are organized as 16 KB four-way set-associative. The following table sum-marizes the PowerPC 604 cache basic characteristics.
Cache organization Set-associative
Block size 32 bytes
Main memory size 4 GB ( M¼128 Mega blocks)
Cache size 16 KB ( N¼512 blocks)
Number of blocks per set Four
Number of cache sets (S) 128 sets
The main memory address should be divided into three ﬁelds: Word, Set, and Tag
(Fig. 6.13). The length of each ﬁeld is computed as follows:
Number of main memory blocks M¼232=25¼227blocks
Number of cache blocks N¼214=25¼512 blocks
S¼512 =4¼128 sets
Set ﬁeld ¼log2128¼7 bits
Word ﬁeld ¼log2B¼log232¼log225¼5 bits
Tag ﬁeld ¼log2(227/27)¼20 bits
Main memory address ¼log2(B/C2M)¼log2(25/C2227)¼32 bits
Figure 6.13 Division of main memory address6.2. CACHE MEMORY 129PMC-Sierra RM7000A 64-bit MIPS RISC Processor The RM7000 uses a
different cache organization compared to that of the Intel and the PowerPC. In
this case, three separate caches are included. These are:
1. Primary instruction cache: A 16 KB, four-way set-associative cache with
32-byte block size (eight instructions)
2. Primary data cache: A 16 KB, four-way set-associative cache with 32 bytes
block size (eight words)
3. Secondary cache: A 256 KB, four-way set-associative cache for both instruc-
tions and data
In addition to the three on-chip caches, the RM7000 provides a dedicated tertiarycache interface, which supports tertiary cache sizes of 512 KB, 2 MB, and 8 MB.This tertiary cache is only accessed after a secondary cache miss.
The primary caches require one cycle each to access. Each of these caches has
64-bit read data path and 128-bit write data path. Both caches can be accessed sim-ultaneously, giving an aggregate bandwidth of over 4 GB per second. The secondarycache has a 64-bit data path and is accessed only on a primary cache miss. It has a
three-cycle miss penalty. Owing to the unusual cache organization of the RM7000, it
uses the two cache access schemes described below.
Non-Blocking Caches In this scheme, the caches do not stall on a miss, rather the
processor continues to operate out of the primary caches until one of the following
events takes place:
1. Two cache misses are outstanding and a third load /store instruction appears
on the instruction bus.
2. A subsequent instruction requires data from either of the instructions that
caused a cache miss.
The use of nonblocking caches improves the overall performance by allowing the
cache to continue operating even though a cache miss has occurred.
Cache Locking In this scheme, critical code or data segments are locked into the pri-
mary and secondary caches. The locked contents can be updated on a write hit, but
cannot be selected for replacement on a miss. RM7000 allows each of the three
caches to be locked separately. However, only two of the available four sets of eachcache can be locked. In particular, RM7000 allows a maximum of 128 KB of data orcode to be locked in the secondary cache, a maximum of 8 KB of code to be locked
in the instruction cache, and a maximum of 8 KB of data to be locked in the data cache.
6.3. SUMMARY
In this chapter, we consider the design and analysis of the ﬁrst level of a memory
hierarchy, that is, the cache memory. In this context, the locality issues were130 MEMORY SYSTEM DESIGN Idiscussed and their effect on the average access time was explained. Three cache
mapping techniques, namely direct, associative, and set-associative mappingswere analyzed and their performance measures compared. We have also introducedthree replacement techniques: Random, FIFO, and LRU replacement. The impact ofthe three techniques on the cache hit ratio was analyzed. Cache writing policies werealso introduced and analyzed. Our discussion on cache ended with a presentation ofthe cache memory organization and characteristics of three real-life examples:
Pentium IV, PowerPC, and PMC-Sierra RM7000, processors. In Chapter 7, we
will discuss the issues related to the design aspects of the internal and externalorganization of the main memory. We will also discuss the issues related to virtualmemory.
EXERCISES
1. Determine the memory interleaving factor required to obtain an average
access time of less that 60 ns given that the main memory has an access
time of 100 ns and the cache has an access time of 20 ns. What is the averageaccess time of the resulting system?
2. What is the average access time of a system having three levels of memory, a
cache memory, a semiconductor main memory, and a magnetic disk second-ary memory, if the access times of the memories are 20 ns, 100 ns, and 1 ms,respectively. The cache hit ratio is 90% and the main memory hit ratio is 95%.
3. A computer system has an MM consisting of 16 MB 32-bit words. It also has
an 8 KB cache. Assume that the computer uses a byte-addressable mechan-ism. Determine the number of bits in each ﬁeld of the address in each ofthe following organizations:
(a) Direct mapping with block size of one word
(b) Direct mapping with a block size of eight words
(c) Associative mapping with a block size of eight words
(d) Set-associative mapping with a set size of four block and a block size of
one word.
4. Consider the execution of the following program segment on an 8 /C28 array A.
For i: ¼0t o7d o
SUM:¼0
For j: ¼0t o7d o
SUM:¼SUMþA(i,j)
End for
AVE(i): ¼SUM/8
End for
Assume that the main memory is divided into eight interleaved memory
blocks and that each cache memory block consists of eight elements.
Assume also that the cache memory access time is 10 ns and that the
memory access time is ten times the cache memory access time. Computethe average access time per element of the array A.EXERCISES 1315. Consider the execution of the following program segment on a 4 /C210 array
A. The two-dimensional array A is stored in the main memory in a column-
major order. Assume that there are eight blocks in the cache, each is justone word, and that the LRU is used for replacement. Show a trace of the con-tents of the cache memory blocks for the different values of indices jandk
assuming three different cache memory organizations, that is, direct ,associ-
ative , and set-associative mapping . Provide your observations on the results
obtained.
SUM:¼0
For j: ¼0t o9d o
SUM:¼SUMþA(0,j)
End for
AVE:¼SUM/10
For k: ¼0t o9d o
A(0,k): ¼A(0, k) /AVE
End for
6. Consider the case of a 4 /C28 two-dimensional array of numbers, A. Assume
that each number in the array occupies one word and that the array elements
are stored row-major in the main memory for location 1000 to location 1031.The cache consists of eight blocks each consisting of four words. Assume alsothat whenever needed, LRU replacement policy is used. We would like toexamine the changes in the cache if each of the above three mapping tech-
niques is used as the following sequence of requests for the array elements
is made:
a
0,0,a0,1,a0,2,a0,3,a0,4,,a0,5,a0,6,a0,7
a1,0,a1,1,a1,2,a1,3,a1,4,a1,5,a1,6,a1,7
a2,0,a2,1,a2,2,a2,3,a2,4,a2,5,a2,6,a2,7
a3,0,a3,1,a3,2,a3,3,a3,4,a3,5,a3,6,a3,7
Show the status of the cache before and after the given requests were made, the
number of replacements made, and an estimate of the cache utilization.
7. A computer system has an MM consisting of 1 M 16-bit words. It also has a
4 K word cache organized in the block-set-associative manner, with four
blocks per set and 64 words per block. Assume that the cache is initially
empty. Suppose that the CPU fetches 4352 words from locations 0, 1,2,..., 4351 (in that order). It then repeats this fetch sequence nine more
times. If the cache is 10 times faster than the MM, estimate the improvementfactor resulting from the use of the cache. Assume that whenever a block is tobe brought from the MM and the correspondence set in the cache is full,
the new block replaces the least recently used block of this set. Repeat for
the case of using the most recently used replacement technique; that is, if
the cache is full, then the new block will replace the most recently usedblock in the cache. Note: This example is quoted from Reference #3.132 MEMORY SYSTEM DESIGN IREFERENCES AND FURTHER READING
S. D. Burd, Systems Architecture , 3rd ed., Thomson Learning Ltd, Boston, USA, 2001.
H. Cragon, Memory Systems and Pipelined Processors , Jones and Bartlett: Sudbury,
MA, 1996.
V. C. Hamacher, Z. G. Vranesic, and S. G. Zaky, Computer Organization , 5th ed.,
McGraw-Hill, New York, 2002.
J. L. Hennessy, and D. A. Patterson, Computer Architecture: A Quantitative Approach ,
Morgan Kaufmann, San Francisco, CA, 1996.
V. P. Heuring, and H. F. Jordan, Computer Systems Design and Architecture , Addison-
Wesley, NJ, USA, 1997.
D. A. Patterson, and J. L. Hennessy, Computer Organization & Design: The Hardware /
Software Interface , Morgan Kaufmann, San Mateo, CA, 1994.
H. S. Stone, High-Performance Computer Architecture , Addison-Wesley, Amsterdam,
Netherlands, 1987.
M. Wilkes, Slave memories and dynamic storage allocation, IEEE Trans. Electron. Comput.,
EC-14(2), 270–271, (1965).
B. Wilkinson, Computer Architecture: Design and Performance , Prentice-Hall,
Hertfordshire, UK, 1996.
Websites
http://www.sysopt.com
http://www.intel.com
http://www.AcerHardware.com
http://www.pmc-sierra.com /products /details /rm7000a
http://physinfo.ulb.ac.be /divers_html /PowerPC_Programming_Info /into_to_ppc /
ppc2_hardware.htmlREFERENCES AND FURTHER READING 133& CHAPTER 7
Memory System Design II
In Chapter 6 we introduced the concept of memory hierarchy. We have also character-
ized a memory hierarchy in terms of the locality of reference and its impact on the aver-age access time. We then moved on to cover the different issues related to the ﬁrst levelof the hierarchy, that is, the cache memory (the reader is advised to carefully reviewChapter 6 before proceeding with this chapter). In this chapter, we continue our cover-age of the different levels of the memory hierarchy. In particular, we start our discus-sion with the issues related to the design and analysis of the (main) memory unit. Issuesrelated to virtual memory design are then discussed. A brief coverage of the different
read-only memory (ROM) implementations is provided at the end of the chapter.
7.1. MAIN MEMORY
As the name implies, the main memory provides the main storage for a computer.
Figure 7.1 shows a typical interface between the main memory and the CPU.Two CPU registers are used to interface the CPU to the main memory. These are
the memory address register (MAR) and the memory data register (MDR). The
MDR is used to hold the data to be stored and /or retrieved in /from the memory
location whose address is held in the MAR.
It is possible to visualize a typical internal main memory structure as consisting
of rows and columns of basic cells. Each cell is capable of storing one bit of infor-mation. Figure 7.2 provides a conceptual internal organization of a memory chip. Inthis ﬁgure, cells belonging to a given row can be assumed to form the bits of a givenmemory word. Address lines A
n/C01An/C02...A1A0are used as inputs to the address
decoder in order to generate the word select lines W2n/C01...W1W0. A given word
select line is common to all memory cells in the same row. At any given time,the address decoder activates only one word select line while deactivating the
remaining lines. A word select line is used to enable all cells in a row for read or
write. Data (bit) lines are used to input or output the contents of cells. Eachmemory cell is connected to two data lines. A given data line is common to allcells in a given column.
135Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.In static CMOS technology, each main memory cell consists of six transistors as
shown in Figure 7.3. The six transistor static CMOS memory cell consists of two
inverters back to back. It should be noted that the cell could exist in one of thetwo stable states. For example, if in Figure 7.3 A¼1, then transistor N
2will be
on and point B¼0, which in turn will cause transistor P1to be on, thus causing
pointA¼1. This represents a cell stable state, call it state 1. In a similar way
A0
A1
A2
Data LinesAn-1
W2n-1·
·
··
··W
2W1W0Cell Cell Cell
Figure 7.2 A conceptual internal organization of a memory chipCPU Main Memory
MARb n
b
MDRb linesn lines0
1
2
2n− 1D0 − Db−1A0 − An−1
R / W
Figure 7.1 A typical CPU and main memory interface136 MEMORY SYSTEM DESIGN IIone can show that if A¼0, then B¼1, which represents the other cell stable state,
call it state 0. The two transistors N3andN4are used to connect the cell to the two
data (bit) lines. Normally (if the word select is not activated) these two transistors are
turned off, thus protecting the cell from the signal values carried by the data lines.The two transistors are turned on when the word select line is activated. What takesplace when the two transistors are turned on will depend on the intended memory
operation as shown below.
Read operation:
1. Both lines band
bare precharged high.
2. The word select line is activated, thus turning on both transistors N3andN4.
3. Depending on the internal value stored in the cell, point A(B) will lead to the
discharge of line b(/C22b b).
Write operation:
1. The bit lines are precharged such that b(/C22b b)¼1(0).
2. The word select line is activated, thus turning on both transistors N3andN4.
3. The bit line precharged with 0 will have to force the point A(B), which has
1, to 0.
The internal organization of the memory array should satisfy an important
memory design factor, that is, efﬁcient utilization of the memory chip. Consider,
for example, a 1K /C24 memory chip. Using the organization shown in Figure 7.2,
the memory array should be organized as 1K rows of cells, each consisting of
four cells. The chip will then have to have 10 pins for the address and four pins
for the data. However, this may not lead to the best utilization of the chip area.Data (bit)
line bData (bit)
line b N3P1
P2
N1AB
Word select lineN4
N2
Figure 7.3 Static CMOS memory cell7.1. MAIN MEMORY 137Another possible organization of the memory cell array is as a 64 /C264, that is, to
organize the array in the form of 64 rows, each consisting of 64 cells. In this
case, six address lines (forming what is called the row address) will be needed inorder to select one of the 64 rows. The remaining four address lines (called thecolumn address) will be used to select the appropriate 4 bits among the available64 bits constituting a row. Figure 7.4 illustrates this organization.
Another important factor related to the design of the main memory subsystem is
the number of chip pins required in an integrated circuit. Consider, for example, thedesign of a memory subsystem whose capacity is 4K bits. Different organization ofthe same memory capacity can lead to a different number of chip pins requirement.Table 7.1 illustrates such an observation. It is clear from the table that increasing thenumber of bits per addressable location results in an increase in the number of pinsneeded in the integrated circuit.
Another factor pertinent to the design of the main memory subsystem is the
required number of memory chips. It is important to realize that the available per
chip memory capacity can be a limiting factor in designing memory subsystems.
W0
A0
A1
A2
A5W1
W2
W63
B63
A6
A7
A8
A9B62 B2
D3D2 D1 D0B1B064× 64 memory cell array
16 to 1 multiplexers
Figure 7.4 Efﬁcient internal organization of a 1K /C24 memory chip138 MEMORY SYSTEM DESIGN IIConsider, for example, the design of a 4M bytes main memory subsystem using 1M
bit chip. The number of required chips is 32 chips. It should be noted that the numberof address lines required for the 4M system is 22, while the number of data lines is
8. Figure 7.5 shows a block diagram for both the intended memory subsystem and
the basic building block to be used to construct such a subsystem.
The memory subsystem can be arranged in four rows, each having eight chips. A
schematic of such an arrangement is shown in Figure 7.6. In this ﬁgure, the least sig-
niﬁcant 20 address lines A
19/C0A0are used to address any of the basic building block
1M single bit chips. The high-order two address lines A21–A20are used as inputs to a
2–4 decoder in order to generate four enable lines; each is connected to the CE line
of the eight chips constituting a row.
The above discussion on main memory system design assumes the use of a six-
transistor static random cell. It is possible however to use a one-transistor dynamiccell. Dynamic memory depends on storing logic values using a capacitor togetherwith one transistor that acts as a switch. The use of dynamic memory leads to
saving in chip area. However, due to the possibility of decay of the stored valuesTABLE 7.1 Impact of Using Different Organizations
on the Number of Pins
OrganizationNumber of needed
address linesNumber of needed
data lines
4K/C211 2 1
1K/C241 0 4
512/C289 8
256/C216 8 16
R/W
R/W
R/W20 address lines
22 address lines
1 data line
8 data lines
CE Mode
0 Tri-state
1R e a d1X
10 Write
X=don’t care
Tri-state = high impedanceCE4M bytes memory
System
(a) Intended memory system (b) Basic memory building block
Figure 7.5 Block diagram of a required memory system and its basic building block7.1. MAIN MEMORY 139(leakage of the stored charge on the capacitor), dynamic memory requires periodical
(every few milliseconds) refreshment in order to restore the stored logic values.Figure 7.7 illustrates the dynamic memory array organization. The read /write cir-
cuitry in Figure 7.7 performs the functions of sensing the value on the bit line, ampli-
fying it, and refreshing the value stored on the capacitor.
In order to perform a read operation, the bit line is precharged high (same as in
static memory) and the word line is activated. That will cause the value stored on the
capacitor to appear on the bit line, thus appearing on the data line D
i. As can be seen,
a read operation is destructive; that is, the capacitor is charged to the bit line. There-
fore, every read operation is followed by a write operation of the same value.
In order to perform a write operation, the intended value is placed on the bit line
and the word line is activated. If the intended value is 1, then the capacitor will be
charged, while if the intended value is 0, then the capacitor will be discharged.
Table 7.2 summarizes the operation of the control circuitry.A21
A20
A19 -A0D7 D1 D0#7 #1 #0
#7 #1 #0
#7 #1 #0
#7 #1 #0
Figure 7.6 Organization of a 4M 8-bit memory using 1M 1-bit memory chips
TABLE 7.2 Operation of the Control Circuitry
CE R=/C22W W Operation
0 /C2 None
1 1 Read
1 0 Write
/C2¼ don’t care140 MEMORY SYSTEM DESIGN IIAs discussed before, appropriate internal organization of a memory subsystem
can lead to a saving in the number of IC pins required, an important IC design
factor. In order to reduce the number of pins required for a given dynamic
memory subsystem, it is a normal practice (as in the case of static memory) todivide the address lines into row and column address lines. In addition, the rowand column address lines are transmitted over the same pins, one after the otherin a scheme known as time-multiplexing . This can potentially cut the number of
address pins required by half. Due to time-multiplexing of address lines, it will benecessary to add two extra control lines, that is, row address strobe (
RAS) and
column address strobe ( CAS). These two control lines are used to indicate to the
memory chip when the row address lines are valid and when the column addresslines are valid, respectively. Consider, for example, the design of a 1M /C21 dynamic
memory subsystem. Figure 7.8 shows a possible internal organization of thememory cell array in which the array is organized as a 1024 /C21024.
It should be noted that only 10 address lines are shown. These are used to multi-
plex both the rows and columns address lines; each is 10 lines. The rows and col-umns latches are used to store the row and column addresses for a duration equalto the memory cycle. In this case, a memory access will consist of a
RAS and a
row address, followed by a CAS and a column address.
Figure 7.7 Dynamic memory array organization7.1. MAIN MEMORY 1417.2. VIRTUAL MEMORY
The concept of virtual memory is in principle similar to that of the cache memory
described in Section 6.2. A virtual memory system attempts to optimize the useof the main memory (the higher speed portion) with the hard disk (the lowerspeed portion). In effect, virtual memory is a technique for using the secondary sto-rage to extend the apparent limited size of the physical memory beyond its actual
physical size. It is usually the case that the available physical memory space will
not be enough to host all the parts of a given active program. Those parts of the pro-gram that are currently active are brought to the main memory while those parts that
are not active will be stored on the magnetic disk. If the segment of the program con-taining the word requested by the processor is not in the main memory at the time ofthe request, then such segment will have to be brought from the disk to the mainmemory. The principles employed in the virtual memory design are the same asthose employed in the cache memory. The most relevant principle is that of keeping
active segments in the high-speed main memory and moving inactive segments back
to the hard disk.
Movement of data between the disk and the main memory takes the form of
pages. A page is a collection of memory words, which can be moved from thedisk to the MM when the processor requests accessing a word on that page. A typicalsize of a page in modern computers ranges from 2K to 16K bytes. A page faultoccurs when the page containing the word required by the processor does notexist in the MM and has to be brought from the disk. The movement of pages of pro-
grams or data between the main memory and the disk is totally transparent to the
application programmer. The operating system is responsible for such movementof data and programs.
Figure 7.8 A 1024 /C21024 memory array organization142 MEMORY SYSTEM DESIGN IIIt is useful to mention at this point that although based on similar principles, a
signiﬁcant difference exists between cache and virtual memories. A cache miss
can cause a time penalty that is 5 to 10 times as costly as a cache hit. A pagefault, on the other hand can be 1000 times as costly as a page hit. It is thereforeunreasonable to have the processor wait on a page fault while a page is being trans-ferred to the main memory. This is because thousands of instructions could be exe-cuted on a modern processor during page transfer.
The address issued by the processor in order to access a given word does not
correspond to the physical memory space. Therefore, such address is called a vir-
tual(logical )address . The memory management unit (MMU) is responsible for the
translation of virtual addresses to their corresponding physical addresses. Threeaddress translation techniques can be identiﬁed. These are direct-mapping ,associ-
ative-mapping , and set-associative-mapping . In all these techniques, information
about the main memory locations and the corresponding virtual pages are keptin a table called the page table . The page table is stored in the main memory.
Other information kept in the page table includes a bit indicating the validity ofa page, modiﬁcation of a page, and the authority for accessing a page. The valid
bitis set if the corresponding page is actually loaded into the main memory.
Valid bits for all pages are reset when the computer is ﬁrst powered on. Theother control bit that is kept in the page table is the dirty bit . It is set if the corres-
ponding page has been altered while residing in the main memory. If while residingin the main memory a given page has not been altered, then its dirty bit will be
reset. This can help in deciding whether to write the contents of a page back
into the disk (at the time of replacement) or just to override its contents with
another page. In the following discussion, we will concentrate on the address trans-lation techniques keeping in mind the use of the different control bits stored in thepage table.
7.2.1. Direct Mapping
Figure 7.9 illustrates the address translation process according to the direct-mapping
technique. In this case, the virtual address issued by the processor is divided intotwo ﬁelds: the virtual page number and the offset ﬁelds. If the number of bits inthe virtual page number ﬁeld is N, then the number of entries in the page table
will be 2
N.
The virtual page number ﬁeld is used to directly address an entry in the page
table. If the corresponding page is valid (as indicated by the valid bit), then the con-tents of the speciﬁed page table entry will correspond to the physical page address.The latter is then extracted and concatenated with the offset ﬁeld in order to form thephysical address of the word requested by the processor. If, on the other hand, thespeciﬁed entry in the page table does not contain a valid physical page number,then this represents a page fault. In this case, the MMU will have to bring thecorresponding page from the hard disk, load it into the main memory, and indicate
the validity of the page. The translation process is then carried out as explained
before.7.2. VIRTUAL MEMORY 143The main advantage of the direct-mapping technique is its simplicity measured
in terms of the direct addressing of the page table entries. Its main disadvantage
is the expected large size of the page table. In order to overcome the need fora large page table, the associative-mapping technique, which is explained below,
is used.
7.2.2. Associative Mapping
Figure 7.10 illustrates the address translation according to the associative mapping
technique. The technique is similar to direct mapping in that the virtual addressissued by the processor is divided into two ﬁelds: the virtual page number and theoffset ﬁelds. However, the page table used in associative mapping could be far shorterthan its direct mapping counterpart. Every entry in the page table is divided into two
parts: the virtual page number and the physical page number. A match is searched(associatively) between the virtual page number ﬁeld of the address and the virtual
page numbers stored in the page table. If a match is found, the corresponding physical
page number stored in the page table is extracted and is concatenated with the offsetﬁeld in order to generate the physical address of the word requested by the processor.If, on the other hand, a match could not be found, then this represents a page fault. Inthis case, the MMU will have to bring the corresponding page from the hard disk, loadit into the main memory, and indicate the validity of the page. The translation processis then carried out as explained before.
The main advantage of the associative-mapping technique is the expected shorter
page table (compared to the direct-mapping technique) required for the translationprocess. Its main disadvantage is the search required for matching the virtual
Figure 7.9 Direct-mapping virtual address translation144 MEMORY SYSTEM DESIGN IIpage number ﬁeld and all virtual page numbers stored in the page table. Although
such a search is done associatively, it requires the use of an added hardwareoverhead.
A possible compromise between the complexity of the associative mapping and
the simplicity of the direct mapping is the set-associative mapping technique. Thishybrid technique is explained below.
7.2.3. Set-Associative Mapping
Figure 7.11 illustrates the address translation according to the set-associative map-
ping. In this case, the virtual address issued by the processor is divided into threeﬁelds: the tag, the index, and the offset. The page table used in set-associative map-ping is divided into sets, each consisting of a number of entries. Each entry in thepage table consists of a tag and the corresponding physical page address. Similarto direct mapping, the index ﬁeld is used to directly determine the set in which a
search should be conducted. If the number of bits in the index ﬁeld is S, then the
number of sets in the page table should be 2
S. Once the set is determined, then a
search (similar to associative mapping) is conducted to match the tag ﬁeld with
all entries in that speciﬁc set. If a match is found, then the corresponding physicalpage address is extracted and concatenated with the offset ﬁeld in order to generatethe physical address of the word requested by the processor. If, on the other hand, amatch could not be found, then this represents a page fault. In this case, the MMUwill have to bring the corresponding page from the hard disk, load it into the main
memory, update the corresponding set and indicate the validity of the page. The
translation process is then carried out as explained before.
Figure 7.10 Associative mapping address translation7.2. VIRTUAL MEMORY 145The set-associative-mapping technique strikes a compromise between the inefﬁ-
ciency of direct mapping, in terms of the size of the page table, and excessive hard-
ware overhead of associative mapping. It also enjoys the best of the two techniques:
the simplicity of the direct mapping and the efﬁciency of the associative mapping.
It should be noted that in all the above address translation techniques extra main
memory access is required for accessing the page table. This extra main memoryaccess could potentially be saved if a copy of a small portion of the page table
can be kept in the MMU. This portion consists of the page table entries that corre-
spond to the most recent accessed pages. In this case, before any address translationis attempted, a search is conducted to ﬁnd out whether the virtual page number (orthe tag) in the virtual address ﬁeld could be matched. This small portion is kept in the
table look-aside buffer (TLB) cache in the MMU. This is explained below.
7.2.4. Translation Look-Aside Buffer (TLB)
In most modern computer systems a copy of a small portion of the page table is kept
on the processor chip. This portion consists of the page table entries that correspondto the most recently accessed pages. This small portion is kept in the translation
look-aside buffer (TLB) cache. A search in the TLB precedes that in the page
table. Therefore, the virtual page ﬁeld is ﬁrst checked against the entries of the
Figure 7.11 Set-associative mapping address translation146 MEMORY SYSTEM DESIGN IITLB in the hope that a match is found. A hit in the TLB will result in the generation
of the physical address of the word requested by the processor, thus saving the extramain memory access required to access the page table. It should be noted that a misson the TLB is not equivalent to a page fault. Figure 7.12 illustrates the use of theTLB in the virtual address translation process. The typical size of a TLB is in therange of 16 to 64 entries. With this small TLB size, a hit ratio of more than 90%is always possible. Owing to its limited size, the search in the TLB is done associa-
tively, thus reducing the required search time.
To illustrate the effectiveness of the use of a TLB, let us consider the case of
using a TLB in a virtual memory system having the following speciﬁcations.
Number of entries in the TLB ¼16 Associative search time in TLB ¼10 ns
Main memory access time ¼50 ns TLB hit ratio ¼0.9
The average access time ¼0.9(10þ50)þ0.1(10þ2*50)¼0.9*60þ
0.1*110¼65 ns. This is to be compared to the 100 ns access time needed in the
absence of the TLB. It should be noted that for simplicity, we overlooked the exist-
ence of the cache in the above illustration.
It is clear from the above discussion that as more requests for items that do not
exist in the main memory (page faults) occur, more pages would have to be broughtfrom the hard disk to the main memory. This will eventually lead to a totally ﬁlledmain memory. The arrival of any new page from the hard disk to a totally full main
memory should promote the following question: Which main memory page should
be removed (replaced) in order to make room for the incoming page(s)? Replace-
ment algorithms (policies) are explained next.
Figure 7.12 Use of the TLB in virtual address translation7.2. VIRTUAL MEMORY 147It should be noted that Intel’s Pentium 4 processor has a 36-bit address bus, which
allows for a maximum main memory size of 64 GB. According to Intel’s speciﬁca-
tions, the virtual memory is 64 TB (65,528 GB). This increases the processor’smemory access space from 2
36to 246bytes. This is to be compared to the PowerPC
604 which has two 12-entry, two-way set-associative translation look-aside buffers
(TLBs): one for instructions and the other for data. The virtual memory space istherefore ¼2
52¼4 Peta-bytes.
7.2.5. Replacement Algorithms (Policies)
Basic to the implementation of virtual memory is the concept of demand paging .
This means that the operating system, and not the programmer, controls the swap-
ping of pages in and out of main memory as they are required by the active pro-cesses. When a process needs a nonresident page, the operating system mustdecide which resident page is to be replaced by the requested page. The techniqueused in the virtual memory that makes this decision is called the replacement policy .
There exists a number of possible replacement mechanisms. The main objective
in all these mechanisms is to select for removal the page that expectedly will not bereferenced in the near future.
Random Replacement According to this replacement policy, a page is selected
randomly for replacement. This is the simplest replacement mechanism. It can be
implemented using a pseudo-random number generator that generates numbersthat correspond to all possible page frames. At the time of replacement, therandom number generated will indicate the page frame that must be replaced.
Although simple, this technique may not result in efﬁcient use of the main
memory, that is, a low hit ratio h. Random replacement has been used in the Intel
i860 family of RISC processor.
First-In-First-Out (FIFO) Replacement According to this replacement policy,
the page that was loaded before all the others in the main memory is selected forreplacement. The basis for page replacement in this technique is the time spentby a given page residing in the main memory regardless of the pattern of usage ofthat page. This technique is also simple. However, it is expected to result in accep-
table performance, measured in terms of the main memory hit ratio, if the page refer-
ences made by the processor are in strict sequential order. To illustrate the use of theFIFO mechanism, we offer the following example.
Example Consider the following reference string of pages made by a processor:
6, 7, 8, 9, 7, 8, 9, 10, 8, 9, 10. In particular, consider two cases: (a) the number of
page frames allocated in the main memory is TWO and (b) the number of pageframes allocated are THREE. Figure 7.13 illustrates a trace of the reference stringfor the two cases. As can be seen from the ﬁgure, when the number of page
frames is TWO, there were 11 page faults (these are shown in bold in the ﬁgure).
When the number of page frames is increased to THREE, the number of page148 MEMORY SYSTEM DESIGN IIfaults was reduced to ﬁve. Since ﬁve pages are referenced, this is the optimum con-
dition. The FIFO policy results in the best (minimum) page faults when the reference
string is in strict order of increasing page number references.
Least Recently Used (LRU) Replacement According to this technique, page
replacement is based on the pattern of usage of a given page residing in the main
memory regardless of the time spent in the main memory. The page that has notbeen referenced for the longest time while residing in the main memory is selectedfor replacement. The LRU technique matches most programs’ characteristics andtherefore is expected to result in the best possible performance in terms of the
main memory hit ratio. It is, however, more involved compared to other techniques.
To illustrate the use of the LRU mechanism, we offer the following example.
Example Consider the following reference string of pages made by a processor:
4, 7, 5, 7, 6, 7, 10, 4, 8, 5, 8, 6, 8, 11, 4, 9, 5, 9, 6, 9, 12, 4, 7, 5, 7. Assume that the
number of page frames allocated in the main memory is FOUR. Compute thenumber of page faults generated. The trace of the main memory contents isshown in Figure 7.14. Number of page faults ¼17.
In presenting the LRU, we have a particular implementation, called stack-based
LRU. In this implementation, the most recently accessed page is now represented by(a)
(b)6 6 8 8 79 9 8 8 10
9 9 8 10 10 9 967
7 77
789 7 8 9 1 0 8 9 1 0
66
668
8 88 888 88810 10 10 10 7 7 77 777
9 99 9 99 9 99 7 8 9 10 8 9 10
Figure 7.13 FIFO replacement technique. (a) FIFO replacement using two page frames
(#PFs¼11), (b) FIFO replacement using three page frames (#PFs ¼5)
Figure 7.14 LRU replacement technique7.2. VIRTUAL MEMORY 149the top page rectangle. The rectangles do not represent speciﬁc page frames as they
did in the FIFO diagram. Thus, each reference generating a page fault is now on thetop row. It should be noted that as more pages are allotted to the program the pagereferences in each row do not change. Only the number of page faults changes. Thiswill make the set of pages in memory for npage frames be a subset of the set of
pages for nþ1 page frames. In fact, the diagram could be considered a STACK
data structure with the depth of the stack representing the number of page frames.
If a page is not on the stack (i.e., is found at a depth greater than the number of
page frames), then a page fault occurs.
Example Consider the case of a two-dimensional 8 /C28 array A. The array is
stored in row-major order. For THREE page frames, compute how many page
faults are generated by the following array-initialization loop. Assume that an
LRU replacement algorithm is used and that all frames are initially empty.
Assume that the page size is 16.
for I ¼0t o7d o
for J ¼0t o7d o
A[I, J] ¼0;
End for
End for
The arrangement of the array elements in the secondary storage is shown in
Figure 7.15. The sequence of requests for the array elements in the ﬁrst TWO
external loop executions is as follows:
I¼0
J¼0, 1, 2, 3, 4, 5, 6, 7
a
00,a01,a02,a03,a04,a05,a06,a07 The number of page faults (PFs) ¼1
I¼1
J¼0, 1, 2, 3, 4, 5, 6, 7
a10,a11,a12,a13,a14,a15,a16,a17 The number of page faults (PFs) ¼1
From the above analysis, it is clear that there will be one PF in every external loop
execution. This makes the total number of PFs be 8. It should be noted that if thearray was stored column-major, then every internal loop execution would generateeight page faults, thus causing the total number of PFs to become 64.
Clock Replacement Algorithm This is a modiﬁed FIFO algorithm. It takes
into account both the time spent by a page residing in the main memory (similar
to the FIFO) and the pattern of usage of the page (similar to the LRU). The tech-
nique is therefore sometimes called the First-In-Not-Used-First-Out (FINUFO). In
keeping track of both the time and the usage, the technique uses a pointer to150 MEMORY SYSTEM DESIGN IIFigure 7.15 Arrangement of array elements in secondary storage and main memory built up7.2. VIRTUAL MEMORY 151indicate where to place the incoming page and a used bit to indicate the usage of a
given page. The technique can be explained using the following three steps.
1. If the used bit ¼1, then reset bit, increment pointer and repeat.
2. If the used bit ¼0, then replace corresponding page and increment pointer.
3. The used bit is SET if the page is referenced after the initial loading.
Example Consider the following page requests (Fig. 7.16) in a THREE-page
frames MM system using the FINUFO technique: 2,3,2,4,6,2,5,6,1,4,6. Estimatethe hit ratio. The estimated Hit Ratio ¼4/11.
7.2.6. Virtual Memory Systems with Cache Memory
A typical computer system will contain a cache, a virtual memory, and a TLB. When
a virtual address is received from the processor, a number of different scenarios canoccur, each dependent on the availability of the requested item in the cache, the mainmemory, or the secondary storage. Figure 7.17 shows a general ﬂow diagram for thedifferent scenarios.
The ﬁrst level of address translation checks for a match between the received vir-
tual address and the virtual addresses stored in the TLB. If a match occurs (TLB hit)then the corresponding physical address is obtained. This physical address can thenbe used to access the cache. If a match occurs (cache hit) then the element requestedby the processor can be sent from the cache to the processor. If, on the other hand, acache miss occurs, then the block containing the targeted element is copied from themain memory into the cache (as discussed before) and the requested element is sentto the processor.
The above scenario assumes a TLB hit. If a TLB miss occurs, then the page table
(PT) is searched for the existence of the page containing the targeted element in themain memory. If a PT hit occurs, then the corresponding physical address is gener-ated (as discussed before) and a search is conducted for the block containingthe requested element (as discussed above). This will require updating the TLB.
If on the other hand a PT miss takes place (indicating a page fault), then the page
containing the targeted element is copied from the disk into the main memory, ablock is copied into the cache, and the element is sent to the processor. This last
Figure 7.16 FINUFO replacement technique152 MEMORY SYSTEM DESIGN IIscenario will require updating the page table, the main memory, and the cache. A
subsequent request of that virtual address by the processor will result in updatingthe TLB.
7.2.7. Segmentation
A segment is a block of contiguous locations of varying size. Segments are used by
the operating system (OS) to relocate complete programs in the main and the disk
memory. Segments can be shared between programs. They provide means for pro-tection from unauthorized access and /or execution. It is not possible to enter seg-
ments from other segments unless the access has been speciﬁcally allowed. Datasegments and code segments are separated. It should also not be possible to alterinformation in the code segment while fetching an instruction nor should it be poss-ible to execute data in a data segment.
7.2.8. Segment Address Translation
In order to support segmentation, the address issued by the processor should consist
of a segment number (base) and a displacement (or an offset) within the segment.Address translation is performed directly via a segment table. The starting address
of the targeted segment is obtained by adding the segment number to the contents
of the segment table pointer. One important content of the segment table is the
Figure 7.17 Memory hierarchy accesses scenarios7.2. VIRTUAL MEMORY 153physical segment base address. Adding the latter to the offset yields the required
physical address. Figure 7.18 illustrates the segment address translation process.
Possible additional information included in the segment table includes:
1. Segment length
2. Memory protection (read-only, execute-only, system-only, and so on)
3. Replacement algorithm (similar to those used in the paged systems)4. Placement algorithm (ﬁnding a suitable place in the main memory to hold the
incoming segment). Examples include
(a) First ﬁt
(b) Best ﬁt
(c) Worst ﬁt
7.2.9. Paged Segmentation
Both segmentation and paging are combined in most systems. Each segment is
divided into a number of equal sized pages. The basic unit of transfer of data
between the main memory and the disk is the page, that is, at any given time, themain memory may consist of pages from various segments. In this case, the virtual
address is divided into a segment number ,apage number , and displacement within
the page . Address translation is the same as explained above except that the physical
segment base address obtained from the segment table is now added to the virtualpage number in order to obtain the appropriate entry in the page table. The outputof the page table is the page physical address, which when concatenated with
the word ﬁeld of the virtual address results in the physical address. Figure 7.19
illustrates the paged segmentation address translation.Segment Number Offset
Segment Table
Displacement
Physical Segment Base Address
Physical AddressΣΣ
Figure 7.18 Segment address translation154 MEMORY SYSTEM DESIGN II7.2.10. Pentium Memory Management
In the Pentium processor, both segmentation and paging are individually available
and can also be disabled. Four distinct views of the memory exist:
1. Unsegmented unpaged memory
2. Unsegmented paged memory3. segmented unpaged memory
4. segmented paged memory
For segmentation, the 16-bit segment number (two of which are used for protection)
and the 32-bit offset produce a segmented virtual address space equal to 2
46¼64
terabytes. The virtual address space is divided into two parts: one half, that is,8K/C24 GB, is global and shared by all processes, and the other half is local and
is distinct for each process.
For paging, a two-level table lookup paging system is used. First level is a page
directory with 1024 entries, that is, 1024 page groups, each with its own page table
and each FOUR MB in length. Each page table contains 1024 entries; each entry
corresponds to a single 4 KB page.Segment Number
Segment TableOffset
Displacement
Physical Segment Base Address
Page Table
Physical Address
Physical AddressVirtual Page Number
Σ Σ
Σ
Figure 7.19 Paged segmentation address translation7.2. VIRTUAL MEMORY 1557.3. READ-ONLY MEMORY
Random access as well as cache memories are examples of volatile memories. A
volatile storage is deﬁned as one that loses its contents when power is turned off.
Nonvolatile memory storages are those that retain the stored information if power
is turned off. As there is a need for volatile storage there is also a need for nonvo-latile storage. Computer system boot subroutines, microcode control, and video
game cartridges are a few examples of computer software that require the use of
nonvolatile storage. Read-only memory (ROM) can also be used to realize combi-national logic functions.
The technology used for implementing ROM chips has evolved over the years. Early
implementations of ROMs were called mask-programmed ROMs . In this case, a made-
to-order one time ROM is programmed according to a speciﬁc encoding pattern sup-plied by the user. The structure of a 4 /C24 CMOS ROM chip is shown in Figure 7.20.
In this ﬁgure an n-type transistor is placed where a 1 is to be stored. A two-to-four
address decoder is used to create four word lines; each is used to activate a row oftransistors. When a 1 appears on the word line, the corresponding transistors will beturned on, thus pulling the corresponding bit line to 0. An inverter at the output of the
Figure 7.20 Example of a 4 /C24 CMOS ROM chip156 MEMORY SYSTEM DESIGN IIbit lines is used to output a 1 at the output of those pulled down bit lines. Table 7.3
shows the patterns stored at each of the four ROM locations.
Mask-programmed ROMs are primarily used to store machine microcode, desk-
top bootstrap loaders, and video game cartridges. Because they can be programmed
only once by the manufacturer, mask-programmed ROMs are inﬂexible. If the user
would like to program his /her ROM on site, then a different type of ROM, called the
Programmable ROM (PROM) should be used. In this case, fuses, instead of transis-tors, are placed at the intersection of word and bit lines. The user can program thePROM by selectively blowing up the appropriate fuses. This can be done by allow-
ing a high current to ﬂow in those particular fuses, thus causing them to blow up.
This process is known as “burning the ROM.”
Although it allows for some added ﬂexibility, PROM is still restricted by the fact
that it can only be programmed once (by the user). A third type of ROM, calledErasable PROM (EPROM), is reprogrammable; that is, it allows stored data to beerased and new data to be stored. In order to provide such ﬂexibility, EPROMsare constructed using a special type of transistors. These transistors are able toassume one of two statuses, normal or disabled. A disabled transistor acts like a
switch that is turned off all the time. A normal transistor can be programmed to
become open all the time by inducing a certain amount of charge to be trappedunder its gate. A disabled transistor can become normal again by removing
the induced charge. This requires exposing those transistors to ultraviolet light.Exposing the EPROM chip to such ultraviolet light will lead to the erasure of theentire chip contents. This is considered a major drawback of EPROMs. BothPROMs and EPROMs are used in prototyping, of moderate size systems.
Flash EPROMs (FEPROMs) have emerged as strong contenders to EPROMs.
This is because FEPROMs are more compact, faster, and removable compared toEPROMs. The erasure time of a FEPROM is far faster than that of an EPROM.
A different type of ROM, which overcomes the drawback of the EPROM, is the
Electrically EPROM or EEPROM. In this case, the erasure of the EPROM can bedone electrically and, moreover, selectively; that is, only the contents of selectivecells can be erased, leaving the other cells’ contents untouched. Both FEPROMsand EEPROMs are used in applications requiring occasional updating of infor-mation, such as Programmable TVs, VCR, and automotives.
Table 7.4 summarizes the main characteristics of the different types of ROM
discussed above.TABLE 7.3 Patterns Stored at Four ROM Locations
Address
linesWord line
activatedOutput
pattern
00 W0 1001
01 W1 0110
10 W2 1010
11 W3 01017.3. READ-ONLY MEMORY 1577.4. SUMMARY
The discussion in this chapter has been a continuation of that conducted in Chapter 6.
In particular, this chapter has been dedicated to cover the design aspects that relate tothe internal and external organization of the main memory. The design of a staticRAM cell was introduced with emphasis on the read and write operations. Our dis-cussion on virtual memory started with the issues related to address translation.Three address translation techniques were discussed and compared. These are the
direct, associative, and the set-associative techniques. The use of a TLB to improve
the average access time was explained. Three replacement techniques were intro-duced. These are the FIFO, LRU, and clock replacement. Segmented paged systemswere also introduced. Our discussion on virtual memory ended with an explanationof the virtual memory aspects of the Pentium IV processor. Toward the end of thechapter, we have touched on a number of implementations for ROMs.
EXERCISES
1. Consider the case of a computer system employing both a cache and a paged
virtual memory as shown below (Fig. 7.21). One can analyze this system
through identifying FIVE combinations of accesses. What are these combi-nations? Determine the probability and the access time in each case assumingthe following information. Compute also the overall average access time.
TLB address translation and search 25 ns
Cache search time to determine whether address in cache 25 nsCache access time 25 ns
Main memory access time 250 ns
Hard disk access time 100 ms
TLB hit ratio 0.9
Cache hit ratio 0.95
Main memory hit ratio 0.8
2. A 64 /C264 array of words (elements) is to be “normalized” as follows. For each
row, the largest element is found and all elements of the row are divided byTABLE 7.4 Characteristics of Different ROM Implementations
ROM type Cost ProgrammabilityTypical
applications
Mask-programmed ROM Truly inexpensive Once at manufacture Microcode
PROM Inexpensive Once on site Prototyping
EPROM Moderate Many times Prototyping
FEPROM Expensive Many times VCR & TVs
EEPROM Truly expensive Many times VCR & TVs158 MEMORY SYSTEM DESIGN IIthis maximum value. Assume that each page in the virtual memory consists of
64 words, and that 2K words of the main memory are allocated for storing data
during this computation. Suppose that it takes 100 ms to load a page from thedisk into the main memory when a page fault occurs.
(a) Write a simple piece of code (in a notational form) that can perform the
above job.
(b) How many page faults would occur if the elements of the array are stored
in column order in the virtual memory?
(c) How many page faults would occur if the elements of the array are stored
in row order in the virtual memory?
(d) Estimate the total time needed to perform this normalization for both
arrangements (b) and (c).
3. Design a 64M /C28-bit memory using a number of 16M /C21-bit static RAM
chips. Assume that each individual chip has a chip select (
CS) line and a
read/write ( R=/C22W W) line. Compute the number of chips required and show a
complete connection diagram of the designed memory.
4. Consider the following stream of page requests: 1,2,3,4,5,1,2,3,4,5,1,2,3,4,5.
Assume that the main memory consists of FOUR page frames. Show a traceof the status of the page frames in the MM and estimate the hit ratio assumingeach of the following page replacement algorithms.
(a) FIFO
(b) LRU
(c) FINUFO
5. Consider the case of a two-dimensional 20 /C220 array A. The array is stored
column-major. For FIVE main memory page frames, compute how many
Figure 7.21 Computer system with cache and paged virtual memoryEXERCISES 159page faults are generated by the following array-initialization loop. Assume
that LRU replacement algorithm is used and that all frames are initially
empty. Assume also that the page size is 40 elements.
for I ¼1t o2 0d o
for J ¼1t o2 0d o
A[I, J] ¼0;
6. In this problem you are asked to pick a real-life computer memory system that
uses both caching and virtual memory schemes. Your job is to apply all the
knowledge that you have gained in this chapter concerning “Memory System
Design and Analysis” in describing and analyzing your selected system.
Examples that you may pick include but are not limited to Intel Pentium 4,The PowerPC, Alpha AXP 21064, and so on. Use examples and illustrationsto support your analysis and be as speciﬁc as possible. Remember that the empha-sis should be on the analysis and the basic design issues involved. You can useany reference material but make sure that your include it in your reference list.
REFERENCES AND FURTHER READING
S. D. Burd, Systems Architecture , 3rd ed., Thomson Learning Ltd, Boston, 2001.
H. Cragon, Memory Systems and Pipelined Processors , Jones and Bartlett: Sudbury, MA,
1996.
V. C. Hamacher, Z. G. Vranesic, and S. G. Zaky, Computer Organization , 5th ed., McGraw-
Hill, NY, 2002.
J. L. Hennessy, and Patterson, D. A., Computer Architecture :A Quantitative Approach ,
Morgan Kaufmann, San Francisco, CA, 1996.
V. P. Heuring, and H. F. Jordan, Computer Systems Design and Architecture , Addison-
Wesley, NJ, USA, 1997.
D. A. Patterson, and J. L. Hennessy, Computer Organization & Design :The Hardware /
Software Interface , Morgan Kaufmann, San Mateo, CA, 1994.
H. S. Stone, High-Performance Computer Architecture , Addison-Wesley, Amsterdam,
Netherlands, 1987.
B. Wilkinson, Computer Architecture: Design and Performance , Prentice-Hall,
Hertfordshire, UK, 1996.
Websites
http://www.sysopt.com
http://www.intel.com
http://www.AcerHardware.com
http://www.pmc-sierra.com /products /details /rm7000a
http: //physinfo.ulb.ac.be /divers_html /PowerPC_Programming_Info /into_to_ppc /ppc2_
hardware.html160 MEMORY SYSTEM DESIGN II& CHAPTER 8
Input–Output Design and Organization
Having considered the fundamental concepts related to instruction set design,
assembly language programming, processor design, and memory design, we nowturn our attention to the issues related to input–output (I /O) design and organiz-
ation. It should be emphasized at the outset that I /O plays a crucial role in any
modern computer system. Therefore, a clear understanding and appreciation ofthe fundamentals of I /O operations, devices, and interfaces are of great importance.
Input–output (I /O) devices vary substantially in their characteristics. One dis-
tinguishing factor among input devices (and also among output devices) is their
data processing rate, deﬁned as the average number of characters that can be pro-
cessed by a device per second. For example, while the data processing rate of aninput device such as the keyboard is about 10 characters (bytes) /second, a scanner
can send data at a rate of about 200,000 characters /second. Similarly, while a laser
printer can output data at a rate of about 100,000 characters /second, a graphic
display can output data at a rate of about 30,000,000 characters /second.
Striking a character on the keyboard of a computer will cause a character (in the
form of an ASCII code) to be sent to the computer. The amount of time passed
before the next character is sent to the computer will depend on the skill of the
user and even sometimes on his /her speed of thinking. It is often the case that the
user knows what he /she wants to input, but sometimes they need to think before
touching the next button on the keyboard. Therefore, input from a keyboardis slow and burst in nature and it will be a waste of time for the computer tospend its valuable time waiting for input from slow input devices. A mechanismis therefore needed whereby a device will have to interrupt the processor askingfor attention whenever it is ready. This is called interrupt-driven communication
between the computer and I /O devices (see Section 8.3).
Consider the case of a disk. A typical disk should be capable of transferring data
at rates exceeding several million bytes /second. It would be a waste of time to trans-
fer data byte by byte or even word by word. Therefore, it is always the case that datais transferred in the form of blocks, that is, entire programs. It is also necessaryto provide a mechanism that allows a disk to transfer this huge volume of datawithout the intervention of the CPU. This will allow the CPU to perform other
161Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.useful operation(s) while a huge amount of data is being transferred between the disk
and the memory. This is the essence of the direct memory access (DMA) mechanism
discussed in Section 8.4.
We begin our discussion by offering some basic concepts in Section 8.1.
8.1. BASIC CONCEPTS
Figure 8.1 shows a simple arrangement for connecting the processor and the memory
in a given computer system to an input device, for example, a keyboard and an outputdevice such as a graphic display. A single bus consisting of the required address, data,and control lines is used to connect the system’s components in Figure 8.1.
The way in which the processor and the memory exchange data has been
explained in Chapters 6 and 7. We are here concerned with the way the processor
and the I /O devices exchange data. It has been indicated in the introduction part
that there exists a big difference in the rate at which a processor can process infor-
mation and those of input and output devices. One simple way to accommodate this
speed difference is to have the input device, for example, a keyboard, deposit thecharacter struck by the user in a register ( input register ), which indicates the avail-
ability of that character to the processor. When the input character has been taken bythe processor, this will be indicated to the input device in order to proceed and inputthe next character, and so on. Similarly, when the processor has a character to output(display), it deposits it in a speciﬁc register dedicated for communication with thegraphic display ( output register ). When the character has been taken by the graphic
display, this will be indicated to the processor such that it can proceed and output thenext character, and so on. This simple way of communication between the processorand I /O devices, called I/O protocol , requires the availability of the input and
output registers. In a typical computer system, there is a number of input registers,each belonging to a speciﬁc input device. There is also a number of output registers,
System BusProcessor Memory
Output Device
(Graphic Display)Input Device
(Keyboard)
Figure 8.1 A single bus system162 INPUT–OUTPUT DESIGN AND ORGANIZATIONeach belonging to a speciﬁc output device. In addition, a mechanism according to
which the processor can address those input and output registers must be adopted.More than one arrangement exists to satisfy the abovementioned requirements.Among these, two particular methods are explained below.
In the ﬁrst arrangement, I /O devices are assigned particular addresses, isolated
from the address space assigned to the memory. The execution of an input instruc-
tionat an input device address will cause the character stored in the input register of
that device to be transferred to a speciﬁc register in the CPU. Similarly, theexecution of an output instruction at an output device address will cause the char-
acter stored in a speciﬁc register in the CPU to be transferred to the output register
of that output device. This arrangement, called shared I /O, is shown schematically
in Figure 8.2. In this case, the address and data lines from the CPU can be sharedbetween the memory and the I /O devices. A separate control line will have to be
used. This is because of the need for executing input and output instructions. In atypical computer system, there exists more than one input and more than one
output device. Therefore, there is a need to have address decoder circuitry for
device identiﬁcation. There is also a need for status registers for each input and
output device. The status of an input device, whether it is ready to send data to
the processor, should be stored in the status register of that device. Similarly, thestatus of an output device, whether it is ready to receive data from the processor,should be stored in the status register of that device. Input (output) registers,status registers, and address decoder circuitry represent the main components of
an I/O interface (module).
Address Bus
Data Bus
Memory Control Lines
Input Device(s) Control Lines
Output Device(s) Control LinesProcessor Memory
Output Device
(Graphic Display)Input Device
(Keyboard)
Figure 8.2 Shared I /O arrangement8.1. BASIC CONCEPTS 163The main advantage of the shared I /O arrangement is the separation between the
memory address space and that of the I /O devices. Its main disadvantage is the need
to have special input andoutput instructions in the processor instruction set. The
shared I /O arrangement is mostly adopted by Intel.
The second possible I /O arrangement is to deal with input andoutput registers as
if they are regular memory locations. In this case, a read operation from the address
corresponding to the input register of an input device, for example, Read Device 6 ,i s
equivalent to performing an input operation from the input register in Device #6.
Similarly, a write operation to the address corresponding to the output register of
an output device, for example, Write Device 9 , is equivalent to performing an
output operation into the output register in Device #9. This arrangement is called
memory-mapped I /O. It is shown in Figure 8.3.
The main advantage of the memory-mapped I /O is the use of the read and write
instructions of the processor to perform the input and output operations, respectively.
It eliminates the need for introducing special I /O instructions. The main disadvantage
of the memory-mapped I /O is the need to reserve a certain part of the memory address
space for addressing I /O devices, that is, a reduction in the available memory address
space. The memory-mapped I /O has been mostly adopted by Motorola.
8.2. PROGRAMMED I /O
In this section, we present the main hardware components required for communi-cations between the processor and I /O devices. The way according to which such
Address Bus
Data Bus
Control  LinesProcessorMemory
Output Device
(Graphic Display)Input Device
(Keyboard)
Figure 8.3 Memory-mapped I /O arrangement164 INPUT–OUTPUT DESIGN AND ORGANIZATIONcommunications take place (protocol) is also indicated. This protocol has to be pro-
grammed in the form of routines that run under the control of the CPU. Consider, forexample, an input operation from Device 6 (could be the keyboard) in the case of
shared I/O arrangement. Let us also assume that there are eight different I /O
devices connected to the processor in this case (see Fig. 8.4).
The following protocol steps (program) have to be followed:
1. The processor executes an input instruction from device 6, for example,
INPUT 6 . The effect of executing this instruction is to send the device
number to the address decoder circuitry in each input device in order to ident-
ify the speciﬁc input device to be involved. In this case, the output of the deco-der in Device #6 will be enabled, while the outputs of all other decoders willbe disabled.
2. The buffers (in the ﬁgure we assumed that there are eight such buffers) holding
the data in the speciﬁed input device (Device #6) will be enabled by the outputof the address decoder circuitry.
3. The data output of the enabled buffers will be available on the data bus.
Figure 8.4 Example eight-I /O device connection to a processor8.2. PROGRAMMED I /O 1654. The instruction decoding will gate the data available on the data bus into the
input of a particular register in the CPU, normally the accumulator .
Output operations can be performed in a way similar to the input operation
explained above. The only difference will be the direction of data transfer, which
will be from a speciﬁc CPU register to the output register in the speciﬁed outputdevice. I /O operations performed in this manner are called programmed I /O.
They are performed under the CPU control. A complete instruction fetch, decode,and execute cycle will have to be executed for every input and every output oper-ation. Programmed I /O is useful in cases whereby one character at a time is to be
transferred, for example, keyboard and character mode printers. Although simple,
programmed I /O is slow.
One point that was overlooked in the above description of the programmed I /Oi s
how to handle the substantial speed difference between I /O devices and the pro-
cessor. A mechanism should be adopted in order to ensure that a character sent to
the output register of an output device, such as a screen, is not overwritten by theprocessor (due to the processor’s high speed) before it is displayed and that a char-acter available in the input register of a keyboard is read only once by the processor.This brings up the issue of the status of the input and output devices. A mechanism
that can be implemented requires the availability of a Status Bit (B
in) in the interface
of each input device and Status Bit (Bin) in the interface of each output device.
Whenever an input device such as a keyboard has a character available in its
input register, it indicates that by setting Bin¼1. A program in the processor can
be used to continuously monitor Bin. When the program sees that Bin¼1, it will
interpret that to mean a character is available in the input register of that device.
Reading such character will require executing the protocol explained above. When-
ever the character is read, then the program can reset Bin¼0, thus avoiding multiple
read of the same character. In a similar manner, the processor can deposit a characterin the output register of an output device such as a screen only when B
out¼0. It is
only after the screen has displayed the character that it sets Bout¼1, indicating to
the program that monitors Boutthat the screen is ready to receive the next character.
The process of checking the status of I /O devices in order to determine their readi-
ness for receiving and /or sending characters, is called software I /O polling .Ahard-
ware I /O polling scheme is shown in Figure 8.5.
Figure 8.5 Hardware polling scheme166 INPUT–OUTPUT DESIGN AND ORGANIZATIONIn the ﬁgure, each of the NI/O devices has access to the interrupt line INR.
Upon recognizing the arrival of a request (called Interrupt Request) on INR, the pro-
cessor polls the devices to determine the requesting device. This is done through
thedLog2Nepolling lines. The priority of the requesting device will determine the
order in which addresses are put on the polling lines. The address of the highest priority
device is put ﬁrst, followed by the next priority, and so on until the least priority device.
In addition to the I /O polling, two other mechanisms can be used to carry out I /O
operations. These are interrupt-driven I /Oanddirect memory access (DMA). These
are discussed in the next two sections.
8.3. INTERRUPT-DRIVEN I /O
It is often necessary to have the normal ﬂow of a program interrupted, for example, to
react to abnormal events, such as power failure. An interrupt can also be used toacknowledge the completion of a particular course of action, such as a printer indicatingto the computer that it has completed printing the character(s) in its input register andthat it is ready to receive ot her character(s). An interrupt can also be used in time-sharing
systems to allocate CPU time among different programs. The instruction sets of modern
CPUs often include instruction(s) that mimic the actions of the hardware interrupts.
When the CPU is interrupted, it is required to discontinue its current activity,
attend to the interrupting condition (serve the interrupt), and then resume its activity
from wherever it stopped. Discontinuity of the processor’s current activity requiresﬁnishing executing the current instruction, saving the processor status (mostly in theform of pushing register values onto a stack), and transferring control (jump) to whatis called the interrupt service routine (ISR). The service offered to an interrupt will
depend on the source of the interrupt. For example, if the interrupt is due to power
failure, then the action taken will be to save the values of all processor registers and
pointers such that resumption of correct operation can be guaranteed upon powerreturn. In the case of an I /O interrupt, serving an interrupt means to perform the
required data transfer. Upon ﬁnishing serving an interrupt, the processor shouldrestore the original status by popping the relevant values from the stack. Once theprocessor returns to the normal state, it can enable sources of interrupt again.
One important point that was overlooked in the above scenario is the issue of ser-
ving multiple interrupts , for example, the occurrence of yet another interrupt while
the processor is currently serving an interrupt. Response to the new interrupt willdepend upon the priority of the newly arrived interrupt with respect to that of theinterrupt being currently served. If the newly arrived interrupt has priority lessthan or equal to that of the currently served one, then it can wait until the processorﬁnishes serving the current interrupt. If, on the other hand, the newly arrived inter-rupt has priority higher than that of the currently served interrupt, for example,power failure interrupt occurring while serving an I /O interrupt, then the processor
will have to push its status onto the stack and serve the higher priority interrupt.
Correct handling of multiple interrupts in terms of storing and restoring the correct
processor status is guaranteed due to the way the push and pop operations are8.3. INTERRUPT-DRIVEN I /O 167performed. For example, to serve the ﬁrst interrupt, STATUS 1 will be pushed onto
the stack. Upon receiving the second interrupt, STATUS 2 will be pushed onto thestack. Upon serving the second interrupt, STATUS 2 will be popped out of the stackand upon serving the ﬁrst interrupt, STATUS 1 will be popped out of the stack.
It is possible to have the interrupting device identify itself to the processor by
sending a code following the interrupt request. The code sent by a given I /O
device can represent its I /O address or the memory address location of the start
of the ISR for that device. This scheme is called vectored interrupt .
8.3.1. Interrupt Hardware
In the above discussion, we have assumed that the processor has recognized the
occurrence of an interrupt before proceeding to serve it. Computers are provided
with interrupt hardware capability in the form of specialized interrupt lines to the
processor. These lines are used to send interrupt signals to the processor. In the
case of I /O, there exists more than one I /O device. The processor should be pro-
vided with a mechanism that enables it to handle simultaneous interrupt requestsand to recognize the interrupting device. Two basic schemes can be implementedto achieve this task. The ﬁrst scheme is called daisy chain bus arbitration
(DCBA) and the second is called independent source bus arbitration (ISBA).
According to the DCBA (see Fig. 8.6 a), I/O devices present their interrupt
requests to the interrupt request line
INR (similar to the polling arrangement).
Upon recognizing the arrival of an interrupt request, the processor, through adaisy chained grant line (GL), sends its grant to the requesting device to start com-
munication with the processor. The GL goes through all devices starting from theﬁrst device nearer to the processor and going to the next device and so on until itreaches the last device (Device #N). If Device #1 has put a request, then it willhold the grant signal and start communication with the processor. If, on the other
hand, Device #1 has no interrupt request, it will pass the grant signal to device
#2, which will repeat the same procedure, and so on. In the case of multiple requests,the DCBA arrangement gives highest priority to the device physically nearer to theprocessor. The furthest device from the processor has the lowest priority.
According to the ISBA (see Fig. 8.6 b), each I /O device has its own interrupt request
line, through which it can send its interrupt request, independent of the other devices.Similarly, each I /O device has its own grant line, through which it receives the grant
signal for its request such that it can start communicating with the processor. I /O
device priority in the ISBA does not depend on the device location. A priority arbitra-tion circuitry is needed in order to deal with simultaneous interrupt requests.
8.3.2. Interrupt in Operating Systems
When an interrupt occurs, the operating system gains control. The operating
system saves the state of the interrupted process, analyzes the interrupt, and
passes control to the appropriate routine to handle the interrupt. There are several168 INPUT–OUTPUT DESIGN AND ORGANIZATIONtypes of interrupts, including I /O interrupts. An I /O interrupt notiﬁes the operating
system that an I /O device has completed or suspended its operation and needs some
service from the CPU. To process an interrupt, the context of the current process
must be saved and the interrupt handling routine must be invoked. This process iscalled context switching . A process context has two parts: processor context and
memory context. The processor context is the state of the CPU’s registers including
program counter (PC), program status words (PSWs), and other registers. The
memory context is the state of the program’s memory including the program
and data. The interrupt handler is a routine that processes each different type ofinterrupt.
The operating system must provide programs with save area for their contexts. It
also must provide an organized way for allocating and deallocating memory for theinterrupted process. When the interrupt handling routine ﬁnishes processing the inter-rupt, the CPU is dispatched to either the interrupted process, or to the highest priorityready process. This will depend on whether the interrupted process is preemptive or
nonpreemptive. If the process is nonpreemptive, it gets the CPU again. First the con-
text must be restored, then control is returned to the interrupts process.
Figure 8.6 Interrupt hardware schemes. ( a) Daisy chain interrupt arrangement
(b) Independent interrupt arrangement8.3. INTERRUPT-DRIVEN I /O 169Figure 8.7 shows the layers of software involved in I /O operations. First, the pro-
gram issues an I /O request via an I /O call. The request is passed through to the I /O
device. When the device completes the I /O, an interrupt is sent and the interrupt handler
is invoked. Eventually, control is relinquished back to the process that initiated the I /O.
Example 1: 80 386 Interrupt Architecture The 80 /C286 processors have just
two hardware interrupt pins. These are labeled INTR and NMI. NMI is a nonmaskable
interrupt, which means it cannot be blocked and the processor must respond to it. TheNMI input is usually reserved for critical system functions. The INTR input is a mask-able interrupt request line between the CPU and the programmable interrupt controller(8259A PIC). Interrupts on INTR can be enabled and disabled using the instructionsSTI (set interrupt ﬂag) and CLI (clear interrupt ﬂag), respectively.
Interrupt handlers are called interrupt service routines (ISR). The address of each
interrupt service routine is stored in four consecutive memory locations in the inter-rupt vector table (IVT). The IVT stores pointers to ISR for each type of interrupt.When an interrupt occurs, an 8-bit type number is supplied to the processor,which identiﬁes the appropriate entry in this table.
When an interrupt is generated by a device, it goes to the PIC. Multiple interrupts
may be generated simultaneously. However, they are all buffered by the PIC. ThePIC decides which one of these interrupts should be forwarded to the CPU. Toinform the CPU that an outstanding interrupt is waiting to be processed, the PIC
sends an interrupt request (INTR) to the CPU, which then, at the appropriate
time, responds with an interrupt acknowledgment (INTA). At this time, PIC willput an 8-bit interrupt type number associated with the device on the bus so thatthe CPU can identify which interrupt handler to invoke. In the case when severalinterrupts are pending, PIC will send next interrupt request to the CPU only afterit receives an end of interrupt command from the current ISR. Figure 8.8 showsthe simple protocol that is used to determine which ISR is to be invoked.
In the computer designs that used a single PIC (PC and XT), eight different inter-
rupt requests are allowed (IRQ0–IRQ7). Table 8.1 shows a list of standard interrupttype numbers for typical devices. When AT was designed, a second PIC was added,HardwareDevice Independent SoftwareUser Processes
I/O RequestI/O Reply
Perform I/OWakeup driver when I/O is doneDevice Drivers
Interrupt Handlers
Figure 8.7 Layered I /O software170 INPUT–OUTPUT DESIGN AND ORGANIZATIONincreasing the number of interrupt inputs to 15. Figure 8.9 shows two PICS wired in
cascade. One PIC is designated as master and the other becomes the slave. As shownin the ﬁgure, all slave interrupts are input via IRQ1 of the master. In general, eight
different slaves can be accommodated by a single PIC.
Example 2: ARM Interrupt Architecture ARM stands for Advanced RISC
Machines. ARM is a 16 /32-bit architecture that is used for portable devices because
of its low power consumption and reasonable performance. Interrupt requests to the
ARM core are collected and controlled by the interrupt controller, which is calledATIC. The interrupt controller provides an interface to the core and can collectup to 64 interrupt requests.
The usual sequence of events for interrupts is as follows. Interrupts would be
enabled at the source (such as a peripheral), then enabled in the interrupt controller,and ﬁnally, enabled to the core. When an interrupt occurs at the source, its signal isrouted to the interrupt controller then to the ARM core. In the interrupt controller,the interrupt can be enabled or disabled to the core and can be assigned a priorityDevice 8259A
PIC
CPU ISR1. IRQ# Interrupt
2. INTR3. INTA 
4. INT #
5. Invoke8. Return7. End Interrupt
6. Service
Figure 8.8 Interrupt handling in 80 /C286
TABLE 8.1 Standard IBM-PC Interrupt Type Numbers
for Typical Devices
DeviceIRQ
no.Interrupt
type number
Programmable interval timer 0 08H
Keyboard 1 09HCascading to the second PICs 2 Reserved
Serial communication port (COM2) 3 0BH
Serial communication port (COM1) 4 0CHFixed disk controller 5 0DH
Floppy disk controller 6 0EH
Parallel printer controller 7 0FH8.3. INTERRUPT-DRIVEN I /O 171level. Once the interrupt request reaches the core, it will halt the core from its normal
processing routines to allow the interrupt request to be serviced.
Among the different interrupt requests that the ARM core can handle are IRQ and
FIQ requests. The IRQ (normal interrupt request) is used for general-purpose inter-rupt handling. It has a lower priority than an FIQ (fast interrupt request) and is
masked out when an FIQ sequence is entered. The FIQ is used to support high-
speed data transfer or channel processes.IRQ0
IRQ1
IRQ2
IRQ3
IRQ4
IRQ5
IRQ6
IRQ7D0-D7INT
Address busINTR8259A Master
INTA
INTA
CS
SP/EN
8259A SlaveIRQ8
IRQ9
IRQ10
IRQ11
IRQ12
IRQ13
IRQ14
IRQ15D0-D7INT
Address busINTA
CS
SP/EN
Figure 8.9 Fifteen different interrupts are supported by two PICs wired in cascade172 INPUT–OUTPUT DESIGN AND ORGANIZATIONSimilar to the 80 /C286, the addresses of the interrupt handlers are stored in a vector
table, which is shown in Table 8.2. For example, when an IRQ is detected by the core,
it accesses address 0 /C218 of the vector table and executes the instruction loaded in that
address. Normally, the instruction found at 0 /C218 of the vector table is of the form:
LDR PC ,IRQ_Handler (load the address of the IRQ interrupt handler in the PC).
When an FIQ is detected by the core, it accesses address 0 /C21C of the vector table
and executes the instruction loaded in that address. Normally, the instruction foundat 0/C21C of the vector table is of the form: LDR PC ,FIQ_Handler .
When an interrupt occurs, the following happens inside the core:
1. The CPSR (current program state register) is copied to the SPSR (saved pro-
gram status register) of the mode being entered.
2. The CPSR bits are set as appropriate to the mode being entered, the core is set
to ARM state, and the relevant interrupt disable ﬂags are set.
3. The appropriate set of banked registers are banked in.
4. The return address is stored in the link register (of the relevant mode).
5. The PC is set to the relevant vector address.
For example, when an IRQ interrupt is detected, the ARM core enables SPSR_irq as
the CPSR, enters the IRQ mode by setting the mode bits in the CSPR to 10010, dis-
ables Normal interrupts by setting the I bit in the CPSR, saves the address of the nextinstruction R14_irq, and loads 0 /C218 into the PC. At address 0 /C218, an instruction
will load the address of the interrupt handler into the PC. Similarly, when an FIQ
interrupt is detected, the ARM core enables SPSR_ﬁq as the CPSR, enters the
FIQ mode by setting the mode bits in the CSPR to 10001, disables Normal andFast interrupts by setting the F and I bits in the CPSR, saves the address of thenext instruction R14_ﬁq, and loads 0 /C21C into the PC. At address 0 /C21C, an instruc-
tion will load the address of the interrupt handler into the PC.
MC9328MX1 /MXL AITC The MC9328MX1 /MXL AITC contains twenty-six
32-bit registers, which are described in Table 8.3. Using these registers, the AITC
allows the selection of whether a pending interrupt source will create a Normal
interrupt (IRQ) or a Fast interrupt (FIQ) to the core. This is accomplished via theTABLE 8.2 Interrupt Vector Table
Exception type Mode Address
Reset Supervisor 0 /C200000000
Undeﬁned instructions Undeﬁned 0 /C200000004
Software interrupts (SWI) Supervisor 0 /C200000008
Prefetch abort Abort 0 /C20000000C
Data abort Abort 0 /C200000010
IRQ (Normal interrupt) IRQ 0 /C200000018
FIQ (Fast interrupt) FIQ 0 /C20000001C8.3. INTERRUPT-DRIVEN I /O 173TABLE 8.3 The AITC Registers
Register Description
INTCNTL Conﬁgures speciﬁc control functions of the AITC.
NIMASK Controls the Normal interrupt mask level. All Normal interrupt
priority levels at or below what is programmed in the NIMASK
register will be masked. Normal interrupt priorities are
programmed via the NIPRIORITY[7 : 0] registers.
INTENNUM Provides hardware accelerated enabling of interrupts. This is done by
programming this register with the interrupt source that is desired
to be enabled. Doing so will immediately enable (set) this interrupt
source bit in the INTENABLEH /L register.
INTDISNUM Provides hardware accelerated disabling of interrupts. This is done
by programming this register with the interrupt source that isdesired to be disabled. Doing so will immediately disable (clear)
this interrupt source bit in the INTENABLEH /L register.
INTENABLEH Used to enable pending interrupt source bits [63–32] to the core.
INTENABLEL Used to enable pending interrupt source bits [31–0] to the core.
INTTYPEH Used to select whether an enabled and pending interrupt source bit
[63–32] will create a Normal interrupt or Fast interrupt to the core.
INTTYPEL Used to select whether an enabled and pending interrupt source bit
[31–0] will create a Normal interrupt or Fast interrupt to the core.
NIPRIORITY[7 : 0] Provides software prioritization of Normal interrupts.
NIVECSR Provides the priority of the highest pending Normal interrupt and
provides the source number of the highest pending Normal interrupt.
FIVECSR Provides the source number of the highest pending Fast interrupt.
INTSRCH Reﬂects the status of interrupt request inputs (sources 63–32) into
the interrupt controller.
INTSCRL Reﬂects the status of interrupt request inputs (sources 31–0) into the
interrupt controller.
INTFRCH Allows for software generation of interrupts for interrupt sources 63
through 32.
INTFRCL Allows for software generation of interrupts for interrupt sources 31
through 0.
NIPNDH Reﬂects the source number(s) of pending Normal interrupt requests,
for interrupt sources 63 through 32.
NIPNDL Reﬂects the source number(s) of pending Normal interrupt requests,
for interrupt sources 31 through 0.
FIPNDH Reﬂects the source number(s) of pending Fast interrupt requests, for
interrupt sources 63 through 32.
FIPNDL Reﬂects the source number(s) of pending Fast interrupt requests, for
interrupt sources 31 through 0.174
INPUT–OUTPUT DESIGN AND ORGANIZATIONINTTYPEH and INTTYPEL registers. Each bit in these registers corresponds to an
interrupt source available in the system. Setting a bit will select its correspondinginterrupt source as a Fast interrupt, whereas clearing this bit will select its corre-sponding bit as a Normal interrupt. In the INTTYPEL register, bit 0 correspondsto interrupt source 0, bit 1 corresponds to interrupt source 1, and so on up to bit31, which corresponds to interrupt source 31. In the INTTYPEH register, bit 0 cor-responds to interrupt source 32, bit 1 corresponds to interrupt source 33, and so on up
to bit 31, which corresponds to interrupt source 63.
After determining the type of the pending interrupt, the next step is to enable the
interrupt. This can be done via the INTENABLEH and INTENABLEL registers. To
enable a pending interrupt to the core, its corresponding interrupt source bit in theINTENABLEH or INTENABLEL must be set. Likewise, to disable the interrupt,clear this bit. In the INTENABLEL register, bit 0 corresponds to interrupt source0, bit 1 corresponds to interrupt source 1, and so on up to bit 31, which correspondsto interrupt source 31. In the INTENABLEH register, bit 0 corresponds to interrupt
source 32, bit 1 corresponds to interrupt source 33, and so on up to bit 31, which
corresponds to interrupt source 63. For example, to select interrupt source bit 15as a Normal interrupt, clear bit 15 in the INTTYPEL register. Then, to enable thisinterrupt, set bit 15 in the INTENABLEL register. Likewise, to select interruptsource bit 45 as a Fast interrupt, set bit 13 in the INTTYPEH register. Then, toenable this interrupt, set bit 13 in the INTENABLEH. The AITC also allows the pro-grammer to prioritize the pending Normal interrupt sources to one of 16 different
priority levels. This can be done in the NIPRIORITY[7:0] registers.
8.4. DIRECT MEMORY ACCESS (DMA)
The main idea of direct memory access (DMA) is to enable peripheral devices to cut
out the “middle man” role of the CPU in data transfer. It allows peripheral devices to
transfer data directly from and to memory without the intervention of the CPU. Having
peripheral devices access memory directly would allow the CPU to do other work,which would lead to improved performance, especially in the cases of large transfers.
The DMA controller is a piece of hardware that controls one or more peripheral
devices. It allows devices to transfer data to or from the system’s memory withoutthe help of the processor. In a typical DMA transfer, some event notiﬁes the DMA
controller that data needs to be transferred to or from memory. Both the DMA and
CPU use memory bus and only one or the other can use the memory at the same
time. The DMA controller then sends a request to the CPU asking its permission
to use the bus. The CPU returns an acknowledgment to the DMA controller grantingit bus access. The DMA can now take control of the bus to independently conductmemory transfer. When the transfer is complete the DMA relinquishes its control ofthe bus to the CPU. Processors that support DMA provide one or more input signalsthat the bus requester can assert to gain control of the bus and one or more output
signals that the CPU asserts to indicate it has relinquished the bus. Figure 8.10
shows how the DMA controller shares the CPU’s memory bus.8.4. DIRECT MEMORY ACCESS (DMA) 175Direct memory access controllers require initialization by the CPU. Typical setup
parameters include the address of the source area, the address of the destination area,
the length of the block, and whether the DMA controller should generate a processorinterrupt once the block transfer is complete. A DMA controller has an address reg-ister, a word count register, and a control register. The address register contains anaddress that speciﬁes the memory location of the data to be transferred. It is typicallypossible to have the DMA controller automatically increment the address register
after each word transfer, so that the next transfer will be from the next memory
location. The word count register holds the number of words to be transferred.The word count is decremented by one after each word transfer. The control registerspeciﬁes the transfer mode.
Direct memory access data transfer can be performed in burst mode or single-
cycle mode. In burst mode, the DMA controller keeps control of the bus until allthe data has been transferred to (from) memory from (to) the peripheral device.This mode of transfer is needed for fast devices where data transfer cannot be
stopped until the entire transfer is done. In single-cycle mode (cycle stealing), the
DMA controller relinquishes the bus after each transfer of one data word. This mini-mizes the amount of time that the DMA controller keeps the CPU from controllingthe bus, but it requires that the bus request /acknowledge sequence be performed for
every single transfer. This overhead can result in a degradation of the performance.The single-cycle mode is preferred if the system cannot tolerate more than a fewcycles of added interrupt latency or if the peripheral devices can buffer very large
amounts of data, causing the DMA controller to tie up the bus for an excessive
amount of time.
The following steps summarize the DMA operations:
1. DMA controller initiates data transfer.
2. Data is moved (increasing the address in memory, and reducing the count of
words to be moved).CPU
MemoryDMA
Controller
DeviceDMA AcknowledgementDMA Request
Control SignalsData BusAddress Bus
Figure 8.10 DMA controller shares the CPU’s memory bus176 INPUT–OUTPUT DESIGN AND ORGANIZATION3. When word count reaches zero, the DMA informs the CPU of the termination
by means of an interrupt.
4. The CPU regains access to the memory bus.
A DMA controller may have multiple channels. Each channel has associated with it
an address register and a count register. To initiate a data transfer the device driversets up the DMA channel’s address and count registers together with the direction ofthe data transfer, read or write. While the transfer is taking place, the CPU is free to
do other things. When the transfer is complete, the CPU is interrupted.
Direct memory access channels cannot be shared between device drivers. A
device driver must be able to determine which DMA channel to use. Some devices
have a ﬁxed DMA channel, while others are more ﬂexible, where the device drivercan simply pick a free DMA channel to use.
Linux tracks the usage of the DMA channels using a vector of dma_chan data
structures (one per DMA channel). The dma_chan data structure contains just twoﬁelds, a pointer to a string describing the owner of the DMA channel and a ﬂag indi-
cating if the DMA channel is allocated or not.
8.5. BUSES
A bus in computer terminology represents a physical connection used to carry a
signal from one point to another. The signal carried by a bus may represent address,data, control signal, or power. Typically, a bus consists of a number of connectionsrunning together. Each connection is called a bus line . A bus line is normally ident-
iﬁed by a number. Related groups of bus lines are usually identiﬁed by a name. Forexample, the group of bus lines 1 to 16 in a given computer system may be used to
carry the address of memory locations, and therefore are identiﬁed as address lines .
Depending on the signal carried, there exist at least four types of buses: address ,
data,control , and power buses. Data buses carry data, control buses carry control
signals, and power buses carry the power-supply /ground voltage. The size
(number of lines) of the address, data, and control bus varies from one system to
another. Consider, for example, the bus connecting a CPU and memory in a givensystem, called the CPU bus . The size of the memory in that system is 512M-
word and each word is 32 bits. In such system, the size of the address bus should
be log
2(512/C2220)¼29 lines, the size of the data bus should be 32 lines, and at
least one control line ( /C22R R=W) should exist in that system.
In addition to carrying control signals, a control bus can carry timing signals.
These are signals used to determine the exact timing for data transfer to and from
a bus; that is, they determine when a given computer system component, such asthe processor, memory, or I /O devices, can place data on the bus and when they
can receive data from the bus. A bus can be synchronous if data transfer over the
bus is controlled by a bus clock . The clock acts as the timing reference for all bus
signals. A bus is asynchronous if data transfer over the bus is based on the avail-
ability of the data and not on a clock signal. Data is transferred over an asynchronous8.5. BUSES 177bus using a technique called handshaking . The operations of synchronous and asyn-
chronous buses are explained below.
To understand the difference between synchronous and asynchronous, let us con-
sider the case when a master such as a CPU or DMA is the source of data to be trans-
ferred to a slave such as an I /O device. The following is a sequence of events
involving the master and slave:
1. Master: send request to use the bus
2. Master: request is granted and bus is allocated to master3. Master: place address /data on bus
4. Slave: slave is selected5. Master: signal data transfer
6. Slave: take data7. Master: free the bus
8.5.1. Synchronous Buses
In synchronous buses, the steps of data transfer take place at ﬁxed clock cycles.
Everything is synchronized to bus clock and clock signals are made available toboth master and slave. The bus clock is a square wave signal. A cycle starts atone rising edge of the clock and ends at the next rising edge, which is the beginningof the next cycle. A transfer may take multiple bus cycles depending on the speedparameters of the bus and the two ends of the transfer.
One scenario would be that on the ﬁrst clock cycle, the master puts an address on
the address bus, puts data on the data bus, and asserts the appropriate control lines.Slave recognizes its address on the address bus on the ﬁrst cycle and reads the newvalue from the bus in the second cycle.
Synchronous buses are simple and easily implemented. However, when connect-
ing devices with varying speeds to a synchronous bus, the slowest device will deter-mine the speed of the bus. Also, the synchronous bus length could be limited toavoid clock-skewing problems.
8.5.2. Asynchronous Buses
There are no ﬁxed clock cycles in asynchronous buses. Handshaking is used instead.
Figure 8.11 shows the handshaking protocol. The master asserts the data-ready line
1
23
41
23
4Data-ready
Data-acceptData Data Data Data-Bus
Figure 8.11 Asynchronous bus timing using handshaking protocol178 INPUT–OUTPUT DESIGN AND ORGANIZATION(point 1 in the ﬁgure) until it sees a data-accept signal. When the slave sees a data-
ready signal, it will assert the data-accept line (point 2 in the ﬁgure). The rising of thedata-accept line will trigger the falling of the data-ready line and the removal of datafrom the bus. The falling of the data-ready line (point 3 in the ﬁgure) will trigger thefalling of the data-accept line (point 4 in the ﬁgure). This handshaking, which iscalled fully interlocked, is repeated until the data is completely transferred. Asyn-chronous bus is appropriate for different speed devices.
8.5.3. Bus Arbitration
Bus arbitration is needed to resolve conﬂicts when two or more devices want to
become the bus master at the same time. In short, arbitration is the process of select-ing the next bus master from among multiple candidates. Conﬂicts can be resolved
based on fairness or priority in a centralized or distributed mechanisms.
Centralized Arbitration In centralized arbitration schemes, a single arbiter is
used to select the next master. A simple form of centralized arbitration uses a bus
request line, a bus grant line, and a bus busy line. Each of these lines is shared bypotential masters, which are daisy-chained in a cascade. Figure 8.12 shows this
simple centralized arbitration scheme.
In the ﬁgure, each of the potential masters can submit a bus request at any time.
A ﬁxed priority is set among the masters from left to right. When a bus request is
received at the central bus arbiter, it issues a bus grant by asserting the bus grantline. When the potential master that is closest to the arbiter (potential master 1) seesthe bus grant signal, it checks to see if it had made a bus request. If yes, it takes overthe bus and stops propagation of the bus grant signal any further. If it has not made arequest, it will simple turn the bus grant signal to the next master to the right (potential
master 2), and so on. When the transaction is complete, the busy line is deasserted.
Instead of using shared request and grant lines, multiple bus request and bus grant
lines can be used. In one scheme, each master will have its own independent request
and grant line as shown in Figure 8.13. The central arbiter can employ any priority-based or fairness-based tiebreaker. Another scheme allows the masters to have mul-tiple priority levels. For each priority level, there is a bus request and a bus grant
line. Within each priority level, daisy chain is used. In this scheme, each device isattached to the daisy chain of one priority level. If the arbiter receives multiple
Central
Bus
ArbiterPotential
Master 1Potential
Master 2Potential
Master n
Bus BusyBus RequestBus Grant 
Figure 8.12 Centralized arbiter in a daisy-chain scheme8.5. BUSES 179bus requests from different levels, it grants the bus to the level with the highest
priority. Daisy chaining is used among the devices of that level. Figure 8.14shows an example of four devices included in two priority levels. Potential
master 1 and potential master 3 are daisy-chained in level 1 and potential master
2 and potential master 4 are daisy-chained in level 2.
Decentralized Arbitration In decentralized arbitration schemes, priority-based
arbitration is usually used in a distributed fashion. Each potential master has a
unique arbitration number, which is used in resolving conﬂicts when multiplerequests are submitted. For example, a conﬂict can always be resolved in favor of
the device with the highest arbitration number. The question now is how to deter-
mine which device has the highest arbitration number? One method is that a request-ing device would make its unique arbitration number available to all other devices.Each device compares that number with its own arbitration number. The device withthe smaller number is always dismissed. Eventually, the requester with the highestarbitration number will survive and be granted bus access.Req-1
Grant-1
Req-2
Grant-2 Req-nGrant-nCentral
Bus
ArbiterPotential
Master 1Potential
Master 2Potential
Master n
Bus Busy
Figure 8.13 Centralized arbiter with independent request and grant lines
Central
Bus
Arbiter
Request level 1
Bus BusyRequest level 2Grant level 1
Grant level 2Potential
Master 1Potential
Master 2Potential
Master 3Potential
Master 4
Figure 8.14 Centralized arbiter with two priority levels (four devices)180 INPUT–OUTPUT DESIGN AND ORGANIZATION8.6. INPUT–OUTPUT INTERFACES
An interface is a data path between two separate devices in a computer system. Inter-
face to buses can be classiﬁed based on the number of bits that are transmitted at agiven time to serial versus parallel ports. In a serial port, only 1 bit of data is trans-ferred at a time. Mice and modems are usually connected to serial ports. A parallelport allows more than 1 bit of data to be processed at once. Printers are the most
common peripheral devices connected to parallel ports. Table 8.4 shows a summary
of the variety of buses and interfaces used in personal computers.
TABLE 8.4 Descriptions of Buses and Interfaces Used in Personal Computers
Bus/Interface Description
PS/2 A type of port (or interface) that can be used to connect mice and
keyboards to the computer. The PS /2 port is sometimes called the
mouse port.
Industry standard
architecture (ISA)ISA was originally an 8-bit bus and later expanded to a 16-bit bus in
1984. In 1993, Intel and Microsoft introduced a plug and play
ISA bus that allowed the computer to automatically detect and set
up computer ISA peripherals such as a modem or sound card.
Extended industry
standardarchitecture
(EISA)EISA is an enhanced form of ISA, which allows for 32-bit data
transfers, while maintaining support for 8- and 16-bit expansion
boards. However, its bus speed, like ISA, is only 8 MHz. EISA is
not widely used, due to its high cost and complicated nature.
Micro channel
architecture
(MCA)MCA was introduced by IBM in 1987. It offered several additional
features over the ISA such as a 32-bit bus, automaticallyconﬁgured cards and bus mastering for greater efﬁciency. It isslightly superior to EISA, but not many expansion boards were
ever made to ﬁt MCA speciﬁcations.
VESA (Video
electronics
standardsassociation) local
bus (VLB)The VESA, a nonproﬁt organization founded by NEC, released the
VLB in 1992. It is a 32-bit bus that had direct access to the system
memory at the speed of the processor, commonly the 486 CPU(33/40 MHz). VLB 2.0 was later released in 1994 and had a
64-bit bus and a bus speed of 50 MHz.
Peripheral
component
interconnect (PCI)PCI was introduced by Intel in 1992, revised in 1993 to version 2.0,
and later revised in 1995 to PCI 2.1. It is a 32-bit bus that is alsoavailable as a 64-bit bus today. Many modern expansion boards
are connected to PCI slots.
Advanced graphic
port (AGP)AGP was introduced by Intel in 1997. AGP is a 32-bit bus designed for
the high demands of 3D graphics. AGP has a direct line to memory,
which allows 3D elements to be stored in the system memory
instead of the video memory. AGP is geared towards data-intensive
graphics cards, such as 3D accelerators; its design allows for datathroughput at rates of 266 MB /s.
(continued )8.6. INPUT–OUTPUT INTERFACES 1818.7. SUMMARY
One of the major features in a computer system is its ability to exchange data with
other devices and to allow the user to interact with the system. This chapter focused
on the I /O system and the way the processor and the I /O devices exchange data in a
computer system. The chapter described three ways of organizing I /O: programmed
I/O, interrupt-driven I /O, and DMA. In programmed I /O, the CPU handles the trans-
fers, which take place between registers and the devices. In interrupt-driven I /O, CPU
handles data transfers and an I /O module is running concurrently. In DMA, data are
transferred between memory and I /O devices without intervention of the CPU. We
also studied two methods for synchronization: polling and interrupts. In polling, the
processor polls the device while waiting for I /O to complete. Clearly processor
cycles are wasted in this method. Using interrupts, processors are free to switch to
other tasks during I /O. Devices assert interrupts when I /O is complete. InterruptsTABLE 8.4 Continued
Bus/Interface Description
Universal serial bus
(USB)USB is an external bus developed by Intel, Compaq, DEC, IBM,
Microsoft, NEC and Northern Telcom. It was released in 1996 with
the Intel 430HX Triton II Mother Board. USB has the capability of
transferring 12 Mbps, supporting up to 127 devices. Many devices
can be connected to USB ports, which support plug and play.
FireWire (IEEE
1394)FireWire is a type of external bus, which supports very fast transfer
rates: 400 Mbps. Because of this, FireWire is suitable for
connecting video devices, such as VCRs, to the computer.
Small computer
system interface(SCSI)SCSI is a type of parallel interface that is commonly used for mass
storage devices. SCSI can transfer data at rates of 4 MB /s; in
addition, there are several varieties of SCSI that support higher
speeds: Fast SCSI (10 MB /s), Ultra SCSI and Fast Wide SCSI
(20 MB /s), as well as Ultra Wide SCSI (40 MB /s).
Integrated drive
electronics (IDE)IDE is a commonly used interface for hard disk drives and
CD-ROM drives. It is less expensive than SCSI, but offers
slightly less in terms of performance.
Enhanced integrated
drive electronics(EIDE)EIDE is an improved version of IDE, which offers better
performance than standard SCSI. It offers transfer rates between4 and 16.6 MB /s.
PCI-X PCI-X is a high performance bus that is designed to meet the
increased I /O demands of technologies such as Fibre Channel,
Gigabit Ethernet, and Ultra3 SCSI.
Communication and
network riser(CNR)CNR was introduced by Intel in 2000. It is a speciﬁcation that
supports audio, modem USB and local area networking interfaces
of core logic chipsets.182
INPUT–OUTPUT DESIGN AND ORGANIZATIONincurs some delay penalty. Two examples of interrupt handling were covered: 80 /C286
family and ARM. The chapter also covered buses and interfaces. A wide variety of
interfaces and buses used in personal computers are summarized.
EXERCISES
1. Conduct an Internet search on I /O devices and prepare a table categorizing
the different devices into separate categories, for example input, output,
character-based, block-based, and so on. For every entry in the table, indicateits speed, interface, and category.
2. What are the advantages and disadvantages of isolated versus memory
mapped I /O.
3. Show how data transfer from disk to memory is conducted under each of the
following I /O schemes: programmed I /O, interrupt-driven I /O, and DMA.
Show the steps taken in each case.
4. If an interrupt requires 50 ms of overhead time, and polling requires 5 ms per
device, describe different situations where each seems better than the other.
5. What entities in a computer system does a device driver communicate with?
What are the functions of a device driver? List all operations.
6. What types of operations is DMA used to accelerate?
7. A DMA module is transferring data to memory using cycle stealing from a
device that transmits data at rate of 19,200 bits per second. The speed of
the CPU is 3 MIPS. By how much would the DMA module affect the perform-
ance of the CPU.
8. Describe the scenarios in which a synchronous bus would outperform an asyn-
chronous bus and vice versa.
9. Discuss the advantages and disadvantages of the different bus arbitration
policies covered in the chapter. Prepare a contract table that compares the
arbitration techniques from both the implementation and operational
aspects.
REFERENCES AND FURTHER READING
I. Englander, The Architecture of Computer Hardware and System Software , John Wiley,
New York, 1996.
C. Hamacher, Z. Vranesic, and S. Zaky, Computer Organization , 5th ed., McGraw-Hill,
New York, 2002.
V. Heuring, and H. Jordan, Computer Systems Design and Architecture , Addison Wesley, NJ,
USA, 1997.
S. Shiva, Computer Design and Architecture , Harper Collins, MA, USA, 1991.
A. Tanenbaum, Structured Computer Organization , 4th ed., Prentice Hall, NJ, USA, 1999.REFERENCES AND FURTHER READING 183J. Uffenbeck, The 80 /C286 Family, Design, Programming, and Interfacing , 3rd ed., Prentice
Hall, Essex, UK, 2002.
Websites
PCI Local Bus Speciﬁcations, www.pcisig.com /developers
SCSI-3 Architecture Model (SAM), www.ansi.org
Universal Serial Bus Speciﬁcation, www.usb.org /developers
www.arm.comwww.motorola.com /semiconductors184
INPUT–OUTPUT DESIGN AND ORGANIZATION& CHAPTER 9
Pipelining Design Techniques
There exist two basic techniques to increase the instruction execution rate of a pro-
cessor. These are to increase the clock rate, thus decreasing the instruction executiontime, or alternatively to increase the number of instructions that can be executedsimultaneously. Pipelining and instruction-level parallelism are examples of thelatter technique. Pipelining owes its origin to car assembly lines. The idea is tohave more than one instruction being processed by the processor at the sametime. Similar to the assembly line, the success of a pipeline depends upon dividingthe execution of an instruction among a number of subunits (stages), each perform-
ing part of the required operations. A possible division is to consider instruction
fetch ( F), instruction decode ( D), operand fetch ( F), instruction execution ( E),
and store of results ( S) as the subtasks needed for the execution of an instruction.
In this case, it is possible to have up to ﬁve instructions in the pipeline at thesame time, thus reducing instruction execution latency. In this Chapter, we discussthe basic concepts involved in designing instruction pipelines. Performancemeasures of a pipeline are introduced. The main issues contributing to instructionpipeline hazards are discussed and some possible solutions are introduced. In
addition, we introduce the concept of arithmetic pipelining together with the prob-
lems involved in designing such a pipeline. Our coverage concludes with a review ofa recent pipeline processor.
9.1. GENERAL CONCEPTS
Pipelining refers to the technique in which a given task is divided into a number of
subtasks that need to be performed in sequence. Each subtask is performed by a
given functional unit. The units are connected in a serial fashion and all of themoperate simultaneously. The use of pipelining improves the performance comparedto the traditional sequential execution of tasks. Figure 9.1 shows an illustration of thebasic difference between executing four subtasks of a given instruction (in this casefetching F, decoding D, execution E, and writing the results W) using pipelining and
sequential processing.
185Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.It is clear from the ﬁgure that the total time required to process three instruc-
tions ( I1,I2,I3) is only six time units if four-stage pipelining is used as compared
to 12 time units if sequential processing is used. A possible saving of up to 50%
in the execution time of these three instructions is obtained. In order to formulatesome performance measures for the goodness of a pipeline in processing a series oftasks, a space time chart (called the Gantt’s chart) is used. The chart shows the suc-cession of the subtasks in the pipe with respect to time. Figure 9.2 shows a Gantt’schart. In this chart, the vertical axis represents the subunits (four in this case) andthe horizontal axis represents time (measured in terms of the time unit requiredfor each unit to perform its task). In developing the Gantt’s chart, we assume
that the time (T) taken by each subunit to perform its task is the same; we call
this the unit time .
As can be seen from the ﬁgure, 13 time units are needed to ﬁnish executing 10
instructions ( I
1toI10). This is to be compared to 40 time units if sequential proces-
sing is used (ten instructions each requiring four time units).
In the following analysis, we provide three performance measures for the good-
ness of a pipeline. These are the Speed-up S(n) ,Throughput U(n) , and Efﬁciency
E(n). It should be noted that in this analysis we assume that the unit time T¼t
units.
1.Speed-up S (n) Consider the execution of mtasks (instructions) using
n-stages (units) pipeline. As can be seen, nþm/C01 time units are required(a) Sequential Processing
I1I1 I2 I3
I2
I3
TimeF1 D1 E1 W1
F1 D1 E1 W1F2 D2 E2 W2
F2 D2 E2 W2F3 D3 E3 W3
F3
123456789 1 0 1 1 1 2D3 E3 W3
(b) Pipelining
Figure 9.1 Pipelining versus sequential processing
U4
U3
U2
U1 I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10I1 I2 I3 I4 I5 I6 I7 I8 I9 I10
123456789 1 0 1 1 1 2 1 3 Time
Figure 9.2 The space–time chart (Gantt chart)186 PIPELINING DESIGN TECHNIQUESto complete mtasks.
Speed -up S(n)¼Time using sequential processing
Time using pipeline processing¼m/C2n/C2t
(nþm/C01)/C2t
¼m/C2n
nþm/C01
Lim
m!1S(n)¼n(i:e:,n-fold increase in speed is theoretically possible)
2.Throughput U (n)
Throughput U (n)¼no:of tasks executed per unit time ¼m
(nþm/C01)/C2t
Lim
m!1U(n)¼1 assuming that t¼1 unit time
3.Efﬁciency E (n)
Efﬁciency E (n)¼Ratio of the actual speed-up to the maximum speed-up
¼Speed -up
n¼m
nþm/C01
Lim
m!1E(n)¼1
9.2. INSTRUCTION PIPELINE
The simple analysis made in Section 9.1 ignores an important aspect that can affect
the performance of a pipeline, that is, pipeline stall . A pipeline operation is said to
have been stalled if one unit (stage) requires more time to perform its function, thusforcing other stages to become idle. Consider, for example, the case of an instruction
fetch that incurs a cache miss . Assume also that a cache miss requires three extra
time units . Figure 9.3 illustrates the effect of having instruction I
2incurring a
cache miss (assuming the execution of ten instructions I1toI10).
U4
U3
U2
U1I1
I1
I1I2I3I4I5I6I7I8I9I10
I2I3I4I5I6I7I8I9I10
I2I3I4I5I6I7I8I9I10
I3 I1I2 I4I5I6I7I8I9I10
123456789 1 0 1 1 1 2 1 4 13 15 16
Figure 9.3 Effect of a cache miss on the pipeline9.2. INSTRUCTION PIPELINE 187The ﬁgure shows that due to the extra time units needed for instruction I2to be
fetched, the pipeline stalls, that is, fetching of instruction I3and subsequent instruc-
tions are delayed. Such situations create what is known as pipeline bubble (or pipe-
linehazards ). The creation of a pipeline bubble leads to wasted unit times, thus
leading to an overall increase in the number of time units needed to ﬁnish executing
a given number of instructions. The number of time units needed to execute the 10instructions shown in Figure 9.3 is now 16 time units, compared to 13 time units if
there were no cache misses.
Pipeline hazards can take place for a number of other reasons. Among these are
instruction dependency and data dependency. These are explained below.
9.2.1. Pipeline “Stall” Due to Instruction Dependency
Correct operation of a pipeline requires that operation performed by a stage MUST
NOT depend on the operation(s) performed by other stage(s). Instruction depen-dency refers to the case whereby fetching of an instruction depends on the resultsof executing a previous instruction. Instruction dependency manifests itself in the
execution of a conditional branch instruction. Consider, for example, the case of a
“branch if negative” instruction. In this case, the next instruction to fetch will notbe known until the result of executing that “branch if negative” instruction is
known. In the following discussion, we will assume that the instruction followinga conditional branch instruction is not fetched until the result of executing thebranch instruction is known (stored). The following example shows the effect ofinstruction dependency on a pipeline.
Example 1 Consider the execution of ten instructions I
1–I10on a pipeline con-
sisting of four pipeline stages: IF(instruction fetch), ID(instruction decode), IE
(instruction execute), and IS(instruction results store). Assume that the instruction
I4is a conditional branch instruction and that when it is executed, the branch is
not taken, that is, the branch condition(s) is(are) not satisﬁed. Assume also that
when the branch instruction is fetched, the pipeline stalls until the result of executingthe branch instruction is stored. Show the succession of instructions in the pipeline;that is, show the Gantt’s chart. Figure 9.4 shows the required Gantt’s chart. Thebubble created due to the pipeline stall is clearly shown in the ﬁgure.
IS
IE
ID
IFI1
I1
I1I2I3I4 I5I6I7I8I9I10
I2I3I4 I5I6I7I8I9I10
I2I3I4 I5I6I7I8I9I10
I1I2I3I4 I5I6I7I8I9I10
123456789 1 0 1 1 1 2 1 4 13 15 16
Figure 9.4 Instruction dependency effect on a pipeline188 PIPELINING DESIGN TECHNIQUES9.2.2. Pipeline “Stall” Due to Data Dependency
Data dependency in a pipeline occurs when a source operand of instruction
Iidepends on the results of executing a preceding instruction, Ij,i.j. It should
be noted that although instruction Iican be fetched, its operand(s) may not be avail-
able until the results of instruction Ijare stored. The following example shows the
effect of data dependency on a pipeline.
Example 2 Consider the execution of the following piece of code:
†
ADD R 1,R2,R3; R3 R1þR2
SL R 3; R3 SL(R3)
SUB R 5,R6,R4; R4 R52R6
†
In this piece of code, the ﬁrst instruction, call it Ii, adds the contents of two registers
R1andR2and stores the result in register R3. The second instruction, call it Iiþ1,
shifts the contents of R3one bit position to the left and stores the result back
intoR3. The third instruction, call it Iiþ2, stores the result of subtracting the content
ofR6from the content of R5in register R4. In order to show the effect of such data
dependency, we will assume that the pipeline consists of ﬁve stages, IF,ID,OF,IE,
andIS. In this case, the OFstage represents the operand fetch stage. The functions of
the remaining four stages remain the same as explained before. Figure 9.5 shows the
Gantt’s chart for this piece of code. As shown in the ﬁgure, although instruction Iiþ1
has been successfully decoded during time unit kþ2, this instruction cannot pro-
ceed to the OFunit during time unit kþ3. This is because the operand to be fetched
byIiþ1during time unit kþ3 should be the content of register R3, which has been
modiﬁed by execution of instruction Ii. However, the modiﬁed value of R3will
not be available until the end of time unit kþ4. This will require instruction Iiþ1
to wait (at the output of the IDunit) until kþ5. Notice that instruction Iiþ2will
IS
IE
OF
ID
IF IiIiIiIiIi
Ii+1Ii+1
Ii+2
kk  + 1k+ 2k+ 3k+ 4k+ 5k+ 6Ii+2Ii+2Ii+2Ii+2
Ii+1Ii+1
Ii+1
Time
Figure 9.5 The write-after-write data dependency9.2. INSTRUCTION PIPELINE 189have also to wait (at the output of the IFunit) until such time that instruction Iiþ1
proceeds to the ID. The net result is that pipeline stall takes place due to the data
dependency that exists between instruction Iiand instruction Iiþ1.
The data dependency presented in the above example resulted because register R3
is the destination for both instructions IiandIiþ1. This is called a write-after-write
data dependency. Taking into consideration that any register can be written into
(or read from), then a total of four different possibilities exist, including the
write-after-write case. The other three cases are read-after-write, write-after-read,
and read-after-read. Among the four cases, the read-after-read case should notlead to pipeline stall. This is because a register read operation does not change
the content of the register. Among the remaining three cases, the write-after-write
(see the above example) and the read-after-write lead to pipeline stall. The followingpiece of code illustrates the read-after-write case:
†
ADD R
1,R2,R3; R3 R1þR2
SUB R 3,1 ,R4; R4 R321
†
In this case, the ﬁrst instruction modiﬁes the content of register R3(through a
write operation) while the second instruction uses the modiﬁed contents of R3
(through a read operation) to load a value into register R4. While these two
instructions are proceeding within a pipeline, care should be taken so that the
value of register R3read in the second instruction is the updated value resulting
from execution of the previous instruction. Figure 9.6 shows the Gantt’s chart for
this case assuming that the ﬁrst instruction is called Iiand the second instruction
is called Iiþ1.
It is clear that the operand of the second instruction cannot be fetched during time
unitkþ3 and that it has to be delayed until time unit kþ5. This is because the modi-
ﬁed value of the content of register R3will not be available until time slot kþ5.
IS
IE
OF
ID
IF IiIiIiIiIi
Ii+1Ii+1
kk  + 1k+ 2k+ 3k+ 4k+ 5k+ 6Ii+1Ii+1
Ii+1
Time
Figure 9.6 The read-after-write data dependency190 PIPELINING DESIGN TECHNIQUESFetching the operand of the second instruction during time slot kþ3 will lead to
incorrect results.
Example 3 Consider the execution of the following sequence of instructions on a
ﬁve-stage pipeline consisting of IF,ID,OF,IE, and IS. It is required to show the
succession of these instructions in the pipeline.
I1!Load 21,R1; R1 21;
I2!Load 5, R2; R2 5;
I3!Sub R2, 1, R2 R2 R221;
I4!Add R1,R2,R3; R3 R1þR2;
I5!Add R4,R5,R6; R6 R4þR5;
I6!SL R 3 R3 SL(R3)
I7!Add R6,R4,R7; R7 R4þR6;
In this example, the following data dependencies are observed:
Instructions Type of data dependency
I3andI2 Read-after-write and write-after-write (W-W)
I4andI1 Read-after-write (R-W)
I4andI3 Read-after-write (R-W)
I6andI4 Read-after-write and write-after-write (W-W)
I7andI5 Read-after-write (R-W)
Figure 9.7 illustrates the progression of these instructions in the pipeline taking
into consideration the data dependencies mentioned above. The assumption made
in constructing the Gantt’s chart in Figure 9.7 is that fetching an operand by aninstruction that depends on the results of a previous instruction execution is delayeduntil such operand is available, that is, the result is stored. A total of 16 time units arerequired to execute the given seven instructions taking into consideration the datadependencies among the different instructions.
IS
IE
OF
ID
IFI1
I1
I1I2I1I2
I3I3I3
I4I5I5
I6
I2 I4I5I4
I6I7I6I7
I7
I2I3 I4 I5I6 I7
I1I2I3I4 I5 I6I7
123456789 1 0 1 1 1 2 1 4 13 15 16
Figure 9.7 Gantt’s chart for Example 39.2. INSTRUCTION PIPELINE 191Based on the results obtained above, we can compute the speed-up and the
throughput for executing the piece of code given in Example 3 as:
Speed -up S(5)¼Time using sequential processing
Time using pipeline processing¼7/C25
16¼2:19
Throughput U (5)¼No :of tasks executed per unit time ¼7
16¼0:44
The discussion on pipeline stall due to instruction and data dependencies should
reveal three main points about the problems associated with having such dependen-
cies. These are:
1. Both instruction and data dependencies lead to added delay in the pipeline.
2. Instruction dependency can lead to the fetching of the wrong instruction.3. Data dependency can lead to the fetching of the wrong operand.
There exist a number of methods to deal with the problems resulting from instruction
and data dependencies. Some of these methods try to prevent the fetching of thewrong instruction or the wrong operand while others try to reduce the delay incurredin the pipeline due to the existence of instruction or data dependency. A number ofthese methods are introduced below.
Methods Used to Prevent Fetching the Wrong
Instruction or Operand
Use of NOP (No Operation) This method can be used in order to prevent
the fetching of the wrong instruction, in case of instruction dependency, or
fetching the wrong operand, in case of data dependency. Recall Example 1. In
that example, the execution of a sequence of ten instructions I
1–I10on a
pipeline consisting of four pipeline stages: IF,ID,IE, and ISwere considered. In
order to show the execution of these instructions in the pipeline, we have assumedthat when the branch instruction is fetched, the pipeline stalls until the result of
executing the branch instruction is stored. This assumption was needed in order to
prevent fetching the wrong instruction after fetching the branch instruction. Inreal-life situations, a mechanism is needed to guarantee fetching the appropriate
instruction at the appropriate time. Insertion of “ NOP ” instructions will help
carrying out this task. A “ NOP ” is an instruction that has no effect on the status
of the processor.
Example 4 Consider the execution of ten instructions I
1–I10on a pipeline con-
sisting of four pipeline stages: IF,ID,IE, and IS. Assume that instruction I4is a con-
ditional branch instruction and that when it is executed, the branch is not taken; that
is, the branch condition is not satisﬁed.192 PIPELINING DESIGN TECHNIQUESIn order to execute this set of instructions while preventing the fetching of the
wrong instruction, we assume that a speciﬁed number of NOP instructions have
been inserted such that they follow instruction I4in the sequence and they precede
instruction I5. Figure 9.8 shows the Gantt’s chart illustrating the execution of the
new sequence of instructions (after inserting the NOP instructions). The ﬁgure
shows that the insertion of THREE NOP instructions after instruction I4will guar-
antee that the correct instruction to fetch after I4, in this case I5, will only be fetched
during time slot number 8 at which the result of executing I4would have been stored
and the condition for the branch would have been known.
It should be noted that the number of NOP instructions needed is equal to ( n21),
where nis the number of pipeline stages.
Example 4 illustrates the use of NOP instructions to prevent fetching the wrong
instruction in the case of instruction dependency. A similar approach can be used to
prevent fetching the wrong operand in the case of data dependency. Consider the
execution of the following piece of code on a ﬁve-stage pipeline ( IF,ID,OF,IE,IS).
ADD R 1,R2,R3; R3 R1þR2
SUB R 3,1 ,R4; R4 R321
MOV R 5,R6; R6 R5
Note the data dependency in the form of read-after-write (R-W) between the ﬁrst
two instructions. Fetching the operand for the second instruction, that is, fetching
the content of R3, cannot proceed until the result of the ﬁrst instruction has been
stored. In order to achieve that, NOP instructions can be inserted between the ﬁrst
two instructions as shown below.
ADD R 1,R2,R3; R3 R1þR2
NOP
NOPSUB R
3,1 ,R4; R4 R321
MOV R 5,R6; R6 R5
Execution of the modiﬁed sequence of instructions is shown in Figure 9.9. The
ﬁgure shows that the use of NOP guarantees that during time unit #6 instructionIS
IE
ID
IFI1
I1
I1I2I3I4 I5I6I7I8I9I10
I2I3I4 I5I6I7I8I9I10
I2I3I4 I5I6I7I8I9I10
I1I2I3I4 I5I6I7I8I9I10
123456789 1 0 1 1 1 2 1 4 13 15 16Nop Nop Nop
Nop Nop Nop
Nop Nop Nop
Nop Nop Nop
Figure 9.8 The use of NOP instructions9.2. INSTRUCTION PIPELINE 193I2will fetch the correct value of R3. This is the value stored as a result of executing
instruction I1during time unit #5.
Methods Used to Reduce Pipeline Stall Due to
Instruction Dependency
Unconditional Branch Instructions In order to be able to reduce the pipeline
stall due to unconditional branches, it is necessary to identify the unconditional
branches as early as possible and before fetching the wrong instruction. It mayalso be possible to reduce the stall by reordering the instruction sequence. Thesemethods are explained below.
REORDERING OF INSTRUCTIONS In this case, the sequence of instructions are reor-
dered such that correct instructions are brought to the pipeline while guaranteeingthe correctness of the ﬁnal results produced by the reordered set of instructions. Con-sider, for example, the execution of the following group of instructions I
1,I2,I3,I4,
I5.,.,Ij,Ijþ1,.,.on a pipeline consisting of three pipeline stages: IF,IE, and IS. In this
group of instructions, I4is an unconditional branch instruction whereby the
target instruction is Ij. Execution of this group of instructions in the same
sequence as given will lead to the incorrect fetching of instruction I5after fetching
instruction I4. However, consider execution of the reordered sequence I1,I4,I2,I3,
I5.,.,Ij,Ijþ1,.,.. Execution of this reordered sequence using the three-stage pipeline
is shown in Figure 9.10.
The ﬁgure shows that the reordering of the instructions causes instruction Ijto be
fetched during time unit #5, that is, after instruction I4has been executed. Reorder-
ing of instructions can be done using a “smart” compiler that can scan the sequenceof code and decide on the appropriate reordering of instructions that will lead toIS
IE
OF
ID
IFI1 I2I1 I2
I3I3
123456789 1 0NOP NOP
NOP NOP
I1 I2I1 I2
I3I3 NOP NOP
NOP NOP
I1 I2I3 NOP NOP
Figure 9.9 Use of NOP in data dependency
IS
IE
IF
1234567I1 I3IjIj+1 I4I2I1 I3IjIj+1 I4I2I1 I3IjIj+1 I4I2
Figure 9.10 Instruction reordering194 PIPELINING DESIGN TECHNIQUESproducing the correct ﬁnal results while minimizing the number of time units lost
due to the instruction dependency. One important condition that must be satisﬁedin order for the reordering of the instruction method to produce correct results isthat the set of instructions that are swapped with the branch instruction hold nodata and /or instruction dependency relationship among them.
USE OF DEDICATED HARDWARE IN THE FETCH UNIT In this case, the fetch unit is
assumed to have associated with it a dedicated hardware unit capable of recognizing
unconditional branch instructions and computing the branch target address as
quickly as possible. Consider, for example, the execution of the same sequence ofinstructions as illustrated above. Assume also that the fetch unit has a dedicatedhardware unit capable of recognizing unconditional branch instructions and comput-ing the branch address using no additional time units. Figure 9.11 shows the Gantt’schart for this sequence of instructions. The ﬁgure shows that the correct sequence ofinstructions is executed while incurring no extra unit times.
The assumption of needing no additional time units to recognize branch instruc-
tions and computing the target branch address is unrealistic. In typical cases, theadded hardware unit to the fetch unit will require additional time unit(s) to carryout its task of recognizing branch instructions and computing target branchaddresses. During the extra time units needed by the hardware unit, if other instruc-tions can be executed, then the number of extra time units needed may be reducedand indeed may be eliminated altogether. This is the essence of the method shownbelow.
PRECOMPUTING OF BRANCHES AND REORDERING OF INSTRUCTIONS This method
can be considered as a combination of the two methods discussed in the previous
two sections above. In this case, the dedicated hardware (used to recognizebranch instructions and computing the target branch address) executes its taskconcurrently with the execution of other instructions. Consider, for example, thesame sequence of instructions given above. Assume also that the dedicatedhardware unit requires one time unit to carry out its task. In this case, reorderingof the instructions to become I
1,I2,I4,I3,I5.,.,Ij,Ijþ1,.,.should produce the correct
results while causing no additional lost time units. This is illustrated using the
Gantt’s chart in Figure 9.12. Notice that time unit #4 is used by the dedicated
IS I4 IjIj+1 I3 I2 I1
I4 IjIj+1 I3 I2 I1 IE
IF I4IjIj+1 I3 I2 I1
12345 6 7
Figure 9.11 Use of additional hardware unit for branch instruction recognition9.2. INSTRUCTION PIPELINE 195hardware unit to compute the target branch address concurrently with the fetching of
instruction I3.
It should be noted that the success of this method depends on the availability of
instructions to be executed concurrently while the dedicated hardware unit is com-puting the target branch address. In the case presented above, it was assumed thatreordering of instructions can provide those instructions that can be executed con-currently with the target branch computation. However, if such reordering is notpossible, then the use of an instruction queue together with prefetching of instruc-tions can help provide the needed conditions. This is explained below.
INSTRUCTION PREFETCHING This method requires that instructions can be fetched
and stored in an instruction queue before they are needed. The method also calls for
the fetch unit to have the required hardware needed to recognize branch instructions
and compute the target branch address. If a pipeline stalls due to data dependencycausing no new instructions to be fetched into the pipeline, then the fetch unit canuse such time to continue fetching instructions and add them to the instructionqueue. On the other hand, if a delay in the fetching of instructions occurs, forexample, due to instruction dependency, then those prefetched instructions in the
instruction queue can be used to provide the pipeline with new instructions, thus
eliminating some of the otherwise lost time units due to instruction dependency. Pro-
viding the appropriate instruction from the instruction queue to the pipeline is
usually done using what is called a “dispatch unit.” The technique of prefetchingof instructions and executing them during a pipeline stall due to instruction depen-dency is called “branch folding.”
Conditional Branch Instructions The techniques discussed above in the context
of unconditional branch instructions may not work in the case of conditional branch
instructions. This is because in conditional branching the target branch address will
not be known until the execution of the branch instruction has been completed.Therefore, a number of techniques can be used to minimize the number of losttime units due to instruction dependency represented by conditional branching.
DELAYED BRANCH Delayed branch refers to the case whereby it is possible to
ﬁll the location(s) following a conditional branch instruction, called the branch
delay slot(s) , with useful instruction(s) that can be executed until the targetIS I3 IjIj+1 I4 I2 I1
I3 IjIj+1 I4 I2 I1 IE
IF I3IjIj+1 I4 I2 I1
12345 6 7
Figure 9.12 Branch folding196 PIPELINING DESIGN TECHNIQUESbranch address is known. Consider, for example, the execution of the following
program loop on a pipeline consisting of two stages: Fetch ( F) and Execute ( E).
I1!Again :Load 5,R1; R1 5;
I2! Sub R 2; R2 R221;
I3! Bnn Again ; Branch to Again if result is Not Negative;
I4! Add R 4,R5,R3; R3 R4þR5;
It should be noted that at the end of the ﬁrst loop, either instruction I1or instruction
I4will have to be fetched depending on the result of executing instruction I3.T h e
way with which such a situation has been dealt will delay fetching of the next
instruction until the result of executing instruction I3is known. This will lead to
incurring extra delay in the pipeline. However, this extra delay may be avoided if
the sequence of instructions has been reordered to become as follows.
Again : Sub R 2; R2 R221;
Load 5,R1; R1 5;
Bnn Again ; Branch to Again if result is Not Negative;
Add R 4,R5,R3; R3 R4þR5;
Figure 9.13 shows the Gantt’s chart for executing the modiﬁed piece of code for thecase R
2¼3 before entering the loop.
The ﬁgure indicates that branching takes place one instruction later than the
actual place where the branch instruction appears in the original instructionsequence, hence the name “ delayed branch .” It is also clear from Figure 9.13 that
by reordering the sequence of instructions, it was possible to ﬁll the branch delaytime slot with a useful instruction, thus eliminating any extra delay in the pipeline.It has been shown in a number of studies that “smart” compilers were able to makeuse of one branch delay time slot in more than 80% of the cases. The use of branchdelay time slots has led to the improvement of both the speed-up and the throughput
of those processors using “smart” compilers.
PREDICTION OF THE NEXT INSTRUCTION TO FETCH This method tries to reduce the
time unit(s) that can potentially be lost due to instruction dependency by predicting
the next instruction to fetch after fetching a conditional branch instruction. The basisis that if the branch outcomes are random, then it would be possible to save about50% of the otherwise lost time. A simple way to carry out such a technique is to
E
FI2 I1 I3 I2 I1 I3 I2 I1 I3 I2 I1 I3 I4
I1 I2 I3 I2 I1 I3 I2 I1 I3 I2 I1 I3 I4
1 2 3 4 5 6 7 8 11 12 13 14 10 9
Figure 9.13 Delayed branch9.2. INSTRUCTION PIPELINE 197assume that whenever a conditional branch is encountered, the system predicts that
the branch will not be taken (or alternatively will be taken). In this way, fetching ofinstructions in sequential address order will continue (or fetching of instructionsstarting from the target branch instruction will continue). At the completion of thebranch instruction execution, the results will be known and a decision will haveto be made as to whether the instructions that were executed assuming that thebranch will not be taken (or taken) were the intended correct instruction sequence
or not. The outcome of this decision is one of two possibilities. If the prediction
was correct, then execution can continue with no wasted time units. If, on theother hand, the wrong prediction has been made, then care must be taken suchthat the status of the machine, measured in terms of memory and register contents,should be restored as if no speculative execution took place.
Prediction based on the above scheme will lead to the same branch prediction
decision every time a given instruction is encountered, hence the name static
branch prediction . It is the simplest branch prediction scheme and is done during
compilation time.
Another technique that can be used in branch prediction is dynamic branch pre-
diction. In this case, prediction is done at run time, rather than at compile time.When a branch is encountered, then a record is checked to ﬁnd out whether thatsame branch has been encountered before and if so, what was the decision madeat that time; that is, was the branch taken or not taken. A run time decision isthen made whether to take or not to take the branch. In making such a decision, a
two-state algorithm, “likely to be taken” (LTK) or “likely not to be taken”
(LNK), can be followed. If the current state is LTK and if the branch is taken,
then the algorithm will maintain the LTK state; otherwise it will switch to theLNK. If, on the other hand, the current state is LNK and the branch is not taken,then the algorithm will maintain the LNK state; otherwise it will switch to theLTK state. This simple algorithm should work ﬁne, particularly if the branch isgoing backwards, for example during the execution of a loop. It will, however,lead to misprediction when control reaches the last pass through the loop. A more
robust algorithm that uses four states has been used by the ARM 11 microarchitec-
ture (see below).
It is interesting to notice that a combination of dynamic and static branch predic-
tion techniques can lead to performance improvement. An attempt to use a dynamic
branch prediction is ﬁrst made, and if it is not possible, then the system can resort tothe static prediction technique.
Consider, for example, the ARM 11 microarchitecture (the ﬁrst implementation
of the ARMv6 instruction set architecture). This architecture uses a dynamic /static
branch prediction combination. A record in the form of a 64-entry, four-state branch
target address cache (BTAC) is used to help the dynamic branch prediction ﬁnding
whether a given branch has been encountered before. If the branch has been encoun-tered, the record will also show whether it was most frequently taken or most fre-quently not taken. If the BTAC shows that a branch has been encountered before,then a prediction is made based on the previous outcome. The four states are:strongly taken, weakly taken, strongly not taken, and weakly not taken.198 PIPELINING DESIGN TECHNIQUESIn the case that a record cannot be found for a branch, then a static branch pre-
diction procedure is used. The static branch prediction procedure investigates the
branch to ﬁnd out whether it is going backwards or forwards. A branch going back-wards is assumed to be part of a loop and the branch is assumed to be taken. Abranch going forwards is not taken. The ARM 11 employs an eight-stage pipeline.Every correctly predicted branch is found to lead to a typical saving of ﬁve processorclock cycles. Around 80% of branches are found to be correctly predicted using the
dynamic /static combination in the ARM 11 architecture. The pipeline features of
the ARM 11 are introduced in the next subsection.
A branch prediction technique based on the use of a 16K-entry branch history
record is employed in the UltraSPARC III RISC processor, a 14-stage pipeline. How-
ever, the impact of a misprediction, in terms of the number of cycles lost due to abranch misprediction is reduced by using the following approach. On predictionsthat a branch will be taken and while the branch target instructions are being fetched,the “fall-through” instructions are prepared for issue in parallel through the use of a
four-entry branch miss queue (BMQ). This reduces the misprediction penalty to
two cycles. The UltraSPARC III has achieved 95% success in branch prediction.The pipeline features of the UltraSPARC III are introduced in the next subsection.
Methods Used to Reduce Pipeline Stall Due to Data Dependency
Hardware Operand Forwarding Hardware operand forwarding allows the
result of one ALU operation to be available to another ALU operation in the
cycle that immediately follows. Consider the following two instructions.
ADD R
1,R2,R3; R3 R1þR2
SUB R 3,1 ,R4; R4 R321
It is easy to notice that there exists a read-after-write data dependency between thesetwo instructions. Correct execution of this sequence on a ﬁve-stage pipeline ( IF,ID,
OF,IE,IS) will cause a stall of the second instruction after decoding it and until
the result of the ﬁrst instruction is stored in R
3. Only at that time, the operand of
the second instruction, that is, the new value stored in R3, can be fetched by the
second instruction. However, if it is possible to have the result of the ﬁrst instruction
forwarded to the ALU during the same time unit as it is being stored in R3, then it
will be possible to reduce the stall time. This is illustrated in Figure 9.14.
The assumption that the operand of the second instruction be forwarded immedi-
ately after it is available and while it is being stored in R3requires a modiﬁcation in
the data path such that an added feedback path is created to allow for such operand
forwarding. This modiﬁcation is shown using dotted lines in Figure 9.15. It should
be noted that the needed modiﬁcation to achieve hardware operand forwarding isexpensive and requires careful issuing of control signals. It should also be noted
that if it is possible to perform both instruction decoding and operand fetching
during the same time unit, then there will be no lost time units.9.2. INSTRUCTION PIPELINE 199Software Operand Forwarding Operand forwarding can alternatively be
performed in software by the compiler. In this case, the compiler should be
“smart” enough to make the result(s) of performing some instructions quicklyavailable, as operand(s), for subsequent instruction(s). This desirable featurerequires the compiler to perform data dependency analysis in order to determine
the operand(s) that can possibly be made available (forwarded) to subsequent
instructions, thus reducing the stall time. This data dependency analysis requiresthe recognition of basically three forms. These are explained below using simpleexamples.
STORE-FETCH This case represents data dependency in which the result of
an instruction is stored in memory followed by a request for a fetch of thesame result by a subsequent instruction. Consider the following sequence of twoinstructions:
Store R
2,(R3); M[R3] R2
Load (R3),R4; R4 M[R3]
In this sequence, the operand needed by the second instruction (the contents ofmemory location whose address is stored in register R
3) is already available in
register R2and therefore can be immediately (forwarded) moved into register R4.I1I2I1I2I1 I2I1 I2
I1 I2
IFIDOFIEIS
12345 6 7
Figure 9.14 Hardware forwarding
Figure 9.15 Hardware forwarding200 PIPELINING DESIGN TECHNIQUESWhen it recognizes such data dependency, a “smart” compiler can replace the above
sequence by the following sequence:
Store R 2,(R3); M[R3] R2
Move R 2,R4; R4 R2
FETCH-FETCH This case represents data dependency in which the data stored by an
instruction is also needed as an operand by a subsequent instruction. Consider thefollowing instruction sequence:
Load (R
3),R2; R2 M[R3]
Load (R3),R4; R4 M[R3]
In this sequence, the operand needed by the ﬁrst instruction (the contents of memorylocation whose address is stored in register R
3) is also needed as an operand for the
second instruction. Therefore, this operand can be immediately (forwarded) movedinto register R
4. When it recognizes such data dependency, a “smart” compiler can
replace the above sequence by the following sequence.
Load (R3),R2; R2 M[R3]
Move R 2,R4; R4 R2
STORE-STORE This is the case in which the data stored by an instruction is overwrit-
ten by a subsequent instruction. Consider the following instruction sequence:
Store R 2,(R3); M[R3] R2
Store R 4,(R3); M[R3] R4
In this sequence, the results written during the ﬁrst instruction (the content ofregister R
2is written into memory location whose address is stored in register R3)
is overwritten during the second instruction by the contents of register R4. Assuming
that these two instructions are executed in sequence and that the result written by theﬁrst instruction will not be needed by an I /O operation, for example, a DMA, then
the sequence of these two instructions can be replaced by the following singleinstruction.
Store R
4,(R3); M[R3] R4
9.3. EXAMPLE PIPELINE PROCESSORS
In this section, we brieﬂy present two pipeline processors that use a variety of the
pipeline techniques presented in this chapter. Our focus in this coverage is on the9.3. EXAMPLE PIPELINE PROCESSORS 201pipeline features of these architectures. The two processors are the ARM 1026EJ-S
and the UltraSPARC III.
9.3.1. ARM 1026EJ-S Processor This processor is part of a family of RISC
processors designed by Advanced RISC Machine (ARM) Company. The series is
designed to suit high-performance, low-cost, and low-power embedded applications.
The ARM 022EJ-S integer core has multiple execution units, thus allowing a numberof instructions to exist in the same pipeline stage. It also allows the execution of sim-ultaneous instructions. The ARM 1026EJ-S can deliver a peak throughput of oneinstruction per cycle. The integer core consists of the following units:
1. Prefetch unit: This unit is responsible for instruction fetch. It also predicts the
outcome of branches whenever possible.
2. Integer unit: This unit is responsible for decoding of instructions coming out
of the prefetch unit. This unit contains a barrel shifter, ALU, and a multiplier.It executes instructions such as MOV ,ADD , and MUL . The integer unit helps
the load /store unit to execute load and store instructions. It also helps in
executing some coprocessor instructions.
3. Load /Store unit: This unit can load or store two registers (64 bits) per cycle.
ARM 1022EJ-S is a pipeline processor whose ALU consists of six stages. These are:
1. Fetch stage: for instruction cache access and branch prediction for instructions
that have already been fetched.
2. Issue stage: for initial instruction decoding.
3. Decode stage: for ﬁnal instruction decode, register read for ALU operations,
forwarding, and initial interlock resolution.
4. Execute stage: for data access address calculation, data processing shift, shift
and saturate, ALU operations, ﬁrst stage multiplication, ﬂag setting, condition
code check, branch mispredict detection, and store data register read.
5. Memory stage: for data cache access, second stage multiplication, and
saturations.
6. Write stage: for register write and instruction retirement.
In this arrangement, the Fetch stage uses a ﬁrst-in-ﬁrst-out (FIFO) buffer that canhold up to three instructions. The Issue and Decode stages can contain a predictedbranch in parallel with one other instruction. The Execute, Memory, and Write
stages can simultaneously contain any of the following.
1. A predicted branch
2. An ALU or multiply instruction3. Ongoing multiply load or store multiple instructions4. Ongoing multicycle coprocessor instructions202 PIPELINING DESIGN TECHNIQUESThe prefetch unit operates in the Fetch stage and can fetch 64 bits every cycle from
an instruction-side cache. It can, however, issue one 32-bit instruction per cycle tothe integer unit. Pending instructions are placed in the prefetch buffer by the prefetchunit. While an instruction is in the prefetch buffer, the branch prediction logic candecode it to see if it is a predictable branch. The prefetch buffer can hold up tothree instructions and enable the prefetch unit to:
1. Detect branch instructions ahead of the fetch stage
2. Predict those branches that are likely to be taken
3. Remove those branches that are not likely to be taken
If the branch is predicted to be taken, then the instruction address is redirected to
the branch target address. If, however, the branch is predicted not to be taken,then the next instruction is fetched. In case there is not enough time to completelyremove a branch, the fetch address is redirected anyway, thus reducing branchpenalty.
The integer unit executes unpredictable branches. To quickly obtain the required
address, a dedicated fast branch adder is used. This is done in order to avoid passing
through the barrel shift.
The prefetch buffer is ﬂushed in the following cases:
1. Entry into an exception processing sequence
2. A load to the program counter (PC)3. An arithmetic manipulation of the PC
4. Execution of an unpredicted branch
5. Detection of an erroneously predicted branch
A taken predicted branch is the only case that does not lead to automatic ﬂush of the
prefetch buffer. Mispredicted branches and unpredicted taken branches lead to athree-cycle penalty.
9.3.2. UltraSPARC III Processor The UltraSPARC III is based on the SUN
SPARC-V9 RISC architectural speciﬁcations. A number of features characterize
the SPARC-V9. Among these are the following:
1. Few and simple instruction formats. All instructions are 32-bit. Memory
access is done exclusively using Load and Store instructions.
2. Few addressing modes. Memory addressing has only two modes, the
RegisterþRegister and the Register þImmediate modes.
3. Triadic register operands. Most instructions operate on two register operands
or one register and a constant operand. The results in both cases are stored in a
third register.
4. Large window register ﬁle.9.3. EXAMPLE PIPELINE PROCESSORS 203The UltraSPARC III processor uses six independent units (see Fig. 9.16).
These are:
1. The Instruction Issue Unit (IIU). This unit predicts the program ﬂows, fetches
the predicted path from memory and directs the fetched instructions to theexecution pipeline. Instructions are forwarded to either the IEU or the FPU.The IIU incorporates a four-way associative instruction cache, an addresstranslation buffer, and a 16K-entry branch predictor.
2. The Integer Execute Unit (IEU). This unit executes all integer instructions,
including the integer loading and storing, the integer arithmetic, the logic,
and the branch instructions. The IEU is capable of executing up to four integer
instructions concurrently during a cycle time.Instruction Issue Unit (IIU)
Instruction Cache
Instruction Queue
Steering Logic
Integer Execution Unit (IEU) Floating-Point Unit (FPU)
Dependency/Trap FP Register File Add/Subtract
Graphics unit
Data Cache Unit (DCU)
Prefetch Store
System Interface Unit (SIU) External Memory Unit
Data Switch Controller Eternal Cache TagsInteger Register File
ALU Pipe
Load/Store/Special DivideMultiply
Data Write
Snoop Pipe Controller SRAM DRAM
Figure 9.16 The functional units of the UltraSPARC III204 PIPELINING DESIGN TECHNIQUES3. The Data Cache Unit (DCU). This unit contains three different level-one (L1)
data caches and a data address translation buffer. The data caches are: a
demand fetch (a four-way associative 64KB with 32-byte block size), a pre-fetch cache (a four-way associative 2KB with 64-byte block size), and awrite cache (a four-way associative 2KB with 64-byte block size).
4. The Floating-Point Unit (FPU). This unit executes all ﬂoating-point and
graphical instructions.
5. The External Memory Unit (EMU). This unit controls access to the two off-
chip memory modules. The two off-chip modules are the level-two (L2)data cache and the main memory.
6. The System Interface Unit (SIU). This unit provides a communication interface
between the microprocessor and the system external to it, such as the mainmemory, I /O devices, and other processors in a multiprocessing conﬁguration.
The UltraSPARC III has a 14-stage instruction pipeline. These are:
1. Address Generation Unit (A). This unit generates instruction fetch addresses.
2. Instruction Prefetch Unit (P). This unit fetches the second cycle of instruc-
tions from the cache and accesses the ﬁrst cycle of branch prediction.
3. Instruction Fetch Unit (F). This unit fetches the second cycle of instructions
from the cache and accesses the second cycle of branch prediction. The F
unit also performs the virtual to physical address translation.
4. Branch Target Calculation Unit (B). This unit computes the target address of
branches and decodes the ﬁrst cycle of instructions.
5. Instruction Decode Unit (I). This unit decodes the second cycle of instruc-
tions and directs them to the queue.
6. Instruction Steer Unit (J). This unit directs instructions to the appropriate
execution unit. Integer instructions are directed to the integer execution
unit while ﬂoating-point and graphical instructions are directed to theﬂoating-point unit.
7. Register File Read Unit (R). This unit reads the operands of the integer
register ﬁle.
8. Integer Execution Unit (E). This unit executes the integer instructions.
9. Date Cache Access Unit (C). This unit accesses the second cycle of date
cache, forwards load data for word and double word loads and executes
the ﬁrst cycle of ﬂoating-point instructions.
10. Memory Bypass Unit (M). This unit loads data alignment for half word and
bytes loads and executes the second cycle of ﬂoating-point instructions.
11. Working Register File Write Unit (W). This unit performs writes to the inte-
ger register ﬁle and executes the third cycle of ﬂoating-point instructions.
12. Pipe Extend Unit (X). This unit extends the integer pipeline for precise ﬂoat-
ing-point traps and executes the fourth cycle of ﬂoating-point instructions.9.3. EXAMPLE PIPELINE PROCESSORS 20513. Trap Unit (T). This unit reports traps upon their occurrences.
14. Done Unit (D). This unit writes the architectural register ﬁle.
Two main techniques are employed in the UltraSPARC III in dealing with branches.
These are explained below:
Branch Prediction The UltraSPARC III uses a branch prediction technique
that combines the static and the dynamic branch prediction techniques explained
before. In this case, branch prediction takes place in the IIU unit. It uses abranch prediction table and a hardware implementation of a dynamic predictionalgorithm.
BRANCH PREDICTION TABLE The branch prediction table (BPT) is a hardware
implementation of a table of a two-bit ﬁnite state machine (FSM). It is a saturated
up–down counter. When a branch is encountered, the branch target address and /
or the branch history are used to ﬁnd the table index of the location where the pre-
diction for the branch is found. The branch condition is predicted to be taken if itcorresponds to one of two FSM states: strong not taken or weak not taken. Thebranch condition is predicted to be taken if it corresponds to one of two FSMstates: weak taken or strong taken. The counter is incremented each time a branchis taken; otherwise it is decremented, hence the name up–down counter. If a counterreaches the strong taken state (11-state), it stays there as long as the branch is taken
and if it reaches the strong not taken (00-state), it stays there as long as the branch is
not taken, hence the name saturation. The BPT in the UltraSPARC III consists of16K-entry (16K 2-bit saturation up–down counters).
GLOBAL SHARE DYNAMIC PREDICTION ALGORITHM The global share ( gshare )
algorithm uses two levels of branch-history information to dynamically predictthe direction of branches. The ﬁrst level registers the history of the last kbranches
faced. This represents the global branching behavior. This level is implemented byproviding a global branch history register. This is basically a shift register that entersa 1 for every taken branch and a 0 for every untaken branch. The second level ofbranch history information registers the branching of the last soccurrences of the
speciﬁc pattern of the kbranches. This information is kept in the branch prediction
table. The gshare algorithm works by taking the lower bits of the branch target
address and XORing them with the history register to get the index that should be
used with the prediction table.
The UltraSPARC III uses a modiﬁed version of the gshare algorithm. This modi-
ﬁcation requires that the predictor be pipelined over two stages, that is, if the originalgshare algorithm were used, the predictor would be indexed by an old copy of the
program counter (PC). With the modiﬁed gshare algorithm, each time the predictor
is accessed, eight counters are read out and the three low-order bits of the PC registerare used to select one of them at the B pipeline stage.206 PIPELINING DESIGN TECHNIQUESInstruction Buffer (Queues) The UltraSPARC III instruction issue unit (IIU)
incorporates two instruction buffering queues: the branch instruction queue (BIQ)
and the branch miss queue (BMQ). These are introduced below.
BRANCH INSTRUCTION QUEUE (BIQ) This is a 20-entry queue that allows the fetch
and the execution unit to operate independently. The fetch unit predicts theexecution path and continuously ﬁlls the BIQ. When a taken branch is encountered,two fetch cycles are lost to ﬁll the BIQ.
BRANCH MISS QUEUE (BMQ) During the lost two cycles, the sequential instructions
that have been already accessed are buffered into a four-entry BMQ. If it is thenfound that the branch has been mispredicted, the instructions from the BMQ aredirected to the execution unit directly.
9.4. INSTRUCTION-LEVEL PARALLELISM
Contrary to pipeline techniques, instruction-level parallelism (ILP) is based on the
idea of multiple issue processors (MIP). An MIP has multiple pipelined datapaths
for instruction execution. Each of these pipelines can issue and execute one instruc-tion per cycle. Figure 9.17 shows the case of a processor having three pipes. Forcomparison purposes, we also show in the same ﬁgure the sequential and thesingle pipeline case. It is clear from the ﬁgure that while the limit on the number
of cycles per instruction in the case of a single pipeline is CPI ¼1, the MIP can
achieve CPI ,1.
In order to make full use of ILP, an analysis should be made to identify the
instruction and data dependencies that exist in a given program. This analysis
should lead to the appropriate scheduling of the group of instructions that can beissued simultaneously while retaining the program correctness. Static schedulingresults in the use of very long instruction word (VLIW) architectures, while dynamicscheduling results in the use of superscalar architectures.
In VLIW, an instruction represents a bundle of many operations to be issued sim-
ultaneously. The compiler is responsible for checking all dependencies and makingthe appropriate groupings /scheduling of operations. This is in contrast with super-
scalar architectures, which rely entirely on the hardware for scheduling ofinstructions.
Superscalar Architectures A scalar machine is able to perform only one arith-
metic operation at once. A superscalar architecture (SPA) is able to fetch, decode,
execute, and store results of several instructions at the same time. It does so by trans-forming a static and sequential instruction stream into a dynamic and parallel one, inorder to execute a number of instructions simultaneously. Upon completion, theSPA reinforces the original sequential instruction stream such that instructionscan be completed in the original order.
In an SPA instruction, processing consists of the fetch, decode, issue, and commit
stages. During the fetch stage, multiple instructions are fetched simultaneously.9.4. INSTRUCTION-LEVEL PARALLELISM 207Branch prediction and speculative execution are also performed during the fetch
stage. This is done in order to keep on fetching instructions beyond branch andjump instructions.
Decoding is done in two steps. Predecoding is performed between the main
memory and the cache and is responsible for identifying branch instructions.Actual decoding is used to determine the following for each instruction: (1) the oper-ation to be performed; (2) the location of the operands; and (3) the location wherethe results are to be stored. During the issue stage, those instructions among thedispatched ones that can start execution are identiﬁed. During the commit stage,generated values /results are written into their destination registers.
The most crucial step in processing instructions in SPAs is the dependency analy-
sis. The complexity of such analysis grows quadratically with the instruction word
size. This puts a limit on the degree of parallelism that can be achieved with SPAs
such that a degree of parallelism higher than four will be impractical. Beyond this(a) Sequential Processing Time
F1
I3I3
I2I2
I1I1
(b) PipeliningTime
(c) Multiple issueTimeF1 D1 E1 W1
D1 E1 W1
F1 D1 E1 W1
F4 D4 E4 W4
F5 D5 E5 W5
F6 D6 E6 W6
F7 D7 E7 W7
F8 D8 E8 W8
F9 D9 E9 W9F2
F2D2
D2E2
E2W2
W2
F2 D2 E2 W2F3 D3 E3 W3
F3 D3 E3 W3
F3 D3 E3 W3123456789 1 0 1 1 1 2
Figure 9.17 Multiple issue versus pipelining versus sequential processing208 PIPELINING DESIGN TECHNIQUESlimit, the dependence analysis and scheduling must be done by the compiler. This is
the basis for the VLIW approach.
Very Long Instruction Word (VLIW) In this approach, the compiler performs
dependency analysis and determines the appropriate groupings /scheduling of oper-
ations. Operations that can be performed simultaneously are grouped into a very
long instruction word (VLIW). Therefore, the instruction word is made longenough in order to accommodate the maximum possible degree of parallelism.For example, the IBM DAISY machine has an instruction word that is eight oper-ation long, called 8-issue machine.
In VLIW, resource binding can be done by devoting each ﬁeld of an instruction
word to one and only one functional unit. However, this arrangement will lead to alimit on the mix of instructions that can be issued per cycle. A more ﬂexible
approach is to allow a given instruction ﬁeld to be occupied by different kinds of
operations. For example, the Philips TriMedia machine, a 5-issue machine, has 27
functional units mapped to a 5-issue slot. In the IBM DAISY, every instruction
implements a multiway path selection scheme. In this case, the ﬁrst 72 bits of theVLIW is called the header and contain information on the tree form, condition
tests, and branch targets. The header is followed by eight 23-bit parcels, each encod-ing an operation. In order to solve the problem of providing operands to a largenumber of functional units, the IBM DAISY keeps eight identical copies of the
same register ﬁle, one for each of the eight functional units.
9.5. ARITHMETIC PIPELINE
The principles used in instruction pipelining can be used in order to improve the per-
formance of computers in performing arithmetic operations such as add, subtract,
and multiply. In this case, these principles will be used to realize the arithmetic cir-cuits inside the ALU. In this section, we will elaborate on the use of arithmetic pipe-line as a means to speed up arithmetic operations. We will start with ﬁxed-pointarithmetic operations and then discuss ﬂoating-point operations.
9.5.1. Fixed-Point Arithmetic Pipelines
The basic ﬁxed point arithmetic operation performed inside the ALU is the addition
of two n-bit operands A¼a
n21an22/C1/C1/C1a2a1a0and B¼bn21bn22/C1/C1/C1b2b1b0.
Addition of these two operands can be performed using a number of techniques.These techniques differ in basically two attributes: degree of complexity andachieved speed. These two attributes are somewhat contradictory; that is, a simplerealization may lead to a slower circuit while a complex realization may lead to afaster circuit. Consider, for example, the carry ripple through (CRTA) and a carry
look-ahead (CLAA) adders. The CRTA is simple, but slower, while the CLAA is
complex, but faster.9.5. ARITHMETIC PIPELINE 209It is possible to modify the CRTA in such a way that a number of pairs of
operands are operated upon, that is, pipelined, inside the adder, thus improving
the overall speed of addition in the CRTA. Figure 9.18 shows an example of amodiﬁed 4-bit CRTA. In this case, the two operands A and B are presented to
the CRTA through the use of synchronizing elements, such as clocked latches.
These latches will guarantee that the movement of the partial carry values withinthe CRTA are synchronized at the input of the subsequent stages of the adderwith the higher order operand bits. For example, the arrival of the ﬁrst carry out(c
0) and the second pair of bits ( a1and b1) is synchronized at the input of
the second full adder (counting from low order bits to high order bits) usinga latch.
Although the operation of the modiﬁed CRTA remains in principle the same; that
is, the carry ripples through the adder, the provision of latches allows for the possi-bility of presenting multiple sets of pairs of operands to the adder at the same time.Consider, for example, the case of adding Mpairs of operands, whereby the oper-
ands of each pair are n-bit. The time needed to perform the addition of these M
pairs using a nonpipelined CRTA is given by T
np¼M/C2n/C2Ta, where Tais the
time needed to perform single bit addition. This is to be compared to the timeneeded to perform the same computation using a pipelined CTRA which is givenbyT
pp¼(nþM21)/C2Ta. For example, if M¼16 and n¼64 bits, then we
have Tnp¼1024/C2TaandTpp¼79/C2Ta, thus resulting in a speed-up of about
13. In the extreme case whereby it is possible to present unlimited number ofpairs of operands ( M) to the CRTA at the same time, the speed up will reach 64,
the number of bits in each operand.
Figure 9.18 A modiﬁed 4-bit CRTA210 PIPELINING DESIGN TECHNIQUES9.5.2. Floating-Point Arithmetic Pipelines
Using a similar approach, it is possible to pipeline ﬂoating-point (FP) addition /sub-
traction. In this case, the pipeline will have to be organized around the operations
needed to perform FP addition. The main operations needed in FP addition are expo-nent comparison (EC), exponent alignment (EA), addition (AD), and normalization(NZ). Therefore, a possible pipeline organization is to have a four-stage pipeline,
Figure 9.19 A schematic for a pipeline FP adder
Figure 9.20 Carry-save addition9.5. ARITHMETIC PIPELINE 211each performing an operation from EC, EA, AD, and NZ. Figure 9.19 shows a sche-
matic for a pipeline FP adder. It is possible to have multiple sets of FP operands pro-
ceeding inside the adder at the same time, thus reducing the overall time needed for
FP addition. Synchronizing latches are needed, as before, in order to synchronize the
operands at the input of a given stage in the FP adder.
9.5.3. Pipelined Multiplication Using Carry-Save Addition
As indicated before, one of the main problems with addition is the fact that the carry
has to ripple through from one stage to the next. Carry rippling through stages can beeliminated using a method called carry-save addition. Consider the case of adding44, 28, 32, and 79. A possible way to add these without having the carry ripplethrough is illustrated in Figure 9.20. The idea is to delay the addition of the carry
resulting in the intermediate stages until the last step in the addition. Only at the
last stage is a carry-ripple stage employed.
Figure 9.21 A carry-save based multiplication of two 8-bit operands MandQ
AB
Latches
Partial Product Generator Circuit
Full Adder
Full Adder
Full AdderFull AdderLatches
Latches
Latches
CLAFull Adder
Figure 9.22 Carry-save addition-based multiplication scheme212 PIPELINING DESIGN TECHNIQUESCarry-save addition can be used to realize a pipelined multiplication building
block. Consider, for example, the multiplication of two n-bit operands AandB.
The multiplication operation can be transformed into an addition as shown in
Figure 9.21. The ﬁgure illustrates the case of multiplying two 8-bit operands A
and B. A carry-save based multiplication scheme using the principle shown in
Figure 9.21 is shown in Figure 9.22. The scheme is based on the idea of producingthe set of partial products needed and then adding them up using a carry-save
addition scheme.
9.6. SUMMARY
In this chapter, we have considered the basic principles involved in designing pipe-
line architectures. Our coverage started with a discussion on a number of metrics
that can be used to assess the goodness of a pipeline. We then moved to present ageneral discussion on the main problems that need to be considered in designinga pipelined architecture. In particular we considered two main problems: instructionand data dependency. The effect of these two problems on the performance ofa pipeline has been elaborated. Some possible techniques that can be used toreduce the effect of the instruction and data dependency have been introducedand illustrated. Two examples of recent pipeline architectures, the ARM 11 micro-
architecture, and the UltraSPARC III Processor, have been presented. Our discus-
sion in the chapter ended up with an introduction of some of the ideas that can beused in realizing pipeline arithmetic architectures.
EXERCISES
1. Consider the execution of 500 instructions on a ﬁve-stage pipeline machine.
Compute the speed-up due to the use of pipelining given that the probability
of an instruction being a branch is p¼0.3? What must be the value of pand
the expected number of branch instructions such that a speed-up of at least 4 ispossible? What must be the value of psuch that a speed-up of at least 5 is poss-
ible? Assume that each stage takes one cycle to perform its task.
2. Assume that a RISC machine executes one instruction per clock cycle if no
branches are executed. Delayed branch is used with three delay clockcycles. Consider the execution of 1000 instructions, 30% of which arebranch instructions, on such a machine in two cases. The ﬁrst case is theuse of a novice compiler that is not able to reduce the extra clock cycleswasted due to branch instructions. In the second case, a smart compiler thatis able to utilize 85% of the extra clock cycles is used. Compute the average
number of instructions per cycle in each case. Compute also the percentage of
performance gain due to the use of the smart compiler.EXERCISES 2133. A computer system has a four-stage pipeline consisting of an instruction fetch
unit (F), an instruction decode unit (D), an instruction execution unit (E), and a
write unit (W). Compute the speed-up time P(4), throughput U(4), and the efﬁ-
ciency z(4) of the pipeline in executing a code segment consisting of 20 instruc-
tions, given that branch instructions occur as follows: I3,I9,I10,I15,I20.A s s u m e
that when a branch instruction is fetched, the pipeline stalls until the next
instruction to fetch is known. Determine the time required to execute those
same 20 instructions using two-way interleaved memory if the functions
performed by the F, E, and W units require the use of the memory. What isthe average number of cycles per instruction in both cases? Use the following
space–time chart to compute the number of time units.
4. Consider the integer multiplication of two 16-bit numbers MandQto produce
a product P. Show that this operation can be represented as
P¼P
15
i¼0Pi
where Pi¼M*Qi*2irepresents a 32-bit partial product.
Design a pipeline unit to perform the above operation using the minimum
number of carry-save adders and one carry-look-ahead adder. Show also the
design of a pipeline for performing ﬂoating-point addition /subtraction.
Give numerical examples to support your design.
5. A computer system has a three-stage pipeline consisting of a Fetch unit (F), a
Decode unit (D), and an Execute (E) unit. Determine (using the space–time
chart) the time required to execute 20 sequential instructions using two-way interleaved memory if all three units require the use of the memory
simultaneously.
6. What is the average instruction processing time of a ﬁve-stage instruc-
tion pipeline for 36 instructions if conditional branch instructions occur as
follows: I
5,I7,I10,I25,I27. Use both the space–time chart and the analytical
model.
7. A computer has a ﬁve-stage instruction pipeline of one cycle each. The
ﬁve stages are: Instruction Fetch (IF), Instruction Decode (ID), OperandFetch (OF), Instruction Execution (IE), and Operand Store (OS). Considerthe following code sequence, which is to be run on this computer.
Load21,R1; R1 21;
Load 5, R2; R2 5;
Again: Sub R2, 1, R2 R2 R221;
Add R1,R2,R3; R3 R1þR2;
Bnn Again; branch to Again if result is
NotNegative;
Add R4,R5,R6; R6 R4þR5;
Add R6,R4,R7; R7 R4þR6;214 PIPELINING DESIGN TECHNIQUESa. Analyze the execution of the above piece of code in order to calculate the
number of cycles needed to execute the above code without pipelining,
assuming that each instruction requires exactly 5 cycles to execute.
b. Calculate (using the Gantt’s chart) the number of cycles needed to execute the
above code if the pipeline described above is used. Assume that there is noforwarding hardware and that when branch instructions are fetched, the pipe-line will “stall” until the target address is calculated and the branch decision ismade. Ignore any data dependency.
c. Repeat (b) above if data dependency is considered with the remaining con-
ditions the same.
d. Calculate the percentage of improvement due to the use of pipeline in each of
the above cases (b) and (c).
REFERENCES AND FURTHER READING
An Overview of UltraSPARC III Cu, Version 1.1 September 2003, A White Paper, Sun
Microsystems, 1–18.
D. Brash, The ARM Architecture Version 6 (ARMv6), ARM Ltd., January 2002, White
Paper, 1–15.
A. Clements, The Principles of Computer Hardware , 3rd ed., Oxford University Press,
New York, 2000.
D. Cormie, The ARTM11 Microarchitecture, ARM Ltd., April 2002, A White Paper, 1–9.
K. Ebcioglu, J. Fritts and S. Kosonocky, An eight-issue tree VLIW processor for dynamic
binary translation, IEEE Proc. , ICCD, (1998).
M. Flynn, Computer Architecture: Pipelined and Parallel Processor Design , Jones and
Bartlett Publisher, New York, 1995.
S. Furber, ARM System-on-Chip Architecture , Addison-Wesley, MA, USA, 2000.
G. Goldman and P. Tirumalai, UltraSPARC-III: The advancement of Ultra Computing, Proc.
IEEE COMPCON’97 , p. 417.
C. Hamacher, Z. Vranesic, S. Zaky, Computer Organization , 5th ed., McGraw-Hill,
New York, 2002.
V. Heuring and H. Jordan, Computer Systems Design and Architecture , Prentice-Hall,
New Jersey, 1997.
S. Hily and A. Seznec, Branch prediction and simultaneous multireading, Proc. IEEE
PACT’96 , p. 170.
J. Hoogerbrugge and L. Augusteijn, Instruction scheduling for TriMedia, J. Instruction-Level
Parallelism (1999).
W.-M. Hwu, Introduction to predicted execution, IEEE Comput. , Vol. 31, No. 1, 49–50
(1998).
D. Jaggar, ARM architecture and systems, IEEE Micro , 17(4), 9–11 (1997).
G. Lauthbatch and T. Horel, UltraSPARC-III: Designing third generation 64-bit performance,
IEEE Micro , May–June, 73–85 (1999).REFERENCES AND FURTHER READING 215R. Nair, Optimal 2-bit branch predictors, IEEE Trans. Comput. , 698, 1995.
R. Oehler and R. Groves, IBM RISC system /6000 processor architecture, IBM J. Res. Dev. ,
34; 23–36 (1990).
B. Rau and J. Fisher, Instruction-level parallel processing: history, overview and perspective,
J. Supercomput. , 7; 9–50 (1993).
A. Scott, K. Burkhart, A. Kumar, R. Blumberg and G. Ranson, Four-way superscalar
PA-RISC processors, Hewlett-Packard J ., August (1997).
J. Smith and G. Sohi, The microarchitecture of superscalar processors, Proc. IEEE , Vol. 83,
No. 12, 1609–1624 (1995).
M. Tremblay, Increasing work, pushing the clock, IEEE Comput. , Vol. 31, No. 1, 40–41
(1998).
B. Wilkinson, Computer Architecture: Design and Performance , 2nd ed., Prentice-Hall, Hert-
fordshire, UK, 1996.
Websites
http://www.ar.com
http://www.arm.com /support /White_Papers
http://www.sun.com /ultrasparc216 PIPELINING DESIGN TECHNIQUES& CHAPTER 10
Reduced Instruction Set
Computers (RISCs)
This chapter is dedicated to a study of reduced instruction set computers (RISCs).
These machines represent a noticeable shift in computer architecture paradigm.
This paradigm promotes simplicity rather than complexity. The RISC approach is sub-stantiated by a number of studies indicating that assignment statements, conditional
branching, and procedure calls /return represent more than 90% and that complex
operations such as long division represent only about 2% of the operations performed
in a typical set of benchmark programs. These studies showed also that among alloperations, procedure calls /return are the most time-consuming. Based on such
results, the RISC approach calls for enhancing architectures with the resourcesneeded to make the execution of the most frequent and the most time-consuming oper-ations most efﬁcient. The seed for the RISC approach started as early as the mid-1970s. Its real-life manifestation appeared in the Berkeley RISC-I and the Stanford
MIPS machines, which were introduced in the mid-1980s. Today, RISC-based
machines are reality and they are characterized by a number of common featuressuch as simple and reduced instruction set, ﬁxed instruction format, one instructionper machine cycle, pipeline instruction fetch /execute units, ample number of general
purpose registers (or alternatively optimized compiler code generation), Load /Store
memory operations, and hardwired control unit design. Our coverage in this chapterstarts with a discussion on the evolution of RISC architectures. We then provide abrief discussion on some of the performance studies that led to the adoption of the
RISC paradigm. Overlapped Register Windows, an essential concept in the RISC
development, is then discussed. Toward the end of the chapter we provide detailson a number of RISC-based architectures, such as the Berkeley RISC, the StanfordMIPS, the Compaq Alpha, and the SUN UltraSparc.
10.1. RISC /CISC EVOLUTION CYCLE
The term RISCs stands for Reduced Instruction Set Computers. It was originally
introduced as a notion to mean architectures that can execute as fast as one
217Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.instruction per clock cycle. RISC started as a notion in the mid-1970s and has even-
tually led to the development of the ﬁrst RISC machine, the IBM 801 minicomputer.The launching of the RISC notion announces the start of a new paradigm in thedesign of computer architectures. This paradigm promotes simplicity in computerarchitecture design. In particular, it calls for going back to basics rather than provid-ing extra hardware support for high-level languages. This paradigm shift relates towhat is known as the semantic gap , a measure of the difference between the oper-
ations provided in the high-level languages (HLLs) and those provided in computerarchitectures.
It is recognized that the wider the semantic gap, the larger the number of undesirable
consequences. These include (a) execution inefﬁciency, (b) excessive machine pro-gram size, and (c) increased compiler complexity. Because of these expected conse-quences, the conventional response of computer architects has been to add layers ofcomplexity to newer architectures. These include increasing the number and complex-ity of instructions together with increasing the number of addressing modes. The archi-
tectures resulting from the adoption of this “add more complexity” are now known as
Complex Instruction Set Computers (CISCs). However, it soon became apparent that acomplex instruction set has a number of disadvantages. These include a complexinstruction decoding scheme, an increased size of the control unit, and increasedlogic delays. These drawbacks prompted a team of computer architects to adopt theprinciple of “less is actually more.” A number of studies were then conducted to inves-tigate the impact of complexity on performance. These are discussed below.
10.2. RISCs DESIGN PRINCIPLES
A computer with the minimum number of instructions has the disadvantage that a
large number of instructions will have to be executed in realizing even a simple
function. This will result in a speed disadvantage. On the other hand, a computer
with an inﬂated number of instructions has the disadvantage of complex decoding
and hence a speed disadvantage. It is then natural to believe that a computer witha carefully selected reduced set of instructions should strike a balance betweenthe above two design alternatives. The question then becomes what constitutes acarefully selected reduced set of instructions? In order to arrive at an answer tothis question, it is necessary to conduct in-depth studies on a number of aspects
of computation. These aspects should include (a) operations that are most frequently
performed during execution of typical (benchmark) programs, (b) operations that are
most time consuming, and (c) the type of operands that are most frequently used.
A number of early studies were conducted in order to ﬁnd out the typical break-
down of operations that are performed in executing benchmark programs. The esti-
mated distribution of operations is shown in Table 10.1.
A careful look at the estimated percentage of operations performed reveals that
assignment statements, conditional branches, and procedure calls constitute about
90% of the total operations performed, while other operations, however complex
they may be, make up the remaining 10%.218 REDUCED INSTRUCTION SET COMPUTERS (RISCs)In addition to the above ﬁndings, studies on time–performance characteristics of
operations revealed that among all operations, procedure calls /return are the most
time-consuming. With regards to the type of operands used during typical compu-
tation, it was noticed that the majority of references (no less than 60%) are madeto simple scalar variables and that no less than 80% of scalars are local variables(to procedures).
The above observations about typical program behavior have led to the following
conclusions:
1. Simple movement of data (represented by assignment statements), rather than
complex operations, are substantial and should be optimized.
2. Conditional branches are predominant and therefore careful attention should
be paid to the sequencing of instructions. This is particularly true when it isknown that pipelining is indispensable to use.
3. Procedure calls /return are the most time-consuming operations and therefore
a mechanism should be devised to make the communication of parametersamong the calling and the called procedures cause the least number of instruc-tions to execute.
4. A prime candidate for optimization is the mechanism for storing and accessing
local scalar variables.
The above conclusions have led to the argument that instead of bringing the instruc-tion set architecture closer to HLLs, it should be more appropriate to rather optimize
the performance of the most time-consuming features of typical HLL programs.This is obviously a call for making the architecture simpler rather than complex.
Remember that complex operations such as long division represent only a small por-
tion (less than 2%) of the operations performed during a typical computation. Onethen should ask the question: how can we achieve that? The answer is by (a) keepingthe most frequently accessed operands in CPU registers and (b) minimizing theregister-to-memory operations.
The above two principles can be achieved using the following mechanisms:
1. Use a large number of registers to optimize operand referencing and reduce
the processor memory trafﬁc.TABLE 10.1 Estimated Distribution of Operations
Operations Estimated percentage
Assignment statements 35
Loops 5
Procedure calls 15
Conditional branches 40
Unconditional branches 3
Others 210.2. RISCs DESIGN PRINCIPLES 2192. Optimize the design of instruction pipelines such that minimum compiler code
generation can be achieved (see Chapter 8).
3. Use a simpliﬁed instruction set and leave out those complex and unnecessary
instructions.
The following two approaches were identiﬁed to implement the above three
mechanisms.
1. Software approach. Use the compiler to maximize register usage by allocat-
ing registers to those variables that are used the most in a given time period(this is the philosophy adopted in the Stanford MIPs machine).
2. Hardware approach. Use ample CPU registers so that more variables can be
held in registers for larger periods of time (this is the philosophy adopted inthe Berkeley RISC machine). The hardware approach necessitates the useof a new register organization, called overlapped register window . This is
explained below.
10.3. OVERLAPPED REGISTER WINDOWS
The main idea behind the use of register windows is to minimize memory accesses.
In order to achieve that, a large number of CPU registers are needed. For example,the number of CPU general-purpose registers available in the original SPARCmachine (one of the earliest RISCs) was 120. However, it is desirable to haveonly a subset of these registers visible at any given time and to have them addressedas if they were the only set of registers available. Therefore, CPU registers aredivided into multiple small sets, each assigned to a different procedure. A procedure
call will automatically switch the CPU to use a different ﬁxed-size window of reg-
isters. In order to minimize the actual movement of parameters among the callingand the called procedures, each set of registers is divided into three subsets: par-ameter registers, local registers, and temporary registers. When a procedure call ismade, a new overlapping window will be created such that the temporary registersof the caller are physically the same as the parameter registers of the called pro-cedure. This overlap allows parameters to be passed among procedure withoutactual movement of data (Fig. 10.1).
Parameter
RegistersLocalRegistersTemporaryRegistersLevel j
(Caller)
Call/Return
Level j+1
(called)ParametersRegistersLocalRegistersTemporaryRegisters
Figure 10.1 Register window overlapping220 REDUCED INSTRUCTION SET COMPUTERS (RISCs)In addition, a set of a ﬁxed number of CPU registers are identiﬁed as global reg-
isters and are available to all procedures. For example, references to registers
0 through 7 in the SPARC architecture refer to unique global registers, and refer-
ences to registers 8 through 31 indicate registers in the current window. The currentwindow is pointed to using what is normally called the current window pointer
(CWP). Upon having all windows ﬁlled, the register window wraps around, thusacting like a “circular buffer.” Table 10.2 shows the number of windows and the
window size for a number of architectures.
It should be noted that a study was conducted in 1985 to ﬁnd out the impact of
using register window on the performance of the Berkeley RISC. In this study,
two versions of the machine were studied. The ﬁrst is designed with register win-dows and the second was a hypothetical Berkeley RISC implemented without win-dows. The results of the study indicated a decrease by a factor of 2 to 4 (dependingon speciﬁc benchmark) in the memory trafﬁc due to the use of register windows.
10.4. RISCs VERSUS CISCs
The choice of RISC versus CISC depends totally on the factors that must be con-
sidered by a computer designer. These factors include size, complexity, andspeed. A RISC architecture has to execute more instructions to perform the samefunction performed by a CISC architecture. To compensate for this drawback,RISC architectures must use the chip area saved by not using complex instructiondecoders in providing a large number of CPU registers, additional executionunits, and instruction caches. The use of these resources leads to a reduction inthe trafﬁc between the processor and the memory. On the other hand, a CISC archi-
tecture with a richer and more complex instructions, will require a smaller number of
instructions than its RISC counterpart. However, a CISC architecture requires acomplex decoding scheme and hence is subject to logic delays. It is therefore reason-able to consider that the RISC and CISC paradigms differ primarily in the strategyused to trade off different design factors.
There is very little reason to believe that an idea that improves performance for a
RISC architecture will fail to do the same thing in a CISC architecture and viceversa. For example, one key issue in RISC development is the use of optimizing
the compiler to reduce the complexity of the hardware and to optimize the use of
CPU registers. These same ideas should be applicable to CISC compilers. IncreasingTABLE 10.2 Different Register Windows Characteristics
ArchitectureNumber of
windowsNumber of registers
per window
Berkeley RISC-I 8 16
Pyramids 16 32
SPARC 32 3210.4. RISCs VERSUS CISCs 221the number of CPU registers could very much improve the performance of a CISC
machine. This could be the reason behind not ﬁnding a pure commercially availableRISC (or CISC) machine. It is not unusual to see a RISC machine with complexﬂoating-point instructions (see the details of the SPARC architecture in the next sec-tion). It is equally expected to see CISC machines making use of the register win-
dows RISC idea. In fact there have been studies indicating that a CISC machine
such as the Motorola 680xx with a register window will achieve a 2 to 4 timesdecrease in the memory trafﬁc. This is the same factor that can be achieved by aRISC architecture, such as the Berkeley RISC, due to the use of a register window.
It should, however, be noted that most processor developers (except for Intel and
its associates) have opted for RISC processors. Computer system manufacturers
such as Sun Microsystems are using RISC processors in their products. However,
for compatibility with the PC-based market, such companies are still producing
CISC-based products.
Tables 10.3 and 10.4 show a limited comparison between an example RISC and
CISC machine in terms of performance and characteristics, respectively.
An elaborate comparison among a number of commercially available RISC and
CISC machines is shown in Table 10.5.
It is worth mentioning at this point that the following set of common character-
istics among RISC machines is observed:
1. Fixed-length instructions
2. Limited number of instructions (128 or less)3. Limited set of simple addressing modes (minimum of two: indexed and
PC-relative)
4. All operations are performed on registers; no memory operations5. Only two memory operations: Load and StoreTABLE 10.3 RISC Versus CISC Performance
ApplicationMIPS CPI
(RISC)VAX CPI
(CISC)CPI
ratioInstruction
ratio
Spice 2G6 1.80 8.02 4.44 2.48
Matrix300 3.06 13.81 4.51 2.37
Nasa 7 3.01 14.95 4.97 2.10
Espresso 1.06 5.40 5.09 1.70
TABLE 10.4 RISC Versus CISC Characteristics
CharacteristicVAX-11
(CISC)Berkeley RISC-1
(RISC)
Number of instructions 303 31
Instruction size (bits) 16-456 32Addressing modes 22 3
No. general purpose registers 16 138222
REDUCED INSTRUCTION SET COMPUTERS (RISCs)6. Pipelined instruction execution
7. Large number of general-purpose registers or the use of advanced compiler
technology to optimize register usage
8. One instruction per clock cycle9. Hardwired control unit design rather than microprogramming
10.5. PIONEER (UNIVERSITY) RISC MACHINES
In this section, we present brief descriptions of the main architectural features of two
pioneer university-introduced RISC machines. The ﬁrst machine is the BerkeleyRISC and the second is the Stanford MIPS machine. These machines are presentedas a means to show how original RISC machines look and also to make the readerappreciate the advances made in RISC machines development since their inception.
10.5.1. The Berkeley RISC
There are two Berkeley RISC machines: RISC-I and RISC-II. Unless otherwise
mentioned, we refer to RISC-I in our discussion. RISC is a 32-bit LOAD /STORE
architecture. There are 138 32-bit registers R
0–R137available to the users. The
ﬁrst ten registers R0–R9are global registers (seen by all procedures). Register R0
is used to synthesize addressing modes and operations that are not directly availableon the machine. Registers R
10–R137are divided into an overlapping register window
scheme with 32 registers visible at any instant. A 5-bit variable, called current
window pointer (CWP) is used to point to the current register set.
All RISC instructions occupy a full word (32 bits). The RISC instruction set is
divided into four categories. These are ALU (a total of 12 instructions), Load /Store
(a total of 16 instructions), Branch & Call (a total of seven instructions), and specialinstructions (a total of four instructions). Some examples of the RISC instructions are:
1. ALU: ADD R
s,S,Rd;Rd RsþS
2. Load /Store: LDXW (Rx)S,Rd;Rd M[RxþS]TABLE 10.5 Summary of Features of a Number of RISC and a CISC
Motorola
88110Alpha AXP
21264 PentiumPower
PC 601
Company Motorola Compaq (DEC) Intel IBM
Architecture RISC RISC CISC RISC
# Registers(I) 32 80 64 32
Cache I /D8 /8K B 6 4 /64 KB 8 /8K B 3 2
# Registers (GP /FP) 32 /32 31 /31 8 /83 2 /32
# Inst /cycle 2 1 2 3
# Pipelines (I /FP) NS 4 /25 /84 /6
Multiprocessing Support No Yes Yes Yes10.5. PIONEER (UNIVERSITY) RISC MACHINES 2233. Branch & Call: JMPX COND ,( R x)S;PC RxþS;w h e r e COND is a condition
4. Special Instructions: GETPSW R d;Rd PSW
All arithmetic and logical instructions have three operands and have the form Desti-
nation :¼source1 op source2 (Fig. 10.2). The LOAD and STORE instructions may use
either of the indicated formats with DST being the register to be loaded or stored. The
low order 19 bits of the instructions are used to determine the effective address.
Instructions load and store 8-, 16-, 32-, and 64-bit quantities into 32-bit registers.
Two methods are provided for calling procedures. The CALL instruction uses a
30-bit PC relative offset (Fig. 10.3).
The JMP instruction uses any of the instruction formats used for arithmetic and
logical operations and allows the return address to be put in any register.
RISC uses a three-address instruction format with the availability of some two-
and one-address instructions. There are only two addressing modes. These are
indexed mode and PC relative modes. The indexed mode can be used to synthesize
three other modes. These are base-absolute (direct), register indirect, and indexedfor linear byte array modes. RISC uses a static two-stage pipeline: fetch and execute.
The ﬂoating-point unit (FPU) contains thirty-two 32-bit registers to hold 32 single
precision (32-bit) ﬂoating-point operands, 16 double-precision (64-bit) operands, or
eight extended-precision (128-bit) operands. The FPU can execute about 20 ﬂoat-ing-point instructions most of them in single-, double-, or extended-precision usingthe ﬁrst instruction format used for arithmetic. In addition to instructions for loadingand storing FPUs registers, the CPU can also test FPUs registers and branch con-
ditionally on results. RISC employs a conventional MMU supporting a single
paged 32-bit address space. The RISC four-bus organization is shown in Figure 10.4.
10.5.2. Stanford MIPS (Microprocessor Without Interlock
Pipe Stages)
MIPS is a 32-bit pipelined LOAD /STORE machine. It uses a ﬁve-stage pipeline
consisting of Instruction Fetch ( IF), Instruction Decode ( ID), Operand DecodeType
Type Immediate Constant25 6 5 5 18
DST Op-Code SRC 1 0
DST Op-Code SRC 1 1FP-OP SRC 2
Figure 10.2 Three operand instructions formats used in RISC
Type PC-Relative Displacement23 0
Figure 10.3 Procedure call instruction in RISC224 REDUCED INSTRUCTION SET COMPUTERS (RISCs)(OD), Operand Store /Execution ( OS/EX), and Operand Fetch ( OF). The ﬁrst three
stages perform respectively instruction fetch, instruction decode, and operand fetch.
The OS /EX stage sends operand to memory in the case of a store instruction or use
the ALU in case of instruction execution. The OF stage receives the operand in caseof a load instruction. MIPS uses a mechanism called pipeline interlock in order to
prevent an instruction from continuing until the needed operand is available.
Unlike the Berkeley RISC, MIPS has a single set of sixteen 32-bit general-
purpose registers. The MIPS compiler optimizes the use of registers in whateverway is best for the program currently being compiled. In addition to the 16 general-purpose registers, MIPS provided four additional registers in order to hold the fourprevious PC values (to support backtracking and restart in case of a fault). A ﬁfth reg-ister is used to hold the future PC value (to support branch instructions).
Four addressing modes are used in MIPS. These are immediate ,indexed ,based
with offset , and base shifted . Four instruction groups were identiﬁed in MIPS.
These are ALU,Load /Store ,Control , and Special instructions. A total of 13 ALU
instructions were provided. These include all register-to-register two- or three-operand formats (Fig. 10.5). A total of 10 LOAD /STORE instructions were pro-
vided. They use 16 or 32 bits. In the latter case, indexed addressing is used byadding a 16-bit signed constant to a register using the second format inFigure 10.5. A total of six control ﬂow instructions were provided. These include78× 32
Register
File
Shifter
PSW
IMMRegister
Decoder
BI AI
Next PCPCLST PC
Figure 10.4 RISC four-bus organization
Op-Code SRC DST Immediate ConstantOp-Code SRC 1 SRC 2 DST SHIFT Function6
655
5555 6
16
Figure 10.5 Three-operand instructions used in MIPS10.5. PIONEER (UNIVERSITY) RISC MACHINES 225jumps ,relative jumps , and compare instructions. Only two special ﬂow instructions
were provided. They support procedure and interrupt linkage. Some examples of
MIPS instructions are:
1. ALU: Add src 1,src2,dst;dst src1þsrc2
2. Load /Store: Ld[src1þsrc2],dst;dst M[src1þsrc2]
3. Control: Jmp dst ;PC dst
4. Special Function: SavePC A ;M[A] PC
MIPS does not provide direct support for ﬂoating-point operations. Floating-
point operations are to be done by a specialized coprocessor. Surprisingly,non-RISC instructions such as MULT and DIV were included and they use specialfunctional units. The contents of two registers can be multiplied or divided and the64-bit product is kept in two special registers LO and HI.
Procedure call can be made through the JUMP instruction shown in Figure 10.6.
The instruction uses a 26-bit jump target address.
The MIPS virtual address is 32 bits long, thus allowing for up to four Gwords
virtual address space. A virtual address is divided into a 20-bit virtual pagenumber and a 12-bit offset within the page. The actual implementation of MIPSwas restricted by packaging constraints allowing only 24 address pins; that is, theactual physical address space is 2
24¼16Mwords (32 bits each). A support for
off-chip TLB for address translation is provided. The MIPS organization is shown
in Figure 10.7.Op-Code Jump Target62 6
Figure 10.6 Jump instruction format used in MIPSMDR
MAR
PC-3
PC-2
PC-1
PC
Register File (16 × 32)
Barrel Shifter
Hi and Lo Registers
Figure 10.7 MIPS organization226 REDUCED INSTRUCTION SET COMPUTERS (RISCs)10.6. EXAMPLE OF ADVANCED RISC MACHINES
In this section, we introduce two representative advanced RISC machines. Our
emphasis in this coverage is on the pipeline features and the branch handling mech-anisms used.
10.6.1. Compaq (Formerly DEC) Alpha 21264
Alpha 21264 (EV6) is a third generation Compaq (formerly DEC) RISC superscalar
processor. It is a full 64-bit processor. The 21264 has an 80-entry integer register ﬁleand a 72-entry ﬂoating-point register ﬁle. It employs a two-level cache. The L1 dataand instruction caches are 64 KB each. They are organized in a two-way set-associ-
ative manner. The L2 data cache can be 1 to 16 MB (shared by instructions and data)organized using direct-mapping. The block size is 64 bytes. The data cache can
receive any combination of two loads or stores from the integer execution pipe
every cycle. This is equivalent to having the 64 KB on-chip data cache delivering16 bytes every cycle, hence twice the clock speed of the processor. The 21264memory system can support up to 32 in-ﬂight loads, 32 in-ﬂight stores, and 8in-ﬂight (64 byte) cache block ﬁlls and 8 cache misses. It has a 64 KB, two-way
set-associative cache (both instruction and data). It can also support up to two
out-of-order operations (Fig. 10.8).
10.6.2. The Alpha 21264 Pipeline
The Alpha 21264 instruction pipeline is shown in Figure 10.9. It consists of SEVEN
stages. These are the Fetch, Slot Assignment, Rename, Issue, Register Read,Execute, and Memory stages.
The fetch stage can fetch and execute up to four instructions per cycle. A block
diagram of the fetch stage is shown in Figure 10.10. This stage uses a unique “block
and set” prediction technique. According to this technique, both the locations of the
next four instructions and the set (there are two sets) in which they are located, are
predicted. The “block and set” prediction technique combines the speed advantages
of a direct-mapped cache with the lower miss ratio of a two-way set-associative
Cache L1
(Data 64 KB, 2-way) +
(Instruction 64 KB, 2-way)
(Block size is 64 Byte)
Cache  L2 (off-chip)
(Data 1-16 MB, direct)
Main Memory (up to 16 GB)
Hard Disk (100s of Terabytes)
Figure 10.8 The 21264 memory hierarchy10.6. EXAMPLE OF ADVANCED RISC MACHINES 227cache. This technique achieves more than an 85% hit ratio. The misprediction pen-
alty is a single cycle. The 21264 uses speculative branch prediction. Branch predic-tion in the 21264 is a two-level scheme. It is based on the observation that branchesexhibit both local and global correlation. Local correlation makes use of thebranch’s past behavior. Global correlation, on the other hand, makes use of the
past behavior of all previous branches. The combined local /global prediction
used in the 21264 correlates the branch behavior pattern with local branch history,
that is, the execution of a single branch at a unique PC location, and global branchhistory, that is, the execution of all previous branches. The scheme dynamicallyselects between local and global branch history (Fig. 10.11).
The local branch predictor has two tables. The ﬁrst is a 1024 /C210 local history
table in which each entry holds a 10-bit local history of the selected branch overthe last executions. The local history table is indexed by the instruction address
(using the PC). The second table is a 1024 /C23 local prediction table in which each
entry has a 3-bit saturating counter to predict the branch outcome. After branches’
retirement, the 21264 updates the local history table with the true branch directionand the referenced counter. This enhances the possibility for correct prediction and
is called predictor training .
The global branch predictor has a 4096 /C22 global prediction table in which each
entry holds a 2-bit saturating counter. It keeps track of the global history of the last
12 branches. The global branch prediction table is indexed by a 4096 /C22 choice pre-
diction table. After branches’ retirement, the 21264 updates the referenced globalprediction counter, enhancing the possibility for correct prediction.
Local prediction is useful in the case of an alternating taken /not-taken sequence
of a given branch. In this case, the local history of the branch will eventually resolveto a pattern of ten alternating zeros and ones indicating the success, or failure, of thebranch on alternate encounters. As the branch executes multiple times, it saturatesthe prediction counters corresponding to the local history values and hence makesthe prediction correct.Fetch
StageS #1SlotAssignmentStage S #2RenameStageS #3 IssueStageS #4 RegisterReadStage S #5ExecuteStageS #6 MemoryStageS #7
Figure 10.9 The 21264 instruction pipeline
64 KB 2-wayBlock/Set
PredictionGlobalLocal
Branch
Predictor Instruction
Cache
Figure 10.10 The 21264 fetch stage228 REDUCED INSTRUCTION SET COMPUTERS (RISCs)Global prediction is useful when the outcome of a branch can be inferred from the
direction of previous branches. Consider, for example, the case of repeated invoca-
tions of two branches. If the ﬁrst branch that checks for a value equal to 1001 suc-
ceeds, the second branch that checks for the same value to be odd must also succeed.
The global history predictor can learn this pattern with repeated invocations of thesetwo branches.
The 2096/C22 choice predictor is a table in which each entry holds a 2-bit saturat-
ing counter and is used to implement the selection (tournament) scheme. If the pre-dictions of the local and global predictors differ, the 21264 updates the selected
choice prediction entry to support the correct predictor. The 21264 updates the
choice prediction table when a branch retires.
The slot assignment stage (S #2) simply assigns instructions to slots associated
with the integer and the ﬂoating-point queues.
The out-of-order (OOO) issue logic in the 21264 receives four fetched instruc-
tions every cycle, renames and remaps the registers (to avoid unnecessary registerdependencies), and queues the instructions until operands and /or functional units
become available. It dynamically issues up to six instructions every cycle, four inte-
ger and two ﬂoating-point instructions. Register renaming means mapping instruc-
tion virtual registers to internal physical registers. There are 31 integer and 31
ﬂoating-point registers that are visible to users. These registers are renamed
during execution to internal registers. It is only when instructions are ﬁnished(retired) that the internal registers are renamed back to visible registers. Registerrenaming eliminates write-after-write and write-after-read data dependencies. How-ever, it preserves all the read-after-write dependencies that are necessary for correctcomputation.
A list of the pending instructions is maintained by the OOO queue logic. In each
cycle, both the integer and the ﬂoating-point queues select those instructions that are
ready to execute. This selection is made based on a scoreboard of the renamed reg-
isters. The scoreboard maintains the status of renamed registers by tracking theLocal
Prediction
Table
1024 × 3Local
History
Table
1024 × 10
Branch
PredictionPast
HistoryChoice
Prediction
4096 × 2Global
Prediction
4096 × 2
PC MUX
Figure 10.11 The 21264 selection branch predictor10.6. EXAMPLE OF ADVANCED RISC MACHINES 229progress of single-cycle, multiple-cycle, and variable-cycle instructions. Upon the
availability of the functional unit(s) or load data results, the scoreboard unit notiﬁesall instructions in the queue of the availability of the required register value. Eachqueue selects the oldest data-ready and functional-unit-ready instructions forexecution of each cycle. The 21264 integer queue statically assigns instructions totwo of four pipes, either the upper or the lower pipe (Fig. 10.12).
The Alpha 21264 has four integer and two ﬂoating-point pipelines. This allows
the processor to dynamically issue up to six instructions in the same cycle. Theissue (or queue) stage maintains an inventory from which it can dynamicallyselect to issue a maximum of six instructions. There is a 20-entry integer issuequeue and a 15-entry ﬂoating-point issue queue. Instruction issue reordering takesplace in the issue stage.
The 21264 uses two integer ﬁles, 80-entry each, to store a duplicate of register con-
tents. Two pipes access a single ﬁle to form a cluster. The two clusters form a four-way integer instruction execution. Results are broadcasted from each cluster to the
other cluster. Instructions are dynamically selected by the integer issue queue to exe-
cute on a given instruction pipe. An instruction can heuristically be selected to executeon the same cluster that produces the result. The 21264 has one 72-entry ﬂoating-pointregister ﬁle. The ﬂoating-point register ﬁle, together with two instruction executionpipes, form a cluster. Figure 10.12 shows the register read /execution pipes.
On a ﬁnal note, we should indicate that the 21264 uses a write-invalidate cache
coherence mechanism in the level 2 cache to provide support for shared-memory
multiprocessing. It also supports the following cache states: modiﬁed, owned,
shared, exclusive, and invalid.
Floating-point
Multiply
Execution
Floating-point
Register File (72)
ClusterClusterCluster
Floating-point
Add, Div, SQRT
ExecutionInteger ExecutionInteger ExecutionInteger ExecutionInteger Execution
Data
Cache64 KB2-wayInteger Register
File (80)Integer Register
File (80)
Figure 10.12 The 21264 execution pipes230 REDUCED INSTRUCTION SET COMPUTERS (RISCs)10.6.3. SUN UltraSPARC III
The UltraSPARC wIII is a high-performance superscalar RISC processor that
implements the 64-bit SPARC w-V9 RISC architecture. There exist a number of
implementations of the SPARC III processor. These include the UltraSPARC IIIi
and the UltraSPARC III Cu. Our coverage in this section will be independent ofany particular implementation. We will however refer to speciﬁc implementationswhenever appropriate.
The UltraSPARC III is a third generation 64-bit SPARC wRISC microprocessor.
It supports a 64-bit virtual address space and a 43-bit physical address space. TheUltraSPARC III employs a multilevel cache architecture. For example, the Ultra-SPARC IIIi (and the UltraSPARC III Cu) architecture has a 32 KB, four-way set-
associative L1 instruction cache, a 64 KB four-way set-associative L1 data cache,
a 2 KB prefetch cache, and a 2 KB write cache. The UltraSPARC IIIi supports a1 MB four-way set-associative, uniﬁed instruction /data on chip L2 cache. A
cache block size of 64 bytes is used in the UltraSPARC IIIi. While the UltraSPARCIII Cu architecture supports a 1, 4, or 8 MB two-way set-associative, uniﬁed instruc-tion/data external cache. Cache block size in the UltraSPARC III Cu varies between
64 bytes (for the 1 MB cache) to 512 bytes (for the 8 MB cache) (Fig. 10.13).
The UltraSPARC III uses two instruction TLBs that can be accessed in parallel
and three data TLBs that can be accessed in parallel. The two instruction TLBsare organized in a 16-entry fully associative manner to hold entries for 8 KB,64 KB, 512 KB, and 4 MB page sizes. A 128-entry two-way set-associative TLBis used exclusively for 8 KB page sizes. The three data TLBs are organized in a16-entry associative manner for 8 KB, 64 KB, 512 KB, and 4 MB page sizes andtwo 512-entry two-way set-associative TLBs that can be programmed to hold any
one page size at a given time. The UltraSPARC III uses a write-allocate, write-
back cache write policy.
The UltraSPARC III pipeline has been covered in Chapter 9 (pages 203–207).On a ﬁnal note, it should be mentioned that the UltraSPARC III has been
designed to support a one-to-four way multiprocessing. For this purpose, it usestheJBus , which supports a small-scale multiprocessor system. The JBus is capable
Main Memory (up to 16 GB)
Hard Disk (100s of Terabytes)Cache L1
(Data 64 KB) +
(Instruction 32 KB) +
(2 KB Prefetch) + (2 Kbyte Write)
Cache  L2 (1-8 MB)
Figure 10.13 UltraSPARC III memory hierarchy10.6. EXAMPLE OF ADVANCED RISC MACHINES 231of delivering the high bandwidth needed for networking and embedded systems
applications. Through the JBus , processors can attach to a coherent shared bus
with no needed glue logic (Fig. 10.14).
10.7. SUMMARY
A RISC architecture saves the extra chip area used by CISC architectures for decod-
ing and executing complex instructions. The saved chip area is then used to provide
an on-chip instruction cache that can be used to reduce instruction trafﬁc betweenthe processor and the memory. Common characteristics shared by most RISCdesigns are: limited and simple instruction set, large number of general purpose reg-isters and /or the use of compiler technology to optimize register usage, and optim-
ization of the instruction pipeline. An essential RISC philosophy is to keep the mostfrequently accessed operands in registers and minimize register-memory operations.This can be achieved using one of two approaches: Software Approach, use the com-
piler to maximize register usage by allocating registers to those variables that will be
used the most in a given time period (this is the philosophy used in Stanford MIPsmachine); or Hardware Approach, use more registers so that more variables can beheld in registers for larger periods of time (this is the philosophy used in the Berke-ley RISC machine). Register windows are multiple small sets of registers, eachassigned to a different procedure. A procedure call automatically switches theCPU to use a different ﬁxed-size window of registers rather than saving registersin memory at the call time. At any time, only ONE window of registers is visible
and is addressed as if it were the only set of registers. Window overlapping requires
that temporary registers at one level are physically the same as the parameter regis-ters at the next level. This overlap allows parameters to be passed without the actualmovement of data.
It is worthwhile mentioning that the classiﬁcation of processors as entirely pure
RISC or entirely pure CISC is becoming more and more inappropriate and may beirrelevant. What actually counts is how much performance gain can be achieved byincluding an element of a given design style. Most modern processors use a calcu-
lated combination of elements of both design styles. The decisive factor in which
element(s) of each design style to include is made based on a trade-off betweenUltraSPARC III
ProcessorUltraSPARC IIIProcessor UltraSPARC IIIProcessorUltraSPARC IIIProcessor
JBUS 128 bit, 200 MHz
Figure 10.14 A four-way UltraSPARC III multiprocessor conﬁguration232 REDUCED INSTRUCTION SET COMPUTERS (RISCs)the required improvement in performance and the expected added cost. A number of
processors are classiﬁed as RISC while employing a number of CISC features, suchas integer /ﬂoating-point division instructions. Similarly, there exist processors that
are classiﬁed as CISC while employing a number of RISC features, such aspipelining.
EXERCISES
1. What are the main principles used to construct a RISC machine?
2. Contrast the two approaches (the software and the hardware) used in RISC
machines to minimize memory operations.
3. Explain, with examples, the concept of register window and window overlap-
ping. Suggest a different approach to achieve the same results as those
achieved using register window and window overlapping.
4. For the purpose of this problem, you are required to pick a recent RISC pro-
cessor of your choice. Submit a small report (no less than 5 and no more than
10 pages in length) that summarizes the main pipelining features used and themain RISC features used. The level of your coverage should be suitable for asenior undergraduate student. Make sure that you be precise and neat in yourcoverage. Use simple examples whenever possible. Provide accurate andmeaningful ﬁgures and tables whenever possible. You are required to cover
all aspects of a pipeline processor and all aspects of a RISC machine.
5. The controversy of RISC versus CISC never ends. Suppose that you represent
an advocate for the RISC approach; write at least a one-page critic of the CISCapproach showing its disadvantages while showing the advantages of the RISCapproach. You may want to use real-life example machine performance as asupport for your support of the RISC philosophy.
6. Repeat question 5 assuming that you are an advocate for the CISC philosophy.
REFERENCES AND FURTHER READING
An Overview of UltraSPARC III Cu, Version 1.1 September 2003, A White Paper, Sun
Microsystems, 1–18.
R. Colewell et al. Computers, complexity, and controversy, IEEE Comput. , 18(9), 8–19
(1985).
Z. Cvetanovic and R. Kessler, Performance analysis of the Alpha 21264-based Compaq E40
system, Proc. 27th Annual International Symposium on Computer Architecture , Vancou-
ver, British Columbia, Canada, 192–202.
Exploring Alpha Power for Technical Computing. A Compaq Report on Compag High
Performance Technical Computing, November 1999, pp. 1–28.
G. Goldman and P. Tirumalai, UltraSPARC-III: The advancement of ultra computing, Proc.
IEEE COMPCON’97 , p. 417.REFERENCES AND FURTHER READING 233J. Hennessy, VLSI processor architecture, IEEE Trans. Comput. , C-33(11), 1221–1246
(1984).
J. Hennessy and D. Patterson, Computer Architecture: A Quantitative Approach , Morgan
Kaufmann: San Mateo, San Francisco, CA, 1996.
R. Kessler, The Alpha 21264 Microprocessor, IEEE Micro , Vol. 19, issue 2, 24–36 (1999).
R. Kessler, E. McLellan and D. Webb, The Alpha 21264 Microprocessor Architecture,
International Conference on Computer Design, Oct. 88, pp. 96–102.
G. Lauthbatch and T. Horel, UltraSPARC-III: Designing third generation 64-bit performance,
IEEE Micro. , 73–85 (1999).
R. Nair, Optimal 2-bit branch predictors, IEEE Trans. Comput. , 698 (1995).
D. Patterson, Reduced instruction set computers, Commun. ACM , 28(1), 8–21 (1985).
D. Patterson and R. Ditzel, The case for the reduced instruction set computer, Comput. Archi-
tecture News , 8(6), 25–33 (1980).
D. Patterson and C. Sequin, A VLSI RISC, IEEE Comput. , 15(9), 8–21 (1982).
G. Radin, The 801 minicomputer, IBM J. Res. Develop. , 27(3), 237–246 (1983).
R. Sherburne, M. Katevenis, D. Patterson and C. Sequin, A 32-bit NMOS processor with a
large register ﬁle, IEEE J. Solid-State Circuits , Sc-19(5), 682–689 (1984).
A. Tanenbaum, Structured Computer Organization , 3rd ed., Prentice-Hall: Englewood Cliffs,
New Jersey.
Websites
http://www.sun.com /processors /UltraSPARC-IIIi
http://www.sun.com /processors /whitepapers234 REDUCED INSTRUCTION SET COMPUTERS (RISCs)& CHAPTER 11
Introduction to Multiprocessors
Having covered the essential issues in the design and analysis of uniprocessors and
pointing out the main limitations of a single-stream machine, we begin in this chap-ter to pursue the issue of multiple processors. Here a number of processors (two ormore) are connected in a manner that allows them to share the simultaneousexecution of a single task. The main argument for using multiprocessors is tocreate powerful computers by simply connecting many existing smaller ones. Amultiprocessor is expected to reach a faster speed than the fastest uniprocessor. Inaddition, a multiprocessor consisting of a number of single uniprocessors is expected
to be more cost-effective than building a high-performance single processor. An
additional advantage of a multiprocessor consisting of nprocessors is that if a
single processor fails, the remaining fault-free n21 processors should be able to
provide continued service, albeit with degraded performance. Our coverage inthis chapter starts with a section on the general concepts and terminology used.We then point to the different topologies used for interconnecting multiple pro-cessors. Different classiﬁcation schemes for computer architectures are thenintroduced and analyzed. We then introduce a topology-based taxonomy for inter-
connection networks. Two memory-organization schemes for MIMD (multiple
instruction multiple data) multiprocessors are also introduced. Our coverage inthis chapter ends with a touch on the analysis and performance metrics for multipro-cessors. It should be noted that interested readers are referred to more elaborate dis-cussions on multiprocessors in Chapters 2 and 3 of our book on Advanced ComputerArchitecture and Parallel Processing (see reference list).
11.1. INTRODUCTION
A multiple processor system consists of two or more processors that are connected in
a manner that allows them to share the simultaneous (parallel) execution of a givencomputational task. Parallel processing has been advocated as a promising approachfor building high-performance computer systems. Two basic requirements areinevitable for the efﬁcient use of the employed processors. These requirements
235Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.are (1) low communication overhead among processors while executing a given task
and (2) a degree of inherent parallelism in the task.
A number of communication styles exist for multiple processor networks. These
can be broadly classiﬁed according to (1) the communication model (CM) or (2) thephysical connection (PC). According to the CM, networks can be further classiﬁedas (1) multiple processors (single address space or shared memory computation) or(2) multiple computers (multiple address space or message passing computation).
According to PC, networks can be further classiﬁed as (1) bus-based or (2) network-
based multiple processors. Typical sizes of such systems are summarized in Table 11.1.
The organization and performance of a multiple processor system are greatly inﬂu-
enced by the interconnection network used to connect them. On the one hand, a singleshared bus can be used as the interconnection network for multiple processors. On theother hand, a crossbar switch can be used as the interconnection network. While theﬁrst technique represents a simple easy-to-expand topology, it is, however, limitedin performance since it does not allow more than one processor /memory transfer at
any given time. The crossbar provides full processor /memory distinct connections
but it is expensive. Multistage interconnection networks (MINs) strike a balance
between the limitation of the single, shared bus system and the expense of acrossbar-based system. In a MIN more than one processor /memory connection can
be established at the same time. The cost of a MIN can be considerably less thanthat of a crossbar, particularly for a large number of processors and /or memories.
The use of multiple buses to connect multiple processors to multiple memory modules
has also been suggested as a compromise between the limited single bus and the
expensive crossbar. Figure 11.1 illustrates the four types of interconnection networks
mentioned above. Interested readers are referred to our book on Advanced ComputerArchitecture and Parallel Processing (see reference list).
11.2. CLASSIFICATION OF COMPUTER ARCHITECTURES
A classiﬁcation means to order a number of objects into categories, each having
common features, among which certain relationship(s) exist(s). In this regard, aclassiﬁcation scheme for computer architectures aims at categorizing them such
that those architectures that have common features fall into one category and
such that different categories represent distinct groups of architectures. In addition,
TABLE 11.1 Typical Sizes of Some Multiprocessor Systems
Category SubcategoriesNumber of
processors
Communication model Multiple processors 2–256
Multiple computers 8–256
Physical connection Bus-based 2–32
Network-based 8–256236 INTRODUCTION TO MULTIPROCESSORSa classiﬁcation scheme for computer architecture should provide a basis for infor-
mation ordering and a basis for predicting the features of a given architecture.
Two broad schemes exist for computer architecture classiﬁcation. The ﬁrst is
based on external (morphological) features of architectures and the second is basedon the evolutionary features of architectures. The ﬁrst scheme emphasizes theﬁnished form of architectures, while the second scheme emphasizes the way anarchitecture has been derived from its predecessor and suggests speculative views
on its successor. Morphological classiﬁcation provides a basis for predictive
power, while evolutionary classiﬁcation provides a basis for better understandingof architectures. Examining the extent to which a classiﬁcation scheme is satisfyingits stated objective(s) could assess the pros and cons of that scheme.
A number of classiﬁcation schemes have been proposed over the last three dec-
ades. These include the Flynn’s classiﬁcation (1966), the Kuck (1978), the Hwangand Briggs (1984), the Erlangen (1981), the Giloi (1983), the Skillicorn (1988), andthe Bell (1992). A number of these are brieﬂy discussed below.
11.2.1. Flynn’s Classiﬁcation
Flynn’s classiﬁcation scheme is based on identifying two orthogonal streams in a
computer. These are the instruction and the data streams. The instruction streamis deﬁned as the sequence of instructions performed by the computer. The datastream is deﬁned as the data trafﬁc exchanged between the memory and the
processing unit. According to Flynn’s classiﬁcation, either of the instruction or
data streams can be single or multiple. This leads to four distinct categories ofP P PM M M
PPM
p
p
p
ppppp
MMMMMMMM
(c)(a)( b)
(d)PMM
Figure 11.1 Four types of multiprocessor interconnection networks. ( a) Single-bus system,
(b) multi-bus system, ( c) multi-stage interconnection network, ( d) crossbar system11.2. CLASSIFICATION OF COMPUTER ARCHITECTURES 237computer architectures:
1. Single-instruction single-data streams (SISD)
2. Single-instruction multiple-data streams (SIMD)
3. Multiple-instruction single-data streams (MISD)4. Multiple-instruction multiple-data streams (MIMD)
Figure 11.2 shows the orthogonal organization of the streams according to Flynn’s
classiﬁcation.
Schematics for the four categories of architectures resulting from Flynn’s classi-
ﬁcation are shown in Figure 11.3. Table 11.2 lists some of the commercial machinesbelonging to each of the four categories.
Observations on Flynn’s Classiﬁcation
1. Flynn’s classiﬁcation is among the ﬁrst of its kind to be introduced and as
such it must have inspired subsequent classiﬁcations.
2. The classiﬁcation helped in categorizing architectures that were available
and those that have been introduced later. For example, the introduction of
the SIMD and MIMD machine models in the classiﬁcation must haveinspired architects to introduce these new machine models.
3. The classiﬁcation stresses the architectural relationship at the memory-
processor level. Other architectural levels are totally overlooked.
4. The classiﬁcation stresses the external (morphological) features of architec-
tures. No information is included on the revolutionary relationship of archi-tectures that belong to the same category.
5. Owing to its pure abstractness, no practically viable machine has exempliﬁed
the MISD model introduced by the classiﬁcation (at least so far). It should,however, be noted that some architects have considered pipelined machines(and perhaps systolic-array computers) as examples for MISD.
6. A very important aspect that is lacking in Flynn’s classiﬁcation is the issue of
machine performance. Although the classiﬁcation gives the impression thatmachines in the SIMD and the MIMD are superior to their SISD and MISDcounterparts, it gives no information on the relative performance of SIMD
and MIMD machines.
Data Stream
Single Multiple
Instruction Single SISD SIMD
Stream Multiple MISD MIMD
Figure 11.2 Flynn’s classiﬁcation238 INTRODUCTION TO MULTIPROCESSORSInstruction Memory Control Unit Processing Unit Data Memory
(a) SISD
(b) SIMD
(d) MIMD(c) MISDData Stream
Data Stream
Data Stream
Data Stream
Data Stream
Data StreamInstruction StreamInstruction StreamInstruction StreamInstruction Stream
Instruction Stream
Instruction StreamControl Unit Processing Unit Data Memory
Processing Unit Data MemoryProcessing Unit Data Memory
Instruction Memory
Control Unit Instruction Memory
Control Unit Instruction MemoryProcessing Unit Data Memory Control Unit Instruction Memory
Processing Unit Data Memory Control Unit Instruction Memory
Processing Unit Data Memory Control Unit Instruction Memory
Processing Unit Data Memory Control Unit Instruction MemoryInstruction Stream
Instruction Stream
Figure 11.3 The four architecture classes resulting from Flynn’s taxonomy. ( a) SISD,
(b) SIMD, ( c) MISD, ( d) MIMD11.2. CLASSIFICATION OF COMPUTER ARCHITECTURES 23911.2.2. Kuck Classiﬁcation Scheme
Flynn’s taxonomy can be considered a general classiﬁcation that has been extended
by a number of computer architects. One such extension is the classiﬁcation intro-duced by D. J. Kuck in 1978. In his classiﬁcation, Kuck extended the instruction
stream further to single (scalar and array) and multiple (scalar and array) streams.
The data stream in Kuck’s classiﬁcation is called the execution stream and is also
extended to include single (scalar and array) and multiple (scalar and array) streams.The combination of these streams results in a total of 16 categories of architectures,as shown in Table 11.3.
Our main observation is that both Flynn’s and Kuck’s classiﬁcations cover the
entire architecture space. However, while Flynn’s classiﬁcation emphasizes thedescription of architectures at the instruction set level, the Kuck’s classiﬁcation
emphasizes the description of architectures at the hardware level.
11.2.3. Hwang and Briggs Classiﬁcation Scheme
The main new contribution of the classiﬁcation due to Hwang and Briggs is the
introduction of the concept of classes . This is a further reﬁnement on Flynn’s classi-
ﬁcation. For example, according to Hwang and Briggs, the SISD category is furtherreﬁned into two subcategories: single functional unit SISD (SISD-S) and multipleTABLE 11.2 Example Machines and Their Flynn’s
Classiﬁcation
Classiﬁcation category Example machines
SISD IBM 704, VAX 11 /780, CRAY-1
SIMD ILLIAC-IV, MPP, CM-2, STARAN
MISD See observation 5 on page 238MIMD Cm
/C3, CRAY XMP, IBM 370 /168M
TABLE 11.3 The 16-Architecture Categories Resulting from Kuck’s Classiﬁcation
Instruction
streamExecution streams
Single Multiple
Scalar Array Scalar Array
Single
Scalar Uniprocessor Uniprocessor SIMD
Array ILLIAC-IV
Multiple
Scalar NYU Ultracomputer Cray X MP
Array240 INTRODUCTION TO MULTIPROCESSORSfunctional units SISD (SISD-M). The MIMD category is further reﬁned into loosely
coupled MIMD (MIMD-L) and tightly coupled MIMD (MIMD-T). The SIMDcategory is further reﬁned into word-sliced processing (SIMD-W) and bit-slicedprocessing (SIMD-B). Therefore, Hwang and Briggs classiﬁcation added a levelto the hierarchy of machine classiﬁcation such that a given machine should beﬁrst classiﬁed as SISD, SIMD, MIMD, and then further classiﬁed according to itsconstituent descendant.
According to the Hwang and Briggs’s taxonomy, it is always true to predict that an
SISD-M will perform better than an SISD-S. It is, however, doubtful that such predic-tion can be made with respect to SIMD-W and SIMD-B. For example, it has been indi-cated that using the maximum degree of potential parallelism as a performancemeasure, then the ILLAC-IV machine (SIMD-W) is inferior to the MPP machine(SIMD-B). A ﬁnal observation on the Hwang and Briggs’s taxonomy is that sharedmemory systems (see Chapter 4 of our book on Advanced Computer Architectureand Parallel Processing, see reference list) map naturally into the MIMD-T
category, while nonshared memory systems map into the MIMD-L category.
11.2.4. Erlangen Classiﬁcation Scheme
In its simplest form, this classiﬁcation scheme adds one more level of details to the
internal structure of a computer, compared to Flynn’s scheme. In particular, thisscheme considers that in addition to the control (CNTL) and processing (ALU)units, a third subunit, called the elementary logic unit (ELU), can be used to charac-
terize a given computer architecture. The ELU represents the circuitry required toperform the bit-level processing within the ALU. An architecture is characterizedusing a three-tuple system ( k,d,w) such that k¼number of CNTLs, d¼number
of ALU units associated with one control unit, and w¼number of ELUs per
ALU (the width of a single data word). For example, in one of its models, theILLAC-IV was made up of a mesh connected array of 64 64-bit ALUs controlledby a Burroughs B6700 computer. According to Erlangen, this model of theILLAC-IV is characterized as (1, 64, 64).
Postulating that pipelining can exist at all three levels of hardware processing, the
classiﬁcation includes three additional parameters. These are w
0¼the number of
pipeline stages per ALU, d0¼the number of functional units per ALU, and
k0¼the number of ELUs forming the control unit. Given the expected multi-unit
nature of each of the three hardware processing levels, a more general six-tuplecan be used to characterize an architecture as follows: ( k/C2k
0,d/C2d0,w/C2w0).
Figure 11.4 illustrates the Erlangen classiﬁcation system.
More complex systems can still be characterized using the Erlangen system
by using two additional operators, the AND operator, denoted by /C2, and the
ALTERNATIVE operator, denoted as _. For example, an architecture consisting
of two computational subunits each having a six-tuple ( k0/C2k00,d0/C2d00,w0/C2w00)
and ( k1/C2k01,d1/C2d01,w1/C2w01) is characterized using both subunits as ( k0/C2k00,
d0/C2d00,w0/C2w00)/C2(k1/C2k01,d1/C2d01,w1/C2w01), while an architecture that can be11.2. CLASSIFICATION OF COMPUTER ARCHITECTURES 241expressed using either of the two subunits is characterized as ,k0/C2k00,d0/C2d00,w0-
0/C2w00._,k1/C2k01,d1/C2d01,w1/C2w01..
For example, a later design of the ILLAC-IV consisted of two DEC PDP-10 as the
front-end controller where data can only be accepted from one PDP-10 at a time.
This version of the ILLAC-IV can be characterized as (2, 1, 36) /C2(1, 64, 64).
Now, since the ILLAC-IV can also work in a half-word mode whereby there are128 32-bit processors rather than the 64 64-bit processors, then an overall character-ization of the ILLAC-IV is given by (2, 1, 36) /C2[(1, 64, 64) _(1, 128, 32)].
As can be seen, this classiﬁcation scheme can be regarded as a hierarchical classi-
ﬁcation that puts more emphasis on the internal structure of the processing hardware.It does not provide any basis for the classiﬁcation and /or grouping of computer
architectures. In particular, the classiﬁcation overlooks the interconnection amongdifferent units.
11.2.5. Skillicorn Classiﬁcation Scheme
Owing to its inherent nature, Flynn’s classiﬁcation may end up grouping computer
systems with similar architectural characteristics but with diverse functionality into
one class. This same observation has been the main motive behind the Skillicorn
classiﬁcation introduced in 1988. According to this classiﬁcation, an abstract von
Neumann machine is modeled as shown in Figure 11.5. As can be seen, the abstract
model includes two memory subdivisions, instruction memory (IM) and datamemory (DM), in addition to the instruction processor (IP) and the data processor(DP). In developing the classiﬁcation scheme, the following possible interconnec-tion relationships were considered: (IP–DP), (IP–IM), (DP–DM), and (DP–IP).The interconnection scheme takes into consideration the type and number of con-
nections among the data processors, data memories, instruction processors, and
instruction memories. There may exist no, one-to-many, and many-to-many such
Figure 11.4 The Erlangen classiﬁcation scheme242 INTRODUCTION TO MULTIPROCESSORSconnections. Table 11.4 illustrates the different connection schemes identiﬁed by the
classiﬁcation.
Using the given connection schemes, Skillicorn arrived at 28 different classes.
Sample classes are shown in Table 11.5. The rightmost column of the table indicates
the corresponding Flynn’s class. Figure 11.6 illustrates four example classes accord-
ing to the classiﬁcation.
Major advantages of the Skillicorn classiﬁcation include (1) simplicity, (2) the
proper consideration of the interconnectivity among units, (3) ﬂexibility, and (4)the ability to represent most current computer systems. However, the classiﬁcationIPInstructions
DP
DP
Operations Instructions Addresses
Data Memory (DM)Instruction Memory
(IM)Operand
AddressesState
Figure 11.5 Abstract model of a simple machine
TABLE 11.4 Possible Connection Schemes
Connection type Meaning1–1 A connection between two single units
1–n A connection between a single unit and nother units
n–nn (1–1) connections
n/C2nn (1–n) connections
TABLE 11.5 Sample Connection Classes
Class IP DP IP–DP IP–IM DP–DM DP–DP Description Flynn
1 1 1 1–1 1–1 1–1 None Von Neumann
uniprocessorSISD
21 N 1 – n 1–1 n–nn/C2n Type 1 array
processorsSIMD
31 N 1 – n 1–1 n/C2n None Type 2 array
processorsSIMD
4N N n–nn –nn – n n/C2n Loosely coupled
von NeumannMIMD
5N N n–nn –nn/C2n None Tightly coupled
von NeumannMIMD11.2. CLASSIFICATION OF COMPUTER ARCHITECTURES 243(1) lacks the inclusion of operational aspects such as pipelining and (2) has difﬁculty
in predicting the relative power of machines belonging to the same class withoutexplicit knowledge of the interconnection scheme used in that class.
Multiple processor systems can be further classiﬁed as tightly coupled versus
loosely coupled. In a tightly coupled system, all processors can equally access a
global memory. In addition, each processor may also have its own local or cachememory. In a loosely coupled system, the memory is divided among processorssuch that each processor will have its own memory attached to it. However, pro-cessors still share the same memory address space. Any processor can directlyaccess any remote memory. Examples of tightly coupled multiple processors includethe CMU C.mmp , Encore Computer Multimax , and the Sequent Corp. Balance
series . Examples of loosely coupled multiple processors include CMU Cm
/C3, the
BBN Butterﬂy , and the IBM RP3.
11.3. SIMD SCHEMES
Recall that Flynn’s classiﬁcation results in four basic architectures. Among those,
the SIMD and the MIMD are frequently used in constructing parallel architectures.In this section, we will provide basic information on the SIMD paradigm. It is
important at the outset to indicate that SIMD are mostly designed to exploit the
inherent parallelism encountered in matrix (array) operations, which are required(a)( b)
(c)( d)IP IP
DP 1: n1: nnxn
D
MIMIMD
M
D
MDP
IPDPnxn
nxn
D
MDPnxn
IP
IMIM
Figure 11.6 Example connection classes. ( a) Array 1 class, ( b) array 2 class, ( c) tightly
coupled multiprocessor, ( d) loosely coupled multiprocessor244 INTRODUCTION TO MULTIPROCESSORSin applications such as image processing. Famous real-life machines that have been
commercially constructed include the ILLIAC-IV (1972), the STARAN (1974), andthe MPP (1982).
Two main SIMD conﬁgurations have been used in real-life machines. These are
shown in Figure 11.7.
In the ﬁrst scheme, each processor has its own local memory. Processors can
communicate with each other through the interconnection network. If the intercon-
nection network does not provide direct connection between a given pair of pro-
cessors, then this pair can exchange data via an intermediate processor. The
Figure 11.7 Two SIMD schemes. ( a) SIMD scheme 1, ( b) SIMD scheme 211.3. SIMD SCHEMES 245ILLIAC-IV used such an interconnection scheme. The interconnection network in
the ILLIAC-IV allowed each processor to communicate directly with four neighbor-ing processors in an 8 /C28 matrix pattern such that the ith processor can communicate
directly with the ( i21)th, ( iþ1)th, ( i28)th, and ( iþ8)th processors.
In the second SIMD scheme, processors and memory modules communicate with
each other via the interconnection network. Two processors can transfer databetween each other via intermediate memory module(s) or possibly via intermediate
processor(s). Assume, for example, that processor iis connected to memory modules
(i21),i, and ( iþ1). In this case, processor 1 can communicate with processor 5 via
memory modules 2, 3, and 4 as intermediaries. The BSP (Burroughs’ Scientiﬁc
Processor) used the second SIMD scheme.
In order to illustrate the effectiveness of SIMD in handling array operations, con-
sider, for example, the operations of adding the corresponding elements of two one-dimensional arrays AandBand storing the results in a third one-dimensional array
C. Assume also that each of the three arrays has Nelements. Assume also that SIMD
scheme 1 is used. The Nadditions required can be done in one step if the elements of
the three arrays are distributed such that M
0contains the elements A(0),B(0), and
C(0),M1contains the elements A(1),B(1), and C(1), ..., and MN21contains the
elements A(N21),B(N21), and C(N21). In this case, all processors will execute
simultaneously an add instruction of the form C AþB. After executing this
single step by all processors, the elements of the resultant array Cwill be stored
across the memory modules such that M0will store C(0),M1will store C(1), ...,
andMN21will store C(N21).
It is customary to formally represent an SIMD machine in terms of ﬁve-tuples
(N,C,I,M,F). The meaning of each argument is given below.
1.Nis the number of processing elements ( N¼2k,k/C211).
2.Cis the set of control instructions used by the control unit, for example, do,
for,step.
3.Iis the set of instructions executed by active processing units.
4.Mis the subset of processing elements that are enabled.
5.Fis the set of interconnection functions that determine the communication
links among processing elements.
11.4. MIMD SCHEMES
MIMD machines use a collection of processors, each having its own memory, which
can be used to collaborate on executing a given task. In general, MIMD systems
can be categorized based on their memory organization into shared-memory andmessage-passing architectures. The choice between the two categories depends onthe cost of communication (relative to that of the computation) and the degree of
load imbalance in the application.246 INTRODUCTION TO MULTIPROCESSORS11.4.1. Shared Memory Organization
There has been recent growing interest in distributed shared memory systems. This is
because shared memory provides an attractive conceptual model for interprocess inter-
action even when the underlying hardware provides no direct support. A shared memory
model is one in which processors communicate by reading and writing locations in ashared memory that is equally accessible by all processors. Each processor may haveregisters, buffers, caches, and local memory banks as additional memory resources.
A number of basic issues in the design of shared memory systems have to be
taken into consideration. These include access control, synchronization, protection,and security. Access control determines which process accesses are possible towhich resources. Access control models make the required check for every access
request issued by the processors to the shared memory, against the contents of
the access control table. The latter contains ﬂags that determine the legality ofeach access attempt. If there are access attempts to resources, then until the desiredaccess is completed, all disallowed access attempts and illegal processes areblocked. Requests from sharing processes may change the contents of the accesscontrol table during execution. The ﬂags of the access control with the synchroniza-tion rules determine the system’s functionality. Synchronization constraints limit thetime of accesses from sharing processes to shared resources. Appropriate synchro-
nization ensures that the information ﬂows properly and ensures system functiona-
lity. Protection is a system feature that prevents processes from making arbitraryaccess to resources belonging to other processes. Sharing and protection are incom-patible; sharing allows access, whereas protection restricts it.
Running two copies of the same program on two processors will decrease the per-
formance relative to that of a single processor, due to contention for shared memory.
The performance degrades further as three, four, or more copies of the program
execute at the same time.
A shared memory computer system consists of (1) a set of independent processors,
(2) a set of memory modules, and (3) an interconnection network. The simplest sharedmemory system consists of one memory module (M) that can be accessed from twoprocessors P
aandPb(Fig. 11.8). Requests arrive at the memory module through its
two ports. An arbitration unit within the memory module passes requests through toa memory controller. If the memory module is not busy and a single request arrives,then the arbitration unit passes that request to the memory controller and the requestis satisﬁed. The module is placed in the busy state while a request is being serviced.
Pa Pb
M
Figure 11.8 A simple shared memory scheme11.4. MIMD SCHEMES 247If a new request arrives while the memory is busy servicing a previous request, the
memory module sends a wait signal through the memory controller to the processormaking the new request. In response, the requesting processor may hold its requeston the line until the memory becomes free or it may repeat its request some timelater. If the arbitration unit receives two requests, it selects one of them and passesit to the memory controller. Again, the denied request can be either held to beserved next or it may be repeated some time later.
The arbitration unit may not be adequate to organize the use of the memory
module by the two processors. The main problem will be in the sequencing of inter-actions between memory accesses from the two processors. Consider the followingtwo scenarios for accessing the same memory location M(1000) by the two pro-cessors P
aandPb(Fig. 11.9). Let us also assume that the initial value stored in
memory location M(1000) is 150. Note that in both cases, the sequence of instruc-tions performed by each processor is the same. The only difference between the twoscenarios is the relative time at which the two processors update the value in
M(1000). A careful examination of the two scenarios will show that the value
stored in location M(1000) after the ﬁrst scenario will be 151 while the storedvalue following the second scenario will be 152.
The above illustrative example presents the case of a nonfunctional behavior of
this simple shared memory system. Such an example should demonstrate the basicrequirements for the success of such systems. These requirements are:
1. A mechanism for conﬂict resolution among rival processors
2. A technique for specifying the sequencing constraints3. A mechanism for enforcing the sequencing speciﬁcations
Approaches for satisfying these basic requirements are covered in Chapter 4 of our
book on Advanced Computer Architecture and Parallel Processing (see reference list).
The use of different interconnection networks in a shared memory multiprocessor
system leads to systems with one of the following characteristics:
1. Shared memory architecture with a uniform memory access (UMA)
2. Cache-only memory architecture (COMA)3. Distributed shared memory architecture with nonuniform memory access
(NUMA)
Cycle Processor Pa Processor Pb
1
23456
Scenario 1a← M(1000);
M(1000) ← a;a← a+ 1;b← M(1000);
M(1000) ← b;b← b+ 1;Cycle Processor P
a Processor Pb
123456
Scenario 2a← M(1000);
M(1000) ← a;a← a+ 1;
b← M(1000)
M(1000) ← b;b← b+ 1;
Figure 11.9 Potential shared memory problem248 INTRODUCTION TO MULTIPROCESSORSFigure 11.10 shows typical organization for the abovementioned three shared-
memory architectures. In the UMA system, a shared memory is accessible by all
processors through an interconnection network in the same way a single processoraccesses its memory. Therefore, all processors have equal access time to anymemory location. The interconnection network used in the UMA can be a singlebus, multiple bus, a crossbar, or a multiport memory.
In the NUMA system, each processor has part of the shared memory attached.
The memory has a single address space. Therefore, any processor could accessany memory location directly using its real address. However, the access time tomodules depends on the distance to the processor. This results in a nonuniformmemory access time. A number of architectures are used to interconnect processorsto memory modules in a NUMA. Among these are the tree and the hierarchical busnetworks (see Chapter 2 of our book on Advanced Computer Architecture andParallel Processing, see reference list).
Similar to the NUMA, each processor has part of the shared memory in the
COMA. However, in this case the shared memory consists of cache memory. ACOMA system requires that data be migrated to the processor requesting it.
11.4.2. Message-Passing Organization
Message passing represents an alternative method for communication and move-
ment of data among multiprocessors. Local, rather than global, memories are usedto communicate messages among processors. A message is deﬁned as a block ofrelated information that travels among processors over direct links. There exist anumber of models for message passing. Examples of message-passing systemsinclude the cosmic cube, workstation cluster, and the transputer.
The introduction of the transputer system T212 in 1983 announced the birth of
the ﬁrst message-passing multiprocessor. Subsequently the T414 was announced
in 1985, while Inmos introduced the VISI transputer processor in 1986. Two
subsequent transputer products, the T800 (1988) and T9000 (1990), have beenP
M
MP
PInter-
Connection
Network
(a)P
C
P
C
P
CInter-
Connection
Network
(b)M
MP
M
P
M
P
MInter-
Connection
Network
(c)
Figure 11.10 Three examples of shared-memory architectures. ( a) UMA, ( b) COMA,
(c) NUMA11.4. MIMD SCHEMES 249introduced. The cosmic cube message-passing multiprocessor was designed at
Caltech during the period 1981–1985. It represented the ﬁrst hypercube multi-processor system that was made to work. Wormhole routing in messagepassing was introduced in 1987 as an alternative to the traditional store-and-forward routing in order to reduce the size of the required buffers and todecrease the message latency. In wormhole routing , a packet is divided into
smaller units that are called ﬂits(ﬂow control bits) such that ﬂitsmove in a
pipeline fashion with the header ﬂitof the packet leading the way to the
destination node. When the header ﬂit is blocked due to network congestion, theremaining ﬂits are blocked as well (see Chapter 5 of our book on AdvancedComputer Architecture and Parallel Processing (see reference list) in Volume IIfor more details).
The elimination of the need for a large global memory, which is usually a reason
for a slowdown of the overall system, together with its asynchronous nature, givemessage-passing schemes an edge over shared-memory schemes. Similar to
shared-memory multiprocessors, application programs are divided into smaller
parts; each can be executed by an individual processor in a concurrent manner.
A simple example of a message-passing multiprocessor architecture is shown in
Figure 11.11. As can be seen from the ﬁgure, processors use local bus (internal chan-nels) to communicate with their local memories while communicating with otherprocessors via an interconnection networks (external channels). Processes runningon a given processor use internal channels to exchange messages among themselves.
Processes running on different processors use external channels to exchange mess-
ages. Such a scheme offers a great deal of ﬂexibility in accommodating a large
number of processors and being readily scalable. It should be noted that the processand the processor, which executes it, are considered as two separate entities. Thesize of a process is determined by the programmer and can be described by its granu-larity, given by:
Granularity¼
computation time
communication time
P1 M1 Pn Mn
Interconnection Network
Figure 11.11 Example of a message-passing multiprocessor architecture250 INTRODUCTION TO MULTIPROCESSORSThree types of granularity can be distinguished. These are:
1. Coarse granularity. Each process holds a large number of sequential instruc-
tions and takes a substantial time to execute.
2. Medium granularity. Since the process communication overhead increases as
the granularity decreases, medium granularity describes a middle ground
whereby communication overhead is reduced in order to enable each nodalcommunication to take less amount of time.
3. Fine granularity. Each process contains a few numbers of sequential instruc-
tions (as few as just one instruction).
Message-passing multiprocessors use mostly medium or coarse granularity.
Message-passing multiprocessors employ static networks in local communica-
tion. In particular, hypercube networks have been receiving special attention foruse in a message-passing multiprocessor. The nearest neighbor two-dimensionaland three-dimensional mesh networks have the potential for being used in amessage-passing system as well. Two important factors have led to the suitabilityof hypercube and mesh networks for use as message-passing networks. Thesefactors are (1) the ease of VLSI implementation and (2) the suitability for two-
and three-dimensional applications.
Two important design factors must be considered in designing such networks.
These are (1) the link bandwidth and (2) the network latency . The link bandwidth
is deﬁned as the number of bits that can be transmitted per unit time (bits /
second). The network latency is deﬁned as the time to complete a message transfer.
For example, links could be unidirectional or bidirectional and they can transfer onebit or several bits at a time. To estimate the network latency, we must ﬁrst determine
the path setup time, which depends on the number of nodes on the path. The actual
transition time, which depends on the message size, must also be considered.
The information transfer from a given source through the network can be done in
two ways:
1. Circuit-switching networks. In this type of network, there is no buffer
required in each node. The path between the source and destination is ﬁrst
determined. All links along that path are reserved. After information transfer,
reserved links are released for use by other messages. Circuit-switching net-works are characterized by producing the smallest amount of delay. Inefﬁ-cient link utilization is the main disadvantage of circuit-switchingnetworks. Circuit-switching networks are, therefore, advantageously usedonly in the case of large message transfer.
2. Packet-switching networks. Here, messages are divided into smaller parts,
called packets, before being transmitted between nodes. Each node mustcontain enough buffers to hold received packets before transmitting them.
A complete path from source to destination may not be available at the
start of transmission. As links become available, packets are moved from11.4. MIMD SCHEMES 251a node to a node until they reach the destination node. The technique is also
known as the store-and-forward packet-switching technique.
Although store-and-forward packet-switching networks eliminate the need for a
complete path at the start of transmission, they tend to increase the overall networklatency. This is because packets are expected to be stored in node buffers waiting forthe availability of outgoing links. In order to reduce the size of the required buffers
and decrease the incurred network latency, wormhole routing (see above) has been
introduced.
Having touched on some of the machine categories based on the Flynn’s classi-
ﬁcations, we now provide an introduction into the interconnection networks used in
these machines. We provide detailed coverage of multiprocessor interconnectionnetworks in Chapter 2 of our book on Advanced Computer Architecture and ParallelProcessing (see reference list).
11.5. INTERCONNECTION NETWORKS
A number of classiﬁcation criteria exist for interconnection networks (INs). Among
these criteria are the following.
11.5.1. Mode of Operation
According to the mode of operation, INs are classiﬁed as synchronous versus
asynchronous . In synchronous mode of operation, a single global clock is used by
all components in the system such that the whole system is operating in a lock-
step manner. Asynchronous mode of operation, on the other hand, does not require
a global clock. Handshaking signals are used instead in order to coordinate theoperation of asynchronous systems. While synchronous systems tend to be slowercompared to asynchronous systems, they are race and hazard-free.
11.5.2. Control Strategy
According to the control strategy, INs can be classiﬁed as centralized versus decen-
tralized . In centralized control systems, a single central control unit is used to over-
see and control the operation of the components of the system. In decentralized
control, the control function is distributed among different components in thesystem. The function and reliability of the central control unit can become the bottle-
neck in a centralized control system. While the crossbar is a centralized system, the
multistage interconnection networks are decentralized.
11.5.3. Switching Techniques
Interconnection networks can be classiﬁed according to the switching mechanism as
circuit versus packet switching networks. In the circuit switching mechanism, a252 INTRODUCTION TO MULTIPROCESSORScomplete path has to be established prior to the start of communication between a
source and a destination. The established path will remain in existence during thewhole communication period. In a packet switching mechanism, communicationbetween a source and destination takes place via messages that are divided intosmaller entities, called packets. On their way to the destination, packets can besent from one node to another in a store-and-forward manner until they reachtheir destination. While packet switching tends to use the network resources more
efﬁciently, compared to circuit switching, it suffers from variable packet delays.
11.5.4. Topology
According to their topology, INs are classiﬁed as static versus dynamic networks. In
dynamic networks, connections among inputs and outputs are made using switching
elements. Depending on the switch settings, different interconnections can be estab-lished. In static networks, direct ﬁxed paths exist between nodes. There are noswitching elements (nodes) in static networks.
Having introduced the general criteria for classiﬁcation of interconnection net-
works, we can now introduce a possible taxonomy for INs that is based on topology.
In Figure 11.12, we provide such a taxonomy.
According to the shown taxonomy, INs are classiﬁed as either static or dynamic.
Static networks can be further classiﬁed according to their interconnection patterns
as one-dimension (1D), two-dimension (2D), or hypercubes (HCs). Dynamic net-works, on the other hand, can be further classiﬁed according to the scheme of inter-connection as bus-based versus switch-based. Bus-based INs are classiﬁed as singlebus or multiple bus. Switch-based dynamic networks can be further classiﬁedaccording to the structure of the interconnection network as single-stage (SS), multi-
stage (MS), or crossbar networks.
Multiprocessor interconnection networks are explained in detail in Chapter 2
of our book on Advanced Computer Architecture and Parallel Processing
(see reference list).
Interconnection Networks
Static
SingleDynamic
1D 2D HC
Bus-based Switch-based
Crossbar MS SS Multiple
Figure 11.12 A topology-based taxonomy for interconnection networks11.5. INTERCONNECTION NETWORKS 25311.6. ANALYSIS AND PERFORMANCE METRICS
Having provided an introduction to the architecture of multiprocessors, we now pro-
vide some basic ideas about the performance issues in multiprocessors. Interestedreaders are referred to Chapter 3 of our book on Advanced Computer Architectureand Parallel Processing (see reference list) (Volume II) for more details.
A fundamental question that is usually asked is how much faster a given problem
can be solved using multiprocessors as compared to a single processor? Thisquestion can be formulated into the speed-up factor deﬁned below.
S(n)¼speed-up factor
¼Increase in speed due to the use of a multiprocessor system consisting of
nprocessors
¼
Execution time using a single processor
Execution time using nprocessors
A related question is that how efﬁciently each of the nprocessors is utilized. The
question can be formulated into the efﬁciency deﬁned below.
E(n)¼Efﬁciency
¼S(n)
n/C2100%
In executing tasks (programs) using a multiprocessor, it may be assumed that agiven task can be divided into nequal subtasks each of which can be executed by
one processor. Therefore, the expected speed-up will be given by the S(n)¼n
while the efﬁciency E(n)¼100%. The assumption that a given task can be
divided into nequal subtasks, each executed by a processor, is unrealistic. In
Chapter 3 of our book on Advanced Computer Architecture and Parallel Processing(see reference list) meaningful computation models will be developed and analyzed.A number of other performance metrics are also introduced and analyzed in the samechapter.
11.7. SUMMARY
In this chapter, we have navigated through a number of concepts and system
conﬁgurations related to the issues of multiprocessing. In particular, we haveprovided the general concepts and terminology used in the context of multiproces-sors. A number of taxonomies for multiprocessors have been introduced andanalyzed. Two memory organization schemes have been introduced. These are
the shared-memory and message-passing systems. In addition, we have introduced
the different topologies used for interconnecting multiple processors. In Chapter 2254 INTRODUCTION TO MULTIPROCESSORSof our book on Advanced Computer Architecture and Parallel Processing
(see reference list) more will be said about interconnection networks and theirperformance metrics. Shared-memory and message-passing architectures areexplained in Chapters 4 and 5, respectively, of the same reference mentionedabove.
EXERCISES
1. Consider the ﬁve classiﬁcations of computer architectures discussed in this
chapter. You are required to provide a list showing the advantages and dis-
advantages of each classiﬁcation in view of the degree to which each classi-ﬁcation satisﬁes the purpose for which a classiﬁcation is needed.
2. You are required to derive, out of the ﬁve provided classiﬁcations, a new
classiﬁcation that outperforms each of the ﬁve classiﬁcations. Provide, in atabular form, the additional advantages and eliminated shortcomings of theproposed classiﬁcation.
3. Provide a list of the main advantages and disadvantages of SIMD and MIMD
machines.
4. Provide a list of the main advantages and disadvantages of shared-memory
and message-passing paradigms.
5. List three engineering applications, with which you are familiar, for which
SIMD is most efﬁcient to use, and another three for which MIMD is most
efﬁcient to use.
6. Consider the case of connecting Nprocessors and Nmemory modules using
each of the interconnection networks shown in Figure 11.1. Assume that T
is the time required for a processor to access an item in a memory module
and that all processors make a request to access distinct memory module.Compute the worst-case possible delay expected in each of the four inter-connection networks.
7. It was mentioned that a given SIMD machine could be characterized using a
ﬁve-tuple ( N,C,I,M,F). You are required to select three different recent
SIMD machines and provide in a tabular form each of the ﬁve-tuples thatcharacterizes them.
8. Assume that a simple addition of two elements requires a unit time. You are
required to compute the execution time needed to perform the addition of a40/C240 element array using each of the following arrangements:
(a) An SIMD system having 64 processing elements connected in nearest-
neighbor fashion. Consider that each processor has only its local memory.
(b) An SIMD system having 64 processing elements connected to a shared
memory through an interconnection network. Ignore the communicationtime.EXERCISES 255(c) An MIMD computer system having 64 independent elements accessing
a shared memory through an interconnection network. Ignore the
communication time.
(d) Repeat (b) and (c) above if the communication time takes two time units.
9. Provide a concise discussion on the suitability of each of the four attributes of
interconnection networks (mode of operation, control strategy, switchingmechanism, and topology) for each of the four different interconnection net-works shown in Figure 11.1. Make sure that you justify the suitability of agiven attribute to a given interconnection network.
10. Consider the case of a multiprocessor system consisting of Nprocessors.
Assume that the time needed for each processor to execute a given critical
section is tand that frepresents the fraction of operations that can be paral-
lelized. Assume also that a single processor will need a time Tto execute the
same task. Show that the total execution time using Nprocessors is given by
T
N¼(1/C0f)/C2Tþf/C2T
Nþt:
What is the number of processors, N, needed in order to minimize the total
execution time TN.
REFERENCES AND FURTHER READING
S. Abraham and K. Padmanabhan, Performance of the direct binary n-cube network for multi-
processors, IEEE Trans. Comput. , 38(7), 1000–1011 (1989).
P. Agrawal, V. Janakiram and G. Pathak, Evaluating the performance of multicomputer
conﬁgurations, IEEE Trans. Comput. , 19(5), 23–27 (1986).
G. Almasi and A. Gottlieb, Highly Parallel Computing , Benjamin Cummings, Redwood City,
CA, USA, 1989.
K. Al-Tawil, M. Abd-El-Barr and F. Ashraf, A survey and comparison of wormhole routing
techniques in mesh networks, IEEE Network , 11(2), 38–45 (1997).
L. Bhuyan, Q. Yang and D. Agrawal, Performance of multiprocessor interconnection net-
works, IEEE Comput. , 22(2), 25–37 (1989).
W.-T. Chen and J.-P. Sheu, Performance analysis of multiple bus interconnection networks
with hierarchical requesting model, IEEE Trans. Comput. , 40(7), 834–842 (1991).
S. Dasgupta, Computer Architecture: A Modern Synthesis , Vol. 2: Advanced Topics, John
Wiley, New York, 1989.
A. Decegama, The Technology of Parallel Processing: Parallel Processing Architectures and
VLSI Hardware Volume 1 , Prentice-Hall, NJ, 1989.
J. Dongarra, Experimental Parallel Computing Architectures , North-Holland, Amsterdam,
1987.
A. Goyal and T. Agerwala, Performance analysis of future shared storage systems,
IBM J. Res. Devel. , 28(1), 95–107 (1984).256 INTRODUCTION TO MULTIPROCESSORSJ.-Y. Juang and B. Wah, A contention-based bus-control scheme for multiprocessor systems,
IEEE Trans. Comput. , 40(9), 1046–1053 (1991).
T. Lewis and H. El-Rewini, Introduction to Parallel Computing, Prentice-Hall, Englewood
Cliffs, NJ, 1992.
D. Linder and J. Harden, An adaptive and fault tolerant wormhole routing strategy for k-ary
n-cubes, IEEE Trans. Comput. , 40(1), 2–12 (1991).
L. Ni and P. McKinely, A survey of wormhole routing techniques in direct networks, IEEE
Comput. , 26(2), 62–76 (1993).
J. Patel, Performance of processor–memory interconnections for multiprocessor computer
systems, IEEE Trans. , 28(9), 296–304 (1981).
D. Reed and R. Fujimoto, Multicomputer Networks: Message-Based Parallel Processing ,
MIT Press, MA, USA, 1987.
H. El-Rewini and T. Lewis, Distributed and Parallel Computing, Manning & Prentice Hall,
1998.
H. El-Rewini and M. Abd-El-Barr, Advanced Computer Architecture and Parallel Processing,
John Wiley, Hoboken, NJ, USA, 2005.
E. Sima, T. Fountain and P. Kacsuk, Advanced Computer Architectures: A Design Space
Approach , Addison Wesley, MA, USA, 1996.
H. Stone, High Performance Computer Architecture , 3rd ed., Addison Wesley, MA, USA,
1993.
B. Wilkinson, Computer Architecture: Design and Performance , 2nd ed., Prentice-Hall,
Hertfordshire, UK, 1996.
Q. Yang and S. Zaky, Communication performance in multiple-bus systems, IEEE Trans.
Comput. , 37(7), 848–853 (1988).
H. Youn and C. Chen, A comprehensive performance evaluation of crossbar networks, IEEE
Trans. Parallel Distribute Syst. , 4(5), 481–489 (1993).
M. Zargham, Computer Architecture: Single and Parallel Systems , Prentice-Hall, NJ, USA,
1996.REFERENCES AND FURTHER READING 257& INDEX
Access type, memory system design,
108–109
Accumulator-based processor
assembly language programming, 38–40
input-output design, 166
Adders, 65–67, 73–74, 209–211
Addition operation
addressing modes, 18–19CPU instruction cycle, 92–94
ﬂoating-point arithmetic pipelines,
211–212
two’ s complement (2’ s) representation,
63–64
Address decoder circuitry, I /O system
design, 163
Address ﬁeld, addressing modes, 18
Addressing modes
instruction set architecture, 18–26
autodecrement mode, 25–26
autoincrement mode, 23–24
direct (absolute) mode, 21–22immediate mode, 21
indexed mode, 23
indirect mode, 22
relative mode, 23
X86 family, assembly language
programming for, 50–55
Address lines, input /output (I /O) buses,
177
Add-shift method, multiplication of
unsigned numbers, 68–70
Add/sub operations
Booth’ s algorithm, 71–72
ﬂoating-point arithmetic, 75–76
Advanced RISC Machines (ARM)
interrupt architecture, input /output
systems, 171–1731026EJ-S processor, pipeline design,
202–203
pipeline stall reduction, 198–199
Aligned exponents, ﬂoating-point arithmetic
addition /subtraction, 75–76
Allocate policies, 127
Alpha 2164 pipeline, RISC design, 227–230
Amdahl’ s law, performance analysis, 10–11
Arbitration
buses, 179–180multiprocessor architecture, multiple-
instruction multiple-data streams(MIMD), 248–249
Architecture classiﬁcations. See also
Instruction set architecture
evolution of, 4–5multiprocessors, 236–244
Erlangen classiﬁcation, 241–242
Flynn’ s classiﬁcation, 237–240
Hwang and Briggs classiﬁcation
scheme, 240–241
Kuck classiﬁcation, 240
Skillicorn classiﬁcation, 242–244
Arithmetic and Logic Unit (ALU)
Booth’ s algorithm, 71–72
central processing unit design, 83–85
datapath, 89–91horizontal vs. vertical microinstruc-
tions, 103–104
three-bus organization, 90–91
two-bus organization, 90
data dependency pipeline stall reduction,
199–200
1026EJ-S processor pipeline design,
202–203
Arithmetic instructions, 27–28
X86 family, 50–55
259
Fundamentals of Computer Organization and Architecture , by M. Abd-El-Barr and H. El-Rewini
ISBN 0-471-46741-3 Copyright #2005 John Wiley & Sons, Inc.Arithmetic pipline design, 209–213
ﬁxed-point arithmetic, 209–211
ﬂoating-point arithmetic, 211–212
multiplication with carry-save addition,
212–213
Array processors
main memory unit, 137–142
ASCII code
input-output design, 161
Assemblers, assembly language
programming, 45
Assembly and execution process, assembly
language programming, 44–47
Assembly language programming
assembler directives and commands,
43–44
basic principles, 37–38
instruction mnemonics and syntax,
40–43
programing assembly and execution,
44–47
assemblers, 45
data structures, 45–46
linker and loader, 46–47
simple machines, 38–40
X86 family example, 47–55
Associative memory
cache memory organization, 118
virtual memory system, 144–145
Asynchronous buses
input /output system design, 178–179
multiprocessor interconnection networks,
252
Autodecrement addressing mode, 25–26Autoincrement addressing mode, 23–24
Bandwidth parameters
memory system design, 108–109
multiprocessors, 251–252
Base pointer register (BPR), X86 family,
assembly language programming for,
50–55
Base radix, ﬂoating-point representation,
75
Base register sets, X86 family, assembly
language programming for, 49–55
Benchmarks
computer systems, 6–11
Berkeley RISC, design features, 223–224Bias, ﬂoating-point representation, 75
Binary coded decimal (BCD) system
assembly language programming, simple
machines, 39–40
computer architecture and, 4Binary digit (bit), memory locations and
operations, 15–18
Binary division structure, integer arithmetic,
73–74
Binary radices
integer arithmetic, 63–74
number system, 59–63
Bit.SeeBinary digit (bit), memory locations
and operations
Block ﬁeld protocol, cache memory
organization, 115–116
Boolean equations, central processing unit
design
control unit, 96
hardwired implementation, 97–98
Booth’ s algorithm, multiplication of
unsigned numbers, 70–72
Branch delay shots, pipeline stall reductions,
conditional branch instructions,
196–197
“Branch if negative” instruction, pipeline
stall, instruction dependency, 188
Branch instruction queue (BIQ),
UltraSPARC III RISC processor
pipeline design, 207
Branch miss queue (BMQ)
pipeline stall reduction, 199UltraSPARC III RISC processor pipeline
design, 207
Branch prediction
Alpha 2164 pipeline, 228–230pipeline stall reduction, 198–199
UltraSPARC III RISC processor,
206–207
Branch target address cache (BTAC),
pipeline stall reduction, 198–199
Buffers
instruction buffers, pipeline design, 207
translation look-aside buffer (TLB),
146–148
Buses, input /output (I /O) systems,
177–180
arbitration, 179–180
asynchronous buses, 178–179
synchronous buses, 178
Bus line connection, input /output (I /O)
systems, 177
Bytes, memory locations and operations,
16–18
Cache hit
cache write policies, 125–127
virtual memory system, cache memory
with, 152–153260
INDEXCache locking, PMC-Sierra RM7000A
processor, 130
Cache memory, 109–130
combined spatial /temporal locality,
111–112
direct mapping, 113–116
fully associative memory, 116–118
mapping function, 112–113
organization, 113–121
real-life organization analysis, 128–130
Pentium IV processor cache,
128–129
PMC-Sierra RM7000A 64-bit MIPS
RISC processor, 130
PowerPC 604 processor cache, 129
replacement techniques, 121–124
set-associative mapping, 118–121spatial locality, 111
temporal locality, 111
virtual memory systems with, 152–153
write policies, 124–127
Cache miss
cache memory organization, 115–116
set-associative mapping, 121
instruction pipeline design, 187–188
write policies, 127
Cache-only memory architecture (COMA),
multiprocessor architecture, multiple-
instruction multiple-data streams
(MIMD), 248–249
Cache unit, memory hierarchy, 107–109
Capacity parameters, memory system
design, 108–109
Carry generate function, hardware structures
for addition and subtraction, 65–67
Carry-look-ahead adders (CLAA)
ﬁxed-point arithmetic pipelines,
209–211
hardware structures for addition and
subtraction, 65–67
Carry propagate function, hardware
structures for addition andsubtraction, 64–67
Carry ripple through adder (CRTA)
ﬁxed-point arithmetic pipelines,
209–211
hardware structures for addition and
subtraction, 65–67
Carry-save addition, pipelined
multiplication, 212–213
Centralized bus arbitration, input /output
system design, 179–180
Centralized interconnections, multiprocessor
interconnection networks, 252Central processing unit (CPU)
buses, 177–178
clock, performance analysis, 6–7
control unit, 95–104
hardwired implementation, 97–98
horizontal vs. vertical
microinstructions, 101–104
microprogrammed unit, 98–100
datapath, 89–91
one-bus organization, 89
three-bus organization, 90–91
two-bus organization, 90
design criteria, basic principles, 83–85
input /output (I /O) system design,
161–162
direct memory access, 175–177
instruction cycle, 91–95
execute simple arithmetic operation,
92–94
fetch instructions, 92
interrupt handling, 94–95
interrupt-driven input /output (I /O)
systems, 167–175
main memory and, 135–142memory locations and operations,
16–18
overlapped register windows, 220–221programmed input /output design,
166–167
register set, 85–88
condition registers, 86instruction fetching registers, 86
memory access registers, 85–86
MIPS registers, 87–88
special-purpose address registers,
86
80x86 registers, 87
Characters
input-output design, 161
Chip memory arrays, main memory unit,
137–142
Chip pin arrangements, main memory unit,
138–142
Circuitry. See also Adders
main memory unit, 140–142
Circuit-switching networks, multiprocessor
architecture
interconnections, 252–253
multiple-instruction multiple-data
streams (MIMD), 251–252
Classes
multiprocessors, 240–241
Clock cycles per instruction (CPI),
performance analysis, 7–8INDEX 261Clock rates
central processing unit design, control
unit, 95–104
computer architectures and, 4–5
performance analysis, 6–7
Clock replacement algorithm, virtual
memory, 150–152
CMOS systems
static memory units, 140–142
Column address strobe (CAS), main
memory unit, 141–142
Combined temporal /spatial locality, cache
memory, 111–112
Commands, assembly language
programming, 43–44
Communication model (CM), multi-
processor design, 236
Compaq (formerly DEC) alpha 21264
architecture, RISC design, 227
Complex instruction set computers (CISC)
architecture and examples of, 4RISC design vs, 221–223
RISC evolution cycle, 217–218
Computer arithmetic
double-precision IEEE format, 78–79
ﬂoating-point arithmetic, 74–77
addition /subtraction, 75–76
division, 77
multiplication, 76–77
representation (scientiﬁc notation),
74–75
IEEE ﬂoating-point standard, 77–78
integer arithmetic, 63–74
addition, 63–64
division, 72–74
multiplication, 68–72
add-shift method, 68–70Booth’ s algorithm, 70–72paper and pencil method
(unsigned numbers), 68
signed numbers, hardware structures,
64–67
subtraction, 64
two’ s complement (2’ s) represen-
tation, 63
number systems, 59–63
diminished radix complement,
62–63
negative integer representation, 61radix complement, 62
radix conversion algorithm, 60–61
sign-magnitude, 61
Conditional branch instructions, pipeline
stall reductions, 196–199Condition registers, central processing unit
design, 86
Context switching, interrupt-driven I /O
systems, 169–170
Control circuitry, main memory unit,
140–142
Control Data Corporation, history of, 3
Control memory (CM), central processing
unit design, microprogrammed units,
100–104
Control strategy, multiprocessor inter-
connection networks, 252
Control transfer instructions, X86 family,
51–55
Control unit (CU), central processing unit
design, 83–85, 95–104
hardwired implementation, 97–98
horizontal vs. vertical microinstructions,
101–104
microprogrammed unit, 98–100
Cost parameters, memory system design,
108–109
Cray Research Corporation, history of, 3Current program state register (CPSR),
advanced RISC machines interrupt
architecture, 173
Current window pointer, Berkeley RISC,
223–224
Cycle count (CC), performance analysis,
6–7
Cycle time (CT)
memory system design, 108–109
performance analysis, 6–7
Daisy chain bus arbitration (DCBa)
input /output system design, 179–180
interrupt-driven I /O device, 168
Data dependency, pipeline stall, 189–192
NOP prevention method, 192–194
reduction methods, 199–201
Data memory (DM), multiprocessor
architecture, Skillicorn classiﬁcation,
242–244
Data movement instructions
basic properties, 26–27
X86 family, 50–55
Data output register, input /output
instructions and, 31
Datapath, central processing unit, 89–91
one-bus organization, 89
three-bus organization, 90–91
two-bus organization, 90
Data structures, assembly language
programming, 45–46262
INDEXDecentralized arbitration, buses, 180
Decentralized interconnections, multi-
processor interconnection networks,
252
Delayed branch, pipeline stall reductions,
conditional branch instructions,
196–197
Demand paging, 148Dependency
data dependency, 189–194, 200–201
instructional dependency, 188, 194–199
Destination registers, arithmetic and logical
instructions, 27–28
CPU design, 92–94
Devices per chip, evolution of integration,
table of, 5
Digital Equipment Corporation (DEC),
history of, 3
Diminished radix complement, number
systems, 62–63
Direct (absolute) addressing mode, 21–22
X86 family, assembly language
programming for, 50–55
Directives, assembly language
programming, 43–44
Direct mapping
cache memory organization, 113–116
replacement techniques, 122–123
virtual memory systems, 143–144
Direct memory access (DMA)
basic properties, 175–177
input /output (I /O) system design, 162,
175–177
Dirty bit
cache write policies, 126–127virtual memory systems, 143
Distributed memory systems, history of, 3Division
ﬂoating-point arithmetic, 77–78
integer arithmetic, 72–74
Double-point format, IEEE ﬂoating-point
standard, 77–79
Dynamic branch prediction, pipeline stall
reduction, conditional branch
instructions, 198–199
Dynamic cells, main memory unit,
139–142
Dynamic interconnections, multiprocessor
interconnections, 253
Dynamic scheduling, instruction-level
parallelism, pipeline design, 207–209
Efﬁciency E(n) measurement, pipelining
design, 186–18780x86 registers
central processing unit design, 87
interrupt-driven I /O systems,
170–171
Electronic Delay Storage Automatic
Calculator (EDSAC), history of, 3
Electronic Discrete Variable Automatic
Computer (EDVAC), history of, 2
Electronic Numerical Integrator and
Calculator (ENIAC) machine,
history of, 2
Elementary logic unit (ELU), multiprocessor
architecture, Erlangen classiﬁcation
scheme, 241–242
Erasable PROM (EPROM), basic properties,
157–158
Erlangen classiﬁcation scheme, multipro-
cessor architecture, 241–242
Evolutionary architecture classiﬁcation,
multiprocessors, 237
Execution stream
multiprocessors, 240
Execution time, performance analysis, 10Exponent
ﬂoating-point representation, 75
IEEE ﬂoating-point standard, 78
Exponent alignment (EA) operation,
ﬂoating-point arithmetic pipelines,
211–212
Exponent comparison (EC) operation,
ﬂoating-point arithmetic pipelines,
211–212
Fetch-fetch operation, data dependency
pipeline reduction, 201
Fetch stage
Alpha 2164 pipeline, 227–230CPU instruction cycle, 82
microprogrammed control unit,
100–104
pipeline stall reduction, 195
1026EJ-S processor pipeline design,
202–203
prediction of, 197–198
FIQ requests, advanced RISC machines
interrupt architecture, 172–173
First-in-ﬁrst-out (FIFO) replacement
cache memory systems, 121–124
1026EJ-S processor pipeline design,
202–203
virtual memory systems, 148–149
First-in-not-used-ﬁrst-out (FINUFO), clock
replacement algorithm, virtual
memory, 150–152INDEX 263Fixed-point arithmetic
history of computers and, 2
pipelines design principles, 209–211
Flag register, X86 family, assembly language
programming for, 49–55
Flash EPROMs (FEPROMs), basic
properties, 157–158
Flits (ﬂow control hits), multiprocessor
architecture, multiple-instruction
multiple-data streams (MIMD),
250–252
Floating-point arithmetic, 74–77
addition /subtraction, 75–76
Alpha 2164 pipeline, 230Berkeley RISC, 224
division, 77
IEEE standard, 77–79multiplication, 76–77
pipeline design, 211–212
representation (scientiﬁc notation),
74–75
Flynn’ s classiﬁcation scheme, multiproces-
sor architecture, 237–240
Full-adder (FA), hardware structures for
addition and subtraction, 65–67
Fully associative mapping, cache memory
organization, 116–118
replacement techniques, 123, 125
Gantt’s chart
pipeline stall
conditional branch instructions, 197
data dependency, 190–192
instruction dependency, 188
pipelining design, 186
General purpose computer systems, 1
central processing unit design, 87
Global share dynamic prediction algorithm,
UltraSPARC III RISC processor
pipeline design, 206–207
Grant line (GL), interrupt-driven I /O device,
168
Granularity, multiprocessor architecture,
multiple- instruction multiple-data
streams (MIMD), 250–252
Handshaking, buses, 178
Hardware operand forwarding, data
dependency pipeline stall reduction,
199–200
Hardware structures
addition /subtraction of signed numbers,
64–67
binary division operations, 73–74interrupt-driven I /O device, 168
I/O polling scheme, programmed input /
output design, 166–167
pipeline stall reduction, fetch unit, 195
Hardwired control units, central processing
unit design, 96–104
direct implementation, 97–98
Harvard Architecture, history of, 3Harvard Organization, PowerPC 604
processor cache, 129
Hexadecimal programming, simple
machines, 40
Hexagonal base, number system, 59–63
Hierarchy parameters, memory system
design, 107–109
High-level languages (HLLs), RISC design,
218–220
Historical background, computer systems,
2–4
Hit ratio
cache memory, 110
memory hierarchy, 109
Horizontal microinstructions, central
processing unit design, 101–104
Hwang /Briggs classiﬁcation scheme,
multiprocessor architecture, 240–241
IBM systems, history of, 3
IEEE standard, ﬂoating-point standard,
77–79
ILLAC-IV design
Erlangen classiﬁcation scheme, 242
multiprocessor architecture,
single-instruction multiple-data
streams (SIMD), 245–246
Immediate addressing mode, 20–21
Independent source bus arbitration (ISBA),
interrupt-driven I /O device, 168
Indexed addressing mode, 20–21, 23
Index register
central processing unit design, 86deﬁned, 23
X86 family, 50–55
Indirect addressing mode, 22
X86 family, 50–55
Input data register
input /output instructions and, 31
input /output (I /O) system design,
162–163
Input /output (I /O) system design
basic concepts, 162–164
buses, 177–180
arbitration, 179–180
asynchronous buses, 178–179264
INDEXsynchronous buses, 178
central processing unit design, 84–85
design and organization, 161–162
direct memory access, 175–177
instruction set architecture and, 1, 30–31
interfaces, 181–182
interrupt-driven I /O, 167–175
ARM architecture, 171–173hardware, 168
MC9328MX1 /MXL AITC, 173–
175
operating systems, 168–175
80x86 architecture, 170–171
programmed I /O, 164–167
Institute for Advanced Study (IAS) machine,
history of, 2
Instruction buffer, UltraSPARC III RISC
processor pipeline design, 207
Instruction cycle, central processing unit
design, 91–95
execute simple arithmetic operation,
92–94
fetch instructions, 92interrupt handling, 94–95
Instruction dependency, pipeline stall, 188
unconditional branch instructions,
194–196
Instruction-level parallelism (ILP),
pipelining design, 207–209
superscalar architectures, 207–209
very long instruction word (VLIW), 209
Instruction memory (IM), multiprocessor
architecture, Skillicorn classiﬁcation,
242–244
Instruction pipeline design, 187–201
data dependency "stall," 189–201
hardware operand forwarding,
199–200
software operand forwarding,
200–201
instruction dependency "stall," 188
conditional branch instructions,
196–199
unconditional branch instructions,
194–196
wrong instruction /operand, prevention,
192–194
Instruction prefetching, pipeline stall
reduction, 196
Instruction register (IR), central processing
unit design, 83–86
one-bus organization, 89
Instruction reordering, pipeline stall
reduction, 194–195precomputing of branches, 195–196
Instruction set architecture
addressing modes, 18–26
autodecrement mode, 25–26
autoincrement mode, 23–24
direct (absolute) mode, 21–22
immediate mode, 21
indexed mode, 23
indirect mode, 22
relative mode, 23
assembly language programming
mnemonics and syntax, 40–43simple machines, 38–40
basic principles, 15
deﬁned, 1
instruction types, 26–31
arithmetic and logical instructions,
27–28
data movement instructions, 26–27
input /output instructions, 30–31
sequencing instructions, 28–30
memory locations and operations, 15–18
programming examples, 31–33
Instruction types, instruction set architecture,
26–31
arithmetic and logical instructions,
27–28
data movement instructions, 26–27
input /output instructions, 30–31
sequencing instructions, 28–30
Integer arithmetic, 63–74
addition, 63–64division, 72–74
multiplication, 68–72
add-shift method, 68–70
Booth’ s algorithm, 70–72
paper and pencil method (unsigned
numbers), 68
signed numbers, hardware structures,
64–67
subtraction, 64
two’ s complement (2’ s) representation, 63
Integer unit
Alpha 2164 pipeline, 230
1026EJ-S processor pipeline design, 203
Integrated circuit (IC), main memory unit,
141–142
Intel microprocessors
central processing unit design, 87
history of, 3real-life cache organization analysis,
128–129
X86 family, assembly language
programming for, 48–55INDEX 265Interconnection networks, multiprocessor
architecture, 252–253
Interrupt acknowledgement (INTA),
interrupt-driven I /O systems,
170–171
Interrupt-driven communication, input /
output (I /O) systems, 161–162
Interrupt-driven input /output (I /O) systems,
167–175
ARM architecture, 171–173
hardware, 168
MC9328MX1 /MXL AITC, 173–175
operating systems, 168–17580x86 architecture, 170–171
Interrupt handling, CPU design, 94–95Interrupt service routine (ISR)
interrupt-driven input /output (I /O)
systems, 167–175
interrupt-driven I /O systems, 170–171
Interrupt vector table (IVT)
advanced RISC machines interrupt
architecture, 173
interrupt-driven I /O systems, 170–171
INTR pin, interrupt-driven I /O systems,
80x86 architecture, 170–171
I/O protocol, input /output (I /O) system
design, 162–163
IRQ request, advanced RISC machines
interrupt architecture, 172–173
JBus , UltraSPARC III RISC processor
design, 231–232
Keyboard, as input /output device, 31
Kuck classiﬁcation scheme, multiprocessor
architecture, 240
Language architecture, deﬁned, 1
Large-scale integration (LSI), evolution of,
5–6
Latency parameters, memory system design,
108–109
Least recently used (LRU) replacement
cache memory, 121–124
virtual memory, 149–150
“Likely not to be taken” (LNK) algorithm,
pipeline stall reduction, conditional
branch instructions, 198–199
“Likely to be taken” (LTK) algorithm,
pipeline stall reduction, conditional
branch instructions, 198–199
Linkers, assembly language programming,
46–47Loaders, assembly language programming,
46–47
Local area networks (LAN), history of, 3Locality of reference, memory hierarchy,
108–109
Logical instructions, 27–28
X86 family, 50–55
Machine language, assembly language
programming, 38
Main memory unit (MMU)
basic properties, 135–142
fully associative mapping, 116–118
hierarchy parameters, 107–109
virtual memory, 142–155
associative mapping, 144–145
cache memory, 152–153
paged segmentation, 154–155
Pentium memory management, 155
replacement algorithms (policies),
148–152
clock replacement algorithm, 150–152
ﬁrst-in-ﬁrst-out (FIFO) replacement,
148–149
least recently used (LUR)
replacement, 149–150
random replacement, 148
segment address translation, 153–154segmentation, 153
set-associative mapping, 145–146
translation look-aside buffer (TLB),
146–148
Mantissa, ﬂoating-point representation,
74–75
Many-to-one mapping technique, cache
memory organization, 113–116
Mapping function, cache memory, 112–113
MARK computer systems, history of, 3Mask-programmed ROMs, 156–158
MC9328MX1 /MXL AITC, input /output
systems, 173–175
Medium-scale integration (MSI), evolution
of, 5–6
Memory access registers, central processing
unit design, 85–86
Memory address register (MAR)
central processing unit design, 85–86
interrupt handling, 94–95
one-bus organization, 89
fetch instructions, 92
main memory, 135–142
write operations and, 17–18
Memory data register (MDR)
central processing unit design, 85–86266
INDEXinterrupt handling, 94–95
one-bus organization, 89
fetch instructions, 92
main memory, 135–142
write operations and, 17–18
Memory hierarchy, Alpha 2164 pipeline,
227–230
Memory indirect addressing, 22Memory interleaving, cache memory,
110–111
Memory locations and operations,
instruction set architecture, 15–18
Memory management unit (MMU),
cache-mapping function, 112–113
Memory-mapped input /output, 31
I/O system design, 164
Memory operations, central processing unit
design, microinstructions, 103–104
Memory system design
basic concepts, 107–109buses, 177–180cache memory, 109–130
combined spatial /temporal locality,
111–112
direct mapping, 113–116
fully associative memory, 116–118
mapping function, 112–113
organization, 113–121
real-life organization analysis,
128–130
replacement techniques, 121–124
set-associative mapping, 118–121spatial locality, 111
temporal locality, 111
write policies, 124–127
hierachy parameters, 107–109
input /output (I /O) interfaces, 181–182
main memory, 135–142
read-only memory, 156–158
virtual memory, 142–155
associative mapping, 144–145cache memory, 152–153
paged segmentation, 154–155
Pentium memory management, 155
replacement algorithms (policies),
148–152
clock replacement algorithm,
150–152
ﬁrst-in-ﬁrst-out (FIFO)
replacement, 148–149
least recently used (LUR)
replacement, 149–150
random replacement, 148
segment address translation, 153–154segmentation, 153
set-associative mapping, 145–146
translation look-aside buffer (TLB),
146–148
Message-passing organization, multi-
processor architecture, multiple-instruction multiple-data streams
(MIMD), 249–252
Microinstructions, central processing unit
design, 84–85
horizontal vs. vertical, 101–104
Microprocessor, history of, 3
Microprogrammed units, central processing
unit design, 96–104
Million ﬂoating-point instructions per
second (MFLOP), performanceanalysis, 9–10
Million instructions-per-second (MIPS) rate
central processing unit registers, 87–88
performance analysis, 8–9
Minicomputers, history of, 3Miss ratio
cache memory, 110memory hierarchy, 109
Mnemonics, assembly language
programming, instruction set, 40–43
Morphological architecture classiﬁcation,
multiprocessors, 237
Most signiﬁcant bit (MSB), ﬂoating-point
representation, 75
Mouse, as input /output device, 31
MPP system, multiprocessor architecture,
single- instruction multiple-data
streams (SIMD), 245–246
Multiple-instruction multiple-data streams
(MIMD), multiprocessor architecture
basic principles, 246–252
Flynn classiﬁcation, 238–240
Hwang /Briggs classiﬁcation scheme,
241
message-passing organization,
249–252
shared memory organization, 247–249
Multiple-instruction single-data streams
(MISD), multiprocessor architecture,
Flynn classiﬁcation, 238–240
Multiple interrupts, interrupt-driven input /
output (I /O) systems, 168–175
Multiple issue processors (MIP), instruction-
level parallelism, pipeline design,
207–209
Multiplication
ﬂoating-point arithmetic, 76–77
integer arithmetic, 68–72INDEX 267Multiplication [ continued ]
pipelined multiplication, carry-save
addition, 212–213
Multiprocessors
architecture classiﬁcations, 236–244
Erlangen classiﬁcation, 241–242
Flynn’ s classiﬁcation, 237–240
Hwang and Briggs classiﬁcation
scheme, 240–241
Kuck classiﬁcation, 240
Skillicorn classiﬁcation, 242–244
basic principles, 235–236
interconnection networks, 252–253
MIMD architecture, 246–252
message-passing organization,
249–252
shared memory organization,
247–249
performance analysis, 254SIMD design, 244–246
(nþ1)-bit adder, binary division oper-
ations, 73–74
Negative integer representation, number
systems, 61
Network systems, history of, 3
NMI pins, interrupt-driven I /O systems,
170–171
Non-blocking caches, real-life organization,
130
Non-restoring division algorithm, integer
arithmetic, 73–74
Nonuniform memory access (NUMA),
multiprocessor architecture, multiple-instruction multiple-data streams
(MIMD), 248–249
Nonvolatile memory, 156
No operation (NOP) method, pipeline stall,
data dependency, 192–194
Normalization (NZ) operation, ﬂoating-
point arithmetic pipelines,211–212
Normalized forms
ﬂoating-point arithmetic addition /
subtraction, 76
ﬂoating-point representation, 75
Number systems, 59–63
diminished radix complement,
62–63
negative integer representation, 61
radix complement, 62
radix conversion algorithm, 60–61sign-magnitude, 61Octagonal base, number system, 59–63One-address instruction, addressing modes,
18–19
One-bus organization, CPU datapath, 89
arithmetic operations, 93–94
Op-code
addressing modes, 18
assembly language programming
data structures, 45–46
mnemonics and syntax, 41–43
central processing unit design, control
unit, 96–104
Operands
addressing modes, 18, 20–21
pipeline stall, data dependency, 192–196
hardware operand forwarding,
199–200
software operand forwarding,
200–201
Operating systems (OS)
assembly language programming,
assembler directives and
commands, 44
interrupt-driven I /O systems, 168–171
virtual memory system, segmentation,
153
Operation modes, multiprocessor architec-
ture, 252
Operations distribution, RISC design,
219–220
Out-of-order (OOO) issue logic, Alpha 2164
pipeline, 229–230
Output register, input /output (I /O) system
design, 162–163
Overlapped register windows, RISC design,
220–222
Packet-switching networks, multiprocessor
architecture
interconnections, 252–253
multiple-instruction multiple-data
streams (MIMD), 251–252
Paged segmentation, virtual memory system,
154–155
Page structure, virtual memory systems,
142–143
Page table
virtual memory system
cache memory with, 152–153
set-associative mapping, 145–146
virtual memory systems, 143
Paper and pencil method, multiplication of
unsigned numbers, 68
Parallel computers, history of, 3268
INDEXPDP-8 minicomputer, history of, 3
Pentium processors
real-life cache organization analysis,
Pentium IV processor cache,
128–129
set-associative mapping, 121virtual memory management, 155
X86 family, assembly language
programming for, 48–55
Performance measurement
computer systems, 6–11
multiprocessor architectures, 254
pipelining design, 186–187
RISC vs. CISC, 222–223
Personal computer (PC), history of, 3Physical connection (PC), multiprocessor
design, 236
Pipeline bubble (hazards), instruction
pipeline design, 188
Pipelined multiplication, carry-save
addition, 212–213
Pipeline interlock, Stanford microprocessor
without interlock pipe stages (MIPS),
225–226
Pipeline stall
data dependency, 189–192
NOP prevention method, 192–194reduction methods, 199–201
instruction dependency, 188
unconditional branch instructions,
194–196
instruction pipeline design, 187–188
Pipelining design
Alpha 2164 pipeline, 227–230
arithmetic pipline, 209–213
ﬁxed-point arithmetic, 209–211
ﬂoating-point arithmetic, 211–212
multiplication with carry-save
addition, 212–213
example processors, 201–207
ARM 1026EJ-S processor, 202–203
UltraSPARC III processor, 203–207
general concepts, 185–187instruction-level parallelism, 207–209
superscalar architectures, 207–209very long instruction word (VLIW),
209
instruction pipeline, 187–201
data dependency "stall," 189–201
hardware operand forwarding,
199–200
software operand forwarding,
200–201
instruction dependency "stall," 188conditional branch instructions,
196–199
unconditional branch instructions,
194–196
wrong instruction /operand,
prevention, 192–194
PMC-Sierra RM7000A 64-bit MIPS RISC
processor, real-life cache organiz-ation, 130
Pop operations
addressing modes, 19–20
central processing unit design, 86
Positional representation, number system,
59–63
PowerPC 604 processor cache, real-life
cache organization analysis, 129
Precomputing of branches, pipeline stall
reduction, 195–196
Predictor training, Alpha 2164 pipeline,
228–230
Prefetch unit, 1026EJ-S processor pipeline
design, 203
Primary memory, hierarchy parameters,
108–109
Processor control instructions, X86 family,
51–55
Program counter (PC)
central processing unit design, 83–85
instruction register, 86
interrupt handling, 94–95
one-bus organization, 89
interrupt-driven I /O systems, 169–170
relative addressing mode, 23
sequencing instructions, 28–30
Programmable ROM (PROM), basic
properties, 157–158
Programmed input /output system, basic
concepts, 164–167
Programming
assembly language programming
assembler directives and commands,
43–44
basic principles, 37–38
instruction mnemonics and syntax,
40–43
programing assembly and execution,
44–47
assemblers, 45
data structures, 45–46
linker and loader, 46–47
simple machines, 38–40
X86 family example, 47–55
instruction set architecture and design,
31–33INDEX 269Program status word (PSW) register
central processing unit design, 86
interrupt-driven I /O systems, 169–170
Pseudo-operations, assembly language
programming, assembler directives
and commands, 43–44
Push operations
addressing modes, 19–20
central processing unit design, 86
Quaternary base, number system, 59–63
Radix complement, number systems, 62
Radix conversion algorithm, number
systems, 60–61
Radix (radices), number systems, 59–63
RAM memory, integration technology and, 6Random access, memory hierarchy,
108–109
Random replacement algorithm, virtual
memory systems, 148
Random selection, cache memory replace-
ment, 121–124
Read-after-write data dependency, pipeline
stall, 190–194
Read-only memory (ROM), basic properties,
156–158
Read operation
cache miss, 127
main memory unit, 137–142
memory hierarchy, 108–109
memory locations and operations, 16–18
Real-life cache organization analysis
Pentium IV processor cache, 128–129
PMC-Sierra RM7000A 64-bit MIPS
RISC processor, 130
PowerPC 604 processor cache, 129
Reduced instruction set computers (RISCs)
advanced machines, 227–232
Alpha 21264 pipeline, 227–230
Compaq (formerly DEC) Alpha
21264, 227
SUN UltraSPARC III, 231–232
architecture and examples of, 5
CISC vs., 221–223
design principles, 218–220
overlapped register windows, 220–221pioneer (university) machines, 223–226
Berkeley RISC, 223–224
Stanford MIPS, 224–226
RISC /CISC evolution cycle, 217–218
Register indirect addressing, 22
Register set, central processing unit design,
83–88condition registers, 86instruction fetching registers, 86
memory access registers, 85–86
MIPS registers, 87–88
special-purpose address registers, 86
80x86 registers, 87
Relative addressing mode, 23
Replacement algorithms, virtual memory,
148–152
clock replacement algorithm, 150–152
ﬁrst-in-ﬁrst-out (FIFO) replacement,
148–149
least recently used (LUR) replacement,
149–150
random replacement, 148
Replacement policy, 148
Replacement techniques, cache memory,
121–124
Representation (scientiﬁc notation),
ﬂoating-point arithmetic, 74–75
Row address strobe (RAS), main memory
unit, 141–142
runtime, Instruction set architecture and, 1
Scale of integration, computer technology
development and, 5–6
Scientiﬁc notation, ﬂoating-point arithmetic,
74–75
SEARCH algorithm, X86 family
programming, 53–55
Seconardy memory, hierarchy parameters,
108–109
Segment address translation, virtual memory
system, 153–154
Segmentation, virtual memory system, 153
Segment pointers, central processing unit
design, 86
Semantic gap
computer architecture and, 4RISC /CISC evolution cycle, 218
Sequencing instructions, computer architec-
tures, 28–30
Sequential processing, pipelining vs., 186
Set-associative mapping
cache memory organization, 118–121
replacement techniques, 123–124,
126
virtual memory system, 145–146
Set ﬁeld, cache memory organization,
set-associative mapping, 118–121
Shared I /O systems
basic design, 163programmed input /output design,
165–167270
INDEXShared memory systems
history of, 3
multiprocessor architecture, multiple-
instruction multiple-data streams(MIMD), 247–249
Signed numbers, hardware structures for
addition and subtraction, 64–67
Sign-magnitude, number system, 61Single-instruction multiple-data streams
(SIMD), multiprocessor architecture
basic features, 244–246
Flynn classiﬁcation, 238–240
Hwang /Briggs classiﬁcation scheme,
241
Single-instruction single-data streams
(SISD), multiprocessor architecture,238–240
Hwang /Briggs classiﬁcation scheme,
240–241
Single-precision format, IEEE ﬂoating-point
standard, 77–79
Skillicorn classiﬁcation, multiprocessor
architecture, 242–244
Slave memory, deﬁned, 109
Small-scale integration (SSI), evolution of,
5–6
Software I /O polling, programmed input /
output design, 166–167
Software operand forwarding, data depen-
dency pipeline reduction, 200–201
Solid-state memory, hierarchy parameters,
107–109
Source registers
arithmetic and logical instructions,
27–28
CPU instruction cycle, addition instruc-
tions, 92–94
Space-time chart, pipelining design, 186
Spatial locality
cache memory, 111
memory hierarchy, 108–109
Special purpose computer system, 1
Speculative execution, pipeline stall
reduction, conditional branch
instructions, 198–199
Speed-up (S( n)) measurements
pipeline stall, data dependency, 192
pipelining design, 186–187
Speedup ( SU
o), performance analysis,
10–11
Stack operation, addressing modes, 19–20
Stack point (SP)
addressing modes, 19–20
central processing unit design, 86Stanford microprocessor without interlock
pipe stages (MIPS), design principles,224–226
STARAN system, multiprocessor
architecture, single-instruction
multiple-data streams (SIMD),
245–246
Static branch prediction, pipeline stall
reduction, conditional branch
instructions, 198–199
Static CMOS technology, main memory
units, 136–142
Static interconnections, multiprocessor
interconnections, 253
Static scheduling, instruction-level
parallelism, pipeline design, 207–209
Status bit, programmed input /output design,
166–167
Status ﬂags, X86 family, assembly language
programming for, 49–55
Status registers, I /O system design, 163
Store-fetch operation, data dependency
pipeline reduction, 200–201
Store-store operation, data dependency
pipeline reduction, 201
Subroutines, instruction set architecture and
design, 32–33
Subtraction, two’ s complement (2’ s)
representation, 64
Supercomputers, history of, 3Superscalar architectures (SPA), instruction-
level parallelism, pipeline design,
207–209
Switching techniques, multiprocessor
interconnection networks,
252–253
Synchronous buses
input /output system design, 178
multiprocessor interconnection networks,
252
Syntax, assembly language programming,
40–43
Synthetic operations, assembly language
programming, assembler directives
and commands, 44
System calls, assembly language
programming, assembler directives
and commands, 44
Table look-aside buffer (TLB), virtual
memory system, 146–148
cache memory with, 152–153set-associative mapping, 146–148INDEX 271Tag ﬁeld, cache memory organization
direct mapping, 115–116
fully associative mapping, 116–118
set-associative mapping, 118–121
Technological development in computing,
evolution of, 5–6
Temporal locality
cache memory, 111
memory hierarchy, 109
Tertiary memory, hierarchy parameters,
108–109
Three-address instruction, addressing
modes, 18–19
Three-bus organization, CPU datapath,
90–91
Throughput U(n) measurement
pipeline stall, data dependency, 192pipelining design, 186–187
Time-multiplexing, main memory unit,
141–142
Time units, instruction pipeline design,
187–188
Topology, multiprocessor interconnections,
253
Two-address instruction, addressing modes,
18–19
Two-bus organization, CPU datapath, 90
arithmetic operations, 93–94
Two’ s complement (2’ s) representation,
integer arithmetic, 63–74
UltraSPARC III RISC processor
design principles, 231–232
pipeline design, 203–207
pipeline stall reduction, 199
Unconditional branch instructions, pipeline
stall reductions, 194–196
Uniform memory access (UMA),
multiprocessor architecture, multiple-instruction multiple-data streams
(MIMD), 248–249
Unit time, pipelining design, 186
UNIVersal Automatic Computer (UNIVAC),
history of, 3
Unsigned numbers, paper and pencil
multiplication, 68
Valid bit, cache memory organization, fully
associative mapping, 118
VAX computer systems
architecture of, 4
history of, 3
Vectored interrupt, interrupt-driven input /
output (I /O) systems, 168Vertical microinstructions, central
processing unit design, 101–104
Very large-scale integration (VLSI),
evolution of, 5–6
Very long instruction word (VLIW)
architecture, instruction-level parallel-
ism, pipeline design, 207, 209
Virtual (logical) address, virtual memory
systems, 143
Virtual memory, 142–155
associative mapping, 144–145
cache memory, 152–153
paged segmentation, 154–155
Pentium memory management, 155
replacement algorithms (policies),
148–152
clock replacement algorithm,
150–152
ﬁrst-in-ﬁrst-out (FIFO) replacement,
148–149
least recently used (LUR)
replacement, 149–150
random replacement, 148
segment address translation, 153–154
segmentation, 153
set-associative mapping, 145–146
translation look-aside buffer (TLB),
146–148
Volatile memory, 156
Wafer-scale integration (WSI), evolution of,
5–6
Wide area networks (WAN), history of, 3
Word ﬁeld
cache memory organization
direct mapping, 115–116fully associative mapping, 116–118
set-associative mapping, 118–121
memory locations and operations, 16–18
Wormhole routing, multiprocessor
architecture, multiple-instructionmultiple-data streams (MIMD),
250–252
Write-after-write data dependency, pipeline
stall, 189–192
Write-allocate scheme, cache misses, 127
Write-back policy
cache hits, 125–127
cache miss, 127
Write-no-allocate policy, cache misses, 127
Write operation
cache memory policies, 124–127main memory unit, 137–142memory hierarchy, 108–109272
INDEXmemory locations and operations, 16–18
Write-through policy, cach hits, 125–127
X86 family, assembly language
programming for, 47–55Z computers, historical background, 2
Zero-address instructions, addressing
modes, 19–20INDEX 273